
Compiling graphical models has recently been
under intense investigation, especially for probabilistic modeling and processing. We present
here a novel data structure for compiling
weighted graphical models (in particular, probabilistic models), called AND/OR Multi-Valued
Decision Diagram (AOMDD). This is a generalization of our previous work on constraint networks, to weighted models. The AOMDD is
based on the frameworks of AND/OR search
spaces for graphical models, and Ordered Binary
Decision Diagrams (OBDD). The AOMDD is a
canonical representation of a graphical model,
and its size and compilation time are bounded exponentially by the treewidth of the graph, rather
than pathwidth as is known for OBDDs. We discuss a Variable Elimination schedule for compilation, and present the general APPLY algorithm
that combines two weighted AOMDDs, and also
present a search based method for compilation
method. The preliminary experimental evaluation is quite encouraging, showing the potential
of the AOMDD data structure.

1

Introduction

We present here an extension of AND/OR Multi-Valued
Decision Diagrams (AOMDDs) [13] to general weighted
graphical models, including Bayesian networks, influence
diagrams and Markov random fields.
The work on AOMDDs is based on two existing frameworks: (1) AND/OR search spaces for graphical models
and (2) decision diagrams (DD). AND/OR search spaces
[9] have proven to be a unifying framework for various
classes of search algorithms for graphical models. The
main characteristic is the exploitation of independencies
between variables during search, which can provide exponential speedups over traditional search methods that can

be viewed as traversing an OR structure. The AND nodes
capture problem decomposition into independent subproblems, and the OR nodes represent branching according to
variable values.
Decision diagrams are widely used in many areas of research, especially in software and hardware verification [5].
A BDD represents a Boolean function by a directed acyclic
graph with two sink nodes (labeled 0 and 1), and every internal node is labeled with a variable and has exactly two
children: low for 0 and high for 1. A BDD is ordered
if variables are encountered in the same order along every path. A BDD is reduced if all isomorphic nodes (i.e.,
with the same label and identical children) are merged, and
all redundant nodes (i.e., whose low and high children are
identical) are eliminated. The result is the celebrated reduced ordered binary decision diagram, or OBDD [3].
AOMDDs combine the two ideas, in order to create a decision diagram that has an AND/OR structure, thus exploiting problem decomposition. As a detail, the number of values is also increased from two to any constant, but this is
less significant for the algorithms.
A decision diagram offers a compilation of a problem. It
typically requires an extended offline effort in order to be
able to support polynomial (in its size) or constant time online queries. The benefit of moving from OR structure to
AND/OR is in a lower complexity of the algorithms and
size of the compiled structure. It typically moves from
being bounded exponentially in pathwidth pw∗ , which is
characteristic to chain decompositions or linear structures,
to being exponentially bounded in treewidth w∗ , which is
characteristic of tree structures (it always holds that w∗ ≤
pw∗ and pw∗ ≤ w∗ · log n).
Our contributions in this paper are as follows. (1) We formally describe the extension of AND/OR multi-valued decision diagram (AOMDD) to weighted graphical models.
(2) We describe the extension to weighted models of the
APPLY operator that combines two AOMDDs by an operation. The output of APPLY is still bounded by the product
of the sizes of the inputs. (3) We present two compilation

MATEESCU & DECHTER
A
0
0
0
0
1
1
1
1

B
0
0
1
1
0
0
1
1

f(ABC)
0
0
0
1
0
1
0
1

C
0
1
0
1
0
1
0
1

A

A

B

B

C

0

C

0

(a) Table

0

0

A

B

C

1

277

C

1

0

C

1

(b) Ordered tree

B

C

C

0

1

A

B

C

(c) Isomorphic nodes

B

C

B

C

0

1

(d) Redundant nodes

C

0

1

(e) OBDD

Figure 1: Boolean function representations
A
f1(AC)
f2(AB)
f3(ABE)

B

C

A:

h4(A)

B:

C2(AB)

E:

C3(ABE)

C:

C1(AC)

D:

C4 (BCD)

A

h3(AB)

AB bucket-B
AB

f4(BCD)

ABE
E

D

(a) Model

h1(BC)

(b) VE execution

bucket-A

A

h2(AB)

bucket-E

AB

ABC bucket-C
BC

BCD bucket-D

algorithms for AOMDDs. One is based on the repeated application of APPLY along a Variable Elimination schedule.
The other is based on search. Both schemes are exponential in the treewidth of the model. (4) We provide encouraging preliminary experimental evaluation of the search based
compilation method. (5) We discuss how AOMDDs relate
to various earlier and recent works, providing a unifying
perspective for all these methods.
The structure of the paper is as follows: Section 2 provides
preliminaries. Section 3 gives an overview of AND/OR
search space. Section 4 describes the AOMDD for constraint networks, the Variable Elimination schedule for
compilation and the APPLY operator, and a search based
compilation scheme. Section 5 contains the main contribution: the extension of AOMDDs to weighted models, and
a discussion of their canonical form and the extensions of
the compilation schedule and APPLY operator. Section 6
provides experimental evaluation and section 7 concludes.

Preliminaries

In this section we describe graphical models, Binary Decision Diagrams (OBDDs) and Variable Elimination.
D EFINITION 1 (graphical model) A graphical model R
is a 4-tuple, R = hX, D, F, ⊗i, where: (1) X =
{X1 , . . . , Xn } is a set of variables; (2) D =
{D1 , . . . , Dn } is the set of their respective finite domains
of values; (3) F = {f1 , . . . , fr } is a set of discrete realvalued functions, each defined over a subset of variables
Si ⊆ X, called its scope,
Q and
P sometimes denoted by
scope(fi ). (4) ⊗i fi ∈ { i fi , i fi , 1i fi } is a combination operator1 . The graphical model represents the combination of all its functions: ⊗ri=1 fi . A reasoning task is
1

Examples of graphical models include Bayesian networks,
constraint networks, influence diagrams, Markov networks.

(c) Bucket tree

Figure 2: Variable Elimination

2

based on a projection (elimination) operator, ⇓, and is defined by: ⇓Z1 ⊗ri=1 fi , . . . , ⇓Zt ⊗ri=1 fi , where Zi ⊆ X.

The combination operator can be defined axiomatically [17].

D EFINITION 2 (universal equivalent graphical model)
Given a graphical model R = hX, D, F1 , ⊗i the universal equivalent model of R is u(R) = hX, D, F2 =
{⊗fi ∈F1 fi }, ⊗i.
Two graphical models are equivalent if they represent the
same set of solutions. Namely, if they have the same universal model.
D EFINITION 3 (primal graph) The primal graph of a
graphical model is an undirected graph that has variables
as its vertices and an edge connects any two variables that
appear in the scope of the same function.
A pseudo tree resembles the tree rearrangements [11]:
D EFINITION 4 (pseudo tree) A pseudo tree of a graph
G = (X, E) is a rooted tree T having the same set of
nodes X, such that every arc in E is a back-arc in T (i.e.,
it connects nodes on the same path from root).
D EFINITION 5 (induced graph,
induced width,
treewidth, pathwidth) An ordered graph is a pair (G, d),
where G is an undirected graph, and d = (X1 , ..., Xn )
is an ordering of the nodes. The width of a node in an
ordered graph is the number of neighbors that precede it in
the ordering. The width of an ordering d, denoted by w(d),
is the maximum width over all nodes. The induced width
of an ordered graph, w∗ (d), is the width of the induced
ordered graph obtained as follows: for each node, from
last to first in d, its preceding neighbors are connected in
a clique. The induced width of a graph, w∗ , is the minimal
induced width over all orderings. The induced width is
also equal to the treewidth of a graph. The pathwidth
pw∗ of a graph is the treewidth over the restricted class of
orderings that correspond to chain decompositions.
2.1

Binary Decision Diagrams

Decision diagrams are widely used in many areas of research to represent decision processes. In particular, they

278

MATEESCU & DECHTER
A

A

A

A

[]

f1(AC)

0

1

0

1

f2(AB)

B

B

B

B

f3(ABE)

B [A]

B

f4(BCD)

E

[AB] E

E

0

C
[AB]

D [BC]

D

(a) Graphical model

C

0 1

1
C

E

0

1

D

D

0 1

0 1

0 1

(b) Pseudo tree

0
C

E

0

1

D
0 1

0 1

0

1
C

E

0

1

D

D

0 1

0 1

0 1

0

1

D

D

D

0 1

0 1

0 1

(c) Search tree

1

E

C

0 1

C
0

E
1

0
C

0 1

E

0

1

1
C

0 1

0

E
1

D

D

D

D

0 1

0 1

0 1

0 1

0 1

C
0

1

(d) Context minimal graph

Figure 3: AND/OR search space
can be used to represent functions. Due to the fundamental importance of Boolean functions, a lot of effort has
been dedicated to the study of Binary Decision Diagrams
(BDDs), which are extensively used in software and hardware verification [5, 15, 12, 3].
A BDD is a representation of a Boolean function. Given
B = {0, 1}, a Boolean function f : Bn → B, has n arguments, X1 , · · · , Xn , which are Boolean variables, and takes
Boolean values. A Boolean function can be represented by
a table (see Figure 1(a)), but this is exponential in n, and
so is the binary tree representation in Figure 1(b). The goal
is to have a compact representation, that also supports efficient operations between functions. OBDDs [3] provide
such a framework by imposing the same order to the variables along each path in the binary tree, and then applying
the following two reduction rules exhaustively:
(1) isomorphism: merge nodes that have the same label
and the same respective children (see Figure 1(c)).
(2) redundancy: eliminate nodes whose low (zero) and
high (one) edges point to the same node, and connect
the parent of removed node directly to the child of removed node (see Figure 1(d)).
The resulting OBDD is shown in Figure 1(e).
2.2

Variable Elimination (VE)

3

AND/OR Search Space

The AND/OR search space [9] is a recently introduced
unifying framework for advanced algorithmic schemes for
graphical models. Its main virtue consists in exploiting independencies between variables during search, which can
provide exponential speedups over traditional search methods oblivious to problem structure.
3.1

AND/OR Search Trees

Given a graphical model M = hX, D, Fi, its primal graph
G and a pseudo tree T of G, the associated AND/OR
search tree, ST (R), has alternating levels of OR and AND
nodes. The OR nodes are labeled Xi and correspond to the
variables. The AND nodes are labeled hXi , xi i and correspond to the value assignments in the domains of the variables. The structure of the AND/OR search tree is based
on the underlying pseudo tree T . The root of the AND/OR
search tree is an OR node labeled with the root of T . The
children of an OR node Xi are AND nodes labeled with
assignments hXi , xi i. that are consistent with the assignments along the path from the root. The children of an
AND node hXi , xi i are OR nodes labeled with the children
of variable Xi in the pseudo tree T .
The AND/OR search tree can be traversed by a depth first
search algorithm, thus using linear space. It was already
shown [11, 1, 6, 9] that:

Variable elimination (VE) [2, 8] is a well known algorithm for inference in graphical models. Consider a graphical model R = hX, D, Fi and an elimination ordering
d = (X1 , X2 , . . . , Xn ) (Xn is eliminated first, X1 last).
Each function placed in the bucket of its latest variable in
d. Buckets are processed from Xn to X1 by eliminating
the bucket variable (the functions residing in the bucket are
combined together, and the bucket variable is projected out)
and placing the resulting function (also called message) in
the bucket of its latest variable in d. Figure 2(a) shows a
graphical model and 2(b) the execution of VE.

T HEOREM 1 Given a graphical model M and a pseudo
tree T of depth m, the size of the AND/OR search tree
based on T is O(n k m ), where k bounds the domains of
variables. A graphical model having treewidth w∗ has a
pseudo tree of depth at most w∗ log n, therefore it has an
∗
AND/OR search tree of size O(n k w log n ).

VE execution defines a bucket tree, by linking the bucket
of each Xi to the destination bucket of its message (called
the parent bucket). A node in the bucket tree has a bucket
variable, a collection of functions, and a scope (the union
of the scopes of its functions). If the nodes of the bucket
tree are replaced by their respective bucket variables, we
obtain a pseudo tree (see Figure 2(c) and 3(b)).

The AND/OR search tree may contain nodes that root identical conditioned subproblems. These nodes are said to be
unifiable. When unifiable nodes are merged, the search
space becomes a graph. Its size becomes smaller at the
expense of using additional memory by the search algorithm. The depth first search algorithm can therefore be
modified to cache previously computed results, and retrieve

3.2

AND/OR Search Graphs

MATEESCU & DECHTER

279

A

A

P(A=0)
0

B

C

B

P(B=0|A=0)

P(B=1|A=0)

0

E

1

D
E
P(E=0|A=0,B=0)

A

E

E

P(E=1|A=0,B=0)

0

B

D

1

D

P(D=0|B=0,C=0)×
P(C=0|A=0)

C

0

P(E=0|A=0,B=1)

0

1

C

C

0

D
P(E=1|A=0,B=1)
1

0

1

C

C

P(D=0|B=0,C=1)× P(D=1|B=0,C=0)×
P(C=1|A=0)
P(C=0|A=0)

P(D=1|B=0,C=1)× P(D=0|B=1,C=0)×
P(C=1|A=0)
P(C=0|A=0)

P(D=0|B=1,C=1)× P(D=1|B=1,C=0)×
P(C=1|A=0)
P(C=0|A=0)

P(D=1|B=1,C=1)×
P(C=1|A=0)

1

1

1

1

0

0

0

Figure 4: Arc weights for probabilistic networks
3.3

A
0

B

…

1

E

…

Weighted AND/OR Search Graphs

2

C

…

E

…

D

…

Figure 5: Meta-node
them when the same nodes are encountered again. Some
unifiable nodes can be identified based on their contexts.
We can define graph based contexts for both OR nodes and
AND nodes, just by expressing the set of ancestor variables
in T that completely determine a conditioned subproblem.
However, it can be shown that using caching based on OR
contexts makes caching based on AND contexts redundant,
so we will only use OR caching.
Given a pseudo tree T of an AND/OR search space, the
context of an OR node X, denoted by context(X) =
[X1 . . . Xp ], is the set of ancestors of X in T ordered descendingly, that are connected in the primal graph to X or
to descendants of X.
It is easy to verify that the context of X separates the subproblem below X from the rest of the network. The context minimal AND/OR graph is obtained by merging all the
context unifiable OR nodes. It was shown that [1, 9]:
T HEOREM 2 Given a graphical model M, its primal
graph G and a pseudo tree T , the size of the context min∗
imal AND/OR search graph based on T is O(n k wT (G) ),
∗
where wT (G) is the induced width of G over the depth first
traversal of T , and k bounds the domain size.
Figure 3(a) shows the primal graph of a graphical model
defined by the functions f1 , . . . , f4 , which are assumed to
be strictly positive (i.e., every assignment is valid). Figure
3(b) shows a pseudo tree for the graph. The dotted lines are
edges in the primal graph, and back-arcs in the pseudo-tree.
The OR context of each node is shown in square brackets.
Figure 3(c) shows the AND/OR search tree and 3(d) shows
the context minimal AND/OR graph.

In some cases (e.g. constraint networks), the functions of
the graphical model take binary values (0 and 1, or true and
false). In this case, an AND/OR search graph expresses
the consistency (valid or not) of each assignment, and can
associate this value with its leaves.
In more general cases, which are the focus of this paper, the
functions of the graphical model take (positive) real values, called weights. For example, in Bayesian networks
the weights express the conditional probability. In the
more general case of weighted models, it is useful to associate weights to the internal OR-AND arcs in the AND/OR
graph, to maintain the global function decomposition and
facilitate the merging of nodes.
D EFINITION 6 (buckets relative to a backbone tree)
Given a graphical model R = hX, D, F, ⊗i and a
backbone tree T , the bucket of Xi relative to T , denoted
by BT (Xi ), is the set of functions whose scopes contain
Xi and are included in pathT (Xi ), which is the set of
variables from the root to Xi in T . Namely, BT (Xi ) =
{f ∈ F |Xi ∈ scope(f ), scope(f ) ⊆ pathT (Xi )}.
D EFINITION 7 (OR-AND weights) Given an AND/OR
tree ST (R), of a graphical model R, the weight
w(n,m) (Xi , xi ) of arc (n, m) where Xi labels n and
xi labels m, is the combination (e.g.
product) of
all the functions in BT (Xi ) assigned by values along
the path to m, πm .
Formally, w(n,m) (Xi , xi ) =
⊗f ∈BT (Xi ) f (asgn(πm )[scope(f )]).
Figure 4 shows a belief network, a DFS tree that drives
its weighted AND/OR search tree, and a portion of the
AND/OR search tree with the appropriate weights on the
arcs expressed symbolically. In this case the bucket of E
contains the function P (E|A, B), and the bucket of C contains two functions, P (C|A) and P (D|B, C). Note that
P (D|B, C) belongs neither to the bucket of B nor to the
bucket of D, but it is contained in the bucket of C, which
is the last variable in its scope to be instantiated in a path
from the root of the pseudo tree.

280

MATEESCU & DECHTER
A
D

A

G

0

B

1

B

C

B

F

E

A

H

C

0

F

D

E

G

H

C
0

B
1

0

C
1

0

C
1

0

C
1

0

F
1

0

1

F
1

0

F
1

0

F
1

0

1

(a) Primal graph and pseudo tree
D

m7

A

0

m7
0

1

0

C
1

0

0

1

C
1

0

F

C
1

0

0

D
1

0

1

0

F
1

0

F
1

B

F

0

1

G

E
1

0

0

0

0

1

0

1

m6

G
1

0

H
1

0

0

1

D

E

G

0

0

C
0

1

D
1

0

0

C
0

1

0

1

0

0

H
1

0

1

A
B

1

0

0

F

B

0

F
0

1

A

1

0

G

1

0

1

0

0

C
0

H
1

0

0

C

C

1

H
1

C

B

F
1

G
1

1

F

F

1

B

A

B
1

B
1

E

D
1

H
1

H

A

A

C
1

0

1

m3
0

B

G
1

(c) Final AOMDD (message m7 )

H
1

1

m3

F

B

A

0

0

0

1

m6

C

1

1

C
D

G

0

B
1

B
0

E
1

A

1

B

B
1

C
0

0

A

A
0

B
0

D
1

D

F

D

D

D

D

D

1

C

0 1

0

1
m5

C9

0

C

0

E

0

C

D
1

0

0

1

1
C8

D

C
1

0

0

1

1

m2

1

0

1

0

0

1

0 1

0 1

C6

C7

0

C

1

0

1

0

1

E

C3

0

0

1

H

1

0 1
C4

A
1

0

H

B

G
1

E

1

A

G

G

F

F

G

F
G

G

1

G
0

0

E

F

F

F
0

B

G

G

E

m1

A

1

B

H

m1

A

A

E

E

E

0

C5

m2

A
1

D

D

D

m4

m5
0

0 1

1
m4

G

0

H
1

0

1

F

0 1

0 1

G

C1

C2

F

H

H

H

1

0

(d) OBDD equivalent to m7

(b) Variable Elimination based compilation

Figure 6: Execution of VE with AOMDDs

4

AND/OR Multi-Valued Decision Diagram
for Constraint Networks

Constraint networks have only binary valued functions. In
[13] we presented a compilation scheme for AOMDDs
for constraint networks based on the Variable Elimination
schedule. For completeness, we only provide below the
main ideas for constraint networks, and then present the
current contribution extending the AOMDD for weighted
graphical models.
The context minimal graph is a data structure that is equivalent to the given graphical model, in the sense that it represents the same set of solutions, and any query on the graphical model can be answered by inspecting the context minimal graph. Our goal is to shrink the context minimal graph
even further, by identifying mergeable nodes beyond those
based on context. Redundant nodes can also be identified
and removed.
Suppose we are given an AND/OR search graph (it could
also be a tree initially). The reduction rules of OBDDs are
also applicable to it, if we maintain the semantics. In particular, we have to detail the treatment of AND nodes and
OR nodes. If we consider only reduction by isomorphism,
then the AND/OR graph can be processed by ignoring the
AND or OR attributes of the nodes. If we consider reduction by redundancy, then it is useful to group each OR node

together with its AND children into a meta-node.
D EFINITION 8 (meta-node) A nonterminal meta-node v
in an AND/OR search graph consists of an OR node labeled
var(v) = Xi and its ki AND children labeled hXi , xi1 i,
. . . , hXi , xiki i that correspond to its value assignments.
We will sometimes abbreviate hXi , xij i, by xij . Each
AND node labeled xij points to a list of child meta-nodes,
u.childrenj .
Consider the pseudo tree in Figure 3(b). An example of
meta-node corresponding to variable A is given in Figure 5,
assuming three values. That is just a portion of an AND/OR
graph, where redundant meta-nodes were removed. For
A = 0, the child meta-node has variable B. For A = 1, B
is irrelevant so the corresponding meta-node was removed,
and there is an AND arc pointing to E and C. For A = 2,
both B and C are irrelevant. This example did not take into
account possible weights on the OR-AND arcs.
4.1

Compiling AOMDDs by Variable Elimination

Consider the network defined by X = {A, B, . . . , H},
DA = . . . = DH = {0, 1} and the constraints (⊕ denotes
XOR): C1 = F ∨H, C2 = A∨¬H, C3 = A⊕B⊕G, C4 =
F ∨G, C5 = B∨F , C6 = A∨E, C7 = C∨E, C8 = C⊕D,
C9 = B ∨ C. The constraint graph is shown in Figure 6(a).

MATEESCU & DECHTER
Algorithm 1:

APPLY (v1 ;

w1 , . . . , wm )

: AOMDDs f with nodes vi and g with nodes wj , based on
compatible pseudo trees T1 , T2 that can be embedded in T .
var(v1 ) is an ancestor of all var(w1 ), . . . , var(wm ) in T .
var(wi ), var(wj ) are not ancestor-descendant in T .
output
: AOMDD v1 ./ (w1 ∧ . . . ∧ wm ), based on T .
if H1 (v1 , w1 , . . . , wm ) 6= null then return H1 (v1 , w1 , . . . , wm )
if (any of v1 , w1 , . . . , wm is 0) then return 0
if (v1 = 1) then return 1
if (m = 0) then return v1
create new nonterminal meta-node u
var(u) ← var(v1 ) (call it Xi , with domain Di = {x1 , . . . , xki } )
for j ← 1 to ki do
u.childrenj ← φ // children of j-th AND node of u
if ( (m = 1) and (var(v1 ) = var(w1 ) = Xi ) ) then
temp Children ← w1 .childrenj
input

1
2
3
4
5
6
7
8
9
10
11
12

else

13

group nodes from v1 .childrenj ∪ temp Children in several {v 1 ;
w1 , . . . , wr }
for each {v 1 ; w1 , . . . , wr } do
y ← APPLY(v 1 ; w1 , . . . , wr )
if (y = 0) then
u.childrenj ← 0; break

14
15
16
17
18
19

temp Children ← {w1 , . . . , wm }

where F has two children, G and H. Some of the nodes
corresponding to F have two outgoing edges for value 1.
The processing continues in the same manner The final output of the algorithm, which coincides with m7 , is shown
in Figure 6(c). The OBDD based on the same ordering d
is shown in Fig. 6(d). Notice that the AOMDD has 18
nonterminal nodes and 47 edges, while the OBDD has 27
nonterminal nodes and 54 edges.
We present the APPLY algorithm for combining AOMDDs
for constraints. It was shown in [13] that the complexity of
the APPLY is at most quadratic in the input.
In [13] it was shown that the time and space complexity
of the VE based compilation scheme is exponential in the
treewidth of the model.
4.2

Compiling AOMDDs by AND/OR Search

else
u.childrenj ← u.childrenj ∪ {y}

20
21

if (u.children1 = . . . = u.childrenki ) then
return u.children1

22

if (H2 (var(u), u.children1 , . . . , u.childrenki ) 6= null)
then
return H2 (var(u), u.children1 , . . . , u.childrenki )

23

281

24 Let H1 (v1 , w1 , . . . , wm ) = u
25 Let H2 (var(u), u.children1 , . . . , u.childrenki ) = u
26 return u

Consider the ordering d = (A, B, C, D, E, F, G, H). The
pseudo tree induced by d is given in Fig. 6(a). Figure 6(b)
shows the execution of VE with AOMDDs along ordering
d. Initially, the constraints C1 through C9 are represented
as AOMDDs and placed in the bucket of their latest variable in d. Each original constraint is represented by an
AOMDD based on a chain. For bi-valued variables, they
are OBDDs, for multiple-valued they are MDDs (multivalued decision diagrams). Note that we depict metanodes: one OR node and its two AND children, that appear inside each larger square node. The dotted edge corresponds to the 0 value (the low edge in OBDDs), the solid
edge to the 1 value (the high edge). We have some redundancy in our notation, keeping both AND value nodes and
arc types (doted arcs from “0” and solid arcs from “1”).
The VE scheduling is used to process the buckets in reverse order of d. A bucket is processed by joining all the
AOMDDs inside it, using the APPLY operator (described
further). However, the step of eliminating the bucket variable will be omitted because we want to generate the full
AOMDD. In our example, the messages m1 = C1 ./ C2
and m2 = C3 ./ C4 are still based on chains, so they are
still OBDDs. Note that they still contain the variables H
and G, which have not been eliminated. However, the message m3 = C5 ./ m1 ./ m2 is not an OBDD anymore.
We can see that it follows the structure of the pseudo tree,

We describe here a search based approach for compiling
an AOMDD. Theorem 2 ensures that the context minimal
(CM) graph can be traversed by AND/OR search in time
∗
and space O(n k wT (G) ). When full caching is used, the
trace of AND/OR search (i.e., the AND/OR graph traversed
by the algorithm) is a subset of the CM graph (if pruning
techniques are used, some portions of the CM graph may
not be traversed). When the AND/OR search algorithm terminates, its trace is an AND/OR graph that expresses the
original graphical model. We can therefore apply the reduction rules (isomorphism and redundancy) to the trace of
the AND/OR search in a single bottom up pass, that has
complexity linear in the size of the trace. In fact, the reduction rules can be included in the depth first AND/OR
search algorithm itself: whenever the entire subgraph of a
meta-node has been visited, the algorithm can check for
isomorphism between the current node and metanodes of
the same variable, and also check redundancy, before the
search retracts to the parent meta-node. The end result
will be the AOMDD of the original graphical model. The
time and space complexity of this scheme is bounded in
the worst case by that of exploring the CM graph, which is
given in Theorem 2 (i.e., exponential in the treewidth of the
model). In Section 6 we provide preliminary evaluation of
the search based compilation.

5

AND/OR Multi-Valued Decision Diagram
for Weighted Graphs

We will now describe an extension of AOMDDs to
weighted graphical models, which include probabilistic
graphical models. The functions defining the model can in
this case take arbitrary positive real values. The AND/OR
search space is well defined for such graphical models, and
in particular the context minimal graph is a decision diagram that represents the same function as the model. The

282

MATEESCU & DECHTER
A

M

A
0
0
1
1
0
0
1
1

0

1

B

A

A

C

B f(M,A,B)
0
12
1
5
0
18
1
2
0
4
1
10
0
6
1
4

M
0
0
0
0
1
1
1
1

B
0
0
1
1
0
0
1
1

M

A
B

C
M
0
0
0
0
1
1
1
1

M

M

C g(M,B,C)
0
3
1
5
0
14
1
12
0
9
1
15
0
7
1
6

0

1

0

1

B

B

B

B

12

5

18

2

4

10

6

4

0

1

0

1

0

1

0

1

C

C

C

C

C

C

C

C

3

5

0

1

14
0

36 60

12

3

5

1

0

1

70 60

(a) Graphical model

14
0

54 90

12

9

15

7

6

9

15

7

1

0

1

0

1

0

1

0

28 24

36 60

70 60

54 90

0

1

A

A

0

1

0

1

B

B

B

B

12

5

18

2

0

1

0

1

6

C

4

10

6

4

1

0

1

0

C

C

C

1

28 24

3

5

0

1

14
0

12

9

15

7

6

1

0

1

0

1

(c) AND/OR context
36 60 minimal graph

(b) AND/OR search tree

Figure 7: Weighted graphical model
M

844

M

M

1/2
0

1

0

1/2

1
0

A

A

A

1

A
A

0

1

0

1

0

1

0

1

B

B

B

B

B

B

B

B

12

5

18

2

0

1

0

1

8

26 C

C

3/8
0

5/8
1

14/26
0

4
0

10

6

4

1

0

1

24 C
12/26
1

3/8
0

8*12

26*5

0

1

13 C
5/8
1

14/26
0

8*18

26*2

0

1

C

12/26
1

3/8
0

24*4

13*10

0

1

C

5/8
1

(a) Normalizing level of C

7/13
0

1

3/8

13*4

0

1

0

1

(b) Promoting constants

7/13
0

0

1

B

96/226

130/226 144/196

0

1

C

5/8

196/422

B

24*6

C

6/13

226/422

52/196

0

1

C

6/13
1

3/8
0

C

5/8

7/13

1

0

6/13
1

(c) AOMDD

Figure 8: Normalizing values bottom up
reduction rules (merge isomorphic nodes and reduce redundant nodes) are also well defined for weighted models
(if we operate with meta-nodes), and guaranteed to produce equivalent decision diagrams. For example, isomorphic nodes should have the same variable, the same sets
of children, and the same weights on their respective ORAND arcs. If we start with the AND/OR tree and apply the
isomorphism rule exhaustively, we are guaranteed to obtain
a graph at least as compact as the context minimal graph.
This is because OR nodes that have the same context also
represent isomorphic meta-nodes when the isomorphic rule
was applied exhaustively to all the levels below.
However, the property of being a canonical representation
of a function is lost in the case of weighted graphs, if we
only use the usual reduction rules.
Figure 7(a) shows a weighted graphical model, defined by
two (cost) functions, f (M, A, B) and g(M, B, C). Assuming the order (M, A, B, C), Figure 7(b) shows the
AND/OR search tree. The arcs are labeled with function
values, and the leaves show the value of the corresponding full assignment (which is the product of numbers on
the arcs of the path). We can see that either value of M (0
or 1) gives rise to the same function (because the leaves in
the two subtrees have the same values). Therefore, the universal model of the conditioned subproblem for M = 0 is
identical to that for M = 1. However, the two subtrees can
not be identified as representing the same function by the
usual reduction rules, because of different weights on the
arcs. Figure 7(c) shows the context minimal graph, which

has a compact representation of each subtree, but does not
share any of their parts. In these figures we do not show the
contours of meta-nodes, to reduce clutter.
What we would like in this case is to have a method of
recognizing that the left and right subtrees corresponding to
M = 0 and M = 1 represent the same function. We do this
by normalizing the weights in each level, and processing
bottom up by promoting the normalization constant.
In Figure 8(a) the weights on the OR-AND arcs of level C
have been normalized, and the normalization constant was
promoted up to the OR node value. In Figure 8(b) the normalization constants are promoted upwards again by multiplication into the OR-AND weights. This process does
not change the value of each full assignment, and therefore
produces equivalent graphs. We can see now that some of
the C level (meta) nodes are mergeable. Continuing this
process gives the final AOMDD for the weighted model, in
Figure 8(c).
D EFINITION 9 (weighted AOMDD) A weighted AOMDD
is an AND/OR graph (with meta-nodes), where for each
OR node, the emanating OR-AND arcs have an associated
weight, such that their sum is 1, and the root meta-node
has a weight (the resulting normalization constant). The
terminal nodes are just 0 and 1.
The following theorem ensures the that the weighted
AOMDD is a canonical representation.
T HEOREM 3 Given two equivalent weighted graphical

MATEESCU & DECHTER
Network
cpcs54
cpcs179
cpcs360b
cpcs422b
c432
c499
s386
s953
90-10-1
90-14-1
90-16-1
90-24-1
EA3
EA4
EA5
EA6
bm-05-01
bm-05-02
mm-03-08-03
mm-04-08-03

Class

CPCS

ISCAS

GRID

LINKAGE

PRIMULA

(n, d)
(54, 2)
(179, 4)
(360, 2)
(422, 2)
(432, 2)
(499, 2)
(172, 2)
(440, 2)
(100, 2)
(196, 2)
(256, 2)
(576, 2)
(711, 5)
(775, 5)
(944, 5)
(1136, 5)
(700, 2)
(1418, 2)
(1220, 2)
(1418, 2)

e
5
5
50
15
40
40
10
120
10
10
20
120
0
0
0
0
100
100
100
300

(w*, h)
(12, 20)
(7, 13)
(16, 21)
(16, 33)
(21, 37)
(20, 50)
(17, 30)
(29, 51)
(10, 31)
(16, 60)
(18, 66)
(17, 77)
(14, 46)
(16, 62)
(13, 53)
(17, 79)
(17, 36)
(16, 50)
(17, 53)
(17, 49)

Zeros (%)
0.00
0.32
0.68
0.78
48.98
47.76
48.18
45.45
42.42
45.25
43.99
44.66
37.49
37.34
36.46
36.60
49.17
49.44
48.94
47.89

Time (sec)
1.02
110.24
105.49
57.74
63.38
20.45
6.27
180.60
0.78
68.16
278.05
102.78
4.55
5.88
6.75
18.12
105.19
60.97
748.71
48.44

283
#cm
24,422
31,389
159,863
429,270
345,401
106,659
33,806
183,199
6,812
317,527
1,305,719
569,282
29,144
21,246
25,973
43,202
275,445
143,866
898,631
158,584

#aomdd
23,845
9,702
155,670
25,754
119,109
105,921
4,425
46,890
3,544
73,245
256,990
179,668
15,654
14,653
12,377
27,960
115,327
74,818
384,049
50,718

Ratio
1.02
3.24
1.03
16.67
2.90
1.01
7.64
3.91
1.92
4.34
5.08
3.17
1.86
1.45
2.10
1.55
2.39
1.92
2.34
3.13

Table 1: Results for experiments with 20 belief networks from 5 problem classes.
models that accept a common pseudo tree T , normalizing
arc values together with exhaustive application of reduction rules yields the same AND/OR graphs.
The proof is omitted here for space reasons. We only mention that the proof is by structural induction bottom up over
the layers of the AND/OR graph.
The APPLY algorithm needs minimal modifications now to
operate on weighted AOMDDs. The hash function H2 ,
which hashes meta-nodes, has to take as extra arguments
the weights of the meta-node. Similarly, when checking
redundancy in line 21, the weights should also be equal for
the node to be redundant, and their common value has to be
promoted by multiplication. When checking isomorphism
in line 23, the corresponding weights are checked via the
hash function H2 . The same VE schedule can now be used
to compile an AOMDD for a weighted graphical model.

6

Experimental Evaluation

Our experimental evaluation is in preliminary stages, but
the results we have are already encouraging. We ran the
search based compile algorithm, by recording the trace
of the AND/OR search, and then reducing the resulting
AND/OR graph bottom up. In these results we only applied
the reduction by isomorphism and still kept the redundant
meta-nodes.
Table 1 shows the results for 20 belief networks from 5
problem classes: medical diagnosis (CPCS), digital circuits (ISCAS), deterministic grid networks (GRID), genetic linkage analysis (LINKAGE) as well as relational belief networks (PRIMULA). For each network we chose randomly e variables and set their values as evidence. For each

query we recorded the compilation time in seconds, the
number of OR nodes in the context minimal graph explored
(#cm) and the size of the resulting AOMDD (#aomdd).
In addition, we also computed the compression ratio of the
AOMDD structure as ratio = #cm/#aomdd. We also
report the number of variables (n), domain size (d), induced
width (w∗ ), pseudo tree depth (h), as well as the percentage
of zero probability tuples (zeros (%)) for each test instance.
We see that in a few cases the compression ratio is significant (e.g., cpcs422b 16.67%, s386 7.64%). Our future work will include the reduction rule by redundancy, as
well as the compilation algorithm by Variable Elimination
schedule.

7

Conclusion and Discussion

We presented the new data structure of weighted AOMDD,
as a target for compilation of weighted graphical models.
It is based on AND/OR search spaces and Binary Decision Diagrams. We argue that the AOMDD has an intuitive
structure, and can easily be incorporated into other already
existing algorithm (e.g., join tree clustering). We provide
two compilation methods, one based on Variable Elimination and the other based on search, both being time and
space exponential in the treewidth of the graphical model.
The preliminary experimental evaluation is quite encouraging, and shows the potential of the new AOMDD data
structure.
Compiling graphical models into weighted AOMDDs also
extends decision diagrams for the computation of semiring valuations [18], from linear variable ordering into treebased partial ordering. This provides an improvement

284

MATEESCU & DECHTER

of the complexity guarantees to exponential in treewidth,
rather than pathwidth.
There are various lines of related research. We only mention here: deterministic decomposable negation normal
form (d-DNNF) [7]; case factor diagrams [14]; compilation of CSPs into tree-driven automata [10]; and the recent
work on compilation [16, 4]. We think that our framework
using AND/OR search graphs has a unifying quality that
helps make connections among seemingly different compilation techniques.
The approach of compiling graphical models into
AOMDDs may seem to go against the current trend in
model checking, which moves away from BDD-based algorithms into CSP/SAT based approaches. However, algorithms that are search-based and compiled data-structures
such as BDDs differ primarily by their choices of time vs
memory. When we move from regular OR search space to
an AND/OR search space the spectrum of algorithms available is improved for all time vs memory decisions. We believe that the AND/OR search space clarifies the available
choices and helps guide the user into making an informed
selection of the algorithm that would fit best the particular
query asked, the specific input function and the computational resources.

Acknowledgments
We thank Radu Marinescu for his help with the experimental evaluation. This research was supported in part by the
NSF grant IIS-0412854.



In this paper we compare search and inference
in graphical models through the new framework of AND/OR search. Specifically, we compare Variable Elimination (VE) and memoryintensive AND/OR Search (AO) and place algorithms such as graph-based backjumping and
no-good and good learning, as well as Recursive Conditioning [7] and Value Elimination [2]
within the AND/OR search framework.

1

work. We show that there is no principled difference between memory-intensive search restricted to fixed variable
ordering and inference beyond: 1. different direction of
exploring a common search space (top down for search vs.
bottom-up for inference); 2. different assumption of control strategy (depth-first for search and breadth-first for inference). We also show that those differences have no practical effect, except under the presence of determinism. Our
analysis assumes a fixed variable ordering. Some of the
conclusions may not hold for dynamic variable ordering.
Section 2 provides background. Section 3 compares VE
with AO search. Section 4 addresses the effect of advanced
algorithmic schemes and section 5 concludes.

INTRODUCTION
2

It is convenient to classify algorithms that solve reasoning problems of graphical models as either search (e.g.,
depth first, branch and bound) or inference (e.g., variable
elimination, join-tree clustering). Search is time exponential in the number of variables, yet it can be accomplished
in linear memory. Inference exploits the model’s graph
structure and can be accomplished in time and space exponential in the problem’s tree-width. When the tree-width
is big, inference must be augmented with search to reduce the memory requirements. In the past three decades
search methods were enhanced with structure exploiting
methods. These improvements often require substantial
memory, making the distinction between search and inference fuzzy. Recently, claims regarding the superiority of
memory-intensive search over inference or vice-versa are
made [3]. Our aim is to clarify this relationship and to create cross-fertilization using the strengths of both schemes.
In this paper we compare search and inference in graphical
models through the new framework of AND/OR search,
recently introduced [11]. Specifically, we compare Variable Elimination (VE) against memory-intensive AND/OR
Search (AO), and place algorithms such as graph-based
backjumping, no-good and good learning, and look-ahead
schemes [9], as well as Recursive Conditioning [7] and
Value Elimination [2] within the AND/OR search frame-

2.1

BACKGROUND
GRAPHICAL MODELS

A graphical model is defined by a collection of functions,
over a set of variables, conveying probabilistic or deterministic information, whose structure is captured by a graph.
D EFINITION 2.1 (graphical models) A graphical model is
a 4-tuple M=hX, D, F, ⊗i, where: 1. X={X1 , . . . , Xn }
is a set of variables; 2. D={D1 , . . . , Dn } is a set of
finite domains of values; 3. F ={f1 , . . . , fr } is a set
of real-valued functions. The scope of function fi , denoted scope(f
⊆ X, is the set of arguments of fi 4.
Q i) P
⊗i fi ∈ { i fi , i fi , ./i fi } is a combination operator.
The graphical model represents the combination of all its
functions, namely the set: ⊗ri=1 fi . When the combination
operator is irrelevant we denote M by hX, D, F i.
D EFINITION 2.2 (primal graph) The primal graph of a
graphical model is an undirected graph that has variables
as its vertices and edges connecting any two variables that
appear in the scope of the same function.
Two central graphical models are belief networks and constraint networks. A belief network B = hX, D, P i is defined over a directed acyclic graph G = (X, E) and its

A

A:

P(A) h4(A)

B:

P(B|A) h3(AB) h2(AB)

E:

P(E|AB)

C:

P(C|A) h1(BC)

bucket-A

A
A

AB bucket-B
B

E

C

D

(a)

D:

AB

AB

ABC bucket-C

ABE
bucket-E

BC

P(D|BC)

(b)

2.3

AND/OR SEARCH SPACE

BCD bucket-D

(c)

Figure 1: Execution of Variable Elimination
functions Pi denotes conditional probability tables (CPTs),
Pi = {P (Xi | pai )}, where pai is the set of parent nodes
pointing to Xi in G. Common tasks are finding the posterior
probability of some variables given the evidence, or finding
the most probable assignment to all the variables given the
evidence. A constraint network R = hX, D, Ci has a
set of constraints C = {C1 , ..., Ct } as its functions. Each
constraint is a pair Ci = (Si , Ri ), where Si ⊆ X is the
scope of the relation Ri defined over Si , denoting the allowed combination of values. Common tasks are finding a
solution and counting the number of solutions.
We assume that the domains of functions include a zero element, “0”. Combining (e.g., multiplying) anything with
“0” yields a “0”. The “0” value expresses inconsistent tuples. This is a primary concept in constraint networks but
can also be defined relative to a graphical model as follows.
Each function fi expresses an implicit flat constraint which
is a relation Ri over its scope that includes all and only the
consistent tuples, namely those that are not mapped to ”0”.
In this paper, a constraint network refers also to the flat
constraints that can be extracted from any graphical model.
When all the assignments are consistent we say that the
graphical model is strictly positive. A partial assignment
is consistent if none of its functions evaluate to zero. A
solution is a consistent assignment to all the variables.
We assume the usual definitions of induced graphs, induced width, tree-width and path-width [9, 1].
2.2

d = (A, B, E, C, D). The buckets are processed from D to
A 1 . Figure 1c shows the bucket tree.

INFERENCE BY VARIABLE ELIMINATION

Variable elimination algorithms [5, 8] are characteristic
of inference methods. Consider a graphical model G =
hX, D, F i and an ordering d = (X1 , X2 , . . . , Xn ). The ordering d dictates an elimination order for VE, from last to
first. All functions in F that contain Xi and do not contain
any Xj , j > i, are placed in the bucket of Xi . Buckets are
processed from Xn to X1 by eliminating the bucket variable (combining all functions and removing the variable by
a marginalization) and placing the resulting function (also
called message) in the bucket of its latest variable in d. This
VE procedure also constructs a bucket tree, by linking each
bucket Xi to the one where the resulting function generated
in bucket Xi is placed, which is called the parent of Xi .
Example 2.1 Figure 1a shows a belief network. Figure1b
shows the execution of Variable Elimination along ordering

The usual way to do search consists of instantiating variables in turn (we only consider fixed variable ordering). In
the simplest case this defines a search tree, whose nodes
represent states in the space of partial assignments. A
depth-first search (DFS) algorithm searching this space
could run in linear space. If more memory is available,
then some of the traversed nodes can be cached, and retrieved when “similar” nodes are encountered. The traditional search space does not capture the structure of the underlying graphical model. Introducing AN D nodes into
the search space can capture the structure decomposing the
problem into independent subproblems by conditioning on
values [10, 12]. Since the size of the AND/OR tree may
be exponentially smaller than the traditional search tree,
any algorithm searching the AND/OR space enjoys a better
computational bound. For more details see [4, 12]. A classical algorithm that explores the AND/OR search space is
Recursive Conditioning [7]. Given a graphical model M,
its AND/OR search space is based on a pseudo tree [12]:
D EFINITION 2.3 (pseudo tree) Given an undirected
graph G = (V, E), a directed rooted tree T = (V, E 0 )
defined on all its nodes is called pseudo tree if any arc
of G which is not included in E 0 is a back-arc, namely it
connects a node to an ancestor in T .
2.3.1

AND/OR Search Tree

Given a graphical model M = hX, D, F i, its primal graph
G and a pseudo tree T of G, the associated AND/OR search
tree, denoted ST (M), has alternating levels of AND and
OR nodes. The OR nodes are labeled Xi and correspond to
the variables. The AND nodes are labeled hXi , xi i and correspond to the values in the domains of the variables. The
structure of the AND/OR search tree is based on the underlying backbone pseudo tree T . The root of the AND/OR
search tree is an OR node labeled with the root of T .
The children of an OR node Xi are AND nodes labeled
with assignments hXi , xi i that are consistent with the
assignments along the path from the root, path(xi ) =
(hX1 , x1 i, hX2 , x2 i, . . . , hXi−1 , xi−1 i). Consistency is
well defined for constraint networks. For probabilistic networks it is defined relative to the underlying flat constraint
network derived from the belief network. The children of
an AND node hXi , xi i are OR nodes labeled with the children of variable Xi in the pseudo tree T .
Arc labeling The arcs from Xi to hXi , xi i are labeled with
the appropriate combined values of the functions in F that
1
This is a non-standard graphical representation, reversing the
top down bucket processing described in earlier papers.

B

0

1

0

1

B

B

B

B

0
E

E

A

A

A

C
D

0 1

1
C

0

E
1

0 1

0
C

0

E
1

0 1

0

0

1
C

E
1

0 1

E

C
0

E

0
C

E

1
C

E

C

1

D

D

D

D

D

D

D

D

0 1

0 1

0 1

0 1

0 1

0 1

0 1

0 1

(a)

1
C

0

1

0

1

0

1

D

D

D

D

0

1

(b)

Figure 2: AND/OR search tree
contain Xi and have their scopes fully assigned. When the
pseudo tree is a chain, the AND/OR search tree coincides
with the regular OR search tree.
Example 2.2 Consider again the belief network in Figure
1a. Figure 2a shows a pseudo tree of its primal graph, together with the back-arcs (dotted lines). Figure 2b shows
the AND/OR search tree based on the pseudo tree, for binary {0, 1} valued variables assuming positive functions.
Arc labels are not included.

Figure 3: Context-minimal AND/OR search graph
Based on earlier work by [4, 11], it can be shown that:
T HEOREM 2.5 (size of minimal context graphs) Given a
graphical model M, a pseudo tree T and w the induced
width of G along the depth-first traversal of T ,
1) The size of CMT (M) is O(n · k w ), when k bounds the
domain size.
2) The context-minimal AND/OR search graph (relative
to all pseudo trees) is bounded exponentially by the treewidth, while the context-minimal OR search graph is
bounded exponentially by the path-width.

Based on earlier work by [12, 4, 6, 7], it can be shown that:
T HEOREM 2.3 Given a graphical model M and a pseudo
tree T , the size of the AND/OR search tree ST is O(n ·
exp(m)) where m is the depth of T . A graphical model
that has a tree-width w∗ has an AND/OR search tree whose
size is O(n · exp(w∗ · log n)).
D EFINITION 2.4 (backtrack-free) An AND/OR search
tree of a graphical model is backtrack-free iff all nodes that
do not root a consistent solution are pruned.
2.3.2

AND/OR Search Graph

The AND/OR search tree may contain nodes that root identical subtrees. These are called unifiable. When unifiable
nodes are merged, the search space becomes a graph. Its
size becomes smaller at the expense of using additional
memory when being traversed. When all unifiable nodes
are merged, a computational intensive task, we get the
unique minimal AND/OR graph. Some unifiable nodes can
be identified based on their contexts [7] or conflict sets [9].
The context of an AND node hXi , xi i is defined as the
set of ancestors of Xi in the pseudo tree, including Xi ,
that are connected (in the induced primal graph) to descendants of Xi . It is easy to verify that the context of Xi dseparates [14] the subproblem below Xi from the rest of
the network. The context-minimal AND/OR graph denoted
CMT (M), is obtained by merging all the context unifiable
AND nodes. When the graphical model is strictly positive,
it yields the full context-minimal graph. The backtrack-free
context-minimal graph, BF -CMT , is the context-minimal
graph where all inconsistent subtrees are pruned.
Example 2.4 Figure 3 shows the full context-minimal
graph of the problem and pseudo tree from Figure 2.

Value function A task over a graphical model (e.g., belief
updating, counting) induces a value function for each node
in the AND/OR space. The algorithmic task is to compute
the value of the root. This can be done recursively from
leaves to root by any traversal scheme. When an AO traversal of the search space uses full caching based on context it actually traverses the context-minimal, CMT , graph.
It is this context minimal graph that allows comparing the
execution of AO search against VE.

3

VE VS. AO SEARCH

VE’s execution is uniquely defined by a bucket-tree, and
since every bucket tree corresponds to a pseudo tree, and a
pseudo tree uniquely defines the context-minimal AND/OR
search graph, we can compare both schemes on this common search space. Furthermore, we choose the contextminimal AND/OR search graph (CM) because algorithms
that traverse the full CM need memory which is comparable to that used by VE, namely, space exponential in the
tree-width of their pseudo/bucket trees.
Algorithm AO denotes any traversal of the CM search
graph, AO-DF is a depth-first traversal and AO-BF is a
breadth-first traversal. We will compare VE and AO via the
portion of this graph that they generate and by the order of
node generation. The task’s value computation performed
during search adds only a constant factor.
We distinguish graphical models with or without determinism, namely, graphical models that have inconsistencies vs.
those that have none. We compare brute-force versions of
VE and AO, as well as versions enhanced by various known
features. We assume that the task requires the examination
of all solutions (e.g. belief updating, counting solutions).

D:

A

h4 (D)

D

bucket-D

D=0

D

C:

h3(CD)

CD

D=1

C=0

bucket-C

C=1

C=0

C=1

CD

B

C

B:

P(D|BC) h2(BC)

BCD

bucket-B

B0C0D0

BC

A:

P(A) P(B|A) P(C|A) h1(AB)

ABC

D

E:

P(E|AB)

ABE

(b)

(a)

B1C0D0

B1C0D1

B0C1D1

B0C1D0

B=1

B=0

B1C1D0

B1C1D1

B=1

bucket-A

AB

E

B0C0D1

B=0
A0B0C0

bucket-E

(c)

A0B0C1

A1B0C0

A=0
E0A0B0

E0A0B1

E=0

A1B0C1

E1A0B0

A0B1C1

A0B1C0

A=1

A=0

E1A0B1

E=1

E=0

A1B1C1

A=1

E0A1B1

E0A1B0

A1B1C0

E1A1B0

E1A1B1

E=1

Figure 4: Variable Elimination
Figure 5: Context-minimal AND/OR search space
3.1

VE VS. AO WITH NO DETERMINISM

We start with the simplest case in which the graphical
model contains no determinism and the bucket tree (pseudo
tree) is a chain.
3.1.1

OR Search Spaces

Figure 4a shows a Bayesian network. Let’s consider the
ordering d = (D, C, B, A, E) which has the tree-width
w(d) = w∗ = 2. Figure 4b shows the bucket-chain and a
schematic application of VE along this ordering (the bucket
of E is processed first, and the bucket of D last). The buckets include the initial CPTs and the functions that are generated and sent (as messages) during the processing. Figure
4c shows the bucket tree.
If we use the chain bucket tree as pseudo tree for the
AND/OR search along d, we get the full CM graph given
in Figure 5. Since this is an OR space, we can eliminate the
OR levels as shown. Each level of the graph corresponds to
one variable. The edges should be labeled with the product
of the values of the functions that have just been instantiated on the current path. We note on the arc just the assignment to the relevant variables (e.g., B1 denotes B = 1). For
example, the edges between C and B in the search graph
are labeled with the function valuation on (BCD), namely
P (D|B, C), where for each individual edge this function
is instantiated as dictated on the arcs.
AO-DF computes the value (e.g., updated belief) of the
root node by generating and traversing the context-minimal
graph in a depth-first manner and accumulating the partial value (e.g., probabilities) using combination (products) and marginalization (summation).
The first
two paths generated by AO-DF are (D0 , C0 , B0 , A0 , E0 )
and (D0 , C0 , B0 , A0 , E1 ), which allow the first accumulation of value h1 (A0 B0 ) = P (E0 |A0 B0 ) +
P (E1 |A0 B0 ).
AO-DF subsequently generates the
two paths (D0 , C0 , B0 , A1 , E0 ) and (D0 , C0 , B0 , A1 , E1 )
and accumulates the next partial value h1 (A1 B0 ) =
P (E0 |A1 B0 ) + P (E1 |A1 B0 ). Subsequently it computes the summation h2 (B0 C0 ) = P (A0 ) · P (B0 |A0 ) ·
P (C0 |A0 ) · h1 (A0 B0 ) + P (A1 ) · P (B0 |A1 ) · P (C0 |A1 ) ·
h1 (A1 B0 ). Notice that due to caching each arc is gener-

ated and traversed just once (in each direction). For example when the partial path (D1 , C0 , B0 ) is created, it is
recognized (via context) that the subtree below was already
explored and its compiled value will be reused.
In contrast, VE generates the full context-minimal graph by
layers, from the bottom of the search graph up, in a manner
that can be viewed as dynamic programming or as breadthfirst search on the explicated search graph. VE’s execution
can be viewed as first generating all the edges between E
and A (in some order), and then all the edges between A
and B (in some order), and so on up to the top. We can see
that there are 8 edges between E and A. They correspond
to the 8 tuples in the bucket of E (the function on (ABE)).
There are 8 edges between A and B, corresponding to the
8 tuples in the bucket of A. And there are 8 edges between
B and C, corresponding to the 8 tuples in the bucket of B.
Similarly, 4 edges between C and D correspond to the 4
tuples in the bucket of C, and 2 edges between D and the
rood correspond to the 2 tuples in the bucket of D.
Since the computation is performed from bottom to top,
the nodes of A store the result of eliminating E (namely
the function h1 (AB) resulting by summing out E). There
are 4 nodes labeled with A, corresponding to the 4 tuples
in the message sent by VE from bucket of E to bucket of
A (the message on (AB)). And so on, each level of nodes
corresponds to the number of tuples in the message sent on
the separator (the common variables) between two buckets.
3.1.2

AND/OR Graphs

The above correspondence between Variable Elimination
and AND/OR search is also maintained in non-chain
pseudo/bucket trees.We refer again to the example in Figures 1, 2 and 3 and assume belief updating. The bucket
tree in Figure 1c has the same structure as the pseudo tree
in Figure 2a. We will show that VE traverses the AND/OR
search graph in Figure 3 bottom up, while AO-DF traverses
the same graph in depth first manner, top down.
AO-DF first sums h3 (A0 , B0 ) = P (E0 |A0 , B0 ) +
P (E1 |A0 , B0 ) and then goes depth first to h1 (B0 , C0 ) =
P (D0 |B0 , C0 ) + P (D1 |B0 , C0 ) and h1 (B0 , C1 ) =
P (D0 |B0 , C1 ) + P (D1 |B0 , C1 ).
Then it computes

h2 (A0 , B0 ) = (P (C0 |A0 ) · h1 (B0 , C0 )) + (P (C1 |A0 ) ·
h1 (B0 , C1 )). All the computation of AO-DF is precisely
the same as the one performed in the buckets of VE.
Namely, h1 is computed in the bucket of D and placed in
the bucket of C. h2 is computed in the bucket of C and
placed in the bucket of B, h3 is computed in the bucket of
E and also placed in the bucket of B and so on, as shown in
Figure 1b. All this corresponds to traversing the AND/OR
graph from leaves to root. Thus, both algorithms traverse
the same graph, only the control strategy is different.
We can generalize both the OR and AND/OR examples,
T HEOREM 3.1 (VE and AO-DF are identical) Given a
graphical model having no determinism, and given the
same bucket/pseudo tree VE applied to the bucket-tree is
a (breadth-first) bottom-up search that will explore all
the full CM search graph, while AO-DF is a depth-first
top-down search that explores (and records) the full CM
graph as well.
Breadth-first on AND/OR. Since one important difference between AO search and VE is the order by which they
explore the search space (top-down vs. bottom-up) we wish
to remove this distinction and consider a VE-like algorithm
that goes top-down. One obvious choice is breadth-first
search, yielding AO-BF. That is, in Figure 3 we can process
the layer of variable A first, then B, then E and C, and then
D. General breadth-first or best-first search of AND/OR
graphs for computing the optimal cost solution subtrees are
well defined procedures. The process involves expanding
all solution subtrees in layers of depth. Whenever a new
node is generated and added to the search frontier the value
of all relevant partial solution subtrees are updated. A well
known Best-first version of AND/OR spaces is the AO*
algorithm [13]. Algorithm AO-BF can be viewed as a topdown inference algorithm. We can now extend the comparison to AO-BF.
Proposition 1 (VE and AO-BF are identical) Given a
graphical model with no determinism and a bucket/pseudo
tree, VE and AO-BF explore the same full CM graph, one
bottom-up (VE) and the other top-down; both perform
identical value computation.
Terminology for algorithms’ comparison. Let A and B
be two algorithms over graphical models, whose performance is determined by an underlying bucket/pseudo tree.
D EFINITION 3.1 (comparison of algorithms) We
say
that: 1. algorithms A and B are identical if for every
graphical model and when given the same bucket-tree
they traverse an identical search space. Namely, every
node is explored by A iff it is explored by B; 2. A is
weakly better than B if there exists a graphical model and
a bucket-tree, for which A explores a strict subset of the
nodes explored by B; 3. A is better than B if A is weakly

better than B but B is not weakly better than A; 4. The
relation of ”weakly-better” defines a partial order between
algorithms. A and B are not comparable if they are not
comparable w.r.t to the ”weakly-better” partial order.
Clearly, any two algorithms for graphical models are either
1. identical, 2. one is better than the other, or 3. they are
not comparable. We can now summarize our observations
so far using the new terminology.
T HEOREM 3.2 For a graphical model having no determinism AO-DF, AO-BF and VE are identical.
Note that our terminology captures the time complexity but
may not capture the space complexity, as we show next.
3.1.3

Space Complexity

To make the complete correspondence between VE and AO
search, we can look not only at the computational effort,
but also at the space required. Both VE and AO search
traverse the context minimal graph, but they may require
different amounts of memory to do so. So, we can distinguish between the portion of the graph that is traversed and
the portion that should be recorded and maintained. If the
whole graph is recorded, then the space is O(n · exp(w∗ )),
which we will call the base case.
VE can forget layers Sometimes, the task to be solved can
allow VE to use less space by deallocating the memory for
messages that are not necessary anymore. Forgetting previously traversed layers of the graph is a well known property
of dynamic programming. In such a case, the space complexity for VE becomes O(dBT · exp(w∗)), where dBT
is the depth of the bucket tree (assuming constant degree
in the bucket tree). In most cases, the above bound is not
tight. If the bucket tree is a chain, then dBT = n, but forgetting layers yields an O(n) improvement over the base
case. AO-DF cannot take advantage of this property of VE.
It is easy to construct examples where the bucket tree is a
chain, for which VE requires O(n) less space than AO-DF.
AO dead caches The straightforward way of caching is to
have a table for each variable, recording its context. However, some tables might never get cache hits. We call these
dead-caches. In the AND/OR search graph, dead-caches
appear at nodes that have only one incoming arc. AO search
needs to record only nodes that are likely to have additional
incoming arcs, and these nodes can be determined by inspection from the pseudo tree. Namely, if the context of
a node includes that of its parent, then AO need not store
anything for that node, because it would be a dead-cache.
In some cases, VE can also take advantage of dead caches.
If the dead caches appear along a chain in the pseudo
tree, then avoiding the storage of dead-caches in AO corresponds to collapsing the subsumed neighboring buckets
in the bucket tree. This results in having cache tables of

Figure 6: CM graphs with determinism: a) AO; b) VE
the size of the separators, rather than the cliques. The time
savings are within a constant factor from the complexity of
solving the largest clique, but the space complexity can be
reduced from exponential in the size of the maximal cique
to exponential in the maximal separator.
However, if the variables having dead caches form connected components that are subtrees (rather than chains)
in the pseudo tree, then the space savings of AO cannot be
achieved by VE. Consider the following example:
Example 3.3 Let variables {X1 , . . . , Xn } be divided in
}
three sets: A = {X1 , . . . , X n3 }, B = {X n3 +1 , . . . , X 2n
3
,
.
.
.
,
X
}.
There
are
two
cliques
on
and C = {X 2n
n
3 +1
A ∪ B and A ∪ C defined by all possibile binary functions over variables in those respective cliques. The input
is therefore O(n2 ). Consider the bucket tree (pseudo tree)
defined by the ordering d = (X1 , . . . , Xn ), where Xn is
eliminated first by VE. In this pseudo tree, all the caches
are dead, and as a result the AO search graph coincides
with the AO search tree. Therefore, AO can solve the problem using space O( 2n
3 ). VE can collapse some neighboring buckets (for variables in B and C), but needs to store at
least one message on the variables in A, which yields space
complexity O(exp( n3 )). In this example, AO and VE have
the same time complexity, but AO uses space linear in the
number of variables while VE needs space exponential in
the number of variables (and exponential in the input too).
The above observation is similar to the known properties
of depth-first vs. breadth-first search in general. When the
search space is close to a tree, the benefit from the inherent
memory use of breadth-first search is nil.
3.2

VE VS. AO WITH DETERMINISM

When the graphical model contains determinism the
AND/OR trees and graphs are dependant not only on the
primal graph but also on the (flat) constraints, namely on
the consistency and inconsistency of certain relationships
(no-good tuples) in each relation. In such cases AO and
VE, may explore different portions of the context-minimal
graphs because the order of variables plays a central role,
dictating where the determinism reveals itself.
Example 3.4 Let’s consider a problem on four variables:
A, B, C, D, each having the domain {1, 2, 3, 4}, and the

constraints A < B, B < C and C < D. The primal
graph of the problem is a chain. Let’s consider the natural
ordering from A to D, which gives rise to a chain pseudo
tree (and bucket-tree) rooted at A. Figure 6a shows the full
CM graph with determinism generated by AO search, and
Figure 6b the graph generated and traversed by VE in reverse order. The thick lines and the white nodes are the
ones traversed. The dotted lines and the black nodes are
not explored (when VE is executed from D, the constraint
between D and C implies that C = 4 is pruned, and therefore not further explored). Note that the intersection of the
graphs explored by both algorithms is the backtrack-free
AND/OR context graph, corresponding to the unique solution (A=1,B=2,C=3,D=4).
As we saw in the example, AO and VE explore different
parts of the inconsistent portion of the full CM. Therefore,
in the presence of determinism, AO-DF and AO-BF are
both un-comparable to VE, as they differ in the direction
they explore the CM space.
T HEOREM 3.5 Given a graphical model with determinism,
then AO-DF and AO-BF are identical and both are uncomparable to VE.
This observation is in contrast with claims of superiority
of one scheme or the other [3], at least for the case when
variable ordering is fixed and no advanced constraint propagation schemes are used and assuming no exploitation of
context independence.

4

ALGORITHMIC ADVANCES AND
THEIR EFFECT

So far we compared brute-force VE to brute-force AO
search. We will now consider the impact of some enhancements on this relationship. Clearly, both VE and AO
explore the portion of the context-minimal graph that is
backtrack-free. Thus they can differ only on the portion
that is included in full CM and not in the backtrack-free
CM. Indeed, constraint propagation, backjumping and nogood recording just reduce the exploration of that portion
of the graph that is inconsistent. Here we compare those
schemes against bare VE and against VE augmented with
similar enhancements whenever relevant.
4.1

VE VS. AO WITH LOOK-AHEAD

In the presence of determinism AO-DF and AO-BF can naturally accommodate look-ahead schemes which may avoid
parts of the context-minimal search graph using some level
of constraint propagation. It is easy to compare AO-BF
against AO-DF when both use the same look-ahead because the notion of constraint propagation as look-ahead
is well defined for search and because both algorithms explore the search space top down. Not surprisingly when

both algorithms have the same level of look-ahead propagation, they explore an identical search space.

4

We can also augment VE with look-ahead constraint propagation (e.g., unit resolution, arc consistency), yielding VELAH as follows. Once VE-LAH processes a single bucket,
it then applies constraint propagation as dictated by the
look-ahead propagation scheme (bottom-up), then continues with the next bucket applied over the modified set of
functions and so on. We can show that:
T HEOREM 4.1 Given a graphical model with determinism
and given a look-ahead propagation scheme, LAH,
1. AO-DF-LAH and AO-BF-LAH are identical.
2. VE and VE-LAH are each un-comparable with each of
AO-DF-LAH and AO-BF-LAH.
4.2

GRAPH-BASED NO-GOOD LEARNING

AO search can be augmented with no-good learning [9].
Graph-based no-good learning means recording that some
nodes are inconsistent based on their context. This is automatically accomplished when we explore the CM graph
which actually amounts to recording no-goods and goods
by their contexts. Therefore AO-DF is identical to AO-BF
and both already exploit no-goods, we get that (AO-NG denotes AO with graph-based no-good learning):
T HEOREM 4.2 For every graphical model the relationship
between AO-NG and VE is the same as the relationship between AO (Depth-first or breadth-first) and VE.
Combined no-goods and look-ahead. No-goods that are
generated during search can also participate in the constraint propagation of the look-ahead and strengthen the
ability to prune the search-space further. The graphical
model itself is modified during search and this affects the
rest of the look-ahead. It is interesting to note that AO-BF
is not able to simulate the same pruned search space as AODF in this case because of its breadth-first manner. While
AO-DF can discover deep no-goods due to its depth-first
nature, AO-BF has no access to such deep no-goods and
cannot use them within a constraint propagation scheme
in shallower levels. However, even when AO exploits nogoods within its look-ahead propagation scheme, VE and
AO remain un-comparable. Any example that does not allow effective no-good learning can illustrate this.
Example 4.3 Consider a constraint problem over n
variables. Variables X1 , . . . , Xn−1 have the domain
{1, 2, . . . , n − 2, ∗}, made of n-2 integer values and a special ∗ value. Between any pair of the n − 1 variables there
is a not-equal constraint between the integers and equality
between stars. There is an additional variable Xn which
has a constraint with each variable, whose values are consistent only with the ∗ of the other n-1 variables. Clearly if
the ordering is d = (X1 , . . . , Xn−1 , Xn ), AO may search

3

8

8

8

1

1

1

3

3

2

6

7

5

(a)

4

5

2

4

5

2

7

7

8

1
5

3
4

2

7

(b)

6

6

6

(c)

(d)

Figure 7: GBJ vs. AND/OR search
all the exponential search space over the first n − 1 variables (the inconsistent portion) before it reaches the ∗ of
the n − th variable. On the other hand, if we apply VE
starting from the n − th variable, we will reveal the only
solution immediately. No constraint propagation, nor nogood learning can help any AO search in this case.
T HEOREM 4.4 Given a graphical model with determinism
and a particular look-ahead propagation scheme LAH:
1. AO-DF-LAH-NG is better than AO-BF-LAH-NG.
2. VE and AO-DF-LAH-NG are not comparable.
4.3

GRAPH-BASED BACKJUMPING

Backjumping algorithms [9] are backtracking search applied to the OR space, which uses the problem structure
to jump back from a dead-end as far back as possible. In
graph-based backjumping (GBJ) each variable maintains a
graph-based induced ancestor set which ensures that no solutions are missed by jumping back to its deepest variable.
DFS orderings. If the ordering of the OR space is a DFS
ordering of the primal graph, it is known [9] that all the
backjumps are from a variable to its DFS parent. This
means that naive AO-DF automatically incorporates GBJ
jumping-back character.
Pseudo tree orderings. In the case of pseudo tree orderings that are not DFS-trees, there is a slight difference between OR-GBJ and AO-DF and GBJ may sometime perform deeper backjumps than those implicitly done by AO.
Figure 7a shows a probabilistic model, 7b a pseudo tree
and 7c a chain driving the OR search (top down). If a
deadend is encountered at variable 3, GBJ retreats to 8 (see
7c), while naive AO-DF retreats to 1, the pseudo tree parent. When the deadend is encountered at 2, both algorithms
backtrack to 3 and then to 1. Therefore, in such cases, augmenting AO with GBJ can provide additional pruning on
top of the AND/OR structure. In other words, GBJ on OR
space along a pseudo tree is never stronger than GBJ on
AND/OR and it is sometimes weaker.
GBJ can be applied using an arbitrary order d for the OR
space. The ordering d can be used to generate a pseudo
tree. In this case, however, to mimic GBJ on d, the AO

A

A

A
B

AB

ABE

B

C

AC

BCD

D

BC

A
D
DF

A

AB

AC

ABE

BCD

DF

C
A

D

B

D

B

F

E
E
F

(a)

(b)

C

C

D

A

F

E

(c)

Figure 8: RC d-trees and AND/OR pseudo trees
traversal will be controlled by d. In Figure 7d we show an
arbitrary order d = (8, 1, 3, 5, 4, 2, 7, 6) which generates
the pseudo tree in 7b. When AO search reaches 3, it goes in
a breadth first manner to 5, according to d. It can be shown
that GBJ in order d on OR space corresponds to the GBJbased AND/OR search based on the associated pseudo tree.
All the backjumps have a one to one correspondence.
Since VE is not comparable with AO-DF, it is also uncomparable with AO-DF-GBJ. Note that backjumping is
not relevant to AO-BF or VE. In summary,
T HEOREM 4.5 1. When the pseudo tree is a DFS tree
AO-DF is identical to AO-DF-GBJ. This is also true when
the AND/OR search tree is explored (rather than the CMgraph). 2. AO-DF-GBJ is superior to AO-DF for general
pseudo trees. 3. VE is not comparable to AO-DF-GBJ.
4.4

RECURSIVE CONDITIONING AND VALUE
ELIMINATION

Recursive Conditioning (RC) [7] defined for belief networks is based on the divide and conquer paradigm. RC
instantiates variables with the purpose of breaking the network into independent subproblems, on which it can recurs
using the same technique. The computation is driven by a
data-structure called dtree, which is a full binary tree, the
leaves of which correspond to the network CPTs.
It can be shown that RC explores an AND/OR space. Consider the example in Figure 8, which shows: (a) a belief network; (b) and (c), two dtrees and the corresponding pseudo
trees for the AND/OR search. It can also be shown that the
context of the nodes in RC is identical to that in AND/OR
and therefore equivalent caching schemes can be used.

We show that there is no principled difference between
memory-intensive search with fixed variable ordering and
inference beyond: 1. different direction of exploring a
common search space (top down for search vs. bottomup for inference); 2. different assumption of control strategy (depth-first for search and breadth-first for inference).
We also show that those differences occur only in the presence of determinism. We show the relationship between
algorithms such as graph-based backjumping and no-good
learning [9], as well as Recursive Conditioning [7] and
Value Elimination [2] within the AND/OR search space.
AND/OR search spaces can also accommodate dynamic
variable and value ordering which can affect algorithmic
efficiency significantly. Variable Elimination and general
inference methods however require static variable ordering.
This issue will be addressed in future work.
Acknowledgments
This work was supported in part by the NSF grant IIS0412854 and the MURI ONR award N00014-00-1-0617.


Inspired by the recently introduced framework of AND/OR search spaces for graphical models, we propose to augment Multi-Valued Decision Diagrams (MDD) with AND nodes, in order
to capture function decomposition structure and to extend these compiled data structures to general weighted graphical models (e.g., probabilistic models). We present the AND/OR Multi-Valued
Decision Diagram (AOMDD) which compiles a graphical model into a canonical form that supports polynomial (e.g., solution counting, belief updating) or constant time (e.g. equivalence of
graphical models) queries. We provide two algorithms for compiling the AOMDD of a graphical
model. The first is search-based, and works by applying reduction rules to the trace of the memory
intensive AND/OR search algorithm. The second is inference-based and uses a Bucket Elimination
schedule to combine the AOMDDs of the input functions via the the APPLY operator. For both
algorithms, the compilation time and the size of the AOMDD are, in the worst case, exponential in
the treewidth of the graphical model, rather than pathwidth as is known for ordered binary decision
diagrams (OBDDs). We introduce the concept of semantic treewidth, which helps explain why
the size of a decision diagram is often much smaller than the worst case bound. We provide an
experimental evaluation that demonstrates the potential of AOMDDs.

1. Introduction
The paper extends decision diagrams into AND/OR multi-valued decision diagrams (AOMDDs)
and shows how graphical models can be compiled into these data-structures. The work presented in
this paper is based on two existing frameworks: (1) AND/OR search spaces for graphical models
and (2) decision diagrams.
1.1 AND/OR Search Spaces
AND/OR search spaces (Dechter & Mateescu, 2004a, 2004b, 2007) have proven to be a unifying
framework for various classes of search algorithms for graphical models. The main characteristic is
the exploitation of independencies between variables during search, which can provide exponential
speedups over traditional search methods that can be viewed as traversing an OR structure. The
c 2008 AI Access Foundation. All rights reserved.

M ATEESCU , D ECHTER & M ARINESCU

AND nodes capture problem decomposition into independent subproblems, and the OR nodes represent branching according to variable values. AND/OR spaces can accommodate dynamic variable
ordering, however most of the current work focuses on static decomposition. Examples of AND/OR
search trees and graphs will appear later, for example in Figures 6 and 7.
The AND/OR search space idea was originally developed for heuristic search (Nilsson, 1980).
In the context of graphical models, AND/OR search (Dechter & Mateescu, 2007) was also inspired
by search advances introduced sporadically in the past three decades for constraint satisfaction and
more recently for probabilistic inference and for optimization tasks. Specifically, it resembles the
pseudo tree rearrangement (Freuder & Quinn, 1985, 1987), that was adapted subsequently for distributed constraint satisfaction by Collin, Dechter, and Katz (1991, 1999) and more recently by
Modi, Shen, Tambe, and Yokoo (2005), and was also shown to be related to graph-based backjumping (Dechter, 1992). This work was extended by Bayardo and Miranker (1996) and Bayardo and
Schrag (1997) and more recently applied to optimization tasks by Larrosa, Meseguer, and Sanchez
(2002). Another version that can be viewed as exploring the AND/OR graphs was presented recently for constraint satisfaction (Terrioux & Jégou, 2003b) and for optimization (Terrioux & Jégou,
2003a). Similar principles were introduced recently for probabilistic inference, in algorithm Recursive Conditioning (Darwiche, 2001) as well as in Value Elimination (Bacchus, Dalmao, & Pitassi,
2003b, 2003a), and are currently at the core of the most advanced SAT solvers (Sang, Bacchus,
Beame, Kautz, & Pitassi, 2004).
1.2 Decision Diagrams
Decision diagrams are widely used in many areas of research, especially in software and hardware
verification (Clarke, Grumberg, & Peled, 1999; McMillan, 1993). A BDD represents a Boolean
function by a directed acyclic graph with two terminal nodes (labeled 0 and 1), and every internal
node is labeled with a variable and has exactly two children: low for 0 and high for 1. If isomorphic
nodes were not merged, we would have the full search tree, also called Shannon tree, which is the
usual full tree explored by a backtracking algorithm. The tree is ordered if variables are encountered
in the same order along every branch. It can then be compressed by merging isomorphic nodes
(i.e., with the same label and identical children), and by eliminating redundant nodes (i.e., whose
low and high children are identical). The result is the celebrated reduced ordered binary decision
diagram, or OBDD for short, introduced by Bryant (1986). However, the underlying structure is
OR, because the initial Shannon tree is an OR tree. If AND/OR search trees are reduced by node
merging and redundant nodes elimination we get a compact search graph that can be viewed as a
BDD representation augmented with AND nodes.
1.3 Knowledge Compilation for Graphical Models
In this paper we combine the two ideas, creating a decision diagram that has an AND/OR structure, thus exploiting problem decomposition. As a detail, the number of values is also increased
from two to any constant. In the context of constraint networks, decision diagrams can be used to
represent the whole set of solutions, facilitating solutions count, solution enumeration and queries
on equivalence of constraint networks. The benefit of moving from OR structure to AND/OR is in
a lower complexity of the algorithms and size of the compiled structure. It typically moves from
being bounded exponentially in pathwidth pw∗ , which is characteristic to chain decompositions or
linear structures, to being exponentially bounded in treewidth w∗ , which is characteristic of tree
466

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

structures (Bodlaender & Gilbert, 1991) (it always holds that w∗ ≤ pw∗ and pw∗ ≤ w∗ · log n,
where n is the number of variables of the model). In both cases, the compactness result achieved in
practice is often far smaller than what the bounds suggest.
A decision diagram offers a compilation of a propositional knowledge-base. An extension of
the OBDDs was provided by Algebraic Decision Diagrams (ADD) (Bahar, Frohm, Gaona, Hachtel,
Macii, Pardo, & Somenzi, 1993), where the terminal nodes are not just 0 or 1, but take values from
an arbitrary finite domain. The knowledge compilation approach has become an important research
direction in automated reasoning in the past decade (Selman & Kautz, 1996; Darwiche & Marquis,
2002; Cadoli & Donini, 1997). Typically, a knowledge representation language is compiled into a
compact data structure that allows fast responses to various queries. Accordingly, the computational
effort can be divided between an offline and an online phase where most of the work is pushed
offline. Compilation can also be used to generate compact building blocks to be used by online
algorithms multiple times. Macro-operators compiled during or prior to search can be viewed in
this light (Korf & Felner, 2002), while in graphical models the building blocks are the functions
whose compact compiled representations can be used effectively across many tasks.
As one example, consider product configuration tasks and imagine a user that chooses sequential options to configure a product. In a naive system, the user would be allowed to choose any valid
option at the current level based only on the initial constraints, until either the product is configured,
or else, when a dead-end is encountered, the system would backtrack to some previous state and
continue from there. This would in fact be a search through the space of possible partial configurations. Needless to say, it would be very unpractical, and would offer the user no guarantee of
finishing in a limited time. A system based on compilation would actually build the backtrack-free
search space in the offline phase, and represent it in a compact manner. In the online phase, only
valid partial configurations (i.e., that can be extended to a full valid configuration) are allowed, and
depending on the query type, response time guarantees can be offered in terms of the size of the
compiled structure.
Numerous other examples, such as diagnosis and planning problems, can be formulated as
graphical models and could benefit from compilation (Palacios, Bonet, Darwiche, & Geffner, 2005;
Huang & Darwiche, 2005a). In diagnosis, compilation can facilitate fast detection of possible faults
or explanations for some unusual behavior. Planning problems can also be formulated as graphical
models, and a compilation would allow swift adjustments according to changes in the environment.
Probabilistic models are one of the most used types of graphical models, and the basic query is to
compute conditional probabilities of some variables given the evidence. A compact compilation of a
probabilistic model would allow fast response to queries that incorporate evidence acquired in time.
For example, two of the most important tasks for Bayesian networks are computing the probability
of the evidence, and computing the maximum probable explanation (MPE). If some of the model
variables become assigned (evidence), these tasks can be performed in time linear in the compilation size, which in practice is in many cases smaller than the upper-bound based on the treewidth or
pathwidth of the graph. Formal verification is another example where compilation is heavily used
to compare equivalence of circuit design, or to check the behavior of a circuit. Binary Decision
Diagram (BDD) (Bryant, 1986) is arguably the most widely known and used compiled structure.
The contributions made in this paper to knowledge compilation in general and to decision diagrams in particular are the following:
1. We formally describe the AND/OR Multi-Valued Decision Diagram (AOMDD) and prove it
to be a canonical representation for constraint networks, given a pseudo tree.
467

M ATEESCU , D ECHTER & M ARINESCU

2. We extend the AOMDD to general weighted graphical models.
3. We give a compilation algorithm based on AND/OR search, that saves the trace of a memory
intensive search and then reduces it in one bottom up pass.
4. We present the APPLY operator that combines two AOMDDs and show that its complexity is
at most quadratic in the input, but never worse than exponential in the treewidth.
5. We give a scheduling order for building the AOMDD of a graphical model starting with
the AOMDDs of its functions which is based on a Variable Elimination algorithm. This
guarantees that the complexity is at most exponential in the induced width (treewidth) along
the ordering.
6. We show how AOMDDs relate to various earlier and recent compilation frameworks, providing a unifying perspective for all these methods.
7. We introduce the semantic treewidth, which helps explain why compiled decision diagrams
are often much smaller than the worst case bound.
8. We provide an experimental evaluation of the new data structure.
The structure of the paper is as follows. Section 2 provides preliminary definitions, a description
of binary decision diagrams and the Bucket Elimination algorithm. Section 3 gives an overview of
AND/OR search spaces. Section 4 introduces the AOMDD and discusses its properties. Section
5 describes a search-based algorithm for compiling the AOMDD. Section 6 presents a compilation
algorithm based on a Bucket Elimination schedule and the APPLY operation. Section 7 proves that
the AOMDD is a canonical representation for constraint networks given a pseudo tree, and Section
8 extends the AOMDD to weighted graphical models and proves their canonicity. Section 9 ties
the canonicity to the new concept of semantic treewidth. Section 10 provides an experimental
evaluation. Section 11 presents related work and Section 12 concludes the paper. All the proofs
appear in an appendix.

2. Preliminaries
Notations A reasoning problem is defined in terms of a set of variables taking values from finite
domains and a set of functions defined over these variables. We denote variables or subsets of
variables by uppercase letters (e.g., X, Y, . . .) and values of variables by lower case letters (e.g.,
x, y, . . .). Sets are usually denoted by bold letters, for example X = {X1 , . . . , Xn } is a set of
variables. An assignment (X1 = x1 , . . . , Xn = xn ) can be abbreviated as x = (hX1 , x1 i, . . . ,
hXn , xn i) or x = (x1 , . . . , xn ). For a subset of variables Y, DY denotes the Cartesian product of
the domains of variables in Y. The projection of an assignment x = (x1 , . . . , xn ) over a subset Y
is denoted by xY or x[Y]. We will also denote by Y = y (or y for short) the assignment of values
to variables in Y from their respective domains. We denote functions by letters f , g, h etc., and the
scope (set of arguments) of the function f by scope(f ).
2.1 Graphical Models
D EFINITION 1 (graphical model) A graphical model M is a 4-tuple, M = hX, D, F, ⊗i, where:
468

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

1. X = {X1 , . . . , Xn } is a finite set of variables;
2. D = {D1 , . . . , Dn } is the set of their respective finite domains of values;
3. F = {f1 , . . . , fr } is a set of positive real-valued discrete functions (i.e., their domains can
be listed), each defined over a subset of variables Si ⊆ X, called its scope, and denoted by
scope(fi ).
Q P
4. ⊗ is a combination operator1 (e.g., ⊗ ∈ { , , 1} – product, sum, join), that can take as
input two (or more) real-valued discrete functions, and produce another real-valued discrete
function.
The graphical model represents the combination of all its functions: ⊗ri=1 fi .
Several examples of graphical models appear later, for example: Figure 1 shows a constraint
network and Figure 2 shows a belief network.
In order to define the equivalence of graphical models, it is useful to introduce the notion of
universal graphical model that is defined by a single function.
D EFINITION 2 (universal equivalent graphical model) Given a graphical model M
=
hX, D, F1 , ⊗i the universal equivalent model of M is u(M) = hX, D, F2 = {⊗fi ∈F1 fi }, ⊗i.
Two graphical models are equivalent if they represent the same function. Namely, if they have
the same universal model.
D EFINITION 3 (weight of a full and a partial assignment) Given a graphical model M =
hX, D, Fi, the weight of a full assignment x = (x1 , . . . , xn ) is defined by w(x) =
⊗f ∈F f (x[scope(f )]). Given a subset of variables Y ⊆ X, the weight of a partial assignment
y is the combination of all the functions whose scopes are included in Y (denoted by FY ) evaluated
at the assigned values. Namely, w(y) = ⊗f ∈FY f (y[scope(f )]).
Consistency For most graphical models, the range of the functions has a special zero value “0”
that is absorbing relative to the combination operator (e.g., multiplication). Combining anything
with “0” yields a “0”. The “0” value expresses the notion of inconsistent assignments. It is a primary
concept in constraint networks but can also be defined relative to other graphical models that have a
“0” element.
D EFINITION 4 (consistent partial assignment, solution) Given a graphical model having a “0”
element, a partial assignment is consistent if its cost is non-zero. A solution is a consistent assignment to all the variables.
D EFINITION 5 (primal graph) The primal graph of a graphical model is an undirected graph that
has variables as its vertices and an edge connects any two variables that appear in the scope of the
same function.
The primal graph captures the structure of the knowledge expressed by the graphical model. In
particular, graph separation indicates independency of sets of variables given some assignments to
other variables. All of the advanced algorithms for graphical models exploit the graphical structure,
by using a heuristically good elimination order, a tree decomposition or some similar method. We
will use the concept of pseudo tree, which resembles the tree rearrangements introduced by Freuder
and Quinn (1985):
1. The combination operator can also be defined axiomatically (Shenoy, 1992).

469

M ATEESCU , D ECHTER & M ARINESCU

E

A

A

D

E

B

D
F

B

G

F

C

G
C

(a) Graph coloring problem

(b) Constraint graph

Figure 1: Constraint network
D EFINITION 6 (pseudo tree) A pseudo tree of a graph G = (X, E) is a rooted tree T having the
same set of nodes X, such that every arc in E is a backarc in T (A path in a rooted tree starts at the
root and ends at one leaf. Two nodes can be connected by a backarc only if there exists a path that
contains both).
We use the common concepts and parameters from graph theory, that characterize the connectivity of the graph, and how close it is to a tree or to a chain. The induced width of a graphical model
governs the complexity of solving it by Bucket Elimination (Dechter, 1999), and was also shown to
bound the AND/OR search graph when memory is used to cache solved subproblems (Dechter &
Mateescu, 2007).
D EFINITION 7 (induced graph, induced width, treewidth, pathwidth) An ordered graph is a
pair (G, d), where G = ({X1 , . . . , Xn }, E) is an undirected graph, and d = (X1 , . . . , Xn ) is an
ordering of the nodes. The width of a node in an ordered graph is the number of neighbors that
precede it in the ordering. The width of an ordering d, denoted w(d), is the maximum width over
all nodes. The induced width of an ordered graph, w∗ (d), is the width of the induced ordered graph
obtained as follows: for each node, from last to first in d, its preceding neighbors are connected
in a clique. The induced width of a graph, w∗ , is the minimal induced width over all orderings.
The induced width is also equal to the treewidth of a graph. The pathwidth pw∗ of a graph is the
treewidth over the restricted class of orderings that correspond to chain decompositions.
Various reasoning tasks, or queries can be defined over graphical models. Those can be defined formally using marginalization operators such as projection, summation and minimization.
However, since our goal is to present a compilation of a graphical model which is independent of
the queries that can be posed on it, we will discuss tasks in an informal manner only. For more
information see the work of Kask, Dechter, Larrosa, and Dechter (2005).
Throughout the paper, we will use two examples of graphical models: constraint networks
and belief networks. In the case of constraint networks, the functions can be understood as relations. In other words, the functions (also called constraints) can take only two values, {0, 1}, or
{f alse, true}. A 0 value indicates that the corresponding assignment to the variables is inconsistent (not allowed), and a 1 value indicates consistency. Belief networks are an example of the more
general case of graphical models (also called weighted graphical models). The functions in this case
are conditional probability tables, so the values of a function are real numbers in the interval [0, 1].

470

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

Example 1 Figure 1(a) shows a graph coloring problem that can be modeled by a constraint network. Given a map of regions, the problem is to color each region by one of the given colors {red,
green, blue}, such that neighboring regions have different colors. The variables of the problems
are the regions, and each one has the domain {red, green, blue}. The constraints are the relation
“different” between neighboring regions. Figure 1(b) shows the constraint graph, and a solution
(A=red, B=blue, C=green, D=green, E=blue, F=blue, G=red) is given in Figure 1(a). A more
detailed example will be given later in Example 8.
Propositional Satisfiability A special case of a CSP is propositional satisfiability (SAT). A formula ϕ in conjunctive normal form (CNF) is a conjunction of clauses α1 , . . . , αt , where a clause
is a disjunction of literals (propositions or their negations). For example, α = (P ∨ ¬Q ∨ ¬R) is
a clause, where P , Q and R are propositions, and P , ¬Q and ¬R are literals. The SAT problem
is to decide whether a given CNF theory has a model, i.e., a truth-assignment to its propositions
that does not violate any clause. Propositional satisfiability (SAT) can be defined as a CSP, where
propositions correspond to variables, domains are {0, 1}, and constraints are represented by clauses,
for example the clause (¬A ∨ B) is a relation over its propositional variables that allows all tuples
over (A, B) except (A = 1, B = 0).
Cost Networks An immediate extension of constraint networks are cost networks where the set
of functions are real-valued cost functions, and the primary task is optimization. Also, GAI-nets
(generalized additive independence, Fishburn, 1970) can be used to represent utility functions. An
example of cost functions will appear in Figure 19.
D EFINITION
P8 (cost network, combinatorial optimization) A cost network is a 4-tuple,
hX, D, C, i, where X is a set of variables X = {X1 , . . . , Xn }, associated with a set of
discrete-valued domains, D = {D1 , . . . , Dn }, and a set of cost functions C = {C1 , . . . , Cr }. Each
Ci is a real-valued function defined on a subset of variables Si ⊆ X. The combination operator, is
P
. The reasoning problem is to find a minimum cost solution.

Belief Networks (Pearl, 1988) provide a formalism for reasoning about partial beliefs under conditions of uncertainty. They are defined by a directed acyclic graph over vertices representing random
variables of interest (e.g., the temperature of a device, the gender of a patient, a feature of an object, the occurrence of an event). The arcs signify the existence of direct causal influences between
linked variables quantified by conditional probabilities that are attached to each cluster of parentschild vertices in the network.
Q
D EFINITION 9 (belief networks) A belief network (BN) is a graphical model P = hX, D, PG , i,
where X = {X1 , . . . , Xn } is a set of variables over domains D = {D1 , . . . , Dn }. Given a directed acyclic graph G over X as nodes, PG = {P1 , . . . , Pn }, where Pi = {P (Xi | pa (Xi ) ) }
are conditional probability tables (CPTs for short) associated with each Xi , where pa(Xi ) are the
parents of Xi in the
Qacyclic graph G. A belief network represents a probability distribution over X,
P (x1 , . . . , xn ) = ni=1 P (xi |xpa(Xi ) ). An evidence set e is an instantiated subset of variables.
When formulated as a graphical model, functions in F denote conditional probability tables
and the scopes of these functions are determined by the directed acyclic graph G: each function
Q
fi ranges over variable Xi and its parents in G. The combination operator is product, ⊗ = .
The primal graph of a belief network (viewed as an undirected model) is called a moral graph. It
connects any two variables appearing in the same CPT.
471

M ATEESCU , D ECHTER & M ARINESCU

A Season

Sprinkler B

Watering D

A

C Rain

B

F Wetness

D

G Slippery

C

F

G

(a) Directed acyclic graph

(b) Moral graph

Figure 2: Belief network
Example 2 Figure 2(a) gives an example of a belief network over 6 variables, and Figure 2(b)
shows its moral graph . The example expresses the causal relationship between variables “Season”
(A), “The configuration of an automatic sprinkler system” (B), “The amount of rain expected”
(C), “The amount of manual watering necessary” (D), “The wetness of the pavement” (F ) and
“Whether or not the pavement is slippery” (G). The belief network expresses the probability distribution P (A, B, C, D, F, G) = P (A) · P (B|A) · P (C|A) · P (D|B, A) · P (F |C, B) · P (G|F ).
Another example of a belief network and CPTs appears in Figure 9.
The two most popular tasks for belief networks are defined below:
D EFINITION 10 (belief updating, most probable explanation (MPE)) Given a belief network
and evidence e, the belief updating task is to compute the posterior marginal probability of variable
Xi , conditioned on the evidence. Namely,
X

Bel(Xi = xi ) = P (Xi = xi | e) = α

n
Y

P (xk , e|xpak ),

{(x1 ,...,xi−1 ,xi+1 ,...,xn )|E=e,Xi =xi } k=1

where α is a normalization constant. The most probable explanation (MPE) task is to find a
complete assignment which agrees with the evidence, and which has the highest probability among
all such assignments. Namely, to find an assignment (xo1 , . . . , xon ) such that
P (xo1 , . . . , xon ) = maxx1 ,...,xn

n
Y

P (xk , e|xpak ).

k=1

2.2 Binary Decision Diagrams Review
Decision diagrams are widely used in many areas of research to represent decision processes. In
particular, they can be used to represent functions. Due to the fundamental importance of Boolean
functions, a lot of effort has been dedicated to the study of Binary Decision Diagrams (BDDs),
which are extensively used in software and hardware verification (Clarke et al., 1999; McMillan,
1993). The earliest work on BDDs is due to Lee (1959), who introduced the binary-decision program, that can be understood as a linear representation of a BDD (e.g., a depth first search ordering
of the nodes), where each node is a branching instruction indicating the address of the next instruction for both the 0 and the 1 value of the test variable. Akers (1978) presented the actual graphical
472

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

A
0
0
0
0
1
1
1
1

B
0
0
1
1
0
0
1
1

C
0
1
0
1
0
1
0
1

f(ABC)
0
0
0
1
0
1
0
1

A

A

B

C

C

0

C

0

(a) Table

0

B

B

1

0

B

0

1

B

C

1

0

(b) Unordered tree

C

0

0

C

1

0

C

1

0

1

(c) Ordered tree

Figure 3: Boolean function representations
representation and further developed the BDD idea. However, it was Bryant (1986) that introduced
what is now called the Ordered Binary Decision Diagram (OBDD). He restricted the order of variables along any path of the diagram, and presented algorithms (most importantly the apply procedure, that combines two OBDDs by an operation) that have time complexity at most quadratic in the
sizes of the input diagrams. OBDDs are fundamental for applications with large binary functions,
especially because in many practical cases they provide very compact representations.
A BDD is a representation of a Boolean function. Given B = {0, 1}, a Boolean function
f : Bn → B, has n arguments, X1 , · · · , Xn , which are Boolean variables, and takes Boolean
values.
Example 3 Figure 3(a) shows a table representation of a Boolean function of three variables. This
explicit representation is the most straightforward, but also the most costly due to its exponential
requirements. The same function can also be represented by a binary tree, shown in Figure 3(b),
that has the same exponential size in the number of variables. The internal round nodes represent
the variables, the solid edges are the 1 (or high) value, and the dotted edges are the 0 (or low) value.
The leaf square nodes show the value of the function for each assignment along a path. The tree
shown in 3(b) is unordered, because variables do not appear in the same order along each path.
In building an OBDD, the first condition is to have variables appear in the same order (A,B,C)
along every path from root to leaves. Figure 3(c) shows an ordered binary tree for our function.
Once an order is imposed, there are two reduction rules that transform a decision diagram into an
equivalent one:
(1) isomorphism: merge nodes that have the same label and the same children.
(2) redundancy: eliminate nodes whose low and high edges point to the same node, and connect
parent of removed node directly to child of removed node.
Applying the two reduction rules exhaustively yields a reduced OBDD, sometimes denoted
rOBDD. We will just use OBDD and assume that it is completely reduced.
Example 4 Figure 4(a) shows the binary tree from Figure 3(c) after the isomorphic terminal nodes
(leaves) have been merged. The highlighted nodes, labeled with C, are also isomorphic, and Figure
4(b) shows the result after they are merged. Now, the highlighted nodes labeled with C and B are
redundant, and removing them gives the OBDD in Figure 4(c).
2.3 Bucket Elimination Review
Bucket Elimination (BE) (Dechter, 1999) is a well known variable elimination algorithm for inference in graphical models. We will describe it using the terminology for constraint networks, but BE
473

M ATEESCU , D ECHTER & M ARINESCU

A

A

B

B

C

C

C

0

1

A

B

C

B

C

C

0

(a) Isomorphic nodes

B

C

1

0

(b) Redundant nodes

1

(c) OBDD

Figure 4: Reduction rules
A:

A
C1(AC)
C2(AB)
C3(ABE)

B

C

C4(BCD)

h4(A)

B:

C2(AB)

E:

C3(ABE)

A

h3(AB)

h2(AB)

AB bucket-B
AB

ABE

C:
E

D

(a) Constraint network

D:

C1(AC)

h1(BC)

bucket-A

A

bucket-E

AB

ABC bucket-C
BC

BCD bucket-D

C4 (BCD)

(b) BE execution

(c) Bucket tree

Figure 5: Bucket Elimination
can also be applied to any graphical model. Consider a constraint network R = hX, D, Ci and an
ordering d = (X1 , X2 , . . . , Xn ). The ordering d dictates an elimination order for BE, from last to
first. Each variable is associated with a bucket. Each constraint from C is placed in the bucket of its
latest variable in d. Buckets are processed from Xn to X1 by eliminating the bucket variable (the
constraints residing in the bucket are joined together, and the bucket variable is projected out) and
placing the resulting constraint (also called message) in the bucket of its latest variable in d. After
its execution, BE renders the network backtrack free, and a solution can be produced by assigning
variables along d. BE can also produce the solutions count if marginalization is done by summation
(rather than projection) over the functional representation of the constraints, and join is substituted
by multiplication.
BE also constructs a bucket tree, by linking the bucket of each Xi to the destination bucket of
its message (called the parent bucket). A node in the bucket tree typically has a bucket variable, a
collection of constraints, and a scope (the union of the scopes of its constraints). If the nodes of the
bucket tree are replaced by their respective bucket variables, it is easy to see that we obtain a pseudo
tree.
Example 5 Figure 5(a) shows a network with four constraints. Figure5(b) shows the execution of
Bucket Elimination along d = (A, B, E, C, D). The buckets are processed from D to A.2 Figure
5(c) shows the bucket tree. The pseudo tree corresponding to the order d is given in Fig. 6(a).
2. The representation in Figure 5 reverses the top down bucket processing described in earlier papers (Dechter, 1999).

474

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

Procedure GeneratePseudoTree(G, d)
1
2
3
4
5

input : graph G = (X, E); order d = (X1 , . . . , Xn )
output : Pseudo tree T
Make X1 the root of T
Condition on X1 (eliminate X1 and its incident edges from G). Let G1 , . . . , Gp be the resulting connected
components of G
for i = 1 to p do
Ti = GeneratePseudoTree (Gi , d|Gi )
Make root of Ti a child of X1

6 return T

2.4 Orderings and Pseudo Trees
Given an ordering d, the structural information captured in the primal graph through the scopes
of the functions F = {f1 , . . . , fr } can be used to create the unique pseudo tree that corresponds
to d (Mateescu & Dechter, 2005). This is precisely the bucket tree (or elimination tree), that is
created by BE (when variables are processed in reverse d). The same pseudo tree can be created by
conditioning on the primal graph, and processing variables in the order d, as described in Procedure
GeneratePseudoTree. In the following, d|Gi is the restriction of the order d to the nodes of
the graph Gi .

3. Overview of AND/OR Search Space for Graphical Models
The AND/OR search space is a recently introduced (Dechter & Mateescu, 2004a, 2004b, 2007)
unifying framework for advanced algorithmic schemes for graphical models. Its main virtue consists in exploiting independencies between variables during search, which can provide exponential
speedups over traditional search methods oblivious to problem structure. Since AND/OR MDDs
are based on AND/OR search spaces we need to provide a comprehensive overview for the sake of
completeness.
3.1 AND/OR Search Trees
The AND/OR search tree is guided by a pseudo tree of the primal graph. The idea is to exploit
the problem decomposition into independent subproblems during search. Assigning a value to a
variable (also known as conditioning), is equivalent in graph terms to removing that variable (and its
incident edges) from the primal graph. A partial assignment can therefore lead to the decomposition
of the residual primal graph into independent components, each of which can be searched (or solved)
separately. The pseudo tree captures precisely all these decompositions given an order of variable
instantiation.
D EFINITION 11 (AND/OR search tree of a graphical model) Given a graphical model M =
hX, D, Fi, its primal graph G and a pseudo tree T of G, the associated AND/OR search tree
has alternating levels of OR and AND nodes. The OR nodes are labeled Xi and correspond to
variables. The AND nodes are labeled hXi , xi i (or simply xi ) and correspond to value assignments.
The structure of the AND/OR search tree is based on T . The root is an OR node labeled with the
root of T . The children of an OR node Xi are AND nodes labeled with assignments hXi , xi i that

475

M ATEESCU , D ECHTER & M ARINESCU

A

A

B

1

B

B

0
E

E

0

C

D

1
C

0 1

E

0

1

D
0 1

(a) Pseudo tree

0 1

0
C

E

0

1

D

D

0 1

0 1

0 1

1
C

E

0

1

D

D

0 1

0 1

0 1

C
0

1

D

D

D

0 1

0 1

0 1

(b) Search tree

Figure 6: AND/OR search tree
are consistent with the assignments along the path from the root. The children of an AND node
hXi , xi i are OR nodes labeled with the children of variable Xi in the pseudo tree T .
Example 6 Figure 6 shows an example of an AND/OR search tree for the graphical model given in
Figure 5(a), assuming all tuples are consistent, and variables are binary valued. When some tuples
are inconsistent, some of the paths in the tree do not exist. Figure 6(a) gives the pseudo tree that
guides the search, from top to bottom, as indicated by the arrows. The dotted arcs are backarcs
from the primal graph. Figure 6(b) shows the AND/OR search tree, with the alternating levels of
OR (circle) and AND (square) nodes, and having the structure indicated by the pseudo tree.
The AND/OR search tree can be traversed by a depth first search algorithm, thus using linear
space. It was already shown (Freuder & Quinn, 1985; Bayardo & Miranker, 1996; Darwiche, 2001;
Dechter & Mateescu, 2004a, 2007) that:
T HEOREM 1 Given a graphical model M over n variables, and a pseudo tree T of depth m, the
size of the AND/OR search tree based on T is O(n k m ), where k bounds the domains of variables.
A graphical model of treewidth w∗ has a pseudo tree of depth at most w∗ log n, therefore it has an
∗
AND/OR search tree of size O(n k w log n ).
The AND/OR search tree expresses the set of all possible assignments to the problem variables
(all solutions). The difference from the traditional OR search space is that a solution is no longer a
path from root to a leaf, but rather a tree, defined as follows:
D EFINITION 12 (solution tree) A solution tree of an AND/OR search tree contains the root node.
For every OR node, it contains one of its child nodes and for each of its AND nodes it contains all
its child nodes, and all its leaf nodes are consistent.
3.2 AND/OR Search Graph
The AND/OR search tree may contain nodes that root identical subproblems. These nodes are said
to be unifiable. When unifiable nodes are merged, the search space becomes a graph. Its size
becomes smaller at the expense of using additional memory by the search algorithm. The depth first
search algorithm can therefore be modified to cache previously computed results, and retrieve them
when the same nodes are encountered again. The notion of unifiable nodes is defined formally next.

476

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

D EFINITION 13 (minimal AND/OR graph, isomorphism) Two AND/OR search graphs G and G0
are isomorphic if there exists a one to one mapping σ from the vertices of G to the vertices of G0
such that for any vertex v, if σ(v) = v 0 , then v and v 0 root identical subgraphs relative to σ. An
AND/OR graph is called minimal if all its isomorphic subgraphs are merged. Isomorphic nodes
(that root isomorphic subgraphs) are also said to be unifiable.
It was shown by Dechter and Mateescu (2007) that:
T HEOREM 2 A graphical model M has a unique minimal AND/OR search graph relative to a
pseudo-tree T .
The minimal AND/OR graph of a graphical model G relative to a pseudo tree T is denoted by
MT (G). Note that the definition of minimality used in the work of Dechter and Mateescu (2007)
is based only on isomorphism reduction. We will extend it here by also including the elimination
of redundant nodes. The previous theorem only shows that given an AND/OR graph, the merge
operator has a fixed point, which is the minimal AND/OR graph. We will show in this paper that
the AOMDD is a canonical representation, namely that any two equivalent graphical models can
be represented by the same unique AOMDD given that they accept the same pseudo tree, and the
AOMDD is minimal in terms of number of nodes.
Some unifiable nodes can be identified based on their contexts. We can define graph based
contexts for both OR nodes and AND nodes, just by expressing the set of ancestor variables in T
that completely determine a conditioned subproblem. However, it can be shown that using caching
based on OR contexts makes caching based on AND contexts redundant and vice versa, so we will
only use OR caching. Any value assignment to the context of X separates the subproblem below X
from the rest of the network.
D EFINITION 14 (OR context) Given a pseudo tree T of an AND/OR search space,
context(X) = [X1 . . . Xp ] is the set of ancestors of X in T , ordered descendingly, that are connected in the primal graph to X or to descendants of X.
D EFINITION 15 (context unifiable OR nodes) Given an AND/OR search graph, two OR nodes n1
and n2 are context unifiable if they have the same variable label X and the assignments of their
contexts is identical. Namely, if π1 is the partial assignment of variables along the path to n1 , and
π2 is the partial assignment of variables along the path to n2 , then their restriction to the context of
X is the same: π1 |context(X) = π2 |context(X) .
The depth first search algorithm that traverses the AND/OR search tree, can be modified to
traverse a graph, if enough memory is available. We could allocate a cache table for each variable X,
the scope of the table being context(X). The size of the cache table for X is therefore the product
of the domains of variables in its context. For each variable X, and for each possible assignment
to its context, the corresponding conditioned subproblem is solved only once and the computed
value is saved in the cache table, and whenever the same context assignment is encountered again,
the value of the subproblem is retrieved from the cache table. Such an algorithm traverses what is
called the context minimal AND/OR graph.
D EFINITION 16 (context minimal AND/OR graph) The context minimal AND/OR graph is obtained from the AND/OR search tree by merging all the context unifiable OR nodes.
477

M ATEESCU , D ECHTER & M ARINESCU

R

F

G

B

[]

C

A
J

K

[C]

H

[C]

L

[CK]

A

[CH]

N

[CKL]

B

[CHA]

O

[CKLN]

P

[CKO]

H
E
C

D

L

E

[CHAB]

R

[HAB]

J

[CHAE]

F

[AR]

D

[CEJ]

G

[AF]

M

[CD]

K
M
N
P
O

(a) Primal graph

(b) Pseudo tree
C

0

0

K

H

K

0

1

0

1

L

L

L

L

H

0

1

A

0

A

1

A

A

0

1

0

1

0

1

0

1

0

1

0

1

0

1

0

1

N

N

N

N

N

N

N

N

B

B

B

B

B

B

B

B

0

1

0

1

0

1

0

1

0

1

0

1

0

1

0

1

O

O

O

O

O

O

O

O

O

O

O

O

O

O

O

O

0

E

0

1

E

E

E

0

1

E

E

E

0

1

E

E

E

E

0

1

E

E

E

E

1

E

0

0

1

R

R

R

0

1

R

R

R

R

1

R

0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1
0 1 0 1 0 1 0 1 0 1
P

P

P

P

P

P

P

0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1

0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1

P
J

J

J

J

J

J

J

J

J

J

J

J

J

J

J

J

0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1
0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1

D

D

D

D

0 1 0 1 0 1 0 1

D

D

D

F

F

F

F

0 1 0 1 0 1 0 1

D

0 1 0 1 0 1 0 1

G

G

G

G

0 1 0 1 0 1 0 1
M

M

M

M

0 1 0 1 0 1 0 1

(c) Context minimal graph

Figure 7: AND/OR search graph
It was already shown (Bayardo & Miranker, 1996; Dechter & Mateescu, 2004a, 2007) that:
T HEOREM 3 Given a graphical model M, its primal graph G and a pseudo tree T , the size of the
context minimal AND/OR search graph based on T , and therefore the size of its minimal AND/OR
∗
search graph, is O(n k wT (G) ), where wT∗ (G) is the induced width of G over the depth first traversal
of T , and k bounds the domain size.

Example 7 Let’s look at the impact of caching on the size of the search space by examining a larger
example. Figure 7(a) shows a graphical model with binary variables and Figure 7(b) a pseudo tree
that drives the AND/OR search. The context of each node is given in square brackets. The context
minimal graph is given in Figure 7(c). Note that it is far smaller than the AND/OR search tree,
which has 28 = 256 AND nodes at the level of M alone (because M is at depth 8 in the pseudo tree).
The shaded rectangles show the size of each cache table, equal to the number of OR nodes that
appear in each one. A cache entry is useful whenever there are more than one incoming edges into
the OR node. Incidentally, the caches that are not useful (namely OR nodes with only one incoming
arc), are called dead caches (Darwiche, 2001), and can be determined based only on the pseudo
478

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

tree inspection, therefore a cache table need not be allocated for them. The context minimal graph
can also explain the execution of BE along the same pseudo tree (or, equivalently, along its depth
first traversal order). The buckets are the shaded rectangles, and the processing is done bottom up.
The number of possible assignments to each bucket equals the number of AND nodes that appear
in it. The message scope is identical to the context of the bucket variable, and the message itself is
identical to the corresponding cache table. For more details on the relationship between AND/OR
search and BE see the work of Mateescu and Dechter (2005).
3.3 Weighted AND/OR Graphs
In the previous subsections we described the structure of the AND/OR trees and graphs. In order
to use them to solve a reasoning task, we need to define a way of using the input function values
during the traversal of an AND/OR graph. This is realized by placing weights (or costs) on the
OR-to-AND arcs, dictated by the function values. Only the functions that are relevant contribute to
an OR-to-AND arc weight, and this is captured by the buckets relative to the pseudo tree:
D EFINITION 17 (buckets relative to a pseudo tree) Given a graphical model M = hX, D, F, ⊗i
and a pseudo tree T , the bucket of Xi relative to T , denoted BT (Xi ), is the set of functions whose
scopes contain Xi and are included in pathT (Xi ), which is the set of variables from the root to Xi
in T . Namely,
BT (Xi ) = {f ∈ F|Xi ∈ scope(f ), scope(f ) ⊆ pathT (Xi )}.

A function belongs to the bucket of a variable Xi iff its scope has just been fully instantiated
when Xi was assigned. Combining the values of all functions in the bucket, for the current assignment, gives the weight of the OR-to-AND arc:
D EFINITION 18 (OR-to-AND weights) Given an AND/OR graph of a graphical model M, the
weight w(n,m) (Xi , xi ) of arc (n, m) where Xi labels n and xi labels m, is the combination of
all the functions in BT (Xi ) assigned by values along the current path to the AND node m, πm .
Formally, w(n,m) (Xi , xi ) = ⊗f ∈BT (Xi ) f (asgn(πm )[scope(f )]).
D EFINITION 19 (weight of a solution tree) Given a weighted AND/OR graph of a graphical model
M, and given a solution tree t having the OR-to-AND set of arcs arcs(t), the weight of t is defined
by w(t) = ⊗e∈arcs(t) w(e).
Example 8 We start with the more straightforward case of constraint networks. Since functions
only take values 0 or 1, and the combination is by product (join of relations), it follows that any ORto-AND arc can only have a weight of 0 or 1. An example is given in Figure 8. Figure 8(a) shows
a constraint graph, 8(b) a pseudo tree for it, and 8(c) the four relations that define the constraint
problem. Figure 8(d) shows the AND/OR tree that can be traversed by a depth first search algorithm
that only checks the consistency of the input functions (i.e., no constraint propagation is used).
Similar to the OBDD representation, the OR-to-AND arcs with a weight of 0 are denoted by dotted
lines, and the tree is not unfolded below them, since it will not contain any solution. The arcs with
a weight of 1 are drawn with solid lines.
479

M ATEESCU , D ECHTER & M ARINESCU

A
B
C

A

D

F

B

E

(a) Constraint graph
A
0
0
0
0
1
1
1
1

B
0
0
1
1
0
0
1
1

B
0
0
0
0
1
1
1
1

C RABC
0
1
1
1
0
0
1
1
0
1
1
1
0
1
1
0

C
0
0
1
1
0
0
1
1

C

E

D

F

(b) Pseudo tree

D RBCD
0
1
1
1
0
1
1
0
0
1
1
0
0
1
1
1

A
0
0
0
0
1
1
1
1

B
0
0
1
1
0
0
1
1

E RABE
0
1
1
0
0
1
1
1
0
0
1
1
0
1
1
0

A
0
0
0
0
1
1
1
1

E
0
0
1
1
0
0
1
1

F RAEF
0
0
1
1
0
1
1
1
0
1
1
1
0
1
1
0

(c) Relations
A

1

1

0

1

B

B

1

1

1
1

0
C

C

E

1
1

0
C

E

C

E

E

1

1

1

0

0

1

1

1

1

1

0

1

1

0

1

0

0

1

0

1

0

1

0

1

0

1

0

1

0

1

0

1

D

D

D

F

1

1 1

0 0

1

0

1

1

1

0

0

1
0

F

D

F

1 0

1 1

1

1

1

1

1

0

0

0

D

D

F

F

1 1

0

1

0

1

0

1

1

0

1

0

1

0

1
0

1
1

(d) AND/OR tree

Figure 8: AND/OR search tree for constraint networks
Example 9 Figure 9 shows a weighted AND/OR tree for a belief network. Figure 9(a) shows the
directed acyclic graph, and the dotted arc BC added by moralization. Figure 9(b) shows the pseudo
tree, and 9(c) shows the conditional probability tables. Figure 9(d) shows the weighted AND/OR
tree.
As we did for constraint networks, we can move from weighted AND/OR search trees to
weighted AND/OR search graphs by merging unifiable nodes. In this case the arc labels should be
also considered when determining unifiable subgraphs. This can yield context-minimal weighted
AND/OR search graphs and minimal weighted AND/OR search graphs.

4. AND/OR Multi-Valued Decision Diagrams (AOMDDs)
In this section we begin describing the contributions of this paper. The context minimal AND/OR
graph (Definition 16) offers an effective way of identifying some unifiable nodes during the execution of the search algorithm. Namely, context unifiable nodes are discovered based only on their
paths from the root, without actually solving their corresponding subproblems. However, merging based on context is not complete, which means that there may still exist unifiable nodes in
the search graph that do not have identical contexts. Moreover, some of the nodes in the context
480

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

P(A)

A

A
0
1

A

P(B | A)

P(A)
.6
.4

A
0
1

P(C | A)
A
0
1

B=1
.6
.9

B=0
.4
.1

P(D | B,C)

B
B

B
0
0
1
1

C

E
E

C
D

D

(a) Belief network

C
0
1
0
1

P(E | A,B)
D=1
.8
.9
.7
.5

D=0
.2
.1
.3
.5

C=1
.8
.3

C=0
.2
.7

(b) Pseudo tree

A
0
0
1
1

B
0
1
0
1

E=0
.4
.5
.7
.2

E=1
.6
.5
.3
.8

(c) CPTs

A

.6

.4

0

1

B

B

.4
0

0

C

.6
1

E

.2

.8

.5

0

1

0

D

.2
0

1

D

.8
1

.1
0

.2

.8

.7

0

1

0

D

.9
1

.3
0

C

0

E

.3

.7

.3

1

0

1

D

D

.7 .5
1

1

E

C

.5

.9

0

1

E

.4

.1

.6

.2

.5

0

1

.2
0

C

.8

.7

.3

1

0

1

D

.8
1

D

.1
0

.9
1

.3
0

D

.7 .5
1

0

.5
1

(d) Weighted AND/OR tree

Figure 9: Weighted AND/OR search tree for belief networks
minimal AND/OR graph may be redundant, for example when the set of solutions rooted at variable Xi is not dependant on the specific value assigned to Xi (this situation is not detectable based
on context). This is sometimes termed as “interchangeable values” or “symmetrical values”. As
overviewed earlier, Dechter and Mateescu (2007, 2004a) defined the complete minimal AND/OR
graph which is an AND/OR graph whose unifiable nodes are all merged, and Dechter and Mateescu
(2007) also proved the canonicity for non-weighted graphical models.
In this paper we propose to augment the minimal AND/OR search graph with removing redundant variables as is common in OBDD representation as well as adopt notational conventions
common in this community. This yields a data structure that we call AND/OR BDD, that exploits
decomposition by using AND nodes. We present the extension over multi-valued variables yielding
AND/OR MDD or AOMDD and define them for general weighted graphical models. Subsequently
we present two algorithms for compiling the canonical AOMDD of a graphical model: the first is
search-based, and uses the memory intensive AND/OR graph search to generate the context minimal
AND/OR graph, and then reduces it bottom up by applying reduction rules; the second is inferencebased, and uses a Bucket Elimination schedule to combine the AOMDDs of initial functions by
APPLY operations (similar to the apply for OBDDs). As we will show, both approaches have the
same worst case complexity as the AND/OR graph search with context based caching, and also the
same complexity as Bucket Elimination, namely time and space exponential in the treewidth of the
∗
problem, O(n k w ). The benefit of each of these generation schemes will be discussed.

481

M ATEESCU , D ECHTER & M ARINESCU

A

A

1

(a) OBDD

2

…

k

(b) MDD

Figure 10: Decision diagram nodes (OR)
A

A

1

…

…

…

(a) AOBDD

2

k

…

…

…

(b) AOMDD

Figure 11: Decision diagram nodes (AND/OR)
4.1 From AND/OR Search Graphs to Decision Diagrams
An AND/OR search graph G of a graphical model M = hX, D, F, ⊗i represents the set of all
possible assignments to the problem variables (all solutions and their costs). In this sense, G can
be viewed as representing the function f = ⊗fi ∈F fi that defines the universal equivalent graphical
model u(M) (Definition 2). For each full assignment x = (x1 , . . . , xn ), if x is a solution expressed
by the tree tx , then f (x) = w(tx ) = ⊗e∈arcs(tx ) w(e) (Definition 19); otherwise f (x) = 0 (the
assignment is inconsistent). The solution tree tx of a consistent assignment x can be read from G
in linear time by following the assignments from the root. If x is inconsistent, then a dead-end is
encountered in G when attempting to read the solution tree tx , and f (x) = 0. Therefore, G can be
viewed as a decision diagram that determines the values of f for every complete assignment x.
We will now see how we can process an AND/OR search graph by reduction rules similar to
the case of OBDDs, in order to obtain a representation of minimal size. In the case of OBDDs,
a node is labeled with a variable name, for example A, and the low (dotted line) and high (solid
line) outgoing arcs capture the restriction of the function to the assignments A = 0 or A = 1. To
determine the value of the function, one needs to follow either one or the other (but not both) of the
outgoing arcs from A (see Figure 10(a)). The straightforward extension of OBDDs to multi-valued
variables (multi-valued decision diagrams, or MDDs) was presented by Srinivasan, Kam, Malik,
and Brayton (1990), and the node structure that they use is given in Figure 10(b). Each outgoing arc
is associated with one of the k values of variable A.
In this paper we generalize the OBDD and MDD representations demonstrated in Figures 10(a)
and 10(b) by allowing each outgoing arc to be an AND arc. An AND arc connects a node to a set of
nodes, and captures the decomposition of the problem into independent components. The number of
AND arcs emanating from a node is two in the case of AOBDDs (Figure 11(a)), or the domain size
of the variable in the general case (Figure 11(b)). For a given node A, each of its k AND arcs can
connect it to possibly different number of nodes, depending on how the problem decomposes based
on each particular assignment of A. The AND arcs are depicted by a shaded sector that connects
the outgoing lines corresponding to the independent components.

482

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

A
1

…

2

…

…k

…

…

(a) Nonterminal meta-node

0

1

(b) Terminal meta-node 0

(c) Terminal meta-node 1

Figure 12: Meta-nodes
We define the AND/OR Decision Diagram representation based on AND/OR search graphs. We
find that it is useful to maintain the semantics of Figure 11 especially when we need to express the
redundancy of nodes, and therefore we introduce the meta-node data structure, which defines small
portions of any AND/OR graph, based on an OR node and its AND children:
D EFINITION 20 (meta-node) A meta-node u in an AND/OR search graph can be either: (1) a
terminal node labeled with 0 or 1, or (2) a nonterminal node, that consists of an OR node labeled
X (therefore var(u) = X) and its k AND children labeled x1 , . . . , xk that correspond to the value
assignments of X. Each AND node labeled xi stores a list of pointers to child meta-nodes, denoted
by u.childreni . In the case of weighted graphical models, the AND node xi also stores the OR-toAND arc weight w(X, xi ).
The rectangle in Figure 12(a) is a meta-node for variable A, that has a domain of size k. Note
that this is very similar to Figure 11, with the small difference that the information about the value of
A that corresponds to each outgoing AND arc is now stored in the AND nodes of the meta-node. We
are not showing the weights in that figure. A larger example of an AND/OR graph with meta-nodes
appears later in Figure 16.
The terminal meta-nodes play the role of the terminal nodes in OBDDs. The terminal metanode 0, shown in Figure 12(b), indicates inconsistent assignments, while the terminal meta-node 1,
shown in figure 12(c) indicates consistent ones.
Any AND/OR search graph can now be viewed as a diagram of meta-nodes, simply by grouping
OR nodes with their AND children, and adding the terminal meta-nodes appropriately.
Once we have defined the meta-nodes, it is easier to see when a variable is redundant with respect to the outcome of the function based on the current partial assignment. A variable is redundant
if any of its assignments leads to the same set of solutions.
D EFINITION 21 (redundant meta-node) Given a weighted AND/OR search graph G represented
with meta-nodes, a meta-node u with var(u) = X and |D(X)| = k is redundant iff:
(a) u.children1 = . . . = u.childrenk and
(b) w(X, x1 ) = . . . = w(X, xk ).
An AND/OR graph G, that contains a redundant meta-node u, can be transformed into an equivalent graph G 0 by replacing any incoming arc into u with its common list of children u.children1 ,
absorbing the common weight w(X, x1 ) by combination into the weight of the parent meta-node
corresponding to the incoming arc, and then removing u and its outgoing arcs from G. The
value X = x1 is picked here arbitrarily, because they are all isomorphic. If u is the root of the
483

M ATEESCU , D ECHTER & M ARINESCU

Procedure RedundancyReduction
: AND/OR graph G; redundant meta-node u, with var(u) = X; List of meta-node parents of u,
denoted by P arents(u).
output : Reduced AND/OR graph G after the elimination of u.
1 if P arents(u) is empty then
2
return independent AND/OR graphs rooted by meta-nodes in u.children1 , and constant w(X, x1 )
input

3 forall v ∈ P arents(u) (assume var(v) == Y ) do
4
forall i ∈ {1, . . . , |D(Y )|} do
5
if u ∈ v.childreni then
6
v.childreni ← v.childreni \ {u}
7
v.childreni ← v.childreni ∪ u.children1
8
w(Y, yi ) ← w(Y, yi ) ⊗ w(X, x1 )
9 remove u
10 return reduced AND/OR graph G

Procedure IsomorphismReduction
: AND/OR graph G; isomorphic meta-nodes u and v; List of meta-node parents of u, denoted by
P arents(u).
output : Reduced AND/OR graph G after the merging of u and v.
forall p ∈ P arents(u) do
if u ∈ p.childreni then
p.childreni ← p.childreni \ {u}
p.childreni ← p.childreni ∪ {v}

input

1
2
3
4

5 remove u
6 return reduced AND/OR graph G

graph, then the common weight w(X, x1 ) has to be stored separately as a constant. Procedure
RedundancyReduction formalizes the redundancy elimination.
D EFINITION 22 (isomorphic meta-nodes) Given a weighted AND/OR search graph G represented
with meta-nodes, two meta-nodes u and v having var(u) = var(v) = X and |D(X)| = k are
isomorphic iff:
(a) u.childreni = v.childreni ∀i ∈ {1, . . . , k} and
(b) wu (X, xi ) = wv (X, xi ) ∀i ∈ {1, . . . , k}, (where wu , wv are the weights of u and v).
Procedure IsomorphismReduction formalizes the process of merging isomorphic metanodes. Naturally, the AND/OR graph obtained by merging isomorphic meta-nodes is equivalent to
the original one. We can now define the AND/OR Multi-Valued Decision Diagram:
D EFINITION 23 (AOMDD) An AND/OR Multi-Valued Decision Diagram (AOMDD) is a weighted
AND/OR search graph that is completely reduced by isomorphic merging and redundancy removal,
namely:
(1) it contains no isomorphic meta-nodes; and
(2) it contains no redundant meta-nodes.

484

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

A

A

2…k

1

B

c

2

d

2

…k

z

…
1

1

z

k

…

y

c

…

d

y

(b) After eliminating the B
meta-node

(a) Fragment of an AOMDD

Figure 13: Redundancy reduction
A
1

A

2…k

1

2

…k

B
1

C

2

d

2

1

C

…
1

B

…k

1

e

2

…

…k

C

…
k

2

…
k

1

y

d

(a) Fragment of an AOMDD

2

k

e

…

y

(b) After merging the isomorphic C meta-nodes

Figure 14: Isomorphism reduction
Example 10 Figure 13 shows an example of applying the redundancy reduction rule to a portion
of an AOMDD. On the left side, in Figure 13(a), the meta-node of variable B is redundant (we
don’t show the weights of the OR-to-AND arcs, to avoid cluttering the figure). Any of the values
{1, . . . , k} of B will lead to the same set of meta-nodes {c, d, . . . , y}, which are coupled in an AND
arc. Therefore, the meta-node of B can be eliminated. The result is shown in Figure 13(b), where
the meta-nodes {c, d, . . . , y} and z are coupled in an AND arc outgoing from A = 1.
In Figure 14 we show an example of applying the isomorphism reduction rule. In this case, the
meta-nodes labeled with C in Figure 14(a) are isomorphic (again, we omit the weights). The result
of merging them is shown in Figure 14(b).
Examples of AOMDDs appear in Figures 16, 17 and 18. Note that if the weight on an OR-toAND arc is zero, then the descendant is the terminal meta-node 0. Namely, the current path is a
dead-end, cannot be extended to a solution, and is therefore linked directly to 0.

5. Using AND/OR Search to Generate AOMDDs
In Section 4.1 we described how we can transform an AND/OR graph into an AOMDD by applying
reduction rules. In Section 5.1 we describe the explicit algorithm that takes as input a graphi485

M ATEESCU , D ECHTER & M ARINESCU

cal model, performs AND/OR search with context-based caching to obtain the context minimal
AND/OR graph, and in Section 5.2 we give the procedure that applies the reduction rules bottom
up to obtain the AOMDD.
5.1 Algorithm AND/OR-S EARCH -AOMDD
Algorithm 1, called AND/OR-S EARCH -AOMDD, compiles a graphical model into an AOMDD.
A memory intensive (with context-based caching) AND/OR search is used to create the context minimal AND/OR graph (see Definition 16). The input to AND/OR-S EARCH -AOMDD is a graphical
model M and a pseudo tree T , that also defines the OR-context of each variable.
Each variable Xi has an associated cache table, whose scope is the context of Xi in T . This
ensures that the trace of the search is the context minimal AND/OR graph. A list denoted by LXi
(see line 35), is used for each variable Xi to save pointers to meta-nodes labeled with Xi . These
lists are used by the procedure that performs the bottom up reduction, per layers of the AND/OR
graph (one layer contains all the nodes labeled with one given variable). The fringe of the search
is maintained on a stack called OPEN. The current node (either OR or AND node) is denoted by
n, its parent by p, and the current path by πn . The children of the current node are denoted by
successors(n). For each node n, the Boolean attribute consistent(n) indicates if the current path
can be extended to a solution. This information is useful for pruning the search space.
The algorithm is based on two mutually recursive steps: Forward (beginning at line 5) and
Backtrack (beginning at line 29), which call each other (or themselves) until the search terminates.
In the forward phase, the AND/OR graph is expanded top down. The two types of nodes, AND and
OR, are treated differently according to their semantics.
Before an OR node is expanded, the cache table of its variable is checked (line 8). If the entry
is not null, a link is created to the already existing OR node that roots the graph equivalent to the
current subproblem. Otherwise, the OR node is expanded by generating its AND descendants. The
OR-to-AND weight (see Definition 18) is computed in line 13. Each value xi of Xi is checked for
consistency (line 14). The least expensive check is to verify that the OR-to-AND weight is non-zero.
However, the deterministic (inconsistent) assignments in M can be extracted to form a constraint
network. Any level of constraint propagation can be performed in this step (e.g., look ahead, arc
consistency, path consistency, i-consistency etc.). The computational overhead can increase, in the
hope of pruning the search space more aggressively. We should note that constraint propagation is
not crucial for the algorithm, and the complexity guarantees are maintained even if only the simple
weight check is performed. The consistent AND nodes are added to the list of successors of n (line
16), while the inconsistent ones are linked to the terminal 0 meta-node (line 19).
An AND node n labeled with hXi , xi i is expanded (line 20) based on the structure of the pseudo
tree. If Xi is a leaf in T , then n is linked to the terminal 1 meta-node (line 22). Otherwise, an OR
node is created for each child of Xi in T (line 24).
The forward step continues as long as the current node is not a dead-end and still has unevaluated
successors. The backtrack phase is triggered when a node has an empty set of successors (line 29).
Note that, as each successor is processed, it is removed from the set of successors in line 42. When
the backtrack reaches the root (line 32), the search is complete, the context minimal AND/OR graph
is generated, and the Procedure B OTTOM U P R EDUCTION is called.
When the backtrack step processes an OR node (line 31), it saves a pointer to it in cache, and
also adds a pointer to the corresponding meta-node to the list LXi . The consistent attribute of

486

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

Algorithm 1: AND/OR S EARCH - AOMDD
input : M = hX, D, Fi; pseudo tree T rooted at X1 ; parents pai (OR-context) for every variable Xi .
output : AOMDD of M.
1 forall Xi ∈ X do
Initialize context-based cache table CacheXi (pai ) with null entries
2
3 Create new OR node t, labeled with Xi ; consistent(t) ← true; push t on top of OPEN
4 while OPEN 6= φ do
5
n ← top(OPEN); remove n from OPEN
// Forward
6
successors(n) ← φ
7
if n is an OR node labeled with Xi then
// OR-expand
if CacheXi (asgn(πn )[pai ]) 6= null then
8
Connect parent of n to CacheXi (asgn(πn )[pai ])
9
// Use the cached pointer
10
11
12
13
14
15
16
17
18
19

else
forall xi ∈ Di do
Create new AN D node t, labeled with hXi , xi i
w(X, xi ) ←
⊗
f (asgn(πn )[pai ])
f ∈BT (Xi )

if hXi , xi i is consistent with πn then
consistent(t) ← true
add t to successors(n)
else
consistent(t) ← f alse
make terminal 0 the only child of t

20
21
22
23
24
25
26
27

if n is an AND node labeled with hXi , xi i then
if childrenT (Xi ) == φ then
make terminal 1 the only child of n
else
forall Y ∈ childrenT (Xi ) do
Create new OR node t, labeled with Y
consistent(t) ← f alse
add t to successors(n)

28
29
30
31
32
33

Add successors(n) to top of OPEN
while successors(n) == φ do
let p be the parent of n
if n is an OR node labeled with Xi then
if Xi == X1 then
Call BottomUpReduction procedure

34
35
36
37
38
39

// Constraint Propagation

// AND-expand

// Backtrack

// Search is complete
// begin reduction to AOMDD

Cache(asgn(πn )[pai ]) ← n
Add meta-node of n to the list LXi
consistent(p) ← consistent(p) ∧ consistent(n)
if consistent(p) == f alse then
remove successors(p) from OPEN
successors(p) ← φ

40
41

if n is an AND node labeled with hXi , xi i then
consistent(p) ← consistent(p) ∨ consistent(n);

42
43

remove n from successors(p)
n←p

487

// Save in cache

// Check if p is dead-end

M ATEESCU , D ECHTER & M ARINESCU

Procedure BottomUpReduction
: A graphical model M = hX, D, Fi; a pseudo tree T of the primal graph, rooted at X1 ; Context
minimal AND/OR graph, and lists LXi of meta-nodes for each level Xi .
output : AOMDD of M.
Let d = {X1 , . . . , Xn } be the depth first traversal ordering of T
for i ← n down to 1 do
Let H be a hash table, initially empty
forall meta-nodes n in LXi do
if H(Xi , n.children1 , . . . , n.childrenki , wn (Xi , x1 ), . . . , wn (Xki , xki )) returns a meta-node
p then
merge n with p in the AND/OR graph
input

1
2
3
4
5
6
7
8
9
10
11
12

else if n is redundant then
eliminate n from the AND/OR graph
combine its weight with that of the parent
else
hash n into the table H:
H(Xi , n.children1 , . . . , n.childrenki , wn (Xi , x1 ), . . . , wn (Xki , xki )) ← n

13 return reduced AND/OR graph

the AND parent p is updated by conjunction with consistent(n). If the AND parent p becomes
inconsistent, it is not necessary to check its remaining OR successors (line 38). When the backtrack
step processes an AND node (line 40), the consistent attribute of the OR parent p is updated by
disjunction with consistent(n).
The AND/OR search algorithm usually maintains a value for each node, corresponding to a task
that is solved. We did not include values in our description because an AOMDD is just an equivalent
representation of the original graphical model M. Any task over M can be solved by a traversal
of the AOMDD. It is however up to the user to include more information in the meta-nodes (e.g.,
number of solutions for a subproblem).
5.2 Reducing the Context Minimal AND/OR Graph to an AOMDD
Procedure BottomUpReduction processes the variables bottom up relative to the pseudo tree T .
We use the depth first traversal ordering of T (line 1), but any other bottom up ordering is as good.
The outer for loop (starting at line 2) goes through each level of the context minimal AND/OR graph
(where a level contains all the OR and AND nodes labeled with the same variable, in other words it
contains all the meta-nodes of that variable). For efficiency, and to ensure the complexity guarantees
that we will prove, a hash table, initially empty, is used for each level. The inner for loop (starting at
line 4) goes through all the metanodes of a level, that are also saved (or pointers to them are saved)
in the list LXi . For each new meta-node n in the list LXi , in line 5 the hash table H is checked to
verify if a node isomorphic with n already exists. If the hash table H already contains a node p corresponding to the hash key (Xi , n.children1 , . . . , n.childrenki , wn (Xi , x1 ), . . . , wn (Xki , xki )),
then p and n are isomorphic and should be merged. Otherwise, if the new meta-node n is redundant,
then it is eliminated from the AND/OR graph. If none of the previous two conditions is met, then
the new meta-node n is hashed into table H.

488

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

A
D

G

C

B

F

E

A

H

B
C
D

(a)

F
E

G

H

(b)

Figure 15: (a) Constraint graph for C = {C1 , . . . , C9 }, where C1 = F ∨ H, C2 = A ∨ ¬H,
C3 = A ⊕ B ⊕ G, C4 = F ∨ G, C5 = B ∨ F , C6 = A ∨ E, C7 = C ∨ E, C8 = C ⊕ D,
C9 = B ∨ C; (b) Pseudo tree (bucket tree) for ordering d = (A, B, C, D, E, F, G, H)

Proposition 1 The output of Procedure BottomUpReduction is the AOMDD of M along the
pseudo tree T , namely the resulting AND/OR graph is completely reduced.
Note that we explicated Procedure BottomUpReduction separately only for clarity. In practice, it can actually be included in Algorithm AND/OR-S EARCH -AOMDD, and the reduction rules
can be applied whenever the search backtracks. We can maintain a hash table for each variable, during the AND/OR search, to store pointers to meta-nodes. When the search backtracks out of an
OR node, it can already check the redundancy of that meta-node, and also look up in the hash table
to check for isomorphism. Therefore, the reduction of the AND/OR graph can be done during the
AND/OR search, and the output will be the AOMDD of M.
From Theorem 3 and Proposition 1 we can conclude:
T HEOREM 4 Given a graphical model M and a pseudo tree T of its primal graph G, the AOMDD
∗
of M corresponding to T has size bounded by O(n k wT (G) ) and it can be computed by Algorithm
∗
AND/OR-S EARCH -AOMDD in time O(n k wT (G) ), where wT∗ (G) is the induced width of G over
the depth first traversal of T , and k bounds the domain size.

6. Using Bucket Elimination to Generate AOMDDs
In this section we propose to use a Bucket Elimination (BE) type algorithm to guide the compilation
of a graphical model into an AOMDD. The idea is to express the graphical model functions as
AOMDDs, and then combine them with APPLY operations based on a BE schedule. The APPLY is
very similar to that from OBDDs (Bryant, 1986), but it is adapted to AND/OR search graphs. It
takes as input two functions represented as AOMDDs based on the same pseudo tree, and outputs
the combination of initial functions, also represented as an AOMDD based on the same pseudo tree.
We will describe it in detail in Section 6.2.
We will start with an example based on constraint networks. This is easier to understand because
the weights on the arcs are all 1 or 0, and therefore are depicted in the figures by solid and dashed
lines, respectively.
Example 11 Consider the network defined by X = {A, B, . . . , H}, DA = . . . = DH = {0, 1} and
the constraints (where ⊕ denotes XOR): C1 = F ∨H, C2 = A∨¬H, C3 = A⊕B⊕G, C4 = F ∨G,
489

M ATEESCU , D ECHTER & M ARINESCU

m7

A

m7
A

A
0

0

1

B
0

1

0

C
0

C
1

0

1

0

0

1

C
1

0

A

1

B

B

0

0

F

C
1

B
1

0

1

F
1

0

1

F
1

B

F

0

1

0

1

B

C
D
0

D

G

E
1

0

0

0

1

0

1

m6

G
1

0

H
1

0

0

1

D

E

1

m3

F

H
1

G

m3

m6

A
0

0

B

C

0

C
0

A

1

0

1

D
1

0

0

C

C
1

0

0

F

B

0

1

0

A

1

0

0

G

1

0

0

1

0

0

H
0

1

B

F
1

G
1

1

F

F

1

F
0

1

A

B
1

B
1

E

D
1

1

B

A

0

H

1

H
1

0

F
1

C

0 1

0

1

0

m5

C9

C

0

E

0

C

D
1

0

0

0

E

D

1

C
1

0

1

0

0

1

G

1

0

1

0

C

A

0 1

C6

C7

1

0

1

0

1

0

1

0

B

H

0 1

1

A
1

0

H

1

G

E

C3

0

1

A

1

G
0

G

F

F
0

B

G

H

m1

m1

1

B

0 1

C8

m2

A

A

1

E

E

1

0

C5

m2

A
1

D

D

D

m4

m5
0

0 1

1
m4

C4

0

H
1

0

F

1

F

0 1

0 1

G

C1

C2

H

Figure 16: Execution of BE with AOMDDs
A
0

A

1

B
0

B
1

0

B

1

B

C
C
0

C
1

0

C
1

0

C
1

0

F
1

0

F
1

0

F
1

0

C

1

0

D

1

D

E

D
0

D
1

0

E
1

0

G
1

0

G
1

0

H
1

0

C

C

F

0

D

D

E

F

H
1

D

E

F

F

F

1

G

G

G

G

H

0

D

1

H

1

(a)

G

0

(b)

Figure 17: (a) The final AOMDD; (b) The OBDD corresponding to d
C5 = B ∨ F , C6 = A ∨ E, C7 = C ∨ E, C8 = C ⊕ D, C9 = B ∨ C. The constraint graph is
shown in Figure 15(a). Consider the ordering d = (A, B, C, D, E, F, G, H). The pseudo tree (or
bucket tree) induced by d is given in Fig. 15(b). Figure 16 shows the execution of BE with AOMDDs
along ordering d. Initially, the constraints C1 through C9 are represented as AOMDDs and placed
in the bucket of their latest variable in d. The scope of any original constraint always appears on a

490

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

Algorithm 2: BE-AOMDD
: Graphical model M = hX, D, Fi, where X = {X1 , . . . , Xn }, F = {f1 , . . . , fr } ; order
d = (X1 , . . . , Xn )
output : AOMDD representing ⊗i∈F fi
1 T = GeneratePseudoTree(G, d);
2 for i ← 1 to r do
// place functions in buckets
3
place Gfaomdd
in
the
bucket
of
its
latest
variable
in
d
i
input

4 for i ← n down to 1 do
message(Xi ) ← G1aomdd
5
6
while bucket(Xi ) 6= φ do
7
pick Gfaomdd from bucket(Xi );
8
9
10

// process buckets
// initialize with AOMDD of 1 ;
// combine AOMDDs in bucket of Xi

bucket(Xi ) ← bucket(Xi ) \ {Gfaomdd };
message(Xi ) ← APPLY(message(Xi ), Gfaomdd )
add message(Xi ) to the bucket of the parent of Xi in T

11 return message(X1 )

path from root to a leaf in the pseudo tree. Therefore, each original constraint is represented by an
AOMDD based on a chain (i.e., there is no branching into independent components at any point).
The chain is just the scope of the constraint, ordered according to d. For bi-valued variables, the
original constraints are represented by OBDDs, for multiple-valued variables they are MDDs. Note
that we depict meta-nodes: one OR node and its two AND children, that appear inside each gray
node. The dotted edge corresponds to the 0 value (the low edge in OBDDs), the solid edge to the
1 value (the high edge). We have some redundancy in our notation, keeping both AND value nodes
and arc-types (dotted arcs from “0” and solid arcs from “1”).
The BE scheduling is used to process the buckets in reverse order of d. A bucket is processed
by joining all the AOMDDs inside it, using the APPLY operator. However, the step of elimination
of the bucket variable is omitted because we want to generate the full AOMDD. In our example,
the messages m1 = C1 ./ C2 and m2 = C3 ./ C4 are still based on chains, therefore they are
OBDDs. Note that they contain the variables H and G, which have not been eliminated. However,
the message m3 = C5 ./ m1 ./ m2 is not an OBDD anymore. We can see that it follows the
structure of the pseudo tree, where F has two children, G and H. Some of the nodes corresponding
to F have two outgoing edges for value 1.
The processing continues in the same manner. The final output of the algorithm, which coincides
with m7 , is shown in Figure 17(a). The OBDD based on the same ordering d is shown in Fig.
17(b). Notice that the AOMDD has 18 nonterminal nodes and 47 edges, while the OBDD has 27
nonterminal nodes and 54 edges.
6.1 Algorithm BE-AOMDD
Algorithm 2, called BE-AOMDD, creates the AOMDD of a graphical model by using a BE schedule for APPLY operations. Given an order d of the variables, first a pseudo tree is created based on
,
the primal graph. Each initial function fi is then represented as an AOMDD, denoted by Gfaomdd
i
and placed in its bucket. To obtain the AOMDD of a function, the scope of the function is ordered
according to d, a search tree (based on a chain) that represents fi is generated, and then reduced
by Procedure BottomUpReduction. The algorithm proceeds exactly like BE, with the only difference that the combination of functions is realized by the APPLY algorithm, and variables are not
491

M ATEESCU , D ECHTER & M ARINESCU

eliminated but carried over to the destination bucket. The messages between buckets are initialized
with the dummy AOMDD of 1, denoted by G1aomdd , which is neutral for combination.
In order to create the compilation of a graphical model based on AND/OR graphs, it is necessary
to traverse the AND/OR graph top down and bottom up. This is similar to the inward and outward
message passing in a tree decomposition. Note that BE-AOMDD describes the bottom up traversal
explicitly, while the top down phase is actually performed by the APPLY operation. When two
AOMDDs are combined, after the top chain portion of their pseudo tree is processed, the remaining
independent branches are attached only if they participate in the newly restricted set of solutions.
This amounts to an exchange of information between the independent branches, which is equivalent
to the top down phase.
6.2 The AOMDD APPLY Operation
We will now describe how to combine two AOMDDs. The APPLY operator takes as input two
AOMDDs representing functions f1 and f2 and returns an AOMDD representing f1 ⊗ f2 .
In OBDDs the apply operator combines two input diagrams based on the same variable ordering.
Likewise, in order to combine two AOMDDs we assume that their pseudo trees are identical. This
condition is satisfied by any two AOMDDs in the same bucket of BE-AOMDD. However, we
present here a version of APPLY that is more general, by relaxing the previous condition from
identical to compatible pseudo trees. Namely, there should be a pseudo tree in which both can be
embedded. In general, a pseudo tree induces a strict partial order between the variables where a
parent node always precedes its child nodes.
D EFINITION 24 (compatible pseudo trees) A strict partial order d1 = (X, <1 ) over a set X is
consistent with a strict partial order d2 = (Y, <2 ) over a set Y, if for all x1 , x2 ∈ X ∩ Y, if
x1 <2 x2 then x1 <1 x2 . Two partial orders d1 and d2 are compatible iff there exists a partial
order d that is consistent with both. Two pseudo trees are compatible iff the partial orders induced
via the parent-child relationship, are compatible.
For simplicity, we focus on a more restricted notion of compatibility, which is sufficient when
using a BE like schedule for the APPLY operator to combine the input AOMDDs (as described in
Section 6). The APPLY algorithm that we will present can be extended to the more general notion
of compatibility.
D EFINITION 25 (strictly compatible pseudo trees) A pseudo tree T1 having the set of nodes X1
can be embedded in a pseudo tree T having the set of nodes X if X1 ⊆ X and T1 can be obtained
from T by deleting each node in X \ X1 and connecting its parent to each of its descendents. Two
pseudo trees T1 and T2 are strictly compatible if there exists T such that both T1 and T2 can be
embedded in T .
Algorithm APPLY (algorithm 3) takes as input one node from Gfaomdd and a list of nodes from
Ggaomdd . Initially, the node from Gfaomdd is its root node, and the list of nodes from Ggaomdd is in fact
also made of just one node, which is its root. We will sometimes identify an AOMDD by its root
node. The pseudo trees Tf and Tg are strictly compatible, having a target pseudo tree T .
The list of nodes from Ggaomdd always has a special property: there is no node in it that can be the
ancestor in T of another (we refer to the variable of the meta-node). Therefore, the list z1 , . . . , zm
492

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

APPLY (v1 ; z1 , . . . , zm )
input : AOMDDs Gfaomdd with nodes vi and Ggaomdd with nodes zj , based on strictly compatible pseudo
trees Tf , Tg that can be embedded in T .
var(v1 ) is an ancestor of all var(z1 ), . . . , var(zm ) in T .
var(zi ) and var(zj ) are not in ancestor-descendant relation in T , ∀i 6= j.
output : v1 ⊗ (z1 ∧ . . . ∧ zm ), based on T .
if H1 (v1 , z1 , . . . , zm ) 6= null then return H1 (v1 , z1 , . . . , zm );
// is in cache
if (any of v1 , z1 , . . . , zm is 0) then return 0
if (v1 = 1) then return 1
if (m = 0) then return v1
// nothing to combine
create new nonterminal meta-node u
var(u) ← var(v1 ) (call it Xi , with domain Di = {x1 , . . . , xki } )
for j ← 1 to ki do
u.childrenj ← φ
// children of the j-th AND node of u
// assign weight from v1
wu (Xi , xj ) ← wv1 (Xi , xj )
if ( (m = 1) and (var(v1 ) = var(z1 ) = Xi ) ) then
temp Children ← z1 .childrenj
// combine input weights
wu (Xi , xj ) ← wv1 (Xi , xj ) ⊗ wz1 (Xi , xj )

Algorithm 3:

1
2
3
4
5
6
7
8
9
10
11
12
13
14

else

15
16
17
18
19

group nodes from v1 .childrenj ∪ temp Children in several {v 1 ; z 1 , . . . , z r }
for each {v 1 ; z 1 , . . . , z r } do
y ← APPLY(v 1 ; z 1 , . . . , z r )
if (y = 0) then
u.childrenj ← 0; break

20
21

temp Children ← {z1 , . . . , zm }

else
u.childrenj ← u.childrenj ∪ {y}

22
23
24

if (u.children1 = . . . = u.childrenki ) and (wu (Xi , x1 ) = . . . = wu (Xi , xki )) then
promote wu (Xi , x1 ) to parent
return u.children1
// redundancy

25
26

if (H2 (Xi , u.children1 , . . . , u.childrenki , wu (Xi , x1 ), . . . , wu (Xki , xki )) 6= null) then
return H2 (Xi , u.children1 , . . . , u.childrenki , wu (Xi , x1 ), . . . , wu (Xki , xki ))
// isomorphism

27 Let H1 (v1 , z1 , . . . , zm ) = u
28 Let H2 (Xi , u.children1 , . . . , u.childrenki , w u (Xi , x1 ), . . . , w u (Xki , xki )) = u
29 return u

// add u to H1
// add u to H2

from g expresses a decomposition with respect to T , so all those nodes appear on different branches.
We will employ the usual techniques from OBDDs to make the operation efficient. First, if one of
the arguments is 0, then we can safely return 0. Second, a hash table H1 is used to store the nodes
that have already been processed, based on the nodes (v1 , z1 , . . . , zr ). Therefore, we never need
to make multiple recursive calls on the same arguments. Third, a hash table H2 is used to detect
isomorphic nodes. This is typically split in separate tables for each variable. If at the end of the
recursion, before returning a value, we discover that a meta-node with the same variable, the same
children and the same weights has already been created, then we don’t need to store it and we simply
return the existing node. And fourth, if at the end of the recursion we discover that we created a
redundant node (all the children are the same and all the weights are the same), then we don’t store
it, and return instead one of its identical lists of children, and promote the common weight.
493

M ATEESCU , D ECHTER & M ARINESCU

A
0
0
0
0
1
1
1
1

B
0
0
1
1
0
0
1
1

C
0
1
0
1
0
1
0
1

f(ABC)
0
0
0
1
0
1
0
1

A
0
0
0
0
1
1
1
1

A
B
C

A

A1
0

0

C

A3

*

0

A4

B2

A5

B3

0

B

B

D

D

1

D
1

D

C

A
0

0

0

A

A1B1
B

1

B4

A

1

B
0

1

1

g(ABC)
0
0
0
1
0
1
1
0

A
0

1

0

D
0
1
0
1
0
1
0
1

B1
1

B

A2

B
0
0
1
1
0
0
1
1

0

B6

=

B

A2B2

B7

0

1

C
0

B5

0

A4B6

1

A4

1

1

D

B4
1

0

B
0

1

D

B7
1

0

1

1

Figure 18: Example of APPLY operation
Note that v1 is always an ancestor of all z1 , . . . , zm in T . We consider a variable in T to be an
ancestor of itself. A few self explaining checks are performed in lines 1-4. Line 2 is specific for
multiplication, and needs to be changed for other combination operations. The algorithm creates a
new meta-node u, whose variable is var(v1 ) = Xi – recall that var(v1 ) is highest (closest to root)
in T among v1 , z1 , . . . , zm . Then, for each possible value of Xi , line 7, it starts building its list of
children.
One of the important steps happens in line 15. There are two lists of meta-nodes, one from
each original AOMDD f and g, and we will refer only to their variables, as they appear in T . Each
of these lists has the important property mentioned above, that its nodes are not ancestors of each
other. The union of the two lists is grouped into maximal sets of nodes, such that the highest node
in each set is an ancestor of all the others. It follows that the root node in each set belongs to one of
the original AOMDD, say v 1 is from f , and the others, say z 1 , . . . , z r are from g. As an example,
suppose T is the pseudo tree from Fig. 15(b), and the two lists are {C, G, H} from f and {E, F }
from g. The grouping from line 15 will create {C; E} and {F ; G, H}. Sometimes, it may be the
case that a newly created group contains only one node. This means there is nothing more to join
in recursive calls, so the algorithm will return, via line 4, the single node. From there on, only one
of the input AOMDDs is traversed, and this is important for the complexity of APPLY, discussed
below.
Example 12 Figure 18 shows the result of combining two Boolean functions by an AND operation
(or product). The input functions f and g are represented by AOMDDs based on chain pseudo
trees, while the results is based on the pseudo tree that expresses the decomposition after variables
A and B are instantiated. The APPLY operator performs a depth first traversal of the two input
AOMDDs, and generates the resulting AOMDD based on the output pseudo tree. Similar to the
case of OBDDs, a function or an AOMDD can be identified by its root meta-node. In this example
the input meta-nodes have labels (A1 , A2 , B1 , B2 , etc.). The output meta-node labeled by A2 B2 is
494

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

the root of a diagram that represents the function obtained by combining the functions rooted by A2
and B2 .
6.3 Complexity of APPLY and BE-AOMDD
We now provide a characterization of the complexity of APPLY, based on different criteria. The
following propositions are inspired by the results that govern OBDD apply complexity, but are
adapted for pseudo tree orderings.
An AOMDD along a pseudo tree can be regarded as a union of regular MDDs, each restricted
to a full path from root to a leaf in the pseudo tree. Let πT be such a path in T . Based on the
definition of strictly compatible pseudo trees, πT has corresponding paths πTf in Tf and πTg in Tg .
The MDDs from f and g corresponding to πTf and πTg can be combined using the regular MDD
apply. This process can be repeated for every path πT . The resulting MDDs, one for each path in T
need to be synchronized on their common parts (on the intersection of the paths). The algorithm we
proposed does all this processing at once, in a depth first search traversal over the inputs. Based on
our construction, we can give a first characterization of the complexity of AOMDD APPLY as being
governed by the complexity of MDD apply.
Proposition 2 Let π1 , . . . , πl be the set of paths in T enumerated from left to right and let Gfi and
Ggi be the MDDs restricted to path πi , then the size of the output of AOMDD apply
by
P
P is bounded
i | · |G i | ≤ n · max |G i | · |G i |. The time complexity is also bounded by
i | · |G i | ≤
|G
|G
i
g
g
g
i f
i f
f
n · maxi |Gfi | · |Ggi |.
A second characterization of the complexity can be given, similar to the MDD case, in terms of
total number of nodes of the inputs:
Proposition 3 Given two AOMDDs Gfaomdd and Ggaomdd based on strictly compatible pseudo trees,
the size of the output of APPLY is at most O(| Gfaomdd | · | Ggaomdd |).
We can further detail the previous proposition as follows. Given AOMDDs Gfaomdd and Ggaomdd ,
based on compatible pseudo trees Tf and Tg and the common pseudo tree T , we define the intersection pseudo tree Tf ∩g as being obtained from T by the following two steps: (1) mark all the
subtrees whose nodes belong to either Tf or Tg but not to both (the leaves of each subtree should be
leaves in T ); (2) remove the subtrees marked in step (1) from T . Steps (1) and (2) are applied just
once (that is, not recursively). The part of AOMDD Gfaomdd corresponding to the variables in Tf ∩g
is denoted by Gff ∩g , and similarly for Ggaomdd it is denoted by Ggf ∩g .
Proposition 4 The time complexity of
|Gfaomdd | + |Ggaomdd |).

APPLY

and the size of the output are O(|Gff ∩g | · |Ggf ∩g | +

We now turn to the complexity of the BE-AOMDD algorithm. Each bucket has an associated
bucket pseudo tree. The top chain of the bucket pseudo tree for variable Xi contains all and only
the variables in context(Xi ). For any other variables that appear in the bucket pseudo tree, their
associated buckets have already been processed. The original functions that belong to the bucket
of Xi have their scope included in context(Xi ), and therefore their associated AOMDDs are based
495

M ATEESCU , D ECHTER & M ARINESCU

on chains. Any other functions that appear in bucket of Xi are messages received from independent branches below. Therefore, any two functions in the bucket of Xi only share variables in the
context(Xi ), which forms the top chain of the bucket pseudo tree. We can therefore characterize
the complexity of APPLY in terms of treewidth, or context size of a bucket variable.
Proposition 5 Given two AOMDDs in the same bucket of BE-AOMDD, the time and space complexity of the APPLY between them is at most exponential in the context size of the bucket variable
(namely the number of the variables in the top chain of the bucket pseudo tree).
We can now bound the complexity of BE-AOMDD and the output size:
T HEOREM 5 The space complexity of BE-AOMDD and the size of the output AOMDD are
∗
O(n k w ), where n is the number of variables, k is the maximum domain size and w∗ is the treewidth
∗
of the bucket tree. The time complexity is bounded by O(r k w ), where r is the number of initial
functions.

7. AOMDDs Are Canonical Representations
It is well known that OBDDs are canonical representations of Boolean functions given an ordering
of the variables (Bryant, 1986), namely a strict ordering of any CNF specification of the same
Boolean function will yield an identical OBDD, and this property extends to MDDs (Srinivasan
et al., 1990). The linear ordering of the variables defines a chain pseudo tree that captures the
structure of the OBDD or MDD. In the case of AOBDDs and AOMDDs, the canonicity is with
respect to a pseudo tree, transitioning from total orders (that correspond to a linear ordering) to
partial orders (that correspond to a pseudo tree ordering). On the one hand we gain the ability to have
a more compact compiled structure, but on the other hand canonicity is no longer with respect to
all equivalent graphical models, but only relative to those graphical models that are consistent with
the pseudo tree that is used. Specifically, if we start from a strict ordering we can generate a chain
AOMDD that will be canonical relative to all equivalent graphical models. If however we want to
exploit additional decomposition we can use a partial ordering captured by a pseudo-tree and create
a more compact AOMDD. This AOMDD however is canonical relative to those equivalent graphical
models that can accept the same pseudo tree that guided the AOMDD. In general, AOMDD can be
viewed as a more flexible framework for compilation that allows both partial and total orderings.
Canonicity is restricted to a subset of graphical models whose primal graph agrees with the partial
order but it is relevant to a larger set of orderings which are consistent with the pseudo-tree.
In the following subsection we discuss the canonicity of AOMDD for constraint networks. The
case of general weighted graphical models is discussed in Section 8.
7.1 AOMDDs for Constraint Networks Are Canonical Representations
The case of constraint networks is more straightforward, because the weights on the OR-to-AND
arcs can only be 0 or 1. We will show that any equivalent constraint networks, that admit the same
pseudo tree T , have the same AOMDD based on T . We start with a proposition that will help prove
the main theorem.
Proposition 6 Let f be a function, not always zero, defined by a constraint network over X. Given
a partition {X1 , . . . , Xm } of the set of variables X (namely, Xi ∩ Xj = φ, ∀ i 6= j, and X =
496

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

i
i
∪m
i=1 X ), if f = f1 ⊗ . . . ⊗ fm and f = g1 ⊗ . . . ⊗ gm , such that scope(fi ) = scope(gi ) = X for
all i ∈ {1, . . . , m}, then fi = gi for all i ∈ {1, . . . , m}. Namely, if f can be decomposed over the
given partition, then the decomposition is unique.

We are now ready to show that AOMDDs for constraint networks are canonical representations
given a pseudo tree.
T HEOREM 6 (AOMDDs are canonical for a given pseudo tree) Given a constraint network, and
a pseudo tree T of its constraint graph, there is a unique (up to isomorphism) AOMDD that represents it, and it has the minimal number of meta-nodes.
A constraint network is defined by its relations (or functions). There exist equivalent constraint
networks that are defined by different sets of functions, even having different scope signatures.
However, equivalent constraint networks define the same function, and we can ask if the AOMDD
of different equivalent constraint networks is the same. The following corollary can be derived
immediately from Theorem 6.
Corollary 1 Two equivalent constraint networks that admit the same pseudo tree T have the same
AOMDD based on T .

8. Canonical AOMDDs for Weighted Graphical Models
Theorem 6 ensures that the AOMDD is canonical for constraint networks, namely for functions that
can only take the values 0 or 1. The proof relied on the fact that the OR-to-AND weights can only
be 0 or 1, and on Proposition 6 that ensured the unique decomposition of a function defined by a
constraint network.
In this section we turn to general weighted graphical models. We can first observe that Proposition 6 is no longer valid for general functions. This is because the valid solutions (having strictly
positive weight) can have their weight decomposed in more than one way into a product of positive
weights.
Therefore we raise the issue of recognizing nodes that root AND/OR graphs that represent the
same universal function, even though the graphical representation is different. We will see that the
AOMDD for a weighted graphical model is not unique under the current definitions, but we can
slightly modify them to obtain canonicity again. We have to note that canonicity of AOMDDs for
weighted graphical models (e.g., belief networks) is far less crucial than in the case of OBDDs that
are used in formal verification. Even more than that, sometimes it may be useful not to eliminate
the redundant nodes, in order to maintain a simpler semantics of the AND/OR graph that represents
the model.
The loss of canonicity of AOMDD for weighted graphical models can happen because of the
weights on the OR-to-AND arcs, and we suggest a possible way of re-enforcing it if a more compact
and canonical representation is needed.
Example 13 Figure 19 shows a weighted graphical model, defined by two (cost) functions,
f (M, A, B) and g(M, B, C). Assuming the order (M,A,B,C), Figure 20 shows the AND/OR search
tree on the left. The arcs are labeled with function values, and the leaves show the value of the
corresponding full assignment (which is the product of numbers on the arcs of the path). We can
497

M ATEESCU , D ECHTER & M ARINESCU

A

M
0
0
0
0
1
1
1
1

M
A

M

B
B
C

C

A
0
0
1
1
0
0
1
1

B f(M,A,B)
0
12
1
5
0
18
1
2
0
4
1
10
0
6
1
4

M
0
0
0
0
1
1
1
1

B
0
0
1
1
0
0
1
1

C g(M,B,C)
0
3
1
5
0
14
1
12
0
9
1
15
0
7
1
6

Figure 19: Weighted graphical model
M
0

1

0

1

A

A

A

A

0

1

B

0

B

5

18

2

0

1

0

1

C

C

5

0

1

36 60

14
0

C

3

5

1

0

1

70 60

54 90

14
0

0

B

4

10

0

C

12

1

B

12

3

M

C

B

6

1

0

C

C

9

15

7

6

9

15

7

1

0

1

0

1

0

1

0

36 60

70 60

54 90

0

B

12

5

18

2

1

0

1

0

1

C

6
1

28 24

5

0

1

14
0

B

4
0

C

3

1

B

4

C

12

28 24

1

10

6

4

1

0

1

C

C

12

9

15

7

6

1

0

1

0

1

36 60

Figure 20: AND/OR search tree and context minimal graph

see that either value of M (0 or 1) gives rise to the same function (because the leaves in the two
subtrees have the same values). However, the two subtrees can not be identified as representing the
same function by the usual reduction rules. The right part of the figure shows the context minimal
graph, which has a compact representation of each subtree, but does not share any of their parts.
What we would like in this case is to have a method of recognizing that the left and right subtrees
corresponding to M = 0 and M = 1 represent the same function. We can do this by normalizing
the values in each level, and processing bottom up. In Figure 21 left, the values on the OR-to-AND
arcs have been normalized, for each OR variable, and the normalization constant was promoted
up to the OR value. In Figure 21 right, the normalization constants are promoted upwards again
by multiplication. This process does not change the value of each full assignment, and therefore
produces equivalent graphs.
We can see already that some of the nodes labeled by C can now be merged, producing the graph
in Figure 22 on the left. Continuing the same process we obtain the AOMDD for the weighted graph,
shown in Figure 22 on the right.
We can define the AOMDD of a weighted graphical model as follows:
D EFINITION 26 (AOMDD of weighted graphical model) The AOMDD of a weighted graphical
model is an AND/OR graph, with meta-nodes, such that: (1) for each meta-node, its weights sum to
1; (2) the root meta-node has a constant associated with it; (3) it is completely reduced, namely it
has no isomorphic meta-nodes, and no redundant meta-nodes.

498

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

M
M

0

1

A

A

0

1

B

0

B

B

12

5

18

2

0

1

0

1

8
3/8
0

5/8

14/26

1

0

12/26

3/8

6

4

1

0

1

1

0

14/26

1

0

1

26*5

0

1

3/8

26*2

0

1

1

0

7/13

1

0

B

24*4

13*10

0

24*6

1

C

5/8

1

B

8*18

C

12/26

0

B

8*12

13 C
5/8

A

B

10

24 C

1

A
0

B

4
0

26 C

C

1

0

13*4

0

1

C

6/13

C

3/8

1

5/8

0

7/13

1

0

6/13
1

Figure 21: Normalizing values bottom up
844

M

1/2
0

M

1/2

1
0

A

1

A
A

0

1

B

96
0

0

B

130

144

1

0

1

B

52
1

96
0

B

130

144

1

0

C

3/8
0

1

7/13
0

196/422

0

1

B

52

B

96/226

1

130/226

0

1

C

5/8

226/422

144/196

52/196

0

1

C

6/13

3/8

1

0

C

5/8
1

7/13
0

6/13
1

Figure 22: AOMDD for the weighted graph
The procedure of transforming a weighted AND/OR graph into an AOMDD is very similar to
Procedure B OTTOM U P R EDUCTION from Section 5. The only difference is that when a new layer
is processed, first the meta-node weights are normalized and promoted to the parent, and then the
procedure continues as usual with the reduction rules.
T HEOREM 7 Given two equivalent weighted graphical models that accept a common pseudo tree
T , normalizing arc values together with exhaustive application of reduction rules yields the same
AND/OR graph, which is the AOMDD based on T .
Finite Precision Arithmetic The implementation of the algorithm described in this section may
prove to be challenging on machines that used finite precision arithmetic. Since the weights are
real-valued, the repeated normalization may lead to precision errors. One possible approach, which
we also used in our experiments, is to define some ε-tolerance, for some user defined sufficiently
small ε, and consider the weights to be equal if they are within ε of each other.

9. Semantic Treewidth
A graphical model M represents a universal function F = ⊗fi . The function F may be represented
by different graphical models. Given a particular pseudo tree T , that captures some of the structural
information of F , we are interested in all the graphical models that accept T as a pseudo tree, namely
their primal graphs only contain edges that are backarcs in T . Since the size of the AOMDD for F
based on T is bounded in the worst case by the induced width of the graphical model along T , we
define the semantic treewidth to be:
499

M ATEESCU , D ECHTER & M ARINESCU

1
A

2

3

B
C

C

o
B

o

1

2

A

3

4

o
o

C
D

A

o

D

B

4

o

A B
1 3
1 4
2 4
3 1
4 1
4 2

o
o

(a) The two solutions

A C
1 2
1 4
2 1
2 3
3 2
3 4
4 1
4 3

A D
1 2
1 3
2 1
2 3
2 4
3 1
3 2
3 4
4 2
4 3

D
B C
1 3
1 4
2 4
3 1
4 1
4 2

B D
1 2
1 4
2 1
2 3
3 2
3 4
4 1
4 3

C D
1 3
1 4
2 4
3 1
4 1
4 2

(b) First model

A

A B
2 4
3 1

B

C

B C
1 4
4 1

D

C D
1 3
4 2

(c) Second model

Figure 23: The 4-queen problem
D EFINITION 27 (semantic treewidth) The semantic treewidth of a graphical model M relative
to a pseudo tree T denoted by swT (M), is the smallest treewidth taken over all models R that
are equivalent to M, and accept the pseudo tree T . Formally, it is defined by swT (M) =
minR,u(R)=u(M) wT (R), where u(M) is the universal function of M, and wT (R) is the induced
width of R along T . The semantic treewidth of a graphical model, M, is the minimal semantic
treewidth over all the pseudo trees that can express its universal function.
Computing the semantic treewidth can be shown to be NP-hard.3
T HEOREM 8 Computing the semantic treewidth of a graphical model M is NP-hard.
Theorem 8 shows that computing the semantic treewidth is hard, and it is likely that the actual
complexity is even higher. However, the semantic treewidth can explain why sometimes the minimal
AND/OR graph or OBDD are much smaller than the exponential in treewidth or pathwidth upper
bounds. In many cases, there could be a huge disparity between the treewidth of M and the semantic
treewidth along T .
Example 14 Figure 23(a) shows the two solutions of the 4-queen problem. The problem is expressed by a complete graph of treewidth 3, given in Figure 23(b). Figure 23(c) shows an equivalent
problem (i.e., that has the same set of solutions), which has treewidth 1. The semantic treewidth of
the 4-queen problem is 1.
Based on the fact that an AOMDD is a canonical representation of the universal function of a
graphical model, we can conclude that the size of the AOMDD is bounded exponentially by the
semantic treewidth along the pseudo tree, rather than the treewidth of the given graphical model
representation.
Proposition 7 The size of the AOMDD of a graphical model M is bounded by O(n k swT (M )),
where n is the number of variables, k is the maximum domain size and swT (M) is the semantic
treewidth of M along the pseudo tree T .
3. We thank David Eppstein for the proof.

500

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

A
A

B

B

C

C

B

M

D

D

C

N

D

P

0

1

(a) OBDD representation

(b) Primal graph with hidden
variables M, N and P .

Figure 24: The parity function
Example 15 Consider a constraint network on n variables such that every two variables are constrained by equality (X = Y ). One graph representation is a complete graph, another is a chain
and another is a tree. If the problem is specified as a complete graph, and if we use a linear order,
the OBDD will have a linear size because there exists a representation having a pathwidth of 1
(rather than n).
While the semantic treewidth can yield a much better upper bound on the AOMDD, it can also
be a very bad bound. It is well known that the parity function on n variables has a very compact,
chain-like OBDD representation. Yet, the only constraint network representation of a parity function
is the function itself (namely a complete graph on all the variables), whose treewidth and semantic
treewidth is its number of variables, n. The OBDD representation of the parity function suggests
that the addition of hidden variables can simplify its presentation. We show an example in Figure
24. On the left side, in Figure 24(a) we have the OBDD representation of the parity function for
four binary variables. A graphical model would represent this function by a complete graph on the
four variables. However, we could add the extra variables M, N and P in Figure 24(b), sometimes
called “hidden” variables, that can help decompose the model. In this case M can form a constraint
together with A and B such that M represents the parity of A and B, namely M = 1 if A ⊕ B = 1,
where ⊕ is the parity (XOR) operator. Similarly, N would capture the parity of M and C, and P
would capture the parity of N and D, and would also give the parity of the initial four variables.
The two structures are surprisingly similar. It would be interesting to study further the connection
between hidden variables and compact AOBDDs, but we leave this for future work.

10. Experimental Evaluation
Our experimental evaluation is in preliminary stages, but the results we have are already encouraging. We ran the search-based compile algorithm, by recording the trace of the AND/OR search, and
then reducing the resulting AND/OR graph bottom up. In these results we only applied the reduction by isomorphism and still kept the redundant meta-nodes. We implemented our algorithms in
C++ and ran all experiments on a 2.2GHz Intel Core 2 Duo with 2GB of RAM, running Windows.

501

M ATEESCU , D ECHTER & M ARINESCU

10.1 Benchmarks
We tested the performance of the search-based compilation algorithm on random Bayesian networks, instances from the Bayesian Network Repository and a subset of networks from the UAI’06
Inference Evaluation Dataset.
Random Bayesian Networks The random Bayesian networks were generated using parameters
(n, k, c, p), where n is the number of variables, k is the domain size, c is the number of conditional
probability tables (CPTs) and p is the number of parents in each CPT. The structure of the network
was created by randomly picking c variables out of n and, for each, randomly picking p parents from
their preceding variables, relative to some ordering. The remaining n − c variables are called root
nodes. The entries of each probability table were generated randomly using a uniform distribution,
and the table was then normalized. It is also possible to control the amount of determinism in the
network by forcing a percentage det of the CPTs to have only 0 and 1 entries.
Bayesian Network Repository The Bayesian Network Repository4 contains a collection of belief
networks extracted from various real-life domains which are often used for benchmarking probabilistic inference algorithms.
UAI’06 Inference Evaluation Dataset The UAI 2006 Inference Evaluation Dataset5 contains a
collection of random as well as real-world belief networks that were used during the first UAI 2006
Inference Evaluation contest. For our purpose we selected a subset of networks which were derived
from the ISCAS’89 digital circuits benchmark.6 ISCAS’89 circuits are a common benchmark used
in formal verification and diagnosis. Each of these circuits was converted into a Bayesian network
by removing flip-flops and buffers in a standard way, creating a deterministic conditional probability
table for each gate, and putting uniform distributions on the input signals.
10.2 Algorithms
We consider two search-based compilation algorithms, denoted by AOMDD-BCP and AOMDDSAT, respectively, that reduce the context minimal AND/OR graph explored via isomorphism, while
exploiting the determinism (if any) present in the network. The approach we take for handling the
determinism is based on unit resolution over a CNF encoding (i.e., propositional clauses) of the zero
probability tuples of the CPTs. The idea of using unit resolution during search for Bayesian networks was first explored by Allen and Darwiche (2003). AOMDD-BCP is conservative and applies
only unit resolution at each node in the search graph, whereas AOMDD-SAT is more aggressive and
detects inconsistency by running a full SAT solver. We used the zChaff SAT solver (Moskewicz,
Madigan, Zhao, Zhang, & Malik, 2001) for both unit resolution as well as full satisfiability. For
comparison, we also ran an OR version of AOMDD-BCP, called MDD-BCP.
For reference we also report results obtained with the ACE7 compiler. ACE compiles a Bayesian
network into an Arithmetic Circuit (AC) and then uses the AC to answer multiple queries with respect to the network. An arithmetic circuit is a representation that is equivalent to AND/OR graphs
(Mateescu & Dechter, 2007). Each time ACE compiler is invoked, it uses one of two algorithms
as the basis for compilation. First, if an elimination order can be generated for the network having
4.
5.
6.
7.

http://www.cs.huji.ac.il/compbio/Repository/
http://ssli.ee.washington.edu/bilmes/uai06InferenceEvaluation
Available at: http://www.fm.vslib.cz/kes/asic/iscas/
Available at: http://reasoning.cs.ucla.edu/ace

502

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

Network

(w*, h)

(n, k)

ACE
#nodes time

MDD w/ BCP
AOMDD w/ BCP
AOMDD w/ SAT
#meta #cm(OR) time
#meta #cm(OR)
time #meta #cm(OR)
time
Bayesian Network Repository
alarm
(4, 13) (37, 4)
1,511 0.01 208,837 682,195 73.35
320
459
0.05
320
459
0.22
cpcs54
(14, 23) (54, 2)
196,933 0.06
- 65,158 66,405
6.97 65,158 66,405
6.97
cpcs179
(8, 14) (179, 4)
67,919 0.05
9,990 32,185 46.56 9,990 32,185 46.56
cpcs360b (20, 27) (360, 2) 5,258,826 1.72
diabetes
(4, 77) (413, 21) 7,615,989 1.81
hailfinder (4, 16) (56, 11)
8,815 0.01
2,068
2,202
0.34 1,893
2,202
1.48
mildew
(4, 13) (35, 100) 823,913 0.39
- 73,666 110,284 1367.81 62,903 65,599 3776.82
mm
(20, 57) (1220, 2)
47,171 1.49
- 38,414 58,144
4.54 30,274 52,523 99.55
munin2
(9, 32) (1003, 21) 2,128,147 1.91
munin3
(9, 32) (1041, 21) 1,226,635 1.27
munin4
(9, 32) (1044, 21) 2,423,009 4.44
pathfinder (6, 11) (109, 63)
18,250 0.05 610,854 1,303,682 352.18
6,984 16,267 30.71 2,265 15,963 50.36
pigs
(11, 26) (441, 3) 636,684 0.19
- 261,920 294,101 174.29 198,284 294,101 1277.72
water
(10, 15) (32, 4)
59,642 0.52 707,283 1,138,096 95.14 18,744 20,926
2.02 18,503 19,225
7.45
UAI’06 Evaluation Dataset
BN 42
(21, 62) (851, 2)
4,860 1.35
- 107,025 341,428 53.50 42,445 43,280 57.36
BN 43
(26, 65) (851, 2)
10,373 1.62
- 1,343,923 1,679,013 1807.63 313,388 314,669 434.38
BN 44
(25, 56) (851, 2)
4,235 1.31
- 155,588 187,589 20.90 47,222 48,540 66.09
BN 45
(22, 54) (851, 2)
12,319 1.50
- 390,795 487,593 68.81 126,182 126,929 177.50
BN 46
(20, 46) (851, 2)
5,912 2.90 1,125,658 1,228,332 94.93 16,711 17,532
1.31 7,337
7,513
5.54
BN 47
(39, 57) (632, 2)
1,448 1.17 42,419 47,128 2.87
1,873
2,663
0.24 1,303
2,614
2.36
BN 49
(40, 60) (632, 2)
1,408 1.16 18,344 19,251 1.32
1,205
1,539
0.19
952
1,515
1.34
BN 51
(41, 68) (632, 2)
1,467 1.15 63,851 68,005 4.22
4,442
5,267
0.50 3,653
5,195
4.58
BN 53
(47, 87) (532, 2)
1,357 0.91 14,210 19,162 1.49
4,819
9,561
0.74 1,365
1,719
1.36
BN 55
(49, 92) (532, 2)
1,288 0.93
5,168
6,088 0.57
1,972
2,816
0.26
790
904
0.75
BN 57
(49, 85) (532, 2)
1,276 0.90 48,436 51,611 3.52
4,036
5,089
0.37
962
1,277
1.01
BN 59
(52, 87) (511, 2)
1,749 0.93 332,030 353,720 25.61 22,963 29,146
2.14 10,655 18,752 14.17
BN 61
(41, 64) (638, 2)
1,411 1.10 20,459 20,806 1.45
1,244
1,589
0.17 1,016
1,528
1.37
BN 63
(53, 95) (511, 2)
1,324 0.90 11,461 17,087 1.28
7,182 14,048
1.07 1,419
2,177
1.69
BN 65
(56, 86) (411, 2)
1,184 0.75
- 20,764 23,102
1.52 12,569 19,778 12.90
BN 67
(54, 88) (411, 2)
1,031 0.74
- 179,067 511,031 154.91
716
1,169
0.78
Positive Random Bayesian Networks (n=75, k=2, p=2, c=65)
r75-1
(12, 22) (75, 2)
67,737 0.31
- 21,619 21,619
2.59 21,619 21,619
2.59
r75-2
(12, 23) (75, 2)
46,703 0.29
- 18,083 18,083
1.88 18,083 18,083
1.88
r75-3
(11, 26) (75, 2)
53,245 0.30
- 18,419 18,419
1.86 18,419 18,419
1.86
r75-4
(11, 19) (75, 2)
28,507 0.29
8,363
8,363
1.16 8,363
8,363
1.16
r75-5
(13, 24) (75, 2)
149,707 0.36
- 42,459 42,459
4.61 42,459 42,459
4.61
r75-6
(14, 24) (75, 2)
132,107 1.19
- 62,621 62,621
6.95 62,621 62,621
6.95
r75-7
(12, 24) (75, 2)
89,913 0.36
- 21,583 21,583
2.42 21,583 21,583
2.42
r75-8
(14, 24) (75, 2)
86,183 0.36
- 49,001 49,001
6.23 49,001 49,001
6.23
r75-9
(11, 19) (75, 2)
29,025 0.30
7,681
7,681
0.81 7,681
7,681
0.81
r75-10
(10, 24) (75, 2)
20,291 0.28
5,905
5,905
0.63 5,905
5,905
0.63
Deterministic Random Bayesian Networks (n=100, k=2, p=2, c=90) and det = 25% of the CPTs containing only 0 and 1 entries
r100d25-1 (13, 31) (100, 2)
68,398 0.38
- 34,035 34,075
2.94 34,035 34,075 12.77
r100d25-2 (16, 28) (100, 2) 150,134 0.46
- 70,241 70,931
7.72 70,241 70,931 27.17
r100d25-3 (16, 29) (100, 2) 705,200 0.96
- 134,079 135,203 13.80 134,079 135,203 50.51
r100d25-4 (16, 31) (100, 2) 161,902 0.54
- 79,366 79,488
7.26 79,366 79,488 28.06
r100d25-5 (16, 29) (100, 2) 185,348 0.53
- 140,627 140,636 14.57 140,627 140,636 49.42
r100d25-6 (18, 28) (100, 2) 148,835 0.66
- 204,232 210,066 17.56 197,134 210,066 92.24
r100d25-7 (16, 29) (100, 2) 264,629 0.60
- 134,344 135,008 14.26 133,850 135,008 55.60
r100d25-8 (17, 27) (100, 2)
65,186 0.46
- 36,857 36,887
2.95 36,857 36,887 11.97
r100d25-9 (14, 27) (100, 2) 140,014 0.40
- 58,421 59,791
6.88 58,172 59,791 23.21
r100d25-10 (16, 27) (100, 2) 173,808 0.58
- 69,110 69,136
7.50 69,110 69,136 26.50

Table 1: Results for experiments with 50 Bayesian networks from 3 problem classes; w∗ =
treewidth, h = depth of pseudo tree, n = number of variables, k = domain size, time
given in seconds; bold types highlight the best results across rows.

503

M ATEESCU , D ECHTER & M ARINESCU

sufficiently small induced width, then tabular variable elimination will be used as the basis. This
algorithm is similar to the one discussed by Chavira and Darwiche (2007), but uses tables to represent factors rather than ADDs. If the induced width is large, then logical model counting will be
used as the basis. Tabular variable elimination is typically efficient when width is small but cannot
handle networks when the width is larger. Logical model counting, on the other hand, incurs more
overhead than tabular variable elimination, but can handle many networks having larger treewidth.
Both tabular variable elimination and logical model counting produce ACs that exploit local structure, leading to efficient online inference. When logical model counting is invoked, it proceeds
by encoding the Bayesian network into a CNF (Chavira & Darwiche, 2005; Chavira, Darwiche, &
Jaeger, 2006), simplifying the CNF, compiling the CNF into a d-DNNF, and then extracting the AC
from the compiled d-DNNF. A dtree over the CNF clauses drives the compilation step.
In all our experiments we report the compilation time in seconds (time), the number of OR
nodes in the context minimal graph explored (#cm), the number of meta-nodes of the resulting
AOMDD (#meta), as well as the size of the AC compiled by ACE (#nodes). For each network we
specify the number of variables (n), domain size (k), induced width (w∗ ) and pseudo tree depth (h).
A ’-’ stands for exceeding the 2GB memory limit by the respective algorithm. The best performance
points are highlighted.
10.3 Evaluation on Bayesian Networks
Table 1 reports the results obtained for experiments with 50 Bayesian networks. The AOMDD
compilers as well as ACE used the min-fill heuristic (Kjæaerulff, 1990) to construct the guiding
pseudo tree and dtree, respectively.
10.3.1 BAYESIAN N ETWORKS R EPOSITORY
We see that ACE is overall the fastest compiler on this domain, outperforming both AOMDD-BCP
and AOMDD-SAT with up to several orders of magnitude (e.g., mildew, pigs). However, the
diagrams compiled by ACE and AOMDD-BCP (resp. AOMDD-SAT) are comparable in size. In
some cases, AOMDD-BCP and AOMDD-SAT were able to compile much smaller diagrams than
ACE. For example, the diagram produced by AOMDD-BCP for the mildew network is 13 times
smaller than the one compiled by ACE. In principle the output produced by ACE and AOMDD
should be similar if both are guided by the same pseudo tree/dtree. Our scheme should be viewed
as a compilation alternative which (1) extends decision diagrams and (2) mimics traces of search
properties that may make this representation accessible. The OR compiler MDD-BCP was able
to compile only 3 out of the 14 test instances, but their sizes were far larger than those produced
by AOMDD-BCP. For instance, on the pathfinder network, AOMDD-BCP outputs a decision
diagram almost 2 orders of magnitude smaller than MDD-BCP.
10.3.2 UAI’06 DATASET
For each of the UAI’06 Dataset instances we picked randomly 30 variables and instantiated as
evidence. We see that ACE is the best performing compiler on this dataset. AOMDD-BCP is
competitive with ACE in terms of compile time only on 9 out the 16 test instances. AOMDD-SAT
is able to compile the smallest diagrams for 6 networks only (e.g., BN 47, BN 49, BN 55, BN 57,
BN 61, BN 67). As before, the difference in size between the compiled data-structures produces
by MDD-BCP and AOMDD-BCP is up to 2 orders of magnitude in favor of the latter.
504

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

10.3.3 R ANDOM N ETWORKS
The problem instances denoted by r75-1 through r75-10 were generated from a class of random
belief networks with parameters (n = 75, k = 2, p = 2, c = 65). Similarly, the instances denoted
by r100d25-1 through r100d25-10 belong to a class with parameters (n = 100, k = 2, p = 2, c =
90). In the latter case, det = 25% of the CPTs are deterministic, namely they contain only 0 and
1 probability tuples. These test instances were compiled without any evidence. We see that on this
domain AOMDD-BCP/AOMDD-SAT were able to compile the smallest diagrams, which were on
average about 2 times smaller than those produced by ACE. However, ACE was again the fastest
compiler. Notice that the OR compiler MDD-BCP ran out of memory in all test cases.
10.4 The Impact of Variable Ordering
As theory dictates, the AOMDD size is influenced by the quality of the guiding pseudo tree. In
addition to the min-fill heuristic we also considered the hypergraph heuristic which constructs the
pseudo tree by recursively decomposing the dual hypergraph associated with the graphical model.
This idea was also explored by Darwiche (2001) for constructing dtrees that guide ACE.
Since both the min-fill and hypergraph partitioning heuristics are randomized (namely ties are
broken randomly), the size of the AOMDD guided by the resulting pseudo tree may vary significantly from one run to the next. Figure 25 displays the AOMDD size using hypergraph and min-fill
based pseudo trees for 6 networks selected from Table 1, over 20 independent runs. We also record
the average induced width and depth obtained for the pseudo trees (see the header of each plot in
Figure 25). We see that the two heuristics do not dominate each other, namely the variance in output
size is quite significant in both cases.
10.5 Memory Usage
Table 2 shows the memory usage (in MBytes) of ACE, AOMDD-BCP and AOMDD-SAT, respectively, on the Bayesian networks from Table 1. We see that in some cases the AOMDD based compilers require far less memory than ACE. For example, on the “mildew” network, both AOMDDBCP and AOMDD-SAT use about 22 MB of memory to compile the AND/OR decision diagram,
while ACE requires as much as 218 MB of memory. Moreover, the compiled AOMDD has in this
case about one order of magnitude fewer nodes than that constructed by ACE. When comparing the
two AND/OR search-based compilers, we observe that on networks with a significant amount of
determinism, such as those from the UAI’06 Evaluation dataset, AOMDD-SAT uses on average two
times less memory than AOMDD-BCP. The most dramatic savings in memory usage due to the aggressive constraint propagation employed by AOMDD-SAT compared with AOMDD-BCP can be
seen on the “BN 67” network. In this case, the difference in memory usage between AOMDD-SAT
and AOMDD-BCP is about 2 orders of magnitude in favor of the former.

11. Related Work
The related work can be viewed along two directions: (1) the work related to the AND/OR search
idea for graphical models and (2) the work related to compilation for graphical models that exploits
problem structure.
An extensive discussion for (1) was provided in the previous work of Dechter and Mateescu
(2007). Since this is not the focus of the paper, we just mention that the AND/OR idea was origi505

M ATEESCU , D ECHTER & M ARINESCU

Figure 25: Effect of variable ordering.

506

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

Network

ACE
AOMDD w/ BCP
AOMDD w/ SAT
#nodes memory (MB) #nodes memory (MB) #nodes memory (MB)
Bayesian Network Repository
alarm
1,511
0.00
320
0.0206
320
0.0206
cpcs54
196,933
4.00 65,158
3.4415 65,158
3.4415
cpcs179
67,919
5.00
9,990
1.9263 9,990
1.9263
cpcs360b 5,258,826
204.00
diabetes
7,615,989
449.00
hailfinder
8,815
0.00
2,068
0.1576 1,893
0.1740
mildew
823,913
218.00 73,666
22.5781 62,903
22.1467
mm
47,171
369.00 38,414
1.5719 30,274
1.3711
munin2
2,128,147
202.00
munin3
1,226,635
150.00
munin4
2,423,009
n/a
pathfinder
18,250
10.00
6,984
0.6009 2,265
0.3515
pigs
636,684
31.00 261,920
23.3761 198,284
17.7096
water
59,642
161.00 18,744
1.09578 18,503
1.3258
UAI’06 Evaluation Dataset
BN 42
4,860
n/a 107,025
4.5622 42,445
1.9323
BN 43
10,373
n/a 1,343,923
57.8422 313,388
14.2828
BN 44
4,235
n/a 155,588
6.5613 47,222
2.1628
BN 45
12,319
n/a 390,795
17.9325 126,182
5.7958
BN 46
5,912
n/a
16,711
0.6929 7,337
0.3401
BN 47
1,448
n/a
1,873
0.0720 1,303
0.0583
BN 49
1,408
n/a
1,205
0.0449
952
0.0409
BN 51
1,467
n/a
4,442
0.1689 3,653
0.1633
BN 53
1,357
n/a
4,819
0.1814 1,365
0.0587
BN 55
1,288
n/a
1,972
0.0723
790
0.0336
BN 57
1,276
n/a
4,036
0.1495
962
0.0411
BN 59
1,749
n/a
22,963
0.8501 10,655
0.4587
BN 61
1,411
n/a
1,244
0.0463 1,016
0.0445
BN 63
1,324
n/a
7,182
0.2728 1,419
0.0607
BN 65
1,184
n/a
20,764
0.7539 12,569
0.5384
BN 67
1,031
n/a 179,067
6.9603
716
0.0304
Positive Random Bayesian Networks with parameters (n=75, k=2, p=2, c=65)
r75-1
67,737
1.00 21,619
1.2503 21,619
1.2503
r75-2
46,703
1.00 18,083
0.9957 18,083
0.9957
r75-3
53,245
1.00 18,419
0.9955 18,419
0.9955
r75-4
28,507
1.00
8,363
0.5171 8,363
0.5171
r75-5
149,707
3.00 42,459
2.3299 42,459
2.3299
r75-6
132,107
3.00 62,621
3.4330 62,621
3.4330
r75-7
89,913
2.00 21,583
1.1942 21,583
1.1942
r75-8
86,183
2.00 49,001
2.8130 49,001
2.8130
r75-9
29,025
1.00
7,681
0.4124 7,681
0.4124
r75-10
20,291
1.00
5,905
0.3261 5,905
0.3261
Deterministic Random Bayesian Networks with parameters (n=100, k=2, p=2, c=90)
r100d25-1
68,398
5.00 34,035
1.6290 34,035
1.7149
r100d25-2 150,134
10.00 70,241
3.6129 70,241
3.7810
r100d25-3 705,200
40.00 134,079
6.6372 134,079
6.9873
r100d25-4 161,902
22.00 79,366
3.8113 79,366
4.0079
r100d25-5 185,348
15.00 140,627
7.0839 140,627
7.4660
r100d25-6 148,835
37.00 204,232
9.1757 197,134
9.6542
r100d25-7 264,629
19.00 134,344
6.9619 133,850
6.9961
r100d25-8
65,186
21.00 36,857
1.6872 36,857
1.8278
r100d25-9 140,014
6.00 58,421
3.1058 58,172
3.2055
r100d25-10 173,808
27.00 69,110
3.5578 69,110
3.6636

Table 2: Memory usage in MBytes of ACE, AOMDD-BCP and AOMDD-SAT on the 50 Bayesian
networks from Table 1. Bold types highlight the best performance across rows. The “n/a”
indicates that the respective memory usage statistic was not available from ACE’s output.

507

M ATEESCU , D ECHTER & M ARINESCU

nally developed for heuristic search (Nilsson, 1980). As mentioned in the introduction, the AND/OR
search for graphical models is based on a pseudo tree that spans the graph of the model, similar to
the tree rearrangement of Freuder and Quinn (1985, 1987). The idea was adapted for distributed
constraint satisfaction by Collin et al. (1991, 1999) and more recently by Modi et al. (2005), and was
also shown to be related to graph-based backjumping (Dechter, 1992). This work was extended by
Bayardo and Miranker (1996), Bayardo and Schrag (1997) and more recently applied to optimization tasks by Larrosa et al. (2002). Another version that can be viewed as exploring the AND/OR
graphs was presented recently for constraint satisfaction (Terrioux & Jégou, 2003b) and for optimization (Terrioux & Jégou, 2003a). Similar principles were introduced recently for probabilistic
inference, in algorithm Recursive Conditioning (Darwiche, 2001) as well as in Value Elimination
(Bacchus et al., 2003b, 2003a), and are currently at the core of the most advanced SAT solvers (Sang
et al., 2004).
For direction (2), there are various lines of related research. The formal verification literature,
beginning with the work of Bryant (1986) contains a very large number of papers dedicated to the
study of BDDs. However, BDDs are in fact OR structures (the underlying pseudo tree is a chain)
and do not take advantage of the problem decomposition in an explicit way. The complexity bounds
for OBDDs are based on pathwidth rather than treewidth.
As noted earlier, the work of Bertacco and Damiani (1997) on Disjoint Support Decomposition
(DSD) is related to AND/OR BDDs in various ways. The main common aspect is that both approaches show how structure decomposition can be exploited in a BDD-like representation. DSD
is focused on Boolean functions and can exploit more refined structural information that is inherent to Boolean functions. In contrast, AND/OR BDDs assume only the structure conveyed in the
constraint graph, and are therefore more broadly applicable to any constraint expression and also
to graphical models in general. They allow a simpler and higher level exposition that yields graphbased bounds on the overall size of the generated AOMDD. The full relationship between these two
formalisms should be studied further.
McMillan (1994) introduced the BDD trees, along with the operations for combining them. For
2w
circuits of bounded tree width, BDD trees have a linear space upper bound of O(|g|2w2 ), where
|g| is the size of the circuit g (typically linear in the number of variables) and w is the treewidth. This
bound hides some very large constants to claim the linear dependence on |g| when w is bounded.
However, McMillan maintains that when the input function is a CNF expression BDD-trees have
the same bounds as AND/OR BDDs, namely they are exponential in the treewidth only.
To sketch just a short comparison between McMillan’s BDD trees and AOMMDs, consider an
example where we have a simple pseudo tree with root α, left child β and right child γ. Each of
these nodes may stand for a set of variables. In BDD trees, the assignments to β are grouped into
equivalence classes according to the cofactors generated by them on the remaining α and γ. For
example assignments β1 and β2 are equivalent if they generate the same function on α and γ. The
node β can be represented by a BDD whose leaves are the cofactors. The same is done for γ. The
node α is then represented by a matrix of BDDs, where each column corresponds to a cofactor of β
and each line to a cofactor of γ. By contrast, an AOMDD represents the node α as a BDD whose
leaves are the cofactors (the number of distinct functions on β and γ) and then each cofactor is the
root of a decomposition (an AND node) between β and γ. Moreover, the representations of β (as
descendants of different cofactor of α) are shared as much as possible and the same goes for γ. This
is only a high level description, that becomes slightly more complicated when redundant nodes are
eliminated, but the idea remains the same.
508

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

The AND/OR structure restricted to propositional theories is very similar to deterministic decomposable negation normal form (d-DNNF) (Darwiche & Marquis, 2002; Darwiche, 2002). More
recently, Huang and Darwiche (2005b) used the trace of the DPLL algorithm to generate an OBDD,
and compared with the typical formal verification approach of combining the OBDDs of the input
function according to some schedule. The structures that were investigated in that case are still OR.
This idea is extended in our present work by the AND/OR search compilation algorithm.
McAllester, Collins, and Pereira (2004) introduced the case factor diagrams (CFD), which subsume Markov random fields of bounded tree width and probabilistic context free grammars (PCFG).
CFDs are very much related to the AND/OR graphs. The CFDs target the minimal representation,
by exploiting decomposition (similar to AND nodes) but also by exploiting context sensitive information and allowing dynamic ordering of variables based on context. CFDs do not eliminate the
redundant nodes, and part of the cause is that they use zero suppression. There is no claim about
CFDs being canonical forms, and also no description of how to combine two CFDs.
There are numerous variants of decision diagrams that are designed to represent integer-valued
or real-valued functions. For a comprehensive view we refer the reader to the survey of Drechsler
and Sieling (2001). Algebraic decision diagrams (ADDs) (Bahar et al., 1993) provide a compilation for general real-valued rather than Boolean functions. Their main drawback is that their
size increases very fast if the number of terminals becomes large. There are several approaches
that try to alleviate this problem. However the structure that they capture is still OR, and they
do not exploit decomposition. Some alternatives introduce edge values (or weights) that enable
more subgraph sharing. Edge-valued binary decision diagrams (EVBDDs) (Lai & Sastry, 1992)
use additive weights, and when multiplicative weights are also allowed they are called factored
EVBDDs (FEVBDDs) (Tafertshofer & Pedram, 1997). Another type of BDDs called K*BMDs
(Drechsler, Becker, & Ruppertz, 1996) also use integer weights, both additive and multiplicative
in parallel. ADDs have also been extended to affine ADDs (Sanner & McAllester, 2005), through
affine transformations that can achieve more compression. The result was shown to be beneficial
for probabilistic inference algorithms, such as tree clustering, but they still do not exploit the AND
structure.
More recently, independently and in parallel to our work on AND/OR graphs (Dechter & Mateescu, 2004a, 2004b), Fargier and Vilarem (2004) and Fargier and Marquis (2006, 2007) proposed the compilation of CSPs into tree-driven automata, which have many similarities to our work.
Their main focus is the transition from linear automata to tree automata (similar to that from OR
to AND/OR), and the possible savings for tree-structured networks and hyper-trees of constraints
due to decomposition. Their compilation approach is guided by a tree-decomposition while ours is
guided by a variable-elimination based algorithms. And it is well known that Bucket Elimination
and cluster-tree decomposition are in principle the same (Dechter & Pearl, 1989).
Wilson (2005) extended OBDDs to semi-ring BDDs. The semi-ring treatment is restricted to
the OR search spaces, but allows dynamic variable ordering. It is otherwise very similar in aim and
scope to our AOMDD. When restricting the AOMDD to OR graphs only, the two are closely related,
except that we express BDDs using the Shenoy-Shafer axiomatization that is centered on the two
operation of combination and marginalization rather then on the semi-ring formulation. Minimality
in the formulation of Wilson (2005) is more general allowing merging nodes having different values
and therefore it can capture symmetries (called interchangeability).
Another framework very similar to AOMDDs, that we became aware of only recently, is Probabilistic Decision Graphs (PDG) of Jaeger (2004). This work preceded most of the relevant work
509

M ATEESCU , D ECHTER & M ARINESCU

we discussed above (Fargier & Vilarem, 2004; Wilson, 2005) and went somewhat unnoticed, perhaps due to notational and cultural differences. It is however similar in motivation, framework and
proposed algorithms. We believe our AND/OR framework is more accessible. We define the framework over multi-valued domains, provide greater details in algorithms and complexity analysis,
make an explicit connection with search frameworks, fully address the issues of canonicity as well
as provide an empirical demonstration. In particular, the claim of canonicity for PDGs is similar to
the one we make for AOMDDs of weighted models, in that it is relative to the trees (or forests) that
can represent the given probability distribution.
There is another line of research by Drechsler and his group (e.g. Zuzek, Drechsler, & Thornton,
2000), who use AND/OR graphs for Boolean function representation, that may seem similar to our
approach. However, the semantics and purpose of their AND/OR graphs are different. They are
constructed based on the technique of recursive learning and are used to perform Boolean reasoning,
i.e. to explore the logic consequences of a given assumption based on the structure of the circuit,
especially to derive sets of implicants. The meaning of AND and OR in their case is related to
the meaning of the gates/functions, while in our case the meaning is not related to the semantic of
the functions. The AND/OR enumeration tree that results from a circuit according to Zuzek et al.
(2000) is not related to the AND/OR decomposition that we discuss.

12. Conclusion
We propose the AND/OR multi-valued decision diagram (AOMDD), which emerges from the study
of AND/OR search spaces for graphical models (Dechter & Mateescu, 2004a, 2004b; Mateescu &
Dechter, 2005; Dechter & Mateescu, 2007) and ordered binary decision diagrams (OBDDs) (Bryant,
1986). This data-structure can be used to compile any graphical model.
Graphical models algorithms that are search-based and compiled data-structures such as BDDs
differ primarily by their choices of time vs. memory. When we move from regular OR search
space to an AND/OR search space the spectrum of algorithms available is improved for all time
vs. memory decisions. We believe that the AND/OR search space clarifies the available choices
and helps guide the user into making an informed selection of the algorithm that would fit best the
particular query asked, the specific input function and the available computational resources.
The contribution of our work is: (1) We formally describe the AOMDD and prove that it is a
canonical representation of a constraint network. (2) We extend the AOMDD to general weighted
graphical models. (3) We give a compilation algorithm based on AND/OR search, that saves the
trace of a memory intensive search (the context minimal AND/OR graph), and then reduces it
in one bottom up pass. (4) We describe the APPLY operator that combines two AOMDDs by an
operation and show that its complexity is quadratic in the input, but never worse than exponential
in the treewidth. (5) We give a scheduling order for building the AOMDD of a graphical model
starting with the AOMDDs of its functions which is based on a Variable Elimination algorithm.
This guarantees that the complexity is at most exponential in the induced width (treewidth) along the
ordering. (6) We show how AOMDDs relate to various earlier and recent compilation frameworks,
providing a unifying perspective for all these methods. (7) We introduce the semantic treewidth,
which helps explain why compiled decision diagrams are often much smaller than the worst case
bound. Finally, (8) we provide a preliminary empirical demonstration of the power of the current
scheme.

510

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

Acknowledgments
This work was done while Robert Mateescu and Radu Marinescu were at the University of California, Irvine. The authors would like to thank the anonymous reviewers for their constructive
suggestions to improve the paper, David Eppstein for a useful discussion of complexity issues, and
Lars Otten and Natasha Flerova for comments on the final version of the manuscript. This work was
supported by the NSF grants IIS-0412854 and IIS-0713118, and the initial part by the Radcliffe fellowship 2005-2006 (through the partner program), with Harvard undergraduate student John Cobb.

Appendix
Proof of Proposition 1
Consider the level of variable Xi , and the meta-nodes in the list LXi . After one pass through the
meta-nodes in LXi (the inner for loop), there can be no two meta-nodes at the level of Xi in the
AND/OR graph that are isomorphic, because they would have been merged in line 6. Also, during
the same pass through the meta-nodes in LXi all the redundant meta-nodes in LXi are eliminated
in line 8. Processing the meta-nodes in the level of Xi will not create new redundant or isomorphic
meta-nodes in the levels that have been processed before. It follows that the resulting AND/OR
graph is completely reduced. 2
Proof of Theorem 4
The bound on the size follows directly from Theorem 3. The AOMDD size can only be smaller than
∗
the size of the context minimal AND/OR graph, which is bounded by O(n k wT (G) ). To prove the
time bound, we have to rely on the use of the hash table, and the assumption that an efficient implementation allows an access time that is constant. The time bound of AND/OR-S EARCH -AOMDD
∗
is O(n k wT (G) ), from Theorem 3, because it takes time linear in the output (we assume here that
no constraint propagation is performed during search). Procedure B OTTOM U P R EDUCTION (procedure 1) takes time linear in the size of the context minimal AND/OR graph. Therefore, the AOMDD
∗
can be computed in time O(n k wT (G) ), and the result is the same for the algorithm that performs
the reduction during the search. 2
Proof of Proposition 2
The complexity of OBDD (and MDD) apply is known to be quadratic in the input. Namely, the
number of nodes in the output is at most the product of number of nodes in the input. Therefore, the
number of nodes that can appear along one path in the output AOMDD can be at most the product
of the number of nodes in each input, along the same path, |Gfi | · |Ggi |. Summing over all the paths
in T gives the result. 2
Proof of Proposition 3
The argument is identical to the case of MDDs. The recursive calls in APPLY lead to combinations
of one node from Gfaomdd and one node from Ggaomdd (rather than a list of nodes). The number of
total possible such combinations is O(| Gfaomdd | · | Ggaomdd |). 2
Proof of Proposition 4
The recursive calls of APPLY can generate one meta-node in the output for each combination of
511

M ATEESCU , D ECHTER & M ARINESCU

nodes from Gff ∩g and Ggf ∩g . Let’s look at combinations of nodes from Gff ∩g and Ggaomdd \ Ggf ∩g .
The meta-nodes from Ggaomdd \ Ggf ∩g that can participate in such combinations (let’s call this set A)
are only those from levels (of variables) right below Tf ∩g . This is because of the mechanics of the
recursive calls in APPLY. Whenever a node from f that belongs to Gff ∩g is combined with a node
from g that belongs to A, line 15 of APPLY expands the node from f , and the node (or nodes) from
A remain the same. This will happen until there are no more nodes from f that can be combined
with the node (or nodes) from A, and at that point APPLY will simply copy the remaining portion of
its output from Ggaomdd . The size of A is therefore proportional to | Ggf ∩g | (because it is the layer
of metanodes immediately below Ggf ∩g ). A similar argument is valid for the symmetrical case. And
there are no combinations between nodes in Ggaomdd \ Ggf ∩g and Ggaomdd \ Ggf ∩g . The bound follows
from all these arguments. 2
Proof of Proposition 5
The APPLY operation works by constructing the output AOMDD from root to leaves. It first creates a
meta-node for the root variable, and then recursively creates its children metanodes by using APPLY
on the corresponding children of the input. The worst case that can happen is when the output is
not reduced at all, and a recursive call is made for each possible descendant. This corresponds to an
unfolding of the full AND/OR search tree based on the context variables, which is exponential in
the context size. When the APPLY finishes the context variables, and arrives at the first branching in
the bucket pseudo tree, the remaining branches are independent. Similar to the case of OBDDs,
where one function occupies a single place in memory, the APPLY can simply create a link to
the corresponding branches of the inputs (this is what happens in line 4 in the APPLY algorithm).
Therefore, the time and space complexity is at most exponential in the context size. 2
Proof of Theorem 5
The space complexity is governed by that of BE. Since an AOMDD never requires more space than
∗
that of a full exponential table (or a tree), it follows that BE-AOMDD only needs space O(n k w ).
The size of the output AOMDD is also bounded, per layers, by the number of assignments to the
context of that layer (namely, by the size of the context minimal AND/OR graph). Therefore,
∗
because context size is bounded by treewidth, it follows that the output has size O(n k w ). The
time complexity follows from Proposition 5, and from the fact that the number of functions in a
bucket cannot exceed r, the original number of functions.
2
Proof of Proposition 6
It suffices to prove the proposition for m = 2. The general result can then be obtained by induction.
It is essential that the function is defined by a constraint network (i.e., the values are only 0 or 1),
and that the function takes value 1 at least for one assignment. The value 1 denotes consistent assignments (solutions), while 0 denotes inconsistent assignments. Suppose f = f1 ⊗f2 . Let’s denote
by x a full assignment to X, and by x1 and x2 the projection of x over X1 and X2 , respectively. We
can write x = x1 x2 (concatenation of partial assignments). It follows that f (x) = f1 (x1 ) ∗ f2 (x2 ).
Therefore, if f (x) = 1, it must be that f1 (x1 ) = 1 and f2 (x2 ) = 1. We claim that for any x1 ,
f1 (x1 ) = 1 only if there exists some x2 such that f (x1 x2 ) = 1. Suppose by contradiction that there
exist some x1 such that f1 (x1 ) = 1 and f (x1 x2 ) = 0 for any other x2 . Since f is not always zero,

512

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

it follows that f2 is not always zero, and therefore there must be some x2 for which f2 (x2 ) = 1.
This leads to a contradiction, and therefore the functions f1 and f2 are uniquely defined by f . 2
Proof of Theorem 6
The proof is by structural induction over the depth of the pseudo tree T . It follows the canonicity
proofs for OBDDs (Bryant, 1986) and MDDs (Srinivasan et al., 1990), but extends them from linear
orderings to tree orderings that capture function decomposition according to the pseudo tree T . The
depth of T , along each of its paths from root to a leaf, is actually the size of the dependency set, or
the set of variables on which the value of the function depends. Remember that the AOMDD is an
AND/OR graph that is completely reduced. We will use the word function, denoted by f , to refer
to the universal relation, or its characteristic function, defined by the constraint network.
Assume the depth of T is 0. This means that the function does not depend on any variable,
and must be one of the constants 0 or 1. Suppose the function is the constant 0. Then, it must be
that the AOMDD does not contain the terminal meta-node 1, since all the nodes must be reachable
along some path, and it would mean that the function can also evaluate to 1. Suppose the AOMDD
contains a nonterminal meta-node, say labeled with X, where X can take k different values. It must
be that all the k children meta-nodes of X are the terminal meta-node 0. If there are more than one
terminal 0, then the AOMDD is not completely reduced. If there is only one 0, it follows that the
meta-node labeled with X is redundant. Therefore, from all the above, it follows that the AOMDD
representing the constant 0 is made of only the terminal 0. This is unique, and contains the smallest
number of nodes. A similar argument applies for the constant 1.
Now, suppose that the statement of the theorem holds for any constraint network that admits a
pseudo tree of depth strictly smaller than p, and that we have a constraint network with a pseudo
tree of depth equal to p, where p > 0. Let X be the root of T , having domain {x1 , . . . , xk }. We
denote by fi , where i ∈ {1, . . . , k}, the functions defined by the restricted constraint network for
X = xi , namely fi = f |X=xi . Let Y1 , . . . , Ym be the children of X in T . Suppose we have two
AOMDDs of f , denoted by G and G 0 . We will show that these two AND/OR graphs are isomorphic.
The functions fi can be decomposed according to the pseudo tree T when the root X is removed.
This can in fact be a forest of independent pseudo trees (they do not share any variables), rooted by
Y1 , . . . , Ym . Based on Proposition 6, there is a unique decomposition fi = fiY1 ∗ . . . ∗ fiYm , for all
Y
i ∈ {1, . . . , k}. Based on the induction hypothesis, each of the function fi j has a unique AOMDD.
In the AND/OR graphs G and G 0 , if we look at the subgraphs descending from X = xi , they both are
completely reduced and define the same function, fi , therefore there exists an isomorphic mapping
σi between them. Let v be the root metanode of G and v 0 the root of G 0 . We claim that G and G 0 are
isomorphic according to the following mapping:
 0
v,
if u = v;
σ(u) =
σi (u), if u is in the subgraph rooted by hX, xi i.
To prove this, we have to show that σ is well defined, and that it is an isomorphic mapping.
If a meta-node u in G is contained in both subgraphs rooted by hX, xi i and hX, xj i, Then the
AND/OR graphs rooted by σi (u) and σj (u) are isomorphic to the one rooted at u, and therefore to
each other. Since G 0 is completely reduced, it does not contain isomorphic subgraphs, and therefore
σi (u) = σj (u). Therefore σ is well defined.
We can now show that σ is a bijection. To show that it is one-to-one, assume two distinct metanodes u1 and u2 in G, with σ(u1 ) = σ(u2 ). Then, the subgraphs rooted by u1 and u2 are isomorphic
513

M ATEESCU , D ECHTER & M ARINESCU

to the subgraph rooted by σ(u1 ), and therefore to each other. Since G is completely reduced, it must
be that u1 = u2 . The fact that σ is onto and is an isomorphic mapping follows from its definition and
from the fact that each σi is onto and the only new node is the root meta-node. Since the AOMDDs
only contain one root meta-node (more than one root would lead to the conclusion that the root
meta-nodes are isomorphic and should be merged), we conclude that G and G 0 are isomorphic.
Finally, we can show that among all the AND/OR graphs representing f , the AOMDD has
minimal number of meta-nodes. Suppose G is an AND/OR graph that represents f , with minimal
number of meta-nodes, but without being an AOMDD. Namely, it is not completely reduced. Any
reduction rule would transform G into an AND/OR graph with smaller number of meta-nodes,
leading to a contradiction. Therefore, G must be the unique AOMDD that represents f . 2
Proof of Corollary 1
The proof of Theorem 6 did not rely on the scopes that define the constraint network. As long as the
network admits the decomposition induced by the pseudo tree T , the universal function defined by
the constraint network will always have the same AOMDD, and therefore any constraint network
equivalent to it that admits T will also have the same AOMDD. 2
Proof of Theorem 7
The constant that is associated with the root is actually the sum of the weights of all solutions. This
can be derived from the definition of the weighted AOMDD. The weights of each meta-node are
normalized (they sum to 1), therefore the values computed for each OR node by AND/OR search
is always 1 (when the task is computing
P the sum of all solution weights). Therefore, the constant
of the weighted AOMDD is always x w(x) regardless of the graphical model. We will prove that
weighted AOMDDs are canonical for functions that are normalized.
Assume we have two different weighted AOMDDs, denoted by G 1 and G 2 , for the same normalized function f . Let the root variable be A, with the domain {a1 , . . . , ak }. Let x denote a full
assignment to all the variables. Similar to the above argument for the root constant,
P because all
the meta-nodes have normalized weights, it follows that w1 (A, a1 ) = w2 (A, a1 ) = x|A=a1 f (x).
The superscript of w1 and w2 indicates the AOMDD, and the summation is over all possible assignments restricted to A = a1 . It follows that the root meta-nodes are identical. For each value of the
root variable, the restricted functions represented in G 1 and G 2 are identical, and we will recursively
apply the same argument as above.
However, for the proof to be complete, we have to discuss the case when a restricted function
is decomposed into independent functions, according to the pseudo tree. Suppose there are two
independent components, rooted by B and C. If one of them is the 0 function, it follows that the
entire function is 0. We will prove that the meta-nodes of B in G 1 and G 2 are identical. If B has only
one value b1 extendable to a solution, its weight must be 1 in both meta-nodes, so the meta-nodes
are identical. If B has more than one value, suppose without loss of generality that the weights are
different for the first value b1 , and
w1 (B, b1 ) > w2 (B, b1 ).

(1)

Since f 6= 0, there must be a value C = c1 such that B = b1 , C = c1 can be extended to a full
solution. The sum of the weights of all these possible extensions is
X
f (x) = w1 (B, b1 ) ∗ w1 (C, c1 ) = w2 (B, b1 ) ∗ w2 (C, c1 ).
(2)
x|B=b1 ,C=c1

514

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

From Equations 1 and 2 and the fact that the weight are non-zero, it follows that
w1 (C, c1 ) < w2 (C, c1 ).

(3)

From Equation 1, the fact that B has more than one value and the fact that the weights of B are
normalized, it follows that there should be a value b2 such that
w1 (B, b2 ) < w2 (B, b2 ).

(4)

From Equations 3 and 4, it follows that
w1 (B, b2 ) ∗ w1 (C, c1 ) < w2 (B, b2 ) ∗ w2 (C, c1 ).

(5)

However, both sides of
P the Equation 5 represent the sum of weights of all solutions when B =
b2 , C = c1 , namely x|B=b2 ,C=c1 f (x), leading to a contradiction. Therefore, it must be that
Equation 1 is false. Continuing the same argument for all values of B, it follows that the metanodes of B are identical, and similarly the meta-nodes of C are identical.
If the decomposition has more than two components, the same argument applies, when B is the
first component and C is a meta-variable that combines all the other components.
2
Proof of Theorem 8
Consider the well known NP-complete problem of 3- COLORING: Given a graph G, is there a
3-coloring of G? Namely, can we color its vertices using only three colors, such that any two
adjacent vertices have different colors? We will reduce 3- COLORING to the problem of computing
the semantic treewidth of a graphical model. Let H be a graph that is 3-colorable, and has a nontrivial semantic treewidth. It is easy to build examples for H. If G is 3-colorable, then G ∪ H is
also 3-colorable and will have a non-trivial semantic treewidth, because adding G will not simplify
the task of describing the colorings of H. However, if G is not 3-colorable, then G ∪ H is also not
3-colorable, and will have a semantic treewidth of zero. 2
Proof of Proposition 7
Since AOMDDs are canonical representations of graphical models, it follows that the graphical
model for which the actual semantic treewidth is realized will have the same AOMDD as M, and
therefore the AOMDD is bounded exponentially in the semantic treewidth. 2



The paper investigates parameterized approximate message-passing schemes that are based on
bounded inference and are inspired by Pearl’s belief propagation algorithm (BP). We start with the
bounded inference mini-clustering algorithm and then move to the iterative scheme called Iterative
Join-Graph Propagation (IJGP), that combines both iteration and bounded inference. Algorithm
IJGP belongs to the class of Generalized Belief Propagation algorithms, a framework that allowed
connections with approximate algorithms from statistical physics and is shown empirically to surpass the performance of mini-clustering and belief propagation, as well as a number of other stateof-the-art algorithms on several classes of networks. We also provide insight into the accuracy of
iterative BP and IJGP by relating these algorithms to well known classes of constraint propagation
schemes.

1. Introduction
Probabilistic inference is the principal task in Bayesian networks and is known to be an NP-hard
problem (Cooper, 1990; Roth, 1996). Most of the commonly used exact algorithms such as jointree clustering (Lauritzen & Spiegelhalter, 1988; Jensen, Lauritzen, & Olesen, 1990) or variableelimination (Dechter, 1996, 1999; Zhang, Qi, & Poole, 1994), and more recently search schemes
(Darwiche, 2001; Bacchus, Dalmao, & Pitassi, 2003; Dechter & Mateescu, 2007) exploit the network structure. While significant advances were made in the last decade in exact algorithms, many
real-life problems are too big and too hard, especially when their structure is dense, since they are
time and space exponential in the treewidth of the graph. Approximate algorithms are therefore
necessary for many practical problems, although approximation within given error bounds is also
NP-hard (Dagum & Luby, 1993; Roth, 1996).

c 2010 AI Access Foundation. All rights reserved.

M ATEESCU , K ASK , G OGATE & D ECHTER

The paper focuses on two classes of approximation algorithms for the task of belief updating.
Both are inspired by Pearl’s belief propagation algorithm (Pearl, 1988), which is known to be exact
for trees. As a distributed algorithm, Pearl’s belief propagation can also be applied iteratively to
networks that contain cycles, yielding Iterative Belief Propagation (IBP), also known as loopy belief
propagation. When the networks contain cycles, IBP is no longer guaranteed to be exact, but in
many cases it provides very good approximations upon convergence. Some notable success cases
are those of IBP for coding networks (McEliece, MacKay, & Cheng, 1998; McEliece & Yildirim,
2002), and a version of IBP called survey propagation for some classes of satisfiability problems
(Mézard, Parisi, & Zecchina, 2002; Braunstein, Mézard, & Zecchina, 2005).
Although the performance of belief propagation is far from being well understood in general,
one of the more promising avenues towards characterizing its behavior came from analogies with
statistical physics. It was shown by Yedidia, Freeman, and Weiss (2000, 2001) that belief propagation can only converge to a stationary point of an approximate free energy of the system, called
Bethe free energy. Moreover, the Bethe approximation is computed over pairs of variables as terms,
and is therefore the simplest version of the more general Kikuchi (1951) cluster variational method,
which is computed over clusters of variables. This observation inspired the class of Generalized
Belief Propagation (GBP) algorithms, that work by passing messages between clusters of variables.
As mentioned by Yedidia et al. (2000), there are many GBP algorithms that correspond to the same
Kikuchi approximation. A version based on region graphs, called “canonical” by the authors, was
presented by Yedidia et al. (2000, 2001, 2005). Our algorithm Iterative Join-Graph Propagation is
a member of the GBP class, although it will not be described in the language of region graphs. Our
approach is very similar to and was independently developed from that of McEliece and Yildirim
(2002). For more information on BP state of the art research see the recent survey by Koller (2010).
We will first present the mini-clustering scheme which is an anytime bounded inference scheme
that generalizes the mini-bucket idea. It can be viewed as a belief propagation algorithm over a tree
obtained by a relaxation of the network’s structure (using the technique of variable duplication). We
will subsequently present Iterative Join-Graph Propagation (IJGP) that sends messages between
clusters that are allowed to form a cyclic structure.
Through these two schemes we investigate: (1) the quality of bounded inference as an anytime
scheme (using mini-clustering); (2) the virtues of iterating messages in belief propagation type
algorithms, and the result of combining bounded inference with iterative message-passing (in IJGP).
In the background section 2, we overview the Tree-Decomposition scheme that forms the basis
for the rest of the paper. By relaxing two requirements of the tree-decomposition, that of connectedness (via mini-clustering) and that of tree structure (by allowing cycles in the underlying graph),
we combine bounded inference and iterative message-passing with the basic tree-decomposition
scheme, as elaborated in subsequent sections.
In Section 3 we present the partitioning-based anytime algorithm called Mini-Clustering (MC),
which is a generalization of the Mini-Buckets algorithm (Dechter & Rish, 2003). It is a messagepassing algorithm guided by a user adjustable parameter called i-bound, offering a flexible tradeoff
between accuracy and efficiency in anytime style (in general the higher the i-bound, the better the
accuracy). MC algorithm operates on a tree-decomposition, and similar to Pearl’s belief propagation algorithm (Pearl, 1988) it converges in two passes, up and down the tree. Our contribution
beyond other works in this area (Dechter & Rish, 1997; Dechter, Kask, & Larrosa, 2001) is in: (1)
Extending the partition-based approximation for belief updating from mini-buckets to general treedecompositions, thus allowing the computation of the updated beliefs for all the variables at once.
280

J OIN -G RAPH P ROPAGATION A LGORITHMS

This extension is similar to the one proposed by Dechter et al. (2001), but replaces optimization
with probabilistic inference. (2) Providing empirical evaluation that demonstrates the effectiveness
of the idea of tree-decomposition combined with partition-based approximation for belief updating.
Section 4 introduces the Iterative Join-Graph Propagation (IJGP) algorithm. It operates on a
general join-graph decomposition that may contain cycles. It also provides a user adjustable i-bound
parameter that defines the maximum cluster size of the graph (and hence bounds the complexity),
therefore it is both anytime and iterative. While the algorithm IBP is typically presented as a generalization of Pearl’s Belief Propagation algorithm, we show that IBP can be viewed as IJGP with the
smallest i-bound.
We also provide insight into IJGP’s behavior in Section 4. Zero-beliefs are variable-value pairs
that have zero conditional probability given the evidence. We show that: (1) if a value of a variable
is assessed as having zero-belief in any iteration of IJGP, it remains a zero-belief in all subsequent
iterations; (2) IJGP converges in a finite number of iterations relative to its set of zero-beliefs; and,
most importantly (3) that the set of zero-beliefs decided by any of the iterative belief propagation
methods is sound. Namely any zero-belief determined by IJGP corresponds to a true zero conditional probability relative to the given probability distribution expressed by the Bayesian network.
Empirical results on various classes of problems are included in Section 5, shedding light on
the performance of IJGP(i). We see that it is often superior, or otherwise comparable, to other
state-of-the-art algorithms.
The paper is based in part on earlier conference papers by Dechter, Kask, and Mateescu (2002),
Mateescu, Dechter, and Kask (2002) and Dechter and Mateescu (2003).

2. Background
In this section we provide background for exact and approximate probabilistic inference algorithms
that form the basis of our work. While we present our algorithms in the context of directed probabilistic networks, they are applicable to any graphical model, including Markov networks.
2.1 Preliminaries
Notations: A reasoning problem is defined in terms of a set of variables taking values on finite
domains and a set of functions defined over these variables. We denote variables or subsets of
variables by uppercase letters (e.g., X, Y, Z, S, R . . .) and values of variables by lower case letters
(e.g., x, y, z, s). An assignment (X1 = x1 , . . . , Xn = xn ) can be abbreviated as x = (x1 , . . . , xn ).
For a subset of variables S, DS denotes the Cartesian product of the domains of variables in S. xS
is the projection of x = (x1 , . . . , xn ) over a subset S. We denote functions by letters f , g, h, etc.,
and the scope (set of arguments) of the function f by scope(f ).
D EFINITION 1 (graphical model) (Kask, Dechter, Larrosa, & Dechter, 2005) A graphical model
M is a 3-tuple, M = hX, D, Fi, where: X = {X1 , . . . , Xn } is a finite set of variables; D =
{D1 , . . . , Dn } is the set of their respective finite domains of values; F = {f1 , . . . , fr } is a set
of positive real-valued discrete functions, each defined over a subset of variables Si ⊆ X, called
its scope, and denoted by scope(f
P i ). A graphical model typically has an associated combination
1
operator ⊗, (e.g., ⊗ ∈ {Π, } - product, sum). The graphical model represents the combination
1. The combination operator can also be defined axiomatically (Shenoy, 1992).

281

M ATEESCU , K ASK , G OGATE & D ECHTER

of all its functions: ⊗ri=1 fi . A graphical model has an associated primal graph that captures the
structural information of the model:
D EFINITION 2 (primal graph, dual graph) The primal graph of a graphical model is an undirected graph that has variables as its vertices and an edge connects any two vertices whose corresponding variables appear in the scope of the same function. A dual graph of a graphical model has
a one-to-one mapping between its vertices and functions of the graphical model. Two vertices in the
dual graph are connected if the corresponding functions in the graphical model share a variable.
We denote the primal graph by G = (X, E), where X is the set of variables and E is the set of
edges.
D EFINITION 3 (belief networks) A belief (or Bayesian) network is a graphical model B =
hX, D, G, P i, where G = (X, E) is a directed acyclic graph over variables X and P = {pi },
where pi = {p(Xi | pa (Xi ) ) } are conditional probability tables (CPTs) associated with each variable Xi and pa(Xi ) = scope(pi )−{Xi } is the set of parents of Xi in G. Given a subset of variables
S, we will write P (s) as the probability P (S = s), where s ∈ DS . A belief network represents
a probability distribution over X, P (x1 , . . . ., xn ) = Πni=1 P (xi |xpa(Xi ) ). An evidence set e is an
instantiated subset of variables. The primal graph of a belief network is called a moral graph. It
can be obtained by connecting the parents of each vertex in G and removing the directionality of
the edges. Equivalently, it connects any two variables appearing in the same family (a variable and
its parents in the CPT).
Two common queries in Bayesian networks are Belief Updating (BU) and Most Probable Explanation (MPE).
D EFINITION 4 (belief network queries) The Belief Updating (BU) task is to find the posterior
probability of each single variable given some evidence e, that is to compute P (Xi |e). The Most
Probable Explanation (MPE) task is to find a complete assignment to all the variables having maximum probability given the evidence, that is to compute argmaxX Πi pi .
2.2 Tree-Decomposition Schemes
Tree-decomposition is at the heart of most general schemes for solving a wide range of automated
reasoning problems, such as constraint satisfaction and probabilistic inference. It is the basis for
many well-known algorithms, such as join-tree clustering and bucket elimination. In our presentation we will follow the terminology of Gottlob, Leone, and Scarcello (2000) and Kask et al. (2005).
D EFINITION 5 (tree-decomposition, cluster-tree) Let B = hX, D, G, P i be a belief network. A
tree-decomposition for B is a triple hT, χ, ψi, where T = (V, E) is a tree, and χ and ψ are labeling
functions which associate with each vertex v ∈ V two sets, χ(v) ⊆ X and ψ(v) ⊆ P satisfying:
1. For each function pi ∈ P , there is exactly one vertex v ∈ V such that pi ∈ ψ(v), and
scope(pi ) ⊆ χ(v).
2. For each variable Xi ∈ X, the set {v ∈ V |Xi ∈ χ(v)} induces a connected subtree of T .
This is also called the running intersection (or connectedness) property.
We will often refer to a node and its functions as a cluster and use the term tree-decomposition and
cluster-tree interchangeably.
282

J OIN -G RAPH P ROPAGATION A LGORITHMS

D EFINITION 6 (treewidth, separator, eliminator) Let D = hT, χ, ψi be a tree-decomposition of
a belief network B. The treewidth (Arnborg, 1985) of D is maxv∈V |χ(v)| − 1. The treewidth of
B is the minimum treewidth over all its tree-decompositions. Given two adjacent vertices u and
v of a tree-decomposition, the separator of u and v is defined as sep(u, v) = χ(u) ∩ χ(v), and
the eliminator of u with respect to v is elim(u, v) = χ(u) − χ(v). The separator-width of D is
max(u,v) |sep(u, v)|. The minimum treewidth of a graph G can be shown to be identical to a related
parameter called induced-width (Dechter & Pearl, 1987).
Join-tree and cluster-tree elimination (CTE) In both Bayesian network and constraint satisfaction communities, the most used tree-decomposition method is join-tree decomposition (Lauritzen
& Spiegelhalter, 1988; Dechter & Pearl, 1989), introduced based on relational database concepts
(Maier, 1983). Such decompositions can be generated by embedding the network’s moral graph G
into a chordal graph, often using a triangulation algorithm and using its maximal cliques as nodes in
the join-tree. The triangulation algorithm assembles a join-tree by connecting the maximal cliques in
the chordal graph in a tree. Subsequently, every CPT pi is placed in one clique containing its scope.
Using the previous terminology, a join-tree decomposition of a belief network B = hX, D, G, P i is
0
a tree T = (V, E), where V is the set of cliques of a chordal graph G that contains G, and E is a set
of edges that form a tree between cliques, satisfying the running intersection property (Maier, 1983).
Such a join-tree satisfies the properties of tree-decomposition and is therefore a cluster-tree (Kask
et al., 2005). In this paper, we will use the terms tree-decomposition and join-tree decomposition
interchangeably.
There are a few variants for processing join-trees for belief updating (e.g., Jensen et al., 1990;
Shafer & Shenoy, 1990). We adopt here the version from Kask et al. (2005), called cluster-treeelimination (CTE), that is applicable to tree-decompositions in general and is geared towards space
savings. It is a message-passing algorithm; for the task of belief updating, messages are computed
by summation over the eliminator between the two clusters of the product of functions in the originating cluster. The algorithm, denoted CTE-BU (see Figure 1), pays a special attention to the
processing of observed variables since the presence of evidence is a central component in belief
updating. When a cluster sends a message to a neighbor, the algorithm operates on all the functions
in the cluster except the message from that particular neighbor. The message contains a single combined function and individual functions that do not share variables with the relevant eliminator. All
the non-individual functions are combined in a product and summed over the eliminator.
Example 1 Figure 2a describes a belief network and Figure 2b a join-tree decomposition for it.
Figure 2c shows the trace of running CTE-BU with evidence G = ge , where h(u,v) is a message that
cluster u sends to cluster v.
T HEOREM 1 (complexity of CTE-BU) (Dechter et al., 2001; Kask et al., 2005) Given a Bayesian
network B = hX, D, G, P i and a tree-decomposition hT, χ, ψi of B, the time complexity of CTE∗
BU is O(deg · (n + N ) · dw +1 ) and the space complexity is O(N · dsep ), where deg is the maximum
degree of a node in the tree-decomposition, n is the number of variables, N is the number of nodes
in the tree-decomposition, d is the maximum domain size of a variable, w∗ is the treewidth and sep
is the maximum separator size.

283

M ATEESCU , K ASK , G OGATE & D ECHTER

Algorithm CTE for Belief-Updating (CTE-BU)
Input: A tree-decomposition hT, χ, ψi, T = (V, E) for B = hX, D, G, P i. Evidence variables
var(e).
Output: An augmented tree whose nodes are clusters containing the original CPTs and the
messages received from neighbors. P (Xi , e), ∀Xi ∈ X.
Denote by H(u,v) the message from vertex u to v, nev (u) the neighbors of u in T excluding v,
cluster(u) = ψ(u) ∪ {H(v,u) |(v, u) ∈ E},
clusterv (u) = cluster(u) excluding message from v to u.

• Compute messages:
For every node u in T , once u has received messages from all nev (u), compute message to node
v:
1. Process observed variables:
Assign relevant evidence to all pi ∈ ψ(u)
2. Compute the combined function:
X

h(u,v) =

Y

f

elim(u,v) f ∈A

where A is the set of functions in clusterv (u) whose scope intersects elim(u, v).
Add h(u,v) to H(u,v) and add all the individual functions in clusterv (u) − A
Send H(u,v) to node v.
• Compute P (Xi , e):
For every Xi ∈ X let u be a vertex in T such that Xi ∈ χ(u). Compute P (Xi , e) =
P
Q
χ(u)−{Xi } ( f ∈cluster(u) f )

Figure 1: Algorithm Cluster-Tree-Elimination for Belief Updating (CTE-BU).
A

1

χ (1) = { A, B, C}
ψ (1) = { p(a ), p(b | a ), p(c | a, b)}

1

ABC
BC

B

2
C

D

χ ( 2) = { B , C , D , F }
ψ (2) = { p(d | b), p( f | c, d }

2 BCDF

E

BF

3

χ (3) = {B, E , F }
ψ (3) = { p (e | b, f )}

4

χ (4) = {E , F , G}
ψ (4) = { p( g | e, f )}

3

F
G

(a)

BEF
EF

4

(b)

EFG

h (1 , 2 ) ( b , c ) =

∑
(b , c ) = ∑
a

h ( 2 ,1 )

p ( a ) ⋅ p (b | a ) ⋅ p (c | a , b )
p ( d | b ) ⋅ p ( f | c , d ) ⋅ h( 3, 2 ) (b , f )

d,f

h( 2 ,3 ) (b , f ) =

∑
h( 3 , 2 ) ( b , f ) = ∑
e

p (d | b)

h( 3 , 4 ) ( e , f ) =

p ( e | b , f ) ⋅ h( 2 ,3 ) (b , f )

c ,d

∑

p ( f | c, d )

h (1 , 2 ) ( b , c )

p ( e | b , f ) ⋅ h( 4 ,3 ) ( e , f )

b

h( 4 ,3 ) ( e , f ) = p ( G = g e | e , f )

(c)

Figure 2: (a) A belief network; (b) A join-tree decomposition; (c) Execution of CTE-BU.

3. Partition-Based Mini-Clustering
The time, and especially the space complexity, of CTE-BU renders the algorithm infeasible for problems with large treewidth. We now introduce Mini-Clustering, a partition-based anytime algorithm
which computes bounds or approximate values on P (Xi , e) for every variable Xi .
284

J OIN -G RAPH P ROPAGATION A LGORITHMS

Procedure MC for Belief Updating (MC-BU(i))
2. Compute the combined mini-functions:
Make an (i)-size mini-cluster partitioning of clusterv (u), {mc(1), . . . , mc(p)};
P
Q
h1(u,v) = elim(u,v) f ∈mc(1) f
Q
hi(u,v) = maxelim(u,v) f ∈mc(i) f i = 2, . . . , p
add {hi(u,v) |i = 1, . . . , p} to H(u,v) . Send H(u,v) to v.
Compute upper bounds P (Xi , e) on P (Xi , e):
For every Xi ∈ X let u ∈ V be a cluster such that Xi ∈ χ(u). Make (i) mini-clusters from
cluster(u), {mc(1), . . . , mc(p)}; Compute P (Xi , e) =
P
Q
Qp
Q
( χ(u)−Xi f ∈mc(1) f ) · ( k=2 maxχ(u)−Xi f ∈mc(k) f ).

Figure 3: Procedure Mini-Clustering for Belief Updating (MC-BU).
3.1 Mini-Clustering Algorithm
Combining all the functions of a cluster into a product has a complexity exponential in its number
of variables, which is upper bounded by the induced width. Similar to the mini-bucket scheme
(Dechter, 1999), rather than performing this expensive exact computation, we partition the cluster into p mini-clusters mc(1), . . . , mc(p), each having at most
Pi variables,
Q where i is an accuracy parameter. Instead of computing by CTE-BU h(u,v) =
elim(u,v)
f ∈ψ(u) f , we can divide the functions
of ψ(u)
mc(k), k ∈ {1, . . . , p}, and rewrite h(u,v) =
P into p mini-clusters
Qp Q
P
Q
f
=
∈mc(k) f . By migrating the summation operator into
elim(u,v)
elim(u,v)
f ∈ψ(u)
Q
P k=1 fQ
p
each mini-cluster, yielding k=1 elim(u,v) f ∈mc(k) f , we get an upper bound on h(u,v) . The
resulting algorithm is called MC-BU(i).
Consequently, the combined functions are approximated via mini-clusters, as follows. Suppose
u ∈ V has received messages from all its neighbors other than v (the message from v is ignored even
if received). The functions in clusterv (u) that are to be combined are partitioned into mini-clusters
{mc(1), . . . , mc(p)}, each one containing at most i variables. Each mini-cluster is processed by
summation over the eliminator, and the resulting combined functions as well as all the individual
functions are sent to v. It was shown by Dechter and Rish (2003) that the upper bound can be
improved by using the maximization operator max rather than the summation operator sum on some
mini-buckets. Similarly, lower bounds can be generated by replacing sum with min (minimization)
for some mini-buckets. Alternatively, we can replace sum by a mean operator (taking the sum and
dividing by the number of elements in the sum), in this case deriving an approximation of the joint
belief instead of a strict upper bound.
Algorithm MC-BU for upper bounds can be obtained from CTE-BU by replacing step 2 of the
main loop and the final part of computing the upper bounds on the joint belief by the procedure given
in Figure 3. In the implementation we used for the experiments reported here, the partitioning was
done in a greedy brute-force manner. We ordered the functions according to their sizes (number of
variables), breaking ties arbitrarily. The largest function was placed in a mini-cluster by itself. Then,
we picked the largest remaining function and probed the mini-clusters in the order of their creation,
285

M ATEESCU , K ASK , G OGATE & D ECHTER

1

ABC

BC

H (1, 2)

h(11, 2 ) (b, c) := ∑ p (a ) ⋅ p (b | a ) ⋅ p(c | a, b)
a
1
( 2 ,1)

h

H ( 2,1)

(b) := ∑ p (d | b) ⋅ h(13, 2 ) (b, f )
d, f

h(22,1) (c) := max p ( f | c, d )
d, f

2

BCDF
1
( 2 , 3)

h

H ( 2 , 3)
BF

3

BEF

EF

c ,d

h(22,3) ( f ) := max p( f | c, d )
c,d

1
( 3, 2 )

(b, f ) := ∑ p(e | b, f ) ⋅ h(14,3) (e, f )

H ( 3, 2 )

h

H ( 3, 4 )

h(13, 4 ) (e, f ) := ∑ p (e | b, f ) ⋅ h(12,3) (b) ⋅ h(22,3) ( f )

H ( 4 , 3)

4

(b) := ∑ p (d | b) ⋅ h(11, 2 ) (b, c)

e

b

1
( 4 , 3)

h

(e, f ) := p(G = g e | e, f )

EFG

Figure 4: Execution of MC-BU for i = 3.
trying to find one that together with the new function would have no more than i variables. A new
mini-cluster was created whenever the existing ones could not accommodate the new function.
Example 2 Figure 4 shows the trace of running MC-BU(3) on the problem in Figure 2. First, evidence G = ge is assigned in all CPTs. There are no individual functions to be sent from cluster 1
to cluster 2. Cluster 1 contains only 3 variables,
χ(1) = {A, B, C}, therefore it is not partitioned.
P
p(a)
·
p(b|a) · p(c|a, b) is computed and the message
The combined function h1(1,2) (b, c) =
a
1
H(1,2) = {h(1,2) (b, c)} is sent to node 2. Now, node 2 can send its message to node 3. Again, there
are no individual functions. Cluster 2 contains 4 variables, χ(2) = {B, C, D, F }, and a partitioning is necessary: MC-BU(3) can choose
P mc(1) = {p(d|b), h(1,2) (b,2c)} and mc(2) = {p(f |c, d)}.
1
The combined functions h(2,3) (b) = c,d p(d|b) · h(1,2) (b, c) and h(2,3) (f ) = maxc,d p(f |c, d) are
computed and the message H(2,3) = {h1(2,3) (b), h2(2,3) (f )} is sent to node 3. The algorithm continues until every node has received messages from all its neighbors. An upper bound on p(a, G = ge )
can now be computed by choosing cluster
1, which contains variable A. It doesn’t need partitionP
ing, so the algorithm just computes b,c p(a) · p(b|a) · p(c|a, b) · h1(2,1) (b) · h2(2,1) (c). Notice that
unlike CTE-BU which processes 4 variables in cluster 2, MC-BU(3) never processes more than 3
variables at a time.
It was already shown that:
T HEOREM 2 (Dechter & Rish, 2003) Given a Bayesian network B = hX, D, G, P i and the evidence e, the algorithm MC-BU(i) computes an upper bound on the joint probability P (Xi , e) of
each variable Xi (and each of its values) and the evidence e.
T HEOREM 3 (complexity of MC-BU(i)) (Dechter et al., 2001) Given a Bayesian network B =
hX, D, G, P i and a tree-decomposition hT, χ, ψi of B, the time and space complexity of MC-BU(i)
is O(n · hw∗ · di ), where n is the number of variables, d is the maximum domain size of a variable
and hw∗ = maxu∈T |{f ∈ P |scope(f ) ∩ χ(u) 6= φ}|, which bounds the number of mini-clusters.
286

J OIN -G RAPH P ROPAGATION A LGORITHMS







   
 



" %&
"

#$ '  " & 
! ! !

   


!! !! !!

!

 



""  &&""'%&&
()* $  

   






 
!



+

Figure 5: Node duplication semantics of MC: (a) trace of MC-BU(3); (b) trace of CTE-BU.
Semantics of Mini-Clustering The mini-bucket scheme was shown to have the semantics of relaxation via node duplication (Kask & Dechter, 2001; Choi, Chavira, & Darwiche, 2007). We
extend it to mini-clustering by showing how it can apply as is to messages that flow in one direction
(inward, from leaves to root), as follows. Given a tree-decomposition D, where CTE-BU computes
a function h(u,v) (the message that cluster u sends to cluster v), MC-BU(i) partitions cluster u into p
mini-clusters u1 , . . . , up , which are processed independently and then the resulting functions h(ui ,v)
are sent to v. Instead consider a different decomposition D0 , which is just like D, with the exception that (a) instead of u, it has clusters u1 , . . . , up , all of which are children of v, and each variable
appearing in more than a single mini-cluster becomes a new variable, (b) each child w of u (in D) is
a child of uk (in D0 ), such that h(w,u) (in D) is assigned to uk (in D0 ) during the partitioning. Note
that D0 is not a legal tree-decomposition relative to the original variables since it violates the connectedness property: the mini-clusters u1 , . . . , up contain variables elim(u, v) but the path between
the nodes u1 , . . . , up (this path goes through v) does not. However, it is a legal tree-decomposition
relative to the new variables. It is straightforward to see that H(u,v) computed by MC-BU(i) on D
is the same as {h(ui ,v) |i = 1, . . . , p} computed by CTE-BU on D0 in the direction from leaves to
root.
If we want to capture the semantics of the outward messages from root to leaves, we need to generate a different relaxed decomposition (D00 ) because MC, as defined, allows a different partitioning
in the up and down streams of the same cluster. We could of course stick with the decomposition in
D0 and use CTE in both directions which would lead to another variant of mini-clustering.
Example 3 Figure 5(a) shows a trace of the bottom-up phase of MC-BU(3) on the network in Figure
4. Figure 5(b) shows a trace of the bottom-up phase of CTE-BU algorithm on a problem obtained
from the problem in Figure 4 by splitting nodes D (into D0 and D00 ) and F (into F 0 and F 00 ).
The MC-BU algorithm computes an upper bound P (Xi , e) on the joint probability P (Xi , e).
However, deriving a bound on the conditional probability P (Xi |e) is not easy when the exact
287

M ATEESCU , K ASK , G OGATE & D ECHTER

Random Bayesian N=50 K=2 P=2 C=48
0.20
0.18
0.16

Avg abs error

0.14
0.12
0.10
0.08
#ev=0
#ev=10
#ev=20
#ev=30

0.06
0.04
0.02
0.00
0

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

Number of iterations

Figure 6: Convergence of IBP (50 variables, evidence from 0-30 variables).
value of P (e) is not available. If we just try to divide (multiply) P (Xi , e) by a constant, the
result is not
P necessarily an upper bound on P (Xi |e). It is easy to show that normalization,
P (xi , e)/ xi ∈Di P (xi , e), with the mean operator is identical to normalization of MC-BU output
when applying the summation operator in all the mini-clusters.
MC-BU(i) is an improvement over the Mini-Bucket algorithm MB(i), in that it allows the computation of P (Xi , e) for all variables with a single run, whereas MB(i) computes P (Xi , e) for just
one variable, with a single run. When computing P (Xi , e) for each variable, MB(i) has to be run
n times, once for each variable, an algorithm we call nMB(i). It was demonstrated by Mateescu
et al. (2002) that MC-BU(i) has up to linear speed-up over nMB(i). For a given i, the accuracy of
MC-BU(i) can be shown to be not worse than that of nMB(i).
3.2 Experimental Evaluation of Mini-Clustering
The work of Mateescu et al. (2002) and Kask (2001) provides an empirical evaluation of MC-BU
that reveals the impact of the accuracy parameter on its quality of approximation and compares with
Iterative Belief Propagation and a Gibbs sampling scheme. We will include here only a subset of
these experiments which will provide the essence of our results. Additional empirical evaluation of
MC-BU will be given when comparing against IJGP later in this paper.
We tested the performance of MC-BU(i) on random Noisy-OR networks, random coding networks, general random networks, grid networks, and three benchmark CPCS files with 54, 360 and
422 variables respectively (these are belief networks for medicine, derived from the Computer based
Patient Case Simulation system, known to be hard for belief updating). On each type of network we
ran Iterative Belief Propagation (IBP) - set to run at most 30 iterations, Gibbs Sampling (GS) and
MC-BU(i), with i from 2 to the treewidth w∗ to capture the anytime behavior of MC-BU(i).
The random networks were generated using parameters (N,K,C,P), where N is the number of
variables, K is their domain size (we used only K=2), C is the number of conditional probability
tables and P is the number of parents in each conditional probability table. The parents in each table
are picked randomly given a topological ordering, and the conditional probability tables are filled
288

J OIN -G RAPH P ROPAGATION A LGORITHMS

0
|e| 10
20

NHD
max

IBP

MC-BU(2)

MC-BU(5)

MC-BU(8)

0
0
0
0
0
0
0
0
0

mean
0
0
0
0
0
0
0
0
0
0
0
0

N=50, P=2, 50 instances
Abs. Error
max

1.6E-03
1.1E-03
5.7E-04
1.1E-03
7.7E-04
2.8E-04
3.6E-04
1.7E-04
3.5E-05

mean
9.0E-09
3.4E-04
9.6E-04
1.1E-03
8.4E-04
4.8E-04
9.4E-04
6.9E-04
2.7E-04
3.2E-04
1.5E-04
3.5E-05

Rel. Error

max

1.9E+00
1.4E+00
7.1E-01
1.4E+00
9.3E-01
3.5E-01
4.4E-01
2.0E-01
4.3E-02

mean
1.1E-05
4.2E-01
1.2E+00
1.3E+00
1.0E+00
5.9E-01
1.2E+00
8.4E-01
3.3E-01
3.9E-01
1.9E-01
4.3E-02

Time
max

0.056
0.048
0.039
0.070
0.063
0.058
0.214
0.184
0.123

mean
0.102
0.081
0.062
0.057
0.049
0.039
0.072
0.066
0.057
0.221
0.190
0.127

Table 1: Performance on Noisy-OR networks, w∗ = 10: Normalized Hamming Distance, absolute
error, relative error and time.

randomly. The grid networks have the structure of a square, with edges directed to form a diagonal
flow (all parallel edges have the same direction). They were generated by specifying N (a square
integer) and K (we used K=2). We also varied the number of evidence nodes, denoted by |e| in
the tables. The parameter values are reported in each table. For all the problems, Gibbs sampling
performed consistently poorly so we only include part of its results here.
In our experiments we focused on the approximation power of MC-BU(i). We compared two
versions of the algorithm. In the first version, for every cluster, we used the max operator in all its
mini-clusters, except for one of them that was processed by summation. In the second version, we
used the operator mean in all the mini-clusters. We investigated this second version of the algorithm
for two reasons: (1) we compare MC-BU(i) with IBP and Gibbs sampling, both of which are also
approximation algorithms, so it would not be possible to compare with a bounding scheme; (2) we
observed in our experiments that, although the bounds improve as the i-bound increases, the quality
of bounds computed by MC-BU(i) was still poor, with upper bounds being greater than 1 in many
cases.2 Notice that we need to maintain the sum operator for at least one of the mini-clusters. The
mean operator simply performs summation and divides by the number of elements in the sum. For
example, if A, B, C are binary variables (taking values 0 and 1), and f (A, B, C) is the aggregated
function of one mini-cluster,
and elim = {A, B}, then computing the message h(C) by the mean
P
operator gives: 1/4 A,B∈{0,1} f (A, B, C).
We computed the exact solution and used three different measures of accuracy: 1) Normalized
Hamming Distance (NHD) - we picked the most likely value for each variable for the approximate
and for the exact, took the ratio between the number of disagreements and the total number of variables, and averaged over the number of problems that we ran for each class; 2) Absolute Error (Abs.
Error) - is the absolute value of the difference between the approximate and the exact, averaged
over all values (for each variable), all variables and all problems; 3) Relative Error (Rel. Error) - is
the absolute value of the difference between the approximate and the exact, divided by the exact,
averaged over all values (for each variable), all variables and all problems. For coding networks,
2. Wexler and Meek (2008) compared the upper/lower bounding properties of the mini-bucket on computing probability
of evidence. Rollon and Dechter (2010) further investigated heuristic schemes for mini-bucket partitioning.

289

M ATEESCU , K ASK , G OGATE & D ECHTER

10
|e| 20
30

N=50, P=3, 25 instances
Abs. Error

NHD
max

mean
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

IBP
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

MC-BU(2)

MC-BU(5)

MC-BU(8)

MC-BU(11)

MC-BU(14)

max

1.3E-03
5.3E-04
2.3E-04
1.0E-03
4.6E-04
2.0E-04
6.6E-04
1.8E-04
3.4E-05
2.6E-04
3.8E-05
6.4E-07
4.2E-05
0
0

mean
1.3E-04
3.6E-04
6.8E-04
9.6E-04
4.0E-04
1.9E-04
8.3E-04
4.1E-04
1.9E-04
5.7E-04
1.8E-04
3.4E-05
2.4E-04
3.8E-05
6.4E-07
4.1E-05
0
0

Rel. Error
max

Time

mean
7.9E-01
2.2E+00
4.2E+00
5.8E+00
2.4E+00
1.2E+00
5.1E+00
2.4E+00
1.2E+00
3.5E+00
1.0E+00
2.1E-01
1.5E+00
2.3E-01
4.0E-03
2.4E-01
0
0

8.2E+00
3.1E+00
1.4E+00
6.4E+00
2.7E+00
1.2E+00
4.0E+00
1.1E+00
2.1E-01
1.6E+00
2.3E-01
4.0E-03
2.5E-01
0
0

max

mean
0.242
0.184
0.121
0.108
0.077
0.064
0.133
0.105
0.095
0.509
0.406
0.308
2.378
1.439
0.624
7.875
2.093
0.638

0.107
0.077
0.064
0.133
0.104
0.098
0.498
0.394
0.300
2.339
1.421
0.613
7.805
2.075
0.630

Table 2: Performance on Noisy-OR networks, w∗ = 16: Normalized Hamming Distance, absolute
error, relative error and time.
Noisy-OR networks, N=50, P=3, evid=20, w*=16, 25 instances

Noisy-OR networks, N=50, P=3, evid=10, w*=16, 25 instances

1e+0

1e+0

MC
IBP
Gibbs Sampling

MC
IBP
Gibbs Sampling

1e-1

Absolute error

Absolute error

1e-1

1e-2

1e-3

1e-2

1e-3

1e-4

1e-4

1e-5

1e-5
0

2

4

6

8

10

12

14

16

0

2

4

6

8

10

12

14

16

i-bound

i-bound

Figure 7: Absolute error for Noisy-OR networks.
we report only one measure, Bit Error Rate (BER). In terms of the measures defined above, BER
is the normalized Hamming distance between the approximate (computed by an algorithm) and the
actual input (which in the case of coding networks may be different from the solution given by
exact algorithms), so we denote them differently to make this semantic distinction. We also report
the time taken by each algorithm. For reported metrics (time, error, etc.) provided in the Tables, we
give both mean and max values.
In Figure 6 we show that IBP converges after about 5 iterations. So, while in our experiments
we report its time for 30 iterations, its time is even better when sophisticated termination is used.
These results are typical of all runs.

290

J OIN -G RAPH P ROPAGATION A LGORITHMS

Random networks, N=50, P=2, k=2, evid=0, w*=10, 50 instances

Random networks, N=50, P=2, k=2, evid=10, w*=10, 50 instances

0.16

0.16

MC
Gibbs Sampling
IBP

0.14

0.14

MC
Gibbs Sampling
IBP

0.12

0.10

Absolute error

Absolute error

0.12

0.08
0.06
0.04

0.10
0.08
0.06
0.04

0.02

0.02

0.00

0.00

0

2

4

6

8

10

0

2

4

i-bound

6

8

10

i-bound

Figure 8: Absolute error for random networks.
BER

σ = .22
max
mean

IBP
GS
MC-BU(2)
MC-BU(4)
MC-BU(6)
MC-BU(8)

0.000
0.483
0.002
0.001
0.000
0.000

0.000
0.483
0.002
0.001
0.000
0.000

IBP
GS
MC-BU(2)
MC-BU(4)
MC-BU(6)
MC-BU(8)
MC-BU(10)

0.000
0.506
0.006
0.006
0.005
0.002
0.001

0.000
0.506
0.006
0.006
0.005
0.002
0.001

σ = .26
σ = .32
σ = .40
max
mean
max
mean
max
mean
N=100, P=3, 50 instances, w*=7
0.000 0.000 0.002 0.002 0.022 0.022
0.483 0.483 0.483 0.483 0.483 0.483
0.004 0.004 0.024 0.024 0.068 0.068
0.002 0.002 0.018 0.018 0.046 0.045
0.000 0.000 0.004 0.004 0.038 0.038
0.000 0.000 0.002 0.002 0.023 0.023
N=100, P=4, 50 instances, w*=11
0.000 0.000 0.002 0.002 0.013 0.013
0.506 0.506 0.506 0.506 0.506 0.506
0.015 0.015 0.043 0.043 0.093 0.094
0.017 0.017 0.049 0.049 0.104 0.102
0.011 0.011 0.035 0.034 0.071 0.074
0.004 0.004 0.022 0.022 0.059 0.059
0.001 0.001 0.008 0.008 0.033 0.032

σ = .51
max
mean

Time

0.088
0.483
0.132
0.110
0.106
0.091

0.088
0.483
0.131
0.110
0.106
0.091

0.00
31.36
0.08
0.08
0.12
0.19

0.075
0.506
0.157
0.158
0.151
0.121
0.101

0.075
0.506
0.157
0.158
0.150
0.122
0.102

0.00
39.85
0.19
0.19
0.29
0.71
1.87

Table 3: Bit Error Rate (BER) for coding networks.

Random Noisy-OR networks results are summarized in Tables 1 and 2, and Figure 7. For NHD,
both IBP and MC-BU gave perfect results. For the other measures, we noticed that IBP is more
accurate when there is no evidence by about an order of magnitude. However, as evidence is added,
IBP’s accuracy decreases, while MC-BU’s increases and they give similar results. We see that
MC-BU gets better as the accuracy parameter i increases, which shows its anytime behavior.
General random networks results are summarized in Figure 8. They are similar to those for
random Noisy-OR networks. Again, IBP has the best result only when the number of evidence
variables is small. It is remarkable how quickly MC-BU surpasses the performance of IBP as
evidence is added (for more, see the results of Mateescu et al., 2002).
Random coding networks results are given in Table 3 and Figure 9. The instances fall within the
class of linear block codes, (σ is the channel noise level). It is known that IBP is very accurate for
this class. Indeed, these are the only problems that we experimented with where IBP outperformed
MC-BU throughout. The anytime behavior of MC-BU can again be seen in the variation of numbers
in each column and more vividly in Figure 9.
291

M ATEESCU , K ASK , G OGATE & D ECHTER

Coding networks, N=100, P=4, sigma=.51, w*=12, 50 instances

Coding networks, N=100, P=4, sigma=.22, w*=12, 50 instances

0.18

0.007
MC
IBP

0.006

MC
IBP

0.16

0.005

Bit Error Rate

Bit Error Rate

0.14
0.004
0.003
0.002

0.12

0.10

0.001

0.08
0.000

0.06
0

2

4

6

8

10

0

12

2

4

6

8

10

12

i-bound

i-bound

Figure 9: Bit Error Rate (BER) for coding networks.
Grid 15x15, evid=10, w*=22, 10 instances

Grid 15x15, evid=10, w*=22, 10 instances

0.06

12
MC
IBP

0.05

MC
IBP

10

Time (seconds)

Absolute error

8
0.04

0.03

0.02

6

4

2
0.01

0

0.00
0

2

4

6

8

10

12

14

16

18

0

2

4

6

i-bound

8

10

12

14

16

18

i-bound

Figure 10: Grid 15x15: absolute error and time.
Grid networks results are given in Figure 10. We notice that IBP is more accurate for no evidence
and MC-BU is better as more evidence is added. The same behavior was consistently manifested
for smaller grid networks that we experimented with (from 7x7 up to 14x14).
CPCS networks results We also tested on three CPCS benchmark files. The results are given
in Figure 11. It is interesting to notice that the MC-BU scheme scales up to fairly large networks,
like the real life example of CPCS422 (induced width 23). IBP is again more accurate when there
is no evidence, but is surpassed by MC-BU when evidence is added. However, whereas MC-BU
is competitive with IBP time-wise when i-bound is small, its runtime grows rapidly as i-bound
increases. For more details on all these benchmarks see the results of Mateescu et al. (2002).
Summary Our results show that, as expected, IBP is superior to all other approximations for
coding networks. However, for random Noisy-OR, general random, grid networks and the CPCS
networks, in the presence of evidence, the mini-clustering scheme is often superior even in its weakest form. The empirical results are particularly encouraging as we use an un-optimized scheme that
exploits a universal principle applicable to many reasoning tasks.

292

J OIN -G RAPH P ROPAGATION A LGORITHMS

CPCS 422, evid=0, w*=23, 1 instance

CPCS 422, evid=10, w*=23, 1 instance

0.05

0.05
MC
IBP

MC
IBP

0.04

Absolute error

Absolute error

0.04

0.03

0.02

0.01

0.03

0.02

0.01

0.00

0.00
2

4

6

8

10

12

14

16

18

2

4

6

i-bound

8

10

12

14

16

18

i-bound

Figure 11: Absolute error for CPCS422.

4. Join-Graph Decomposition and Propagation
In this section we introduce algorithm Iterative Join-Graph Propagation (IJGP) which, like miniclustering, is designed to benefit from bounded inference, but also exploit iterative message-passing
as used by IBP. Algorithm IJGP can be viewed as an iterative version of mini-clustering, improving
the quality of approximation, especially for low i-bounds. Given a cluster of the decomposition,
mini-clustering can potentially create a different partitioning for every message sent to a neighbor.
This dynamic partitioning can happen because the incoming message from each neighbor has to be
excluded when realizing the partitioning, so a different set of functions are split into mini-clusters for
every message to a neighbor. We can define a version of mini-clustering where for every cluster we
create a unique static partition into mini-clusters such that every incoming message can be included
into one of the mini-clusters. This version of MC can be extended into IJGP by introducing some
links between mini-clusters of the same cluster, and carefully limiting the interaction between the
resulting nodes in order to eliminate over-counting.
Algorithm IJGP works on a general join-graph that may contain cycles. The cluster size of the
graph is user adjustable via the i-bound (providing the anytime nature), and the cycles in the graph
allow the iterative application of message-passing. In Subsection 4.1 we introduce join-graphs and
discuss their properties. In Subsection 4.2 we describe the IJGP algorithm itself.
4.1 Join-Graphs
D EFINITION 7 (join-graph decomposition) A join-graph decomposition for a belief network B =
hX, D, G, P i is a triple D = hJG, χ, ψi, where JG = (V, E) is a graph, and χ and ψ are labeling
functions which associate with each vertex v ∈ V two sets, χ(v) ⊆ X and ψ(v) ⊆ P such that:
1. For each pi ∈ P , there is exactly one vertex v ∈ V such that pi ∈ ψ(v), and scope(pi ) ⊆
χ(v).
2. (connectedness) For each variable Xi ∈ X, the set {v ∈ V |Xi ∈ χ(v)} induces a connected
subgraph of JG. The connectedness requirement is also called the running intersection property.

293

M ATEESCU , K ASK , G OGATE & D ECHTER

2,4

1,2,4

2,3,4

A

C
1,4

2,3,4

A

3,4

B

2,4

1,2,4

C
1,4

1,3,4

B

a)

3
1,3,4

b)

Figure 12: An edge-labeled decomposition.
We will often refer to a node in V and its CPT functions as a cluster3 and use the term joingraph decomposition and cluster-graph interchangeably. Clearly, a join-tree decomposition or a
cluster-tree is the special case when the join-graph D is a tree.
It is clear that one of the problems of message propagation over cyclic join-graphs is overcounting. To reduce this problem we devise a scheme, which avoids cyclicity with respect to any
single variable. The algorithm works on edge-labeled join-graphs.
D EFINITION 8 (minimal edge-labeled join-graph decompositions) An edge-labeled join-graph
decomposition for B = hX, D, G, P i is a four-tuple D = hJG, χ, ψ, θi, where JG = (V, E)
is a graph, χ and ψ associate with each vertex v ∈ V the sets χ(v) ⊆ X and ψ(v) ⊆ P and θ
associates with each edge (v, u) ⊂ E the set θ((v, u)) ⊆ X such that:
1. For each function pi ∈ P , there is exactly one vertex v ∈ V such that pi ∈ ψ(v), and
scope(pi ) ⊆ χ(v).
2. (edge-connectedness) For each edge (u, v), θ((u, v)) ⊆ χ(u) ∩ χ(v), such that ∀Xi ∈ X,
any two clusters containing Xi can be connected by a path whose every edge label includes
Xi .
Finally, an edge-labeled join-graph is minimal if no variable can be deleted from any label while
still satisfying the edge-connectedness property.
D EFINITION 9 (separator, eliminator of edge-labeled join-graphs) Given two adjacent vertices
u and v of JG, the separator of u and v is defined as sep(u, v) = θ((u, v)), and the eliminator of u
with respect to v is elim(u, v) = χ(u) − θ((u, v)). The separator width is max(u,v) |sep(u, v)|.
Edge-labeled join-graphs can be made label minimal by deleting variables from the labels while
maintaining connectedness (if an edge label becomes empty, the edge can be deleted). It is easy to
see that,
Proposition 1 A minimal edge-labeled join-graph does not contain any cycle relative to any single
variable. That is, any two clusters containing the same variable are connected by exactly one path
labeled with that variable.
Notice that every minimal edge-labeled join-graph is edge-minimal (no edge can be deleted), but
not vice-versa.
3. Note that a node may be associated with an empty set of CPTs.

294

J OIN -G RAPH P ROPAGATION A LGORITHMS

Example 4 The example in Figure 12a shows an edge minimal join-graph which contains a cycle
relative to variable 4, with edges labeled with separators. Notice however that if we remove variable 4 from the label of one edge we will have no cycles (relative to single variables) while the
connectedness property is still maintained.
The mini-clustering approximation presented in the previous section works by relaxing the jointree requirement of exact inference into a collection of join-trees having smaller cluster size. It
introduces some independencies in the original problem via node duplication and applies exact inference on the relaxed model requiring only two message passings. For the class of IJGP algorithms
we take a different route. We choose to relax the tree-structure requirement and use join-graphs
which do not introduce any new independencies, and apply iterative message-passing on the resulting cyclic structure.
Indeed, it can be shown that any join-graph of a belief network is an I-map (independency map,
Pearl, 1988) of the underlying probability distribution relative to node-separation. Since we plan
to use minimally edge-labeled join-graphs to address over-counting problems, the question is what
kind of independencies are captured by such graphs.
D EFINITION 10 (edge-separation in edge-labeled join-graphs) Let D = hJG, χ, ψ, θi, JG =
(V, E) be an edge-labeled decomposition of a Bayesian network B = hX, D, G, P i. Let NW , NY ⊆
V be two sets of nodes, and EZ ⊆ E be a set of edges in JG. Let W, Y, Z be their corresponding
sets of variables (W = ∪v∈NW χ(v), Z = ∪e∈EZ θ(e)). We say that EZ edge-separates NW and
NY in D if there is no path between NW and NY in the JG graph whose edges in EZ are removed.
In this case we also say that W is separated from Y given Z in D, and write hW |Z|Y iD . Edgeseparation in a regular join-graph is defined relative to its separators.
T HEOREM 4 Any edge-labeled join-graph decomposition D = hJG, χ, ψ, θi of a belief network
B = hX, D, G, P i is an I-map of P relative to edge-separation. Namely, any edge separation in D
corresponds to conditional independence in P .
Proof: Let M G be the moral graph of BN . Since M G is an I-map of P , it is enough to prove that
JG is an I-map of M G. Let NW and NY be disjoint sets of nodes and NZ be a set of edges in JG,
and W, Z, Y be their corresponding sets of variables in M G. We will prove:
hNW |NZ |NY iJG =⇒ hW |Z|Y iM G
by contradiction. Since the sets W, Z, Y may not be disjoint, we will actually prove that hW −
Z|Z|Y − ZiM G holds, this being equivalent to hW |Z|Y iM G .
Supposing hW − Z|Z|Y − ZiM G is false, then there exists a path α = γ1 , γ2 , . . . , γn−1 , β = γn
in M G that goes from some variable α = γ1 ∈ W − Z to some variable β = γn ∈ Y − Z without
intersecting variables in Z. Let Nv be the set of all nodes in JG that contain variable v ∈ X, and
let us consider the set of nodes:
S = ∪ni=1 Nγi − NZ
We argue that S forms a connected sub-graph in JG. First, the running intersection property
ensures that every Nγi , i = 1, . . . , n, remains connected in JG after removing the nodes in NZ
(otherwise, it must be that there was a path between the two disconnected parts in the original JG,
which implies that a γi is part of Z, which is a contradiction). Second, the fact that (γi , γi+1 ), i =
295

M ATEESCU , K ASK , G OGATE & D ECHTER

1, . . . , n − 1, is an edge in the moral graph M G implies that there is a conditional probability table
(CPT) on both γi and γi+1 , i = 1, . . . , n − 1 (and perhaps other variables). From property 1 of the
definition of the join-graph, it follows that for all i = 1, . . . , n − 1 there exists a node in JG that
contains both γi and γi+1 . This proves the existence of a path in the mutilated join-graph (JG with
NZ pulled out) from a node in NW containing α = γ1 to the node containing both γ1 and γ2 (Nγ1
is connected), then from that node to the one containing both γ2 and γ3 (Nγ2 is connected), and
so on until we reach a node in NY containing β = γn . This shows that hNW |NZ |NY iJG is false,
concluding the proof by contradiction. 2
Interestingly however, deleting variables from edge labels or removing edges from edge-labeled
join-graphs whose clusters are fixed will not increase the independencies captured by edge-labeled
join-graphs. That is,
Proposition 2 Any two (edge-labeled) join-graphs defined on the same set of clusters, sharing (V ,
χ, ψ), express exactly the same set of independencies relative to edge-separation, and this set of
independencies is identical to the one expressed by node separation in the primal graph of the
join-graph.
Proof: This follows by looking at the primal graph of the join-graph (obtained by connecting any
two nodes in a cluster by an arc over the original variables as nodes) and observing that any edgeseparation in a join-graph corresponds to a node separation in the primal graph and vice-versa. 2
Hence, the issue of minimizing computational over-counting due to cycles appears to be unrelated to the problem of maximizing independencies via minimal I-mapness. Nevertheless, to avoid
over-counting as much as possible, we still prefer join-graphs that minimize cycles relative to each
variable. That is, we prefer minimal edge-labeled join-graphs.
Relationship with region graphs There is a strong relationship between our join-graphs and the
region graphs of Yedidia et al. (2000, 2001, 2005). Their approach was inspired by advances in
statistical physics, when it was realized that computing the partition function is essentially the same
combinatorial problem that expresses probabilistic reasoning. As a result, variational methods from
physics could have counterparts in reasoning algorithms. It was proved by Yedidia et al. (2000,
2001) that belief propagation on loopy networks can only converge (when it does so) to stationary
points of the Bethe free energy. The Bethe approximation is only the simplest case of the more
general Kikuchi (1951) cluster variational method. The idea is to group the variables together in
clusters and perform exact computation in each cluster. One key question is then how to aggregate
the results, and how to account for the variables that are shared between clusters. Again, the idea
that everything should be counted exactly once is very important. This led to the proposal of region
graphs (Yedidia et al., 2001, 2005) and the associated counting numbers for regions. They are
given as a possible canonical version of graphs that can support Generalized Belief Propagation
(GBP) algorithms. The join-graphs accomplish the same thing. The edge-labeled join-graphs can
be described as region graphs where the regions are the clusters and the labels on the edges. The
tree-ness condition with respect to every variable ensures that there is no over-counting.
A very similar approach to ours, which is also based on join-graphs appeared independently by
McEliece and Yildirim (2002), and it is based on an information theoretic perspective.

296

J OIN -G RAPH P ROPAGATION A LGORITHMS

Algorithm Iterative Join-Graph Propagation (IJGP)
Input An arc-labeled join-graph decomposition hJG, χ, ψ, θi, JG = (V, E) for B = hX, D, G, P i. Evidence variables var(e).
Output An augmented graph whose nodes are clusters containing the original CPTs and the messages received
from neighbors. Approximations of P (Xi |e), ∀Xi ∈ X.
Denote by h(u,v) the message from vertex u to v, nev (u) the neighbors of u in JG excluding v.
cluster(u) = ψ(u) ∪ {h(v,u) |(v, u) ∈ E}.
clusterv (u) = cluster(u) excluding message from v to u.
• One iteration of IJGP:
For every node u in JG in some topological order d and back, do
1. Process observed variables:
Assign relevant evidence to all pi ∈ ψ(u) χ(u) := χ(u) − var(e), ∀u ∈ V
2. Compute individual functions:
Include in H(u,v) each function in clusterv (u) whose scope does not contain variables in elim(u, v).
Denote by A the remaining functions.
P
Q
3. Compute and send to v the combined function: h(u,v) = elim(u,v) f ∈A f .
Send h(u,v) and the individual functions H(u,v) to node v.
Endfor
• Compute an approximation of P (Xi |e):
For every Xi ∈ X let u be
P a vertex in JG
Q such that Xi ∈ χ(u).
Compute P (Xi , e) = α χ(u)−{Xi } ( f ∈cluster(u) f )

Figure 13: Algorithm Iterative Join-Graph Propagation (IJGP).
4.2 Algorithm IJGP
Applying CTE iteratively to minimal edge-labeled join-graphs yields our algorithm Iterative JoinGraph Propagation (IJGP) described in Figure 13. One iteration of the algorithm applies messagepassing in a topological order over the join-graph, forward and back. When node u sends a message
(or messages) to a neighbor node v it operates on all the CPTs in its cluster and on all the messages
sent from its neighbors excluding the ones received from v. First, all individual functions that share
no variables with the eliminator are collected and sent to v. All the rest of the functions are combined
in a product and summed over the eliminator between u and v.
Based on the results by Lauritzen and Spiegelhalter (1988) and Larrosa, Kask, and Dechter
(2001) it can be shown that:
T HEOREM 5
1. If IJGP is applied to a join-tree decomposition it reduces to join-tree clustering, and therefore it is guaranteed to compute the exact beliefs in one iteration.
∗
2. The time complexity of one iteration of IJGP is O(deg · (n + N ) · dw +1 ) and its space
complexity is O(N · dθ ), where deg is the maximum degree of a node in the join-graph, n
is the number of variables, N is the number of nodes in the graph decomposition, d is the
maximum domain size, w∗ is the maximum cluster size and θ is the maximum label size.
For proof, see the properties of CTE presented by Kask et al. (2005).

297

M ATEESCU , K ASK , G OGATE & D ECHTER

3

A
2
B

C

a)

A

AB

3

A
A

B

b)

1

ABC

2
AB

A

A

1
AB

ABC

c)

Figure 14: a) A belief network; b) A dual join-graph with singleton labels; c) A dual join-graph
which is a join-tree.

The special case of Iterative Belief Propagation Iterative belief propagation (IBP) is an iterative application of Pearl’s algorithm that was defined for poly-trees (Pearl, 1988), to any Bayesian
network. We will describe IBP as an instance of join-graph propagation over a dual join-graph.
D EFINITION 11 (dual graphs, dual join-graphs) Given a set of functions F = {f1 , . . . , fl } over
scopes S1 , . . . , Sl , the dual graph of F is a graph DG = (V, E, L) that associates a node with
each function, namely V = F and an edge connects any two nodes whose function’s scope share a
variable, E = {(fi , fj )|Si ∩ Sj 6= φ}. L is a set of labels for the edges, each edge being labeled
by the shared variables of its nodes, L = {lij = Si ∩ Sj |(i, j) ∈ E}. A dual join-graph is an edgelabeled edge subgraph of DG that satisfies the connectedness property. A minimal dual join-graph
is a dual join-graph for which none of the edge labels can be further reduced while maintaining the
connectedness property.
Interestingly, there may be many minimal dual join-graphs of the same dual graph. We will
define Iterative Belief Propagation on any dual join-graph. Each node sends a message over an edge
whose scope is identical to the label on that edge. Since Pearl’s algorithm sends messages whose
scopes are singleton variables only, we highlight minimal singleton-label dual join-graphs.
Proposition 3 Any Bayesian network has a minimal dual join-graph where each edge is labeled by
a single variable.
Proof: Consider a topological ordering of the nodes in the acyclic directed graph of the Bayesian
network d = X1 , . . . , Xn . We define the following dual join-graph. Every node in the dual graph
D, associated with pi is connected to node pj , j < i if Xj ∈ pa(Xi ). We label the edge between pj
and pi by variable Xj , namely lij = {Xj }. It is easy to see that the resulting edge-labeled subgraph
of the dual graph satisfies connectedness. (Take the original acyclic graph G and add to each node
its CPT family, namely all the other parents that precede it in the ordering. Since G already satisfies
connectedness so is the minimal graph generated.) The resulting labeled graph is a dual graph with
singleton labels. 2

Example 5 Consider the belief network on 3 variables A, B, C with CPTs 1.P (C|A, B),
2.P (B|A) and 3.P (A), given in Figure 14a. Figure 14b shows a dual graph with singleton labels on the edges. Figure 14c shows a dual graph which is a join-tree, on which belief propagation
can solve the problem exactly in one iteration (two passes up and down the tree).

298

J OIN -G RAPH P ROPAGATION A LGORITHMS

Algorithm Iterative Belief Propagation (IBP)
Input: An edge-labeled dual join-graph DG = (V, E, L) for a Bayesian network B = hX, D, G, P i. Evidence e.
Output: An augmented graph whose nodes include the original CPTs and the messages received from neighbors. Approximations of P (Xi |e), ∀Xi ∈ X. Approximations of P (Fi |e), ∀Fi ∈ B.
Denote by: hvu the message from u to v; ne(u) the neighbors of u in V ; nev (u) = ne(u) − {v}; luv the
label of (u, v) ∈ E; elim(u, v) = scope(u) − scope(v).
• One iteration of IBP
For every node u in DJ in a topological order and back, do:
1. Process observed variables
Assign evidence variables to the each pi and remove them from the labeled edges.
2. Compute and send to v the function:
X
Y
hvu =
(pu ·
hui )
elim(u,v)

{hu
i ,i∈nev (u)}

Endfor
• Compute approximations of P (Fi |e), P (Xi |e):
For every Xi ∈QX let u be the vertex of family Fi in DJ,
P (Fi , e) = α( hu ,u∈ne(i) hui ) · pu ;
P i
P (Xi , e) = α scope(u)−{Xi } P (Fi , e).

Figure 15: Algorithm Iterative Belief Propagation (IBP).
For completeness, we present algorithm IBP, which is a special case of IJGP, in Figure 15. It
is easy to see that one iteration of IBP is time and space linear in the size of the belief network. It
can be shown that when IBP is applied to a minimal singleton-labeled dual graph it coincides with
Pearl’s belief propagation applied directly to the acyclic graph representation. Also, when the dual
join-graph is a tree IBP converges after one iteration (two passes, up and down the tree) to the exact
beliefs.
4.3 Bounded Join-Graph Decompositions
Since we want to control the complexity of join-graph algorithms, we will define it on decompositions having bounded cluster size. If the number of variables in a cluster is bounded by i, the
time and space complexity of processing one cluster is exponential in i. Given a join-graph decomposition D = hJG, χ, ψ, θi, the accuracy and complexity of the (iterative) join-graph propagation
algorithm depends on two different width parameters, defined next.
D EFINITION 12 (external and internal widths) Given an edge-labeled join-graph decomposition
D = hJG, χ, ψ, θi of a network B = hX, D, G, P i, the internal width of D is maxv∈V |χ(v)|, while
the external width of D is the treewidth of JG as a graph.
Using this terminology we can now state our target decomposition more clearly. Given a graph
G, and a bounding parameter i we wish to find a join-graph decomposition D of G whose internal
width is bounded by i and whose external width is minimized. The bound i controls the complexity
of join-graph processing while the external width provides some measure of its accuracy and speed
of convergence, because it measures how close the join-graph is to a join-tree.
299

M ATEESCU , K ASK , G OGATE & D ECHTER

Algorithm Join-Graph Structuring(i)
1. Apply procedure schematic mini-bucket(i).
2. Associate each resulting mini-bucket with a node in the join-graph, the variables of the
nodes are those appearing in the mini-bucket, the original functions are those in the minibucket.
3. Keep the edges created by the procedure (called out-edges) and label them by the regular
separator.
4. Connect the mini-bucket clusters belonging to the same bucket in a chain by in-edges
labeled by the single variable of the bucket.

Figure 16: Algorithm Join-Graph Structuring(i).
Procedure Schematic Mini-Bucket(i)
1. Order the variables from X1 to Xn minimizing (heuristically) induced-width, and associate a bucket for each variable.
2. Place each CPT in the bucket of the highest index variable in its scope.
3. For j = n to 1 do:
Partition the functions in bucket(Xj ) into mini-buckets having at most i variables.
For each mini-bucket mb create a new scope-function (message) f where scope(f ) =
{X|X ∈ mb} − {Xi } and place scope(f) in the bucket of its highest variable. Maintain
an edge between mb and the mini-bucket (created later) of f .

Figure 17: Procedure Schematic Mini-Bucket(i).
We can consider two classes of algorithms. One class is partition-based. It starts from a given
tree-decomposition and then partitions the clusters until the decomposition has clusters bounded by
i. An alternative approach is grouping-based. It starts from a minimal dual-graph-based join-graph
decomposition (where each cluster contains a single CPT) and groups clusters into larger clusters
as long as the resulting clusters do not exceed the given bound. In both methods one should attempt
to reduce the external width of the generated graph-decomposition. Our partition-based approach
inspired by the mini-bucket idea (Dechter & Rish, 1997) is as follows.
Given a bound i, algorithm Join-Graph Structuring(i) applies the procedure Schematic MiniBucket(i), described in Figure 17. The procedure only traces the scopes of the functions that would
be generated by the full mini-bucket procedure, avoiding actual computation. The procedure ends
with a collection of mini-bucket trees, each rooted in the mini-bucket of the first variable. Each of
these trees is minimally edge-labeled. Then, in-edges labeled with only one variable are introduced,
and they are added only to obtain the running intersection property between branches of these trees.
Proposition 4 Algorithm Join-Graph Structuring(i) generates a minimal edge-labeled join-graph
decomposition having bound i.
Proof: The construction of the join-graph specifies the vertices and edges of the join-graph, as well
as the variable and function labels of each vertex. We need to demonstrate that 1) the connectedness
property holds, and 2) that edge-labels are minimal.

300

J OIN -G RAPH P ROPAGATION A LGORITHMS

G: (GFE)

GFE

P(G|F,E)
EF

E: (EBF)

(EF)

EBF

P(E|B,F)

P(F|C,D)

F: (FCD)

(BF)

BF
F

FCD

BF

CD

D: (DB)

(CD)

P(D|B)

CDB
CB

C: (CAB) (CB)

P(C|A,B)

B
CAB
BA

B: (BA)

(AB)

A: (A)

(A)

(B)

P(B|A)

BA
A
P(A)

A

(b)

(a)

Figure 18: Join-graph decompositions.
Connectedness property specifies that for any 2 vertices u and v, if vertices u and v contain
variable X, then there must be a path u, w1 , . . . , wm , v between u and v such that every vertex on
this path contains variable X. There are two cases here. 1) u and v correspond to 2 mini-buckets
in the same bucket, or 2) u and v correspond to mini-buckets in different buckets. In case 1 we
have 2 further cases, 1a) variable X is being eliminated in this bucket, or 1b) variable X is not
eliminated in this bucket. In case 1a, each mini-bucket must contain X and all mini-buckets of the
bucket are connected as a chain, so the connectedness property holds. In case 1b, vertexes u and v
connect to their (respectively) parents, who in turn connect to their parents, etc. until a bucket in
the scheme where variable X is eliminated. All nodes along this chain connect variable X, so the
connectedness property holds. Case 2 resolves like case 1b.
To show that edge labels are minimal, we need to prove that there are no cycles with respect to
edge labels. If there is a cycle with respect to variable X, then it must involve at least one in-edge
(edge connecting two mini-buckets in the same bucket). This means variable X must be the variable
being eliminated in the bucket of this in-edge. That means variable X is not contained in any of the
parents of the mini-buckets of this bucket. Therefore, in order for the cycle to exist, another in-edge
down the bucket-tree from this bucket must contain X. However, this is impossible as this would
imply that variable X is eliminated twice. 2

Example 6 Figure 18a shows the trace of procedure schematic mini-bucket(3) applied to the problem described in Figure 2a. The decomposition in Figure 18b is created by the algorithm graph
structuring. The only cluster partitioned is that of F into two scopes (FCD) and (BF), connected by
an in-edge labeled with F.
A range of edge-labeled join-graphs is shown in Figure 19. On the left side we have a graph
with smaller clusters, but more cycles. This is the type of graph IBP works on. On the right side
we have a tree decomposition, which has no cycles at the expense of bigger clusters. In between,
there could be a number of join-graphs where maximum cluster size can be traded for number of
cycles. Intuitively, the graphs on the left present less complexity for join-graph algorithms because
the cluster size is smaller, but they are also likely to be less accurate. The graphs on the right side
301

M ATEESCU , K ASK , G OGATE & D ECHTER

A

A

A

C

ABC

AB
ABDE

BC

BE

C

A

A

ABC

AB

C

BCE

C
BC

BC

ABDE

ABCDE

DE

CE

CDE

C
DE

CE

CE

CDEF

CDEF

CDEF

CDEF
F

FGH

H

FGH

H

FGI

GH

GI

F

H

GHIJ

H

FGH
H
F

F

F
FG

ABCDE

BCE

C
DE

BCE

FGI

GI

F
F

GH

GHIJ

FGI

GH

GI

GHI
GHIJ

FGHI

GHIJ

more accuracy
less complexity

Figure 19: Join-graphs.
are computationally more complex, because of the larger cluster size, but they are likely to be more
accurate.
4.4 The Inference Power of IJGP
The question we address in this subsection is why propagating the messages iteratively should help.
Why is IJGP upon convergence superior to IJGP with one iteration and superior to MC? One clue
can be provided when considering deterministic constraint networks which can be viewed as “extreme probabilistic networks”. It is known that constraint propagation algorithms, which are analogous to the messages sent by belief propagation, are guaranteed to converge and are guaranteed
to improve with iteration. The propagation scheme of IJGP works similar to constraint propagation
relative to the flat network abstraction of the probability distribution (where all non-zero entries
are normalized to a positive constant), and propagation is guaranteed to be more accurate for that
abstraction at least.
In the following we will shed some light on the IJGP’s behavior by making connections with
the well-known concept of arc-consistency from constraint networks (Dechter, 2003). We show
that: (a) if a variable-value pair is assessed as having a zero-belief, it remains as zero-belief in
subsequent iterations; (b) that any variable-value zero-beliefs computed by IJGP are correct; (c) in
terms of zero/non-zero beliefs, IJGP converges in finite time. We have also empirically investigated
the hypothesis that if a variable-value pair is assessed by IBP or IJGP as having a positive but very
close to zero belief, then it is very likely to be correct. Although the experimental results shown in
this paper do not contradict this hypothesis, some examples in more recent experiments by Dechter,
Bidyuk, Mateescu, and Rollon (2010) invalidate it.

302

J OIN -G RAPH P ROPAGATION A LGORITHMS

4.4.1 IJGP

AND

A RC -C ONSISTENCY

For any belief network we can define a constraint network that captures the assignments having
strictly positive probability. We will show a correspondence between IJGP applied to the belief
network and an arc-consistency algorithm applied to the constraint network. Since arc-consistency
algorithms are well understood, this correspondence not only proves the target claims, but may
provide additional insight into the behavior of IJGP. It justifies the iterative application of belief
propagation, and it also illuminates its “distance” from being complete.
D EFINITION 13 (constraint satisfaction problem) A Constraint Satisfaction Problem (CSP) is a
triple hX, D, Ci, where X = {X1 , . . . , Xn } is a set of variables associated with a set of discretevalued domains D = {D1 , . . . , Dn } and a set of constraints C = {C1 , . . . , Cm }. Each constraint
Ci is a pair hSi , Ri i where Ri is a relation Ri ⊆ DSi defined on a subset of variables Si ⊆ X and
DSi is a Cartesian product of the domains of variables Si . The relation Ri denotes all compatible
tuples of DSi allowed by the constraint. A projection operator π creates a new relation, πSj (Ri ) =
{x|x ∈ DSj and ∃y, y ∈ DSi \Sj and x∪y ∈ Ri }, where Sj ⊆ Si . Constraints can be combined with
the join operator 1, resulting in a new relation, Ri 1 Rj = {x|πSi (x) ∈ Ri and πSj (x) ∈ Rj }.
A solution is an assignment of values to all the variables x = (x1 , . . . , xn ), xi ∈ Di , such that
∀Ci ∈ C, xSi ∈ Ri . The constraint network represents its set of solutions, 1i Ci .
Given a belief network B, we define a flattening of the Bayesian network into a constraint
network called f lat(B), where all the zero entries in a probability table are removed from the corresponding relation. The network f lat(B) is defined over the same set of variables and has the same
set of domain values as B.
D EFINITION 14 (flat network) Given a Bayesian network B = hX, D, G, P i, the flat network
f lat(B) is a constraint network, where the set of variables is X, and for every Xi ∈ X and its
CPT P (Xi |pa(Xi )) ∈ B we define a constraint RFi over the family of Xi , Fi = {Xi } ∪ pa(Xi ) as
follows: for every assignment x = (xi , xpa(Xi ) ) to Fi , (xi , xpa(Xi ) ) ∈ RFi iff P (xi |xpa(Xi ) ) > 0.
T HEOREM 6 Given a belief network B = hX, D, G, P i, where X = {X1 , . . . , Xn }, for any tuple
x = (x1 , . . . , xn ): PB (x) > 0 ⇔ x ∈ sol(f lat(B)), where sol(f lat(B)) is the set of solutions of
the flat constraint network.
Proof: PB (x) > 0 ⇔ Πni=1 P (xi |xpa(Xi ) ) > 0 ⇔ ∀i ∈ {1, . . . , n}, P (xi |xpa(Xi ) ) > 0 ⇔ ∀i ∈
{1, . . . , n}, (xi , xpa(Xi ) ) ∈ RFi ⇔ x ∈ sol(f lat(B)). 2
Constraint propagation is a class of polynomial time algorithms that are at the center of constraint processing techniques. They were investigated extensively in the past three decades and the
most well known versions are arc-, path-, and i-consistency (Dechter, 1992, 2003).
D EFINITION 15 (arc-consistency) (Mackworth, 1977) Given a binary constraint network
(X, D, C), the network is arc-consistent iff for every binary constraint Rij ∈ C, every value v ∈ Di
has a value u ∈ Dj s.t. (v, u) ∈ Rij .

303

M ATEESCU , K ASK , G OGATE & D ECHTER

Note that arc-consistency is defined for binary networks, namely the relations involve at most
two variables. When a binary constraint network is not arc-consistent, there are algorithms that
can process it and enforce arc-consistency. The algorithms remove values from the domains of
the variables that violate arc-consistency until an arc-consistent network is generated. There are
several versions of improved performance arc-consistency algorithms, however we will consider a
non-optimal distributed version, which we call distributed arc-consistency.
D EFINITION 16 (distributed arc-consistency algorithm) The
algorithm
distributed
arcconsistency is a message-passing algorithm over a constraint network. Each node is a variable,
and maintains a current set of viable values Di . Let ne(i) be the set of neighbors of Xi in the
constraint graph. Every node Xi sends a message to any node Xj ∈ ne(i), which consists of the
values in Xj ’s domain that are consistent with the current Di , relative to the constraint Rji that
they share. Namely, the message that Xi sends to Xj , denoted by Dij , is:
Dij ← πj (Rji 1 Di )

(1)

Di ← Di ∩ (1k∈ne(i) Dki )

(2)

and in addition node i computes:

Clearly the algorithm can be synchronized into iterations, where in each iteration every node
computes its current domain based on all the messages received so far from its neighbors (Eq. 2), and
sends a new message to each neighbor (Eq. 1). Alternatively, Equations 1 and 2 can be combined.
The message Xi sends to Xj is:
Dij ← πj (Rji 1 Di 1k∈ne(i) Dki )

(3)

Next we will define a join-graph decomposition for the flat constraint network so that we can
establish a correspondence between the join-graph decomposition of a Bayesian network B and the
join-graph decomposition of its flat network f lat(B). Note that for constraint networks, the edge
labeling θ can be ignored.
D EFINITION 17 (join-graph decomposition of the flat network) Given a join-graph decomposition D = hJG, χ, ψ, θi of a Bayesian network B, the join-graph decomposition Df lat =
hJG, χ, ψf lat i of the flat constraint network f lat(B) has the same underlying graph structure
JG = (V, E) as D, the same variable-labeling of the clusters χ, and the mapping ψf lat maps
each cluster to relations corresponding to CPTs, namely Ri ∈ ψf lat (v) iff CPT pi ∈ ψ(v).
The distributed arc-consistency algorithm of Definition 16 can be applied to the join-graph decomposition of the flat network. In this case, the nodes that exchange messages are the clusters
(namely the elements of the set V of JG). The domain of a cluster of V is the set of tuples of the
join of the original relations in the cluster (namely the domain of cluster u is ./R∈ψf lat (u) R). The
constraints are binary, and involve clusters of V that are neighbors. For two clusters u and v, their
corresponding values tu and tv (which are tuples representing full assignments to the variables in
the cluster) belong to the relation Ruv (i.e., (tu , tv ) ∈ Ru,v ) if the projections over the separator (or
labeling θ) between u and v are identical, namely πθ((u,v)) tu = πθ((u,v)) tv .
304

J OIN -G RAPH P ROPAGATION A LGORITHMS

We define below the algorithm relational distributed arc-consistency (RDAC), that applies distributed arc-consistency to any join-graph decomposition of a constraint network. We call it relational to emphasize that the nodes exchanging messages are in fact relations over the original
problem variables, rather than simple variables as is the case for arc-consistency algorithms.
D EFINITION 18 (relational distributed arc-consistency algorithm: RDAC over a join-graph)
Given a join-graph decomposition of a constraint network, let Ri and Rj be the relations of two
clusters (Ri and Rj are the joins of the respective constraints in each cluster), having the scopes Si
and Sj , such that Si ∩ Sj 6= ∅. The message Ri sends to Rj denoted h(i,j) is defined by:
h(i,j) ← πSi ∩Sj (Ri )

(4)

where ne(i) = {j|Si ∩ Sj 6= ∅} is the set of relations (clusters) that share a variable with Ri . Each
cluster updates its current relation according to:
Ri ← Ri 1 (1k∈ne(i) h(k,i) )

(5)

Algorithm RDAC iterates until there is no change.
Equations 4 and 5 can be combined, just like in Equation 3. The message that Ri sends to Rj
becomes:
h(i,j) ← πSi ∩Sj (Ri 1 (1k∈ne(i) h(k,i) ))

(6)

To establish the correspondence with IJGP, we define the algorithm IJGP-RDAC that applies
RDAC in the same order of computation (schedule of processing) as IJGP.
D EFINITION 19 (IJGP-RDAC algorithm) Given the Bayesian network B = hX, D, G, P i, let
Df lat = hJG, χ, ψf lat , θi be any join-graph decomposition of the flat network f lat(B). The algorithm IJGP-RDAC is applied to the decomposition Df lat of f lat(B), and can be described as
IJGP applied to D, with the following modifications:
Q
1. Instead of , we use 1.
P
2. Instead of , we use π.
3. At end end, we update the domains of variables by:
Di ← Di ∩ πXi ((1v∈ne(u) h(v,u) ) 1 (1R∈ψ(u) R))

(7)

where u is the cluster containing Xi .
Note that in algorithm IJGP-RDAC, we could first merge all constraints in each cluster u into
a single constraint Ru =1R∈ψ(u) R. From our construction, IJGP-RDAC enforces arc-consistency
over the join-graph decomposition of the flat network. When the join-graph Df lat is a join-tree,
IJGP-RDAC solves the problem namely it finds all the solutions of the constraint network.

305

M ATEESCU , K ASK , G OGATE & D ECHTER

Proposition 5 Given the join-graph decomposition Df lat = hJG, χ, ψf lat , θi, JG = (V, E), of
the flat constraint network f lat(B), corresponding to a given join-graph decomposition D of a
Bayesian network B = hX, D, G, P i, the algorithm IJGP-RDAC applied to Df lat enforces arcconsistency over the join-graph Df lat .
Proof: IJGP-RDAC applied to the join-graph decomposition Df lat = hJG, χ, ψf lat , θi, JG =
(V, E), is equivalent to applying RDAC of Definition 18 to a constraint network that has vertices V
as its variables and {1R∈ψ(u) R|u ∈ V } as its relations. 2
Following the properties of convergence of arc-consistency, we can show that:
Proposition 6 Algorithm IJGP-RDAC converges in O(m · r) iterations, where m is the number of
edges in the join-graph and r is the maximum size of a separator Dsep(u,v) between two clusters.
Proof: This follows from the fact messages (which are relations) between clusters in IJGP-RDAC
change monotonically, as tuples are only successively removed from relations on separators. Since
the size of each relation on a separator is bounded by r and there are m edges, no more than O(m·r)
iterations will be needed. 2
In the following we will establish an equivalence between IJGP and IJGP-RDAC in terms of
zero probabilities.
Proposition 7 When IJGP and IJGP-RDAC are applied in the same order of computation, the
messages computed by IJGP are identical to those computed by IJGP-RDAC in terms of zero / nonzero probabilities. That is, h(u,v) (x) 6= 0 in IJGP iff x ∈ h(u,v) in IJGP-RDAC.
Proof: The proof is by induction. The base case is trivially true since messages h in IJGP are initialized to a uniform distribution and messages h in IJGP-RDAC are initialized to complete relations.
The induction step. Suppose that hIJGP
(u,v) is the message sent from u to v by IJGP. We will show
IJGP −RDAC
IJGP −RDAC
that if hIJGP
where h(u,v)
is the message sent by IJGP(u,v) (x) 6= 0, then x ∈ h(u,v)
RDAC from u to v. Assume that the claim holds for all messages received by u from its neighbors.
Let f ∈ clusterv (u) in IJGP and Rf be the corresponding relation in IJGP-RDAC,
and
P
Q t be an asIJGP
signment of values to variables in elim(u, v). We have h(u,v) (x) 6= 0 ⇔ elim(u,v) f f (x) 6= 0
Q
⇔ ∃t, f f (x, t) 6= 0 ⇔ ∃t, ∀f, f (x, t) 6= 0 ⇔ ∃t, ∀f, πscope(Rf ) (x, t) ∈ Rf ⇔ ∃t, πelim(u,v) (1Rf
IJGP −RDAC
IJGP −RDAC
. 2
⇔ x ∈ h(u,v)
πscope(Rf ) (x, t)) ∈ h(u,v)

Next we will show that IJGP computing marginal probability P (Xi = xi ) = 0 is equivalent to
IJGP-RDAC removing xi from the domain of variable Xi .
Proposition 8 IJGP computes P (Xi = xi ) = 0 iff IJGP-RDAC decides that xi 6∈ Di .
Proof: According to Proposition 7 messages computed by IJGP and IJGP-RDAC are identical in
terms of zero probabilities. Let f ∈ cluster(u) in IJGP and Rf be the corresponding relation in
IJGP-RDAC, and t be an assignment of values to variables in χ(u)\Xi . We will show that when
IJGP computes P (Xi = xi ) = 0 (upon convergence), then IJGP-RDAC computes xi 6∈ Di . We
306

J OIN -G RAPH P ROPAGATION A LGORITHMS

Q
Q
P
have P (Xi = xi ) =
f f (xi , t) = 0 ⇔ ∀t, ∃f, f (xi , t) = 0 ⇔
f f (xi ) = 0 ⇔ ∀t,
X\Xi
∀t, ∃Rf , πscope(Rf ) (xi , t) 6∈ Rf ⇔ ∀t, (xi , t) 6∈ (1Rf Rf (xi , t)) ⇔ xi 6∈ Di ∩ πXi (1Rf Rf (xi , t))
⇔ xi 6∈ Di . Since arc-consistency is sound, so is the decision of zero probabilities. 2
Next we will show that P (Xi = xi ) = 0 computed by IJGP is sound.
T HEOREM 7 Whenever IJGP finds P (Xi = xi ) = 0, then the probability P (Xi ) expressed by the
Bayesian network conditioned on the evidence is 0 as well.
Proof: According to Proposition 8, whenever IJGP finds P (Xi = xi ) = 0, the value xi is removed
from the domain Di by IJGP-RDAC, therefore value xi ∈ Di is a no-good of the network f lat(B),
and from Theorem 6 it follows that PB (Xi = xi ) = 0. 2
In the following we will show that the time it takes IJGP to find all P (Xi = xi ) = 0 is bounded.
Proposition 9 IJGP finds all P (Xi = xi ) = 0 in finite time, that is, there exists a number k, such
that no P (Xi = xi ) = 0 will be found after k iterations.
Proof: This follows from the fact that the number of iterations it takes for IJGP to compute P (Xi =
xi ) = 0 is exactly the same number of iterations IJGP-RDAC takes to remove xi from the domain
Di (Proposition 7 and Proposition 8), and the fact the IJGP-RDAC runtime is bounded (Proposition
6). 2
Previous results also imply that IJGP is monotonic with respect to zeros.
Proposition 10 Whenever IJGP finds P (Xi = xi ) = 0, it stays 0 during all subsequent iterations.
Proof: Since we know that relations in IJGP-RDAC are monotonically decreasing as the algorithm
progresses, it follows from the equivalence of IJGP-RDAC and IJGP (Proposition 7) that IJGP is
monotonic with respect to zeros. 2

4.4.2 A F INITE P RECISION P ROBLEM
On finite precision machines there is the danger that an underflow can be interpreted as a zero
value. We provide here a warning that an implementation of belief propagation should not allow
the creation of zero values by underflow. We show an example in Figure 20 where IBP’s messages
converge in the limit (i.e., in an infinite number of iterations), but they do not stabilize in any finite
number of iterations. If all the nodes Hk are set to value 1, the belief for any of the Xi variables as a
function of iteration is given in the table in Figure 20. After about 300 iterations, the finite precision
of our computer is not able to represent the value for Bel(Xi = 3), and this appears to be zero,
yielding the final updated belief (.5, .5, 0), when in fact the true updated belief should be (0, 0, 1).
Notice that (.5, .5, 0) cannot be regarded as a legitimate fixed point for IBP. Namely, if we would
initialize IBP with the values (.5, .5, 0), then the algorithm would maintain them, appearing to have
a fixed point, but initializing IBP with zero values cannot be expected to be correct. When we

307

M ATEESCU , K ASK , G OGATE & D ECHTER

X1

Prior for Xi

H3

H1

X3

X2

Xi

P ( Xi )

1

.45

2

.45

3

.1

H2
Hk Xi

CPT for Hk

Xj

P ( Hk | Xi , Xj )

1

1

2

1

1

2

1

1

1

3

3

1

1

… …

0

#iter

Bel(Xi = 1) Bel(Xi = 2) Bel(Xi = 3)

1

.45

.45

.1

2

.49721

.49721

.00545

3

.49986

.49986

.00027

100

…

…

1e-129

200

…

…

1e-260

300

.5

.5

0

True
belief

0

0

1

Figure 20: Example of a finite precision problem.
initialize with zeros we forcibly introduce determinism in the model, and IBP will always maintain
it afterwards.
However, this example does not contradict our theory because, mathematically, Bel(Xi = 3)
never becomes a true zero, and IBP never reaches a quiescent state. The example shows that a close
to zero belief network can be arbitrarily inaccurate. In this case the inaccuracy seems to be due to
the initial prior belief which are so different from the posterior ones.
4.4.3 ACCURACY OF IBP ACROSS B ELIEF D ISTRIBUTION
We present an empirical evaluation of the accuracy of IBP’s prediction for the range of belief distribution from 0 to 1. These results also extend to IJGP. In the previous section, we proved that zero
values inferred by IBP are correct, and we wanted to test the hypothesis that this property extends
to  small beliefs (namely, that are very close to zero). That is, if IBP infers a posterior belief close
to zero, then it is likely to be correct. The results presented in this paper seem to support the hypothesis, however new experiments by Dechter et al. (2010) show that it is not true in general. We
do not have yet a good characterization of the cases when the hypothesis is confirmed.
To test this hypothesis, we computed the absolute error of IBP per intervals of [0, 1]. For a given
interval [a, b], where 0 ≤ a < b ≤ 1, we use measures inspired from information retrieval: Recall
Absolute Error and Precision Absolute Error.
Recall is the absolute error averaged over all the exact posterior beliefs that fall into the interval
[a, b]. For Precision, the average is taken over all the approximate posterior belief values computed
by IBP to be in the interval [a, b]. Intuitively, Recall([a,b]) indicates how far the belief computed
by IBP is from the exact, when the exact is in [a, b]; Precision([a,b]) indicates how far the exact is
from IBP’s prediction, when the value computed by IBP is in [a, b].
Our experiments show that the two measures are strongly correlated. We also show the histograms of distribution of belief for each interval, for the exact and for IBP, which are also strongly
correlated. The results are given in Figures 21 and 22. The left Y axis corresponds to the histograms
(the bars), the right Y axis corresponds to the absolute error (the lines).
We present results for two classes of problems: coding networks and grid network. All problems
have binary variables, so the graphs are symmetric about 0.5 and we only show the interval [0, 0.5].
The number of variables, number of iterations and induced width w* are reported for each graph.

308

J OIN -G RAPH P ROPAGATION A LGORITHMS

Recall Abs. Error

noise = 0.20

noise = 0.40

0.4

Absolute Error

0.45

0.3

0
0.35

0.35

0.01

0.2

0%

0.02

0.25

0%

0.03

0.1

0%

0.04

0.15

5%

Precision Abs. Error
0.05

0

10%

5%
0.4

10%

5%

0.45

10%

0.3

15%

0.2

20%

15%

0.25

20%

15%

0.1

20%

0.15

25%

0

30%

25%

0.05

30%

25%

0.4

30%

0.45

35%

0.3

40%

35%

0.35

40%

35%

0.2

40%

0.25

45%

0.1

45%

0.15

45%

0

50%

0.05

IBP Histogram
50%

0.05

Percentage

Exact Histogram
50%

noise = 0.60

Figure 21: Coding, N=200, 1000 instances, w*=15.
Recall Abs. Error

evidence = 0

evidence = 10

0.004
0.003
0.002
0.001

Absolute Error

0.005

0.4

0.45

0.3

0.35

0.2

0.25

0.1

0.15

0
0

0.4

0.45

0.3

0.35

0%
0.2

0%
0.25

5%
0.1

10%

5%
0.15

10%

0

15%

0.05

20%

15%

0.4

20%

0.45

25%

0.3

30%

25%

0.35

30%

0.2

35%

0.25

40%

35%

0.1

40%

0.15

45%

0

45%

Precision Abs. Error

0.05

IBP Histogram

50%

0.05

Percentage

Exact Histogram
50%

evidence = 20

Figure 22: 10x10 grids, 100 instances, w*=15.
Coding networks IBP is famously known to have impressive performance on coding networks.
We tested on linear block codes, with 50 nodes per layer and 3 parent nodes. Figure 21 shows the
results for three different values of channel noise: 0.2, 0.4 and 0.6. For noise 0.2, all the beliefs
computed by IBP are extreme. The Recall and Precision are very small, of the order of 10−11 . So,
in this case, all the beliefs are very small ( small) and IBP is able to infer them correctly, resulting
in almost perfect accuracy (IBP is indeed perfect in this case for the bit error rate). When the noise
is increased, the Recall and Precision tend to get closer to a bell shape, indicating higher error for
values close to 0.5 and smaller error for extreme values. The histograms also show that less belief
values are extreme as the noise is increased, so all these factors account for an overall decrease
in accuracy as the channel noise increases. These networks are examples with a large number of
-small probabilities and IBP is able to infer them correctly (absolute error is small).
Grid networks We present results for grid networks in Figure 22. Contrary to the case of coding
networks, the histograms show higher concentration of beliefs around 0.5. However, the accuracy is
still very good for beliefs close to zero. The absolute error peaks close to 0 and maintains a plateau,
as evidence is increased, indicating less accuracy for IBP.

5. Experimental Evaluation
As we anticipated in the summary of Section 3, and as can be clearly seen now by the structuring
of a bounded join-graph, there is a close relationship between the mini-clustering algorithm MC(i)
309

M ATEESCU , K ASK , G OGATE & D ECHTER

IBP
#it
1

5

10

MC

#evid
0
5
10
0
5
10
0
5
10
0
5
10

0.02988
0.06178
0.08762
0.00829
0.05182
0.08039
0.00828
0.05182
0.08040

Absolute error
IJGP
i=2
i=5
0.03055 0.02623
0.04434 0.04201
0.05777 0.05409
0.00636 0.00592
0.00886 0.00886
0.01155 0.01073
0.00584 0.00514
0.00774 0.00732
0.00892 0.00808

IBP
i=8
0.02940
0.04554
0.05910
0.00669
0.01123
0.01399
0.00495
0.00708
0.00855

0.04044 0.04287 0.03748
0.05303 0.05171 0.04250
0.06033 0.05489 0.04266

0.06388
0.15005
0.23777
0.01726
0.12589
0.21781
0.01725
0.12590
0.21782

Relative error
IJGP
i=5
0.05677
0.12056
0.14278
0.01239
0.01965
0.02553
0.01069
0.01628
0.01907

i=2
0.15694
0.12340
0.18071
0.01326
0.01967
0.03014
0.01216
0.01727
0.02101

IBP
i=8
0.07153
0.11154
0.15686
0.01398
0.02494
0.03279
0.01030
0.01575
0.02005

0.08811 0.09342 0.08117
0.12375 0.11775 0.09596
0.14702 0.13219 0.10074

0.00213
0.00812
0.01547
0.00021
0.00658
0.01382
0.00021
0.00658
0.01382

KL distance
IJGP
i=5
0.00208
0.00478
0.00768
0.00015
0.00026
0.00042
0.00010
0.00017
0.00024

i=2
0.00391
0.00582
0.00915
0.00014
0.00024
0.00055
0.00012
0.00018
0.00028

IBP
i=8
0.00277
0.00558
0.00899
0.00018
0.00044
0.00073
0.00010
0.00016
0.00029

0.00403 0.00435 0.00369
0.00659 0.00636 0.00477
0.00841 0.00729 0.00503

0.0017
0.0013
0.0013
0.0066
0.0060
0.0048
0.0130
0.0121
0.0109

Time
IJGP
i=5
0.0058
0.0052
0.0036
0.0226
0.0185
0.0138
0.0436
0.0355
0.0271

i=2
0.0036
0.0040
0.0040
0.0145
0.0120
0.0100
0.0254
0.0223
0.0191

i=8
0.0295
0.0200
0.0121
0.1219
0.0840
0.0536
0.2383
0.1639
0.1062

0.0159 0.0173 0.0552
0.0146 0.0158 0.0532
0.0119 0.0143 0.0470

Table 4: Random networks: N=50, K=2, C=45, P=3, 100 instances, w*=16.

and IJGP(i). In particular, one iteration of IJGP(i) is similar to MC(i). MC sends messages up and
down along the clusters that form a set of trees. IJGP has additional connections that allow more
interaction between the mini-clusters of the same cluster. Since this is a cyclic structure, iterating is
facilitated, with its virtues and drawbacks.s
In our evaluation of IJGP(i), we focus on two different aspects: (a) the sensitivity of parametric
IJGP(i) to its i-bound and to the number of iterations; (b) a comparison of IJGP(i) with publicly
available state-of-the-art approximation schemes.
5.1 Effect of i-bound and Number of Iterations
We tested the performance of IJGP(i) on random networks, on M-by-M grids, on the two benchmark CPCS files with 54 and 360 variables, respectively and on coding networks. On each type
of networks, we ran IBP, MC(i) and IJGP(i), while giving IBP and IJGP(i) the same number of
iterations.
We use the partitioning method described in Section 4.3 to construct a join-graph. To determine
the order of message computation, we recursively pick an edge (u,v), such that node u has the fewest
incoming messages missing.
For each network except coding, we compute the exact solution and compare the accuracy
using the absolute and relative error, as before, as well as the KL (Kullback-Leibler) distance Pexact (X = a) · log(Pexact (X = a)/Papproximation (X = a)) averaged over all values, all variables
and all problems. For coding networks we report the Bit Error Rate (BER) computed as described
in Section 3.2. We also report the time taken by each algorithm.
The random networks were generated using parameters (N,K,C,P), where N is the number of
variables, K is their domain size, C is the number of conditional probability tables (CPTs) and P
is the number of parents in each CPT. Parents in each CPT are picked randomly and each CPT
is filled randomly. In grid networks, N is a square number and each CPT is filled randomly. In
each problem class, we also tested different numbers of evidence variables. As before, the coding
networks are from the class of linear block codes, where σ is the channel noise level. Note that we
are limited to relatively small and sparse problem instances because our evaluation measures are
based on comparing against exact figures.
Random networks results for networks having N=50, K=2, C=45 and P=3 are given in Table 4
and in Figures 23 and 24. For IJGP(i) and MC(i) we report 3 different values of i-bound: 2, 5, 8. For
IBP and IJGP(i) we report results for 3 different numbers of iterations: 1, 5, 10. We report results
310

J OIN -G RAPH P ROPAGATION A LGORITHMS

Random networks, N=50, K=2, P=3, evid=5, w*=16
0.010

IJGP 1 it
IJGP 2 it
IJGP 3 it
IJGP 5 it
IJGP 10 it
IJGP 15 it
IJGP 20 it
MC
IBP 1 it
IBP 2 it
IBP 3 it
IBP 5 it
IBP 10 it

0.008

KL distance

0.006

0.004

0.002

0.000

0

1

2

3

4

5

6

7

8

9

10

11

i-bound

(a) Performance vs. i-bound.
Random networks, N=50, K=2, P=3, evid=5, w*=16
0.010

IBP
IJGP(2)
IJGP(10)

0.008

KL distance

0.006

0.004

0.002

0.000

0

5

10

15

20

25

30

35

Number of iterations

(b) Convergence with iterations.

Figure 23: Random networks: KL distance.
for 3 different numbers of evidence: 0, 5, 10. From Table 4 and Figure 23a we see that IJGP(i)
is always better than IBP (except when i=2 and number of iterations is 1), sometimes by an order
of magnitude, in terms of absolute error, relative error and KL distance. IBP rarely changes after 5
iterations, whereas IJGP(i)’s solution can be improved with more iterations (up to 15-20). As theory
predicted, the accuracy of IJGP(i) for one iteration is about the same as that of MC(i). But IJGP(i)
improves as the number of iterations increases, and is eventually better than MC(i) by as much as
an order of magnitude, although it clearly takes more time, especially when the i-bound is large.

311

M ATEESCU , K ASK , G OGATE & D ECHTER

Random networks, N=50, K=2, P=3, evid=5, w*=16
1.0
IJPG 1 it
IJGP 2 it
IJGP 3 it
IJGP 5 it
IJGP 10 it
IJGP 15 it
IJGP 20 it
MC
IBP 1 it
IBP 20 it

Time (seconds)

0.8

0.6

0.4

0.2

0.0

0

1

2

3

4

5

6

7

8

9

10

11

i-bound

Figure 24: Random networks: Time.
IBP
#it
1

5

10

MC

#evid
0
5
10
0
5
10
0
5
10
0
5
10

0.03524
0.05375
0.07094
0.00358
0.03224
0.05503
0.00352
0.03222
0.05503

Absolute error
IJGP
i=2
i=5
0.05550 0.04292
0.05284 0.04012
0.05453 0.04304
0.00393 0.00325
0.00379 0.00319
0.00364 0.00316
0.00352 0.00232
0.00357 0.00248
0.00347 0.00239

IBP
i=8
0.03318
0.03661
0.03966
0.00284
0.00296
0.00314
0.00136
0.00149
0.00141

0.05827 0.04036 0.01579
0.05973 0.03692 0.01355
0.05866 0.03416 0.01075

0.08075
0.16380
0.23624
0.00775
0.11299
0.19403
0.00760
0.11295
0.19401

Relative error
IJGP
i=5
0.10252
0.09889
0.12492
0.00702
0.00710
0.00756
0.00502
0.00549
0.00556

i=2
0.13533
0.13225
0.14588
0.00849
0.00844
0.00841
0.00760
0.00796
0.00804

IBP
i=8
0.07904
0.09116
0.12202
0.00634
0.00669
0.01313
0.00293
0.00330
0.00328

0.13204 0.08833 0.03440
0.13831 0.08213 0.03001
0.14120 0.07791 0.02488

0.00289
0.00725
0.01232
0.00005
0.00483
0.00994
0.00005
0.00483
0.00994

KL distance
IJGP
i=5
0.00602
0.00570
0.00681
0.00007
0.00007
0.00009
0.00003
0.00003
0.00003

i=2
0.00859
0.00802
0.00905
0.00006
0.00006
0.00006
0.00005
0.00005
0.00005

IBP
i=8
0.00454
0.00549
0.00653
0.00010
0.00010
0.00019
0.00001
0.00002
0.00001

0.00650 0.00387 0.00105
0.00696 0.00348 0.00099
0.00694 0.00326 0.00075

0.0010
0.0016
0.0013
0.0049
0.0053
0.0036
0.0090
0.0096
0.0090

Time
IJGP
i=5
0.0106
0.0092
0.0072
0.0347
0.0309
0.0271
0.0671
0.0558
0.0495

i=2
0.0053
0.0041
0.0038
0.0152
0.0131
0.0127
0.0277
0.0246
0.0223

i=8
0.0426
0.0315
0.0256
0.1462
0.1127
0.0913
0.2776
0.2149
0.1716

0.0106 0.0142 0.0382
0.0102 0.0130 0.0342
0.0099 0.0116 0.0321

Table 5: 9x9 grid, K=2, 100 instances, w*=12.

Figure 23a shows a comparison of all algorithms with different numbers of iterations, using the
KL distance. Because the network structure changes with different i-bounds, we do not necessarily
see monotonic improvement of IJGP with i-bound for a given number of iterations (as is the case
with MC). Figure 23b shows how IJGP converges with more iterations to a smaller KL distance
than IBP. As expected, the time taken by IJGP (and MC) varies exponentially with the i-bound (see
Figure 24).
Grid networks results with networks of N=81, K=2, 100 instances are very similar to those of
random networks. They are reported in Table 5 and in Figure 25, where we can see the impact
of having evidence (0 and 5 evidence variables) on the algorithms. IJGP at convergence gives the
best performance in both cases, while IBP’s performance deteriorates with more evidence and is
surpassed by MC with i-bound 5 or larger.
CPCS networks results with CPCS54 and CPCS360 are given in Table 6 and Figure 26, and are
even more pronounced than those of random and grid networks. When evidence is added, IJGP(i)
is more accurate than MC(i), which is more accurate than IBP, as can be seen in Figure 26a.
Coding networks results are given in Table 7. We tested on large networks of 400 variables, with
treewidth w*=43, with IJGP and IBP set to run 30 iterations (this is more than enough to ensure
312

J OIN -G RAPH P ROPAGATION A LGORITHMS

Grid network, N=81, K=2, evid=5, w*=12
0.010
IJGP 1 it
IJGP 2 it
IJGP 3 it
IJGP 5 it
IJGP 10 it
MC
IBP 1 it
IBP 2 it
IBP 3 it
IBP 5 it
IBP 10 it

0.008

KL distance

0.006

0.004

0.002

0.000

0

1

2

3

4

5

6

7

8

9

10

11

i-bound

(a) Performance vs. i-bound.
Grid network, N=81, K=2, evid=5, w*=12
7e-5
IJGP 20 iterations
(at convergence)
6e-5

KL distance

5e-5

4e-5

3e-5

2e-5

1e-5

0
1

2

3

4

5

6

7

8

9

10

11

i-bound

(b) Fine granularity for KL.

Figure 25: Grid 9x9: KL distance.
convergence). IBP is known to be very accurate for this class of problems and it is indeed better
than MC. However we notice that IJGP converges to slightly smaller BER than IBP even for small
values of the i-bound. Both the coding network and CPCS360 show the scalability of IJGP for large
size problems. Notice that here the anytime behavior of IJGP is not clear.
In summary, we see that IJGP is almost always superior to both IBP and MC(i) and is sometimes
more accurate by several orders of magnitude. One should note that IBP cannot be improved with
more time, while MC(i) requires a large i-bound for many hard and large networks to achieve
reasonable accuracy. There is no question that the iterative application of IJGP is instrumental to its
success. In fact, IJGP(2) in isolation appears to be the most cost-effective variant.

313

M ATEESCU , K ASK , G OGATE & D ECHTER

IBP
#it

1

5

10

MC

1
10
20
MC

#evid

Absolute error
IJGP
i=2
i=5

Relative error
IJGP
i=5

IBP
i=8

i=2
0.02716
0.05736
0.08475
0.00064
0.04067
0.07302
0.00064
0.04067
0.07302

0.08966
0.09007
0.09156
0.00033
0.00124
0.00215
0.00018
0.00078
0.00123
0.05648
0.05687
0.06002

KL distance
IJGP
i=5

IBP
i=8

CPCS54
0.07761 0.05616
0.07676 0.05856
0.08246 0.06687
0.00255 0.00225
0.00194 0.00203
0.00298 0.00302
0.00029 0.00031
0.00071 0.00080
0.00109 0.00122
0.05128 0.03047
0.05314 0.03713
0.05318 0.03409

Time
IBP

i=2
0.00041
0.00199
0.00357
7.75e-7
0.00161
0.00321
7.75e-7
0.00161
0.00321

0.00583
0.00573
0.00567
0.00000
0.00000
0.00001
0.0000
0.00000
4.0e-6
0.00218
0.00201
0.00216

0.00512
0.00493
0.00506
0.00002
0.00001
0.00003
0.00000
0.00000
3.0e-6
0.00171
0.00186
0.00177

i=8
0.00378
0.00366
0.00390
0.00001
0.00001
0.00002
0.00000
0.00000
4.0e-6
0.00076
0.00098
0.00091

0.0097
0.0072
0.005
0.0371
0.0337
0.0290
0.0736
0.0633
0.0575

i=2

IJGP
i=5

i=8

0.0137
0.0094
0.0047
0.0334
0.0215
0.0144
0.0587
0.0389
0.0251
0.0144
0.0103
0.0094

0.0146
0.0087
0.0052
0.0384
0.0260
0.0178
0.0667
0.0471
0.0297
0.0125
0.0126
0.0090

0.0275
0.0169
0.0115
0.0912
0.0631
0.0378
0.1720
0.1178
0.0723
0.0333
0.0346
0.0295

0
5
10
0
5
10
0
5
10
0
5
10

0.01324
0.02684
0.03915
0.00031
0.01874
0.03348
0.00031
0.01874
0.03348

0.03747
0.03739
0.03843
0.00016
0.00058
0.00101
0.00009
0.00037
0.00058
0.02721
0.02702
0.02825

0.03183
0.03124
0.03426
0.00123
0.00092
0.00139
0.00014
0.00034
0.00051
0.02487
0.02522
0.02504

0.02233
0.02337
0.02747
0.00110
0.00098
0.00144
0.00015
0.00038
0.00057
0.01486
0.01760
0.01600

10
20
10
20
10
20
10
20

0.26421
0.26326
0.01772
0.02413
0.01772
0.02413

0.14222
0.12867
0.00694
0.00466
0.00003
0.00001
0.03389
0.02715

0.13907
0.12937
0.00121
0.00115
3.0e-6
9.0e-6
0.01984
0.01543

CPCS360
0.14334 7.78167 2119.20 2132.78 2133.84 0.17974 0.09297 0.09151 0.09255 0.7172 0.5486 0.5282 0.4593
0.13665 370.444 28720.38 30704.93 31689.59 0.17845 0.08212 0.08269 0.08568 0.6794 0.5547 0.5250 0.4578
0.00258 1.06933 6.07399 0.01005 0.04330 0.017718 0.00203 0.00019 0.00116 7.2205 4.7781 4.5191 3.7906
0.00138 62.99310 26.04308 0.00886 0.01353 0.02027 0.00118 0.00015 0.00036 7.0830 4.8705 4.6468 3.8392
3.0e-6
1.06933 0.00044
8.0e-6
7.0e-6
0.01771
5.0e-6
0.0
0.0
14.4379 9.5783 9.0770 7.6017
9.0e-6
62.9931 0.00014 0.00013 0.00004 0.02027
0.0
0.0
0.0
13.6064 9.4582 9.0423 7.4453
0.01402
0.65600 0.20023 0.11990
0.01299 0.00590 0.00390
2.8077 2.7112 2.5188
0.00957
0.81401 0.17345 0.09113
0.01007 0.00444 0.00234
2.8532 2.7032 2.5297

Table 6: CPCS54 50 instances, w*=15; CPCS360 10 instances, w*=20.

σ
0.22 IJGP
MC
0.28 IJGP
MC
0.32 IJGP
MC
0.40 IJGP
MC
0.51 IJGP
MC
0.65 IJGP
MC

2
0.00005
0.00501
0.00062
0.02170
0.00238
0.04018
0.01202
0.08726
0.07664
0.15396
0.19070
0.21890

IJGP 0.36262
MC 0.25281

Bit Error Rate
i-bound
4
6
8
0.00005 0.00005 0.00005
0.00800 0.00586 0.00462
0.00062 0.00062 0.00062
0.02968 0.02492 0.02048
0.00238 0.00238 0.00238
0.05004 0.04480 0.03878
0.01188 0.01194 0.01210
0.09762 0.09272 0.08766
0.07498 0.07524 0.07578
0.16048 0.15710 0.15452
0.19056 0.19016 0.19030
0.22056 0.21928 0.21904
Time
0.41695 0.86213 2.62307
0.21816 0.31094 0.74851

10
0.00005
0.00392
0.00062
0.01840
0.00238
0.03558
0.01192
0.08334
0.07554
0.15180
0.19056
0.21830

IBP
0.00005
0.00064
0.00242
0.01220
0.07816
0.19142

9.23610 0.019752
2.33257

Table 7: Coding networks: N=400, P=4, 500 instances, 30 iterations, w*=43.

5.2 Comparing IJGP with Other Algorithms
In this section we provide a comparison of IJGP with state-of-the-art publicly available schemes.
The comparison is based on a recent evaluation of algorithms performed at the Uncertainty in AI
2008 conference4 . We will present results on solving the belief updating task (also called the task
of computing posterior node marginals - MAR). We first give a brief overview of the schemes that
we experimented and compared with.
1. EDBP - Edge Deletion for Belief Propagation
4. Complete results are available at http://graphmod.ics.uci.edu/uai08/Evaluation/Report.

314

J OIN -G RAPH P ROPAGATION A LGORITHMS

CPCS360, evid=10, w*=20
0.20
IJGP 1 it
IJGP 10 it
IJGP 20 it
MC
IBP 1 it
IBP 10 it
IBP 20 it

0.18
0.16

KL distance

0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00

0

1

2

3

4

5

6

7

8

9

10

11

i-bound

(a) Performance vs. i-bound.
CPCS360, evid=10, w*=20
6e-6
IJGP 20 iterations
(at convergence)
5e-6

KL distance

4e-6

3e-6

2e-6

1e-6

0

1

2

3

4

5

6

7

8

9

10

11

i-bound

(b) Fine granularity for KL.

Figure 26: CPCS360: KL distance.
EDBP (Choi & Darwiche, 2006a, 2006b) is an approximation algorithm for Belief Updating.
It solves exactly a simplified version of the original problem, obtained by deleting some of
the edges of the problem graph. Edges to be deleted are selected based on two criteria:
quality of approximation and complexity of computation (tree-width reduction). Information
loss from lost dependencies is compensated for by introducing auxiliary network parameters.
This method corresponds to Iterative Belief Propagation (IBP) when enough edges are deleted
to yield a poly-tree, and corresponds to generalized BP otherwise.
2. TLSBP - A truncated Loop series Belief propagation algorithm

315

M ATEESCU , K ASK , G OGATE & D ECHTER

TLSBP is based on the loop series expansion formula of Chertkov and Chernyak (2006) which
specifies a series of terms that need to be added to the solution output by BP so that the exact
solution can be recovered. This series is basically a sum over all so-called generalized loops in
the graph. Unfortunately, because the number of these generalized loops can be prohibitively
large, the series is of little value. The idea in TLSBP is to truncate the series by decomposing
all generalized loops into simple and smaller loops, thus limiting the number of loops to be
summed. In our evaluation, we used an implementation of TLSBP available from the work
of Gomez, Mooji, and Kappen (2007). The implementation can handle binary networks only.
3. EPIS - Evidence Pre-propagation Importance Sampling
EPIS (Yuan & Druzdzel, 2003) is an importance sampling algorithm for Belief Updating. It is
well known that sampling algorithms perform poorly when presented with unlikely evidence.
However, when samples are weighted by an importance function, good approximation can be
obtained. This algorithm computes an approximate importance function using loopy belief
propagation and -cutoff heuristic. We used an implementation of EPIS available from the
authors. The implementation works on Bayesian networks only.
4. IJGP - Iterative Join-Graph Propagation
In the evaluation, IJGP(i) was first run with i=2, until convergence, then with i=3, until convergence, etc. until i= treewidth (when i-bound=treewidth, the join-graph becomes a join-tree
and IJGP becomes exact). As preprocessing, the algorithm performed SAT-based variable domain pruning by converting zero probabilities in the problem to a SAT problem and performing singleton-consistency enforcement. Because the problem size may reduce substantially,
in some cases, this preprocessing step may have a significant impact on the time-complexity
of IJGP, amortized over the increasing i-bound. However, for a given i-bound, this step improves the accuracy of IJGP only marginally.
5. SampleSearch
SampleSearch (Gogate & Dechter, 2007) is a specialized importance sampling scheme for
graphical models that contain zero probabilities in their CPTs. On such graphical models,
importance sampling suffers from the rejection problem in that it generates a large number
of samples which have zero weight. SampleSearch circumvents the rejection problem by
sampling from the backtrack-free search space in which every assignment (sample) is guaranteed to have non-zero weight. The backtrack-free search space is constructed on the fly by
interleaving sampling with backtracking style search. Namely, when a sample is supposed
to be rejected because its weight is zero, the algorithm continues instead with systematic
backtracking search, until a non zero weight sample is found. For the evaluation version,
the importance distribution of SampleSearch was constructed from the output of IJGP with
i-bound of 3. For more information on how the importance distribution is constructed from
the output of IJGP, see the work by Gogate (2009).
The evaluation was conducted on the following benchmarks (see footnote 4 for details):
1. UAI06-MPE - from UAI-06, 57 instances, Bayesian networks (40 instances were used).
2. UAI06-PE - from UAI-06, 78 instances, Bayesian networks (58 instances were used).
316

J OIN -G RAPH P ROPAGATION A LGORITHMS

IJGP
EDBP
TLSBP
EPIS
SampleSearch

WCSPs BN2O Grids Linkage Promedas UAI06-MPE UAI06-PE Relational
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
Table 8: Scope of our experimental study.
Score vs KL distance
1
Score vs KL distance
0.9
0.8

Score

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0.2

0.4

0.6

0.8

1

KL distance

Figure 27: Score as a function of KL distance.
3. Relational Bayesian networks - constructed from the Primula tool, 251 instances, binary variables, large networks with large tree-width, but with high levels of determinism (30 instances
were used).
4. Linkage networks - 22 instances, tree-width 20-35, Markov networks (5 instances were used).
5. Grids - from 12x12 to 50x50, 320 instances, treewidth 12-50.
6. BN2O networks - Two-layer Noisy-OR Bayesian networks, 18 instances, binary variables, up
to 55 variables, treewidth 24-27.
7. WCSPs - Weighted CSPs, 97 instances, Markov networks (18 instances were used).
8. Promedas - real-world medical diagnosis, 238 instances, tree-width 1-60, Markov networks
(46 instances were used).
√
Table 8 shows the scope of our experimental study. A indicates that the solver was able to
√
handle the benchmark type and therefore evaluated on it while a lack of a indicates otherwise.
We measure the performance of the algorithms in terms of a KL-distance based score. Formally,
the score of a solver on a problem instance is equal to 10−avgkld where avgkld is the average KL
distance between the exact marginal (which was computed using the UCLA Ace solver, see Chavira
& Darwiche, 2008) and the approximate marginal output by the solver. If a solver does not output a
solution, we consider its KL-distance to be ∞. A score lies between 0 and 1, with 1 indicating that
the solver outputs exact solution while 0 indicating that the solver either does not output a solution
or has infinite average KL distance. Figure 27 shows the score as a function of KL distance.
317

M ATEESCU , K ASK , G OGATE & D ECHTER

In Figures 28-35 we report the results of experiments with each of the problem sets. Each
solver has a timeout of 20 minutes on each problem instance; when solving a problem, each solver
periodically outputs the best solution found so far. Using this, we can compute, for each solver, at
any point in time, the total sum of its scores over all problem instances in a particular set, called
SumScore(t). On the horizontal axis, we have the time and on the vertical axis, the SumScore(t).
The higher the curve of a solver is, the better (the higher the score).
In summary, we see that IJGP shows the best performance on the first four classes of networks
(UAI-MPE, UAI-PE, Relational and Linkage), it is tied with other algorithms on two classes (Grid
and BN2O), and is surpassed by EDBP on the last two classes (WCSPs and Promedas). EPIS and
SampleSearch, which are importance sampling schemes, are often inferior to IJGP and EDBP. In
theory, the accuracy of these importance sampling schemes should improve with time. However,
the rate of improvement is often unknown in practice. On the hard benchmarks that we evaluated
on, we found that this rate is quite small and therefore the improvement cannot be discerned from
the Figures. We discuss the results in detail below.
As mentioned earlier, TLSBP works only on binary networks (i.e., two variables per function)
and therefore it was not evaluated on WCSPs, Linkage, UAI06-MPE and UAI06-PE benchmarks.
The UAI-MPE and UAI-PE instances were used in the UAI 2006 evaluation of exact solvers (for
details see the report by Bilmes & Dechter, 2006). Exact marginals are available on 40 UAI-MPE
instances and 58 UAI-PE instances. The results for UAI-MPE and UAI-PE instances are shown
in Figures 28 and 29 respectively. IJGP is the best performing scheme on both benchmark sets
reaching a SumScore very close to the maximum possible value in both cases after about 2 minutes
of CPU time. EDBP and SampleSearch are second best in both cases.
Relational network instances are generated by grounding the relational Bayesian networks using
the Primula tool (Chavira, Darwiche, & Jaeger, 2006). Exact marginals are available only on 30
out of the submitted 251 instances. From Figure 30, we observe that IJGP’s SumScore steadily
increases with time and reaches a value very close to the maximum possible score of 30 after about
16 minutes of CPU time. SampleSearch is the second best performing scheme. EDBP, TLSBP and
EPIS perform quite poorly on these instances reaching the SumScore of 10, 13 and 13 respectively
after 20 minutes of CPU time.
The Linkage instances are generated by converting linkage analysis data into a Markov network
using the Superlink tool (Fishelson & Geiger, 2003). Exact marginals are available only on 5 out of
the 22 instances. The results are shown in Figure 31. After about one minute of CPU time, IJGP’s
SumScore is close to 5 which remains steady thereafter while EDBP only reaches a SumScore of 2
in 20 minutes. SampleSearch is the second best performing scheme while EDBP is third best.
The results on Grid networks are shown in Figure 32. The sink node of the grid is the evidence
node. The deterministic ratio p is a parameter specifying the fraction of nodes that are deterministic,
that is, whose values are determined given the values of their parents. The evaluation benchmark
set consists of 30 instances having p = 50%,75% and 90% with exact marginals available on 27
instances only. EPIS, IJGP, SampleSearch and EDBP are in a close tie on this network, while
TLSBP has the lowest performance. While hard to see, EPIS is just slightly the best performing
scheme, IJGP is the second best followed by SampleSearch and EDBP. On this instances IJGP’s
SumScore increases steadily with time.
The results on BN2O instances appear in Figure 33. This is again a very close tie, in this case
of all five algorithms. IJGP has a minuscule decrease of SumScore with time from 17.85 to 17.7.
Although in general an improvement in accuracy is expected for IJGP with higher i-bound, it is not
318

J OIN -G RAPH P ROPAGATION A LGORITHMS

Approximate Mar Problem Set uai06-mpe

40
35

Sum Score

30
25
20
15
10
5
0
0

2

4

6

8

10

12

14

16

18

20

Time in minutes
SampleSearch

IJGP

EDBP

EPIS

Figure 28: Results on UAI-MPE networks. TLSBP is not plotted because it cannot handle UAIMPE benchmarks.

Approximate Mar Problem Set uai06-pe

50

Sum Score

40

30

20

10

0
0

2

4

6

8

10

12

14

16

18

20

Time in minutes
SampleSearch

IJGP

EDBP

EPIS

Figure 29: Results on UAI-PE networks. TLSBP is not plotted because it cannot handle UAI-PE
benchmarks.

319

M ATEESCU , K ASK , G OGATE & D ECHTER

Approximate Mar Problem Set Relational
35

30

Sum Score

25

20

15

10

5

0
0

2

4

6

8

10

12

14

16

18

20

18

20

Time in minutes
SampleSearch
IJGP

EDBP
TLSBP

EPIS

Figure 30: Results on relational networks.

Approximate Mar Problem Set Linkage
6

5

Sum Score

4

3

2

1

0
0

2

4

6

8

10

12

14

16

Time in minutes
SampleSearch

IJGP

EDBP

Figure 31: Results on Linkage networks. EPIS and TLSBP are not plotted because they cannot
handle Linkage networks.

320

J OIN -G RAPH P ROPAGATION A LGORITHMS

Approximate Mar Problem Set Grids

25

Sum Score

20

15

10

5

0
0

2

4

6

8

10

12

14

16

18

20

16

18

20

Time in minutes
SampleSearch
IJGP

EDBP
TLSBP

EPIS

Figure 32: Results on Grid networks.

Approximate Mar Problem Set bn2o
18
16

Sum Score

14
12
10
8
6
4
2
0
0

2

4

6

8

10

12

14

Time in minutes
SampleSearch
IJGP

EDBP
TLSBP

EPIS

Figure 33: Results on BN2O networks. All solvers except IJGP quickly converge to the maximum
possible score of 18 and are therefore indistinguishable in the Figure.

321

M ATEESCU , K ASK , G OGATE & D ECHTER

Approximate Mar Problem Set WCSPs
18
16
14

Sum Score

12
10
8
6
4
2
0
0

2

4

6

8

10

12

14

16

18

20

Time in minutes
SampleSearch

IJGP

EDBP

Figure 34: Results on WCSPs networks. EPIS and TLSBP are not plotted because they cannot
handle WCSPs.

Approximate Mar Problem Set Promedas
45
40
35

Sum Score

30
25
20
15
10
5
0
0

2

4

6

8

10

12

14

16

18

20

Time in minutes
SampleSearch

IJGP

EDBP

TLSBP

Figure 35: Results on Promedas networks. EPIS is not plotted because it cannot handle Promedas
benchmarks, which are Markov networks.

322

J OIN -G RAPH P ROPAGATION A LGORITHMS

guaranteed, and this is an example when it does not happen. The other solvers reach the maximum
possible SumScore of 18 (or very close to it) after about 6 minutes of CPU time.
The WCSP benchmark set has 97 instances. However we used only the 18 instances for which
exact marginals are available. Therefore the maximum SumScore that an algorithm can reach is
18. The results are shown in Figure 34. EDBP reaches a SumScore of 17 after almost 3 minutes
of CPU time while IJGP reaches a SumScore of 13 after about 3 minutes. The SumScores of
both IJGP and EDBP remain unchanged in the interval from 3 to 20 minutes. After looking at the
raw results, we found that IJGP’s score was zero on 5 instances out of 18. This was because the
singleton consistency component implemented via the SAT solver did not finish in 20 minutes on
these instances. Although the singleton consistency step generally helps to reduce the practical time
complexity of IJGP on most instances, it adversely affects it on these WCSP instances.
The Promedas instances are Noisy-OR binary Bayesian networks (Pearl, 1988). These instances
are characterized by extreme marginals. Namely, for a given variable, the marginals are of the form
(1 − , ) where  is a very small positive constant. Exact marginals are available only on 46 out of
the submitted 238 instances. On these structured problems (see Figure 35), we see that EDBP is the
best performing scheme reaching a SumScore very close to 46 after about 7 minutes of CPU time
while TLSBP and IJGP are able to reach a SumScore of about 40 in 20 minutes.

6. Related Work
There are numerous lines of research devoted to the study of belief propagation algorithms, or
message-passing schemes in general. Throughout the paper we have mentioned and compared with
other related work, especially in the experimental evaluation section. We give here a short summary
of the developments in belief propagation and present some related schemes that were not mentioned
before. For additional information see also the recent review by Koller (2010).
About a decade ago, Iterative Belief Propagation (Pearl, 1988) received a lot of interest from
the information theory and coding community. It was realized that two of the best error-correcting
decoding algorithms were actually performing belief propagation in networks with cycles. The
LDPC code (low-density parity-check) introduced long time ago by Gallager (1963), is now considered one of the most powerful and promising schemes that often performs impressively close to
Shannon’s limit. Turbo codes (Berrou, Glavieux, & Thitimajshima, 1993) are also very efficient in
practice and can be understood as an instance of belief propagation (McEliece et al., 1998).
A considerable progress towards understanding the behavior and performance of BP was made
through concepts from statistical physics. Yedidia et al. (2001) showed that IBP is strongly related
to the Bethe-Peierls approximation of variational (Gibbs) free energy in factor graphs. The Bethe
approximation is a particular case of the more general Kikuchi (1951) approximation. Generalized
Belief Propagation (Yedidia et al., 2005) is an application of the Kikuchi approximation that works
with clusters of variables, on structures called region graphs. Another algorithm that employs the
region-based approach is Cluster Variation Method (CVM) (Pelizzola, 2005). These algorithms
focus on selecting a good region-graph structure to account for the over-counting (and over-overcounting, etc.) of evidence. We view generalized belief propagation more broadly as any belief
propagation over nodes which are clusters of functions. Within this view IJGP, and GBP as defined
by Yedidia et al. (2001), as well as CVM, are special realizations of generalized belief propagation.
Belief Propagation on Partially Ordered Sets (PBP) (McEliece & Yildirim, 2002) is also a generalized form of Belief Propagation that minimizes the Bethe-Kikuchi variational free energy, and

323

M ATEESCU , K ASK , G OGATE & D ECHTER

that works as a message-passing algorithm on data structures called partially ordered sets, which
has junction graphs and factor graphs as examples. There is one-to-one correspondence between
fixed points of PBP and stationary points of the free energy. PBP includes as special cases many
other variants of belief propagation. As we noted before, IJGP is basically the same as PBP.
Expectation Propagation (EP) (Minka, 2001) is a an iterative approximation algorithm for computing posterior belief in Bayesian networks. It combines assumed-density filtering (ADF), an
extension of the Kalman filter (used to approximate belief states using expectations, such as mean
and variance), with IBP, and iterates until these expectations are consistent throughout the network.
TreeEP (Minka & Qi, 2004) deals with cyclic problem by reducing the problem graph to a tree subgraph and approximating the remaining edges. The relationship between EP and GBP is discussed
by Welling, Minka, and Teh (2005).
Survey Propagation (SP) (Braunstein et al., 2005) solves hard satisfiable (SAT) problems using a
message-passing algorithm on a factor graph consisting of variable and clause nodes. SP is inspired
by an algorithm called Warning Propagation (WP) and by BP. WP can determine if a tree-problem is
SAT, and if it is then it can provide a solution. BP can compute the number of satisfying assignments
for a tree-problem, as well as the fraction of the assignments where a variable is true. These two
algorithms are used as heuristics to define the SP algorithm, that is shown to be more efficient
than either of them on arbitrary networks. SP is still a heuristic algorithm with no guarantee of
convergence. SP was inspired by the new concept of “cavity method” in statistical physics, and can
be interpreted as BP where variables can not only take the values true or false, but also the extra
“don’t care” value. For a more detailed treatment see the book by Mézard and Montanari (2009).

7. Conclusion
In this paper we investigated a family of approximation algorithms for Bayesian networks, that
could also be extended to general graphical models. We started with bounded inference algorithms
and proposed Mini-Clustering (MC) scheme as a generalization of Mini-Buckets to arbitrary tree
decompositions. Its power lies in being an anytime algorithm governed by a user adjustable i-bound
parameter. MC can start with small i-bound and keep increasing it as long as it is given more time,
and its accuracy usually improves with more time. If enough time is given to it, it is guaranteed to
become exact. One of its virtues is that it can also produce upper and lower bounds, a route not
explored in this paper.
Inspired by the success of iterative belief propagation (IBP), we extended MC into an iterative
message-passing algorithm called Iterative Join-Graph Propagation (IJGP). IJGP operates on general join-graphs that can contain cycles, but it is sill governed by an i-bound parameter. Unlike IBP,
IJGP is guaranteed to become exact if given enough time.
We also make connections with well understood consistency enforcing algorithms for constraint
satisfaction, giving strong support for iterating messages, and giving insight into the performance
of IJGP (IBP). We show that: (1) if a value of a variable is assessed as having zero-belief in any
iteration of IJGP, then it remains a zero-belief in all subsequent iterations; (2) IJGP converges in a
finite number of iterations relative to its set of zero-beliefs; and, most importantly (3) that the set
of zero-beliefs decided by any of the iterative belief propagation methods is sound. Namely any
zero-belief determined by IJGP corresponds to a true zero conditional probability relative to the
given probability distribution expressed by the Bayesian network.

324

J OIN -G RAPH P ROPAGATION A LGORITHMS

Our experimental evaluation of IJGP, IBP and MC is provided, and IJGP emerges as one of the
most powerful approximate algorithms for belief updating in Bayesian networks.


