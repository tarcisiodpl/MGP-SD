
Various tasks in decision making and decision support systems require selecting a preferred
subset of a given set of items. Here we focus on problems where the individual items are described
using a set of characterizing attributes, and a generic preference specification is required, that is,
a specification that can work with an arbitrary set of items. For example, preferences over the
content of an online newspaper should have this form: At each viewing, the newspaper contains a
subset of the set of articles currently available. Our preference specification over this subset should
be provided offline, but we should be able to use it to select a subset of any currently available
set of articles, e.g., based on their tags. We present a general approach for lifting formalisms
for specifying preferences over objects with multiple attributes into ones that specify preferences
over subsets of such objects. We also show how we can compute an optimal subset given such a
specification in a relatively efficient manner. We provide an empirical evaluation of the approach
as well as some worst-case complexity results.

1. Introduction
Work on reasoning with preferences focuses mostly on the task of recognizing preferred elements
within a given set. However, another problem of interest is that of selecting an optimal subset of
elements. Optimal subset selection is an important problem with many applications: the choice of
feature subsets in machine learning, selection of a preferred bundle of goods (as in, e.g., a home
entertainment system), finding the best set of items to display on the user’s screen, selecting the best
set of articles for a newspaper or the best members for a committee, etc.
Earlier work on this problem has mostly focused on the question of how one can construct an
ordering over subsets of elements given an ordering over the elements of the set (Barberà, Bossert,
& Pattanaik, 2004). The main distinction made has been between sets of items that are mutually
exclusive, in the sense that only one can eventually materialize, and sets in which the items will
jointly materialize. Our formalism is agnostic on this issue, although we are clearly motivated by
the latter case. As Barberà et al. note, most past work focused on the case of mutually exclusive
elements. This, for example, would be the case if we are selecting a set of alternatives from which
some decision-maker (or nature) will ultimately choose only one (e.g., courses of action). However,

©2009 AI Access Foundation. All rights reserved.

B INSHTOK , B RAFMAN , D OMSHLAK , & S HIMONY

there is a substantial body of work on the latter setting in which items might materialize jointly, and
individual items are preferentially comparable.
This paper focuses on a somewhat different context for set-preference specification. First, we
assume that the items from which our subsets are composed are structured, in the sense that some set
of attributes is associated with them. For example, if the items are movies, these attributes could be
the genre, language, year, director; if the “items” are politicians, the attributes could be the political
views of the politicians on various topics, their party affiliation, their level of experience. Second,
we require a generic preference specification, in the sense that it can be used with diverse collections
of items. For example, if we are specifying guidelines for the composition of some committee, these
guidelines are generic, and can be used to induce a preference relation over subsets of any given set
of politicians, provided that the set of attributes is fixed. Third, we do not assume any preferential
ordering over the individual items, although that can certainly be captured by one of the attributes
describing the items.
An instructive example of the type of domain we have in mind is that of personalized online
newspapers. First, the problem of selection for a newspaper is one of subset selection – we have
to select a subset of the set of available articles to place in the newspaper. Second, the database
of articles is constantly changing. Therefore, an approach that requires explicitly specifying preferences for the inclusion of each specific item is inappropriate, both because the number of such
items is very large, and because this would require us to constantly change the preference specification as the set of items changes. Finally, we would not want to base our approach on a method for
transforming an ordering over items into an ordering over subsets of items, because we do not want
to have to rank each item, and because there are obvious instances of complementarity and substitutability. For instance, even if I prefer articles on Britney Spears to articles on any other topic, two
very similar articles about her may be less interesting than a set comprising one about her and one
about the Spice Girls.1
One recent work that considers a similar setting is that of desJardins and Wagstaff (2005), which
works by specifying preferences over more abstract properties of sets. In particular, desJardins and
Wagstaff offer a formalism for preference specification in which users can specify their preferences
about the set of values each attribute attains within the selected set of items. One could assert
whether the values attained by an attribute on the selected subset should be diverse or concentrated
around some specific value. In addition, desJardins and Wagstaff also suggest a heuristic search
algorithm for finding good, though not necessarily optimal, such sets of items.
In this work, we present a more general, two-tiered approach for dealing with set preferences in
the above setting. This approach combines a language for specifying certain types of set properties,
and an arbitrary preference specification language for expressing preferences over single, attributed
items. The basic idea is to first specify the set properties we care about, and then specify preferences
over the values of these properties. Such a specification induces a preference ordering over sets
based on the values these sets provide to the properties of interest. We believe that the suggested
approach is both intuitive and powerful. Although in this paper we focus on a particular set of
properties for which we have devised a relatively efficient optimization algorithm, in its most general
form, this two-tiered approach generalizes the approach of desJardins and Wagstaff (2005) because
diversity and specificity are just two set properties. In principle, one can express both more general
1. We realize that common rules of rationality may not apply to users with such preferences.

134

G ENERIC P REFERENCES OVER S UBSETS OF S TRUCTURED O BJECTS

properties referring to multiple attributes, as well as more general conditional preferences over the
values of these properties.
Essentially, our approach re-states the problem of specifying preferences over sets in terms used
to specify preferences over single items. In our formulation, “items” stand for the possible sets,
and attributes of such “items” are their (user-defined) set-property values. Thus, in principle, this
approach allows us to re-use any formalism for specifying preferences over single items. In this paper we will consider two specific instantiations of such a formalism: qualitative preferences based
on CP or TCP-nets (Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004; Brafman, Domshlak, &
Shimony, 2006a), and quantitative preferences represented as generalized additively independent
(GAI) value functions (Bacchus & Grove, 1995; Fishburn, 1969). The algorithm we suggest for
computing an optimal subset given qualitative preferences is based on a similar optimization algorithm for TCP-nets. But because the number of “items” in our case is very large, this algorithm is
modified substantially to exploit the special structure of these “items”. These modifications enable
us to compute an optimal subset faster.

2. Specifying Set Preferences
The formalism we use for set-preference specification makes one fundamental assumption: the
items from which sets of interest are built are described in terms of some attributes, and the values
of these attributes are what distinguishes different items. We shall use S to denote the set of individual items, and X to denote the set of attributes describing these items. For example, imagine that
the “items” in question are US senate members, and the attributes and their values are: Party affiliation (Republican, Democrat), Views (liberal, conservative, ultra conservative), and Experience
(experienced, inexperienced).
2.1 From Properties of Items to Properties of Item Sets
Given the set X of item-describing attributes, first, we can already talk about more complex item
properties, e.g., “senate members with liberal views”, or “inexperienced, conservative senate members”. More formally, let X be the union of the attribute domains, that is,
X = {X = x | X ∈ X , x ∈ Dom(X)} ,
and let LX be the propositional language defined over X with the usual logical operators. LX
provides us with a language for describing complex properties of individual items. Since items in S
can be viewed as models of LX , we can write o |= ϕ whenever o ∈ S and o is an item that satisfies
the property ϕ ∈ LX .
Given the language LX , we can now specify arbitrary properties of item sets based on the
attribute values of items in a set, such as the property of having at least two Democrats, or having
more Democrats than Republicans. More generally, given any item property ϕ ∈ LX , we can
talk about the number of items in a set that have property ϕ, which we denote by |ϕ|(S), that
is, |ϕ|(S) = |{o ∈ S|o |= ϕ}|. Often the set S is implicitly defined, and we simply write |ϕ|.
Thus, |Experience=experienced|(S) is the number of experienced members in S. Often, we simply
abbreviate this as |experienced|.
While |ϕ|(·) is an integer-valued property of sets, we can also specify boolean set properties as
follows: h|ϕ| REL ki, where ϕ ∈ LX , REL is a relational operator over integers, and k ∈ Z∗ is a
135

B INSHTOK , B RAFMAN , D OMSHLAK , & S HIMONY

non-negative integer. This property is satisfied by a set S if |{o ∈ S|o |= ϕ}| REL k. In our running
example we use the following three set properties:
• P1 : h|Party affiliation = Republican ∨ Political view = conservative| ≥ 2i
• P2 : h|Experience = experienced| ≥ 2i
• P3 : h|Political view = liberal| ≥ 1i
P1 is satisfied (only) by sets with at least two members that are either Republican or conservative.
P2 is satisfied by sets with at least 2 experienced members. P3 is satisfied by sets with at least one
liberal.
We can also write h|ϕ| REL |ψ|i, with a similar interpretation. For example, h|Republican| >
|Democrat|i holds for sets containing more Republicans than Democrats. An even more general
language could include arithmetic operators (e.g., require twice as many Republicans as Democrats)
and aggregate functions (e.g., the average number of years on the job). All these are instances of
the general notion of specifying properties of sets as a function of the attribute values of the set’s
members. In this paper, we focus on the above language with the relational operators restricted to
equalities and inequalities. We do so because having a clear, concrete setting eases the presentation,
and because restricting the language allows us to provide more efficient subset-selection algorithms.
Indeed, many of the ideas we present here apply to more general languages. In particular, this
generality holds both for the overall preference-specification methodology, and for the search-overCSPs technique for computing optimal subsets introduced later in the paper. However, the more
specific techniques we use to implement these ideas, such as bounds generation, and the specific
translation of properties into CSPs, rely heavily on the use of specific, more restrictive languages.
Finally, we note an important property of our preference specification approach of being independent of the actual set of items available at the moment. This generality is important for many
applications where the same reasoning about set preferences must be performed on different, and
often initially unknown sets of items. For example, this is the case with specifying guidelines for
selecting articles for an online newspaper, or for selecting a set of k results for an information query.
2.2 Reasoning with Set Preferences
Once we have specified the set properties of interest, we can define preferences over the values of
these properties using any preference specification formalism. Here we discuss two specific formalisms, namely TCP-nets (Brafman et al., 2006a), an extension of CP-nets (Boutilier et al., 2004),
and Generalized Additively Independent (GAI)-value functions (Bacchus & Grove, 1995; Fishburn,
1969). The former is a formalism for purely qualitative preference specification, yielding a partial
preference order over the objects of interest. The latter is a quantitative specification formalism that
can represent any value function.
Let P = {P1 , . . . , Pk } be some collection of set properties. A TCP-net over P captures statements of the following two types:
(1) Conditional Value Preference Statements. “If Pi1 = pi1 ∧ · · · ∧ Pij = pij then Pl = pl is
preferred to Pl = p0l .” That is, when Pi1 , . . . , Pij have a certain value, we prefer one value for
Pl to another value for Pl .

136

G ENERIC P REFERENCES OVER S UBSETS OF S TRUCTURED O BJECTS

(2) Relative Importance Statements. “If Pi1 = pi1 ∧ · · · ∧ Pij = pij then Pl is more important than
Pm .” That is, when Pi1 , . . . , Pij have a certain value, we prefer a better value for Pl even if we
have to compromise on the value of Pm .
Each such statement allows us to compare between certain pairs of item sets as follows:
- The statement “if Pi1 = pi1 ∧ · · · ∧ Pij = pij then Pl = pl is preferred to Pl = p0l ” implies
that given any two sets S, S 0 for which (1) Pi1 = pi1 ∧ · · · ∧ Pij = pij holds, (2) S satisfies
Pl = pl and S 0 satisfies Pl = p0l , and (3) S and S 0 have identical values on all properties
except Pl , we have that S is preferred to S 0 .
- The statement “if Pi1 = pi1 ∧ · · · ∧ Pij = pij then Pl is more important than Pm ” implies
that given any two sets S, S 0 for which (1) Pi1 = pi1 ∧ · · · ∧ Pij = pij holds, (2) S has a more
preferred value for Pl , and (3) S and S 0 have identical values on all attributes except Pl and
Pm , we have that S is preferred to S 0 . (Notice that we do not care about the value of Pm if Pl
is improved.)
We refer the reader to the work of Brafman et al. (2006a) for more details on TCP-nets, their
graphical structure, their consistency, etc. The algorithms in this paper, when used with TCP-nets,
assume an acyclic TCP-net Brafman et al.. The latter property ensures both consistency of the
provided preferences, as well as existence of certain “good” orderings of P with respect to the
TCP-net.
As an example, consider the following preferences of the president for forming a committee.
He prefers at least two members that are either Republican or conservative, that is, he prefers P1
to P1 unconditionally. (Depending on the context, we use P to denote both the property P and the
value P = true. We use P to denote P = false.) If P1 holds, he prefers P2 over P2 (that is, at least
two experienced members), so that the committee recommendations carry more weight. If P1 holds,
he prefers P2 to P2 (that is, all but one are inexperienced) so that it would be easier to influence
their decision. The president unconditionally prefers to have at least one liberal, that is, he prefers
P3 to P3 , so as to give the appearance of balance. However, P3 is less important than both P1 and
P2 . There is an additional “external” constraint (or possibly a preference) that the total number of
members be three.2
GAI value functions map the elements of interest (item sets in our case) into real values quantifying theP
relative desirability of these elements. Structure-wise, GAI value functions have the form
U (S) = i=1,...,n Ui (Pi (S)), where each Pi ⊂ P is a subset of properties. For example, the President’s preferences imply the following GAI structure: U (S) = U1 (P1 (S), P2 (S)) + U2 (P3 (S))
because the President’s conditional preferences over P2 ’s value tie P1 and P2 together, but are independent of P3 ’s value. U1 would capture the weight of this conditional preference, combined
with the absolute preference for P1 ’s value. U2 would represent the value of property P3 . We
might quantify these preferences as follows: U1 (P1 , P2 ) = 10, U1 (P1 , P2 ) = 8, U1 (P1 , P2 ) = 2,
U1 (P1 , P2 ) = 5; while U2 (P3 ) = 1, U2 (P3 ) = 0. Of course, infinitely many other quantifications
are possible.
2. Some external constraints, such as this cardinality constraint, can be modeled as a preference with high
value/importance. In fact, this is how we model cardinality constraints in our implementation.

137

B INSHTOK , B RAFMAN , D OMSHLAK , & S HIMONY

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

Q ← {∅}
Sopt ← ∅
while Q contains a set S such that UB(S) > Value(Sopt ) do
S ← argmaxS 0 ∈Q UB(S 0 )
Q ← QS
\ {S 0 | LB(Sopt ) ≥ UB(S 0 )}
Q ← Q {S ∪ {o} | o ∈ S \ S}
S ← argmaxS 0 ∈Q Value(S 0 )
if Value(S) > Value(Sopt ) then
Sopt ← S
end if
end while
return Sopt
Figure 1: Subset-space branch-and-bound search for an optimal subset of available items S.

3. Finding an Optimal Subset
In general, given a preference specification and a set S of available items, our goal is to find an
optimal subset Sopt ⊆ S with respect to the preference specification. That is, for any other set
S 0 ⊆ S, we have that the properties Sopt satisfies are no less desirable than the properties S 0 satisfies.
We now consider two classes of algorithms for finding such an optimal subset. These two classes
of algorithm differ in the space in which they search. In the next section, we describe a comparative
empirical evaluation of these algorithms. For our running example we use the following set of
available items S:
o1
o2
o3
o4

Republican
Republican
Democrat
Democrat

conservative
ultra conservative
conservative
liberal

inexperienced
experienced
experienced
experienced

3.1 Searching in Sets Space
The most obvious approach for generating an optimal subset is to search directly in the space of
subsets. A priori this approach is not too attractive, and indeed, we shall see later that our implementation of this approach did not scale up. However, given that often we are interested in sets of
small size and that heuristics can be used to enhance search quality, we thought it is worth exploring
this approach.
A branch-and-bound (B&B) algorithm in the space of sets is depicted in Figure 1. For each set
S, the algorithm assumes access to an upper bound UB(S) and to a lower bound LB(S) estimates on
the maximal value of a superset of S. The algorithm maintains a queue Q of sets, and this queue is
initialized to contain only the empty set. At each step, the algorithm selects a highest upper-bound
set S from the queue. Next, the algorithm removes from Q all sets S 0 with upper bound UB(S 0 )
being at most as good as the lower bound LB(S) of the selected set S, and adds to Q all the minimal
(that is, one-item) extensions of S. The latter sets correspond to the successors of S in the search
space. Different implementations of the algorithm differ in how they sort the queue. The best-first
version depicted in the pseudo-code sorts the queue according to a heuristic value of the set, and in
138

G ENERIC P REFERENCES OVER S UBSETS OF S TRUCTURED O BJECTS

our case this heuristic is an upper bound on the value of the set’s supersets. In contrast, the depthfirst version always positions the children of the newly expanded node at the front of the queue. We
implemented and tested both versions.
The method used to generate bounds for a set S must depend on the actual preference representation formalism, as well as on the type of set properties being used, and the idea is more natural
given a quantitative value function. For a lower bound LB(S) we can use the actual value Value(S)
of S. Note that it is possible that all descendants of S will have lower values because, in general,
set-properties may not be monotonic (e.g., “average value higher than 5.”) However, since S itself
is a possible solution, this is a valid lower bound.
For an upper bound, we proceed as follows: First, we consider which set-property values are
consistent with S. That is, for each set property, we examine what values S and any of its supersets
can potentially provide to that property. For example, consider P2 and suppose S contains a single
experienced member. So currently, P2 holds. However, we can satisfy P2 if we add one more
experienced member. Thus, both values of P2 are consistent with S. In contrast, if we had two
experienced members in S, then P2 is inconsistent with S because no matter who we add to S,
we can never satisfy P2 . Next, given such sets of possible set-properties’ values with respect to
the set S, we can bound the value of S and of any of its supersets by maximizing values locally.
Specifically, in a GAI value function, we can look at each local function Ui , and consider which
assignment to it, from among the consistent values, would maximize Ui . Clearly, this may result
in an overall value overestimation, since we do not know whether these “locally optimizing” joint
assignments are consistent. Similar ideas can be used with other quantitative representations, as
in various soft-constraint formalisms (Bistarelli, Fargier, Montanari, Rossi, Schiex, & Verfaillie,
1999).
Consider our running example with the GAI value function as at the end of Section 2, and
consider searching for an optimal subset of S = {o1 , o2 , o3 , o4 } using a depth-first version of B&B.
We start with the empty set, and the property values provided by the empty set are P1 , P2 , P3 . Thus,
the lower bound LB(∅), which is the value of the empty-set, is 5. For the upper bound UB(∅), we
consider the best property values that are individually consistent with the extensions of ∅, which
are P1 , P2 , P3 , and their accumulative value is 11. Sopt is also initialized to the empty set, and
next we generate all of the children of the (only possible) selected set ∅, which are all singleton
sets: {o1 }, {o2 }, {o3 }, {o4 }. Except for {o4 }, they all have lower and upper bounds identical to
those of the empty set, and are inserted into the queue. {o4 } has a lower bound of 6 and the upper
bound is 11. Suppose {o1 } is the first queue element, and we select it for expansion. This results in
adding {o1 , o2 }, {o1 , o3 }, {o1 , o4 } into the queue, and the lower and upper bounds of these sets are
(8, 11), (8, 11), (6, 11), respectively. Next, the set {o1 , o2 } is examined with respect to the current
Sopt = ∅, and Sopt is assigned to {o1 , o2 }. Since we assumed here a depth-first version of B&B we
proceed with expanding {o1 , o2 }, obtaining {o1 , o2 , o3 }, {o1 , o2 , o4 } with lower and upper bounds
being, respectively, (10, 11) and (11, 11). With a lower bound of 11 for {o1 , o2 , o4 } we can prune
away all the rest of the nodes in the queue, and we are done.
An important issue for depth-first B&B is the order in which sets are generated. In our implementation, at each node in the search space, the items in S are ordered according to the sum of the
value of the properties they can help satisfy. For example, initially, a conservative member such as
o1 could help us satisfy P1 .
In contrast to quantitative preference representation formalisms, qualitative preferences typically induce a partial ordering over property collections. In this case, it is harder to generate strict
139

B INSHTOK , B RAFMAN , D OMSHLAK , & S HIMONY

upper and lower bounds – as they must be comparable to any possible solution. One way to handle
this is to linearize the ordering and require the stronger property of optimality with respect to the
resulting total order. Here, TCP-nets present themselves as a good choice because there is an efficient and simple way of generating a value function consistent with an acyclic TCP-net (Brafman
& Domshlak, 2008). This value function retains the structure of the original network which is important to make the bounds computation efficient (notably, each Ui depends on a small number of
property values).
3.2 Searching over CSPs
The attractiveness of the item subsets is evaluated in terms of a fixed collection of set-properties P,
and thus different sets that provide all identical property values are equivalent from our perspective.
The immediate conclusion is that considering separately such preferentially equivalent subsets of
available items S is redundant. To remove this redundancy, we suggest an alternative method in
which we search directly over set-property value combinations. Of course, the problem is that given
a set-property value combination, it is not obvious whether we can find an actual subset of S that
has such a combination of properties. To answer this question, we generate a CSP that is satisfiable
if and only if there exists a subset of S with the considered set-property values. The overall search
procedure schematically works as follows.
1. Systematically generate combinations of set-property values.
2. For each such combination, search for a subset of S providing that combination of setproperty values.
3. Output a subset of S satisfying an optimal (achievable) combination of set-property values.
To make this approach as efficient as possible, we have to do two things, namely:
(1) Find a way to prune sub-optimal set-property value combinations as early as possible.
(2) Given a set-property value combination, quickly determine whether a subset of S satisfies this
combination.
Considering the first task, let P1 , . . . , Pk be an ordering of the set-properties P.3 Given such
an ordering of P, we incrementally generate a tree of property combinations. The root of that tree
corresponds to an empty assignment to P. For each node n corresponding to a partial assignment
P1 = p1 , . . . , Pj = pj , and for every possible value pj+1 of the property Pj+1 , the tree contains
a child of n corresponding to the partial assignment P1 = p1 , . . . , Pj = pj , Pj+1 = pj+1 . The
tree leaves correspond to (all) complete assignments to P. Such a tree for our running example
is depicted in Figure 2. Note that, implicitly, each node in this tree is associated with a (possibly
empty) set of subsets of S, notably, the subsets that provide the set-property value combination
associated with that node.
In our search for an optimal set, we expand this tree of set-property value combinations while
trying to expand as few tree nodes as possible by pruning certain value combinations of P as either
3. Throughout this paper, we will assume that in preference specifications using TCP nets, there are only conditional
preference (CP) arcs, and importance arcs, but no conditional importance (CI) arcs. While our scheme and implementations allow these arcs, CI arcs force the ordering of the set properties to be dynamic, as it may depend on value
assignments to previous properties. For clarity of exposition, we thus preferred not to present these technical details.

140

G ENERIC P REFERENCES OVER S UBSETS OF S TRUCTURED O BJECTS

∅

P¯1

P1

P1 P2 P3

P¯1 P¯2

P1 P¯2

P1 P2
P1 P2 P¯3

P1 P¯2 P3 P1 P¯2 P¯3

P¯1 P¯2 P3

P¯1 P2

P¯1 P¯2 P¯3 P¯1 P2 P3

P¯1 P2 P¯3

Figure 2: Illustration of a search tree for our running example.
sub-optimal with respect to set preferences, or unsatisfiable with respect to S. A standard way
to do this is, again, by using a branch-and-bound search procedure, and this requires from us to
derive effective upper and lower bounds on the value of the best subset satisfying a partial value
combination for P. In addition, the order we associate with properties and their values affects our
pruning ability throughout the search process. To get the most leverage out of our bounds, we
would like to explore the children of a node in the decreasing order of their purported attractiveness.
Moreover, when fixing the ordering of the set-properties themselves, we would like properties that
can potentially contribute more to appear earlier in this ordering. For instance, P1 ’s value in our
running example has a greater influence on the overall attractiveness of a subset than the value of
P2 , and thus P1 should better be branched on first. In addition, P1 is preferred to be true, and thus
the subtree corresponding to P1 = true should better be explored first. Similarly, P2 is preferred to
be true when P1 = true, and preferred to be false, otherwise. This ordering is reflected in the tree
in Figure 2, for a left to right pre-order traversal of the tree.
Now, let us consider the second task of determining whether a subset of S satisfies a given setproperty value combination. Given such a partial assignment α to P, we set up the following CSP.
First, the CSP has a boolean variable xi for every available item oi ∈ S. In our example, the CSP
contains the variables x1 , . . . , x4 for items o1 , . . . , o4 respectively. Intuitively, xi = 1 encodes oi
being a part of our (searched for) subset of S, whereas xi = 0 means that oi is not in that subset.
Next, we translate every set-property value in α into a certain constraint on these variables. For
instance, if α[P1 ] = true, the constraint C1 : x1 + x2 + x3 ≥ 2 is added to the CSP. Note that
C1 explicitly encodes the requirement (of P1 = true) for the subset to have at least two of the
elements that satisfy Republican ∨ conservative. That is because {o1 , o2 , o3 } are all the candidates
in S that are either Republican or conservative. Alternately, if α[P1 ] = f alse, then the constraint
C1 : x1 + x2 + x3 < 2 is added to the CSP. Finally, if α does not specify a value for P1 , then no
constraints related to P1 should be added at all. Likewise, for α[P2 ] = true and α[P3 ] = true we
would add the constraints C2 : x2 + x3 + x4 ≥ 2 and C3 : x4 ≥ 1, respectively. In general, it
is not hard to verify that the CSP constructed this way for a concrete item set S and a set-property
value combination α is solvable if and only if S has a subset satisfying α. Moreover, if this CSP is
solvable, then any of its solutions explicitly provides us with such a subset of S.
It is worth briefly pointing out the difference between the CSPs we generate here and the more
typical CSPs usually discussed in the literature. Most work on general CSPs deals with constraints
141

B INSHTOK , B RAFMAN , D OMSHLAK , & S HIMONY

over small, typically just two-variable, subsets of problem variables. In contrast, the constraints
in the CSPs generated in our optimization process are global, with each constraint being possibly defined over all the CSP variables. Yet another special property of CSPs constructed for our
purposes is that there is a sense in which it is meaningful to talk about partial assignments in our
context—unassigned variables can always be regarded de facto as if assigned the value “0” since
the corresponding items, by default, do not belong to the subset we search for.
Because partial assignments to set-properties P map to CSPs, each node in our tree of setproperty value combinations maps to a CSP, and the entire tree can be viewed as a tree of CSPs.
The important property of this tree-of-CSPs is that the children of each CSP node are CSPs obtained
by adding one additional constraint to the parent CSP, notably the constraint corresponding to the
additional property value that we want the set to satisfy. This implies that if some CSP node in
the tree is unsatisfiable, then all of its descendants are unsatisfiable as well. In fact, we can make
a stronger use of the nature of this search tree, recognizing that we can reuse the work done on a
parent node to speed up the solution of its children. To see the latter, consider some CSP C in our
tree-of-CSPs, some child CSP C 0 of C , and let S ⊆ S be a solution to C . As C 0 extends C with
a constraint C, any subset S 0 ⊆ S ruled out by C will be also ruled out by C 0 . Hence, if solving
C and C 0 considers subsets of S in the same order (that is, by using the same ordering over set
elements), then solving C 0 can start from the leaf node corresponding to S, the solution generated
for C . Moreover, if a constraint C represents a boolean set property, and S is not a solution to
C 0 = C ∪ {C}, then S has to be a solution to C ∪ {¬C}, which is the sibling of C 0 . Using these
ideas, we share the work done on different CSP nodes of our tree-of-CSPs. In fact, when all set
properties are boolean, this approach needs to backtrack over each property at most once (we call
this property “limited backtracking”), thereby considerably improving the empirical performance
of the algorithm.
The overall branch-and-bound algorithm in the space of CSPs is depicted in Figure 3. As is,
the algorithm is formulated for the case of quantitative preference formalisms. The formulation
of the algorithm for the qualitative case is essentially the same, with minor technical differences
and an important computational property. For CP/TCP-nets, we can guarantee that only limited
backtracking is required if we follow the following guidelines. First, we must order the variables
(line 1) in an order consistent with the topology of the network. Note that for TCP-nets, this ordering
may be conditional, that is, the order of two variables may vary depending on the value of some of
the earlier variables. Second, in line 2, the property values must be (possibly partially) ordered
from best to worst, given the values of the parent properties (which must be and will be instantiated
earlier). In that case, the first satisfiable set of properties constitutes an optimal choice (Brafman
et al., 2006a). Assuming we solve intermediate nodes in the tree-of-CSPs, we know that we should
backtrack at most once in each level assuming boolean set-properties, but, again, more backtracks
may occur with integer-valued properties.
The node data structure used by the algorithm has two attributes. For a search node n,
• n.α captures a partial assignment to the set-properties P associated with the node n, and
• n.S captures a subset of S satisfying n.α if such exists, and otherwise has the value false.
The functions Value, LB, and UB have the same semantics as in the subset-space search algorithm
in Figure 1. In the pseudocode we assume a fixed ordering over set-property values (line 2), but
one can vary it depending on earlier values (and we exploit that in our implementation). Finally, the
142

G ENERIC P REFERENCES OVER S UBSETS OF S TRUCTURED O BJECTS

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

Fix an ordering over set-properties P
Fix an ordering over the values of each set property P ∈ P
Fix an ordering over all available items S
Q ← {n[∅; ∅]}
Sopt ← ∅
while Q is not empty do
n ← pop(Q)
construct-and-solve-csp(n)
if n.S 6= f alse and UB(n.S) > Value(Sopt ) then
if Value(n.S) > Value(Sopt ) then
Sopt ← n.S
end if
Let P be the highest-ordered set property unassigned by n.α
for each possible value p of P do
n0 ← [n.α ∪ {P = p}; n.S]
Q ← Q ∪ {n0 }
. The position of n0 in Q depends on the search strategy
end for
end if
end while
return Sopt
Figure 3: CSP-space branch-and-bound search for an optimal subset of available items S.

pseudo-code leaves open the choice of search strategy for used by the branch-and-bound, and this
choice is fully captured by the queue insertion strategy in line 16.
To illustrate the flow of the algorithm, let us consider again our running example. Recall that
the example already has a requirement for the discovered subset to be of size 3, and this translates
into a constraint C : x1 + x2 + x3 + x4 = 3. The first CSP we consider has {C, C1 } as its only
constraints. Assume the CSP variables are ordered as {x1 , x2 , x3 , x4 }, with value 1 preceding value
0 for all xi . In that case, the first solution we find is S1 : x1 = 1, x2 = 1, x3 = 1, x4 = 0. Our
next CSP adds the constraint C2 . When solving this CSP, we continue to search (using the same
order on the xi ’s and their values) from the current solution S1 , which turns out to satisfy C2 as
well. Thus, virtually no effort is required to solve this CSP. Next, we want to also satisfy C3 . This
set of constraints corresponds to a leaf node in the tree-of-CSPs which corresponds to the complete
assignment P1 P2 P3 to the set-properties. Our current item set Sopt = S1 does not have a liberal,
so we have to continue to the assignment S2 : x1 = 1, x2 = 1, x3 = 0, x4 = 1 (requiring us to
backtrack in the CSP-solution space over the assignments to x4 and x3 ). We now have a set that
satisfies the properties in the leftmost leaf node in our tree-of-CSPs. If we can prove that this setproperty value combination is optimal using our upper/lower bounds, we are done. Otherwise, we
need to explore additional nodes in our tree-of-CSPs. In the latter case, the next CSP will correspond
to P1 , P2 , P3 , with constraints {C, C1 , C2 , C3 }. However, we already have a solution to this node,
and it is exactly S1 . To see that, note that S1 was a solution to the parent of our current CSP, but
it was not a solution to its sibling {C, C1 , C2 , C3 }. Hence, since P3 is a boolean property, S1 must
satisfy {C, C1 , C2 , C3 }.

143

B INSHTOK , B RAFMAN , D OMSHLAK , & S HIMONY

3.3 Solving the underlying CSPs
Our algorithm for solving the intermediate CSPs is based on the well known backtrack-search algorithm, first presented by Prosser (1993) in a simple iterative form. At the same time, we have
adapted both the algorithm and some well known enhancements in CSP solving (such as NoGood
recording and forward checking (FC)) to the specifics of the CSPs in our setting.
Initially, variables and their values are statically ordered from the most to least constrained
(although we also discuss a few experiments performed with dynamic variable/value ordering). Our
motivation for static ordering is two-fold. First, because the constraints are very much global, we
can do the ordering at a preprocessing stage. Second, as discussed in the previous section, static
ordering allows us to better utilize solutions of CSPs when solving descendent CSPs.
The basic backtrack algorithm, which on its own, unsurpisingly performs quite poorly in our
setting, is refined by utilizing the following observations and techniques.
• Monotonicity of improving constraints. If the operator of the constraint is “=” and there
are more items having the constrained property already in the current partial solution, then
one cannot satisfy the constraint by making additional assignments. The same property holds
for the constraint operators “< ” and “≤”. Using this observation, it is possible to detect the
need to backtrack early on in the search.
• Forward Checking. A certain type of “forward checking” can be performed for our constraints. Clearly, if satisfying some constraint requires at least k items to be added to the
subset, and the number of remaining items that satisfy the desired property is less than k, then
the search algorithm must backtrack.
• “Can/Must” strategy. The “can/must” strategy corresponds to a more advanced check of the
interactions between the constraints. The idea is quite simple: if (i) at least p items must be
added to the constructed subset to satisfy the constraint Ci , (ii) at most q items can be added
to the constructed subset without violating another constraint Cj , (iii) all the items that can
be added and have the property constrained by Ci also have the property constrained by Cj ,
and, finally, (iv) p > q, then both Ci and Cj cannot be satisfied simultaneously. Moreover, no
further assignments to yet unassigned variables can resolve this conflict, and thus the situation
is a dead end. This kind of reasoning allows discovery of such barren nodes quite early in the
search, pruning large portions of the search tree. To reason correctly about the “can/must”
strategy, we have to maintain a data structure of unique items for each pair of constraints, as
well as to keep track of the number of remaining items that influence property constrained by
Ci and do not influence properties constrained by Cj .
As an example, assume we are in the middle of the search and we have two set properties:
SP1 : |A1 = a| ≥ 5 and SP2 : |A2 = b| ≤ 3. Suppose that we have already picked 3 items
that influence SP1 and 2 items that influence SP2 . As a result, to satisfy SP1 , we must add
at least another two items that influence it and to satisfy SP2 we can add at most one item
that influences SP2 . If all the items that we can choose from {ok ...on } have a value “a” for
the attribute A1 and value “b” for the attribute A2 , then obviously we cannot satisfy both SP1
and SP2 within this setting, and thus we should backtrack.
Finally, below we discuss recording NoGoods, an improvement of the basic backtracking algorithm
that proved to have the most impact in our setting.
144

G ENERIC P REFERENCES OVER S UBSETS OF S TRUCTURED O BJECTS

3.3.1 N O G OOD R ECORDING
The standard definition of a NoGood in the CSP literature is that of a partial assignment that cannot
be extended into a full solution of the problem. Once we learn a NoGood, we can use it to prune
certain paths in the search tree. The smaller the NoGood, the more occasions we can use it, the
greater its pruning power. Thus, it is of interest to recognize minimal NoGoods, and different
techniques have been developed to perform NoGood resolution in order to produce the best and most
general NoGoods possible (see, e.g., Dechter, 1990; Schiex & Verfaillie, 1993; Dago & Verfaillie,
1996).
As noted earlier, the CSPs we generate differ significantly from the more typical binary CSPs.
Consequently, the NoGood recording algorithm has to be adapted accordingly. In particular, because our constraints are global, it makes sense to try generating NoGoods that are global, too.
Thus, instead of recording assignments to variables, we record the influence of the current assignment on the constraints. Every variable influences a set of constraints.4 Thus, as a NoGood, we
store the influence the set selected so far has on all the constraints. Specifically, suppose we have
generated the set S1 , and recognized that it is not extensible into a set satisfying the constraints.
(This immediately follows from the fact that we backtracked over this set.) We now generate a NoGood N that records for each property associated with each constraint, how many items satisfying
that property occur in S1 . Now, suppose we encounter a different set S2 that has the same effect
N on the constraints. If there are fewer options to extend S2 than there are to extend S1 , we know
that S2 , as well, cannot be extended into a solution. However, if there are more options to extend
S2 than S1 , we cannot conclude that S2 is a NoGood at this point. In order to better quantify the
options that were available to extend S1 we record, beyond the actual NoGood N , the level (depth)
in the assignment tree at which it was generated. Given that the CSP solver uses a static variable
ordering, we know that if we encounter a set S that generates the same properties as the NoGood
N , at a level no higher than that of S1 , we can safely prune its extensions. The reason for that is,
there are no additional extension options available for S than there were for S1 .
The correctness of the NoGood recording mechanism proposed here depends on having a static
variable ordering, as well as a specific value ordering for all the variables in the CSP, namely,
h1, 0i. To show correctness, we should note that a NoGood can be used only after it is recorded.
Consequently, any node using a NoGood would be to the right in the search tree of a node the
NoGood was recorded at. Here we would like to stress again that, since the constraints are global,
it does not matter which items are added to the subset, but rather what influence these items had on
the constraints. Any two sets having exactly the same influence on the constraints are identical with
respect to the optimization process.
3.3.2 S EARCH A LGORITHM
The procedure depicted in Figure 4 extends the basic backtrack algorithm by a subroutine C AN I M PROVE which can be altered to include any combination of the in-depth checks discussed earlier,
to utilize early conflict detection techniques, including the NoGoods check. Also added is a call to
the A DD N O G OOD subroutine for recording NoGoods while backtracking. P and n, the generated
instance of a CSP problem with variables indexed from 1 to |S| and the node in the tree-space search
4. We assume without loss of generality that every item in the set of available items influences at least one constraint in
the constraint set C , since items that influence no constraint can be safely eliminated.

145

B INSHTOK , B RAFMAN , D OMSHLAK , & S HIMONY

respectively, are the inputs to the procedure. The algorithm systematically tries to assign values to
the problem variables, backtracking and recording NoGoods when facing a dead end.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

consistent ← n.S satisfies n.α
while not(consistent) do


if H AS VALUES(P .vars[i]) and C AN I MPROVE(P ) then
P.i ← L ABEL(P.i, consistent) . If current CSP variable has available values, try to set, update consistency
else

A
DD N O G OOD (P ,i)
. Record NoGood


P.i ← U NLABEL(P.i)
. Backtrack
end if
if P.i = 0 then
. If backtracked over the first indexed variable — no solution available
return false
end if
end while
return true
Figure 4: Conflict backtrack algorithm with NoGood recording

4. Experimental Results
We evaluate the different algorithms using a subset of the movie database publicly available from
imdb.com. We simulated a scenario of selecting movies for a three-day film festival according to
organizers preferences. Three models of growing complexity have been engineered to reflect the
preferences of the organizers; these models are defined in terms of 5, 9, and 14 set-properties, respectively. In addition, the total number of films is constrained to be 5 (which we actually modeled
using a very strong preference). Figure 5 depicts the list P14 of the 14 properties and their alterations; P5 and P9 consist of the corresponding prefixes (SP1 through SP5 , and SP1 through SP9 ,
respectively) of P14 . To produce even more complex problem instances that cause many backtracks
in the space of set-property assignments we slightly altered the 14-properties model, creating two
0 and P 00 .
additional models that are denoted henceforth as P14
14
4.1 Preference Specification
Figure 6 provides a verbal description of qualitative preferences for the film festival program which
we used in our experiments. Figure 7 depicts a TCP-net that encodes these preferences in terms of
the more concrete set-properties listed in Figure 5. For the experiments with GAI value functions,
these preferences were quantified by compiling this TCP-net into a GAI value function that orders
the items consistently with that TCP-net (Brafman & Domshlak, 2008). The task in our empirical
evaluation was to find an optimal subset of a set of available movies S ∈ {S400 , S1000 , S1600 , S3089 },
where Si corresponds to a set of i movies, and that with respect to each of the five models of
preferences over sets. All the experiments were conducted using Pentium 3.4 GHz processor with
2GB memory running Java 1.5 under Windows XP Professional. The runtimes reported in the tables
below are all in seconds, with “–” indicating process incompletion after four hours.

146

G ENERIC P REFERENCES OVER S UBSETS OF S TRUCTURED O BJECTS

SP1 = h|Year ≥ 2002| = 5i
SP2 = h|Genre = Comedy| ≥ 2i
SP3 = h|Genre = Thriller| ≤ 3i
SP4 = h|Genre = Family| > 1i
SP5 = h|Color = B&W| > 1i
SP6 = h|Director = Spielberg| ≥ 1i
SP6 ∗ = h|Director = Spielberg| ≤ 1i
SP7 = h|Sound = Mono| ≥ 2i
SP8 = h|Genre = War ∨ Genre = Film-noir| = 0i

SP8 ∗ = h|Genre = War ∨ Genre = Film-noir| ≥ 4i
SP8 ∗∗ = h|Genre = Film-noir| ≥ 4i
SP9 = h|Location = North America| > 1i
SP10 = h|Actor = Famous ∨ Actress = Famous| = 5i
SP11 = h|Actress = Famous| ≥ 2i
SP12 = h|Genre = Drama| ≥ 2i
SP13 = h|Release Date < 1970| ≤ 1i
SP14 = h|Net Profit ≥ 1000000| ≥ 2i
SP14 ∗∗ = h|Net Profit ≥ 1000000| ≥ 5i

Figure 5: Set-properties used in modeling user preferences in the movies selection domain.
∗

0
Alteration of P14 , to achieve more backtracking - denoted as P14

∗∗

0
00
Further alteration of P14
to achieve even more backtracking - denoted as P14

1. I prefer new movies to old movies, and therefore prefer that all movies be from 2002 or later, and this is important
to me.
2. I love comedies, thrillers and family movies.
3. I prefer not to have too many movies in black and white (not more than one such movie).
4. If all the movies are new (after 2002) then I would prefer to have at least 2 comedies.
5. If I can find at least 2 comedies then I also prefer to have more than 1 family movie, but less then 3 thrillers.
However having the right number of family movies is more important to me than having the right number of
thrillers.
6. If not all the movies are new, I prefer to have at least 2 movies in black and white for the vintage touch.
7. If not all the movies are new, I prefer at least one movie to be directed by Steven Spielberg, but otherwise, I don’t
like his newer films
8. If the previous condition holds, then the number of movies with mono sound may be greater than 2.
9. I prefer not to have any war films or film-noir in the festival. However if this condition can not be satisfied,
then I prefer not to have any films that were filmed in North America and this is more important to me than my
preferences about the movie being in color or in B&W.
10. To draw more attention, I prefer all 5 movies to have famous actors or actresses.
11. To highlight female roles, I prefer at least 2 movies with a famous actress.
12. I prefer to have at least 2 dramas because people tend to think dramas are more sophisticated movies than any
other genre.
13. I prefer to have at least one classical movie.
14. I prefer to have at least one commercially successful movie, i.e. a movie whose net profit was more than one
million dollars.

Figure 6: Informal description of the assumed preferences for selecting a set of movies for a film
festival program.

First, our initial experiments quickly showed that the search in the space of subsets (Table 1)
does not scale up. With just over 20 elements, it did not converge to an optimal solution within an
hour, even when the preference specification involved only 5 set-properties. This outcome holds for
all combinations of qualitative and quantitative preference specifications, depth-first and best-first
schemes of branch-and-bound, and queue ordering based on set’s upper bound, lower bound, and

147

B INSHTOK , B RAFMAN , D OMSHLAK , & S HIMONY

weighted combinations of both. Table 1 provides a snapshot of the corresponding results for a TCPnet specified over nine set properties. The table describes the total number of subsets generated
until an optimal subset was found (see the column “Subset until Sopt ”), the total number of subsets
generated until the optimal subset was recognized as optimal (under “Subsets generated”). DFS appears to be much more effective than BFS, but the branching factor of larger databases overwhelms
this approach. Also, it may be thought that with larger databases it should be easier to quickly
generate good sets, but we found that for moderately larger (e.g,. 25+) and much larger (e.g., 3000)
datasets, this approach is too slow. Various improvements may be possible, but given the much
better performance of the other approach discussed later, they are unlikely to make a difference.

SP2 : SP4 ≻ SP4
SP2 : SP4 ≻ SP4

SP2 : SP3 ≻ SP3
SP2 : SP3 ≻ SP3

SP1 : SP2 ≻ SP2
SP1 : SP2 ≻ SP2

SP1 ≻ SP1

SP3

SP2

SP1

SP4

SP5

SP6

SP8

SP1 ∧ SP6
SP1 ∧ SP6
SP1 ∧ SP6
SP1 ∧ SP6

:
:
:
:

SP7
SP7
SP7
SP7

SP9

SP14

SP7 ∧ SP9
SP7 ∧ SP9
SP7 ∧ SP9
SP7 ∧ SP9

:
:
:
:

SP14
SP14
SP14
SP14

SP8 : SP9 ≻ SP9
SP8 : SP9 ≻ SP9

SP8 ∧ SP9
SP8 ∧ SP9
SP8 ∧ SP9
SP8 ∧ SP9

:
:
:
:

SP12
SP12
SP12
SP12

≻ SP12
≻ SP12
≻ SP12
≻ SP12

SP12

≻ SP7
≻ SP7
≻ SP7
≻ SP7

SP1 : SP6 ≻ SP6
SP1 : SP6 ≻ SP6

SP1 : SP5 ≻ SP5
SP1 : SP5 ≻ SP5

SP8 ≻ SP8

SP7

SP13

SP11

SP9 : SP13 ≻ SP13
SP9 : SP13 ≻ SP13

SP10 : SP11 ≻ SP11
SP10 : SP11 ≻ SP11

≻ SP14
≻ SP14
≻ SP14
≻ SP14

SP10
SP14 : SP10 ≻ SP10
SP14 : SP10 ≻ SP10

Figure 7: TCP-net model of preference over sets of movies for the film festival program.
Next, we consider the CSP-space branch-and-bound search. In particular, here we compared
between the two variants of this approach that use dynamic and static variable and value orderings.
In what follows, these two variants are denoted as BB-D and BB-S, respectively. While static
variable/value orderings are usually considered to be a weaker approach to CSP solving, earlier
we have shown that, in our domain, static ordering allows for certain optimizations that have a
potential to improve the efficiency of the overall problem solving. In particular, static variable
ordering allows to record global NoGoods as described in Section 3.3.1; the results for algorithms
that record NoGoods are denoted by a name suffix “+ng”. In addition, we have tried to share

148

G ENERIC P REFERENCES OVER S UBSETS OF S TRUCTURED O BJECTS

S8
S8
S10
S10
S15
S15
S20
S20

Method
BFS
DFS
BFS
DFS
BFS
DFS
BFS
DFS

Subsets until Sopt
18
83
40
672
7879
11434
28407
28407

Subsets generated
4075
630
15048
2935
104504
30547
486079
231616

Time (sec)
0.56
0.19
2.34
0.47
68.23
3.13
1584.67
28.578

Table 1: A snapshot of the results for subsets-space search. The preferences here are specified by a
TCP-net over nine set properties.
Method

S400

S1000

S1600

S3089

P5
P5
P5
P5
P5

BB-D
BB-S
BB-S+inc
BB-S+ng
BB-S+ng+inc

0.3
0.14
0.05
0.17
0.05

0.77
0.14
0.1
0.1
0.11

1.30
0.17
0.12
0.15
0.13

4.02
0.25
0.18
0.21
0.19

P9
P9
P9
P9
P9

BB-D
BB-S
BB-S+inc
BB-S+ng
BB-S+ng+inc

0.43
0.14
0.06
0.17
0.06

1.42
0.24
0.14
0.25
0.14

2.42
0.26
0.17
0.34
0.18

6.58
0.34
0.15
0.35
0.17

P14
P14
P14
P14
P14

BB-D
BB-S
BB-S+inc
BB-S+ng
BB-S+ng+inc

0.66
0.17
0.06
0.3
0.1

2.03
0.43
0.15
0.57
0.19

4.69
1.09
0.43
1.06
0.38

14.92
0.78
0.5
0.95
0.54

0
P14
0
P14
0
P14
0
P14

BB-S
BB-S+inc
BB-S+ng
BB-S+ng+inc

6.5
2.1
16.1
4.68

27.1
27
19.4
18.4

278
259
54.8
76.3

–
–
230.2
210.8

00
P14
00
P14
00
P14
00
P14
00
P14

BB-D
BB-S
BB-S+inc
BB-S+ng
BB-S+ng+inc

4113.48
101.4
81.03
110
107.9

–
5370
5523
269.9
266.8

–
16306
16643
646.1
646.8

–
–
–
3335
3013

Set-properties

Table 2: Empirical results of evaluating the CSP-space search procedures with qualitative preference specification using TCP-nets.

information between consecutive CSP problem instances while doing the search in the tree of CSPs;
the algorithms adopting this technique are denoted by a name suffix “+inc”.
Table 2 depicts the results of the evaluation of all variants of the CSP-space branch-and-bound
search algorithm (Figure 3). First, the table shows that the overhead of maintaining NoGoods does
not pay off for the simple preference specifications. However, for the more complex problems requiring more intense CSP solving, the use of NoGood recording proved to be very useful, letting us

149

B INSHTOK , B RAFMAN , D OMSHLAK , & S HIMONY

Method

S400

S1000

S1600

S3089

P5
P9

BB-S
BB-S

0.24
0.16

0.14
0.25

0.17
0.28

0.26
0.41

P14
P14

BB-S+inc
BB-S+ng+inc

38.91
19

2376.40
160.53

–
494.98

–
1349.9

Set-properties

Table 3: Results for the CSP-space search with quantitative preference specification using GAI
value functions.

solve previously unsolvable instances. Next, the reader may notice from the table that, at least for
the problems used in our tests, the contribution of the incremental approach is not substantial. For
instance, NoGood recording by itself seems to contribute much more to the efficiency of the optimization process. Moreover, for the more complex problems, switching to the incremental version
sometimes even leads to performance degradation. It appears that the overhead of maintaining and
copying the partial solution in these cases does not pay off.
Our next set of experiments mirrored the first one, but now with GAI value functions instead
of the purely qualitative TCP-nets. The GAI functions were obtained by properly quantifying the
qualitative preferences used for the first tests. Table 3 provides a representative snapshots of the
results. With value functions over set-properties P5 and P9 the basic branch-and-bound algorithm
with static variable/value orderings performs and scales up (with growing set of alternatives S) quite
well. With the more complex value functions over the larger set of properties P14 the performance
significantly degrades, and even the incrementality-enhanced algorithm cannot solve problem instances with more than 1000 CSP variables. On the other hand, adding NoGoods recording proves
to dramatically improve the performance, leading to solving even the largest problem instances.
Tables 2 and 3 suggest a qualitative difference in the performance of the CSP-space search with
quantitative and qualitative preference representation models. There are good reasons to expect
such behavior. First, compact qualitative models of preference may (and typically do) admit more
than one optimal (that is, non-dominated) solution. That, in principle, makes finding one such
optimal solution easier. Second, if the preferences are captured by a TCP-net, then there are variable
orderings ensuring that the first solution found will be an optimal one. In contrast, with GAI value
functions, after we generate an optimal solution, typically we still have to explore the search tree to
prove that no better solution exists. In the worst case, we have to explore the entire tree of CSPs,
forcing us to explore a number of CSPs that is exponential in |P|.
In summary, the first conclusion to be taken from our experiments is that subsets-space search
fails to escape the trap of the large branching factor, while the stratified procedures for CSP-space
search show a much higher potential. On the problems that require little backtracking in the space of
CSPs, the latter procedures are actually very effective for both TCP-net and GAI function preference
specification. Obviously, if the procedure is forced to explore many different CSPs, the performance
unavoidably degrades. We note that, on larger databases, such backtracks often indicate an inherent
conflict between desirable set-properties, and such conflicts might possibly be recognized and resolved off-line. In this work we do not investigate this issue, leaving it as an optional direction for
future improvement.
The rather non-trivial example used in this section provides the reader also with the opportunity
to assess the suitability of different preference specification languages. For example, although we

150

G ENERIC P REFERENCES OVER S UBSETS OF S TRUCTURED O BJECTS

used boolean-valued set properties, it may be argued that some of our natural-language preference
statements would better be expressed using integer-valued set properties. Similarly, users may find
that some other preference specification formalism, such as soft-constraints (Bistarelli et al., 1999),
can more naturally capture these natural language preferences. This is an opportunity for us to
reemphasize that while, for obvious reasons, we had to focus on a concrete choice of language, we
believe that the two-tiered approach suggested here is far more general.

5. Complexity Analysis
Though reasonable runtimes have been obtained by us empirically with search over CSPs, both
algorithm classes described above have a worst-case exponential running time. This begs the question of whether the problem itself is computationally hard. Obviously, with external constraints,
subset optimization is NP-hard. Below we show that even without external constraints, the problem
typically remains NP-hard, even with significant restrictions on the problem.
Naturally, the complexity of subset selection depends on the precise nature of the preference
specification formalism used. Most of the results presented here assume TCP-net-based specification. Hardness results for this model immediately apply to the GAI model, based on an existing
reduction (Brafman & Domshlak, 2008). In some cases, problems that are tractable under the TCPnet model become NP-hard when a GAI model is used, instead. Thus, unless stated otherwise, we
assume henceforth that preferences over properties are specified by a TCP-net.
In analyzing the complexity of the problem we consider the following problem parameters:
• n, the overall number of items in the data set.
• a, the number of attributes of the items.
• m, the number of set properties, i.e. number of nodes in the TCP-net.
• k, maximal property formula size, defined as the number of logical connectives (and, or, not)
in the formula.
• d maximum attribute domain size, i.e. the maximum number of distinct values for each
attribute.
• µ, the number of times an attribute value can appear in the dataset.
5.1 NP-Hard Classes
Theorem 1. When using TCP-based preferences over set properties, finding an optimal subset of a
given set of items (POS) is NP-hard even if the items are described only in terms of binary-valued
attributes, and all the set properties are atomic (that is, we have d = 2 and k = 0).
Proof. The proof is by a polynomial reduction from the well-known NP-hard Vertex Cover (VC)
problem. Given a graph G = (V, E), a vertex cover of G is a vertex subset V 0 ⊆ V covering all the
edges in the graph, that is, for every edge e ∈ E, there is a vertex v ∈ V 0 such that e is incident on
v. The optimization version of VC corresponds to finding a minimal size vertex cover of G.
Given an VC problem instance G = (V, E), we construct a POS problem instance by specifying a TCP-net N and an item set S as follows. For each vertex v ∈ V we create an item o (denoted
151

B INSHTOK , B RAFMAN , D OMSHLAK , & S HIMONY

Pe1 ≻ Pe1

Pe2 ≻ Pe2

Pe3 ≻ Pe3

Pe1

Pe2

Pe3

Pek ≻ Pek
...

Pek

SUM
SUM = 0 ≻ SUM=1 ≻ SUM=2 ≻ . . . ≻ SUM=n

Figure 8: TCP-net in the reduction from VC to POS in the proof of Theorem 1.
by ov ), and thus we have |S| = |V | = n items. For each edge e ∈ E we define an attribute X
(denoted by Xe ), and thus we have |X | = |E| = a attributes. All the attributes in X are defined
to have a binary, {0, 1}, domain. For each item ov , the value of each attribute Xe is ov [Xe ] = 1
if and only if e is incident on v in G. Next, for each edge e ∈ E, we define a binary set property
Pe = h|Xe | > 0i that takes the value true if and only if at least one item in the selected subset provides the value 1 to the attribute Xe . In addition, we define a single multi-valued empty set property
SUM ≡ h||i5 . The domain of the SUM property is defined to be the integer-value range [0..n].
Note that, by construction, the properties utilize only one attribute per property, and thus no logical
connectives, providing us with k = 0. The preferences over these set properties are
1. For each binary property Pe , the preference is for the value true, that is, Pe  Pe .
2. For the empty property SUM we simply prefer smaller values, that is
(SUM = 0)  (SUM=1)  (SUM=2)  . . .  (SUM=n)
The only edges in the TCP-net N , depicted in Figure 8, are the importance arcs from each Pe to
SUM, meaning that we would rather have to temporize in the value of the SUM property than have
any of the Pe being f alse.
Proposition 1 ensures that any optimal subset in the POS problem constructed as above always
corresponds to a proper vertex cover of G.
Proposition 1. For any subset S of S that is undominated with respect to the constructed TCP-net
N , and every edge e ∈ E, we have Pe (S) = true.
Proof. Given an undominated (with respect to N ) subset S ⊆ S, let Pe be a set property such that
Pe (S) = f alse. By construction, there exists an item o ∈ S such that o[Xe ] = 1. Considering
S 0 = S ∪ {o}, we have S 0 being preferred to S with respect to N because (i) S and S 0 provide
exactly the same values to all the set properties except for Pe and SUM, (ii) S provides a preferred
5. Since the formula ϕ inside this set property is degenerate, and in fact equivalent to h|true|i, every item in the selection
set will have to comply with it. This set property is the simplest implementation of a counter

152

G ENERIC P REFERENCES OVER S UBSETS OF S TRUCTURED O BJECTS

value to SUM while S 0 provides a preferred value to Pe , and (iii) preferential improvement of Pe
dominates that of SUM. Thus S 0 dominates S, contradicting the assumption that S is undominated.
Lemma 1. For any subset S of S that is undominated with respect to the constructed TCP-net N ,
there exists a vertex cover VS of G with |VS | = |S|.
Proof. The proof is straightforward. Let VS = {v | ov ∈ S}. Because S is undominated with
respect to N , from Proposition 1 we have Pe (S) = true for all binary “edge-related” properties
Pe . In turn, Pe (S) = true implies that o[Xe ] = 1 for at least one item o ∈ S. By the construction,
o[Xe ] = 1 if and only if vertex v covers edge e. Together with the mapping between the vertices V
and items S being bijective, the latter implies |VS | = |S|.
Lemma 2. There exists a minimal vertex cover of G of size s if and only if there exists a subset
S ⊆ S undominated with respect to N such that SUM(S) = s.
Proof. Let S be an undominated subset of S with |S| = s. By construction, we have Pe (S) = true
for all binary set properties Pe , and SUM(S 0 ) = s. By Lemma 1, there exists a vertex cover VS
of G with |VS | = s. Suppose to the contrary that VS is not minimal, that is, there exists a vertex
cover V 0 of G with |V 0 | < s. Now, construct the subset S 0 = {ov | v ∈ V 0 }. Since the mapping
between S and V is bijective, we have |S 0 | = |V 0 | < s, and thus SUM(S 0 ) < s. Likewise, by
construction of our set properties and V 0 being a vertex cover, we have Pe (S) = true for all Pe .
This, however, implies that S 0 is preferred to S with respect to N , contradicting the statement that
S is undominated.
Theorem 1 now follows immediately from Lemma 2 and the fact that the reduction is clearly
polynomial.
Theorem 2. Given TCP-based preferences over set properties, finding an optimal subset of a given
set of items (POS) is NP-hard even if the items are described in terms of a single attribute, all the
set properties are binary-valued, each containing at most 2 logical connectives (that is, we have
a = 1 and k = 2).
Proof. The proof is by a polynomial reduction from k-SAT, for any k ≥ 3. Given a k-SAT problem
instance over propositional variables V and logical formula Φ, we construct a POS problem instance by specifying a TCP-net N and an item set S as follows. For each variable v ∈ V , construct
an item ov and an item ov̄ , and thus S contains an item for every possible literal in the formula. The
value of the only attribute X is defined as follows: for each item ol , we have A(ol ) = l (where l is a
literal, either v or v̄, for all v ∈ V ). The binary set properties P for the TCP-net N are now defined
as follows.
• Properties ensuring that a variable assignment is legitimate. For each variable v ∈ V ,

Pv =h|X = v ∨ X = v̄| = 1i,
that is, for any S ⊆ S, Pv (S) = true if and only if S contains exactly one of the items
{ov , ov̄ }.
153

B INSHTOK , B RAFMAN , D OMSHLAK , & S HIMONY

• Properties ensuring that Φ is satisfied. For each clause C = (l1 ∨ l2 ∨ l3 ∨ ...) ∈ Φ:
PC =h|X = l1 ∨ X = l2 ∨ X = l3 ∨ ...| ≥ 1i
that is, for any S ⊆ S, PC (S) = true if and only if S contains at least one item corresponding
to a literal in C.
Finally, to complete the preference specification, we make all properties independent (that is, the
TCP-net has no edges), and for each of the properties we prefer value true to value false.
To illustrate the above construction, consider a 3-SAT formula Φ = (x ∨ y ∨ z) ∧ (y) ∧ (x ∨ z).
For this formula, the construction leads to
item
ox
ox
oy
oy
oz
oz

X
x
x̄
y
ȳ
z
z̄

Set properties:
Px =h|X = x ∨ X = x| = 1i
Py =h|X = y ∨ X = y| = 1i
Pz =h|X = z ∨ X = z| = 1i
PC1 =h|X = x ∨ X = y ∨ X = z| ≥ 1i
PC2 =h|X = y| ≥ 1i
PC3 =h|X = x ∨ X = z| ≥ 1i

We now show that finding an undominated subset of S with respect to N as above is equivalent
to finding a satisfying assignment to Φ. Let S ⊆ S be undominated with respect to N . We can show
that S provides value true to all set propositions Pv and PC (in this case we call S an ultimately
preferred subset) if and only if Φ is satisfiable.
First, let S be an ultimately preferred subset of S. Given such S, we can construct a mapping
A : V 7→ {true, f alse} such that A(v) = true if ov ∈ S, and A(v) = f alse if ov̄ ∈ S. Note
that A is well-defined because, for an ultimately preferred subset S, all Pv (S) = true, and thus,
for each v ∈ V , exactly one item from {ov , ov̄ } is present in S. Clearly, A is a legal assignment for
Φ. In addition, we have all PC (S) = true. Thus, for each clause C ∈ Φ, at least one item with
X = li ∈ C belongs to S. By construction, this implies that A satisfies all the clauses in Φ, and
thus Φ is satisfiable.
Converesly, suppose that S ⊆ S is preferentially undominated with respect to N , but is not
ultimately preferred. If our POS problem has such an undominated subset S, we show that Φ is
unsatisfiable. Assuming the contrary, let A be a satisfying assignment of Φ. Given A, we construct
a subset SA ⊆ S as SA = {ol | literal l ∈ A}, and show that SA dominates S with respect to N
(contradicting the assumed undominance of S, and finalizing the proof of Theorem 2).
By construction, since A is a legal assignment to V , we have Pv (SA ) = true for all set properties Pv . Also, since A is a satisfying assignment for Φ, we have PC (SA ) = true for all set
properties PC . Therefore, SA is actually an ultimately preferred subset of S. Finally, since all the
set properties P are preferentially independent in N , and value true is always preferred to value
f alse for all the set properties, we have that SA dominates S with respect to N .
Notice that Theorems 1 and 2 do not subsume each other. Theorem 1 poses no restriction on the
number of item attributes in the problem instance, but does restrict the domain of all the attributes.
Theorem 2 restricts the number of attributes to 1, but has no restriction on the domain size of this
attribute, and its restriction on the property size is looser than that imposed in Theorem 1.
154

G ENERIC P REFERENCES OVER S UBSETS OF S TRUCTURED O BJECTS

Finally, we note that tightening the condition of Theorem 2, by allowing only at most 1 connective in each set-property definition prevents us from using the same reduction as in the proof
of Theorem 2 because the respectibe satisfiability problems would be the polynomial-time solvable
2-SAT problems. Our conjecture, however, is that this fragment of POS is still NP-hard. In fact, in
Section 5.3 we show that the corresponding fragment of POS with the GAI preference specification
(instead of TCP-nets) is indeed NP-hard.
5.2 Tractable Classes
Several tractable classes of POS, obtained by further restricting the problem class discussed in
Theorem 2, and characterized by single-attribute item description (that is, a = 1), are discussed
below. In both trivially tractable (Section 5.2.1) and non-trivially tractable (Section 5.2.2) cases, we
assume that the relational symbols are either equalities or inequalities, that in the specification of
a property only equalities (“attribute = value”) are used, and in addition we do not allow an empty
set property to be specified. The latter restriction is due to the fact that the empty set property is
somewhat special, as it enriches the descriptive power by allowing one to simulate an additional
attribute in certain cases, and the single-attribute restriction is crucial for our tractability result.
Before we proceed with the actual results, note that, with a single-attribute item description, no
two set properties can be in a conflict that demands backtracking while choosing items (i.e. during
CSP solution). To illustrate such conflicts, consider the following examples.
1.

1.a h|A = ai | ≤ 5i
1.b h|A = ai | ≤ 3i

Set property 1.a is redundant, subsumed by 1.b

2.

2.a h|A = as | = 5i
2.b h|A = as | > 6i

One of these set properties must be false.

3.

3.a h|A = al | < 7i
3.b h|A = al | ≥ 9i

One of these set properties must be false.

All such conflicts between set-properties can be resolved offline, prior to the actual process of subset
selection, totally disregarding the available items. Hence, within the process of subset selection, we
assume that there are no conflicts between set properties. Consequently, subset selection can be
done in a greedy manner.
5.2.1 T RIVIALLY T RACTABLE C LASS
Theorem 3. Finding an optimal subset of a given set of items (POS) with respect to a TCP-net
preference specification is in P if the items are described in terms of a single attribute, and all the
set properties are atomic (that is, we have a = 1 and k = 0).
An algorithm for the problem class in Theorem 3 is depicted in Figure 9. The algorithm runs in
time O(m2 n), where m is the number of set properties and n is the number of available items S.
The for loop in line 4 of the algorithm iterates over all the set properties, each time checking compatibility with the previously considered properties, which requires Θ(m2 ) time. The procedures
G ET S ATISFYING S ET (·) and H AS S ATISFYING S ET (·) have to process each item in S only once.
Hence, the total running time of the algorithm is O(m2 n).6
6. This runtime analysis does not include the ordering of the TCP-net variables that is assumed to be given. One way to
do that would be a topological sort of the net, that obviously can be done in polynomial time.

155

B INSHTOK , B RAFMAN , D OMSHLAK , & S HIMONY

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

Sopt ← ∅
Fix a preference ordering over set properties P
Pass ← ∅
for each property P ∈ P do
while (not (P .isSatisfied)) do
if P is in conflict with Pass then
Set next value to P w.r.t. Pass
else
if H AS S ATISFYING S ET(P ) then
Sopt ← Sopt ∪ G ET S ATISFYING S ET(P )
P .isSatisfied ← true
Pass ← Pass ∪ {P }
end if
end if
end while
end for
return Sopt
procedure G ET S ATISFYING S ET(P )
S←∅
for each item o ∈ S do
if o has the property value defined by P then
S ← S ∪ {o}
end if
if |S| P .op P .cardinality then
return S
end if
end for
end procedure

. Offline conflict resolution

. If cardinality of S satisfies P

Figure 9: A polynomial-time algorithm for the POS problems with TCP-net preference specification, single-attribute item description, and all the set properties being atomic (that is,
a = 1 and k = 0).

5.2.2 N ON -T RIVIALLY T RACTABLE C LASS
At the end of Section 5.1 we have mentioned that the complexity of POS under limiting the setproperty description to at most one logical connective is still an open problem. If, however, we
impose the limitations summarized in Table 4, we can show that the problem becomes tractable.
Theorem 4. Finding an optimal subset of a given set of items (POS) with respect to a TCP-net
preference specification is in P if it is restricted as in Table 4.
First we should discuss the implicit limitations (or special problem properties) that are imposed
by the explicit limitations listed in Table 4.
156

G ENERIC P REFERENCES OVER S UBSETS OF S TRUCTURED O BJECTS

1. All the items have only one attribute (a = 1)
2. All the property formulas have at most 1 connective (k = 1), and are positive (that is, we
disallow negation)
3. The empty property is disallowed
4. The number of attribute value appearances is limited to at most µ = 1 (that is, values in the
attribute domain cannot be repeated)
Table 4: Characteristics of the tractable subclass of POS presented in Section 5.2.2.
1. The restriction to at most one attribute-value appearance in the data set provides a one-toone correspondence between attribute values and items in S. This means that each item can
uniquely represent a specific attribute-value combination, and vice versa.
2. The restriction to a single-attribute item description renders the “∧” connective redundant.
That is because the properties using the “∧” logical connective can only be of the form:
X = xi ∧ X = xj .
(Without loss of generality we assume i 6= j, or otherwise we can simply drop one of the
terms.) These properties obviously cannot be satisfied because no item can have two different
values for the only attribute X. In fact, set properties defined this way are equivalent to a
property that is always f alse.
3. The only relevant cardinalities for the set properties are [0..2]. A property defined using only
one connective with the restriction on the number of repetitions is not expressive enough to
state a set property involving more than 2 items. If the value in a set property:
h|A = ai ∨ A = aj |

op

valuei

is greater than 2, and op ∈ {≥, >}, then again it cannot be satisfied. If the op of a property is
≤ or <, and the value is greater than 2, then it can be substituted by an effectively equivalent
set property with op being ≤ and value = 2 .
The algorithm for the problem class in Theorem 4 is depicted in Figure 10. This algorithm bears
some similarity to the algorithm in Figure 9, except that here the procedures G ET S ATISFYING S ET
and H AS S ATISFYING S ET reason simultaneously about satisfaction of collections of set-property
values, and do that by utilizing 2-SAT solving. Specifically, in Table 5 we show how any valid
property in such a POS problem can be translated into a 2-SAT CNF formula. In Lemma 3 we
prove the correctness of this translation. We should note that by using 2-SAT we can have an
answer to the question “Is there a subset of items satisfying some already evaluated set-property
values”. The procedures G ET S ATISFYING S ET and H AS S ATISFYING S ET use the aforementioned
reduction to 2-SAT to provide the answer in polynomial time.
Lemma 3. There is a subset S satisfying all the property-values Pass if and only if there is a
satisfying assignment A to the 2-SAT formula constructed from Pass .
157

B INSHTOK , B RAFMAN , D OMSHLAK , & S HIMONY

h|X
h|X
h|X
h|X
h|X
h|X

= xi | > 2i ⇒ infeasible
= xi | ≥ 2i ⇒ infeasible
= xi | ≥ 1i ⇒ substituted by
= xi | = 1i and translated to (vi ) clause
= xi | ≥ 0i ⇒ translated to (vi ∨ v¯i ) clause
= xi | = 0i ⇒ translated to (v¯i ) clause
Properties having 0 logical connectives

h|X = xi ∨ X = xj | > 2i ⇒ infeasible
h|X = xi ∨ X = xj | ≥ 2i ⇒ substituted by
h|X = xi ∨ X = xj | = 2i and translated to (vi )
and (vj ) clauses
h|X = xi ∨ X = xj | ≥ 1i ⇒ translated to
(vi ∨ vj ) clause
h|X = xi ∨ X = xj | = 1i ⇒ translated to
(vi ∨ vj ) and (v¯i ∨ v¯j ) clauses
h|X = xi ∨ X = xj | ≥ 0i ⇒ translated to
(v¯i ∨ v¯j ) clause
h|X = xi ∨ X = xj | = 0i ⇒ translated to (v¯i )
and (v¯j ) clauses
Properties having 1 logical connective

Table 5: Translation of the set properties for the POS subclass in Section 5.2.2 to 2-SAT.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

Fix a preference ordering over set properties P
Sopt ← ∅
Pass ← ∅
for each property P ∈ P do
while (not (P .isSatisfied)) do
Set next value to P w.r.t. Pass
if H AS S ATISFYING S ET(Pass ) then
Sopt ← G ET S ATISFYING S ET(Pass )
P .isSatisfied ← true
Pass ← Pass ∪ {P }
end if
end while
end for
return Sopt

. Use reduction to 2-SAT
. Use reduction to 2-SAT

Figure 10: A poly-time algorithm for the POS problems with TCP-net preference specification,
and characteristics as in Table 4.

Proof. By construction, we have an injective correspondence between the properties in the POS
problem and clauses in the 2-SAT problem. Every property P ∈ P injectively corresponds to a
certain clause ϕP . Every item o ∈ S injectively corresponds to a propositional variable vi ∈ V .
Thus, the correspondence between the selected subset S and the assignment A is simply
vi = true ⇔ o ∈ S.

158

(1)

G ENERIC P REFERENCES OVER S UBSETS OF S TRUCTURED O BJECTS

Because the translation is injective and rather straightforward (without introducing any auxiliary
clauses or properties), it is trivial that S is a subset that satisfies all the properties in Pass if and only
if A is an assignment that satisfies all the clauses in the corresponding 2-SAT formula.
The above shows correctness of the algorithm in Figure 10, and finalizes the proof of Theorem 4.
5.3 Complexity of POS: TCP-nets vs. GAI Preference Specification
With the restrictions as in Table 4 we were able to show that the POS problem with TCP-net
preference specification is tractable by reduction to 2-SAT, because there is no need to backtrack
while searching in the attribute value space. An interesting question is, what if the specification
were done using GAI functions?
Theorem 5. Finding an optimal subset of a given set of items (POS) with respect to a GAI preference specification is NP-hard even if the items are described in terms of a single attribute, all the set
properties are binary-valued, each containing at most 1 logical connective (that is, we have a = 1
and k = 1).
Proof. The proof is by a polynomial reduction from MAX-2SAT. As far as item definitions and
properties are concerned, the reduction is essentially the same as the reduction from k-SAT in the
proof of Theorem 2. That is, for each variable v ∈ V , construct an item ov and an item ov̄ . The
value of the only attribute X is defined as follows: for item ol , we have A(ol ) = l (where l is a
literal, either v or v̄, for all v ∈ V ). Set properties are also as in the proof of Theorem 2, but now
they are limited to only 2 variables per clauses (re-stated for convenience below):
• For each variable v ∈ V :

Pv =h|X = v ∨ X = v̄| = 1i,
that is, properties ensuring that a variable assignment is legitimate.
• For each clause C = (l1 ∨ l2 ) ∈ Φ:

PC =h|X = l1 ∨ X = l2 | ≥ 1i,
that is, properties ensuring that Φ is satisfied.
The value function specification is such that legitimate variable assignments are enforced, and a
larger number of clauses satisfied is preferred. This is achieved by using an additively independent
value function (i.e., where each factor contains a single variable), with values being as follows.
Each clause-satisfying property has a value of 1 for being true, and 0 for being f alse. Each literalsatisfying property has a value of 0 for being true, and a negative value of −2m for being f alse,
where m is the number of clauses.
Lemma 4. Given a GAI value function and item set S constructed as above for a 2-CNF formula
Φ, there exists a subset S ⊆ S with value of U (S) = p if and only if there exists an assignment A
satisfying p clauses in Φ.
159

B INSHTOK , B RAFMAN , D OMSHLAK , & S HIMONY

Proof. Let S be any subset of S that has non-negative value. This implies by construction (since
there are only m clause-satisfying properties PC ) that all literal-satisfying properties must be true
for S, and the respective assignment AS can be constructed as in Equation 1. Conversely, let A
be a legitimate assignment to the variables V . One can define a corresponding set SA , for which
(by construction) all properties Pv are true. Also, observe that by construction the number of PC
set properties that are true on SA is the same as the number of clauses satisfied by the assignment
A.
The theorem follows immediately from the properties of the construction of the set properties
and preferences.
At the end of Section 5.1 we have noted that if the restrictions on the problem parameters are
more severe than in Theorem 2, by limiting the number of logical connectives per set property to
at most 1, we can no longer show whether the problem is tractable or NP-hard under the TCPnet preference specification. However, Theorem 5 shows that, with preferences specified using a
GAI value function, the problem is in fact NP-hard. Moreover, the problem class from Theorem
5 subsumes the class from Theorem 4, and thus provides an additional result showing that even
though with the TCP-net specification the respective problem is tractable, with a GAI preference
specification it becomes NP-hard.

6. Related Work
In the introduction, we mentioned the closely related work of desJardins and Wagstaff (2005). In
that approach, the motivation to provide the user with a diverse collection of values is either to
reflect the set of possible choices better for applications where the user must eventually select a
single item, or when the diversity of the selected set is an objective on its own. The work of Price
and Messinger (2005) is explicitly concerned with this problem. Specifically, they consider the
problem of recommending items to a user, and view it as a type of subset selection problem. For
example, suppose we want to recommend a digital camera to a user. We have a large set of available
cameras, and we are able to recommend k cameras. Price and Messinger consider the question
of how to select this set, proposing that the candidate set will maximize the expected value of the
user’s choice from this set. They suggest a concrete algorithmic approach for handling this problem.
The input to their problem is some form of partial representation of the user’s preferences (which
can be diverse, as in our work) and naturally, the concrete techniques are different from ours. Both
these papers share the assumption on ranking sets, common to most previous work as discussed
by Barberà et al. (2004), that ultimately one item will be selected from this set. However, they
do not necessarily start out with an initial ranking over single items, and as in our case, the work
of desJardins and Wagstaff utilizes the attribute value of items in the selection process.
Earlier work on ranking subsets was motivated by problems such as the college admissions
problem (Gale & Shapley, 1962), where we need to select the best set of fixed cardinality among a
pool of college candidates. The admissions officer has various criteria for a good class of students
and wishes to come up with an optimal choice. Some of the key questions that concerned this line
of work were what are good properties of such set rankings and whether they have some simple
representation. An example of a property of the set ranking that may be desirable is the following:
given a set S, if we replace some member c ∈ S with some other member c0 to obtain the set S 0 ,
and c0 is preferred to c, then S 0 is preferred to S. An example of a representation of the ranking
160

G ENERIC P REFERENCES OVER S UBSETS OF S TRUCTURED O BJECTS

is an additive representation where items are associated with real values and one set is preferred to
another if the sum of its elements’ values is larger. It would be interesting to study similar question
in our context of structured objects.
This question of ranking sets appears in other areas, such as logics of preference and likelihood.
For example, the main question considered by Halpern (1997) is how to construct an ordering over
formulas based on an ordering over truth assignments. Formulas are associated with the set of
worlds in which they are satisfied, and hence, the question of comparing the likelihood of formulas
ψ and φ corresponds to that of ranking their respective set of models given the initial ranking on
single models. Much work on non-monotonic logics uses Shoham’s preference semantics (Shoham,
1987), and semantically, such work (see, e.g., Kraus, Lehmann, & Magidor, 1990) can be viewed as
attempting to answer the opposite question – define a ranking over single truth assignments given
some, possibly partial, ordering over formulas, i.e., sets of models.
A number of lines of work are related to our specification and solution methods. The first is
the work on Russian Doll Search (RDS), a well known algorithm for combinatorial optimization,
originally presented by Verfaillie, Lemaı̂tre, and Schiex (1996) as an efficient algorithm for Constraint Optimization Problems (COP). The idea behind the approach is to solve consecutively harder
problems. Initially, the problem is solved while considering only one variable. The optimal result
provides a lower bound. Each iteration, additional variables are considered, until eventually the original problem is solved. By using the lower bound obtained from the previous iteration (and other
optimizations) this technique is often able to solve the original problem more efficiently. Recently
Rollon and Larrosa (2007) extended Russian Doll Search to support multi-objective optimization
problems. In a multi-objective optimization problem the goal is to optimize several parameters
(attributes) of the variables in the problem. Usually all the parameters cannot be simultaneously
optimized. The technique of Rollon and Larrosa involves incremental solution with more and more
objectives included, and, in this sense, it is related to our search over CSPs approach in which we
incrementally consider more and more set properties. Indeed, different desirable set properties can
be viewed as different objectives.
Another related area is that of Pseudo-Boolean Constraint (PBC) Satisfaction Problems (Sheini
& Sakallah, 2005). A PBC has the form:
X
wi li ≥ k.
i

Here the li ’s are literals and we interpret their values as being either 0 (false) or 1 (true); the wi are
real-valued coefficients; and k is an integer. Thus Pseudo-Boolean CSPs are a special form of integer programs, and can nicely represent the cardinality constraints we generate. Thus, one option for
solving the type of CSPs generated here would be using a dedicated PBC solver. We run several popular PBC solvers on the satisfiability instances generated during the optimization: Pueblo (Sheini &
Sakallah, 2005), MiniSat (Eén & Sörensson, 2005), and Galena (Dixon & Ginsberg, 2002). These
solvers showed comparable results for satisfiable cases, while for the unsatisfiable cases, the PBC
solvers showed better performance. This appears to be due to their use of linear programming as a
preliminary test for satisfiability.
Another line of work that bears important connection to ours is that of winner determination
in combinatorial auctions. In regular auctions, bidders bid for a single item. In combinatorial auctions, bidders bid on bundles of items. Thus, bidders must provide their preferences over different
subsets of the set of auctioned items. The goal in combinatorial auctions is to allocate the set of
161

B INSHTOK , B RAFMAN , D OMSHLAK , & S HIMONY

goods to different bidders in the best manner (e.g., maximizing the payment to the seller or maximizing total welfare). This differs from the problem of selecting a single optimal subset with which
we are concerned. However, in both cases, preferences over subsets must be provided to the optimization algorithm. As the number of subsets is exponential in the number of items, researchers in
combinatorial auctions have sought bidding languages that can succinctly describe preferences of
interest (Boutilier & Hoos, 2001; Nisan, 2006). What distinguishes our specification approach is
its reliance on the existence of item features and the desire to provide a generic specification that
does not depend on the concrete set of items. Work in combinatorial auctions also attempts to break
the specification in some way. This is typically done by specifying values for small bundles and
providing rules for deriving the value of larger sets from the values of the smaller sets.

7. Conclusion
We suggested a simple, yet general approach to lifting any attribute-based preference specification
formalism to one for specifying preferences over sets. We then focused one instantiation of this idea
via a concrete language for specifying set properties, and suggested two methods for computing an
optimal subset given such a specification. One method is based on searching the space of explicit
subsets, while the other searches over implicit subsets represented as CSPs. Both search spaces
are meaningful regardless of the specific underlying preference specification algorithm although the
precise search and bounds generation method will vary. We focused on two concrete and popular
specification formalisms, one qualitative and one quantitative, on which we experiment and provide complexity results. Although the problem is generally NP-hard, as expected, the experimental
results are quite encouraging.
We wish to reemphasize that other choices, both for the set property language and the preference specification formalism are possible, and may be more appropriate in various cases. Indeed,
an interesting topic for future research would be to see which choices fit best some natural application areas; whether and how the algorithm presented in this paper can be modified to handle such
languages; and how the complexity of the optimal subset selection problem is affected by such
choices.
Though incremental search over CSPs appears to be the better method for optimal subset selection, it leaves a few questions open. First, it is an interesting question whether an efficient NoGood
recording scheme that does not rely on static variable and value orderings exists. Intuitively, such
a scheme should exist since the CSPs generated can be efficiently encoded into SAT as a boolean
CNF formula (Bailleux & Boufkhad, 2004; Eén & Sörensson, 2005), and clause learning is a well
known technique in SAT solving. Second, we have seen that while the incremental approach usually
improves the overall performance, its contribution is not substantial and what really improves the
performance is better individual CSP solving. This begs two questions: (1) Can we better utilize solutions across CSPs, and (2) Would representing and solving the CSPs generated as pseudo-boolean
CSPs (Manquinho & Roussel, 2006) or SAT instances lead to faster solution times? Naturally,
alternative approaches are also feasible.
Finally, in various applications, the set of elements gradually changes, and we need to adapt
the selected subset to these changes. An example is when we use this approach to choose the
most interesting current articles, and new articles constantly appear. It is likely that in this case the
preferred set is similar to the current set, and we would like to formulate an incremental approach
that adapts to such changes quickly.

162

G ENERIC P REFERENCES OVER S UBSETS OF S TRUCTURED O BJECTS

Acknowledgments
Preliminary versions of this work appeared in (Brafman, Domshlak, Shimony, & Silver, 2006b;
Binshtok, Brafman, Shimony, Mani, & Boutilier, 2007). The authors wish to thank our anonymous
reviewers for their useful comments and suggestions. Brafman was supported in part by NSF grant
IIS-0534662, Brafman and Domshlak were supported by the COST action IC0602, Binshtok, Brafman and Shimony were supported by Deutsche Telekom Laboratories at Ben-Gurion University, by
the Paul Ivanier Center for Robotics Research and Production Management, and by the Lynn and
William Frankel Center for Computer Science.



We consider online planning in Markov decision processes (MDPs). In online planning,
the agent focuses on its current state only, deliberates about the set of possible policies from
that state onwards and, when interrupted, uses the outcome of that exploratory deliberation
to choose what action to perform next. The performance of algorithms for online planning
is assessed in terms of simple regret, which is the agent’s expected performance loss when
the chosen action, rather than an optimal one, is followed.
To date, state-of-the-art algorithms for online planning in general MDPs are either
best effort, or guarantee only polynomial-rate reduction of simple regret over time. Here
we introduce a new Monte-Carlo tree search algorithm, BRUE, that guarantees exponentialrate reduction of simple regret and error probability. This algorithm is based on a simple
yet non-standard state-space sampling scheme, MCTS2e, in which different parts of each
sample are dedicated to different exploratory objectives. Our empirical evaluation shows
that BRUE not only provides superior performance guarantees, but is also very effective in
practice and favorably compares to state-of-the-art. We then extend BRUE with a variant
of “learning by forgetting.” The resulting set of algorithms, BRUE(α), generalizes BRUE,
improves the exponential factor in the upper bound on its reduction rate, and exhibits even
more attractive empirical performance.

1. Introduction
Markov decision processes (MDPs) are a standard model for planning under uncertainty (Puterman, 1994). An MDP hS, A, T r, Ri is defined by a set of possible agent states S, a set of
agent actions A, a stochastic transition function T r : S × A × S → [0, 1], and a reward function R : S ×A×S → R. Depending on the problem domain and the representation language,
the description of the MDP can be either declarative or generative (or mixed). In any case,
the description of the MDP is assumed to be concise. While declarative models provide
the agents with greater algorithmic flexibility, generative models are more expressive, and
both types of models allow for simulated execution of all feasible action sequences, from
any state of the MDP. The current state of the agent is fully observable, and the objective
of the agent is to act so to maximize its accumulated reward. In the finite horizon setting
that will be used for most of the paper, the reward is accumulated over some predefined
number of steps H.
The desire to handle MDPs with state spaces of size exponential in the size of the model
description has led researchers to consider online planning in MDPs. In online planning,
1

the agent, rather than computing a quality policy for the entire MDP before taking any
action, focuses only on what action to perform next. The decision process consists of a
deliberation phase, aka planning, terminated either according to a predefined schedule or
due to an external interrupt, and followed by a recommended action for the current state.
Once that action is applied in the real environment, the decision process is repeated from
the obtained state to select the next action and so on.
The quality of the action a, recommended for state s with H steps-to-go, is assessed in
terms of the probability that a is sub-optimal, and in terms of the (closely related) measure
of simple regret ∆H [s, a]. The latter captures the performance loss that results from taking
a and then following an optimal policy π ∗ for the remaining H −1 steps, instead of following
π ∗ from the beginning (Bubeck & Munos, 2010). That is,
∆H [s, a] = QH (s, π ∗ (s, H)) − QH (s, a),
where


QH (s, a) = Es0 R(s, a, s0 ) + QH−1 (s0 , π ∗ (s0 , H − 1)) .
With a few recent exceptions developed for declarative MDPs (Bonet & Geffner, 2012;
Kolobov, Mausam, & Weld, 2012; Busoniu & Munos, 2012), most algorithms for online
MDP planning constitute variants of what is called Monte-Carlo tree search (MCTS). One
of the earliest and best-known MCTS algorithms for MDPs is the sparse sampling algorithm
by Kearns, Mansour, and Ng (Kearns, Mansour, & Ng, 1999). Sparse sampling offers a nearoptimal action selection in discounted MDPs by constructing a sampled lookahead tree in
time exponential in discount factor and suboptimality bound, but independent of the state
space size. However, if terminated before an action has proved to be near-optimal, sparse
sampling offers no quality guarantees on its action selection. Thus it does not really fit
the setup of online planning. Several later works introduced interruptible, anytime MCTS
algorithms for MDPs, with UCT (Kocsis & Szepesvári, 2006) probably being the most
widely used such algorithm these days. Anytime MCTS algorithms are designed to provide
convergence to the best action if enough time is given for deliberation, as well as a gradual
reduction of performance loss over the deliberation time (Sutton & Barto, 1998; Péret &
Garcia, 2004; Kocsis & Szepesvári, 2006; Coquelin & Munos, 2007; Cazenave, 2009; Rosin,
2011; Tolpin & Shimony, 2012). While UCT and its successors have been devised specifically
for MDPs, some of these algorithms are also successfully used in partially observable and
adversarial settings (Gelly & Silver, 2011; Sturtevant, 2008; Bjarnason, Fern, & Tadepalli,
2009; Balla & Fern, 2009; Eyerich, Keller, & Helmert, 2010).
In general, the relative empirical attractiveness of the various MCTS planning algorithms
depends on the specifics of the problem at hand and cannot usually be predicted ahead of
time. When it comes to formal guarantees on the expected performance improvement over
the planning time, very few of these algorithms provide such guarantees for general MDPs,
and none breaks the barrier of the worst-case only polynomial-rate reduction of simple regret
and choice-error probability over time.
This is precisely our contribution here. We introduce a new Monte-Carlo tree search
algorithm, BRUE, that guarantees exponential-rate reduction of both simple regret and
choice-error probability over time, for general MDPs over finite state spaces. The algorithm
is based on a simple and efficiently implementable sampling scheme, MCTS2e, in which
2

MCTS: [input: hS, A, T r, Ri; s0 ∈ S]
search tree T ← root node s0
while time permits:
ρ ← sample(s0 , T )
T ← expand-tree(T , ρ)
update-statistics(T , ρ)
return recommend-action(s0 , T )
Figure 1: High-level scheme for regular Monte-Carlo tree sampling.
different parts of each sample are dedicated to different competing exploratory objectives.
The motivation for this objective decoupling came from a recently growing understanding
that the current MCTS algorithms for MDPs do not optimize the reduction of simple regret
directly, but only via optimizing what is called cumulative regret, a performance measure
suitable for the (very different) setting of reinforcement learning (Bubeck & Munos, 2010;
Busoniu & Munos, 2012; Tolpin & Shimony, 2012; Feldman & Domshlak, 2012). Our
empirical evaluation on some standard MDP benchmarks for comparison between MCTS
planning algorithms shows that BRUE not only provides superior performance guarantees,
but is also very effective in practice and favorably compares to state of the art. We then
extend BRUE with a variant of “learning by forgetting.” The resulting family of algorithms,
BRUE(α), generalizes BRUE, improves the exponential factor in the upper bound on its
reduction rate, and exhibits even more attractive empirical performance.

2. Monte-Carlo Planning
MCTS, a high-level scheme for Monte-Carlo tree search that gives rise to various specific
algorithms for online MDP planning, is depicted in Figure 1. Starting with the current state
s0 , MCTS performs an iterative construction of a tree T rooted at s0 . At each iteration,
MCTS issues a state-space sample from s0 , expands the tree T using the outcome of that
sample, and updates information stored at the nodes of T . Once the simulation phase is
over, MCTS uses the information collected at the nodes of T to recommend an action to
perform in s0 . For compatibility of the notation with prior literature, in what follows we
refer to the tree nodes via the states associated with these nodes. Note that, due to the
Markovian nature of MDPs, it is unreasonable to distinguish between nodes associated with
the same state at the same depth. Hence, the actual graph constructed by most instances
of MCTS forms a DAG over nodes (s, h) ∈ S × {0, 1, . . . , H}. By A(s) ⊆ A in what follows,
we refer to the subset of actions applicable in state s.
Numerous concrete instances of MCTS have been proposed, with UCT (Kocsis & Szepesvári,
2006) probably being the most popular such algorithm these days (Gelly & Silver, 2011;
Sturtevant, 2008; Bjarnason et al., 2009; Balla & Fern, 2009; Eyerich et al., 2010; Keller
& Eyerich, 2012a). To give a concrete sense of MCTS’s components, as well as to ground
some intuitions discussed later on, below we describe the specific setting of MCTS corresponding to the core UCT algorithm, and Figure 2 illustrates the UCT tree construction,
with n denoting the number of state-space samples.
3

• sample: The samples ρ = hs0 , a1 , s1 , . . . , ak , sk i are all issued from the root node s0 .
The sample ends either when a sink state is reached, that is, A(sk ) = ∅, or when
k = H. Each node/action pair (s, a) is associated with a counter n(s, a) and a value
b a). Both n(s, a) and Q(s,
b a) are initialized to 0, and then updated
accumulator Q(s,
by the update-statistics procedure. Given si , the next-on-the-sample action ai+1 is
selected according to the deterministic UCB1 policy (Auer, Cesa-Bianchi, & Fischer,
2002a), originally proposed for optimal cumulative regret minimization in stochastic
multi-armed bandit (MAB) problems (Robbins, 1952): If n(si , a) > 0 for all a ∈ A(si ),
then
s
"
#
log
n(s
)
i
b i , a) + c
ai+1 = argmax Q(s
,
(1)
n(si , a)
a
P
where n(s) = a n(s, a). Otherwise, ai+1 is selected uniformly at random from the
still unexplored actions {a ∈ A(si ) | n(si , a) = 0}. In both cases, si+1 is then sampled according to the conditional probability P(S|si , ai+1 ), induced by the transition
function T r.
• expand-tree: Each state-space sample ρ = hs0 , a1 , s1 , . . . , ak , sk i induces a state trace
hs0 , s1 , . . . , si i inside T , as well as a state trace hsi+1 , . . . , sk i outside of T . In principle,
T can be expanded with any prefix of hsi+1 , . . . , sk i; a popular choice in prior work
appears to be expanding T with only the upper-most node si+1 . (If T is constructed
as a DAG, it is expanded with the first node along ρ that leaves T .)
• update-statistics: For each node si along ρ that is now part of the expanded tree T ,
the counter n(si , ai+1 ) is incremented and the estimated Q-value is updated as
b
b i , ai+1 ) ← Q(s
b i , ai+1 ) + Ri − Q(si , ai+1 ) ,
Q(s
n(si , ai+1 )
where Ri =

Pk−1
j=i

(2)

R(sj , aj+1 , sj+1 ).

• recommend-action: Interestingly, the action recommendation protocol of UCT was
never properly specified, and different applications of UCT adopt different decision
rules, including maximization of the estimated Q-value, of the augmented estimated
Q-value as in Eq. 1, of the number of times the action was selected during the simulation, as well as randomized protocols based on the information collected at the
root.
The key property of UCT is that its exploration of the search space is obtained by
considering a hierarchy of forecasters, each minimizing its own cumulative regret, that is,
the loss of the total reward incurred by exploring the environment (Auer et al., 2002a).
Each such pseudo-agent forecaster corresponds to a state/steps-to-go pair (s, h). In that
respect, according to Theorem 6 of Kocsis and Szepesvári (2006), UCT asymptotically
achieves the best possible (logarithmic) cumulative regret. However, as recently pointed out
in numerous works (Bubeck & Munos, 2010; Busoniu & Munos, 2012; Tolpin & Shimony,
2012; Feldman & Domshlak, 2012), cumulative regret does not seem to be the right objective
for online MDP planning, and this is because the rewards “collected” at the simulation
4

n=1




n=4

n=3

n=2


Qˆ  8

Qˆ  8



Qˆ  3

Qˆ  8

Qˆ  3

n=5

Qˆ  12



























Qˆ  3

Qˆ  10





Qˆ  7










Qˆ  8





…

…

…

…

…





Qˆ  8

n=3

n=2

n=1
Qˆ ! 8







Qˆ ! 3

Qˆ  8

n=5

n=4

Qˆ  3

Qˆ  8

Qˆ  12

Qˆ  3

Qˆ ! 8

Qˆ  10

Qˆ  7

!"#$"

Qˆ ! 3

Qˆ ! 10

Qˆ ! 7

n=50
Qˆ  3

Qˆ  9

Qˆ  7

Qˆ  6

Qˆ  6

Qˆ  5

Qˆ  4

Qˆ  7

Qˆ  2

Qˆ  6

Qˆ  5

Qˆ  0

Qˆ  9

Qˆ  1


Qˆ  7

Qˆ  2

Qˆ  3

Qˆ  3



Figure 2: Illustration of the UCT dynamics

phase are fictitious. Furthermore, the work of Bubeck, Munos, and Stoltz (2011) on multiarmed bandits shows that minimizing cumulative regret and minimizing simple regret are
somewhat competing objectives. Indeed, the same Theorem 6 of Kocsis and Szepesvári
(2006) claims only a polynomial-rate reduction of the probability of choosing a non-optimal
action, and the results of Bubeck et al. (2011) on simple regret minimization in MABs with
stochastic rewards imply that UCT achieves only polynomial-rate reduction of the simple
regret over time. Some attempts have recently been made to adapt UCT, and MCTS-based
planning in general, to optimizing simple regret in online MDP planning directly, and some
of these attempts were empirically rather successful (Tolpin & Shimony, 2012; Hay, Shimony,
Tolpin, & Russell, 2012). However, to the best of our knowledge, none of them breaks UCT’s
barrier of the worst-case polynomial-rate reduction of simple regret over time.

3. Simple Regret Minimization in MDPs
We now show that exponential-rate reduction of simple regret in online MDP planning is
achievable. To do so, we first motivate and introduce a family of MCTS algorithms with a
two-phase scheme for generating state space samples, and then describe a concrete algorithm
from this family, BRUE, that (1) guarantees that the probability of recommending a non5

optimal action asymptotically convergences to zero at an exponential rate, and (2) achieves
exponential-rate reduction of simple regret over time.
3.1 Exploratory concerns in online MDP planning
The work of Bubeck et al. (2011) on pure exploration in multi-armed bandit (MAB) problems was probably the first to stress that the minimal simple regret can increase as the
bound on the cumulative regret is decreases. At a high level, Bubeck et al. (2011) show
that efficient schemes for simple regret minimization in MAB should be as exploratory as
possible, thus improving the expected quality of the recommendation issued at the end of
the learning process. In particular, they showed that the simple round-robin sampling of
MAB actions, followed by recommending the action with the highest empirical mean, yields
exponential-rate reduction of simple regret, while the UCB1 strategy that balances between
exploration and exploitation yields only polynomial-rate reduction of that measure. In that
respect, the situation with MDPs is seemingly no different, and thus Monte-Carlo MDP
planning should focus on exploration only. However, the answer to the question of what it
means to be “as exploratory as possible” with MDPs is less straightforward than it is in
the special case of MABs.
For an intuition as to why the “pure exploration dilemma” in MDPs is somewhat complicated, consider the state/steps-to-go pairs (s, h) as pseudo-agents, all acting on behalf of
the root pseudo-agent (s0 , H) that aims at minimizing its own simple regret in a stochastic
MAB induced by the applicable actions A(s0 ). Clearly, if an oracle would provide (s0 , H)
with an optimal action π ∗ (s0 , H), then no further deliberation would be needed until after
the execution of π ∗ (s0 , H). However, the task characteristics of (s0 , H) are an exception
rather than a rule. Suppose that an oracle provides us with optimal actions for all pseudoagents (s, h) but (s0 , H). Despite the richness of this information, (s0 , H) in some sense
remains as clueless as it was before: To choose between the actions in A(s0 ), (s0 , H) needs,
at the very least, some ordinal information about the expected value of these alternatives.
Hence, when sampling the futures, each non-root pseudo-agent (s, h) should be devoted to
two objectives:
(1) identifying an optimal action π ∗ (s, h), and
(2) estimating the actual value of that action, because this information is needed by the
predecessor(s) of (s, h) in T .
Note that both these objectives are exploratory, yet the problem is that they are somewhat competing. In that respect, the choices made by UCT actually make sense: Each
sample ρ issued by UCT at (s, h) is a priori devoted both to increasing the confidence in that
some current candidate a† for π ∗ (s, h) is indeed π ∗ (s, h), as well as to improving the estimate
of Qh (s, a† ), while as if assuming that π ∗ (s, h) = a† . However, while such an overloading of
the samples is unavoidable in the “learning while acting” setup of reinforcement learning,
this should not necessarily be the case in online planning. Moreover, this sample overloading in UCT comes with a high price: As it was shown by Coquelin and Munos (2007), the
number of samples after which the bounds of UCT on both simple and cumulative regret
become meaningful might be as high as hyper-exponential in H.
6

3.2 Separation of Concerns at the Extreme
Separating the two aforementioned exploratory concerns is at the focus of our investigation
here. Let s0 be a state of an MDP hS, A, T r, Ri with rewards in [0, 1], K applicable actions
at each state, B possible outcome states for each action, and finite horizon H. First, to
get a sense of what separation of exploratory concerns in online planning can buy us, we
begin with a MAB perspective on MDPs, with each arm in the MAB corresponding to a
“flat” policy of acting for H steps starting from the current state s0 . A “flat” policy π is a
minimal partial mapping from state/steps-to-go pairs to actions that fully specifies an acting
strategy in the MDP for H steps, starting at s0 . Sampling such an arm π is straightforward
as π prescribes precisely which action should be applied at every state that can possibly
be encountered along the execution of π. The reward of such an arm π isPstochastic, with
H−1 i
H
support [0, H], and the number of arms in this schematic MAB is K 0 = K i=0 B ≈ K B .
Now, consider a simple algorithm, NaiveUniform, which systematically samples each
”flat” policy in a loop, and updates the estimation of the corresponding arm with the
obtained reward. If stopped at iteration n, the algorithm recommends π(s0 ), where π is
the arm/policy with best empirical value µ̂π,n . By the iteration n of this algorithm, each
arm will be sampled at least b Bn H c times. Therefore, using the Hoeffding’s inequality, the
K
probability that the chosen arm π is sub-optimal in our MAB is bounded by

P {µ̂π,n > µ̂π∗ ,n } = P {µ̂π,n − µ̂π∗ ,n − (−∆π ) ≥ ∆π } ≤ exp −

b

n
K BH

c∆2π

2H 2

!
,

(3)

where ∆π = µπ∗ − µπ , and thus the expected simple regret can be bounded as
Ern ≤ HK

BH

exp −

b

n
K BH

cd2

2H 2

!
.

(4)

Note that NaiveUniform uses each sample ρ = (s0 , a0 , s1 , a1 , . . . , aH−1 , sH ) to update the
estimation of only a single policy π. However, recalling that arms in our MAB problem
are actually compound policies, the same sample can in principle be used to update the
estimates of all policies π 0 that are consistent with ρ in the sense that, for 0 ≤ i ≤ H − 1,
π 0 (si , H − i) is defined and it is defined as π 0 (si , H − i) = ai . The resulting algorithm,
CraftyUniform, generates samples by choosing the actions along them uniformly at random,
and uses the outcome of each sample to update all the policies consistent with it. Note
that sampling the arms in CraftyUniform cannot be done systematically as in NaiveUniform
because the set of policies updated at each iteration is stochastic.
Since the sampling is uniform, the probability of any policy to be updated by the sample
issued at any iteration of CraftyUniform is K1H . For an arm π 0 , let Nπ0 ,n denote the number
of samples issued at the n iterations of CraftyUniform that are consistent with the policy π 0 .
The probability that π, the best empirical arm after n iterations, is sub-optimal is bounded
by




∆π
∆π
P {µ̂π,n > µ̂π∗,n } ≤ P µ̂π,n − µπ ≥
+ P µ̂π∗,n − µπ∗ ≥
.
(5)
2
2
7

Each of the two terms on the right-hand side can be bounded as:




n
∆π
n o
n
∆π
P µ̂π,n − µπ ≥
≤ P Nπ,n ≤
+ P Nπ,n >
, µ̂π,n − µπ ≥
2
2K H
2K H
2


n
X
(†)
∆π
− n2H
≤ e 2K +
Nπ,n = i
P {Nπ,n = i} P µ̂π,n − µπ ≥
2
n
i=

≤e

≤e
(‡)

−

−

n
2K 2H

n
2K 2H

−

≤e

n
2K 2H

−

≤ 2e

2K H

+1



∆π
+ P µ̂π,n − µπ ≥
2

∆π
+ P µ̂π,n − µπ ≥
2
−

+e

n∆2
π
4K 2H H 2

Nπ,n

n
+1
=
2K H



n
+1
2K H



Nπ,n =

n
X
i= nH
2K

P {Nπ,n = i}
+1

n∆2
π
4K H H 2

,
(6)

where (†) and (‡) are by the Hoeffding inequality. In turn, similarly to Eq. 4, the simple
regret for CraftyUniform is bounded by
H

Ern ≤ 4HK B e

−

nd2
4K 2H H 2

.

(7)

Since H isa trivial upper-bound
on Ern , the bound in Eq. 7 becomes effective only when

H
nd2
B
4K
exp − 4K 2H H 2 < 1, that is, for
2

n> K B

H


·4

H
d

2
log K.

(8)

Note that this transition period length is still much better than that of UCT, which is
hyper-exponential in H. Moreover, unlike in UCT, the rate of the simple regret reduction
is then exponential in the number of iterations.
3.3 Two-phase sampling and BRUE
While both the simple regret convergence rate, as well as the length of the transition period
of CraftyUniform, are more attractive than those of UCT, this in itself is not much of a
H
help: CraftyUniform requires explicit reasoning about K B arms, and thus it cannot be
efficiently implemented. However, it does show the promise of separation of concerns in
online planning. We now introduce an MCTS family of algorithms, referred to as MCTS2e,
that allows utilizing this promise to a large extent.
The instances of the MCTS2e family vary along four parameters: switching point function σ : N → {1, . . . , H}, exploration policy, estimation policy, and update policy. With
respect to these four parameters, the MCTS components in MCTS2e are as follows.
• Similarly to UCT, each node/action pair (s, a) is associated with variables n(s, a)
b a). However, while counters n(s, a) are initialized to 0, value accumulators
and Q(s,
b a) are schematically initialized to −∞.
Q(s,
8

• sample: Each iteration of BRUE corresponds to a single state-space sample of the
MDP, and these samples ρ = hs0 , a1 , s1 , . . . , ak , sk i are all issued from the root node
s0 . The sample ends either when a sink state is reached, that is, A(sk ) = ∅, or when
k = H. The generation of ρ is done in two phases: At iteration n, the actions at states
s0 , . . . , sσ(n)−1 are selected according to the exploration policy of the algorithm, while
the actions at states sσ(n) , . . . , sk−1 are selected according to its estimation policy.
• expand-tree: T is expanded with the suffix of state sequence s1 , . . . , sσ(n)−1 that is
new to T .
• update-statistics: For each state si ∈ {s0 , . . . , sσ(n)−1 }, the update policy of the algorithm prescribes whether it should be updated. If si should be updated, then the
counter n(si , ai+1 ) is incremented and the estimated Q-value is updated according to
Eq. 2 (p. 4).
• recommend-action: The recommended action is chosen uniformly at random among
b 0 , a).
the actions a maximizing Q(s
In what follows, for n > 0, the n-th iteration of BRUE will be called H-iteration if σ(n) = H.
At a high level, the two phases of sample generation respectively target the two exploratory
objectives of online MDP planning: While the sample prefixes aim at exploring the options,
the sample suffixes aim at improving the value estimates for the current candidates for π ∗ .
In particular, this separation allows us to introduce a specific MCTS2e instance, BRUE,1
that is tailored to simple regret minimization. The BRUE setting of MCTS2e is described
below, and Figure 3 illustrates its dynamics.
• The switching point function σ : N → {1, . . . , H} is
σ(n) = H − ((n − 1) mod H),

(9)

that is, the depth of exploration is chosen by a round-robin on {1, . . . , H}, in reverse
order.
• At state s, the exploration policy samples an action uniformly at random, while the
estimation policy samples an action uniformly at random, but only among the actions
b a).
a ∈ A(s) that maximize Q(s,
• For a sample ρ issued at iteration n, only the state/action pair (sσ(n)−1 , aσ(n) ) immediately preceding the switching state sσ(n) along ρ is updated. That is, the information
obtained by the second phase of ρ is used only for improving the estimate at state
sσ(n)−1 , and is not pushed further up the sample. While that may appear wasteful
and even counterintuitive, this locality of update is required to satisfy the formal
guarantees of BRUE discussed below.
Before we proceed with the formal analysis of BRUE, a few comments on it, as well as
on the MCTS2e sampling scheme in general, are in place. First, the template of MCTS2e is
1. Short for Best Recommendation with Uniform Exploration; the name is carried on from our first
presentation of the algorithm in (Feldman & Domshlak, 2012), where “estimation” was referred to as
“recommendation.”

9

n=2

n=1

Uniform

n=3
a1,r1

a1,r1

a2,r2

a3,r3

a3,r3

a4,r4

a4,r4

a5,r5

a5,r5

…

Switching
point

Switching
point

Empirical

aH,rH

Best

a3,r3

Uniform

a4,r4

…

a6,r6

a2,r2

Uniform

…

a2,r2

a1,r1

Switching
point

Empirical

a5,r5

Best
a6,r6

Qˆ = 1

a6,r6

Empirical
Best

aH,rH

Qˆ = 1.5
Qˆ = 1

aH,rH

n=2

n=1

n=3

Switching
point

Switching
point

Switching
point

Qˆ = 1

Qˆ = 1
Qˆ = 1.5

Qˆ = 1.5

Qˆ = 1

Qˆ = 1

···
n=50

n=20

n=10

Qˆ = 5

Qˆ = 7
Qˆ = 5

Qˆ = 5

Qˆ = 6

Qˆ = 4

Qˆ = 5
Qˆ = 2

Qˆ = 3
Qˆ = 0

Qˆ = 5

Qˆ = 5

Qˆ = 2

Qˆ = 4

Qˆ = 5

Qˆ = 0

Qˆ = 7

Qˆ = 5
Qˆ = 5

Qˆ = 2

Qˆ = 5

Qˆ = 7

Qˆ = 6

Qˆ = 6

Qˆ = 5

Qˆ = 2

Qˆ = 0

Qˆ = 1

Qˆ = 7
Qˆ = 5
Qˆ = 2

Qˆ = 7
Qˆ = 5

Qˆ = 5

Qˆ = 6
Qˆ = 5

Qˆ = 8

Qˆ = 5

Qˆ = 3
Qˆ = 1

Qˆ = 7

Qˆ = 2

Qˆ = 2

Qˆ = 4

Qˆ = 3

Qˆ = 6

Qˆ = 2

Qˆ = 6
Qˆ = 2 Qˆ = 3

Qˆ = 7 Qˆ = 3

Qˆ = 2
Qˆ = 7

Qˆ = 7

Qˆ = 1
Qˆ = 1.5

Qˆ = 7

Qˆ = 1

Qˆ = 5

Qˆ = 2

Qˆ = 1.5

Qˆ = 0

Qˆ = 7

Qˆ = 1.5
Qˆ = 5

Qˆ = 5

Qˆ = 1

Qˆ = 3

Qˆ = 1

Qˆ = 1

Qˆ = 2

Qˆ = 0

Qˆ = 1

Figure 3: Illustration of the BRUE dynamics

rather general, and some of its parametrizations will not even guarantee convergence to the
optimal action. This, for instance, will be the case with a (seemingly minor) modification
of BRUE to purely uniform estimation policy. In short, MCTS2e should be parametrized
with care. Second, while in what follows we focus on BRUE, other instances of MCTS2e
may appear to be empirically effective as well with respect to the reduction of simple regret
over time. Some of them, similarly to BRUE, may also guarantee exponential-rate reduction
of simple regret over time. Hence, we clearly cannot, and do not, claim any uniqueness of
BRUE in that respect. Finally, some other families of MCTS algorithms, more sophisticated
that MCTS2e, can give rise to even more (formally and/or empirically) efficient optimizers of
simple regret. The BRUE(α) set of algorithms that we discuss later on is one such example.
10

4. Upper Bounds on Simple Regret Reduction Rate with BRUE
For the sake of simplicity, in our formal analysis of BRUE we assume uniqueness of the
optimal policy π ∗ ; that is, at each state s and each number h of steps-to-go, there is a
single optimal action, and it is π ∗ (s, h). Let Tn be the graph obtained by BRUE after n
b h (s, a) denote the accumulated value Q(s,
b a) for s at depth H − h. For
iterations, and let Q
B
all state/steps-to-go pairs (s, h) ∈ Tn , πn (s, h) is a randomized strategy, uniformly choosing
b h (s, a). We also use some additional auxiliary notation.
among actions a maximizing Q
K = maxs∈S |A(s)|, i.e., the maximal number of actions per state.
p = mins,a,s0 :T r(s,a,s0 )>0 T r(s, a, s0 ), i.e., the likelihood of the least likely (but still possible) outcome of an action in our problem.
d = mins,a ∆1 [s, a], i.e., the smallest difference between the value of the optimal and a
second-best action at a state with just one step-to-go.
Our key result on the BRUE algorithm is Theorem 1 below. The proof of Theorem 1, as
well as of several required auxiliary claims, is given in Appendix A. Here we outline only
the key issues addressed by the proof, and provide a high-level flow of the proof in terms of
a few central auxiliary claims.
Theorem 1 Let BRUE be called on a state s0 of an MDP hS, A, T r, Ri with rewards in
[0, 1] and finite horizon H. There exist pairs of parameters c, c0 > 0, dependent only on
{p, d, K, H}, such that, after n > H iterations of BRUE, we have simple regret bounded as
0

E∆H [s, πnB (s0 , H)] ≤ Hc · e−c n ,
and choice-error probability bounded as

0
P πnB (s0 , H) 6= π ∗ (s0 , H) ≤ c · e−c n .

(10)

(11)

In particular, these bounds hold for
c=

4K 3H

2 −2H

Q
4 H−1 16(H−1)2
(H!)3 H−1
h=1 (h!) 24
,
d2H 2 −4H+2 p3H 2 −3H

and
c0 =

3d2H−2 p2H−1
.
2H16H−1 (H!)2 K 2H

(12)

(13)

Before we proceed any further, some discussion of the statements in Theorem 1 are in
place. First, the parameters c and c0 in the bounds established by Theorem 1 are problemdependent: in addition to the dependance on the horizon H and the choice branching factor
K (which is unavoidable), the parameters c and c0 also depend on the distribution parameters p and d. While it is possible that this dependence can be partly alleviated, Bubeck
et al. (2011) showed that distribution-free exponential bounds on the simple regret reduction rate cannot be achieved even in MABs, that is, even in single-step-to-go MDPs (see
Remark 2 of Bubeck et al. (2011), which is based on a lower bound on the cumulative
11

regret established by Auer, Cesa-Bianchi, Freund, & Schapire, 2002b). Second, the specific
parameters c and c0 provided by Eqs. 12 and 13 are worst-case for MDPs with parameters
d, p, and K, and the bound in Eq. 10 becomes effective after
"
 2#
ln(c)
KH εH
n> 0 =O
c
pd
iterations, for some small constant ε > 1. While there is still some gap with this transition
period length and the transition period length of the theoretical CraftyUniform algorithm
(see Eq. 8), this gap is not that large.2
The proof of Lemma 2 below constitutes the crux of the proof of Theorem 1. Once
we have proven this lemma, the proof of Theorem 1 stems from it in a more-or-less direct
manner.
Lemma 2 Let BRUE be called on a state s0 of an MDP hS, A, T r, Ri with rewards in [0, 1]
and finite horizon H. For each h ∈ JHK, there exist parameters ch , c0h > 0, dependent only
on {p, d, K, H}, such that, for each state s reachable from s0 in H − h steps and any t > 0,
it holds that


b h (s, a) − Qh (s, a) ≥ d nh (s, a) = t ≤ ch e−c0h t ,
P Q
2


(14)
d
−c0h t
b
P Qh (s, a) − Qh (s, a) ≤ −
nh (s, a) = t ≤ ch e
.
2
In particular, these bounds hold for
Q
4 h−1 16(h−1)2
(h!)3 h−1
i=1 (i!) 24
,
d2(h−1)2 · p2Hh+h2 −2H−h

2 −2H−1

ch =

K 2Hh+h

and
c0h =

3d2(h−1) pH+h−1
.
16h−1 (h!)2 K H+h−1

(15)

(16)

The proof for Lemma 2 is by induction on h. Starting with the induction basis for h = 1,
it is easy to verify that, by the Chernoff-Hoeffding inequality,


d2
d
b
P Q1 (s, a) − Q1 (s, a) ≥
n (s, a) = t ≤ 2e− 2 t ,
(17)
2
2

that is, the assertion is satisfied with c1 = 1 and c01 = d2 . Now, assuming the claim holds
for h ≥ 1, below we outline the proof for h + 1, relegating the actual proof in full detail to
Appendix A.
In the proof for h > 1, it is crucial to note the invalidity of applying the ChernoffHoeffding bound directly, as was done in Eq. 17. There are two reasons for this.
2. Some of this gap can probably be eliminated by more accurate bounding in the numerous bounding steps
towards the proof of Theorem 1. However, all such improvements we tried made the already lengthy
proof of Theorem 1 even more involved.

12

b is an unbiased estimator of Q, that is, EQ
b = Q. In contrast, the
(F1) For h = 1, Q
b
estimates inside the tree (at nodes with h > 1) are biased. This bias stems from Q
possibly being based on numerous sub-optimal choices in the sub-tree rooted in (s, h).
b are independent. This is not so for h > 1,
(F2) For h = 1, the summands accumulated by Q
where the accumulated reward depends on the selection of actions in subsequent nodes,
which in turn depends on previous rewards.
However, we show that these deficiencies of h > 1 can still be overcome through a novel
modification of the seminal Hoeffding-Azuma inequality.
Lemma 3 (Modified Hoeffding-Azuma inequality) Let {Xi }∞
i=1 be a sequence of random variables with support [0, h] and µi , EXi . If limi→∞ µi = µ, and
P {E [Xi | X1 , . . . , Xi−1 ] 6= µ} ≤ cp e−ce i ,

(18)

for some 0 < cp and 0 < ce ≤ 1, then, for all 0 < δ ≤ h2 , it holds that
( t
X

)


3δ 2 ce
2h2
≤ 1 + cp 2 2 · e− 2h2 t ,
P
Xi ≥ µt + tδ
δ ce
i=1
( t
)


X
3δ 2 ce
2h2
P
≤ 1 + cp 2 2 · e− 2h2 t .
Xi ≤ µt − tδ
δ ce


(19)

(20)

i=1

Together with Lemma 4 below, the inequalities provided by Lemma 3 allow us to prove
the induction hypothesis in the proof of the central Lemma 2. Note that the specific bound
in Lemma 3 is selected so to maximize the exponent coefficient. For any 0 ≤ β ≤ 1, the
probabilities of interest in Eqs. 19-20 can also be bounded by


c (1−β)
3δ 2 ce β
cp
− e 2
2h
e− 2h2 t ;
1+
e
ce (1 − β)
for further details, we refer the reader to Discussion 14 in Appendix A.
Definition 1 Let hS, A, T r, Ri be an MDP with rewards in [0, 1], planned for initial state
s0 ∈ S and finite horizon H. Let s be a state reachable from s0 with h steps still to go, let
a be an action applicable in s, and let πtB be a policy induced by running BRUE on s0 until
exactly t > 0 samples have finished their exploration phase with applying action a at s with
h − 1 steps still to go. Given that,
• Xt,h (s, a) is a random variable, corresponding to the reward obtained by taking a at
s, and then following πtB for the remaining h − 1 steps.
• Et,h (s, a) is the event in which Xt,h (s, a) is sampled along the optimal actions at each
of the h − 1 choice points delegated to πtB .
• δt,h (s, a) = Qh (s, a) − E [Xt,h (s, a)] .
13

Lemma 4 Let hS, A, T r, Ri be an MDP with rewards in [0, 1], planned for initial state
s0 ∈ S and finite horizon H. Let s be a state reachable from s0 with h + 1 steps still
to go, and a be an action applicable in s. Considering Et,h+1 (s, a) and δt,h+1 (s, a) as in
Definition 1, for any t > 0, if Lemma 2 holds for horizon h, then
pc0h

P {¬Et,h+1 (s, a)} ≤ 2Kh (2 + ch ) e− 6K t ,
pc0
− 6Kh t

δt,h+1 (s, a) ≤ 2Kh2 (2 + ch ) e

(21)
.

(22)

Together with a modified version of the Hoeffding-Azuma bound in Lemma 3, the bounds
b h+1 around Qh+1 as
established in Lemma 4 allow us to derive concentration bounds for Q
in Lemma 5 below, which serves the key building block for proving the induction hypothesis
in the proof of Lemma 2.
Lemma 5 Let BRUE be called on a state s0 of an MDP hS, A, T r, Ri with rewards in [0, 1]
and finite horizon H. For each state s reachable s0 with h + 1 steps still to go, each action
a applicable, and any t > 0, it holds that

P

 

d2 pc0h
3 (h + 1)3 c
K
d
−
h
b h+1 (s, a) − Qh+1 (s, a) ≥
16(h+1)2 K .
nh+1 (s, a) = t ≤ 3456 ·
Q
e
2
d2 p2 c02
h
(23)

5. Learning With Forgetting and BRUE(α)
When we consider the evolution of action value estimates in BRUE over time (as well
as in all other Monte-Carlo algorithms for online MDP planning), we can see that, in
internal nodes these estimates are based on biased samples that stem from the selection
of non-optimal actions at descendant nodes. This bias tends to shrink as more samples
are accumulated down the tree. Consequently, the estimates become more accurate, the
probability of selecting an optimal action increases accordingly, and the bias of ancestor
nodes shrinks in turn. An interesting question in this context is: shouldn’t we weigh
differently samples obtained at different stages of the sampling process? Intuition tells
us that biased samples still provide us with valuable information, especially when they
are all we have, but the value of this information decreases as we obtain more and more
accurate samples. Hence, in principle, putting more weight on samples with smaller bias
could increase the accuracy of our estimates. The key question, of course, is which of all
possible weighting schemes are both reasonable to employ and preserve the exponential-rate
reduction of expected simple regret.
Here we describe BRUE (α), an algorithm that generalizes BRUE ≡ BRUE(1) by basing
the estimates only on the α fraction of most recent samples. We discuss the value of this
addition both from the perspective of the formal guarantees, as well as from the perspective
of empirical prospects. BRUE(α) differs from BRUE in two points:
b a), each node/action pair (s, a) in BRUE(α)
• In addition to the variables n(s, a) and Q(s,
is associated with a list L(s, a) of rewards, collected at each of the n(s, a) samples that
b a).
are responsible for the current estimate Q(s,
14

• When a sample ρ = hs0 , a1 , s1 , . . . , ak , sk i is issued at iteration n, and update-statistics
updates the variables at x = (sσ(n)−1 , aσ(n) ), that update is done not according to
Eq. 2 as in BRUE, but according to:
n(x) ← n(x) + 1,
L(x)[n(x)] ←

k−1
X

R(si , ai+1 , si+1 ),
(24)

i=σ(n)−1

b
Q(x)
←

1
dα · n(x)e

n(x)

X

L(x)[i].

i=n(x)−dα·n(x)e

Theorem 6 Let BRUE (α) be called on a state s0 of an MDP hS, A, T r, Ri with rewards
in [0, 1] and finite horizon H. There exist pairs of parameters c, c0 > 0, dependent only on
{α, p, d, K, H}, such that, after n > H iterations of BRUE, we have simple regret bounded
as
0
E∆H [s, πnB (s0 , H)] ≤ Hc · e−c n ,
(25)
and choice-error probability bounded as

0
P πnB (s0 , H) 6= π ∗ (s0 , H) ≤ c · e−c n .

(26)

The proof for Theorem 6 follows from Lemma 7 below similarly to the way Theorem 1
follows from Lemma 2. Note that in Theorem 6 we do not provide explicit expressions for
the constants c and c0 as we did in Theorem 1 (for α = 1). This is because the expressions
that can be extracted from the recursive formulas in this case do not bring much insight.
However, we discuss the potential benefits of choosing α < 1 in the context of our proof of
Theorem 6.
Lemma 7 Let BRUE(α) be called on a state s0 of an MDP hS, A, T r, Ri with rewards in
[0, 1] and finite horizon H. For each h ∈ JHK, there exist parameters ch , c0h > 0, dependent
only on {α, p, d, K, H}, such that, for each state s reachable from s0 in H − h steps and any
t > 0, it holds that


d
0
b
P Qh (s, a) − Qh (s, a) ≥
nh (s, a) = t ≤ ch e−ch t ,
2


(27)
d
−c0h t
b
P Qh (s, a) − Qh (s, a) ≤ −
nh (s, a) = t ≤ ch e
.
2
The proof for Lemma 7 is by induction, following the same line of the proof for Lemma 2.
In fact, it deviates from the latter only in the application of the modified Hoeffding-Azuma
inequality, which has to be further modified to capture the partial sums as in BRUE(α).
Lemma 8 (Modified Hoeffding-Azuma inequality for partial sums) Let {Xi }∞
i=1 be
a sequence of random variables with support [0, h] and µi , EXi . If limi→∞ µi = µ, and
P {E [Xi | X1 , . . . , Xi−1 ] 6= µ} ≤ cp e−ce i ,
15

(28)

for some 0 < cp and 0 < ce ≤ 1, then, for all 0 < δ ≤ h2 , it holds that

P

P


t
 X

i=t−dαte

t
 X

Xi ≥ µt + tδ

Xi ≤ µt − tδ



i=t−dαte









≤

≤


3δ 2 ce
cp
−ce (1−α)2 t
1+
e− 2h2 αt ,
e
ce (1 − α)

(29)


3δ 2 ce
cp
−ce (1−α)2 t
1+
e− 2h2 αt .
e
ce (1 − α)

(30)

Considering the benefits of “sample forgetting” as in BRUE(α), let us compare the bound
in Lemma 8 to the bound


2
c (1−β)
cp
− 3δ c2e β t
− e 2
e 2h
1+
e 2h
,
ce (1 − β)
provided by Lemma 3 for BRUE, that is, when all accumulated samples are averaged. While
both bounds are very similar, the exponent of the second exponential term is multiplied for
BRUE(α < 1) by (1 − α) t. This poses a tradeoff: Decreasing α reduces the sampling bias,
c
and thus decreases the term cpe , but increases the other exponential term with no leading
constant. Obviously, since there is no bias at leaf nodes, it makes no sense to set α < 1
c
there. However, as we go further up the tree, the bias tends to grow ( cpe >> 1), but we also
expect to have more samples (t is larger). Thus, from the perspective of formal guarantees,
it seems appealing to choose smaller values of α. Nevertheless, we do not try to optimize
here the value of α: First, optimizing bounds doesn’t necessarily lead to optimized empirical
accuracy. Second, the underlying optimization would have to be specific to each horizon h
and each sample size t (which is obviously out of the question), and thus anyway we would
have to consider only some rough approximations to this optimization problem. Finally,
biased samples in practice might be more valuable than what the theory suggests, as long
as all actions at the same state/steps-to-go decision point experience a similar bias.

6. Experimental Evaluation
We have evaluated BRUE empirically on the MDP sailing domain (Péret & Garcia, 2004)
that was used in previous works for evaluating MC planning algorithms (Péret & Garcia,
2004; Kocsis & Szepesvári, 2006; Tolpin & Shimony, 2012), as well as on random game trees
used in the original empirical evaluation of UCT (Kocsis & Szepesvári, 2006).
In the sailing domain, a sailboat navigates to a destination on an 8-connected grid
representing a marine environment, under fluctuating wind conditions. The goal is to reach
the destination as quickly as possible, by choosing at each grid location a neighbor location
to move to. The duration of each
√ such move depends on the direction of the move (ceteris
paribus, diagonal moves take 2 more time than straight moves), the direction of the wind
relative to the sailing direction (the sailboat cannot sail against the wind and moves fastest
with a tail wind), and the tack. The direction of the wind changes over time, but its strength
is assumed to be fixed. This sailing problem can be formulated as a goal-driven MDP over
finite state space and a finite set of actions, with each state capturing the position of the
sailboat, wind direction, and tack.
16

0.8

0.35

0.3

brue
brueper(0.9)

0.7

uct
gct

0.6

brue
brueper(0.9)
uct
gct

Average Error

Average Error

0.25

0.2

0.15

0.5
0.4
0.3

0.1

0.2
0.05

0

0.1

1

2

3

4

5
6
7
Running Time (sec)

8

9

0

10

0

5

10

15

5×5

35

40

45

50

10 × 10
3

1.6

brue
brueper(0.9)

brue
brueper(0.9)

1.4

uct
gct

uct
gct

2.5

Average Error

1.2

Average Error

20
25
30
Running Time (sec)

1
0.8

2

1.5

0.6
0.4

1

0.2
0

0.5
0

10

20

30

40
50
60
Running Time (sec)

70

80

90

100

20 × 20

0

50

100

150

200
250
300
Running Time (sec)

350

400

450

500

40 × 40

Figure 4: Empirical performance of BRUE, BRUE(0.9), UCT, and -greedy + UCT (denoted
as GCT, for short) in terms of the average error on sailing domain problems on
n × n grids with n ∈ {5, 10, 20, 40}.

In a goal-driven MDP, the lengths of the paths to a terminal state are not necessarily
bounded, and thus it is not entirely clear to what depth BRUE shall construct its tree.
In the sailing domain, we chose H to be 4 × n, where n is the grid-size of the problem
instance, as it is unlikely that the optimal path between any two locations on the grid will
be larger than a complete encircling of the considered area. We note, however, that the
recommendation-oriented samples ρ̄ always end at a terminal state, similar to the rollouts
issued by UCT and -greedy + UCT.
Snapshots of the results for different grid sizes are shown in Figure 4. We compared
BRUE with two MCTS-based algorithms: the UCT algorithm, and a recent modification of
UCT, -greedy + UCT, obtained from the former by replacing the UCB1 policy at the root
node with the -greedy policy (Tolpin & Shimony, 2012). The motivation behind the design
of -greedy + UCT was to improve the empirical simple regret of UCT, and the results for
-greedy + UCT reported by (Tolpin & Shimony, 2012) (and confirmed by our experiments
17

3.5

0.4

brue
brueper(0.9)

0.35

uct
gct

uct
gct

0.3

2.5

0.25

Average Error

Average Error

brue
brueper(0.9)

3

0.2
0.15

2

1.5

1

0.1

0.5

0.05

10

20

30
40
Running Time (sec)

50

0
0

60

B = 6/D = 6

10

20

30
40
Running Time (sec)

50

60

B = 2/D = 16

Figure 5: Empirical performance of BRUE, UCT, and -greedy + UCT (denoted as GCT) in
terms of the average error on the random game trees with branching factor B
and tree depth D.

here) are very impressive. We also show the results for BRUEper (0.9), a slight modification
of BRUE(0.9) with a more permissive update scheme: Instead of updating only the stateaction node at the level of the switching point, we also update any ancestor for which either
not all applicable actions have been sampled or the chosen action was identical to the best
empirical one.
All four algorithms were implemented within a single software infrastructure. As suggested by more recent works on UCT, the exploration coefficient for UCT and -greedy + UCT
(parameter c in Eq. 1) was set to the empirical best value of an action at the decision
point (Keller & Eyerich, 2012b). (This setting of the exploration coefficient resulted in better performance of both UCT and -greedy + UCT than with the settings reported on the
sailing domain in the respective original publications.) The  parameter in -greedy + UCT
was set to 0.5 as in the experiments of Tolpin & Shimony, 2012. Each algorithm was run
on 1000 randomly chosen initial states s0 , and the performance of the algorithm was assessed in terms of the average error Q(s0 , a) − V (s0 ), that is, the difference between the
true values of the action a chosen by the algorithm and that of the optimal action π ∗ (s0 ).
Consistently with the results reported by Tolpin and Shimony (2012), on the smaller tasks
-greedy + UCT outperformed UCT by a very large margin, with the latter exhibiting very
little improvement over time even on the smallest, 5 × 5, grids. The difference between
-greedy + UCT and UCT on the larger tasks was less notable. In turn, BRUE substantially
outperformed -greedy + UCT, with the improvement being consistent except for relatively
short planning deadlines, and BRUEper (0.9) performed even better than BRUE.
The above allows us to conclude that BRUE is not only attractive in terms of the
formal performance guarantees, but can also be very effective in practice for online planning.
Likewise, the “learning with forgetting” extension of BRUE(α) also has its practical merits.
Under the same parameter setting of UCT and -greedy + UCT, we have also evaluated the
18

three algorithms in a domain of random game trees whose goal is a simple modeling of
two-person zero-sum games such as Go, Amazons and Globber. In such games, the winner
is decided by a global evaluation of the end board, with the evaluation employing this or
another feature counting procedure; the rewards thus are associated only with the terminal
states. The rewards are calculated by first assigning values to moves, and then summing up
these values along the paths to the terminal states. Note that the move values are used for
the tree construction only and are not made available to the players. The values are chosen
uniformly from [0, 127] for the moves of MAX, and from [−127, 0] for the moves of MIN.
The players act so to (depending on the role) maximize/minimize their individual payoff:
the aim of MAX is to reach terminal s with as high R(s) as possible, and the objective of
MIN is similar, mutatis mutandis. This simple game tree model is similar in spirit to many
other game tree models used in previous work (Kocsis & Szepesvári, 2006; Smith & Nau,
1994), except that the success/failure of the players in measured not on a ternary scale of
win/lose/draw, but via the actual payoffs they receive. We ran some experiments with two
different settings of the branching factor (B) and tree depths (D). As in the sailing domain,
we compared the convergence rate obtained by BRUE, UCT and -greedy + UCT. Figure 5
plots the average error rate for two configurations, B = 6, D = 6 and B = 2, D = 16, with
the average in each setting obtained over 500 trees. The results here appear encouraging as
well, with BRUE overtaking the other two algorithms more quickly on the deeper trees.

7. SUMMARY
We have introduced BRUE, a simple Monte-Carlo algorithm for online planning in MDPs
that guarantees exponential-rate reduction of the performance measures of interest, namely
the simple regret and the probability of erroneous action choice. This improves over previous
algorithms such as UCT, which guarantee only polynomial-rate reduction of these measures.
The algorithm has been formalized for finite horizon MDPs, and it was analyzed as such.
However, our empirical evaluation shows that it also performs well on goal-driven MDPs
and two-person games.
A few questions remain for future work. In the setting of γ-discounted MDPs with
infinite horizons, a straightforward way to employ BRUE is to fix a horizon H, use the
algorithm as is, and derive guarantees on the aforementioned measures of interest by simply accounting for the additive gap of γ H Rmax /(1 − γ) between the state/action values
under horizon H and those under an infinite horizon. However, this is not necessarily the
best way to plan online for infinite-horizon MDPs, and thus this setting requires further
inspection. Second, it is not unlikely that the state-space independent factors ch , and c0h
in the guarantees of BRUE can be improved by employing more sophisticated combinations
of exploration and estimation samples. Another important point to consider is the speed
of convergence to the optimal action, as opposed to the speed of convergence to “good”
actions. BRUE is geared towards identifying the optimal action, although in many large
MDPs, “good” is often the best one can hope for. To identify the optimal solution, BRUE
devotes samples equally to all depths. However, focusing on nodes closer to the root node
may improve the quality of the recommendation if the planning time is severely limited.
Finally, the core tree sampling scheme employed by BRUE differs from the more standard
scheme employed in previous work. While this difference plays a critical role in establishing
19

the formal guarantees of BRUE, it is still unclear whether that difference is necessary for
establishing exponential-over-time reduction of the performance measures.
Acknowledgements
This work is partially supported by and carried out at the Technion-Microsoft Electronic
Commerce Research Center, as well as partially supported by the Air Force Office of Scientific Research, USAF, under grant number FA8655-12-1-2096.



Tackling the problem of ordinal preference
revelation and reasoning, we propose a novel
methodology for generating an ordinal utility function from a set of qualitative preference statements. To the best of our knowledge, our proposal constitutes the first nonparametric solution for this problem that is
both efficient and semantically sound. Our
initial experiments provide strong evidence
for practical effectiveness of our approach.

1

INTRODUCTION

Human preferences are a key concept in decision theory, and as such have been studied extensively in philosophy, psychology, and economics (e.g., [10, 12, 14]).
The central goals have been to provide logical, cognitive, and mathematical models of human decision
making. More recently this research effort was joined
by AI researchers, motivated by the goal of automating
the process of decision support. To illustrate the need
for automated decision support, consider the nowadays common task of searching for some goods in a
database of an online vendor such as Amazon.com or
eBay. Such databases are too large for a user to search
exhaustively. Using the purchase of a used car as an
example, a decision support system might allow a user
to state preferences like “I like ecologically friendly
cars”, “I prefer Mercedes to Lada”, or “For a sport
car, I prefer red color to black color”. The system
should then use these preference statements to guide
the user to the relevant parts of the database.
Various logics of preference, graphical preference representation models, preference learning and reasoning
algorithms have been proposed in AI in the last three
decades [4, 8, 9]. While these works have made significant contributions, there is still a substantial gap between theory and practice of decision support. In par-

Thorsten Joachims
Computer Science Dept.
Cornell University
Ithaca, NY 14853

ticular, so far there is no single framework for revealing
user preferences and reasoning about them that is both
generically scalable and generically robust, i.e. both efficient and effective for any set of decision alternatives
and any form of preference information. It is clear
nowadays that getting closer to such a universal framework requires new insights into the problem [8, 21].
In this paper, we tackle this challenge in the scope
of revelation and reasoning about ordinal preferences
(i.e., as in the database search example above), and
develop a robust solution for this problem that is both
efficient and effective. Specifically, we propose a novel
methodology for generating an ordinal utility function from a set of qualitative preference statements.
Our proposal is based on a somewhat surprising mixture of techniques from knowledge representation and
machine learning. We show formally that it leads to
a flexible and unprecedentedly powerful tool for reasoning about ordinal preference statements. Furthermore, we present experiments that provide initial evidence for practical applicability and effectiveness of
our method, making it promising for a wide spectrum
of decision-support applications.
1.1

PROBLEM STATEMENT AND
BACKGROUND

Using the used-car database search as our running example, the content of the database constitutes the relevant subset of all possible choice alternatives Ω. The
ordinal preferences of a user who wants to buy a car
can be viewed as a (possibly weak, possibly partial)
binary preference relation P over Ω [12]. A decision
support system should allow its user to state her preferences, use these statements to approximate P , and
present the database content in a way that enables the
user to quickly home in on desirable alternatives.
The choice alternatives in such scenarios are typically described in terms of some attribution X =
{X1 , . . . , Xn } abstracting Ω to X = ×Dom(Xi ) (e.g.,
attributes of the database schema), and the user can

express her preferences in terms of X. Now, what preference information can we expect the users to provide?
As suggested in the used-car example (and supported
by multi-disciplinary literature [8, 12]), typically users
should be expected to provide only qualitative preference statements that either compare between pairs of
complete alternatives (e.g., “I prefer this alternative
to that alternative”), or generalize user’s preference
over some properties of Ω (e.g., “In a minivan, I prefer automatic transmission to manual transmission.”)
Formally, this means that the user provides us with a
qualitative preference expression

lead to restricting user expressions S to simplified languages that incorporate this prior assumption.
In
summary, computationally efficient schemes for multiattribute utility revelation proposed in economics and
AI are parametrized by the structure that user preferences induce on X, and thus are applicable only when
such structure exists and is known to the system.

S = {s1 , . . . , sm } = {hϕ1 =1 ψ1 i, · · · , hϕm =m ψm i}, (1)

Having in mind these limitations, let us return back
to the needs of decision-support application, and list
the challenges these applications pose to the research
on OUR. The vision here is threefold. First, the
user should be able to provide preference expressions
S while being as little constrained in her language
as possible. Second, the utility revelation machinery should be completely non-parametric, i.e., free of
any explicit assumptions about the structure of user
preferences. Third, both utility revelation (i.e., generating U from S) and using the revealed utility function should be computationally efficient, including the
case where user preferences pose no significant independence structure on X whatsoever.

consisting of a set of such preference statements
{s1 , . . . , sm }, where ϕi , ψi are logical formulas, =i ∈
{, , ∼}, and , , and ∼ have the standard semantics of strong preference, weak preference, and preferential equivalence, respectively. For ease of presentation, we assume attributes X are boolean1 (denoting Dom(Xi ) = {xi , xi }), and ϕi , ψi are propositional
logic formulas over X.
Given such a preference expression S, one has to interpret what information S conveys about P , decide
on a representation for this information, and decide on
the actual reasoning machinery. Several proposals for
direct logical reasoning about S have been made, yet
all these proposals are limited by (this or another) efficiency/expressiveness tradeoff. In attempt to escape
this tradeoff as much as possible, several works in AI
(e.g., see [5, 18]) proposed to compile information carried by S into an ordinal utility function
U : X 7→ R

(2)

consistent with (what we believe S tells us about) P ,
that is requiring
∀x, x0 ∈ X . U (x) ≥ U (x0 ) ⇒ P 6|= x0  x .

(3)

In what follows, we refer to the task of constructing
such a utility function U from S as ordinal utility revelation (OUR). Observe that specifying a utility function U as in (2) can be expensive due to the fact that
|X | = O(2n ). Hence, previous works on OUR searched
for special conditions under which U can be represented compactly (e.g., see [1, 3, 5, 11, 17, 18]). The
general scheme followed by these works (which we refer
to as independence-based methodology) is as follows.
First, one defines certain “independence conditions”
on X, and provides a “representation theorem” stating that under these conditions U can be compactly
specified. Second, one possibly defines some additional
conditions under which U can also be efficiently generated from S. Finally, assuming all these conditions
1

Extending our framework to arbitrary finite-domain
variables is straightforward, yet requires a more involved
notation that we decided to avoid here.

1.2

CHALLENGES AND OUR RESULTS

In this paper, we present the first approach that fulfills these goals. Combining ideas from knowledge representation, machine learning, and philosophical logic
we provide a concrete mathematical setting in which
all the above desiderata can be successfully achieved,
and formally show that this setting is appealing both
semantically and computationally. The mathematical framework we propose is based on a novel highdimensional structure for preference decomposition,
and a specific adaptation of certain standard techniques for high-dimensional continuous optimization,
frequently used in machine learning in the context of
Support Vector Machines (SVMs) [22].

2

HIGH-DIMENSIONAL
PREFERENCE
DECOMPOSITION

Considering our vision for preference revelation, one
can certainly be somewhat skeptical. Indeed, how can
OUR be efficient if the user preferences pose no significant independence structure on X, or, if they do,
the system is not provided with this independence information? The basic idea underlying our proposal is
simple: Since we are not provided with a sufficiently
useful independence information in the original space
X , maybe we should move to a different space in which
no independence information is required?
Specifically, let us schematically map the alternatives

X into a new, higher dimensional space F using
n

Φ : X 7→ F = R4 .

(4)

As one would expect, the mapping Φ is not arbitrary.
Let FS= {f1 , · · · , f4n } be the dimensions of F, and
D = Dom(Xi ) be the union of attribute domains
in X. Let val : F → 2D be a bijective mapping from
the dimensions of F onto the power set of D, uniquely
associating each dimension fi with a subset val(fi ) ⊆
{x1 , x1 , · · · , xn , xn }. In what follows, by Var(fi ) ⊆ X
we denote the subset of attributes “instantiated” by
val(fi ). Given that, for each x ∈ X and fi ∈ F, we set:
(
1, val(fi ) ⊆ x
Φ(x)[i] =
(5)
0, otherwise
From (5) it is easy to see that dimensions fi with val(fi )
containing both a literal and its negation are effectively
redundant. Indeed, later we show that we actually use
only the (3n − 1)-dimensional subspace of F, dimensions of which correspond to all the non-empty partial
assignments on X. Hence, for ease of presentation,
in what follows we discuss F as if ignoring its redundant dimensions. However, for some technical reasons
important for our computational machinery, the structure of F and Φ has to be defined as in (4)-(5).
To illustrate our mapping Φ, if X = {X1 , X2 } and
x = x1 x2 , we have Φ(x)[i] = 1 if and only if val(fi ) ∈
{x1 , x2 , x1 x2 }, that is
0 1
B 0
B 0
B
B 1
Φ(x) = B
B 0
B
B 1
@
0
0

1 val(f ) = x
1
1
C val(f2 ) = x1
C val(f3 ) = x2
C
C val(f4 ) = x2
C
C val(f5 ) = x1 x2
C
C val(f6 ) = x1 x2
A
val(f7 ) = x1 x2
val(f8 ) = x1 x2

(6)

where (6) addresses only the non-redundant dimensions of F.
Geometrically, Φ maps each n-dimensional vector x ∈
X to the 4n -dimensional vector in F that uniquely encodes the set of all projections of x onto the subspaces
of X . But is F semantically intuitive? After all, why
should we adopt this and not some another dimensional structure for Ω? To answer this question, recall
that X is just an attribution of Ω (induced by some
application-dependent considerations), and as such it
does not necessarily correspond to the criteria affecting
preference of the user over the actual physical alternatives. However, if the user provides us with some preference statements in terms of X, the implicit criteria
behind these statements obviously have some encoding in terms of X. Given that, the semantic attractiveness of F is apparent: it is not hard to see that

evaluation of any such implicit, preference-related criterion on x ∈ X necessarily corresponds to a single
dimension of F. In addition, Theorem 1 shows that
F is not only semantically intuitive, but also satisfies
our requirement of “no need for independence information”.
Theorem 1 Any preference ordering P over X is additively decomposable in F, that is, the existence of a
linear function
n

U (Φ(x)) =

4
X

wi Φ(x)[i]

(7)

i=1

satisfying (3) is guaranteed for any such P over X .
The proof of Theorem 1 is straightforward since we can
always specify weights wi associated with all complete
assignments to X such that (3) is satisfied. It is important to note, however, that this explicit “construction”
of U only serves the existential proof of Theorem 1, and
does not reflect whatsoever the machinery of our proposal.
Since, by Theorem 1, dimensions F can successfully
“linearize” any preference ordering P over X , in what
follows we can focus only on linear utility functions as
in (7). Of course, the reader may rightfully wonder
whether this linearization in a space of dimension 4n
can be of any practical use, and not just a syntactic
sugar. However, at this stage we ask the reader to
postpone the computational concerns, and focus on the
interpretation of preference expressions in the scope of
our new high dimensional space F.
There are two major categories of preference statements one would certainly like to allow in S [12],
notably dyadic (comparative) statements (indicating
a relation between two referents using the concepts
such as ‘better’, ‘worse’, and ‘equal in value to’), and
monadic (classificatory) statements (evaluating a single referent using ordinal language concepts such as
‘good’, ‘very bad’, and ‘worst’.)2 For ease of presentation, let us focus on dyadic statements for now. In
particular, consider an ”instance comparison” statement ”x is better than x0 ”, where x, x0 ∈ X . The
interpretation of this statement poses no serious difficulties because it explicitly compares between complete descriptions of two alternatives. However, this is
the exception, rather than the rule. Most of the preference statements that we use in our everyday activities
(e.g., “I prefer compact cars to SUVs”) have this or another generalizing nature. As such, these statements
2
This classification does not cover more ”higher order”
preferences, such as ”x is preferred to y more than z is
preferred to w” [19]. Although here we do not discuss such
statements, they as well can be processed in our framework.

typically mention only a subset of attributes. This
creates an ambiguity with respect to their actual referents. Several proposals on how to interpret preference
statements have been made both in philosophy and
AI, but there is no (and cannot be?) an agreed-upon
solution to this problem [12]. However, all the proposals suggest to interpret generalizing preference statements as comparing between complete descriptions X
of the alternatives, while disagreeing on what complete
descriptions are actually compared by each statement
separately, and/or by a multi-statement preference expression as a whole.
Considering interpretation of qualitative preference
expressions in F, observe that each parameter wi of U
as in (7) can be seen as capturing the marginal value
of the interaction between Var(fi ) when these take the
value val(fi ). Note that wi corresponds to this specific
interaction only; all the syntactically related interactions of subsets and supersets of val(fi ) are captured
by other parameters w, and the dimensional structure
of F allows such an independent bookkeeping of all
possible value-related criteria.
Now, consider an arbitrary dyadic statement ϕ  ψ.
Let Xϕ ⊆ X (and similarly Xψ ) be the variables involved in ϕ, and M (ϕ) ⊆ Dom(Xϕ ) be the set of ϕ’s
models. Following the most standard (if not the only)
interpretation scheme for OUR, we compile ϕ  ψ into
a set of constraints on the space of candidate utility
functions [15]. In our case, however, these constraints
are posed on the functions of form (7), which are linear, real-valued functions from the feature space F,
and not from the original attribute space X as in previous works. Specifically, we compile ϕ  ψ into a set
of |M (ϕ)| × |M (ψ)| constraints 3
∀m ∈ M (ϕ), ∀m0 ∈ M (ψ).

X

X

wi >

fi :val(fi )∈2m

wj (8)

fj :val(fj )∈2m

0

where 2m denotes the set of all value subsets of m.
For example, statement (X1 ∨ X2 )  (¬X3 ) (e.g., “It
is more important that the car is powerful or fast than
not having had an accident”) is compiled into
wx1 + wx2 + wx1 x2 > wx3
wx1 + wx2 + wx1 x2 > wx3
wx1 + wx2 + wx1 x2 > wx3

In first view, we clearly have some complexity issues
here. First, while the constraint system C is linear,
n
it is linear in the exponential space R4 . Second, the
summations in each constraint as in (8) are exponential in the arity of ϕ and ψ (i.e., in |Xϕ | and |Xψ |).
Finally, the number of constraints generated for each
preference statement can be exponential in the arity
of ϕ and ψ as well.
While exponential dimensionality of F is inherit in our
framework (and we promised to do something about
it later), the description complexity of C deserves a
closer look. First, the description size of each constraint is clearly something to worry about. For instance, each “instance comparison” between a pair of
complete alternatives in X is translated into a constraint with up to 2n+1 summation terms, and this is
a very natural form of everyday preference statements.
Fortunately, in Section 3 we efficiently overcome this
obstacle. On the other hand, the number of constraints
per preference statement seems to be less problematic
in practice, because the number of constraints equals
the number of models of ϕ and ψ, and explicit simultaneous preferential comparison between large sets of
models are rarely natural.

3

COMPUTATIONAL MACHINERY

At this point, we hope to have convinced the reader
that semantically our construction is appealing. What
still remains to be shown is that it is computationally
realistic. We begin with summarizing the complexity
issues that we have to resolve.
(a) Our target utility function U is a linear, real-valued
function from a 4n dimensional space F. Thus, not
only generating U, but even keeping and evaluating this function explicitly might be infeasible.

(9)

The constraint system C resulting from such compilation of a user expression S defines the space of solutions for our formulation of OUR. On the side of the
semantics, we argue that C corresponds to a least committing interpretation of preference statements. This
3

encodes the principle that, if there is no reason for a
bias towards certain explanations for ϕ  ψ, a most
general explanation should be preferred. In Section 3
we describe how we pick a particular assignment to wi
for a given set of constraints, and justify this choice in
Section 3.1.

The constraints for dyadic statements of the form ϕ 
ψ and ϕ ∼ ψ are similar to (8) with > being replaced by
≥ and =, respectively.

(b) The space of all suitable functions U is defined by
n
a set of linear constraints C in R4 . In addition to
the dimensionality of this satisfiability problem,
even the description of each constraint can be exponential in n = |X| for many natural preference
statements.
In the following we show that both these complexity
issues can be overcome. For ease of presentation and
without loss of generality, we introduce our machinery on preference expressions consisting only of strict

”instance comparisons” x  x0 , where x, x0 ∈ X. Our
translation of each such dyadic preference statement
x  x0 leads to a linear constraint of the form:
n

`
´
U (Φ(x)) > U Φ(x0 )

⇔

4
X

n

wi Φ(x)[i] >

i=1

⇔

4
X

wi Φ(x0 )[i]

i=1

w · Φ(x) > w · Φ(x0 )

(10)

According to this formulation, the set of utility functions consistent with a set of k such preference statements is defined by the solutions of the linear system
C:
∀1 ≤ i ≤ k. w · Φ(xi ) > w · Φ(x0i ),

(11)

4n

consisting of k constraints in R . Clearly, naive approaches to solving such systems will be computationally intractable for interesting n. In what follows, we
will exploit duality techniques from optimization theory (see [2]) and Mercer kernels (see [22]) as used in
machine learning to solve such systems in time that is
linear in n and polynomial in k.
At the first step, we reformulate our task of satisfying
C as an optimization problem. Since the solution of
(11) is typically not unique, we select a particular solution by adding an objective function and a “margin”
by which the inequality constraints should be fulfilled.
Specifically, similar to an ordinal regression SVM [13],
we search for the smallest L2 weight vector w that fulfills all constraints with margin 1. The corresponding
constrained optimization problem is:
1
Minimize (w .r .t. w) : w · w
2
subject to :
∀1 ≤ i ≤ k. w · Φ(xi ) ≥ w ·

many kinds of mappings Φ, inner products can be computed efficiently using a Mercer kernel (see [22]), even
if Φ maps into a high-dimensional (or infinite dimensional) space. Our task, thus, is to find such a kernel
for the specific mapping Φ that we use in our construction (4)-(5).
Let us define an injective representation of attribute
vectors x by projecting them to indicator vectors
~x ∈ R2n . Each attribute value is mapped onto a single
dimension. If an attribute value is present in x, the
corresponding component of ~x is 1, otherwise 0. If an
attribute is unspecified, all corresponding components
of ~x are set to 0. Using this construction, inner products for an (effectively equivalent) variant Φλ of our
mapping Φ can be computed as follows.
Theorem 2 For the mapping Φλ : X 7→ F = R4
(p
cλ (|val(fi )|),
Φλ (x)[i] =
0,

cλ (k) =

n
X

X

λl

l=k

l1 ≥ 1, ..., lk ≥ 1
l1 + ... + lk = l

l!
,
l1 !...lk !

(15)

and any x, x0 ∈ X and λ1 , ..., λn ≥ 0, the kernel
K(x, x ) =

n
X

λl (~x · ~x0 )l

(16)

l=1

(12)

computes the inner product Φλ (x) · Φλ (x0 ) = K(x, x0 ).

+1

Note that this reformulation of the problem does not
affect its satisfiability, and that the solution of (12) is
unique, since it is a strictly convex quadratic program.

Proof The following chain of equalities holds.
K(x, x0 ) =

In the second step we consider the Wolfe dual [2] of
(12):
(13)

k
X

k
k
1 XX
αi −
αi αj ((Φ(xi ) − Φ(x0i )) · (Φ(xj ) − Φ(x0j )))
2
i=1
i=1 j=1

=

n
X
l=1

=

The third and final step is based on the observation
that the dual (13) can be expressed in terms of inner
products in the high-dimensional feature space. For

n
X

λl (x · x0 )l

n
X
k=1

(xi1 x0i1 xi2 x0i2 ...xil x0il )

X

λl

l=1

subject to : α ≥ 0

This is a standard technique frequently used in the
context of SVMs [22, 13]. The Wolfe dual (13) has
the same optimum value as the primal (12). From the
parameter vector α∗ that solves the dual one can derive
P
∗
the solution w∗ of the primal as w∗ = m
i=1 αi (Φ(xi ) −
0
Φ(xi )).

n
X
l=1

=
Maximize (w .r .t. α) :

(14)

where

0

Φ(x0i )

val(fi ) ⊆ x
otherwise

n

(i1 ,...,il

)∈{1,..,2n}l

(xi1 xi2 ...xil )(x0i1 x0i2 ...x0il ))

X

λl
(i1 ,...,il

)∈{1,..,2n}l

X

cλ (k)

(xi1 xi2 ...xik )(x0i1 x0i2 ...x0ik )

{i1 ,...,ik }⊆{1,..,2n}

= Φλ (x) · Φλ (x0 )

cλ (k) is the multiplicity with which a monomial of
size k occurs. The multiplicity is influenced by two
factors. First, different orderings of the index sequence
(i1 , ..., il ) lead to the same term. This is counted by the
l!
, where l1 , ..., lk are the
multinomial coefficient l1 !...l
k!
powers of each factor. Second, all positive powers of
any xi x0i are equal. We therefore sum over all such

equivalent terms
X
l1 ≥ 1, ..., lk ≥ 1
l1 + ... + lk = l

l!
l1 !...lk !

Note that many of the monomials always evaluate to
zero under our encoding ~x of x. Specifically, monomials corresponding to expressions (xi ∧ xi ∧ · · · ) will
always be nullified. In particular, it is therefore sufficient to consider only those monomials of size less or
equal to n, since all others will always evaluate to zero.

The kernel (16), which is similar to a polynomial kernel
[22], allows us to compute inner products in the highdimensional space in linear time, and, for strictly positive λ1 , ..., λn , moving from Φ to Φλ does not change
the satisfiability of our constraint system C. To see
the latter, observe that any solution wλ of (11) for Φλ
corresponds to a solution w of (11) for Φ via
w[i]

wλ [i] = p

cλ (|val(fi )|)

.

The difference between the mappings Φλ and Φ is that
the former biases the inference’s prior towards smaller
size monomials, “preferring” more general explanations for user preference statements. On the other
hand, this bias can be controlled to a large degree via
the kernel parameters λ1 , ..., λn .
Now, using the kernel inside of the dual leads to the
following equivalent optimization problem.
Maximize (w .r .t. α) :
k
X
i=1

αi −

k
k
1 XX
αi αj (K(xi , xj ) − K(xi , x0j )−
2 i=1 j=1

(17)

K(x0i , xj ) + K(x0i , x0j ))

As a final comment on the mechanics of our inference
procedure, note that it would be unreasonable to expect that a user’s preference statements will always
be consistent. In case of inconsistent preference specification, one can use the standard soft-margin technique [7], trading-off constraint violations against margin size.
3.1

INFERENCE SEMANTICS

Since the user’s statements typically provide only partial information about her preferences, the constraint
system in (11) is underconstrained, and thus the utility revelation takes the form of inductive reasoning.
If the system has access to a prior P r(U) over utility
functions, a reasonable inductive inference procedure
would be to pick the most likely utility function U that
fulfills all constraints. In particular, for the Gaussian
2
prior P r(U) ∼ e||w|| this procedure results in finding
the weight vector with minimum L2 -norm that fulfills
the constraints. This is exactly our objective in (12).
To illustrate the behavior arising from this prior, consider the statements
s1
s2
s3

= (X1 ∨ X2 )  (¬X3 ),
= (X3 )  (X4 ),
= (X1 )  (X2 ).

For this small set of constraints, we can compute the
solution without the use of kernel and get the following
weights.
wx1 = 0.75
wx2 = −0.25 wx3 = 0.5
wx4 = −0.5
wx2 = 0
wx3 = −0.45 wx4 = 0
wx1 = 0.4
wx1 x2 = 0.05 wx1 x2 = 0.4
All other weights are set to zero. Below is an illustrative excerpt of the ordering induced by the utility
function generated in our framework:

subject to : α ≥ 0

It is known that such convex quadratic programs can
be solved in polynomial time [2]. To compute the value
of U for a given alternative x00 ∈ X , it is sufficient to
know only the dual solution and the kernel:
k
X
`
´
U Φ(x00 ) = w∗·Φλ (x00 ) =
αi (K(xi ,x00 )−K(x0i ,x00 ))
i=1

(18)

Hence, neither computing the solutions of the constraint system C, nor computing the values of U on X
n
requires any explicit computations in R4 . Through
the use of kernels, all computations can be done efficiently in the low-dimensional input space.4
4
We have extended SV M light to solve this type of
quadratic optimization problem. The implementation is
available at http://svmlight.joachims.org/. It can efficiently
handle large-scale problems with n, m ≈ 10, 000.

U (Φ(x1 x2 x3 x4 ))
U (Φ(x1 x2 x3 x4 ))
U (Φ(x1 x2 x3 x4 ))
U (Φ(x1 x2 x3 x4 ))
U (Φ(x1 x2 x3 x4 ))
U (Φ(x1 x2 x3 x4 ))
U (Φ(x1 x2 x3 x4 ))

=
=
=
=
=
=
=

1.25
1.05
0.9
0.55
0.1
−0.4
−0.55

We believe that this ordering reflects a natural interpretation of the statements. Furthermore, alternatives
for which the statements give no clear judgment receive utility values closer to zero than those for which
a statement clearly applies. In general, the Gaussian
prior appears reasonable in situations where we expect
the utility function to have a compact form (i.e. the
weights in w are small).

Now, recall that (i) each wi is devoted to capture
the marginal value of the event val(fi ), and that (ii)
we strive to a least committing interpretation of preference expressions. Observe that our inference procedure implicitly provides us with a reference point
n
0 ∈ R4 . In short, we have wi = 0 in case there
is no reason to believe the user associates some (positive/negative) value with val(fi ). Thus, consistent with
standard logics of monadic preference concepts [6],
utility U(Φ(x)) = 0 indicates that a user is either indifferent about x (i.e., has no reason to like it or dislike it), or neutral about it (user’s reasons to like x
somehow “balance” her reasons to dislike it). Moreover, this reference point provides us with an intuitive
encoding of monadic preference statements. For instance, a statement ”ϕ isP
good” is translated into a
set of |M (ϕ)| constraints fi :val(fi )∈2m wi > 0, which
can be seen as a special case of (8).

4

EVALUATION OF EMPIRICAL
EFFECTIVENESS

To demonstrate practical effectiveness of our approach,
we conducted experiments on the EachMovie dataset5 .
The dataset consists of six-point-scale movie ratings
collected from 72916 users on a corpus of 1628 movies.
Each movie is described by a set of attributes, out
of which we use the decade of the movie, whether it
is currently in the movie theaters, and a binary classification according to ten (non-disjoint) genre categories. In our experiments we generate one ordinal
utility function for each user.
The EachMovie dataset contains ratings for individual
movies, but no generalizing preference statements. To
simulate generalizing preference statements, we generated such statements using the C4.5 decision trees
learning algorithm [20] on the following binary classification problem. As training examples, we form all
pairs of movies by concatenating their attribute vectors. For each user we generate a separate training
set. If the first movie was rated higher (lower) than
the second movie, the pair is labeled positive (negative). No pair is generated if at least one of the movies
was not rated or if both movies have the same rating,
since it was unclear how to translate such cases into
training examples for the classification task. On this
data, we run the C4.5 decision tree learner6 . Using
the c45rules software included in the C4.5 package we
then convert the resulting decision tree into a set of
rules ordered by their level of confidence, and interpret each of these learned rules as a single preference
statement. For example, the highest ranked rule for
5
6

http://research.compaq.com/SRC/eachmovie/
http://www.cse.unsw.edu.au/∼quinlan/

the user that rated the largest number of movies was
the rule (a) below.

(a)

B_decade = 90s
B_Art_Foreign = 1
B_Family = 0
B_Romance = 0
-> A preferred to B

(b)

A_decade = 80s
A_Thriller = 1
B_Classic = 0
B_Horror = 1
-> A preferred to B

This rule can be interpreted as the monadic preference
statement “the user does not like foreign films from the
90s that are not Romance or Family movies”. For the
same user, the highest ranked dyadic rule is rule (b),
meaning “the user prefers thrillers from the 80s over
non-classic horror movies”.
The quality of the orderings induced by the generated
utility functions is measured in terms of ordering error, that is the fraction of times where the user rating
and the utility function disagree on the ordering of two
movies. For this error measure we consider only movie
pairs unequally rated by the user. Ties in the ordering
induced by the utility function are broken randomly.
Note that random performance according to this error
measure is a score of 0.5, and that a score of 0.0 indicates a perfect ordering. All results that follow are
averaged over the 45 users that provided the largest
number of movie ratings. To normalize for different
numbers of ratings, for each user we consider exactly
500 movie ratings randomly selected from her rating
list.
The left-hand panel of Figure 1 shows how well the
utility function orders the movies depending on the
number of preference statements used to generate this
function. In this analysis we use the top k preference
statements as returned by c45rules. Each curve in
Figure 1 gives the performance for a different choice
of kernel degree, i.e., different choice of kernel parameters λ1 , ..., λn . The “degree” d indicates that all λi
with i > d are set to zero, while all others are one.
This eliminates all monomials of size greater then d.
For small numbers of preference statements, all degrees perform roughly equivalently, but for larger sets
of preference statements, high-degree kernels substantially outperform low-degree kernels. It appears that
low-degree kernels cannot capture the dependencies in
the preference statements used in the evaluation, and
thus the ability to handle large-degree monomials (i.e.,
non-linear interactions between attributes X) is beneficial.
Since we are using a very coarse description of the
movies, the attributes do not suffice to produce a perfect ordering from a small number of preference statements. In particular, the average error rate of the complete set of C4.5 rules is 0.24. Note that this pairwise
classification performed by C4.5 is potentially easier
than the utility revelation problem, since the rules do

0.5

0.45

0.45

0.4

0.4

Error

Error

0.5

degree 1
degree 2
degree 3
degree 4
degree 5
degree 6
degree 7
degree 8
degree 9

0.35

0.3

1

2
4
8
16
32
Number of Preference Statements

degree 1
degree 2
degree 3
degree 4
degree 5
degree 6
degree 7
degree 8
degree 9

0.35

0.3

64

1

2
4
8
16
32
Number of Instance Preference Statements

64

Figure 1: Average error rate as a function of the number of statements in S, where S contains unrestricted
generalizing (left), or (right) only instance level statements.
not have to form an ordering. Comparing the C4.5
performance against the error rates of around 0.27
achieved by the ordinal utility function for the highdegree kernels, we conclude that our method performs
the translation into a consistent ordering effectively
and with good accuracy.
The right-hand panel shows an analog plot for using
only instance-level statements. Compared to the generalizing statements, the error rates here are worse,
and this indicates the beneficial expressive power of
generalizing statements. For both instance and generalizing statements we performed additional experiments using a soft-margin approach. This reduced
error rates, but gave qualitatively similar results. Regarding computational efficiency, the average CPUtime of SV M light for solving the quadratic program
for a set of 64 generalizing statements was less than
0.1 seconds.

5

RELATED WORK AND
CONCLUSIONS

We have described a novel approach to ordinal utility
revelation from a set of qualitative preference statements. To the best of our knowledge, our proposal
constitutes the first solution to this problem that can
handle heterogeneous preference statements both efficiently and effectively. The key technical contribution
is a computationally tractable, non-parametric transformation into a space where ordinal utility functions
decompose linearly and where dimensions have clear
and intuitive semantics. As such, our approach addresses a long-standing open question in the area of
preference representation, formulated by Doyle [8] as:
“Can one recast the underlying set [of attributed alternatives] in terms of a different [from the original
attribution] span of dimensions such that the utility

function becomes linear? If so, can one find new linearizing dimensions that also mean something to human interpreters?”
We have found in the literature only one work directly attempting to shed some light on this question, namely the work of Shoham on utility distributions [21]. Specifically, Shoham shows that a set of
linearizing dimensions exists for any utility function,
and that this set of dimensions may have to be exponentially larger than the original set of attributes. The
result of Shoham, however, is more foundational than
operational. First, the connection between the original attribution and the particular set of dimensions
proposed in [21] is not generally natural, and thus it
is rather unclear how to perform preference elicitation
with respect to this set of dimensions. Second, no efficient computational scheme for reasoning about this
set of dimensions has been proposed so far. Thus, we
believe that our work is the first to provide an affirmative, practically usable, answer to the question of
generic existence of an intuitive linearizing space of
dimensions.
Our ongoing and future work builds upon the foundations laid in this paper in several directions. First, we
would like to provide some informative upper bounds
on the number of preference statements that a user
will have to specify before the inferred utility function
approximates her preferences sufficiently well. Furthermore, we would like to study applicability and efficiency of standard active learning techniques to mixedinitiated preference elicitation in our framework. Finally, we would like to perform a deeper analysis of
the semantics of our inference procedure, connecting
it, for instance, with the recent axiomatic approaches
for preference revelation such as [16].

This work was funded in part under NSF CAREER
Award IIS-0237381.

[14] R. L. Keeney and H. Raiffa. Decision with Multiple Objectives. Wiley, 1976.




Bayesian knowledge bases (BKBs) are a gen­
eralization of Bayes networks and weighted
proof graphs (WAODAGs), that allow cycles
in the causal graph. Reasoning in BKBs re­
quires finding the most probable inferences
consistent with the evidence. The cost­
sharing heuristic for finding least-cost ex­
planations in WAODAGs was presented and
shown to be effective by Charniak and Hu­
sain. However, the cycles in BKBs would
make the definition of cost-sharing cyclic as
well, if applied directly to BKBs. By treat­
ing the defining equations of cost-sharing as
a system of equations, one can properly de­
fine an admissible cost-sharing heuristic for
BKBs. Empirical evaluation shows that cost­
sharing improves performance significantly
when applied to BKBs.
1

INTRODUCTION

Bayes networks [7] are a commonly used reasoning
tool within the uncertainty in AI community. Lately,
graphical causal probabilistic models have shown up
that generalize on the acyclic Bayes networks, in or­
der to cater for causal phenomena which cannot be
strictly partially ordered. These models have causal
cycles [1, 8], or undirected sections in the directed
graphs [2J. Clearly, one stii! needs to do either be­
lief revision or belief updating [7] in order to perform
reasoning in these schemes. These more general mod­
els, being less restrictive, pause interesting problems
in implementing reasoning algorithms for them.
Bayesian knowledge bases (BKBs) [8] are a general­
ization of Bayes networks and weighted (AND/OR,
directed acyclic) proof graphs (acronym WAODAGs)
[4], that allow cycles in the causal graph. Consider the
problem of finding the most probable inference ("ex­
planation") consistent with the evidence in a BKB.
This problem is analogous to (and more general than)
the NP-hard problem of belief revision in Bayes net-

Eugene Santos Jr.

Dept. of Electrical and Comp. Eng.
Air Force Institute of Technology
Wright-Patterson AFB, OH
e-mail:esantos@afit.af.mil

works, or finding minimum-cost proof on a WAODAG.
As for Bayes networks, reasoning with tree-shaped
BKBs can be done efficiently. However, it is clear that
in actual applications we cannot usually force our rep­
resentation to belong to the easy class of problems.
To-date, finding most-probable inference in general
BKBs has been implemented as best-first heuristic
search, where the heuristic used was cost-so-far, with
dismal results. The reason is that this local heuris­
tic does not take into account the cost of nodes (or
variables) to be assigned later on in the search. Prop­
agation of costs to be incurred is much preferable, but
it is non-trivial to do so in a manner resulting in an
admissible heuristic. The latter was first achieved by
using the cost-sharing propagated cost method [3] (see
next section for a brief definition).
It was shown by Charniak and Husain [3] that for find­
ing least-cost explanations in WAODAGs, the ( admis­
sible) cost-sharing heuristic has a much better perfor­
mance. The cost sharing heuristic was also found use­
ful for belief revision in Bayes networks [9). Here, we
generalize cost-sharing to apply to cyclic graphs, and
show that the resulting heuristic is also admissible.
The generalization of the cost-sharing heuristic, while
straightforward, causes several problems. First, the
cycles in the BKB make the problem of properly defin­
ing the heuristic nontrivial. If we just used the same
defining equations, the fact that there are cycles would
make the defining equations cyclic. But by looking
at these equations as a system of equations, we state
that a solution to the system is our heuristic. Any
such solution to the system of equations is shown to
be an admissible heuristic. A second problem is how
to solve these equations. The standard top-down algo­
rithm used in prior work would be hindered by the cy­
cles: even if we convert it to a kind of message-passing
updating algorithm, in many cases the algorithm will
loop indefinitely. Instead, we show that converting the
system of semi-linear equations to a linear program, we
can evaluate the heuristic in polynomial time.
We begin with a motivating BKB example, followed
by a formal definition of BKBs (section 2). We then
relate BKBs to WAODAGs, and review the cost shar-

422

Shimony, Domshlak, and Santos

.,

Figure

1:

Example graph with RVs as nodes.

ing heuristic for WAODAGs. In Section 4 we extend
cost-sharing to handle cy cles, and present an efficient
method of computing the heuristic. Section 5 discusses
several implementation issues and refinements. Sec­
tion 6 compares search with cost-sharing to search with
a local "cost-so-far" heuristic.

2

Figure

2:

Example Knowledge Graph

BAYESIAN KNOWLEDGE BASES

In modeling an uncertain world, we designate random
variables (abbrev. RVs) to represent the discrete ob­
jects or events in question. We then assign a joint
probability distribution to each possible state of the
world, i.e., a specific value assignment for each RV.
Graphical probabilistic models, such as Bayes net­
works [7], represent the existing dependencies in the
model (variables not shown as dependent are assumed
independent) , facilitating a concise representation of
the distribution- as a set of conditional probabilities.
Let D, E and F be RVs. The conditional probabil­
ity, P(DIE, F), identifies the belief in D's truth given
that E and F are both known to be true, and repre­
sents an uncertain causal rule. We call D the head of
P(DIE, F) and {E, F} the tail.
The distribution of the model is defined by,
n

P(A1, ... , An)=

II P(A;IX(A;))

(1)

i=l

where X(A;) is the set of RVs which Ai condition­
ally depends upon. If set X(A;) is small, the amount
of information we must actually store to be able to
compute the required joint probability is considerably
(exponentially) less than the size of the cross product
of the domains.
In Bayes networks, the conditional dependencies are
represented with a directed acyclic graph. Let A, B
and C be RVs representing a traffic light, its asso­
ciated vehicle detector and pedestrian signal, respec­
tively. Suppose that the vehicle detector affects the
traffic light, which in turn affects the pedestrian sig­
nal. Figure 1 graphically depicts this network over
these variables. Since the signal depends upon the
light, we say that A is the parent of C. Similarly, B is
the parent of A.
Now, expand the model with an additional variable
denoting time of day, and suppose that the domain

expert wishes to add the conditional probability that
the detector is tripped during rush hour when the light
is red. Such an inclusion would introduce a cycle into
our graph, which would not be permitted in a Bayes
network. In the application domain, however, such
cycles are frequently a natural representation.
By introducing a finer level of distinction than one
node per RV, using instead one node for each possi­
ble RV instantiation, the BK B representation finesses
this problem. Assuming the same trio of RV s and the
partial set of values below:
P(C ="Don't Walk" lA =red)=x1

P(A = greenjB = On) = X3
P(C = "Walk"IA =green) = x2
P(A = redlB

=

Off) =X4

We can legally add the new constraint, P(B =On lA =
red, D =rush hour) =xs, without creating a directed
cycle, as shown in Figure 2. Additionally, it is possi­
ble to have cycles in the knowledge graph in certain
cases, without jeopardizing consistency of the distri­
bution (see [8]).
A BK B graph has two distinct types of nodes. The
first, shown as lettered ovals, corresponds to individ­
ual RV instantiations. These are called instantiation
nodes or !-nodes for short. The second type of node,
depicted as a blackened circle, is called a support node
or 5-node. These nodes, which represent the condi­
tional probability value, have exactly one out-bound
arrow to the instantiation node representing the head
of the conditioning case. Support nodes also have zero
or more in-bound dependency or conditioning arrows
representing the tail of the conditioning case.
The above representation of the conditional probabili­
ties, by separating out the variable-states and the (pos­
sibly partial) conditioning, results both in more flexi­
bility, and a more compact representation [8]. These
properties are extremely useful in knowledge acqui-

423

Cost-Sharing in Bayesian Knowledge Bases

sition and in learning models from data, for various
applications such as data-mining [5].
2.1

I

t

\

DEFINING KNOWLEDGE GRAPHS

'

'

10

1

'

I

I

sl ',

-

'

'

\

I

'

5

'

s4 �
I

We define the topology as follows:
Definition 1 A correlation-graph G

= (IUS, E) is a
directed graph such that InS = 4> and E � {I x S} U
{S xI}. Furthermore, for all a E S, (a,b) and (a,b')
are in E if and onl y if b = b'. {IUS} are the nodes
of G and E are the edges of G. A node in I is called
an instantiation-node (abbrev. 1-node) and a node in
S is called a support-node (abbrev. S-node).

1-nodes represent the various states of the world such
as the truth or falsity of a proposition.
S-nodes, on
the other hand, explicitly e mbody the relationships
between the 1-nodes.
Let 1r be a partition on I. Intuitively, 1r denotes the
groups of 1-nodes (states) which are mutually exclu­
sive. This can be used to represent random variables
with discrete but multiple instantiations, with each
partition cell corresponding to an RV.
Definition 2 G

is said to I- respect 1r if for all cells O"
in 1r, for any S-node b E S such that (b, a) E E, b does
not have a parent in O" except, possibly, a.
Basically, mutually exclusive 1-nodes cannot be di­
rectly related to each other through the S-nodes. Next,
we define mutual exclusion between S-nodes.
Definition 3 Two S-nodes b1 and
be mutually exclusive with respect

b2 in S are said to
to 1r if there exist
different /-nodes c1, c2 that are parents of b1, bz, re­
spectively and c1, c2 are in the same cell in 11".

Definition 4 G is said to S-respect 1r if for all /-nodes
a in I, any two distinct parents of a {S-nodes b1 and
b2) are mutually exclusive.

G is said to
respects and S-respects 1r.

Definition 5

respect

1r

if G both !­

To complete our knowledge-graph, we define a function
w from S to �- This serves as the mechanism for
handling uncertainty in the relationships.
Definition 6 A knowledge-graph K is a 3-tuple
(G, w, 1r) where G = (/US, E) is a correlation-graph,
w is a function from S to the positive reals (for each
a E S, w(a) is the weight of a), 1r is a partition on I,
and G respects 1r.

The probabilistic semantics of a knowledge graph is
provided by relating weights to probabilities, as fol­
lows: P'(a) = e-w(a), where P'(a) is the conditional
probability that the child of a is true given that the

's5 ,

Figure

3:

0

Knowledge Graph with a Cycle

parents of a are true. To make sure that the probabil­
ities obey the axioms of probability theory, a normal­
ization constraint is enforced [8] on BKBs. However,
this issue is irrelevant to finding most-probable infer­
ence, and is thus beyond the scope of this paper.

2.2

INFERENCE GRAPHS

A BKB is a knowledge graph, together with an infer­
ence scheme. The latter is defined by a set of permissi­
ble inference graphs. An i nferen ce graph is a subgr aph
of the knowledge graph corresponding to an inference
chain (or proof) . Let r
(I' U S', E') be some sub­
graph of our correlation-graph G = (I U S, E) where
I' � I, S' � S, and E' � E. Furthermore, r has a
weight w ( r) defined as follows:
=

( )

w r

=

L w(s).
&ES'

An 1-node a E J' is said to be well-supported in r i f
i t h as a n incoming S-node i n r (that is, if there exists
an edge (b, a) in E'). An S-node b is said to be well­
founded in r if all its incoming 1-nodes (conditions)
are also present in r. An S-node b E S' is said to be
well-defined in r if it supports some I-node.
is said to be an inference over K if i t is acyclic,
consistent (i.e. for all cells O" in 1r, II' n O"l � 1), all of
its 1-nodes are well supported, and all of its S-nodes
are well-founded and well-defined. An inference thus
corresponds to a proof. Given the knowledge graph in
F igure 3, one possible inference can be seen in Figure 4.

r

For an abductive BKB, the problem we are address­
ing is, given a set s of 1-nodes, find an inference r of
minimum weight that contains all of s. Given the se­
mantics of costs in BKBs, such an inference (proof)
is equivalent to a maximum probability explanation
(abductive inference) for s.

3

COST-SHARING IN WAODAGS

WAODAGs [4] are essentially acyclic knowledge
graphs, with a single sink (out-degree 0) node s (called

424

Shimony, Domshlak, and Santos

)' - '

/ s2

'

5
I

/' s4 '�
'

-

'

!_

�-,

's5 '

Figure

4:

cS

An Inference in the Knowledge Graph

edges. The actual solution is the set of nodes abutting
the dummy edges. Since we need to find the minimal
cost solution, we need a heuristic value for each cut. In
fact, the heuristic value is defined over both edges and
nodes, as follows. Let w ( v ) be the weights of the root
nodes. We define the heuristic cost function c from
E U V U 2E to the non-negative reals as:
if v is a root node
if v is an AND node
if v is an OR node
for nodes,

v
ce
( )
the evidence node), and a partition of nodes into AND
nodes (corresponding to S-nodes in a BKB) and OR
nodes (I-nodes in a BKB). The evidence node is an
AND node. Each WAODAG node has an associated
cost (or weight). A proof r of sis a subgraph contain­
ing s where for each AND node in r, all of its parents
are in r, and for each OR node in r, at least one of its
parents are in r. Proofs in WAODAGs correspond to
inferences in BKBs. The partition function 1r has no
counterpart in WAODAGs.
The cost of a proof r is the sum of the weights of all
nodes in r. As for knowledge graphs, one would like to
find a least cost proof that contains the evidence node
s. This is an NP-hard problem, usually solved by best­
first search, starting from s, and adding parents when
necessary (branching when several possibilities exist at OR nodes). An obvious admissible heuristic, cost­
so-far, estimates the cost of a partial proof p as the
sum of costs of nodes currently in p.
The heuristic can be improved upon by propagating
costs, but preserving admissibility is non-trivial. Such
an improved heuristic, cost-sharing, was first presented
in [3], where the search is in terms of edges, rather
than nodes. We review the cost-sharing heuristic for
WAODAGs below, beginning with some necessary no­
tation borrowed from [3].
Let edge e = ( a, b) (from node a to node b); we call a
the source of e, and b the sink of e. Also, we say that
b is a child (immediate descendent) of a. If e is an
edge, then v. denotes that node v is its source, and u•
denotes that u is its sink (we also sometimes use the
notation Ve also to denote the source node of e, likewise
for sinks). If v is a node, then e is an arbitrary edge
v
v
incoming to v, and e an outgoing edge from v. Also,
Ev is the set of all incoming edges of v, and Ev is the
set of all outgoing edges. A node with no parents is
called a root node.
For convenience, a dummy edge e' leading from the
evidence node is added to the graph, as well as one
dummy edge leading into each root node. A state
in the search space is a cut of the WAODAG (a set­
wise minimal set of edges that separates s from the
root nodes). The initial state is the set { e'}, and a
final state is a cut consisting only of root node dummy

(v

c e)
=�

for edges, and for sets of edges

c(C)

=

C � E:

L c(e)
eEC

4

COST-SHARING IN BKBS

For BKBs, we intend to use the same definitions for the
cost-sharing heuristic. One difference between BKBs
and WAODAGs is that in WAODAGs, only root nodes
have weights, whereas in BKBs every S-node has a
weight. The difference can be overcome by observing
that for each S-node v we can always add one new 1'
node and S-node pair (call the latter v ) , set w ( v' ) =
w (v), and let the new w ( v ) be 0.
The semantics of
the BKB stay the same, and now only root nodes have
non-0 cost. Instead of doing that, we will note that
the new I-node only has one parent and one child, and
absorb w ( v' ) into the equation for v, to get:
_

c( v ) -

{ c(Ev)

+

( )
c( ev)

w v

mine.EE.

if v is an S node
if v is an I node

(2)

for nodes, and the same equations as above for edges.
Noting, however, the optimization in [3], observe that
disjoint S-nodes are never in the same inference, and
thus we can replace the equation for edges by:

( v)
ce

=

c(ve)
k(ev)

(3)

where k(ev) is the number of consistent immediate
support paths. Specifically, if v is an S-node, then
k(ev) = 1 since there is only one outgoing edge from
an S-node. If v is an I-node, k( ev) is the number of
consistent 1-nodes immediately supported by S-nodes
that are children of v.1 As for WAODAGs, we have
for sets of edges:

c(C)

=

L c(e)

(4)

eEC
1This number should be an upper bound on the number
of edges outgoing from v that are in any inference. It may
be possible to get a tighter bound in some cases, and if so
that bound can be used in place of k( e").

Cost-Sharing in Bayesian Knowledge Bases

It is by no means clear whether these equations are suf­
ficient to uniquely define the cost function. However,
treating equations 2, 3, 4 as a system of equations in
the variables c( v ), c( e) with v E V, e E E rather than
a definition, we can refer to solutions of the system.
Henceforth, we will denote by c an arbitrary solution
to equations 2, 3, 4, whenever unambiguous. We will
show that an arbitrary solution c to the system (hence­
forth called a cost-sharing solution) is an admissible
heuristic, by extending the proof of [ 3].

425

Consider the knowledge graph in figure 3. The follow­
ing values can be computed immediately: c((sl, il))

=

=

and c ((s4 , i2))
c( s4) = 5.
All other
equations now contain undefined terms, so we proceed
by evaluating partially defined minima. For example,

c(sl)

10,

=

=

we could now set (temporarily), c(i2)
5. As a result,
we get the edge cost c((i2, s2)) = 2.5, since i2 has two
children. We now have c(s2)
3.5, and this in turn
=

makes c(il) = 3.5, c((il, s3))
1.75, and
This causes a re-evaluation of c(i2)
=

=

c(s3) = 2.75.
2.75, which

causes re-evaluation to proceed indefinitely, until even­

Theorem 1 Any cost-sharing solution c is an admis­

sible heuristic for BKBs.
Proof outline: we first note that while the BKB is a
graph with cycles, an inference is acyclic, and corre­
sponds to an AND-DAG. A cut of an inference is de­
fined exactly as for an AND-DAG in [3]: a minimal set
of edges that any path from the evidence to the leaves
must intersect. As in WAODAGs, we define a cut of
the BKB as a cut of some BKB inference.
Now, we proceed with the same proof as in [3]. All
steps of the proof are the same, it does not matter
that we have cycles, as the cycles only serve to further
decrease c, and thus it is still an underestimate of the
true weight.
The remaining problems are with applying the
WAODAG expansion operator Sr, which requires a
topological sort T of the DAG. Since we have cycles,
this is no longer possible. We must guarantee that
once the we apply the expansion operator at a node,
its outgoing edges will not be used anymore. To do
that, we modify the expansion operator as follows. A
state s is a set of edges and a set of deleted edges. Our
expansion operator S, applied at node n is the same
as Sr, except that when S is applied at node n, the
edges En are added to the set of deleted edges. A state
which contains a deleted edge is illegal, and discarded.
In order that all possible inferences be reachable, it is
not sufficient to apply the expansion according to a
topological ordering. If S is applied, at each state, at
all nodes where there is some en in the current set of
edges, reachability is maintained. 0
We now address the problem of computing a solution
c(v). Clearly, the seemingly obvious solution of using
the equations directly will not help: some values will
be undefined initially. One could think of a scheme
that, to compute a minimum over several terms, some
defined and some not, just takes the minimum over the
currently defined terms, and propagates the resulting
value. If every node participates in some inference,
such a scheme is guaranteed to assign a cost value at
each node eventually. However, in order to have all
equations satisfied, it may be necessary to update cost
values already derived, for example, due to finding a
lower value than already used before, at an 1-node. It
turns out that such a scheme may loop indefinitely, as
the following example shows.

tually (in the limit of an infinite number of loops,
or i n practice determined by computational numeri­
cal accuracy) we get convergence at: c(il) = c(i2) =

c(s2) = c(s3) = 2, c((il, s2)) = c((il, s3)) = 1, and
c(i3) = c((s5, i3)) = c(s5) = 3. Not e further that the

costs we get reflect an illegal, cyclic inference, but since

we need an underestimate in order to get an admissible
heuristic, this is not a problem.
Solving the system of equations efficiently is non­
trivial. In fact, if we had max functions in addition
to the min functions, or if the summation included
negative terms, it would be easy to show that the prob­
lem of finding a solution is NP-hard (and deciding the
existence of a solution is NP-complete). However, in
our case , we can use linear programming techniques to
derive a solution, by transforming the equations to a
linear system, as follows. For each node and edge, we
have a variable, which for convenience we denote by
the same name. Linear equations are left as they are.
Minimization equations are translated as follows:

is replaced by the set of inequalities:
V

�

Ut

1

V

�

U2

,

. ..

V

�

Uk

Observe that the latter set of inequalities is weaker
than the minimization. F inally, the objective function
to maximize is:
8(c)

=

L

vel

( )

c v

An optimal solution c* to the above linear program can
be found using standard linear programming methods,
such as the simplex method [6]. A solution always ex­

=

ists, since setting all c( v )
0, v E I clearly determines
a unique, not necessarily optimal, solution to the equa­
tions and inequalities.

Theorem 2 Let c• be an optimal solution to the lin­
. ear program. Then c* is also a solution to the cost­
sharing equation system (equations 2, 3, 4).
Proof: Let c* be an optimal solution to the linear pro­
gram. Assume that c* violates some of equations 2,
3, 4. Since the linear linear program equations are
the same as equations 2, 3, 4, except that minimiza­
tion was replaced by inequalities, only equations of the
form:
c(v ) = min c( ev)
e ,eE.

426

Shimony, Domshlak, and Santos

can be violated. Let v be a variable (node) where the
above equation is violated. Then, since the linear pro­
gram enforces c*(v ) S c* (ev ) for all eu E Ev, then it
must be the case that for this variable, c*(v) < c• (ev)
for all ev E Eu (otherwise it will indeed be the min­
imum, thus not violating the equation). Define R to
be the set of nodes consisting of v and its immediate
descendents (all immediate descendents are S-nodes),
and let ER be all edges with sources in R. Define an­

and, for I-nodes with just one parent:

c(ve) = w(ue ) +

e'"'EE.

c(we')
-k(e'"')

(6)

Next, observe that the linear program is only neces­
sary within each strongly connected component. The
implementation is, thus:

'

other solution c* to the linear program as follows. For

L

1.

Initialization: find strongly connected compo­
nents, and sort them in a total ordering consistent
with a topological ordering of the components,
such that the evidence node(s) is first.

program equations (starting with edges v E, then the
S-nodes, then the rest of the edges in ER) determine

2.

the as yet undefined costs in c•'. The resulting solution
is unique, because it uses equations for which all values
on the right-hand-side are already determined, and the

Add to the graph a dummy edge
S-node v.

3. Proceed from the last component down to the
first, and for each component do:

every node

u

not in R let

for every edge

e

c*'(v) = Vmin

=

c*'(u) = c*(u), and likewise
not in ER, Jet c•' ( e ) = c* ( e ) . Let
mi ne v EE v c* ( ev ) , and let the linear

8(c*),

and

As an example, consider the graph of Figure 3, where
we would get the following set of inequalities:

il S 1 + (i2, s2)

(b) Solve the linear program, to get the heuristic
costs.

i2Sl+(il,s3)
i
i
(il, s3) =
(i2, s2) =
;

4. Initialize an agenda with a single state s, with
edges(s)=e', the evidence dummy edge, and an
empty list of expanded nodes.

il s

10

i2S5

;

�

5.

85 = (il, s5) + (i2, s5) ; i3 = s5
i2
il
.
.
(z2, s5) = 2 ; (zl, s5) = 2
where we need to maximize c(i l ) + c(i2) + c(i3). The
optimal solution is the same as the convergence value
shown above, i. e. c(il) = c(i2) = 2 and c(i3) = 3.

5

for every

(a) Set up the linear program over variables de­
termined by nodes and edges for the cur­
rent component (including edges to and from
other components). Incoming edges will have
costs set in previous components, if any, and
these costs are considered as constants for
this component.

'

left-hand side is not. Clearly, c• is also a solution to
the linear program2, where only one I-node cost was
changed (increased). We have 8(c*') >
thus c* is not optimal, a contradiction. D

( *• v )

IMPLEMENTATION DETAILS

In applying the cost-sharing heuristic, we actually use
a simplified linear program, as follows. First, whenever
an 1-node v has only a single parent, we use c( v ) =
c( ev ) rather than the inequality. We also cancel out
all edge cost variables by substituting them according
to equations (3, 4). Finally, we can also cancel out by
substitution all the S-node costs (noting that S-nodes
all have only one child), to get a system of inequalities
just for the 1-node costs. For !-nodes with more than
one parent, we get:

c(ve ) S w(ue) +

�
L...J
e'wEEu

c(we1)
k(e'w)

Looping until time limit, or required number of
solutions found, get states of lowest heuristic cost
from agenda, and do:
(a) Find the first strongly connected component
containing a node v with some outgoing edge
v
e
in edges(8 ) .

(b) If there is no such component, output
solution).

of the i neq ualities for other !-nodes, which cannot cause
the inequalities to be v iolated .

(a

(c) Otherwise, expand 8 at the current strongly
connected component, as follows. For each
unexpanded node v in the current compo­
nent for which there is an edge dv E E11 in
edges(8 ) , and for each edge ev E Ev such that
no parent of u. (the source node of e11) has
been expanded do:
1.

n.

(5)

2
Costs of ed ges and S-nodes can only be increased by
this change, thus can (at worst ) affect variables not in R
through introducing higher values on the right-hand-side

s

m.

create a new state s', with node v added
to the list of expanded nodes. The edges
of s' are the edges of 8 with all edges E11
removed and all edges Eu added.
Evaluate the cost of s' by subtracting the
cost of removed edges and adding the cost
of added edges, from the cost of s.
1

If 8 is consistent (does not contain any
pair of 1-nodes from the same cell in the
partition), insert it into the agenda.

For example, let us trace the algorithm as run on the
BKB fragment of figure 3, with i3 being the evidence.

Cost-Sharing in Bayesian Knowledge Bases

The heuristic costs are computed as shown in the pre­
vious section. The strongly connected components are
{il, i2} and { i3}. The starting state, So, contains just
the dummy edge (i3, *). Search proceeds as shown in
table 1, where "Pop" is the ID of the popped state. For
lack of space, the dummy edges (*,i5),(*,s2),(*,s3)
are missing from the table, but this should not ad­
versely affect clarity.
In the actual implementation, several details should be
observed. First, instead of maintaining a list of deleted
edges, it is sufficient, and more efficient, to treat any
edge outgoing from an expanded node as if it were a
deleted edge, and maintain a list of expanded I-nodes.
Second, the fact that S-nodes are all AND nodes with
a single outgoing edge is used to save some time: once
an edge outgoing from an S-node is picked, we are
forced to select all the incoming edges into the S-node
anyway, so we do all that in one expansion step, and
do not keep track of expanded S-nodes.
6

•Caa-so-f•
• Cou-...,...1"• (tai.WJ

-�-�---4--f----i- • CWJ-Sharinj

l

i!r-�----+-----i·�·-1

Results are depicted in Figures 5, 6 using log-scale of
time to solution and number of expansion steps, re­
spectively for the Y axis (X axis is just the number
of the problem instance, and thus essentially meaning­
less here). Times for cost-sharing include initialization
of the heuristic costs. The cases labeled cost-so-far
(failed) are those taking 2000 seconds without reach­
ing a solution, or crashing due to lack of swap space.
Figure 7 plots total number of problems solved vs. to­
tal CPU time. Cost-sharing does better than cost-so­
far by at least one order of magnitude. Finding several
best solutions is also useful [9]. A timing comparison
for the 10 best solutions is depicted in Figure 8.
These preliminary experiments suggest that cost­
sharing is an extremely useful search heuristic for
graphs with cycles, as well as directed acyclic graphs.
We know of no other heuristics for this search prob­
lem. Nor is it clear how one would apply schemes
such as clustering to BKBs, and even if they could, a
strongly-connected component size of over 40 for most
of the problem instances in the experiments suggest
that the clique size would be too large to handle by
such schemes. Thus, heuristic search with cost-sharing
appears to be the only viable method for BKBs not in
one of the (topologically) easier classes of problems.

I

'
le-(H

___;,_

___,--+-----

___

6.00

!tOO

UtOO

Figure 5: Time: Cost-Sharing vs. Cost-So-Far

EXPERIMENTAL RESULTS

The above algorithm was tested on several BKB's pro­
duced from an available acyclic BKB for reasoning
about raising gold-fish. The network has 165 !-nodes
and 350 S-nodes. Cycles were introduced by random
reversal of several arc pairs. Evidence selection was
also random. Runtime and number of expansion steps
(iterations through step 4 of the algorithm) were com­
pared between a search algorithm using cost-sharing
and one using cost-so-far. The program was imple­
mented in C++, and run on a SPARC-10.

427

--.,.-- ----:---+----+-

•Cc.i-K>-fu

:�:::;eikd
; )

__:_

Jc.H.JJ

---1------f----j--

��

2-

''""'' --'-------+---'-

'"'

Figure 6: Expansion Count Comparison

�
--"tiiilt:�

10.00

/
/
/

0.00

'"'

-----��

6.50
6.00
'"'

,.,

'·"'
l.OO

--

L
/
'

'

l.lO
1.00

•

I

_j_

_j_
J

i

---

- �1�=�

'"' ----+ -2.00

l

I

I
I

......----�

-----, _,_

. ________ _

I

/

/

L

..d.
l

l
j
I

_

I

I

I'C'-01

Figure 7: Number of Problems Solved vs. CPU Time

Shimony, Domshlak, and Santos

428

Iteration

0
1
2

Pop

So
s1

s5
s3
s1

3
4
5

Expand Edge

Edges

(i3, *)
(i1, s5) , (i2, s5)

(i1,s3), (i1, s5)
(i2, s2) , (i2, s5)
(*,s4)

Delete Edges

i3, *)
i1, s5)(i1 , s3)
i1, s5 , il, s3
i2,s2 , i2, s5
i2, s5 , i2, s2
i1, s5 ,(i1,s3
i2, s5 , i2, s2

i3, *)
i1, s5
i1, s5
i2, s5
i2, s5
( i1, s5
i2, s5

Add Edges

New State ID

i 3 , *)
So
i1 , s5) , (i2, s5) s1
* , sn
s2
s3
i2,s2)
* , s4)
--g4
i1, s3)
s5
(*,s1)
s6
*, s4)
s1

NONE
Table

1:

Cost

3
3
12
4
7

4
12
7
7

Trace of the Search Algorithm



Domain-independent planning is one of the foundational areas in the field of Artificial Intelligence. A description of a planning task consists of an initial world state, a goal, and a set of actions
for modifying the world state. The objective is to find a sequence of actions, that is, a plan, that
transforms the initial world state into a goal state. In optimal planning, we are interested in finding not just a plan, but one of the cheapest plans. A prominent approach to optimal planning these
days is heuristic state-space search, guided by admissible heuristic functions. Numerous admissible
heuristics have been developed, each with its own strengths and weaknesses, and it is well known
that there is no single “best” heuristic for optimal planning in general. Thus, which heuristic to
choose for a given planning task is a difficult question. This difficulty can be avoided by combining
several heuristics, but that requires computing numerous heuristic estimates at each state, and the
tradeoff between the time spent doing so and the time saved by the combined advantages of the
different heuristics might be high. We present a novel method that reduces the cost of combining admissible heuristics for optimal planning, while maintaining its benefits. Using an idealized
search space model, we formulate a decision rule for choosing the best heuristic to compute at each
state. We then present an active online learning approach for learning a classifier with that decision
rule as the target concept, and employ the learned classifier to decide which heuristic to compute at
each state. We evaluate this technique empirically, and show that it substantially outperforms the
standard method for combining several heuristics via their pointwise maximum.

1. Introduction
At the center of the problem of intelligent autonomous behavior is the task of selecting the actions
to take next. Planning in AI is best conceived as the model-based approach to automated action
selection (Geffner, 2010). The models represent the current situation, goals, and possible actions.
Planning-specific languages are used to describe such models concisely. The main challenge in
planning is computational, as most planning languages lead to intractable problems in the worst
case. However, using rigorous search-guidance tools often allows for efficient solving of interesting
problem instances.
In classical planning, which is concerned with the synthesis of plans constituting goal-achieving
sequences of deterministic actions, significant algorithmic progress has been achieved in the last
two decades. In turn, this progress in classical planning is translated to advances in more involved
planning languages, allowing for uncertainty and feedback (Yoon, Fern, & Givan, 2007; Palacios
c 2012 AI Access Foundation. All rights reserved.

D OMSHLAK , K ARPAS , & M ARKOVITCH

& Geffner, 2009; Keyder & Geffner, 2009; Brafman & Shani, 2012). In optimal planning, the
objective is not just to find any plan, but to find one of the cheapest plans.
A prominent approach to domain-independent planning, and to optimal planning in particular,
is state-space heuristic search. It is very natural to view a planning task as a search problem, and
use a heuristic search algorithm to solve it. Recent advances in automatic construction of heuristics
for domain-independent planning established many heuristics to choose from, each with its own
strengths and weaknesses. However, this wealth of heuristics leads to a new question: given a
specific planning task, which heuristic to choose?
In this paper, we propose selective max — an online learning approach that combines the
strengths of several heuristic functions, leading to a speedup in optimal heuristic-search planning.
At a high level, selective max can be seen as a hyper-heuristic (Burke, Kendall, Newall, Hart, Ross,
& Schulenburg, 2003) — a heuristic for choosing among other heuristics. It is based on the seemingly trivial observation that, for each state, there is one heuristic which is the “best” for that state.
In principle, it is possible to compute several heuristics for each state, and then choose one according to the values they provide. However, heuristic computation in domain-independent planning is
typically expensive, and thus computing several heuristic estimates for each state takes a long time.
Selective max works by predicting for each state which heuristic will yield the “best” heuristic
estimate, and computes only that heuristic.
As it is not always clear how to decide what the “best” heuristic for each state is, we first
analyze an idealized model of a search space and describe how to choose there the best heuristic for
each state in order to minimize the overall search time. We then describe an online active learning
procedure that uses a decision rule formulated for the idealized model. This procedure constitutes
the essence of selective max.
Our experimental evaluation, which we conducted using three state-of-the-art heuristics for
domain-independent planning, shows that selective max is very effective in combining several
heuristics in optimal search. Furthermore, the results show that using selective max results in a
speedup over the baseline heuristic combination method, and that selective max is robust to different parameter settings. These claims are further supported by selective max having been a runnerup ex-aequo in the last International Planning Competition, IPC-2011 (Garcı́a-Olaya, Jiménez, &
Linares López, 2011).
This paper expands on the conference version (Domshlak, Karpas, & Markovitch, 2010) in
several ways. First, we improve and expand the presentation of the selective max decision rule.
Second, we explain how to handle non-uniform action costs in a principled way. Third, the empirical
evaluation is greatly extended, and now includes the results from IPC-2011, as well as controlled
experiments with three different heuristics, and an exploration of how the parameters of selective
max affect its performance.

2. Previous Work
Selective max is a speedup learning system. In general, speedup learning is concerned with improving the performance of a problem solving system with experience. The computational difficulty of
domain-independent planning has led many researchers to use speedup learning techniques in order
to improve the performance of planning systems; for a survey of many of these, see the work of
Minton (1994), Zimmerman and Kambhampati (2003), and Fern, Khardon, and Tadepalli (2011).
710

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

Speedup learning systems can be divided along several dimensions (Zimmerman & Kambhampati, 2003; Fern, 2010). Arguably the most important dimension is the phase in which learning takes
place. An offline, or inter-problem, speedup learner analyzes the problem solver’s performance on
different problem instances in an attempt to formulate some rule which would not only improve this
performance but would also generalize well to future problem instances. Offline learning has been
applied extensively to domain-independent planning, with varying degrees of success (Fern et al.,
2011). However, one major drawback of offline learning is the need for training examples — in our
case, planning tasks from the domains of interest.
Learning can also take place online, during problem solving. An online, or intra-problem,
speedup learner is invoked by the problem solver on a concrete problem instance the solver is
working on, and it attempts to learn online, with the objective of improving the solver’s performance
on that specific problem instance being solved. In general, online learners are not assumed to be pretrained on some other, previously seen problem instances; all the information they can rely on has to
be collected during the process of solving the concrete problem instance they were called for. Online
learning has been shown to be extremely helpful in propositional satisfiability (SAT) and general
constraint satisfaction (CSP) solving, where nogood learning and clause learning are now among
the essential components of any state-of-the-art solver (Schiex & Verfaillie, 1993; Marques-Silva
& Sakallah, 1996; Bayardo Jr. & Schrag, 1997). Thus, indirectly, SAT- and CSP-based domainindependent planners already benefit from these online learning techniques (Kautz & Selman, 1992;
Rintanen, Heljanko, & Niemelä, 2006). However, to the best of our knowledge, our work is the first
application of online learning to optimal heuristic-search planning.

3. Background
A domain-independent planning task (or planning task, for short) consists of a description of an
initial state, a goal, and a set of available operators. Several formalisms for describing planning tasks
are in use, including STRIPS (Fikes & Nilsson, 1971), ADL (Pednault, 1989), and SAS+ (Bäckström
& Klein, 1991; Bäckström & Nebel, 1995). We describe the SAS+ formalism, the one used by
the Fast Downward planner (Helmert, 2006), on top of which we have implemented and evaluated
selective max. Nothing, however, precludes using selective max in the context of other formalisms.
A SAS+ planning task is given by a 4-tuple Π = hV, A, s0 , Gi. V = {v1 , . . . , vn } is a set of state
variables, each associated with a finite domain dom(vi ). A complete assignment s to V is called a
state. s0 is a specified state called the initial state, and the goal G is a partial assignment to V . A is
a finite set of actions. Each action a is given by a pair hpre(a), eff(a)i of partial assignments to V
called preconditions and effects, respectively. Each action a also has an associated cost C(a) ∈ R0+ .
An action a is applicable in a state s iff s |= pre(a). Applying a changes the value of each state
variable v to eff(a)[v] if eff(a)[v] is specified. The resulting state is denoted by sJaK. We denote
the state obtained from sequential application of the (respectively applicable) actions a1 , . . . , ak
starting at state s by sJha1 , . . . , ak iK. Such an action sequence is a plan if s0 Jha1 , . . . , ak iK |= G.
In optimal planning, we are interested in finding one of
Pthe cheapest plans, where the cost of a plan
ha1 , . . . , ak i is the sum of its constituent action costs ki=1 C(ai ).
A SAS+ planning task Π = hV, A, s0 , Gi can be easily seen as a state-space search problem
whose states are simply complete assignments to the variables V , with transitions uniquely determined by the actions A. The initial and goal states are also defined by the initial state and goal of Π.
An optimal solution for a state-space search problem can be found by using the A∗ search algorithm
711

D OMSHLAK , K ARPAS , & M ARKOVITCH

with an admissible heuristic h. A heuristic evaluation function h assigns an estimate of the distance
to the closest goal state from each state it evaluates. The length of a cheapest path from state s to the
goal is denoted by h∗ (s), and h is called admissible if it never overestimates the true goal distance
— that is, if h(s) ≤ h∗ (s) for any state s. A∗ works by expanding states in the order of increasing
f (s) := g(s) + h(s), where g(s) is the cost of the cheapest path from the initial state to s known so
far.

4. Selective Max as a Decision Rule
Many admissible heuristics have been proposed for domain-independent planning; these vary from
cheap to compute yet not very accurate, to more accurate yet expensive to compute. In general,
the more accurate a heuristic is, the fewer states would be expanded by A∗ when using it. As the
accuracy of heuristic functions varies for different planning tasks, and even for different states of
the same task, we may be able to produce a more robust optimal planner by combining several admissible heuristics. Presumably, each heuristic is more accurate, that is, provides higher estimates,
in different regions of the search space. The simplest and best-known way for doing that is using the point-wise maximum of the heuristics in use at each state. Given n admissible heuristics,
h1 , . . . , hn , a new heuristic, maxh , is defined by maxh (s) := max1≤i≤n hi (s). It is easy to see that
maxh (s) ≥ hi (s) for any state s and for any heuristic hi . Thus A∗ search using maxh is expected to
expand fewer states than A∗ using any individual heuristic.PHowever, if we denote the time needed
to compute hi by ti , the time needed to compute maxh is ni=1 ti .
As mentioned previously, selective max is a form of hyper-heuristic (Burke et al., 2003) that
chooses which heuristic to compute at each state. We can view selective max as a decision rule dr,
which is given a set of heuristics h1 , . . . , hn and a state s, and chooses which heuristic to compute
for that state. One natural candidate for such a decision rule is the heuristic which yields the highest,
that is, most accurate, estimate:
drmax ({h1 , . . . , hn }, s) := hargmax1≤i≤n hi (s) .
Using this decision rule yields a heuristic which is as accurate as maxh , while still computing only
one heuristic per state — in time targmax1≤i≤n hi (s) .
This analysis, however, does not take into account the different computation times of the different heuristics. For instance, let h1 and h2 be a pair of admissible heuristics such that h2 ≥ h1 .
A priori, it seems that using h2 should always be preferred to using h1 because the former should
cause A∗ to expand fewer states. However, suppose that on a given planning task, A∗ expands 1000
states when guided by h1 and only 100 states when guided by h2 . If computing h1 for each state
takes 10 ms, and computing h2 for each state takes 1000 ms, then switching from h1 to h2 increases
the overall search time. Using maxh over h1 and h2 only makes things worse, because h2 ≥ h1 ,
and thus computing the maximum simply wastes the time spent on computing h1 . It is possible,
however, that computing h2 for a few carefully chosen states, and computing h1 for all other states,
would result in expanding 100 states, while reducing the overall search time when compared to
running A∗ with only h2 .
As this example shows, even given knowledge of the heuristics’ estimates in advance, it is not
clear what heuristic should be computed at each state when our objective is to minimize the overall
search time. Therefore, we begin by formulating a decision rule for choosing between one of two
heuristics, with respect to an idealized state-space model. Selective max then operates as an online
712

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

s0

s
f2 = c∗

f1 = c∗
sg

Figure 1: An illustration of the idealized search space model and the f -contours of two admissible
heuristics

active learning procedure, attempting to predict the outcome of that decision rule and choose which
heuristic to compute at each state.
4.1 Decision Rule with Perfect Knowledge
We now formulate a decision rule for choosing which of two given admissible heuristics, h1 and h2 ,
to compute for each state in an idealized search space model. In order to formulate such a decision
rule, we make the following assumptions:
• The search space is a tree with a single goal, constant branching factor b, and uniform cost
actions. Such an idealized search space model was used in the past to analyze the behavior of
A∗ (Pearl, 1984).
• The time ti required for computing heuristic hi is independent of the state being evaluated;
w.l.o.g. we assume t2 ≥ t1 .
• The heuristics are consistent. A heuristic h is said to be consistent if it obeys the triangle
inequality: For any two states s, s0 , h(s) ≤ h(s0 ) + k(s, s0 ), where k(s, s0 ) is the optimal cost
of reaching s0 from s.
• We have: (i) perfect knowledge about the structure of the search tree, and in particular the
cost of the optimal solution c∗ , (ii) perfect knowledge about the heuristic estimates for each
state, and (iii) a perfect tie-breaking mechanism.
Obviously, none of the above assumptions holds in typical search problems, and later we examine
their individual influence on our framework.
Adopting the standard notation, let g(s) be the cost of the cheapest path from s0 to s. Defining
maxh (s) = max(h1 (s), h2 (s)), we then use the notation f1 (s) = g(s) + h1 (s), f2 (s) = g(s) +
h2 (s), and maxf (s) = g(s) + maxh (s). The A∗ algorithm with a consistent heuristic h expands
states in increasing order of f = g + h (Pearl, 1984). In particular, every state s with f (s) <
h∗ (I) = c∗ will surely be expanded by A∗ , and every state with f (s) > c∗ will surely not be
713

D OMSHLAK , K ARPAS , & M ARKOVITCH

expanded by A∗ . The states with f (s) = c∗ might or might not be expanded by A∗ , depending on
the tie-breaking rule being used. Under our perfect tie-breaking assumption, the only states with
f (s) = c∗ that will be expanded are those that lie along some optimal plan.
Let us consider the states satisfying f1 (s) = c∗ (the dotted line in Fig. 1) and those satisfying
f2 (s) = c∗ (the solid line in Fig. 1). The states above the f1 = c∗ and f2 = c∗ contours are those
that are surely expanded by A∗ with h1 and h2 , respectively. The states above both these contours
(the grid-marked region in Fig. 1), that is, the states SE = {s | maxf (s) < c∗ }, are those that are
surely expanded by A∗ using maxh (Pearl, 1984, Thm. 4, p. 79).
Under the objective of minimizing the search time, note that the optimal decision for any state
s ∈ SE is not to compute any heuristic at all, since all these states are surely expanded anyway.
Assuming that we still must choose one of the heuristics, we would choose to compute the cheaper
heuristic h1 . Another easy case is when f1 (s) ≥ c∗ . In these states, computing h1 (s) suffices to
ensure that s is not surely expanded, and using a perfect tie-breaking rule, s will not be expanded
unless it must be. Because h1 is also cheaper to compute than h2 , h1 should be preferred, regardless
of the heuristic estimate of h2 for state s.
Let us now consider the optimal decision for all other states, that is, those with f1 (s) < c∗ and
f2 (s) ≥ c∗ . In fact, it is enough to consider only the shallowest such states; in Figure 1, these are the
states on the part of the f2 = c∗ contour that separates between the grid-marked and line-marked
areas. Since f1 (s) and f2 (s) are based on the same g(s), we have h2 (s) > h1 (s), that is, h2 is
more accurate in state s than h1 . If we were interested solely in reducing state expansions, then h2
would obviously be the right heuristic to compute at s. However, for our objective of reducing the
actual search time, h2 may actually be the wrong choice because it might be much more expensive
to compute than h1 .
Let us consider the effects of each of our two alternatives. If we compute h2 (s), then s is
not surely expanded, because f2 (s) = c∗ , and thus whether or not A∗ expands s depends on tiebreaking. As before, we are assuming perfect tie-breaking, and thus s will not be expanded unless
it must be. Computing h2 would “cost” us t2 time.
In contrast, if we compute h1 (s), then s is surely expanded because f1 (s) < c∗ . Note that not
computing h2 for s and then computing h2 for one of the descendants s0 of s is clearly a sub-optimal
strategy as we do pay the cost of computing h2 , yet the pruning of A∗ is limited only to the search
sub-tree rooted in s0 . Therefore, our choices are really either computing h2 for s, or computing h1
for all the states in the sub-tree rooted in s that lie on the f1 = c∗ contour. Suppose we need to
expand l complete levels of the state space from s to reach the f1 = c∗ contour. Thus, we need to
generate an order of bl states, and then invest bl t1 time in calculating h1 for all these states that lie
on the f1 = c∗ contour.
Considering these two options, the optimal decision in state s is thus to compute h2 iff t2 < bl t1 ,
or to express it differently, if l > logb ( tt12 ). As a special case, if both heuristics take the same time to
compute, this decision rule reduces to l > 0, that is, the optimal choice is simply the more accurate
heuristic for state s.
Putting all of the above cases together yields the decision rule dropt , as below, with ls being the
depth to go from s until f1 (s) = c∗ :
714

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING



h1 , f1 (s) < c∗ , f2 (s) < c∗



h , f (s) ≥ c∗
1
1
.
dropt ({h1 , h2 }, s) :=

h1 , f1 (s) < c∗ , f2 (s) ≥ c∗ , ls ≤ logb ( tt12 )



h , f (s) < c∗ , f (s) ≥ c∗ , l > log ( t2 )
2
1
2
s
b t1
4.2 Decision Rule without Perfect Knowledge
The idealized model above makes several assumptions, some of which appear to be very problematic
to meet in practice. Here we examine these assumptions more closely, and when needed, suggest
pragmatic compromises.
First, the model assumes that the search space forms a tree with a single goal state, that the
heuristics in question are consistent, and that we have a perfect tie-breaking rule. Although the
first assumption does not hold in most planning tasks, the second assumption is not satisfied by
many state-of-the-art heuristics (Karpas & Domshlak, 2009; Helmert & Domshlak, 2009; Bonet
& Helmert, 2010), and the third assumption is not realistic, they do not prevent us from using the
decision rule suggested by the model.
The idealized model also assumes that both the branching factor and the heuristic computation
times are constant across the search states. In our application of the decision rule to planning
in practice, we deal with this assumption by adopting the average branching factor and heuristic
computation times, estimated from a random sample of search states.
Finally, the decision rule dropt above requires unrealistic knowledge of both heuristic estimates,
as well as of the optimal plan cost c∗ and the depth ls to go from state until f1 (s) = c∗ . As we
obviously do not have this knowledge in practice, we must use some approximation of the decision
rule.
The first approximation we make is to ignore the “trivial” cases that require knowledge of c∗ ;
these are the cases where either s is surely expanded, or h1 is enough to prune s. Instead, we apply
the reasoning for the “complicated” case for all states, resulting in the following decision rule:
(
h1 , ls ≤ logb ( tt12 )
drapp1 ({h1 , h2 }, s) :=
.
h2 , ls > logb ( tt21 )
The next step is to somehow estimate the “depth to go” ls — the number of layers we need to
expand in the tree until f1 reaches c∗ . In order to derive a useful decision rule, we assume that ls
has a positive correlation with ∆h (s) = h2 (s) − h1 (s); that is, if h1 and h2 are close, then ls is low,
and if h1 yields a much lower estimate than h2 , implying that h1 is not very accurate for s, then the
depth to go until f1 (s) = c∗ is large. Our approximation uses the simplest such correlation — a
linear one — between ∆h (s) and ls , with a hyper-parameter α for controlling the slope.
Recall that in our idealized model, all actions were unit cost, and thus cost-to-go and depthto-go are the same. However, some planning tasks, and notably, all planning tasks from the 2008
International Planning Competition, feature non-uniform action costs. Therefore, our decision rule
converts heuristic estimates of cost-to-go into heuristic estimates of depth-to-go by dividing the
cost-to-go estimate by the average action cost. We do this by modifying our estimate of the depthto-go, ls , with the average action cost, which we denote by ĉ. Plugging all of the above into our
715

D OMSHLAK , K ARPAS , & M ARKOVITCH

decision rule yields:
(
h1 ,
drapp2 ({h1 , h2 }, s) :=
h2 ,

∆h (s) ≤ α · ĉ · logb ( tt12 )
.
∆h (s) > α · ĉ · logb ( tt21 )

Given b, t1 , t2 , and ĉ, the quantity α · ĉ · logb (t2 /t1 ) becomes fixed, and in what follows we denote
it simply by threshold τ .
Note that linear correlation between ∆h (s) and ls occurs in some simple cases. The first such
case is when the h1 value remains constant in the subtree rooted at s, that is, the additive error of
h1 increases by 1 for each level below s. In this case, f1 increases by 1 for each expanded level of
the sub-tree (because h1 remains the same, and g increases by 1), and it will take expanding exactly
∆h (s) = h2 (s) − h1 (s) levels to reach the f1 = c∗ contour. The second such case is when the
absolute error of h1 remains constant, that is, h1 increases by 1 for each level expanded, and so f1
increases by 2. In this case, we will need to expand ∆h (s)/2 levels. This can be generalized to the
case where the estimate h1 increases by any constant additive factor c, which results in ∆h (s)/(c+1)
levels being expanded.
Furthermore, there is some empirical evidence to support our conclusion about exponential
growth of the search effort as a function of heuristic error, even when the assumptions made by the
model do not hold. In particular, the experiments of Helmert and Röger (2008) on IPC benchmarks
with heuristics with small constant additive errors show that the number of expanded nodes most
typically grows exponentially as the (still very small and additive) error increases.
Finally, we remark that because our decision rule always chooses an admissible heuristic, the
resulting heuristic estimate will always be admissible. Thus, even if the chosen heuristic is not the
“correct” one according to dropt , this will not result in loss of optimality of the solution, but only in
a possible increase in search time.

5. Online Learning of the Decision Rule
While decision rule drapp2 still requires knowledge of h1 and h2 , we can now use it as a binary
label for each state. We can compute the value of the decision rule by “paying” the computation
time of both heuristics, t1 + t2 , and, more importantly, we can use a binary classifier to predict the
value of this decision rule for some unknown state. Note that we use the classifier online, during the
problem solving process, and the time spent on learning and classification is counted as time spent
on problem solving. Furthermore, as in active learning, we can choose to “pay” for a label for some
state, where the payment is also in computation time. Therefore we refer to our setting as active
online learning.
In what follows, we provide a general overview of the selective max procedure, and describe
several alternatives for each of its components. Our decision rule states that the more expensive
heuristic h2 should be computed at a search state s when h2 (s) − h1 (s) > τ . This decision rule
serves as a binary target concept, which corresponds to the set of states where the more expensive
heuristic h2 is ”significantly” more accurate than the cheaper heuristic h1 — the states where, according to our model, the reduction in expanded states by computing h2 outweighs the extra time
needed to compute it. Selective max then uses a binary classifier to predict the value of the decision
rule. There are several steps to building the classifier:
716

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

evaluate(s)
hh, conf idencei := CLASSIFY(s, model)
if (conf idence > ρ) then
return h(s)
else
label := h1
if h2 (s) − h1 (s) > α · ĉ · logb (t2 /t1 ) then label := h2
update model with hs, labeli
return max(h1 (s), h2 (s))
Figure 2: The selective max state evaluation procedure
1. Training Example Collection: We first need to collect training examples, which should be
representative of the entire search space. Several state-space sampling methods are discussed
in Section 5.1.
2. Labeling Training Examples: After the training examples are collected, they are first used to
estimate the average branching factor b, average heuristic computation times t1 and t2 , and
the average action cost ĉ. Once b, t1 , t2 , and ĉ are estimated, we use them to estimate the
threshold τ = α · ĉ · logb (t2 /t1 ) for the decision rule.
We then generate a label for each training example by calculating ∆h (s) = h2 (s) − h1 (s),
and comparing it to the decision threshold: If ∆h (s) > τ , we label s with h2 , otherwise with
h1 . If t1 > t2 we simply switch between the heuristics — our decision is always whether or
not to compute the more expensive heuristic; the default is to compute the cheaper heuristic,
unless the classifier says otherwise.
3. Feature Extraction: Having obtained a set of training examples, we must decide about the
features to characterize each example. Since our target concept is based on heuristic values,
the features should represent the information that heuristics are derived from — typically the
problem description and the current state.
While several feature-construction techniques for characterizing states of planning tasks have
been proposed in previous literature (Yoon, Fern, & Givan, 2008; de la Rosa, Jiménez, &
Borrajo, 2008), they were all designed for inter-problem learning, that is, for learning from
different planning tasks which have already been solved offline. However, in our approach,
we are only concerned with one problem, in an online setting, and thus these techniques are
not applicable. In our implementation, we use the simplest features possible, taking each
state variable as a feature. As our empirical evaluation demonstrates, even these elementary
features suffice for selective max to perform well.
4. Learning: Once we have a set of labeled training examples, each represented by a vector of
features, we can train a binary classifier. Several different choices of classifier are discussed
in Section 5.2.
After completing the steps described above, we have a binary classifier that can be used to
predict the value of our decision rule. However, as the classifier is not likely to have perfect accuracy,
717

D OMSHLAK , K ARPAS , & M ARKOVITCH

we further consult the confidence the classifier associates with its classification. The resulting state
evaluation procedure of selective max is depicted in Figure 2. For every state s evaluated by the
search algorithm, we use our classifier to decide which heuristic to compute. If the classification
confidence exceeds a confidence threshold ρ, a parameter of selective max, then only the indicated
heuristic is computed for s. Otherwise, we conclude that there is not enough information to make
a selective decision for s, and compute the regular maximum over h1 (s) and h2 (s). However, we
use this opportunity to improve the quality of our prediction for states similar to s, and update our
classifier by generating a label based on h2 (s)−h1 (s) and learning from the newly labeled example.
These decisions to dedicate computation time to obtain a label for a new example constitute the
active part of our learning procedure. It is also possible to update the estimates for b, t1 , t2 , and ĉ,
and change the threshold τ accordingly. However, this would result in the concept we are trying
to learn constantly changing — a phenomenon known as concept drift — which usually affects
learning adversely. Therefore, we do not update the threshold τ .
5.1 State-Space Sampling
The initial state-space sample serves two purposes. First, it is used to estimate the branching factor
b, the heuristic computation times t1 and t2 , the average action cost ĉ, and then to compute the
threshold τ = α · ĉ · logb (t2 /t1 ), which is used to specify our concept. After the concept is specified,
the state-space sample also provides us with a set of examples on which the classifier is initially
trained. Therefore, it is important to have an initial state-space sample that is representative of the
states which will be evaluated during search. The number of states in the initial sample is controlled
by a parameter N .
One option is to use the first N states of the search. However, this method is biased towards
states closer to the initial state, and therefore is not likely to represent the search space well. Thus,
we discuss three more sophisticated state-space sampling procedures, all of which are based on
performing random walks, or “probes,” from the initial state. While the details of these sampling
procedures vary, each such “probe” terminates at some pre-set depth limit.
The first sampling procedure, which we refer to as “biased probes,” uses an inverse heuristic
selection bias for choosing the next state to go to in the probe. Specifically, the probability of
choosing state s as the successor from which the random walk will continue is proportional to
1/ maxh (s). This biases the sample towards states with lower heuristic estimates, which are more
likely to be expanded during the search.
The second sampling procedure is similar to the first one, except that it chooses the successor
uniformly, and thus we refer to it as “unbiased probes.” Both these sampling procedures add all
of the generated states (that is, the states along the probe as well as their “siblings”) to the statespace sample, and they both terminate after collecting N training examples. The depth limit for all
random walks is the same in both sampling schemes, and is set to some estimate of the goal depth;
we discuss this goal depth estimate later.
The third state-space sampling procedure, referred to here as PDB sampling, has been proposed
by Haslum, Botea, Helmert, Bonet, and Koenig (2007). This procedure also uses unbiased probes,
but only adds the last state reached in each probe to the state-space sample. The depth of each
probe is determined individually, by drawing a random depth from a binomial distribution around
the estimated goal depth.
718

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

Note that all three sampling procedures rely on some estimate of the minimum goal depth.
When all actions are unit cost, the minimum goal depth is the same as h∗ (s0 ), and thus we can use
a heuristic to estimate it. In our evaluation, we used twice the heuristic estimate of the initial state,
2 · maxh (s0 ), as the goal depth estimate. However, with non-uniform action costs, goal depth and
cost are no longer measured in the same units. While it seems we could divide the above heuristicbased estimate by the average action cost ĉ, recall that we use the state-space sample in order to
obtain an estimate for estimate ĉ, thus creating a circular dependency. Although it is possible to
estimate ĉ by taking the average cost of all actions in the problem description, there is no reason
to assume that all actions are equally likely to be used. Another option is to modify the above
state-space sampling procedures, and place a cost limit, rather than a depth limit, on each probe.
However, this would pose a problem in the presence of 0-cost actions. In such a case, when a probe
reaches its cost limit yet has a possible 0-cost action to apply, it is not clear whether the probe
should terminate. Therefore, we keep using depth-limited probes and attempt to estimate the depth
of the cheapest goal. We compute a heuristic estimate for the initial state, and then use the number
of actions which the heuristic estimate is based on as our goal depth estimate. While this is not
possible with every heuristic, we use in our empirical evaluation the monotonically-relaxed plan
heuristic. This heuristic, also known as the FF heuristic (Hoffmann & Nebel, 2001), does provide
such information: we first use this heuristic to find a relaxed plan from the initial state, and then use
the number of actions in the relaxed plan as our goal depth estimate.
5.2 Classifier
The last decision to be made is the choice of classifier. Although many classifiers can be used here,
several requirements must be met due to our particular setup. First, both training and classification must be very fast, as both are performed during time-constrained problem solving. Second,
the classifier must be incremental to support active learning. This is achieved by allowing online
updates of the learned model. Finally, the classifier should provide us with a meaningful measure
of confidence for its predictions.
While several classifiers meet these requirements, we found the Naive Bayes classifier to provide
a good balance between speed and accuracy. One note on the Naive Bayes classifier is that it
assumes a very strong conditional independence between the features. Although this is not a fully
realistic assumption for planning tasks, using a SAS+ task formulation in contrast to the classical
STRIPS formulations helps a lot: instead of many highly dependent binary variables, we have a
much smaller set of less dependent ones.
Although, as the empirical evaluation will demonstrate, Naive Bayes appears to be the most
suitable classifier to use with selective max, other classifiers can also be used. The most obvious
choice for a replacement classifier would be a different Bayesian classifier. One such classifier is
AODE (Webb, Boughton, & Wang, 2005), an extension of Naive Bayes, which somewhat relaxes
the assumption of independence between the features, and is typically more accurate than Naive
Bayes. However, this added accuracy comes at the cost of increased training and classification time.
Decision trees are another popular type of classifier that allows for even faster classification.
While most decision tree induction algorithms are not incremental, the Incremental Tree Inducer
(ITI) algorithm (Utgoff, Berkman, & Clouse, 1997) supports incremental updating of decision trees
by tree restructuring, and also has a freely available implementation in C. In our evaluation, we used
ITI in incremental mode, and incorporated every example into the tree immediately, because the
719

D OMSHLAK , K ARPAS , & M ARKOVITCH

tree is likely to be used for many classifications between pairs of consecutive updates with training
examples from active learning. The classification confidence with the ITI classifier is obtained by
the frequency of examples at the leaf node from which the classification came.
A different family of possible classifiers is k-Nearest Neighbors (kNN) (Cover & Hart, 1967).
In order to use kNN, we need a distance metric between examples, which, with our features, are
simply states. As with our choice of features, we opt for simplicity and use Euclidean distance
as our metric. kNN enjoys very fast learning time but suffers from slow classification time. The
classification confidence is obtained by a simple (unweighted) vote between the k nearest neighbors.
Another question related to the choice of classifier is feature selection. In some planning tasks,
the number of variables, and accordingly, features, can be over 2000 (for example, task 35 of the
AIRPORT domain has 2558 variables). While the performance of Naive Bayes and kNN can likely be
improved using feature selection, doing so poses a problem when the initial sample is considered.
Since feature selection will have to be done right after the initial sample is obtained, it will have to
be based only on the initial sample. This could cause a problem since some features might appear to
be irrelevant according to the initial sample, yet turn out to be very relevant when active learning is
used after some low-confidence states are encountered. Therefore, we do not use feature selection
in our empirical evaluation of selective max.
5.3 Extension to Multiple Heuristics
To this point, we have discussed how to choose which heuristic to compute for each state when
there are only two heuristics to choose from. When given more than two heuristics, the decision
rule presented in Section 4 is inapplicable, and extending it to handle more than two heuristics is
not straightforward. However, extending selective max to use more than two heuristics is straightforward — simply compare heuristics in a pair-wise manner, and use a voting rule to choose which
heuristic to compute.
While there are many possible such voting rules, we go with the simplest one, which compares
every pair of heuristics, and chooses the winner by a vote, weighted by the confidence for each pairwise decision. The overall winner is simply the heuristic which has the highest total confidence from
all pairwise comparisons, with ties broken in favor of the cheaper-to-compute heuristic. Although
this requires a quadratic number of classifiers, training and classification time (at least with Naive
Bayes) appear to be much lower than the overall time spent on heuristic computations, and thus
the overhead induced by learning and classification is likely to remain relatively low for reasonable
heuristic ensembles.

6. Experimental Evaluation
To evaluate selective max empirically, we implemented it on top of the open-source Fast Downward
planner (Helmert, 2006). Our empirical evaluation is divided into three parts. First, we examine the performance of selective max using the last International Planning Competition, IPC-2011,
as our benchmark. Selective max was the runner-up ex-aequo at IPC-2011, tying for 2nd place
with a version of Fast Downward using an abstraction “merge-and-shrink” heuristic (Nissim, Hoffmann, & Helmert, 2011), and losing to a sequential portfolio combining the heuristics used in both
runners-up (Helmert, Röger, & Karpas, 2011). Second, we present a series of controlled parametric
experiments, where we examine the behavior of selective max under different settings. Finally, we
720

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

Parameter
α
ρ
N
Sampling method
Classifier

Default value
1
0.6
1000
Biased probes
Naive Bayes

Meaning
heuristic difference bias
confidence threshold
initial sample size
state-space sampling method
classifier type

Table 1: Parameters for the selmax entry in IPC-2011.
compare selective max to a simulated sequential portfolio, using the same heuristics as selective
max.
6.1 Performance Evaluation: Results from IPC-2011
The IPC-2011 experiments (Garcı́a-Olaya et al., 2011) were run by the IPC organizers, on their
own machines, with a time limit of 30 minutes and a memory limit of 6 GB per planning task.
The competition included some new domains, which none of the participants had seen before, thus
precluding the participants from using offline learning approaches.
Although many planners participated in the sequential optimal track of IPC-2011, we report here
only the results relevant to selective max. The selective max entry in IPC-2011 was called selmax,
and consisted of selective max over the uniform action cost partitioning version of hLA (Karpas &
Domshlak, 2009) and hLM-CUT (Helmert & Domshlak, 2009) heuristics. The parameters used for
selective max in IPC-2011 are reported in Table 1. Additionally, each of the heuristics selmax used
was entered individually as BJOLP (hLA ) and lmcut (hLM-CUT ), and we report results for all three
planners. While a comparison of selective max with the regular maximum of hLA and hLM-CUT
would be interesting, there was no such entry at IPC-2011, and thus we can not report on it. In our
controlled experiments, we do compare selective max to the regular maximum, as well as to other
baseline combination methods.
Figure 3 shows the anytime profile of these three planners on IPC-2011 tasks, plotting the number of tasks solved under different timeouts, up to the time limit of 30 minutes. Additionally, Table
2 shows the number of tasks solved in each domain of IPC-2011, after 30 minutes, and includes the
number of problems solved by the winner, Fast Downward Stone Soup 1 (FDSS-1), for reference.
As these results show, selective max solves more problems than each of the individual heuristics
it uses. Furthermore, the anytime profile of selective max dominates each of these heuristics, in the
range between 214 seconds until the full 30 minute timeout. The behavior of the anytime plot with
shorter timeouts is due to the overhead of selective max, which consists of obtaining the initial statespace sample, as well as learning and classification. However, it appears that selective max quickly
compensates for its relatively slow start.
6.2 Controlled Experiments
In our series of controlled experiments, we attempted to evaluate the impact of different parameters
on selective max. We controlled the following independent variables:
• Heuristics: We used three state-of-the-art admissible heuristics: hLA (Karpas & Domshlak,
2009), hLM-CUT (Helmert & Domshlak, 2009), and hLM-CUT+ (Bonet & Helmert, 2010). None
721

D OMSHLAK , K ARPAS , & M ARKOVITCH

160

Solved Instances

140

120

100

80

BJOLP
lmcut
selmax

60
0

200

400

600

800
1000
Timeout (seconds)

1200

1400

1600

1800

Figure 3: IPC-2011 anytime performance. Each line shows the number of problems from IPC-2011
solved by the BJOLP, lmcut, and selmax planners, respectively, under different timeouts.
Domain
barman
elevators
floortile
nomystery
openstacks
parcprinter
parking
pegsol
scanalyzer
sokoban
tidybot
transport
visitall
woodworking
TOTAL

BJOLP
4
14
2
20
14
11
3
17
6
20
14
7
10
9
151

lmcut
4
18
7
15
16
13
2
18
12
20
14
6
10
12
167

selmax
4
18
7
20
14
13
4
17
10
20
14
6
10
12
169

FDSS-1
4
18
7
20
16
14
7
19
14
20
14
7
13
12
185

Table 2: Number of planning tasks solved at IPC 2011 in each domain by the BJOLP, lmcut, and
selmax planners. The best result from these 3 planners is in bold. The number of problems
solved by Fast Downward Stone Soup 1 (FDSS-1) in each domain is also included for
reference.

722

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

of these base heuristics yields better search performance than the others across all planning
domains. Of these heuristics, hLA is typically the fastest to compute and the least accurate,
hLM-CUT is more expensive to compute and more accurate, and hLM-CUT+ is the most expensive to compute and the most accurate.1 From the data we have gathered in these experiments,
hLM-CUT takes on average 4.5 more time per state than hLA , and hLM-CUT+ takes 53 more time
per state than hLA . We evaluate selective max with all possible subsets of two or more of these
three heuristics.
While there are other admissible heuristics for SAS+ planning that are competitive with the
three above (for example, Helmert, Haslum, & Hoffmann, 2007; Nissim et al., 2011; Katz &
Domshlak, 2010), they are based on expensive offline preprocessing, followed by very fast
online per-state computation. In contrast, hLA , hLM-CUT and hLM-CUT+ perform most of their
computation online, and thus can be better exploited by selective max.
Additionally, we empirically examine the effectiveness of selective max in deciding whether
to compute a heuristic value at all. This is done by combining our most accurate heuristic,
hLM-CUT+ , with the blind heuristic.
• Heuristic difference bias α: The hyper-parameter α controls the tradeoff between computation time and heuristic accuracy. Setting α = 0 sets the threshold τ to 0, forcing the decision
rule to always choose the more accurate heuristic. Increasing α increases the threshold, forcing the decision rule to choose the more accurate heuristic h2 only if its value is much higher
than that of h1 . We evaluate selective max with values for α of 0.1, 0.5, 1, 1.5, 2, 3, 4, and 5.
• Confidence threshold ρ: The confidence threshold ρ controls the active learning part of selective max. Setting ρ = 0.5 turns off active learning completely, because the chosen heuristic
always comes with a confidence of at least 0.5. Setting ρ = 1 would mean using active learning almost always, essentially reducing selective max to regular point-wise maximization. We
evaluate selective max with values for ρ of 0.51, 0.6, 0.7, 0.8, 0.9, and 0.99.
• Initial sample size N : The initial sample size N is an important parameter, not just because it
is used to train the initial classifier before any active learning is done, but also because it is the
only source of estimates for branching factor, average action cost, and heuristic computation
times. It thus affects the threshold τ : Increasing N increases the accuracy of the initial
classifier and of the various aforementioned estimates, but also increases the preprocessing
time. We evaluate selective max with values for N of 10, 100, and 1000.
• Sampling method: The sampling method used to obtain the initial state-space sample is important in that it affects this initial sample, and thus the accuracy of both the threshold τ and
of the initial classifier. We evaluate selective max with three different sampling methods, all
P
described in Section 5.1: biased probes (selPh ), unbiased probes (selU
h ), and the sampling
method of Haslum et al. (2007) (selPDB
h ).
• Classifier: The choice of classifier is also very important. The Naive Bayes classifier comB
bines very fast learning and classification (selN
h ). A more sophisticated variant of Naive
Bayes called AODE (Webb et al., 2005) is also considered here (selAODE
). AODE is more
h
1. Of course, all three heuristics are computable in polynomial time from the SAS+ description of the planning task.

723

D OMSHLAK , K ARPAS , & M ARKOVITCH

Parameter
Heuristics
α
ρ
N
Sampling method
Classifier

Default value
hLA / hLM-CUT
1
0.6
100
PDB (Haslum et al., 2007)
Naive Bayes

Meaning
heuristics used
heuristic difference bias
confidence threshold
initial sample size
state-space sampling method
classifier type

Table 3: Default parameters for selh .
accurate than Naive Bayes, but has higher classification and learning times, as well as increased memory overhead. Another possible choice is using incremental decision trees (Utgoff et al., 1997), which offer even faster classification, but more expensive learning when the
I
tree structure needs to be changed (selIT
h ). We also consider kNN classifiers (Cover & Hart,
1967), which offer faster learning than Naive Bayes, but usually more expensive classificaN
tion, especially as k grows larger (selkN
, for k = 3, 5).
h
Table 3 describes our default values for each of these independent variables. In each of the
subsequent experiments, we vary one of these independent variables, keeping the rest at their default
values. In all of these experiments, the search for each planning task instance was limited to 30
minutes2 and to 3 GB of memory. The search times do not include the time needed for translating
the planning task from PDDL to SAS+ and building some of the Fast Downward data structures,
which is common to all planners, and is tangential to the issues considered in our study. The search
times do include learning and classification time for selective max.
• Heuristics
We begin by varying the set of heuristics in use. For every possible choice of two or more
heuristics out of the uniform action cost partitioning version of hLA (which we simply refer
to as hLA ), hLM-CUT and hLM-CUT+ , we compare selective max to other methods of heuristic
combination, as well as to the individual heuristics. We compare selective max (selh ) to the
regular maximum (maxh ), as well as to a planner which chooses which heuristic to compute
at each state randomly (rndh ). As it is not clear whether the random choice should favor the
more expensive and accurate heuristic or the cheaper and less accurate one, we simply use a
uniform random choice.
This experiment was conducted on all 31 domains with no conditional effects and axioms
(which none of the heuristics we used support) from the International Planning Competitions
1998–2008. Because domains vary in difficulty and in the number of tasks, we normalize
the score for each planner in each domain between 0 and 1. Normalizing by the number of
problems in the domain is not a good idea, as it is always possible to generate any number
of effectively unsolvable problems in each domain, so that the fraction of solved problems
will approach zero. Therefore, we normalize the number of problems solved in each domain
by the number of problems in that domain that were solved by at least one of our planners.
While this measure of normalized coverage has the undesirable property that introducing a
2. Each search was given a single core of a 3GHz Intel E8400 CPU machine.

724

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

Heuristic

hLA

hLM-CUT

hLM-CUT+

High variance unit cost
Low variance unit cost
Non-uniform cost

0.89 (175)
0.98 (345)
0.80 (136)

0.83 (136)
0.96 (343)
0.94 (160)

0.81 (132)
0.94 (336)
0.86 (146)

TOTAL

0.91 (656)

0.92 (639)

0.89 (614)

(a) Individual Heuristics
Domains
High variance unit cost
Low variance unit cost
Non-uniform cost
TOTAL

maxh
0.90 (164)
0.97 (345)
0.92 (156)
0.94 (665)

rndh
0.74 (123)
0.95 (342)
0.79 (138)
0.85 (603)

selh
0.93 (174)
0.97 (346)
0.93 (157)
0.95 (677)

hLA / hLM-CUT+

High variance unit cost
Low variance unit cost
Non-uniform cost
TOTAL

0.84 (149)
0.93 (335)
0.85 (144)
0.89 (628)

0.68 (115)
0.88 (327)
0.71 (122)
0.78 (564)

0.90 (164)
0.96 (342)
0.86 (145)
0.92 (651)

hLM-CUT / hLM-CUT+

High variance unit cost
Low variance unit cost
Non-uniform cost
TOTAL

0.80 (131)
0.94 (336)
0.87 (147)
0.89 (614)

0.75 (122)
0.93 (335)
0.86 (145)
0.87 (602)

0.80 (130)
0.97 (344)
0.93 (156)
0.91 (630)

hLA / hLM-CUT / hLM-CUT+

High variance unit cost
Low variance unit cost
Non-uniform cost
TOTAL

0.84 (149)
0.93 (335)
0.85 (144)
0.89 (628)

0.69 (116)
0.90 (332)
0.75 (130)
0.81 (578)

0.87 (154)
0.97 (345)
0.89 (150)
0.92 (649)

Heuristics
hLA / hLM-CUT

(b) Combinations of two or more heuristics

Table 4: Average normalized coverage, and total coverage in parentheses, broken down by groups
of domains with unit cost actions and high variance in coverage, domains with unit cost
actions and low variance in coverage, and domains with non-uniform action costs. Table
(a) shows the results for A∗ with individual heuristics, and table (b) shows the results for
the maximum (maxh ), random choice (rndh ), and selective max (selh ) combinations of
the set of heuristics listed in each major row.

new planner could change the normalized coverage of the other planners, we believe that
it best reflects performance nonetheless. As an overall performance measure, we list the
average normalized coverage score across all domains. Using normalized coverage means
that domains have equal weight in the aggregate score. Additionally, we list for each domain
the number of problems that were solved by any planner (in parentheses next to the domain
name), and for each planner we list the number of problems it solved in parentheses.
Tables 4 and 5 summarize the results of this experiment. We divided the domains in our
experiment into 3 sets: domains with non-uniform action costs, domains with unit action
costs which exhibited a high variance in the number of problems solved between different
725

D OMSHLAK , K ARPAS , & M ARKOVITCH

Heuristics

Domains

hLA

hLM-CUT

hLA / hLM-CUT

High variance unit cost
Low variance unit cost
Non-uniform cost
TOTAL

3.23
3.48
13.23
4.82

2.8
1.14
1.01
1.4

hLA / hLM-CUT+

High variance unit cost
Low variance unit cost
Non-uniform cost
TOTAL

4.01
4.55
13.66
5.85

hLM-CUT / hLM-CUT+

High variance unit cost
Low variance unit cost
Non-uniform cost
TOTAL

hLA / hLM-CUT / hLM-CUT+

hLM-CUT+

maxh

rndh

selh

1.0
1.0
1.0
1.0

3.88
2.14
3.99
2.93

1.46
1.2
1.17
1.25

1.77
1.01
1.0
1.16

1.0
1.0
1.0
1.0

3.17
2.38
3.85
2.9

2.16
1.85
1.72
1.89

2.29
1.58
1.32
1.66

1.01
1.01
1.03
1.01

1.0
1.0
1.0
1.0

1.7
1.29
1.18
1.35

1.24
1.19
1.16
1.2

High variance unit cost
Low variance unit cost
Non-uniform cost

4.06
4.65
15.2

3.81
1.59
1.37

1.78
1.02
1.03

1.0
1.0
1.0

3.61
2.05
2.74

2.1
1.57
1.49

TOTAL

6.1

1.91

1.18

1.0

2.56

1.67

Table 5: Geometric mean of ratio of expansions relative to maxh , broken down by groups of domains with unit cost actions and high variance in coverage, domains with unit cost actions
and low variance in coverage, and domains with non-uniform action costs.

planners, and domains with unit action costs which exhibited a low variance in the number of
problems solved between different planners. We make this distinction because we conducted
the following experiments, which examine the effects of the other parameters of selective
max, only on the unit cost action domains which exhibited high variance. Tables 4 and 5
summarize the results for these three sets of domains, as well as for all domains combined.
Detailed, per-domain results are relegated to Appendix A.
Table 4 lists the normalized coverage score, averaged across all domains, and the total number
of problems solved in parentheses. Table 4a lists these for each individual heuristic, and
Table 4b for every combination method of every set of two or more heuristics. Table 5 shows
how accurate each of these heuristic combination methods is. Since, for a given set of base
heuristics, maxh is the most accurate heuristic possible, the accuracy is evaluated relative to
maxh . We evaluate each heuristic’s accuracy on each task as the number of states expanded
by A∗ using that heuristic, divided by the number of states expanded by A∗ using maxh . We
compute the geometric mean for each domain over the tasks solved by all planners of this
“accuracy ratio,” and list here the geometric mean over these numbers. Each row lists the
results for a combination of two or three heuristics; for combinations of two heuristics, we
leave the cell representing the heuristic that is not in the combination empty.
Looking at the results of individual heuristics first, we see that the most accurate heuristic
(hLM-CUT+ ) does not do well overall, while the least accurate heuristic (hLA ) solved the most
tasks in total, and hLM-CUT wins in terms of normalized coverage. However, when looking at
the results for individual domains, we see that the best heuristic to use varies, indicating that
combining different heuristics could indeed be of practical value.
We now turn our attention to the empirical results for the combinations of all possible subsets
of two or more heuristics. The results clearly demonstrate that when more than one heuristic
is used, selective max is always better than regular maximum or random choice, both in terms
of normalized coverage and absolute number of problems solved. Furthermore, the poor
performance of rndh , in both coverage and accuracy, demonstrates that the decision rule and
726

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

700

650

Solved Instances

600

550

500

450

400
maxh

350

rndh
selh

300
200

400

600

800
1000
Timeout (seconds)

1200

1400

1600

1800

Figure 4: hLA / hLM-CUT / hLM-CUT+ anytime profile. Each line shows the number of problems from
IPC 1998 – 2006 solved by the maximum (maxh ), random choice (rndh ), and selective
max (selh ) combination methods of the hLA , hLM-CUT , and hLM-CUT+ heuristics, under
different timeouts.

the classifier used in selective max are important to its success, and that computing only one
heuristic at each state randomly is insufficient, to say the least.
When compared to individual heuristics, selective max does at least as well as each of the
individual heuristics it uses, for all combinations except that of hLM-CUT and hLM-CUT+ . This
is most likely because hLM-CUT and hLM-CUT+ are based on a very similar procedure, and
thus their heuristic estimates are highly correlated. To see why this hinders selective max,
consider the extreme case of two heuristics which have a correlation of 1.0 (that is, yield the
same heuristic values), where selective max can offer no benefit. Finally, we remark that the
best planner in this experiment was the selective max combination of hLA and hLM-CUT .
The above results are all based on a 30 minute time limit, which, while commonly used in the
IPC, is arbitrary, and the number of tasks solved after 30 minutes does not tell the complete
tale. Here, we examine the anytime profile of the different heuristic combination methods, by
plotting the number of tasks solved under different timeouts, up to a timeout of 30 minutes.
Figure 4 shows this plot for the three combination methods when all three heuristics are used.
As the figure shows, the advantage of selh over the baseline combination methods is even
greater under shorter timeouts. This indicates that the advantage of selh over maxh is even
727

D OMSHLAK , K ARPAS , & M ARKOVITCH

Heuristics
hLA / hLM-CUT

Overhead
12%

hLA / hLM-CUT+

15%

hLM-CUT / hLM-CUT+

9%

hLA / hLM-CUT / hLM-CUT+

10%

Table 6: Selective max overhead. Each row lists the average percentage of time spent on learning
and classification, out of the total time taken by selective max, for each set of heuristics.

greater than is evident from the results after 30 minutes, and that selh is indeed effective for
minimizing search time. Since the anytime plots for the combinations of pairs of heuristics
are very similar, we omit them here for the sake of brevity.
Finally, we present overhead statistics for using selective max — the proportion of time spent
on learning and classification, including the time spent obtaining the initial state-space sample, out of the total solution time. Table 6 presents the average overhead on selective max
for each of the combinations of two or more heuristics. Detailed, per-domain results are
presented in Table 18 in Appendix A. As these results show, selective max does incur a noticeable overhead, but it is still relatively low. It is also worth mentioning that the overhead
varies significantly between different domains.
We also performed an empirical evaluation of using selective max with an accurate heuristic
alongside the blind heuristic. The blind heuristic returns 0 for goal states, and the cost of
the cheapest action for non-goal states. For this experiment, we chose our most accurate
heuristic, hLM-CUT+ . We compare the performance of A∗ using hLM-CUT+ alone, to that of A∗
using selective max of hLM-CUT+ and the blind heuristic. Because the blind heuristic returns
a constant value for all non-goal states, the decision rule that selective max uses to combine
some heuristic h with the blind heuristic hb is simply h(s) ≥ τ + hb , that is, compute h
when the predicted value of h is greater than some constant threshold. Recall that, when
h(s) + g(s) < c∗ , computing h is simply a waste of time, because s will not be pruned.
Therefore, it only makes sense to compute h(s) when h(s) ≥ c∗ − g(s). Note that this
threshold for computing h depends on g(s), and thus is not constant. This shows that a
constant threshold for computing h(s) is not the best possible decision rule. Unfortunately,
the selective max decision rule is based on an approximation that fails to capture the subtleties
of this case.
Table 7 shows the normalized coverage of A∗ using hLM-CUT+ , and A∗ using selective max of
hLM-CUT+ and the blind heuristic. As the results show, selective max has little effect in most
domains, though it does harm performance in some, and in one domain — OPENSTACKS — it
actually performs better than the single heuristic. Table 8 shows the average expansions ratio,
using the number of states expanded by hLM-CUT+ as the baseline; note that using the blind
heuristic never increases heuristic accuracy. As these results show, selective max chooses to
use the blind heuristic quite often, expanding on average more than twice as many states than
A∗ with hLM-CUT+ alone.
728

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

coverage

hLM-CUT+

selh

airport (31)
freecell (13)
logistics00 (17)
mprime (24)
mystery (17)
pipesworld-tankage (9)
satellite (9)
zenotravel (12)

1.00 (31)
1.00 (13)
1.00 (17)
1.00 (24)
1.00 (17)
1.00 (9)
1.00 (9)
1.00 (12)

1.00 (31)
1.00 (13)
1.00 (17)
1.00 (24)
1.00 (17)
1.00 (9)
1.00 (9)
1.00 (12)

blocks (27)
depot (7)
driverlog (14)
grid (2)
gripper (6)
logistics98 (6)
miconic (140)
pathways (5)
pipesworld-notankage (17)
psr-small (48)
rovers (7)
schedule (27)
storage (15)
tpp (6)
trucks-strips (9)

1.00 (27)
1.00 (7)
1.00 (14)
1.00 (2)
1.00 (6)
1.00 (6)
1.00 (140)
1.00 (5)
1.00 (17)
1.00 (48)
1.00 (7)
1.00 (27)
1.00 (15)
1.00 (6)
1.00 (9)

1.00 (27)
1.00 (7)
1.00 (14)
1.00 (2)
1.00 (6)
1.00 (6)
0.86 (121)
1.00 (5)
1.00 (17)
1.00 (48)
1.00 (7)
1.00 (27)
0.93 (14)
1.00 (6)
1.00 (9)

elevators-opt08-strips (18)
openstacks-opt08-strips (19)
parcprinter-08-strips (21)
pegsol-08-strips (27)
scanalyzer-08-strips (13)
sokoban-opt08-strips (25)
transport-opt08-strips (11)
woodworking-opt08-strips (14)

1.00 (18)
0.89 (17)
1.00 (21)
1.00 (27)
1.00 (13)
1.00 (25)
1.00 (11)
1.00 (14)

0.83 (15)
1.00 (19)
1.00 (21)
1.00 (27)
0.77 (10)
1.00 (25)
1.00 (11)
0.93 (13)

TOTAL

1.00 (614)

0.98 (589)

Table 7: Normalized coverage of hLM-CUT+ and selective max combining hLM-CUT+ with the blind
heuristic. Domains are grouped into domains with unit cost actions and high variance in
coverage, domains with unit cost actions and low variance in coverage, and domains with
non-uniform action costs, respectively.

729

D OMSHLAK , K ARPAS , & M ARKOVITCH

expansions

hLM-CUT+

selh

airport (31)
freecell (13)
logistics00 (17)
mprime (24)
mystery (18)
pipesworld-tankage (9)
satellite (9)
zenotravel (12)

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

1.0
3.13
1.02
1.22
3.2
4.23
3.11
2.37

blocks (27)
depot (7)
driverlog (14)
grid (2)
gripper (6)
logistics98 (6)
miconic (121)
pathways (5)
pipesworld-notankage (17)
psr-small (48)
rovers (7)
schedule (27)
storage (14)
tpp (6)
trucks-strips (9)

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

1.92
1.36
1.15
7.67
1.0
1.18
14.24
1.0
1.27
2.12
1.56
1.21
5.11
1.6
1.01

elevators-opt08-strips (15)
openstacks-opt08-strips (17)
parcprinter-08-strips (21)
pegsol-08-strips (27)
scanalyzer-08-strips (10)
sokoban-opt08-strips (25)
transport-opt08-strips (11)
woodworking-opt08-strips (13)

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

13.41
1.08
1.24
1.01
4.87
1.0
5.86
46.97

GEOMETRIC MEAN

1.0

2.3

Table 8: Average ratio of expanded states between the baseline of hLM-CUT+ and selective max
combining hLM-CUT+ with the blind heuristic. Domains are grouped into domains with
unit cost actions and high variance in coverage, domains with unit cost actions and low
variance in coverage, and domains with non-uniform action costs, respectively.

730

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

The above experiments have varied the heuristics which selective max uses. In the following
experiments, we fix the set of heuristics, and examine the impact of the other parameters of selective max on performance. As we still need to evaluate over 20 different configurations of selective
max, we will focus on eight selected domains: AIRPORT, FREECELL, LOGISTICS 00, MPRIME, MYS TERY , PIPESWORLD - TANKAGE, SATELLITE , and ZENOTRAVEL. These are the eight domains with the
highest observed variance in the number of tasks solved across different planners, out of the unit
action cost domains we used. These domains were chosen in order to reduce the computation time
required for these experiments to a manageable quantity. We excluded domains with non-uniform
action costs, because they use a different method of estimating the goal depth for the state-space
sampling method, which is one of the parameters we examine. Below, we focus on one parameter
of selective max at a time, and present the total number of tasks solved in our eight chosen domains,
under different values of that parameter. Detailed, per-domain results for each parameter appear in
Appendix A.
• hyper-parameter α
Figure 5a plots the total number of problems solved, under different values of α. As these
results show, selective max is fairly robust with respect to the value of α, unless a very large
value for α is chosen, making it more difficult for selective max to choose the more accurate
heuristic.
Detailed, per-domain results appear in Table 19 in Appendix A, as well as in Figure 6. These
results show a more complex picture, where there seems to be some cutoff value for each
domain, such that increasing α past that value impairs performance. The one exception to
this is the PIPESWORLD - TANKAGE domain, where setting α = 5 helps.
• confidence threshold ρ
Figure 5b plots the total number of problems solved, under different values of ρ, Detailed,
per-domain results appear in Table 20 in Appendix A. These results indicate that selective
max is also robust to values of ρ, unless it is set to a very low value, causing selective max to
behave like the regular point-wise maximum.
• initial sample size N
Figure 5c plots the total number of problems solved under different values of N . with the
x-axis in logscale. Detailed, per-domain results appear in Table 21 in Appendix A. As the
results show, our default value of N = 100 is the best (of the three values we tried), although
selective max is still fairly robust with respect to the choice of parameter.
• sampling method
Figure 7 shows the total number of problems solved using different methods for the initial
state-space sampling. Detailed, per-domain results appear in Table 22 in Appendix A. As
the results demonstrate, the choice of sampling method can notably affect the performance of
selective max. However, as the detailed results show, this effect is only evident in the FREE CELL domain. We also remark that our default sampling method, PDB, performs worse than
the others. Indeed by using the probe based sampling methods, selective max outperforms A∗
using hLA alone. However, as this difference is only due to the FREECELL domain, we can not
state with certainty that this would generalize across all domains.
731

Solved Instances

D OMSHLAK , K ARPAS , & M ARKOVITCH

174
172
170
168
166
0

0.5

1

1.5

2

2.5
α

3

3.5

4

4.5

5

0.8

0.85

0.9

0.95

1

Solved Instances

(a) Hyper-parameter α

174
172
170
168
166
0.5

0.55

0.6

0.65

0.7

0.75
ρ

Solved Instances

(b) Confidence threshold ρ

174
172
170
168
166
10

100
Ν

1000

(c) Initial Sample Size N
Figure 5: Number of problems solved by selective max under different values for (a) hyperparameter α (b) confidence threshold ρ, and (c) initial sample size N .

732

Solved Instances

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

50
45
40
35
30
25
20
15
10
5

airport
freecell
logistics00
mprime
mystery
pw-tankage
satellite
zenotravel
0

1

2

3

4

5

α

Figure 6: Number of problems solved by selective in each domain under different values for α.

180
160

Solved Instances

140
120
100
80
60
40
20
0

PDB
174

Probe
178
Sampling Method

UnbiasedProbe
180

Figure 7: Number of problems solved by selective max with different sampling methods.

733

D OMSHLAK , K ARPAS , & M ARKOVITCH

180
160

Solved Instances

140
120
100
80
60
40
20
0
NB
174

AODE
168

ITI
156
Classifier

3NN
158

5NN
161

Figure 8: Number of problems solved by selective max with different classifiers.
• classifier
Figure 8 shows the total number of problems solved using different classifiers. Detailed,
per-domain results appear in Table 23 in Appendix A. Naive Bayes appears to be the best
classifier to use with selective max, although AODE also performs quite well. Even though
kNN enjoys very fast learning, the classifier is used mostly for classification, and as expected,
kNN does not do well. However, the increased accuracy of k = 5 seems to pay off against
the faster classification when k = 3.
6.3 Comparison with Sequential Portfolios
Sequential portfolio solvers for optimal planning are another approach for exploiting the merits of
different heuristic functions, and they have been very successful in practice, with the Fast Downward
Stone Soup sequential portfolio (Helmert et al., 2011) winning the sequential optimal track at IPC2011. A sequential portfolio utilizes different solvers by running them sequentially, each with a prespecified time limit. If one solver fails to find a solution under its allotted time limit, the sequential
portfolio terminates it, and moves on to the next solver. However, a sequential portfolio solver
needs to know the time allowance for the problem it is trying to solve beforehand, a setting known
as contract anytime (Russell & Zilberstein, 1991). In contrast, selective max can be used in an
interruptible anytime manner, where the time limit need not be known in advance.
Here, we compare selective max to sequential portfolios of A∗ with the same heuristics. As
we have the exact time it took A∗ search using each heuristic alone to solve each problem, we can
determine whether a sequential portfolio which assigns each heuristic some time limit will be able
to solve each problem. Using this data, we simulate the results of two types of sequential portfolio
planners. In the first setting, we assume that the time limit is known in advance, and simulate the
results of a contract portfolio giving an equal share of time to all heuristics. In the second setting, we
simulate an interruptible anytime portfolio by using binary exponential backoff time limits: starting
734

700

700

650

650
Solved Instances

Solved Instances

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

600
550
500

selh
portctr

450

600
550
500

selh
portctr

450

portint

portint

400

400
200

400

600 800 1000 1200 1400 1600 1800
Timeout (seconds)

200

700

700

650

650

600
550
500

selh
portctr

450

600 800 1000 1200 1400 1600 1800
Timeout (seconds)

hLA / hLM-CUT+
(b)

Solved Instances

Solved Instances

hLA / hLM-CUT
(a)

400

600
550
500

selh
portctr

450

portint

portint

400

400
200

400

600 800 1000 1200 1400 1600 1800
Timeout (seconds)

200

hLM-CUT / hLM-CUT+
(c)

400

600 800 1000 1200 1400 1600 1800
Timeout (seconds)

hLA / hLM-CUT / hLM-CUT+
(d)

Figure 9: Anytime profiles of sequential portfolios and selective max. Each plot shows the number of problems solved by selective max (selh ), a simulated contract anytime portfolio
(portctr ), and a simulated interruptible portfolio (portint ) using (a) hLA and hLM-CUT (b)
hLA and hLM-CUT+ (c) hLM-CUT and hLM-CUT+ , and (d) hLA , hLM-CUT , and hLM-CUT+ .

with a time limit of 1 second for each heuristic, we increase the time limit by a factor of 2 if none
of the heuristics were able to guide A∗ to solve the planning problem. There are several possible
orderings for the heuristics here, and we use the de facto best ordering for each problem. We denote
the contract anytime portfolio by portctr , and the interruptible anytime portfolio by portint .
Figure 9 shows the number of problems solved under different time limits for selective max,
the contract anytime sequential portfolio, and the interruptible anytime sequential portfolio. As
these results show, the contract anytime sequential portfolio almost always outperforms selective
max. On the other hand, when the sequential portfolio does not know the time limit in advance, its
performance deteriorates significantly. The best heuristic combination for selective max, hLA and
hLM-CUT , outperforms the interruptible anytime portfolio using the same heuristics, and so does the
735

D OMSHLAK , K ARPAS , & M ARKOVITCH

selective max combination of hLM-CUT and hLM-CUT+ . With the other combinations of heuristics,
the interruptible anytime portfolio performs better than selective max.

7. Discussion
Learning for planning has been a very active field since the early days of planning (Fikes, Hart,
& Nilsson, 1972), and is recently receiving growing attention in the community. However, despite
some early work (Rendell, 1983), relatively little work has dealt with learning for state-space search
guided by distance-estimating heuristics, one of the most prominent approaches to planning these
days. Most works in this direction have been devoted to learning macro-actions (see, for example,
Finkelstein & Markovitch, 1998; Botea, Enzenberger, Müller, & Schaeffer, 2005; Coles & Smith,
2007). Recently, learning for heuristic search planning has received more attention: Yoon et al.
(2008) suggested learning (inadmissible) heuristic functions based upon features extracted from
relaxed plans. Arfaee, Zilles, and Holte (2010) attempted to learn an almost admissible heuristic
estimate using a neural network. Perhaps the most closely related work to ours is that of Thayer,
Dionne, and Ruml (2011), who learn to correct errors in heuristic estimates online. Thayer et al. attempt to improve the accuracy of a single given heuristic, while selective max attempts to choose one
of several given heuristics for each state. The two works differ technically on this point. More importantly, however, none of the aforementioned approaches can guarantee that the resulting heuristic
will be admissible, and thus that an optimal solution will be found. In contrast, our focus is on optimal planning, and we are not aware of any previous work that deals with learning for optimal
heuristic search.
Our experimental evaluation demonstrates that selective max is a more effective method for
combining arbitrary admissible heuristics than the baseline point-wise maximization. Also advantageous is selective max’s ability to exploit pairs of heuristics, where one is guaranteed to always
be at least as accurate as the other. For example, the hLA heuristic can be used with two action
cost partitioning schemes: uniform and optimal (Karpas & Domshlak, 2009). The heuristic induced
by the optimal action cost partitioning is at least as accurate the one induced by the uniform action
cost partitioning, but takes much longer to compute. Selective max might be used to learn when
it is worth spending the extra time to compute the optimal cost partitioning, and when it is not. In
contrast, the max-based combination of these two heuristics would simply waste the time spent on
computing the uniform action cost partitioning.
The controlled parametric experiments demonstrate that the right choice of classifier and of
the sampling method for the initial state-space sample is very important. The other parameters of
selective max do not appear to affect performance too much, as long as they are set to reasonable
values. This implies that selective max could be improved by using faster, more accurate, classifiers,
and by developing sampling methods that can represent the state-space well.
Finally, we remark that the Fast Downward Autotune entry in the sequential optimal track of
the 2011 edition of the International Planning Competition, which used ParamILS (Hutter, Hoos,
Leyton-Brown, & Stützle, 2009) to choose the “best” configuration for the Fast Downward planner,
chose to use selective-max to combine hLM-CUT and hmax (Bonet, Loerincs, & Geffner, 1997).
This provides further evidence that selective max is a practically valuable method for combining
heuristics in optimal planning.
736

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

coverage

hLA

hLM-CUT

hLM-CUT+

airport (33)
freecell (58)
logistics00 (21)
mprime (24)
mystery (17)
pipesworld-tankage (13)
satellite (10)
zenotravel (13)

0.91 (30)
1.00 (58)
1.00 (21)
0.88 (21)
0.88 (15)
1.00 (13)
0.70 (7)
0.77 (10)

0.85 (28)
0.26 (15)
0.95 (20)
1.00 (24)
1.00 (17)
0.92 (12)
0.70 (7)
1.00 (13)

0.94 (31)
0.22 (13)
0.81 (17)
1.00 (24)
1.00 (17)
0.69 (9)
0.90 (9)
0.92 (12)

blocks (28)
depot (7)
driverlog (14)
grid (3)
gripper (7)
logistics98 (6)
miconic (142)
pathways (5)
pipesworld-notankage (18)
psr-small (49)
rovers (8)
schedule (30)
storage (15)
tpp (6)
trucks-strips (10)

0.96 (27)
1.00 (7)
1.00 (14)
1.00 (3)
1.00 (7)
1.00 (6)
1.00 (142)
0.80 (4)
1.00 (18)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
0.90 (9)

1.00 (28)
1.00 (7)
0.93 (13)
0.67 (2)
1.00 (7)
1.00 (6)
0.99 (141)
1.00 (5)
0.94 (17)
1.00 (49)
0.88 (7)
1.00 (30)
1.00 (15)
1.00 (6)
1.00 (10)

0.96 (27)
1.00 (7)
1.00 (14)
0.67 (2)
0.86 (6)
1.00 (6)
0.99 (140)
1.00 (5)
0.94 (17)
0.98 (48)
0.88 (7)
0.90 (27)
1.00 (15)
1.00 (6)
0.90 (9)

elevators-opt08-strips (22)
openstacks-opt08-strips (20)
parcprinter-08-strips (22)
pegsol-08-strips (28)
scanalyzer-08-strips (16)
sokoban-opt08-strips (30)
transport-opt08-strips (12)
woodworking-opt08-strips (19)

0.77 (17)
0.90 (18)
0.68 (15)
0.96 (27)
0.56 (9)
0.83 (25)
1.00 (12)
0.68 (13)

1.00 (22)
1.00 (20)
0.82 (18)
1.00 (28)
0.94 (15)
1.00 (30)
0.92 (11)
0.84 (16)

0.82 (18)
0.85 (17)
0.95 (21)
0.96 (27)
0.81 (13)
0.83 (25)
0.92 (11)
0.74 (14)

TOTAL

0.91 (656)

0.92 (639)

0.89 (614)

Table 9: Detailed per-domain results of A∗ with each individual heuristic. Normalized coverage is
shown, with the number of problems solved shown in parentheses. Domains are grouped
into domains with unit cost actions and high variance in coverage, domains with unit
cost actions and low variance in coverage, and domains with non-uniform action costs,
respectively.

Acknowledgments
The work was partly supported by the Israel Science Foundation (ISF) grant 1045/12.

Appendix A. Detailed Results of Empirical Evaluation
In this appendix, we present detailed per-domain, results of the experiments described in Section 6.
Table 9 shows the normalized coverage and number of problems solved in each domain, for
individual heuristics. The normalized coverage score of planner X on domain D is the number of
problems from domain D solved by planner X, divided by the number of problems from domain
D solved by at least one planner. Tables 10 – 17 give the results for combinations of two or more
heuristics. Tables 10, 12, 14, and 16 list the normalized coverage of the individual heuristics used,
and of their combination using selective max (selh ), regular maximum (maxh ), and random choice
of heuristic at each state (rndh ) after 30 minutes. Tables 11, 13, 15, and 17 give the geometric
mean of the ratio of expanded states relative to maxh in each domain, over problems solved by
all configurations. The number of tasks solved by all planners is listed in parentheses next to each
domain. The final row gives the geometric mean over the geometric means of each domain.
737

D OMSHLAK , K ARPAS , & M ARKOVITCH

coverage

hLA

hLM-CUT

maxh

rndh

selh

airport (33)
freecell (58)
logistics00 (21)
mprime (24)
mystery (17)
pipesworld-tankage (13)
satellite (10)
zenotravel (13)

0.91 (30)
1.00 (58)
1.00 (21)
0.88 (21)
0.88 (15)
1.00 (13)
0.70 (7)
0.77 (10)

0.85 (28)
0.26 (15)
0.95 (20)
1.00 (24)
1.00 (17)
0.92 (12)
0.70 (7)
1.00 (13)

0.91 (30)
0.71 (41)
0.95 (20)
1.00 (24)
1.00 (17)
0.92 (12)
0.70 (7)
1.00 (13)

0.85 (28)
0.28 (16)
0.95 (20)
0.75 (18)
0.76 (13)
0.85 (11)
0.70 (7)
0.77 (10)

0.91 (30)
0.84 (49)
1.00 (21)
1.00 (24)
1.00 (17)
0.92 (12)
0.80 (8)
1.00 (13)

blocks (28)
depot (7)
driverlog (14)
grid (3)
gripper (7)
logistics98 (6)
miconic (142)
pathways (5)
pipesworld-notankage (18)
psr-small (49)
rovers (8)
schedule (30)
storage (15)
tpp (6)
trucks-strips (10)

0.96 (27)
1.00 (7)
1.00 (14)
1.00 (3)
1.00 (7)
1.00 (6)
1.00 (142)
0.80 (4)
1.00 (18)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
0.90 (9)

1.00 (28)
1.00 (7)
0.93 (13)
0.67 (2)
1.00 (7)
1.00 (6)
0.99 (141)
1.00 (5)
0.94 (17)
1.00 (49)
0.88 (7)
1.00 (30)
1.00 (15)
1.00 (6)
1.00 (10)

1.00 (28)
1.00 (7)
1.00 (14)
0.67 (2)
1.00 (7)
1.00 (6)
0.99 (141)
1.00 (5)
0.94 (17)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
1.00 (10)

1.00 (28)
1.00 (7)
0.93 (13)
0.67 (2)
1.00 (7)
1.00 (6)
0.99 (141)
0.80 (4)
0.94 (17)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
0.90 (9)

1.00 (28)
1.00 (7)
1.00 (14)
0.67 (2)
1.00 (7)
1.00 (6)
1.00 (142)
1.00 (5)
0.94 (17)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
1.00 (10)

elevators-opt08-strips (22)
openstacks-opt08-strips (20)
parcprinter-08-strips (22)
pegsol-08-strips (28)
scanalyzer-08-strips (16)
sokoban-opt08-strips (30)
transport-opt08-strips (12)
woodworking-opt08-strips (19)

0.77 (17)
0.90 (18)
0.68 (15)
0.96 (27)
0.56 (9)
0.83 (25)
1.00 (12)
0.68 (13)

1.00 (22)
1.00 (20)
0.82 (18)
1.00 (28)
0.94 (15)
1.00 (30)
0.92 (11)
0.84 (16)

1.00 (22)
0.90 (18)
0.82 (18)
0.96 (27)
0.94 (15)
0.97 (29)
0.92 (11)
0.84 (16)

0.77 (17)
0.90 (18)
0.68 (15)
0.96 (27)
0.44 (7)
1.00 (30)
0.92 (11)
0.68 (13)

1.00 (22)
0.90 (18)
0.82 (18)
0.96 (27)
0.94 (15)
0.97 (29)
0.92 (11)
0.89 (17)

TOTAL

0.91 (656)

0.92 (639)

0.94 (665)

0.85 (603)

0.95 (677)

Table 10: Detailed per-domain normalized coverage using hLA and hLM-CUT . Each line shows the
normalized coverage in each domain, with the number of problems solved is shown in
parentheses. Domains are grouped into domains with unit cost actions and high variance
in coverage, domains with unit cost actions and low variance in coverage, and domains
with non-uniform action costs, respectively.

738

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

expansions

hLA

hLM-CUT

maxh

rndh

selh

airport (28)
freecell (15)
logistics00 (20)
mprime (18)
mystery (14)
pipesworld-tankage (11)
satellite (7)
zenotravel (10)

2.88
1.01
1.0
6.34
7.9
1.61
6.27
7.98

1.12
529.61
1.0
1.89
1.15
2.35
1.26
1.0

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

1.61
116.96
1.0
4.2
5.19
1.62
2.32
3.3

2.2
2.14
1.0
1.52
1.17
1.12
1.09
2.02

blocks (27)
depot (7)
driverlog (13)
grid (2)
gripper (7)
logistics98 (6)
miconic (141)
pathways (4)
pipesworld-notankage (17)
psr-small (49)
rovers (7)
schedule (30)
storage (15)
tpp (6)
trucks-strips (9)

7.4
3.45
7.2
2.15
1.0
7.74
1.0
39.65
2.01
1.27
2.18
1.15
2.16
1.74
46.11

1.0
1.32
1.09
1.73
1.04
1.0
1.0
1.0
2.16
1.0
1.31
1.0
1.0
1.0
1.02

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

2.2
1.91
2.89
1.57
1.02
2.69
1.0
17.91
1.97
1.11
1.77
1.03
1.45
1.42
12.12

1.61
1.3
1.24
1.83
1.0
1.08
1.0
1.0
1.36
1.15
1.09
1.15
1.56
1.0
1.01

elevators-opt08-strips (17)
openstacks-opt08-strips (18)
parcprinter-08-strips (15)
pegsol-08-strips (27)
scanalyzer-08-strips (7)
sokoban-opt08-strips (25)
transport-opt08-strips (11)
woodworking-opt08-strips (12)

21.51
1.17
24.13
3.72
69.2
15.74
12.09
31.6

1.03
1.0
1.0
1.01
1.0
1.07
1.01
1.0

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

5.99
1.03
9.34
1.8
21.47
1.33
3.79
5.68

1.37
1.15
1.0
1.01
1.14
1.04
1.44
1.28

GEOMETRIC MEAN

4.82

1.4

1.0

2.93

1.25

Table 11: Detailed per-domain expansions relative to maxh using hLA and hLM-CUT . Each row
shows the geometric mean of the ratio of expanded nodes relative to maxh . Domains are
grouped into domains with unit cost actions and high variance in coverage, domains with
unit cost actions and low variance in coverage, and domains with non-uniform action
costs, respectively.

739

D OMSHLAK , K ARPAS , & M ARKOVITCH

coverage

hLA

hLM-CUT+

maxh

rndh

selh

airport (33)
freecell (58)
logistics00 (21)
mprime (24)
mystery (17)
pipesworld-tankage (13)
satellite (10)
zenotravel (13)

0.91 (30)
1.00 (58)
1.00 (21)
0.88 (21)
0.88 (15)
1.00 (13)
0.70 (7)
0.77 (10)

0.94 (31)
0.22 (13)
0.81 (17)
1.00 (24)
1.00 (17)
0.69 (9)
0.90 (9)
0.92 (12)

0.94 (31)
0.53 (31)
0.76 (16)
1.00 (24)
1.00 (17)
0.69 (9)
0.90 (9)
0.92 (12)

0.85 (28)
0.26 (15)
0.95 (20)
0.67 (16)
0.71 (12)
0.62 (8)
0.70 (7)
0.69 (9)

0.91 (30)
0.71 (41)
1.00 (21)
1.00 (24)
1.00 (17)
0.69 (9)
1.00 (10)
0.92 (12)

blocks (28)
depot (7)
driverlog (14)
grid (3)
gripper (7)
logistics98 (6)
miconic (142)
pathways (5)
pipesworld-notankage (18)
psr-small (49)
rovers (8)
schedule (30)
storage (15)
tpp (6)
trucks-strips (10)

0.96 (27)
1.00 (7)
1.00 (14)
1.00 (3)
1.00 (7)
1.00 (6)
1.00 (142)
0.80 (4)
1.00 (18)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
0.90 (9)

0.96 (27)
1.00 (7)
1.00 (14)
0.67 (2)
0.86 (6)
1.00 (6)
0.99 (140)
1.00 (5)
0.94 (17)
0.98 (48)
0.88 (7)
0.90 (27)
1.00 (15)
1.00 (6)
0.90 (9)

0.96 (27)
1.00 (7)
1.00 (14)
0.67 (2)
0.71 (5)
1.00 (6)
0.99 (140)
1.00 (5)
0.94 (17)
0.98 (48)
0.88 (7)
0.90 (27)
1.00 (15)
1.00 (6)
0.90 (9)

0.93 (26)
0.86 (6)
0.93 (13)
0.67 (2)
0.86 (6)
0.83 (5)
0.99 (140)
0.80 (4)
0.83 (15)
0.98 (48)
0.88 (7)
0.90 (27)
1.00 (15)
1.00 (6)
0.70 (7)

0.93 (26)
1.00 (7)
0.93 (13)
0.67 (2)
1.00 (7)
1.00 (6)
1.00 (142)
1.00 (5)
0.94 (17)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
0.90 (9)

elevators-opt08-strips (22)
openstacks-opt08-strips (20)
parcprinter-08-strips (22)
pegsol-08-strips (28)
scanalyzer-08-strips (16)
sokoban-opt08-strips (30)
transport-opt08-strips (12)
woodworking-opt08-strips (19)

0.77 (17)
0.90 (18)
0.68 (15)
0.96 (27)
0.56 (9)
0.83 (25)
1.00 (12)
0.68 (13)

0.82 (18)
0.85 (17)
0.95 (21)
0.96 (27)
0.81 (13)
0.83 (25)
0.92 (11)
0.74 (14)

0.82 (18)
0.80 (16)
0.95 (21)
0.96 (27)
0.81 (13)
0.77 (23)
0.92 (11)
0.79 (15)

0.59 (13)
0.85 (17)
0.55 (12)
0.96 (27)
0.38 (6)
0.83 (25)
0.92 (11)
0.58 (11)

0.73 (16)
0.85 (17)
1.00 (22)
0.96 (27)
0.81 (13)
0.80 (24)
0.92 (11)
0.79 (15)

TOTAL

0.91 (656)

0.89 (614)

0.89 (628)

0.78 (564)

0.92 (651)

Table 12: Detailed per-domain normalized coverage using hLA and hLM-CUT+ . Each line shows the
normalized coverage in each domain, with the number of problems solved is shown in
parentheses. Domains are grouped into domains with unit cost actions and high variance
in coverage, domains with unit cost actions and low variance in coverage, and domains
with non-uniform action costs, respectively.

740

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

expansions

hLA

hLM-CUT+

maxh

rndh

selh

airport (28)
freecell (13)
logistics00 (16)
mprime (16)
mystery (13)
pipesworld-tankage (8)
satellite (7)
zenotravel (9)

3.05
1.22
1.0
8.45
7.76
2.17
19.26
6.62

1.0
47.57
1.0
1.23
1.11
1.42
1.03
1.0

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

1.43
10.54
1.0
5.2
4.77
1.48
5.94
3.09

2.81
2.05
1.0
1.57
1.7
1.86
4.12
4.04

blocks (26)
depot (6)
driverlog (13)
grid (2)
gripper (5)
logistics98 (5)
miconic (140)
pathways (4)
pipesworld-notankage (15)
psr-small (48)
rovers (7)
schedule (27)
storage (15)
tpp (6)
trucks-strips (7)

6.97
21.8
11.11
5.04
1.0
6.1
1.0
40.56
3.08
1.31
2.75
1.09
2.29
2.72
46.09

1.0
1.0
1.01
1.01
1.0
1.0
1.0
1.0
1.12
1.0
1.01
1.0
1.0
1.0
1.01

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

2.15
5.46
3.71
2.14
1.0
2.14
1.0
18.03
1.75
1.14
1.81
1.0
1.53
1.88
12.02

4.28
3.96
2.56
4.74
1.0
3.79
1.0
1.0
2.46
1.27
1.45
1.09
2.16
1.17
1.01

elevators-opt08-strips (13)
openstacks-opt08-strips (16)
parcprinter-08-strips (12)
pegsol-08-strips (27)
scanalyzer-08-strips (6)
sokoban-opt08-strips (21)
transport-opt08-strips (11)
woodworking-opt08-strips (11)

28.6
1.17
24.87
4.92
23.07
15.66
15.34
53.27

1.01
1.0
1.0
1.0
1.0
1.0
1.0
1.0

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

7.1
1.03
9.23
2.15
6.88
1.33
4.26
8.53

7.46
1.09
1.19
1.0
1.43
1.01
2.84
1.91

GEOMETRIC MEAN

5.85

1.16

1.0

2.9

1.89

Table 13: Detailed per-domain expansions relative to maxh using hLA and hLM-CUT+ . Each row
shows the geometric mean of the ratio of expanded nodes relative to maxh . Domains are
grouped into domains with unit cost actions and high variance in coverage, domains with
unit cost actions and low variance in coverage, and domains with non-uniform action
costs, respectively.

741

D OMSHLAK , K ARPAS , & M ARKOVITCH

coverage

hLM-CUT

hLM-CUT+

maxh

rndh

selh

airport (33)
freecell (58)
logistics00 (21)
mprime (24)
mystery (17)
pipesworld-tankage (13)
satellite (10)
zenotravel (13)

0.85 (28)
0.26 (15)
0.95 (20)
1.00 (24)
1.00 (17)
0.92 (12)
0.70 (7)
1.00 (13)

0.94 (31)
0.22 (13)
0.81 (17)
1.00 (24)
1.00 (17)
0.69 (9)
0.90 (9)
0.92 (12)

0.94 (31)
0.22 (13)
0.76 (16)
1.00 (24)
1.00 (17)
0.69 (9)
0.90 (9)
0.92 (12)

0.82 (27)
0.21 (12)
0.95 (20)
0.88 (21)
0.88 (15)
0.62 (8)
0.70 (7)
0.92 (12)

0.85 (28)
0.22 (13)
0.95 (20)
1.00 (24)
0.94 (16)
0.69 (9)
0.80 (8)
0.92 (12)

blocks (28)
depot (7)
driverlog (14)
grid (3)
gripper (7)
logistics98 (6)
miconic (142)
pathways (5)
pipesworld-notankage (18)
psr-small (49)
rovers (8)
schedule (30)
storage (15)
tpp (6)
trucks-strips (10)

1.00 (28)
1.00 (7)
0.93 (13)
0.67 (2)
1.00 (7)
1.00 (6)
0.99 (141)
1.00 (5)
0.94 (17)
1.00 (49)
0.88 (7)
1.00 (30)
1.00 (15)
1.00 (6)
1.00 (10)

0.96 (27)
1.00 (7)
1.00 (14)
0.67 (2)
0.86 (6)
1.00 (6)
0.99 (140)
1.00 (5)
0.94 (17)
0.98 (48)
0.88 (7)
0.90 (27)
1.00 (15)
1.00 (6)
0.90 (9)

0.96 (27)
1.00 (7)
1.00 (14)
0.67 (2)
0.86 (6)
1.00 (6)
0.99 (140)
1.00 (5)
0.94 (17)
0.98 (48)
0.88 (7)
0.90 (27)
1.00 (15)
1.00 (6)
0.90 (9)

1.00 (28)
1.00 (7)
0.93 (13)
0.67 (2)
0.86 (6)
1.00 (6)
0.99 (140)
1.00 (5)
0.89 (16)
0.98 (48)
0.88 (7)
0.90 (27)
1.00 (15)
1.00 (6)
0.90 (9)

1.00 (28)
1.00 (7)
1.00 (14)
0.67 (2)
1.00 (7)
1.00 (6)
0.99 (141)
1.00 (5)
0.94 (17)
1.00 (49)
0.88 (7)
1.00 (30)
1.00 (15)
1.00 (6)
1.00 (10)

elevators-opt08-strips (22)
openstacks-opt08-strips (20)
parcprinter-08-strips (22)
pegsol-08-strips (28)
scanalyzer-08-strips (16)
sokoban-opt08-strips (30)
transport-opt08-strips (12)
woodworking-opt08-strips (19)

1.00 (22)
1.00 (20)
0.82 (18)
1.00 (28)
0.94 (15)
1.00 (30)
0.92 (11)
0.84 (16)

0.82 (18)
0.85 (17)
0.95 (21)
0.96 (27)
0.81 (13)
0.83 (25)
0.92 (11)
0.74 (14)

0.82 (18)
0.85 (17)
0.95 (21)
0.96 (27)
0.81 (13)
0.83 (25)
0.92 (11)
0.79 (15)

0.82 (18)
0.95 (19)
0.82 (18)
0.96 (27)
0.81 (13)
0.83 (25)
0.92 (11)
0.74 (14)

0.95 (21)
0.95 (19)
0.91 (20)
0.96 (27)
0.94 (15)
0.83 (25)
0.92 (11)
0.95 (18)

TOTAL

0.92 (639)

0.89 (614)

0.89 (614)

0.87 (602)

0.91 (630)

Table 14: Detailed per-domain normalized coverage using hLM-CUT and hLM-CUT+ . Each line shows
the normalized coverage in each domain, with the number of problems solved is shown in
parentheses. Domains are grouped into domains with unit cost actions and high variance
in coverage, domains with unit cost actions and low variance in coverage, and domains
with non-uniform action costs, respectively.

742

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

expansions

hLM-CUT

hLM-CUT+

maxh

rndh

selh

airport (26)
freecell (12)
logistics00 (16)
mprime (21)
mystery (16)
pipesworld-tankage (8)
satellite (7)
zenotravel (12)

1.16
9.55
1.0
2.2
1.69
3.09
3.66
1.61

1.0
1.0
1.0
1.01
1.01
1.01
0.98
1.09

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

1.04
4.37
1.0
1.84
1.52
1.75
2.39
1.3

1.16
1.26
1.0
1.0
1.32
1.61
1.51
1.22

blocks (27)
depot (7)
driverlog (13)
grid (2)
gripper (6)
logistics98 (6)
miconic (140)
pathways (5)
pipesworld-notankage (16)
psr-small (48)
rovers (7)
schedule (27)
storage (15)
tpp (6)
trucks-strips (9)

1.02
7.53
1.71
4.03
1.05
1.08
1.0
1.22
3.49
1.03
1.66
1.0
1.07
1.56
1.32

1.0
1.0
1.02
1.0
1.0
1.05
1.0
1.02
1.01
1.0
1.01
1.0
1.0
1.0
1.0

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

1.01
4.07
1.36
1.9
1.03
1.06
1.0
1.13
1.9
1.02
1.28
1.0
1.03
1.16
1.14

1.02
1.25
1.49
1.28
1.05
1.06
1.0
1.22
1.4
1.03
1.3
1.0
1.07
1.56
1.26

elevators-opt08-strips (18)
openstacks-opt08-strips (17)
parcprinter-08-strips (17)
pegsol-08-strips (27)
scanalyzer-08-strips (13)
sokoban-opt08-strips (25)
transport-opt08-strips (11)
woodworking-opt08-strips (13)

1.75
1.0
1.71
1.33
1.22
1.04
1.29
1.45

1.09
1.0
1.0
1.01
1.02
1.04
1.01
1.06

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

1.4
1.0
1.37
1.15
1.14
1.01
1.15
1.26

1.72
1.0
1.0
1.2
1.13
1.03
1.26
1.12

GEOMETRIC MEAN

1.66

1.01

1.0

1.35

1.2

Table 15: Detailed per-domain expansions relative to maxh using hLM-CUT and hLM-CUT+ . Each
row shows the geometric mean of the ratio of expanded nodes relative to maxh . Domains
are grouped into domains with unit cost actions and high variance in coverage, domains
with unit cost actions and low variance in coverage, and domains with non-uniform action
costs, respectively.

743

D OMSHLAK , K ARPAS , & M ARKOVITCH

coverage

hLA

hLM-CUT

hLM-CUT+

maxh

rndh

selh

airport (33)
freecell (58)
logistics00 (21)
mprime (24)
mystery (17)
pipesworld-tankage (13)
satellite (10)
zenotravel (13)

0.91 (30)
1.00 (58)
1.00 (21)
0.88 (21)
0.88 (15)
1.00 (13)
0.70 (7)
0.77 (10)

0.85 (28)
0.26 (15)
0.95 (20)
1.00 (24)
1.00 (17)
0.92 (12)
0.70 (7)
1.00 (13)

0.94 (31)
0.22 (13)
0.81 (17)
1.00 (24)
1.00 (17)
0.69 (9)
0.90 (9)
0.92 (12)

0.94 (31)
0.53 (31)
0.76 (16)
1.00 (24)
1.00 (17)
0.69 (9)
0.90 (9)
0.92 (12)

0.79 (26)
0.26 (15)
0.95 (20)
0.75 (18)
0.71 (12)
0.69 (9)
0.70 (7)
0.69 (9)

0.91 (30)
0.57 (33)
0.95 (20)
0.96 (23)
1.00 (17)
0.85 (11)
0.80 (8)
0.92 (12)

blocks (28)
depot (7)
driverlog (14)
grid (3)
gripper (7)
logistics98 (6)
miconic (142)
pathways (5)
pipesworld-notankage (18)
psr-small (49)
rovers (8)
schedule (30)
storage (15)
tpp (6)
trucks-strips (10)

0.96 (27)
1.00 (7)
1.00 (14)
1.00 (3)
1.00 (7)
1.00 (6)
1.00 (142)
0.80 (4)
1.00 (18)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
0.90 (9)

1.00 (28)
1.00 (7)
0.93 (13)
0.67 (2)
1.00 (7)
1.00 (6)
0.99 (141)
1.00 (5)
0.94 (17)
1.00 (49)
0.88 (7)
1.00 (30)
1.00 (15)
1.00 (6)
1.00 (10)

0.96 (27)
1.00 (7)
1.00 (14)
0.67 (2)
0.86 (6)
1.00 (6)
0.99 (140)
1.00 (5)
0.94 (17)
0.98 (48)
0.88 (7)
0.90 (27)
1.00 (15)
1.00 (6)
0.90 (9)

0.96 (27)
1.00 (7)
1.00 (14)
0.67 (2)
0.71 (5)
1.00 (6)
0.99 (140)
1.00 (5)
0.94 (17)
0.98 (48)
0.88 (7)
0.90 (27)
1.00 (15)
1.00 (6)
0.90 (9)

0.96 (27)
1.00 (7)
0.93 (13)
0.67 (2)
0.86 (6)
0.83 (5)
0.99 (140)
0.80 (4)
0.83 (15)
0.98 (48)
0.88 (7)
0.93 (28)
1.00 (15)
1.00 (6)
0.90 (9)

1.00 (28)
1.00 (7)
0.93 (13)
0.67 (2)
1.00 (7)
1.00 (6)
1.00 (142)
1.00 (5)
0.94 (17)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
1.00 (10)

elevators-opt08-strips (22)
openstacks-opt08-strips (20)
parcprinter-08-strips (22)
pegsol-08-strips (28)
scanalyzer-08-strips (16)
sokoban-opt08-strips (30)
transport-opt08-strips (12)
woodworking-opt08-strips (19)

0.77 (17)
0.90 (18)
0.68 (15)
0.96 (27)
0.56 (9)
0.83 (25)
1.00 (12)
0.68 (13)

1.00 (22)
1.00 (20)
0.82 (18)
1.00 (28)
0.94 (15)
1.00 (30)
0.92 (11)
0.84 (16)

0.82 (18)
0.85 (17)
0.95 (21)
0.96 (27)
0.81 (13)
0.83 (25)
0.92 (11)
0.74 (14)

0.82 (18)
0.80 (16)
0.95 (21)
0.96 (27)
0.81 (13)
0.77 (23)
0.92 (11)
0.79 (15)

0.64 (14)
0.90 (18)
0.59 (13)
0.96 (27)
0.38 (6)
0.90 (27)
0.92 (11)
0.74 (14)

0.95 (21)
0.80 (16)
0.86 (19)
0.96 (27)
0.94 (15)
0.87 (26)
0.92 (11)
0.79 (15)

TOTAL

0.91 (656)

0.92 (639)

0.89 (614)

0.89 (628)

0.81 (578)

0.92 (649)

Table 16: Detailed per-domain normalized coverage using hLA , hLM-CUT and hLM-CUT+ . Each line
shows the normalized coverage in each domain, with the number of problems solved is
shown in parentheses. Domains are grouped into domains with unit cost actions and high
variance in coverage, domains with unit cost actions and low variance in coverage, and
domains with non-uniform action costs, respectively.

744

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

expansions

hLA

hLM-CUT

hLM-CUT+

maxh

rndh

selh

airport (26)
freecell (13)
logistics00 (16)
mprime (18)
mystery (13)
pipesworld-tankage (9)
satellite (7)
zenotravel (9)

2.29
1.22
1.0
9.21
7.85
2.68
18.81
7.26

1.16
417.8
1.0
2.74
1.41
5.08
3.78
1.23

1.0
47.65
1.0
1.21
1.13
1.38
1.01
1.1

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

1.04
45.83
1.0
4.26
4.48
2.27
4.53
3.07

1.71
6.73
1.0
1.99
1.43
1.93
2.45
2.45

blocks (27)
depot (7)
driverlog (13)
grid (2)
gripper (5)
logistics98 (5)
miconic (140)
pathways (4)
pipesworld-notankage (15)
psr-small (48)
rovers (7)
schedule (27)
storage (15)
tpp (6)
trucks-strips (9)

7.59
19.63
11.36
5.04
1.0
6.43
1.0
40.63
3.09
1.31
2.77
1.09
2.3
2.73
60.39

1.02
7.53
1.73
4.06
1.06
1.08
1.0
1.02
4.29
1.03
1.67
1.0
1.07
1.56
1.33

1.0
1.01
1.03
1.01
1.0
1.05
1.0
1.0
1.13
1.0
1.01
1.0
1.01
1.0
1.01

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

1.58
5.22
2.79
2.15
1.02
1.79
1.0
7.76
2.35
1.1
1.78
0.99
1.33
1.91
6.05

1.67
2.46
2.03
4.91
1.0
1.58
1.0
1.0
2.53
1.24
1.38
1.09
1.58
1.41
1.33

elevators-opt08-strips (14)
openstacks-opt08-strips (16)
parcprinter-08-strips (13)
pegsol-08-strips (27)
scanalyzer-08-strips (6)
sokoban-opt08-strips (21)
transport-opt08-strips (11)
woodworking-opt08-strips (11)

33.16
1.17
45.31
4.94
24.13
16.43
15.5
53.33

1.65
1.0
2.02
1.34
1.5
1.03
1.29
1.37

1.1
1.0
1.0
1.01
1.05
1.05
1.01
1.0

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

4.65
1.03
5.91
1.69
5.5
1.14
2.66
4.02

2.9
1.07
1.0
1.26
1.87
1.13
1.82
1.63

GEOMETRIC MEAN

6.1

1.91

1.18

1.0

2.56

1.67

Table 17: Detailed per-domain expansions relative to maxh using hLA , hLM-CUT and hLM-CUT+ .
Each row shows the geometric mean of the ratio of expanded nodes relative to maxh .
Domains are grouped into domains with unit cost actions and high variance in coverage,
domains with unit cost actions and low variance in coverage, and domains with nonuniform action costs, respectively.

745

D OMSHLAK , K ARPAS , & M ARKOVITCH

overhead

hLA /hLM-CUT

hLA /hLM-CUT+

hLM-CUT /hLM-CUT+

All Three

airport (28)
freecell (13)
logistics00 (20)
mprime (23)
mystery (17)
pipesworld-tankage (9)
satellite (7)
zenotravel (12)
blocks (26)
depot (7)
driverlog (13)
grid (2)
gripper (7)
logistics98 (6)
miconic (141)
pathways (5)
pipesworld-notankage (17)
psr-small (49)
rovers (7)
schedule (30)
storage (15)
tpp (6)
trucks-strips (9)
elevators-opt08-strips (16)
openstacks-opt08-strips (16)
parcprinter-08-strips (18)
pegsol-08-strips (27)
scanalyzer-08-strips (13)
sokoban-opt08-strips (24)
transport-opt08-strips (11)
woodworking-opt08-strips (14)

4%
4%
8%
7%
3%
11%
14%
15%
21%
45%
29%
26%
13%
15%
1%
5%
22%
8%
15%
13%
18%
2%
3%
32%
15%
2%
9%
2%
5%
12%
5%

7%
8%
7%
7%
3%
11%
18%
35%
35%
29%
45%
17%
13%
31%
4%
1%
17%
11%
24%
13%
12%
1%
2%
75%
9%
6%
2%
4%
2%
23%
5%

1%
13%
2%
6%
8%
10%
10%
26%
2%
14%
26%
1%
5%
6%
3%
4%
20%
3%
26%
5%
2%
2%
12%
8%
10%
1%
28%
10%
14%
7%
2%

9%
1%
6%
3%
2%
5%
8%
21%
5%
10%
21%
6%
22%
5%
4%
7%
22%
12%
19%
24%
10%
3%
7%
9%
23%
5%
15%
1%
7%
3%
4%

AVERAGE

12%

15%

9%

10%

Table 18: Selective max overhead. Each row lists the average percentage of time spent on learning
and classification, out of the total time taken by selective max, in each domain, for each
set of heuristics. Domains are grouped into domains with unit cost actions and high
variance in coverage, domains with unit cost actions and low variance in coverage, and
domains with non-uniform action costs, respectively.

746

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

coverage
airport (50)
freecell (80)
logistics00 (28)
mprime (35)
mystery (30)
pipesworld-tankage (50)
satellite (36)
zenotravel (20)
SUM

selα=0.1
h
30
49
21
24
17
12
8
13
174

selα=0.5
h
30
49
21
24
17
12
8
13
174

selα=1
h
30
49
21
24
17
12
8
13
174

selα=1.5
h
30
49
21
24
17
12
8
13
174

selα=2
h
30
49
21
22
17
12
7
12
170

selα=3
h
30
49
21
23
16
12
7
11
169

selα=4
h
30
49
21
21
15
13
7
10
166

selα=5
h
30
49
21
21
15
13
7
10
166

Table 19: Number of problems solved by selective max in each domain with varying values of
hyper-parameter α

coverage
airport (50)
freecell (80)
logistics00 (28)
mprime (35)
mystery (30)
pipesworld-tankage (50)
satellite (36)
zenotravel (20)
SUM

selρ=0.51
h
30
48
21
24
17
12
8
13
173

selρ=0.6
h
30
49
21
24
17
12
8
13
174

selρ=0.7
h
30
49
21
24
17
12
8
13
174

selρ=0.8
h
30
49
21
24
17
12
8
13
174

selρ=0.9
h
30
49
21
24
17
12
8
13
174

selρ=0.99
h
30
49
21
24
17
12
8
13
174

Table 20: Number of problems solved by selective max in each domain with varying values of
confidence threshold ρ

Table 18 lists the average overhead of selective max in each domain, for each combination of
two or more heuristics.
Tables 19, 20, 21, 22 and 23 list the number of problems solved in each domain, under various
values for α, ρ, N , sampling method and classifier, respectively.

coverage
airport (50)
freecell (80)
logistics00 (28)
mprime (35)
mystery (30)
pipesworld-tankage (50)
satellite (36)
zenotravel (20)
SUM

=10
selN
h
30
47
21
24
17
12
8
13
172

=100
selN
h
30
49
21
24
17
12
8
13
174

=1000
selN
h
30
46
21
24
17
12
8
13
171

Table 21: Number of problems solved by selective max in each domain with varying values of
initial Sample Size N

747

D OMSHLAK , K ARPAS , & M ARKOVITCH

coverage
airport (50)
freecell (80)
logistics00 (28)
mprime (35)
mystery (30)
pipesworld-tankage (50)
satellite (36)
zenotravel (20)
SUM

selPDB
h
30
49
21
24
17
12
8
13
174

selP
h
30
53
21
24
17
12
8
13
178

P
selU
h
30
55
21
24
17
12
8
13
180

Table 22: Number of problems solved by selective max in each domain with different sampling
methods. PDB is the sampling method of Haslum et al. (2007), P is the biased probes
sampling method, and U P is the unbiased probes sampling method.

coverage
airport (50)
freecell (80)
logistics00 (28)
mprime (35)
mystery (30)
pipesworld-tankage (50)
satellite (36)
zenotravel (20)
SUM

B
selN
h
30
49
21
24
17
12
8
13
174

selAODE
h
25
49
20
24
17
12
8
13
168

I
selIT
h
30
34
20
24
17
12
7
12
156

N
sel3N
h
30
35
20
24
17
12
7
13
158

N
sel5N
h
28
46
20
23
17
10
6
11
161

Table 23: Number of problems solved by selective max in each domain with different classifiers

748

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

coverage

selh

portint

portctr

airport (33)
freecell (58)
logistics00 (21)
mprime (24)
mystery (17)
pipesworld-tankage (13)
satellite (10)
zenotravel (13)

0.91 (30)
0.84 (49)
1.00 (21)
1.00 (24)
1.00 (17)
0.92 (12)
0.80 (8)
1.00 (13)

0.91 (30)
0.91 (53)
0.95 (20)
0.96 (23)
1.12 (19)
0.92 (12)
0.70 (7)
0.92 (12)

0.91 (30)
0.93 (54)
1.00 (21)
0.96 (23)
1.24 (21)
1.00 (13)
0.70 (7)
0.92 (12)

blocks (28)
depot (7)
driverlog (14)
grid (3)
gripper (7)
logistics98 (6)
miconic (142)
pathways (5)
pipesworld-notankage (18)
psr-small (49)
rovers (8)
schedule (30)
storage (15)
tpp (6)
trucks-strips (10)

1.00 (28)
1.00 (7)
1.00 (14)
0.67 (2)
1.00 (7)
1.00 (6)
1.00 (142)
1.00 (5)
0.94 (17)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
1.00 (10)

1.00 (28)
1.00 (7)
1.00 (14)
0.67 (2)
1.00 (7)
1.00 (6)
1.00 (142)
1.00 (5)
1.00 (18)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
0.90 (9)

1.00 (28)
1.00 (7)
1.00 (14)
0.67 (2)
1.00 (7)
1.00 (6)
1.00 (142)
1.00 (5)
1.00 (18)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
1.00 (10)

elevators-opt08-strips (22)
openstacks-opt08-strips (20)
parcprinter-08-strips (22)
pegsol-08-strips (28)
scanalyzer-08-strips (16)
sokoban-opt08-strips (30)
transport-opt08-strips (12)
woodworking-opt08-strips (19)

1.00 (22)
0.90 (18)
0.82 (18)
0.96 (27)
0.94 (15)
0.97 (29)
0.92 (11)
0.89 (17)

0.82 (18)
0.90 (18)
0.82 (18)
0.96 (27)
0.81 (13)
0.97 (29)
0.92 (11)
0.84 (16)

0.86 (19)
0.95 (19)
0.82 (18)
0.96 (27)
1.00 (16)
0.97 (29)
1.00 (12)
0.89 (17)

TOTAL

0.95 (677)

0.94 (672)

0.96 (685)

Table 24: Detailed coverage of portfolio using hLA / hLM-CUT . Number of problems solved by selective max (selh ), a simulated interruptible portfolio (portint ), and a simulated contract
anytime portfolio (portctr ) in each domain using heuristics hLA / hLM-CUT . Domains are
grouped into domains with unit cost actions and high variance in coverage, domains with
unit cost actions and low variance in coverage, and domains with non-uniform action
costs, respectively.

749

D OMSHLAK , K ARPAS , & M ARKOVITCH

coverage

selh

portint

portctr

airport (33)
freecell (58)
logistics00 (21)
mprime (24)
mystery (17)
pipesworld-tankage (13)
satellite (10)
zenotravel (13)

0.91 (30)
0.71 (41)
1.00 (21)
1.00 (24)
1.00 (17)
0.69 (9)
1.00 (10)
0.92 (12)

0.91 (30)
0.91 (53)
0.95 (20)
1.00 (24)
1.12 (19)
0.92 (12)
0.80 (8)
0.85 (11)

0.91 (30)
0.93 (54)
1.00 (21)
1.00 (24)
1.18 (20)
1.00 (13)
0.80 (8)
0.85 (11)

blocks (28)
depot (7)
driverlog (14)
grid (3)
gripper (7)
logistics98 (6)
miconic (142)
pathways (5)
pipesworld-notankage (18)
psr-small (49)
rovers (8)
schedule (30)
storage (15)
tpp (6)
trucks-strips (10)

0.93 (26)
1.00 (7)
0.93 (13)
0.67 (2)
1.00 (7)
1.00 (6)
1.00 (142)
1.00 (5)
0.94 (17)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
0.90 (9)

0.93 (26)
1.00 (7)
1.00 (14)
0.67 (2)
1.00 (7)
1.00 (6)
1.00 (142)
1.00 (5)
1.00 (18)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
0.70 (7)

0.96 (27)
1.00 (7)
1.00 (14)
0.67 (2)
1.00 (7)
1.00 (6)
1.00 (142)
1.00 (5)
1.00 (18)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
0.80 (8)

elevators-opt08-strips (22)
openstacks-opt08-strips (20)
parcprinter-08-strips (22)
pegsol-08-strips (28)
scanalyzer-08-strips (16)
sokoban-opt08-strips (30)
transport-opt08-strips (12)
woodworking-opt08-strips (19)

0.73 (16)
0.85 (17)
1.00 (22)
0.96 (27)
0.81 (13)
0.80 (24)
0.92 (11)
0.79 (15)

0.82 (18)
0.85 (17)
1.00 (22)
0.96 (27)
0.75 (12)
0.83 (25)
0.92 (11)
0.79 (15)

0.82 (18)
0.85 (17)
1.00 (22)
0.96 (27)
0.94 (15)
0.83 (25)
1.00 (12)
0.79 (15)

TOTAL

0.92 (651)

0.93 (666)

0.94 (676)

Table 25: Detailed coverage of portfolio using hLA / hLM-CUT+ . Number of problems solved by selective max (selh ), a simulated interruptible portfolio (portint ), and a simulated contract
anytime portfolio (portctr ) in each domain using heuristics hLA / hLM-CUT+ . Domains
are grouped into domains with unit cost actions and high variance in coverage, domains
with unit cost actions and low variance in coverage, and domains with non-uniform action
costs, respectively.

750

O NLINE S PEEDUP L EARNING FOR O PTIMAL P LANNING

coverage

selh

portint

portctr

airport (33)
freecell (58)
logistics00 (21)
mprime (24)
mystery (17)
pipesworld-tankage (13)
satellite (10)
zenotravel (13)

0.85 (28)
0.22 (13)
0.95 (20)
1.00 (24)
0.94 (16)
0.69 (9)
0.80 (8)
0.92 (12)

0.88 (29)
0.24 (14)
0.95 (20)
1.00 (24)
1.18 (20)
0.69 (9)
0.80 (8)
0.92 (12)

0.88 (29)
0.26 (15)
0.95 (20)
1.00 (24)
1.24 (21)
0.85 (11)
0.80 (8)
0.92 (12)

blocks (28)
depot (7)
driverlog (14)
grid (3)
gripper (7)
logistics98 (6)
miconic (142)
pathways (5)
pipesworld-notankage (18)
psr-small (49)
rovers (8)
schedule (30)
storage (15)
tpp (6)
trucks-strips (10)

1.00 (28)
1.00 (7)
1.00 (14)
0.67 (2)
1.00 (7)
1.00 (6)
0.99 (141)
1.00 (5)
0.94 (17)
1.00 (49)
0.88 (7)
1.00 (30)
1.00 (15)
1.00 (6)
1.00 (10)

1.00 (28)
1.00 (7)
0.93 (13)
0.67 (2)
0.86 (6)
1.00 (6)
0.99 (140)
1.00 (5)
0.89 (16)
1.00 (49)
0.88 (7)
0.93 (28)
1.00 (15)
1.00 (6)
0.90 (9)

1.00 (28)
1.00 (7)
0.93 (13)
0.67 (2)
1.00 (7)
1.00 (6)
0.99 (141)
1.00 (5)
0.94 (17)
1.00 (49)
0.88 (7)
1.00 (30)
1.00 (15)
1.00 (6)
1.00 (10)

elevators-opt08-strips (22)
openstacks-opt08-strips (20)
parcprinter-08-strips (22)
pegsol-08-strips (28)
scanalyzer-08-strips (16)
sokoban-opt08-strips (30)
transport-opt08-strips (12)
woodworking-opt08-strips (19)

0.95 (21)
0.95 (19)
0.91 (20)
0.96 (27)
0.94 (15)
0.83 (25)
0.92 (11)
0.95 (18)

0.82 (18)
0.90 (18)
1.00 (22)
0.96 (27)
0.81 (13)
0.93 (28)
0.92 (11)
0.79 (15)

0.86 (19)
0.95 (19)
1.00 (22)
0.96 (27)
0.94 (15)
0.93 (28)
0.92 (11)
0.84 (16)

TOTAL

0.91 (630)

0.90 (625)

0.93 (640)

Table 26: Detailed coverage of portfolio using hLM-CUT / hLM-CUT+ . Number of problems solved
by selective max (selh ), a simulated interruptible portfolio (portint ), and a simulated
contract anytime portfolio (portctr ) in each domain using heuristics hLM-CUT / hLM-CUT+ .
Domains are grouped into domains with unit cost actions and high variance in coverage,
domains with unit cost actions and low variance in coverage, and domains with nonuniform action costs, respectively.

751

D OMSHLAK , K ARPAS , & M ARKOVITCH

coverage

selh

portint

portctr

airport (33)
freecell (58)
logistics00 (21)
mprime (24)
mystery (17)
pipesworld-tankage (13)
satellite (10)
zenotravel (13)

0.91 (30)
0.57 (33)
0.95 (20)
0.96 (23)
1.00 (17)
0.85 (11)
0.80 (8)
0.92 (12)

0.91 (30)
0.91 (53)
0.95 (20)
1.00 (24)
1.18 (20)
0.92 (12)
0.80 (8)
0.92 (12)

0.91 (30)
0.93 (54)
1.00 (21)
1.00 (24)
1.18 (20)
0.92 (12)
0.80 (8)
0.92 (12)

blocks (28)
depot (7)
driverlog (14)
grid (3)
gripper (7)
logistics98 (6)
miconic (142)
pathways (5)
pipesworld-notankage (18)
psr-small (49)
rovers (8)
schedule (30)
storage (15)
tpp (6)
trucks-strips (10)

1.00 (28)
1.00 (7)
0.93 (13)
0.67 (2)
1.00 (7)
1.00 (6)
1.00 (142)
1.00 (5)
0.94 (17)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
1.00 (10)

1.00 (28)
1.00 (7)
1.00 (14)
0.67 (2)
1.00 (7)
1.00 (6)
1.00 (142)
1.00 (5)
1.00 (18)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
0.90 (9)

1.00 (28)
1.00 (7)
1.00 (14)
0.67 (2)
1.00 (7)
1.00 (6)
1.00 (142)
1.00 (5)
1.00 (18)
1.00 (49)
1.00 (8)
1.00 (30)
1.00 (15)
1.00 (6)
0.90 (9)

elevators-opt08-strips (22)
openstacks-opt08-strips (20)
parcprinter-08-strips (22)
pegsol-08-strips (28)
scanalyzer-08-strips (16)
sokoban-opt08-strips (30)
transport-opt08-strips (12)
woodworking-opt08-strips (19)

0.95 (21)
0.80 (16)
0.86 (19)
0.96 (27)
0.94 (15)
0.87 (26)
0.92 (11)
0.79 (15)

0.82 (18)
0.90 (18)
1.00 (22)
0.96 (27)
0.81 (13)
0.97 (29)
0.92 (11)
0.84 (16)

0.86 (19)
0.95 (19)
1.00 (22)
0.96 (27)
0.81 (13)
0.97 (29)
0.92 (11)
0.89 (17)

TOTAL

0.92 (649)

0.95 (679)

0.95 (684)

Table 27: Detailed coverage of portfolio using hLA / hLM-CUT / hLM-CUT+ . Number of problems
solved by selective max (selh ), a simulated interruptible portfolio (portint ), and a simulated contract anytime portfolio (portctr ) in each domain using heuristics hLA / hLM-CUT
/ hLM-CUT+ . Domains are grouped into domains with unit cost actions and high variance
in coverage, domains with unit cost actions and low variance in coverage, and domains
with non-uniform action costs, respectively.

Tables 24, 25, 26 and 27 list the normalized coverage in each domain for selective max, and for
the simulated contract and interruptible sequential portfolios.


