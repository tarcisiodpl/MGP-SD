
When the information about uncertainty
cannot be quantified in a simple, probabilistic way, the topic of possibilistic decision theory is often a natural one to consider. The
development of possibilistic decision theory
has lead to a series of possibilistic criteria,
e.g pessimistic possibilistic qualitative utility,
possibilistic likely dominance , binary possibilistic utility and possibilistic Choquet integrals. This paper focuses on sequential decision making in possibilistic decision trees. It
proposes a complexity study of the problem
of finding an optimal strategy depending on
the monotonicity property of the optimization criteria which allows the application of
dynamic programming that offers a polytime
reduction of the decision problem. It also
shows that possibilistic Choquet integrals do
not satisfy this property, and that in this case
the optimization problem is N P − hard.

1

Introduction

For several years, there has been a growing interest
in the Artificial Intelligence community towards the
foundations and computational methods of decision
making under uncertainty. This is especially relevant
for applications to sequential decision making under
uncertainty, where a suitable strategy is to be found,
that associate a decision to each state of the world.
Several formalisms can be used for sequential decision
problems, such as decision trees, influence diagrams
or Markov decision processes. A decision tree is an
explicit representation of a sequential decision problem, while influence diagrams or Markov decision processes are compact representations. Even in the simple
and explicit case of decision trees, the set of potential
strategies increases exponentially with the tree size.

Wided Guezguez
LARODEC Laboratory
ISG, University of Tunis
Tunisia, 2000
widedguezguez@gmail.com

A popular criterion to compare decisions under risk is
the expected utility (EU ) model axiomatized by von
Neumann and Morgenstern [12]. This model relies on
a probabilistic representation of uncertainty: an elementary decision (i.e. a one-step decision problem)
is modeled by a probabilistic lottery over its possible
outcomes. The preferences of the decision maker are
supposed to be captured by a utility function assigning
a numerical value to each outcome. The evaluation of
a lottery is then performed through the computation
of its expected utility (the greater is the better). In
sequential decision making, each possible strategy is
viewed as compound lottery. It can be reduced to an
equivalent simple lottery, and thus compared to the
others according to its expected utility. Although the
high combinatorial nature of the set of possible strategies, the selection of an optimal strategy can be performed in polynomial time (polytime) with the size of
the decision tree: the EU model indeed satisfies a property of monotonicity that guarantees completeness of
a polytime algorithm of dynamic programming.
When the information about uncertainty cannot be
quantified in a simple, probabilistic way the topic of
possibilistic decision theory is often a natural one to
consider [1, 2, 4]. Giving up the probabilistic quantification of uncertainty has led to give up the EU criterion as well. The development of possibilistic decision
theory has lead a series of possibilistic counterparts
of the EU criterion. [15] for instance advocates the
use of possibilistic Choquet integrals, which relies on
a numerical interpretation of both possibility and utility degrees. On the contrary, [4] have studied the case
of a qualitative interpretation and propose two criteria
based on possibility theory, an optimistic and a pessimistic one (denoted by Uopt and Upes ), whose definitions only require a finite ordinal, non compensatory,
scale for evaluating both utility and plausibility.
The axiomatization of Uopt and Upes yielded the development of sophisticated qualitative models for sequential decision making, e.g. possibilistic Markov decision

processes [16, 17], possibilistic ordinal decision trees
[5] and even possibilistic ordinal influence diagrams.
One of the most interesting properties of this qualitative model is indeed that it obeys a weak form of the
monotonicity property. As a consequence, dynamic
programming may be used and a strategy optimal with
respect to Upes or Uopt can be built in polytime.
On the contrary general Choquet integrals are incompatible with dynamic programming. Worst, the problem of determining a strategy optimal with respect to
Choquet integrals is NP-Hard in the general case [10].
We show in the present paper that the problem of determining a strategy optimal with respect to a possibilistic Choquet integrals is NP-Hard as well. More
generally, we propose a study of the complexity of the
problem of finding an optimal strategy for possibilistic
decision trees: which criteria obey the monotonicity
property (and then may be solved in polytime thanks
to dynamic programming) and which ones lead to NPHard problems?
This paper is organized as follows: Section 2 presents a
refresher on possibilistic decision making under uncertainty and especially a survey on most common possibilistic decision criteria. Section 3 details possibilistic
decision trees. Section 4 is devoted to the complexity study regarding different decision criteria. Finally,
Section 5 presents an extension to order of magnitude expected utility. Proofs are omitted for space
reasons but are available at ftp://ftp.irit.fr/IRIT/
ADRIA/PapersFargier/uai11.pdf.

2
2.1

Background
Possibilitic lotteries

The basic building block in possibility theory is the
notion of possibility distribution [3]. Let x1 , . . . , xn
be a set of state variables whose value are ill-known,
D1 . . . Dn their respective domains and denote Ω =
D1 × · · · × Dn the joint domain of x1 , . . . , xn . Vectors
ω ∈ Ω are often called realizations or simply ”states”
(of the world). The agent’s knowledge about the value
of xi ’s is by a possibility distribution π : Ω → [0, 1];
π(ω) = 1 means that ω is totally possible and π(ω) = 0
means that ω is an impossible state. It is generally
assumed that there exist at least one state ω which is
totally possible, i.e. that π is normalized.
Extreme cases of knowledge are presented by complete
knowledge i.e. ∃ω0 s.t. π(ω0 ) = 1 and ∀ω 6= ω0 , π(ω) =
0 and total ignorance i.e. ∀ω ∈ Ω, π(ω) = 1 (all values
in Ω are possible). From π, one can compute the possibility Π(A) and necessity N (A) of an event A ⊆ Ω:
Π(A) = sup π(ω).
ω∈A

(1)

N (A) = 1 − Π(Ā) = 1 − sup π(ω).

(2)

ω ∈A
/

Π(A) evaluates to which extend A is consistent with
the knowledge represented by π; N (A) corresponds to
the extent to which ¬A is impossible: it evaluates at
which level A is implied by the knowledge.
In possibility theory, conditioning is defined by the
following counterpart of the Bayesian rule:
∀ω, π(ω) = min(π(ω | ψ), Π(ψ)).

(3)

In this equation, π(ω | ψ) and Π(ψ) are combined
according to a min operation, according to the ordinal
interpretation of the possibilistic scale1 . The following
min-based definition of the conditioning corresponds
to the least specific solution of (3) (see [9]):

if π(ω) = Π(ψ) and ω ∈ ψ
 1
π(ω) if π(ω) < Π(ψ) and ω ∈ ψ
π(ω|ψ) =

0
otherwise
(4)
Following [2, 4], a decision can be seen as a possibility
distribution over its outcomes. In a single stage
problem, a utility function maps each outcome to a
utility value in a totally ordered set U = {u1 , . . . , un }
(we assume without loss of generality that u1 ≤ · · · ≤
un ). This function models the attractiveness of each
outcome for the decision maker. An act can then
be represented by a possibility distribution on U ,
called a (simple) possibilistic lottery, and denoted by
hλ1 /u1 , . . . , λn /un i: λi = π(ui ) is the possibility that
the decision leads to an outcome of utility ui .
In the following, L denotes the set of simple lotteries (i.e. the set of possibility distributions over U ).
A possibilistic compound lottery hλ1 /L1 , . . . , λm /Lm i
(also denoted by (λ1 ∧ L1 ∨ · · · ∨ λm ∧ Lm )) is a possibility distribution over a subset of L. The possibility
πi,j of getting a utility degree uj ∈ U from one of its
sub−lotteries Li depends on the possibility λi of getting Li and on the conditional possibility λij = π(uj |
Li ) of getting uj from Li i.e. πi,j = min(λj , λij ) by
equation (3). Hence, the possibility of getting uj from
a compound lottery hλ1 /L1 , . . . , λm /Lm i is the max,
over all Li , of πi,j . Thus, [2, 4] have proposed to reduce
(λ1 /L1 , . . . , λm /Lm ) into a simple lottery, denoted by
Reduction(hλ1 /L1 , . . . , λm /Lm i), that is considered as
equivalent to the compound one:
Reduction(hλ1 /L1 , . . . , λm /Lm i)
= h max min(λj , λj1 )/u1 , . . . , max min(λj , λjn )/un i.
j=1..m

j=1..m

(5)
1

The other, numerical interpretation of possibility theory is the use a product instead of a min operation, but
this is out the scope of the present study.

Obviously, the reduction of a simple lottery is the
simple lottery itself. Since min and max are polynomial operations, the reduction of a compound lottery
is polynomial in the size of the compound lottery 2 .
We review in the following the different criteria that
have been proposed to evaluate and/or compare (simple) lotteries; thanks to the notion of reduction, they
also apply to compound lotteries: to evaluate/compare
compound lotteries, simply reduce each to an equivalent simple one; then use one of the criteria proposed
for the evaluation/the comparison of simple lotteries.
Formally, any comparison criterion O, i.e. any preference order ≥O defined on simple lotteries can be extended to compound lotteries as follows:
L ≥O L′ ⇐⇒ Reduction(L) ≥O Reduction(L′ ). (6)

totally ordered:

hu, ui b hv, vi ⇐⇒


u = v = 1 and u ≤ v




 or
u ≥ v and u = v = 1


 or


u = v = 1 and v < 1

(9)

Each ui = hui , ui i in the utility scale is thus understood as a small lottery hui /⊤, ui /⊥i. A lottery
hλ1 /u1 , . . . , λn /un i can be viewed as a compound lottery, and its P U utility is computed by reduction:
P U (hλ1 /u1 , . . . , λn /un i)
= Reduction(λ1 /hu1 /⊤, u1 /⊥i, . . . , λn ∧ hun /⊤, un /⊥i)
= h max (min(λj , uj ))/⊤, max (min(λj , uj ))/⊥i
j=1..n

j=1..n

(10)
2.2

Possibilistic qualitative utilities
(Upes , Uopt , P U )

Under the assumption that the utility scale and the
possibility scale are commensurate and purely ordinal,
[4] have proposed the following qualitative pessimistic
and optimistic utility degrees for evaluating any simple
lottery L = hλ1 /u1 , . . . , λn /un i (possibly issued from
the reduction of a compound lottery):
Upes (L) = max min(ui , N (L ≥ ui )).

(7)

Uopt (L) = max min(ui , Π(L ≥ ui )).

(8)

i=1..n

i=1..n

where N (L ≥ ui ) = 1 − Π(L < ui ) = 1 − max λj
j=1,i−1

and Π(L ≥ ui ) = max λj are the necessity and posj=1..n

sibility degree that L reaches at least the utility value
ui . Upes generalizes the Wald criterion and estimates
to what extend it is certain (i.e. necessary according
to measure N ) that L reaches a good utility. Its optimistic counterpart, Uopt , estimates to what extend
it is possible that L reaches a good utility. Because
decision makers are rather cautious than adventurous,
the former is generally preferred to the latter.
Claiming that the lotteries that realize in the best prize
or in the worst prize play an important role in decision
making, Giang and Shenoy [7] have proposed a bipolar
model in which the utility of an outcome is a pair u =
hu, ui where max(u, u) = 1: the utility is binary in
this sense that u is interpreted as the possibility of
getting the ideal, good reward (denoted ⊤) and u is
interpreted as the possibility of getting the anti ideal,
bad reward (denoted ⊥).
Because of the normalization constraint max(u, u) =
1, the set U = {hu, ui ∈ [0, 1]2 , max(λ, µ) = 1} is
2

The size of a simple lottery is the number of its outcomes; the size of a compound lottery is the sum of the sizes
of its sub-lotteries plus the number of its sub-lotteries.

We thus get, for any lottery L a binary utility
P U (L) = hu, ui in U . Lotteries can then be compared
according to Equation (9):
L ≥P U L′ ⇐⇒ Reduction(L) b Reduction(L′ ).
(11)
In [8] Giang and Shenoy show that the order induced by PU collapse with the one induced by Uopt
whenever for any lottery, the possibility u of getting
the worst utility is equal to 1 (any ”compound” lottery λ1 /h0, α1 i, . . . , λn /h0, αn i reduces to the lottery
h1, max min(λi , αi )i : max min(λi , αi ) is precisely
i=1..n

i=1..n

the optimistic utility value). In the same way, Giang
and Shenoy have shown that the order induced on the
lotteries by PU collapse with the one induced by Upes
as soon as for any lottery, the possibility u of getting
the best utility is equal to 1. We shall thus say that
PU captures Uopt and Upes as particular cases.
2.3

Possibilistic likely dominance (LN, LΠ)

When the scales evaluating the utility and the possibility of the outcomes are not commensurate, [1, 6] propose to prefer, among two possibilistic decisions, the
one that is more likely to overtake the other. Such a
rule does not assign a global utility degree to the decisions, but draws a pairwise comparison. Although designed on a Savage-like framework rather than on lotteries, it can be translated on lotteries. This rule states
that given two lotteries L1 = hλ11 /u11 , . . . , λ1n /u1n i and
L2 = hλ21 /u21 , . . . , λ2n /u2n >, L1 is as least as good as
L2 as soon as the likelihood (here, the necessity or the
possibility) of the event the utility of L1 is as least as
good as the utility of L2 is greater or equal to the likelihood of the event the utility of L2 is as least as good
as the utility of L1 . Formally:
L1 ≥LN L2 ⇐⇒ N (L1 ≥ L2 ) ≥ N (L2 ≥ L1 ), (12)

L1 ≥LΠ L2 ⇐⇒ Π(L1 ≥ L2 ) ≥ Π(L2 ≥ L1 )

(13)

where Π(L1 ≥ L2 ) = supu1i ,u2i s.t. u1i ≥u2i min(λ1i , λ2i )
and N (L1 ≥ L2 ) = 1−supu1i ,u2i s.t. u1i <u2i min(λ1i , λ2i ).
The preference order induced on the lotteries is not
transitive, but only quasitransitive: obviously L1 >N
L2 and L2 >LN L3 implies L1 >LN L3 (resp. L1 >LΠ
L2 and L2 >LΠ L3 implies L1 >LΠ L3 ) but it may
happen that L1 ∼LN L2 , L2 ∼LN L3 (resp. L1 ∼LΠ
L2 , L2 ∼LΠ L3 ) and L1 >LN L3 (resp. L1 >LΠ L3 ).
2.4

Proposition 2. Let L1 , L2 be two lotteries such
that
max ui ≤
max ui .
It holds that
ui ∈L2 ,λi >0

ui ∈L1 ,λi >0

ChN (Reduction(h1/L1 , 1/L2 i)) ≤ ChN (L1 ).
No such property holds for ChΠ , as shown by the following counter example:
Counter Example 1. Let U = {0, 1, . . . , 9},
L1 = h0.2/0, 1/2, 0.5/9i, L2 = h0.4/4, 1/7i4
and
L3
=
Reduction(h1/L1 , 1/L2 i)
=
h0.2/0, 1/2, 0.4/4, 1/7, 0.5/9i.
We can check that
ChΠ (L1 ) = 5.5 and ChΠ (L3 ) = 8.

Possibilistic Choquet integrals

In presence of heterogeneous information, i.e. when
the knowledge about the state of the world is possibilistic while the utility degrees are numerical and
compensatory the previous models cannot be applied
anymore. Following [18] Choquet integrals appear as
a right way to extend expected utility to non Bayesian
models. Like the EU model, this model is a numerical,
compensatory, way of aggregating uncertain utilities.
But it does not necessarily resort on a Bayesian modeling of uncertain knowledge. Indeed, this approach
allows the use of any monotonic set function 3 , and
thus of a necessity measure.
As the qualitative case, but assuming that the utility degrees have a richer, cardinal interpretation, the
utility of L is given by a Choquet integrals:
Chµ (L) = Σi=1,n (ui − ui−1 ) . µ(L ≥ ui ).

(14)

If µ is a probability measure then Chµ (L) is simply
the expected utility of L. In the present paper, we are
interested in studying the possibilistic framework for
decision making: for cautious (resp. adventurous) decision makers, the capacity µ is the necessity measure
N (resp. the possibility measure Π):
ChN (L) = Σi=1,n (ui − ui−1 ) . N (L ≥ ui ).

(15)

ChΠ (L) = Σi=1,n (ui − ui−1 ) . Π(L ≥ ui ).

(16)

From Equations (5) and (15), it can be shown that:
Proposition 1.
Given a lottery L = hλ1 /u1 , . . . , λn /un i, an utility ui s.t. ui ≤
max uj and a lottery L′ =
uj ∈L,λj >0
hλ′1 /u1 , . . . , λ′n /un i s.t. λ′i ≥ λi and
it holds that ChN (L′ ) ≤ ChN (L).

∀j 6= i, λ′j = λj ,

This emphasizes the pessimistic character of ChN :
adding to a lottery any consequence that is not better than its best one decreases its evaluation. As a
consequence, we get the following result
3
This kind of set function is often called capacity or
fuzzy measure.

3

Possibilistic decision trees

Decision trees are graphical representations of sequential decision problems under the assumption of full observability. A decision tree is a tree T = (N , E) whose
set of nodes, N , contains three kinds of nodes:
• D = {D0 , . . . , Dm } is the set of decision nodes
(represented by rectangles). The labeling of the
nodes is supposed to be in accordance with the
temporal order i.e. if Di is a descendant of Dj ,
then i > j. The root node of the tree is necessarily
a decision node, denoted by D0 .
• LN = {LN1 , . . . , LNk } is the set of leaves, also
called utility leaves: ∀LNi ∈ LN , u(LNi ) is the
utility of being eventually in node LNi . For the
sake of simplicity we assume that only leave nodes
lead to utilities.
• C = {C1 , . . . , Cn } is the set of chance nodes
represented by circles. For any Xi ∈ N , let
Succ(Xi ) ⊆ N be the set of its children. For
any Di ∈ D, Succ(Di ) ⊆ C: Succ(Di ) is the set of
actions that can be decided when Di is observed.
For any Ci ∈ C, Succ(Ci ) ⊆ LN ∪ D: Succ(Ci ) is
the set of outcomes of the action Ci - either a leaf
node is observed, or a decision node is observed
(and then a new action should be executed).
The size of T is its number of edges (the number of
nodes is equal to the number of edges plus 1).
In classical, probabilistic, decision trees the uncertainty pertaining to the possible outcomes of each
Ci ∈ C, is represented by a conditional probability distribution pi on Succ(Ci ), such that ∀N ∈ Succ(Ci ),
pi (N ) = P (N |path(Ci )) where path(Ci ) denotes all
the value assignments to chance and decision nodes
on the path from the root node to Ci . In this work,
4
For the sake of simplicity, we shall forget about the
utility degrees that receive a possibility degree equal to
0 in a lottery, i.e. we write h0.2/0, 1/2, 0.5/9i instead of
h0.2/0, 1/2, 0/3, 0/4, 0/5, 0/6, 0/7, 0/8, 0.5/9i.






























lottery and can be reduced to an equivalent simple one.
Formally, the composition of lotteries will be applied
from the leafs of the strategy to its root, according to
the following recursive definition for any Ni in N :

L(δ(Ni ), δ) if Ni ∈ D


Reduction(hπi (Xj )/L(Xj , δ)Xj ∈Succ(Ni ) i)
L(Ni , δ) =

 if Ni ∈ C
h1/u(Ni )i if Ni ∈ LN
(17)




























Figure 1: Example of possibilistic decision tree with
C = {C1 , C2 , C3 , C4 , C5 , C6 }, D = {D0 , D1 , D2 } and
LN = U = {0, 1, 2, 3, 4, 5}.

we obviously use a possibilistic labeling (see Figure 1).
The difference with probabilistic decision trees is that
the chance nodes are viewed as possibilistic lotteries.
More precisely, for any Ci ∈ C, the uncertainty pertaining to the more or less possible outcomes of each Ci
is represented by a conditional possibility distribution
πi on Succ(Ci ), such that ∀N ∈ Succ(Ci ), πi (N ) =
Π(N |path(Ci )).
Solving the decision tree amounts at building a strategy
that selects an action (i.e. a chance node) for each
reachable decision node. Formally, we define a strategy
as a function δ from D to C ∪ {⊥}. δ(Di ) is the action
to be executed when a decision node Di is observed.
δ(Di ) = ⊥ means that no action has been selected
for Di (because either Di cannot be reached or the
strategy is partially defined). Admissible strategies
must be:
- sound : ∀Di ∈ D, δ(Di ) ∈ Succ(Di ) ∪ {⊥}.
- complete: (i) δ(D0 ) 6= ⊥ and (ii) ∀Di s.t. δ(Di ) 6=
⊥, ∀N ∈ Succ(δ(Di )), either δ(N ) 6= ⊥ or N ∈ LN .
Let ∆ be the set of sound and complete strategies that
can be built from T . Any strategy in ∆ can be viewed
as a connected subtree of T whose edges are of the
form (Di , δ(Di )). The size of a strategy δ is the sum
of its number of nodes and edges, it is obviously lower
than the size of the decision tree.
Strategies can be evaluated and compared thanks to
the notion of lottery reduction. Recall indeed that leaf
nodes in LN are labeled with utility degrees. Then a
chance node can be seen as a simple lottery (for the
most right chance nodes) or as a compound lottery (for
the inner chance nodes). Each strategy is a compound

Equation (17) is simply the adaptation to strategies of
lottery reduction (Equation (5)). We can then compute Reduction(δ) = L(D0 , δ): Reduction(δ)(ui ) is
simply the possibility of getting utility ui when δ is
applied from D0 . Since, the operators max and min
are polytime Equation (17) define a polytime computation of the reduced lottery.
Proposition 3. For any strategy δ in ∆, an equivalent
simple possibilistic lottery can be computed in polytime.
We are now in position to compare strategies, and thus
to define the notion of optimality. Let O be one of the
criteria defined in Section 2 (i.e. depending on the
application, ≥O is either ≥LΠ , or ≥LN , or the order
induced by Upes , or by Uopt , etc.). A strategy δ ∈ ∆,
is said to be optimal w.r.t. ≥O iff:
∀δ ′ ∈ ∆, Reduction(δ) ≥O Reduction(δ ′ ).

(18)

Notice that this definition does not require the full
transitivity (nor the completeness) of ≥O and is meaningful as soon as the strict part of ≥O , >O , is be transitive. This means that it is applicable to the preference
relations that rely on the comparison of global utilities
(qualitative utilities, binary utility, Choquet integrals)
but also to ≥LN and ≥LΠ . We show in the following
that the complexity of the problem of optimization depends on the criterion at work.

4

On the complexity of decision
making in possibilistic decision trees

Finding optimal strategies via an exhaustive enumeration of ∆ is a highly computational task. For instance,
in a decision tree with n decision nodes and a branching factor√equal to 2, the number of potential strategies
is in O(2 n ). For standard probabilistic decision trees,
where the goal is to maximize expected utility, an optimal strategy can be computed in polytime thanks to
an algorithm of dynamic programming which builds
the best strategy backwards, optimizing the decisions
from the leaves of the tree to its root.
Regarding possibilistic decision trees, [5] shows that
such a method can also be used to get a strategy max-

imizing Upes and Uopt . The reason is that like EU, Upes
satisfies the key property of weak monotonicity. We
formulate it for any criterion O over possibilistic lotteries: ≥O is said to be weakly monotonic iff whatever
L, L′ and L”, whatever (α,β) such that max(α, β) = 1:
L ≥O L′ ⇒ (α ∧ L) ∨ (β ∧ L”) ≥O (α ∧ L′ ) ∨ (β ∧ L”).
(19)
This property states that the combination of L (resp.
L′ ) with L”, does not change the initial order induced
by O between L and L′ - this allows dynamic programming to decide in favor of L or L′ before considering
the compound decision. The principle of backwards
reasoning procedure is depicted in a recursive manner
by Algorithm 1 for any preference order ≥O among
lotteries. When each chance node is reached, an optimal sub-strategy is built for each of its children these sub-strategies are combined w.r.t. their possibility degrees, and the resulting compound strategy
is reduced: we get an equivalent simple lottery, representing the current optimal sub-strategy. When a
decision node X is reached, a decision Y ∗ leading to a
sub-strategy optimal w.r.t. ≥O is selected among all
the possible decisions Y ∈ Succ(X), by comparing the
simple lotteries equivalent to each sub strategies.
This procedure crosses each edge in the tree only once.
When the comparison of simple lotteries by ≥O (Line
(2)) and the reduction operation on a 2-level lottery
(Line (1)) can be performed in polytime, its complexity is polynomial w.r.t. the size of the tree. Then:
Proposition 4. If ≥O satisfies the monotonicity property, then Algorithm 1 computes a strategy optimal
w.r.t. O in polytime.
We will see in the following that, beyond Upes and Uopt
criteria, several other criteria satisfy the monotonicity
property and that their optimization can be managed
in polytime by dynamic programming. The possibilistic Choquet integrals, on the contrary, do not satisfy
weak monotonicity; we will show that they lead to NPComplete decision problems. Formally, for any of the
optimization criteria of Sections 2.2 to 2.4, the corresponding decision problem can be defined as follows:
[DT-OPT-O] (Strategy optimization w.r.t. an optimization criterion O in possibilistic decision trees)
INSTANCE: A possibilistic decision tree T , a level α.
QUESTION: Does there exist a strategy δ ∈ ∆ such
as Reduction(δ) ≥O α?
For instance DT-OPT-ChN corresponds to the optimization of the necessity-based Choquet integrals.
DT-OPT-Upes and DT-OPT-Uopt correspond to the
optimization of the possibilistic qualitative utilities
Upes and Uopt , respectively.

Algorithm 1: Dynamic programming
Data: In: a node X, In/Out: a strategy δ
Result: A lottery L
begin
for i ∈ {1, . . . , n} do L[ui ] ← 0
if N ∈ LN then L[u(N )] ← 1
if N ∈ C then
% Reduce the compound lottery
foreach Y ∈ Succ(N ) do
LY ← P rogDyn(Y, δ)
for i ∈ {1, . . . , n} do
L[ui ] ←
max(L[ui ], min(πN (Y ), Ly [ui ])) (Line (1))
if N ∈ D then
% Choose the best decision
Y ∗ ← Succ(N ).f irst
foreach Y ∈ Succ(N ) do
LY ← P rogDyn(Y, δ)
if LY >O LY ∗ then Y ∗ ← Y (Line (2))
δ(N ) ← Y ∗
L ← LY ∗
return L
end

4.1

Possibilistic qualitative utilities
(Upes , Uopt , P U )

Possibilistic qualitative utilities Upes and Uopt satisfy
the weak monotonicity principle. Although not referring to a classical, real-valued utility scale, but to a 2
dimensional scale, this is also the case of P U .
Proposition 5. ≥Upes , ≥Uopt and ≥P U satisfy the
weak monotonicity property.
As a consequence, dynamic programming applies to
the optimization of these criteria in possibilistic decision trees. Although not explicitly proved in the literature, Proposition 5 is common knowledge in possibilistic decision theory. It is also known that dynamic
programming applies to the optimization of Upes , Uopt
and P U in possibilistic Markov decision processes and
thus to decision trees (see [5, 14, 17]).
Corollary 1. DT-OPT-Upes , DT-OPT- Uopt and
DT-OPT-P U belong to P .
4.2

Possibilistic likely dominance (LN, LΠ)

Fortunately, the optimization of the possibilistic likely
dominance criteria also belongs to P. Indeed:
Proposition 6. ≥LΠ and ≥LN satisfy the weak
monotony principle
Algorithm 1 is thus sound and complete for ≥LΠ and
≥LN , and provides in polytime any possibilistic decision tree with a strategy optimal w.r.t. these criteria.
Corollary 2. DT − OP T − LN and DT − OP T − LΠ
belong to P .

It should be noticed that, contrarily to what can be
done with the three previous rules, the likely dominance comparison of two lotteries will be reduced to
a simple comparison of aggregated values (Line (2))
Anyway, since only one best strategy is looked for, the
transitivity of >LΠ (resp. >LΠ ) guarantees the correctness of the procedure - the non transitivity on the
indifference is not a handicap when only one among
the best strategies is looked for. The difficulty would
be raised if we were looking for all the best strategies.
4.3

Possibilistic Choquet integrals (ChN , ChΠ )

The situation is thus very confortable with qualitative
utilities, binary possibilistic utility and likely dominance. It is much lesser comfortable for the case of
numerical utilities, i.e. when the aim is to optimize
a possibilistic Choquet integral (either ChN or ChΠ ).
The point is that the possibilistic Choquet integrals
(as many other Choquet integrals) do no satisfy the
monotonicity principle:
Counter Example 2 ( From [11] ). Let L =
h0.2/0, 0.5/0.51, 1/1i, L′ = h0.1/0, 0.6/0.5, 1/1i and
L” = h0.01/0, 1/1i.
L1 = (α ∧ L) ∨ (β ∧ L”) and L2 = (α ∧ L′ ) ∨ (β ∧ L”),
with α = 0.55 and β = 1. Using Equation (5)
we have: Reduction(L1 ) = h0.2/0, 0.5/0.51, 1/1i and
Reduction(L2 ) = h0.1/0, 0.55/0.5, 1/1i.
Computing ChN (L) = 0.653 and ChN (L′ ) = 0.650 we
get L ≥ChN L′ . But ChN (Reduction(L1 )) = 0.653 <
ChN (Reduction(L3 )) = 0.675, i.e. (α ∧ L) ∨ (β ∧
L”) <ChN (α ∧ L′ ) ∨ (β ∧ L”): this contradicts the
monotonicity property.
Let L
=
h1/0, 0.5/0.51, 0.2/1i,
L′
=
h1/0, 0.6/0.5, 0.1/1i and L” = h1/0, 0.49/0.51i.
L1 = (α ∧ L) ∨ (β ∧ L”) and L2 = (α ∧ L′ ) ∨ (β ∧ L”),
with α = 1 and β = 0.55. Using Equation (5) we
have: Reduction(L1 ) = h1/0, 0.5/0.51, 0.2/1i and
Reduction(L2 ) = h1/0, 0.6/0.5, 0.49/0.51, 0.1/1i.
Computing ChΠ (L) = 0.353 and ChΠ (L′ ) = 0.350
we get L >ChΠ L′ . But ChΠ (Reduction(L1 )) =
0.3530 < ChΠ (Reduction(L2 )) = 0.3539, i.e.
(α ∧ L) ∨ (β ∧ L”) <ChΠ (α ∧ L′ ) ∨ (β ∧ L”): this
contradicts the monotonicity property.
Proposition 7. DT − OP T − ChN and DT − OP T −
ChΠ are NP-Complete.

5

Extension to Order of Magnitude
Expected Utility

Order of Magnitude Expected Utility theory relies on a
qualitative representation of beliefs, initially proposed
by Spohn [19], via Ordinal Conditional Functions, and
later popularized under the term kappa-rankings. κ :

2Ω → Z + ∪ {+∞} is a kappa-ranking if and only if:
S1 minω∈Ω κ({ω}) = 0
6 A ⊆ A, κ(∅) = +∞
S2 κ(A) = minω∈A κ({ω}) if ∅ =
Note that event A is more likely than event B if
and only if κ(A) < κ(B): kappa-rankings have been
termed as disbelief functions. They receive an interpretation in terms of order of magnitude of small probabilities. κ(A) = i is equivalent to P (A) is of the same
order of εi , for a given fixed infinitesimal ε. There
exists a close link between kappa-rankings and possibility measures, insofar as any kappa-ranking can be
represented by a possibility measure, and vice versa.
Order of magnitude utilities have been defined in the
same way [13, 20]. Namely, an order of magnitude
function µ : X → Z + ∪ {+∞} can be defined in order
to rank outcomes x ∈ X in terms of degrees of “dissatisfaction”. Once again, µ(x) < µ(x′ ) if and only
if x is more desirable than x′ , µ(x) = 0 for the most
desirable consequences, and µ(x) = +∞ for the least
desirable consequences. µ is interpreted as: µ(x) = i
is equivalent to say that the utility of x is of the same
order of εi , for a given fixed infinitesimal ε. An order
of magnitude expected utility (OMEU) model can then
be defined (see [13, 20] among others). Considering
an order of magnitude lottery L = hκ1 /µ1 , . . . , κn /µn i
as representing a some probabilistic lottery, it is possible to compute the order of magnitude of the expected utility of this probabilistic lottery: it is equal to
mini=1,n {κi + µi }. Hence the definition of the OMEU
value of a κ lottery L = hκ1 /µ1 , . . . , κn /µn i:
OM EU (L) = min {κi + ui }.
i=1,n

(20)

The preference relation ≥OM EU is thus defined as:
L ≥OM EU L′ ⇐⇒ OM EU (L) ≤ OM EU (L′ ). (21)
We shall now define kappa decision trees: for any
Ci ∈ C, the uncertainty pertaining to the more or less
possible outcomes of each Ci is represented by a kappa
degree κi (N ) = M agnitude(P (N |past(Ci ))), ∀N ∈
Succ(Ci ) (with the normalization condition that the
degree κ = 0 is given to at least one N in Succ(Ci )).
According to the interpretation of kappa ranking in
terms of order of magnitude of probabilities, the product of infinitesimal the conditional probabilities along
the paths lead to a sum of the kappa levels. Hence the
following principle of reduction of the kappa lotteries:
Reduction(κ1 ∧ L1 ∨ · · · ∨ κm ∧ Lm )
= h min (κj1 + κj )/u1 , . . . , min (κjn + κj )/un i.
j=1..m

(22)

j=1..m

The last result of this paper is that OMEU satisfies
the weak monotonicity principle:

Proposition 8. ∀L, L′ , L” ∈ L,
OM EU (L) ≥ OM EU (L′ )
⇒ OM EU ((α∧L)∨(β ∧L”)) ≥ OM EU ((α∧L′ )∨(β ∧L”))

As a consequence dynamic programming is sound and
complete for the optimization of Order of Magnitude
Expected Utility:
Corollary 3. There exists a polynomial algorithm for
finding a strategy optimal w.r.t. the Order of Magnitude Expected Utility for any kappa decision tree.

6

Conclusion

In this paper, we have shown that the strategy optimization in possibilistic decision trees is tractable
for most of the criteria, extending the results about
the qualitative utility criteria to other criteria, namely
the likely dominance rule. We have also shown that
the problem is intractable for the Choquet-based criteria. Finally, we have extended this work to OMEU,
defining a new model for sequential decision trees, extending the notion of reduction to kappa lotteries and
showing that this models obey the weak monotonicity principle. These results are summarized in Table
1. It should be noticed that the optimization of the
Table 1: Results about the complexity of DT − OP T .
Upes Uopt P U
ChN
ChΠ
LΠ LN OMEU
P
P
P NP-hard NP-hard P P
P
possibilistic Choquet integrals is ”only” NP-hard: the
computation of the Choquet value of a possibilistic
strategy is polynomial, whereas this computation can
be more costly for other capacity measures; for instance computing the Choquet value of a strategy on
the basis of its multi prior expected utility is itself a
NP-hard problem - and the corresponding optimization problem is probably beyond NP. So, it appears
that the use of possibilistic decision criteria does not
lead to an increase in complexity, except for Choquet
integrals. This is good news and allows the extension
of our work to possibilistic decision diagrams. Concerning the Choquet case, further work includes the
development of a direct evaluation algorithm for possibilistic influence diagrams where possibilistic Choquet
integrals are used as a decision criteria inspired by the
variable elimination approach.



Qualitative possibilistic networks, also
known as min-based possibilistic networks,
are important tools for handling uncertain
information in the possibility theory framework. Despite their importance, only the
junction tree adaptation has been proposed
for exact reasoning with such networks.
This paper explores alternative algorithms
using compilation techniques.
We first
propose possibilistic adaptations of standard
compilation-based probabilistic methods.
Then, we develop a new, purely possibilistic,
method based on the transformation of the
initial network into a possibilistic base. A
comparative study shows that this latter
performs better than the possibilistic adaptations of probabilistic methods. This result
is also confirmed by experimental results.

1

INTRODUCTION

In possibility theory there are two different ways to
define the counterpart of Bayesian networks. This is
due to the existence of two definitions of possibilistic
conditioning: product-based and min-based conditioning (Dubois and Prade, 1988). When we use the product form of conditioning, we get a possibilistic network
close to the probabilistic one sharing the same features
and having the same theoretical and practical results.
However, this is not the case with min-based networks.
In this paper, we are interested in the inference problem in multiply connected networks, which is known
as a hard problem (Cooper, 1990). More precisely,
we propose three compilation methods for min-based
possibilistic networks.
The compilation of Bayesian networks is always considered as an important area. Recently, researchers

Salem Benferhat
CRIL-CNRS
University of Artois
France, 62307
benferhat@cril.univ-artois.fr

Rolf Haenni
RISIS
Bern University
Switzerland, CH-2501
rolf.haenni@bfh.ch

have been interested in various kinds of exact and approximate Bayesian networks inference algorithms using compilation techniques (Darwiche, 2003) (Chavira
and Darwiche, 2005) (Wachter and Haenni, 2007), etc.
Despite the importance of possibility theory, there is
no compilation that has been proposed for possibilistic networks. This paper analyzes this issue by first
adapting well-known compilation-based probabilistic
inference approaches, namely the arithmetic circuit
method (Darwiche, 2003) and the logical compilation
of Bayesian Networks (Wachter and Haenni, 2007).
Both of them are based on a network’s encoding into a
logical representation and a compilation into a target
compilation language, namely Π-DNNF. From there,
all possible queries are answered in polynomial time.
The third method exploits results obtained on one
hand in (Benferhat et al., 2002) that transforms a minbased possibilistic network into a possibilistic knowledge base, and on the other hand results obtained regarding compilation of possibilistic bases (Benferhat
et al., 2007) in order to assure inference in polytime.
This method that is purely possibilistic is flexible since
it permits to exploit efficiently all the existing propositional compilers.
The rest of this paper is organized as follows: Section 2 gives a briefly background on possibility theory, possibilistic logic, possibilistic networks and introduces some compilation concepts. Section 3 is dedicated to possibilistic adaptations of compilation-based
probabilistic inference methods. Section 4 presents a
new inference method in possibilistic networks using
compiled possibilistic knowledge bases. Experimental
study is presented in Section 5.

2
2.1

BASIC CONCEPTS
POSSIBILITY THEORY

This subsection briefly recalls some elements of possibility theory, for more details we refer to (Dubois and
Prade, 1988). Let V = {X1 , X2 , ..., XN } be a set of

variables. We denote by DXi = {x1 , .., xn } the domain
associated with the variable Xi . By xi we denote any
instance of Xi . Ω denotes the universe of discourse,
which is the Cartesian product of all variable domains
in V . Each element ω ∈ Ω is called a state of Ω.
The notion of possibility distribution denoted by π is
a mapping from the universe of discourse to the unit
interval [0, 1]. To this scale, two interpretations can be
attributed, a quantitative one when values have a real
sense and a qualitative one when values reflect only an
order between the different states of the world. This
paper focuses on the qualitative interpretation of possibility theory.
Given a possibility distribution π, we can define a mapping grading the possibility measure of an event φ ⊆ Ω
by Π(φ) = maxω∈φ π(ω). Π has a dual measure which
is the necessity measure N (φ) = 1 − Π(¬φ).
Conditioning consists in modifying our initial knowledge, encoded by a possibility distribution π, by the
arrival of a new certain piece of information φ ⊆ Ω.
The qualitative interpretation of the scale [0, 1] leads
to the well known definition of min-conditioning (Hisdal, 1978), (Dubois and Prade, 1988):

Π(ψ ∧ φ) if Π(ψ ∧ φ) < Π(φ)
Π(ψ | φ) =
(1)
1
otherwise
2.2

POSSIBILISTIC LOGIC

Possibilistic logic (Dubois et al., 1994) handles qualitative uncertainty in a logical setting. A possibilistic
logic formula is a pair (p, a) where p is a propositional
formula and a its uncertainty degree which estimates
to what extent it is certain that p is true. The higher
is the weight, the more certain is the formula. A possibilistic knowledge base Σ is made up of a finite set
of weighted formulas, i.e.,
Σ = {(pi , ai ), i = 1, .., n}

(2)

where ai is the lower bound on N (pi ).
Each possibilistic knowledge base induces a unique
possibility distribution such that ∀ ω ∈ Ω and ∀
(pi , ai ) ∈ Σ:

1
if ω |= pi
πΣ (ω) =
(3)
1 − max {ai : ω 2 pi } otherwise
where |= is propositional logic entailment.
2.3

POSSIBILISTIC NETWORKS

A min-based possibilistic network over a set of variables V , denoted by ΠGmin is composed of:
- a graphical component that is a DAG (Directed

Acyclic Graph) where nodes represent variables and
edges encode the links between the variables. The
parent set of a node Xi is denoted by Ui =
{Ui1 , Ui2 , ..., Uim }. For any ui of Ui we have ui =
{ui1 , ui2 , ..., uim } where m is the number of parents of
Xi . In what follows, we use xi , ui , uij to denote, respectively, possible instances of Xi , Ui and Uij .
- a numerical component that quantifies different links.
For every root node Xi (Ui = ∅), uncertainty is represented by the a priori possibility degree Π(xi ) of each
instance xi ∈ DXi , such that maxxi Π(xi ) = 1. For the
rest of the nodes (Ui 6= ∅) uncertainty is represented
by the conditional possibility degree Π(xi |ui ) of each
instances xi ∈ DXi and ui ∈ DUi . These conditional
distributions satisfy the following normalization condition: maxxi Π(xi |ui ) = 1, for any ui .
The set of a priori and conditional possibility degrees
in a min-based possibilistic network induce a unique
joint possibility distribution defined by the following
chain rule:
πmin (X1 , .., XN ) = min

i=1..N

2.4

Π(Xi | Ui )

(4)

COMPILATION CONCEPTS

A target compilation language is a class of formulas
which is tractable for a set of transformations and
queries. Compilation languages are compared in terms
of their spatial efficiency via the succinctness criteria and also in terms of the set of logical queries and
transformations they support in polynomial time (see
(Darwiche and Marquis, 2002) for more details).
Within the most effective target compilation languages, we cite the Decomposable Negation Normal
Form (DNNF) (Darwiche, 2001). This language is universal and presents a number of properties (determinism, smoothness, etc.) that makes it of a great interest.
It supports a rich set of polynomial-time logical operations. To define DNNF, the starting point is Negation
Normal Form (NNF) which is a set of propositional
formulas where possible connectives are conjunctions,
disjunctions and negations. A set of important properties may be imposed to NNF, such that:
- Decomposability: the conjuncts of any conjunction in
NNF do not share variables.
- Determinism: two disjuncts of any disjunction in
NNF are logically contradictory.
- Smoothness: the disjunct of any disjunction in NNF
mentions the same variables.
These properties lead to a number of interesting subsets of NNF. Within these subsets, the language
DNNF (Darwiche, 2001) is one of the most effective
target compilation languages that supports the decomposability. We can also mention, the d-DNNF sat-

isfying determinism, sd-DNNF satisfying smoothness
and determinism, etc. Each compilation language supports some queries and transformations in polynomial
time. In what follows we are in particular interested
by conditioning and forgetting transformations (Darwiche and Marquis, 2002).

∨ and ∧ as max and min operators, respectively). A
sentence in Π-sd-DNNF is a sentence in Π-DNNF satisfying decomposability, determinism and smoothness.

3

In (Darwiche, 2003), authors have focused on inference in compiled Bayesian networks. The main idea is
based on representing the network using a polynomial
and then retrieving answers to probabilistic queries by
evaluating and differentiating the polynomial. This
latter itself is exponential in size, so it has been represented efficiently using an arithmetic circuit that can
be evaluated and differentiated in time and space linear in the circuit size. In what follows, we propose
a direct adaptation of this method in the possibilistic setting. Given a min-based possibilistic network,
we first encode it using a possibilistic function fmin
defined by two types of variables:

POSSIBILISTIC ADAPTATIONS
OF COMPILATION-BASED
PROBABILISTIC INFERENCE
METHODS

There are several compilation methods which handle
the inference problem in probabilistic graphical models. In this section, we first propose an adaptation
of the arithmetic circuit method of (Darwiche, 2003).
Then we will study one of its variants proposed in
(Wachter and Haenni, 2007), namely the logical compilation of Bayesian Networks.
DNNF has been introduced for propositional language.
Recall that in qualitative possibility theory, we basically manipulate two main operators Max and Min.
These operators fully make sense when we deal with
qualitative plausibility ordering. Therefore, we propose to define concepts of Π-DNNF (resp. Π-d-DNNF,
Π-sd-DNNF) as adaptations of the DNNF language
(resp. d-DNNF, sd-DNNF) (Darwiche, 2001) in the
possibilistic setting (definition 1).
Definition 1. A sentence in Π-DNNF is a rooted
DAG where each leaf node is labeled with true, false
or variable’s instances and each internal node is labeled with max or min operators and can have arbitrarily several children. Roughly speaking, Π-DNNF is
the same as the classical DNNF although its operators
are max and min instead of ∨ and ∧, respectively.
Example 1. Figure 1 depicts a sentence in Π-DNNF.
Consider the Min-node (root) in this figure. This node has
two children, the first contains variables A, B while the
second contains variables C, D. This node is decomposable
since its two children do not share variables.

3.1

INFERENCE USING POSSIBILISTIC
CIRCUITS

• Evidence indicators: for each variable Xi in the
network , we have a variable λxi for each instance
xi ∈ DXi .
• Network parameters: for each variable Xi and its
parents Ui in the network, we have a variable
θxi |ui for each instance xi ∈ DXi and ui ∈ DUi .
fmin = max
x

min
(xi ,ui )∼x

λxi θxi |ui

(5)

where x represents instantiations of all network variables and ui ∼ x denotes the compatibility relationship among ui and x. The possibilistic function fmin
of a possibilistic network represents the possibility distribution and allows to compute possibility degrees of
variables of interest. Namely, for any piece of evidence
e which is an instantiation of some variables E in the
network, we can instantiate fmin as it returns the possibility of e, Π(e) (Definition 2 and Proposition 1).
Definition 2. The value of the possibilistic function
fmin at evidence e, denoted by fmin (e), is the result of
replacing each evidence indicator λxi in fmin with 1 if
xi is consistent with e, and with 0 otherwise.
Proposition 1. Let ΠGmin be a possibilistic network
representing the possibility distribution π and having
the possibilistic function fmin . For any evidence e, we
have fmin (e) = π(e).

Figure 1: A sentence in Π-DNNF.

A sentence in Π-d-DNNF is a sentence in Π-DNNF
satisfying decomposability and determinism (viewing

Let figure 2 be the min-based possibilistic network
used throughout the paper.
The possibilistic function of the network in figure 2
has 8 terms corresponding to the 8 instantiations of
variables F, B, D. Two of these terms are as follows:

is outlined by algorithm 1. Note that the suffix P F is
added to signify that this method uses a possibilistic
function (fmin ) before ensuring the CNF encoding.
Algorithm 1: Inference using Π-DNNF (Π-DNNFP F )

Figure 2: Example of ΠGmin .

fmin = max(min(λd1 , λf1 , λb1 , θd1 |f1 ,b1 , θf1 , θb1 ); min
(λd1 , λf2 , λb1 , θd1 |f2 ,b1 , θf2 , θb1 ); · · · )
If the evidence e = (d1 , b1 ) then fmin (d1 , b1 ) is obtained by applying the following substitutions to fmin :
λd1 = 1, λd2 = 0, λb1 = 1, λb2 = 0, λf1 = λf2 = 1. This
leads to Π(e) = 0.7.
The possibilistic function fmin is then encoded on a
propositional theory (CNF) using λxi and θxi |ui . For
each network variable Xi , the encoding contains the
following clauses:
λxi ∨ λxj
(6)
¬λxi ∨ ¬λxj , i 6= j

(7)

Moreover, for each propositional variable θxi |ui , the
encoding contains the clause:
λxi ∧ λui1 ∧ . . . ∧ λuim ↔ θxi |ui

(8)

The CNF encoding, denoted by Kfmin recovers the
min-joint possibility distribution (proposition 2).
Proposition 2. The CNF encoding Kfmin of a possibilistic network encodes the joint distribution of given
network.
Once the CNF encoding is accomplished, it is then
compiled into a Π-DNNF, from which we extract the
possibilistic circuit ζp (definition 3) that implements
the encoded fmin .
Definition 3. A possibilistic circuit ζp encoded by a
Π-DNNF sentence ξ c is a DAG in which leaf nodes
correspond to circuit inputs, internal nodes correspond
to max and min operators, and the root corresponds to
the circuit output.
As in the probabilistic case (Darwiche, 2003), this circuit can be used for linear-time inference. More precisely, computing the possibility degree of an event
consists on evaluating ζp by setting each evidence indicator λx to 1 if the event is consistent with x, to 0
otherwise and applying operators in a bottom-up way.
This possibility degree corresponds exactly to the one
computed from the min-joint possibility distribution
(proposition 3). This method referred to Π-DNNFP F

Data: ΠGmin , instance of interest x, evidence e
Result: Π(x|e)
begin
Compilation into Π-DNNF
Encode ΠGmin into fmin using equation 5
EncodeCNF of ΠGmin into ξ using equations 6, 7, 8
Compile ξ into ξ c
ζp ← Possibilistic Circuit of ξ c
Inference
Applying Operators on ζp
Π(x, e) ← Root Value (ζp ; (x,e))
Π(e) ← Root Value (ζp ; e)
if Π(x, e) ≺ Π(e) then Π(x|e) ← Π(x, e)
else Π(x|e) ← 1
return Π(x|e)
end

Proposition 3. Let ΠGmin be a possibilistic network.
Let πmin be a joint distribution obtained by chain rule.
Then for any a ∈ Da and e ∈ DE , we have Π(A =
a|E = e) = Πmin (A = a|E = e) where Πmin (A =
a|E = e) is obtained from πmin using equation 1 and
Π(A = a|E = e) is obtained from algorithm 1.
The key point to observe here is that this approach
can handle possibilistic circuits of manageable size as
in the probabilistic case since some possibility values
may have some specific values; for instance, whether
they are equal to 0 or 1, and whether some possibilities are equal. In this case, we can say that the
network exhibit some local structure. By exploiting
it, the produced circuits can be smaller. In fact, the
normalization constraint relative to the initial network
will mean that we will have several values equal to 1.
Thus the idea is to make an advantage from such a
local structure which has a particular behavior with
the max operator in order to construct more compact
possibilistic circuits w.r.t. standard ones as stated by
the following proposition:
Proposition 4. Let N bposs and N bproba be the number of clauses in the possibilistic and probabilistic
cases, respectively. Then N bposs ≤ N bproba .
Note that for particular situations where probability
values are 1 or 0, we have N bposs = N bproba , otherwise
N bposs ≺ N bproba .
Example 2. To illustrate algorithm 1 we will consider
the min-based possibilistic network represented in figure 2.
We are looking for Π(f2 |d1 ) with f2 as instance of interest and d1 as evidence. First, we encode the network as
a possibilistic function and encode it on CNF. This latter is then compiled into Π-DNNF from which a possibilistic circuit is extracted. The possibility degree Π(f2 |d1 ) is
computed using this circuit in polynomial time. For instance, Π(f2 , d1 ) is computed using ζp by just replacing

λf2 = λd1 = λb1 = λb2 = 1 and applying possibilistic
operators in a bottom-up way as shown in figure 3. Hence,
Π(f2 |d1 ) = Π(f2 , d1 ) = 0.4 since Π(f2 , d1 ) = 0.4 ≺ 1.

from a function fψ encoding the CNF. Then, we have
πmin (xi , ..., xj ) = Π(xi , ..., xj ), i.e. fψ recovers the
min-joint possibility distribution πmin .
Comparing theoretically the probabilistic and the possibilistic case allows us to deduce the following proposition:
Proposition 6. The possibilistic encoding of a possibilistic network given by Kψ (equation 10) is more
compact than the probabilistic encoding given in
(Wachter and Haenni, 2007).
In fact, the number of variables used in Kψ is less than
the one used in (Wachter and Haenni, 2007). In particular for parameters, our approach uses one variable
per different weight, while in the probabilistic encoding
one variable per parameter. For each clause in Kψ
there exists a clause of the same size in the probabilistic encoding. The converse is false.

Figure 3: Inference using the possibilistic circuit (ζp ).

3.2

INFERENCE USING POSSIBILISTIC
COMPILED REPRESENTATIONS

DNNF plays an interesting role in compiling propositional knowledge bases. It has been used to compile
probabilistic networks. More precisely in (Wachter
and Haenni, 2007), authors have been interested in
performing a CNF logical encoding of the probability distribution induced by a bayesian network, then
a compilation phase from CNF to d-DNNF. In this
section, we propose to adapt this encoding in the possibilistic setting by taking into consideration the local
structure aspect. This allows to reduce the number
of additional variables comparing to the probabilistic
encoding. Let ∆ be propositions linked to network’s
variables and let θ be propositions linked to the possibility distribution entries (equal to 1). We start by
looking at the possibility distribution encoding. The
logical representation of a network variable Xi is defined by: ψXi =

^
^

ui1 ∧ · · · ∧ uim ∧ θxi |ui → xi
(9)
ui

θxi |ui ∈ΩθX

i

|ui

By taking the conjunction of all logical representations
of variables, we obtain the network’s representation ψ
as follows:
^
ψ=
ψXi
(10)
Xi ∈∆

The CNF encoding, denoted by Kψ indeed recovers
the min-joint possibility distribution (proposition 5).
Proposition 5. Let πmin be the joint possibility distribution obtained using the chain rule with the minimum operator and Π be the possibility degree computed

Once the qualitative network is encoded by Kψ , it is
compiled into a compilation language that supports
the transformations conditioning and forgetting and
the query possibilistic computation. This language is
Π-DNNF (proposition 7). Therefore, the CNF encoding is first compiled, and the resulting Π-DNNF is then
used to compute efficiently, i.e. in polynomial time
a-posteriori possibility degrees (proposition 8). This
method referred to Π-DNNF is outlined by algo. 2.
Proposition 7. Π-DNNF supports conditioning, forgetting and possibilistic computation.
Algorithm 2: Inference using Π-DNNF
Data: ΠGmin , instance of interest x, evidence e
Result: Π(x|e)
begin
Compilation into Π-DNNF
EncodeCNF of ΠGmin into ψ using equation 10
Compile ψ into ψpc
Inference
v1 ← Explore Π-DNNF(x ∧ e, ψpc )
v2 ← Explore Π-DNNF(e, ψpc )
if v1 ≺ v2 then Π(x|e) ← v1 else Π(x|e) ← 1
return Π(x|e)
end

Proposition 8. Let ΠGmin be a possibilistic network.
Let πmin be a joint distribution obtained by chain rule.
Then for any a ∈ Da and e ∈ DE , we have Π(A =
a|E = e) = Πmin (A = a|E = e) where Πmin (A =
a|E = e) is obtained from πmin using equation 1 and
Π(A = a|E = e) is obtained from algorithm 2.
Example 3. Let us illustrate algorithm 2. In fact, ψ of
the network of figure 2 is : ψ = ψF ∧ ψB ∧ ψD = {(θ1 ∨
f2 ) ∧ (θ2 ∨ b1 ) ∧ (f2 ∨ b2 ∨ θ2 ∨ d1 ) ∧ (f2 ∨ b1 ∨ θ1 ∨ d2 ) ∧
(f1 ∨ b2 ∨ θ3 ∨ d2 ) ∧ (f1 ∨ b1 ∨ θ4 ∨ d2 )} such as θ1 , θ2 , θ3
and θ4 correspond respectively to 0.8, 0.7, 0.4 and 0.2.
To compute Π(f2 |d1 ), we should first compute Π(f2 , d1 ) using algorithm 3. The first step is to check if we have at least

Algorithm 3: Explore Π-DNNF
Data: a set of instances x, compiled representation ψpc
Result: Π(x)
begin
if ∀ xi ∈ x, θxi |Ui is not a leaf node then
Π(x) ← 1
else
y= {xi | ∀, θxi |Ui is a leaf node ∀ Ui ⊆ x}
c
ψp|y
← Condition ψpc on y
c
ψpc ↓|y ← Forget ∆ from ψp|y
c
Applying Operators on ψp ↓|y
Π(x) ← Root Value of ψpc ↓|y
return Π(x)
end

one θ as a leaf node. In this example, we have θd1 |f2 ,b1
and θd1 |f2 ,b2 as leaf nodes, hence conditioning should be
performed. Then, a computation step is required by applying in a bottom-up way Min and Max operators on the
forgotten Π-DNNF. Therefore, Π(f2 |d1 ) = Π(f2 , d1 ) = 0.4.

4

NEW POSSIBILISTIC
INFERENCE ALGORITHM

In (Benferhat et al., 2002), authors have been interested in the transition of possibilistic networks into
possibilistic logic bases. The starting point is that the
possibilistic base associated to a possibilistic network
is the result of the fusion of elementary bases. Definition 4 presents the transformation of a min-based possibilistic network into a possibilistic knowledge base.
Definition 4. A binary variable Xi of a possibilistic network can be expressed by a local possibilistic knowledge base as follows:
ΣXi
=
{(¬xi ∨ ¬ui , αi ) : αi = 1 − π(xi |ui ) 6= 0}. The possibilistic knowledge base of the whole network is: Σmin =
ΣX1 ∪ ΣX2 ∪ · · · ∪ ΣXn .
In another angle, researchers in (Benferhat et al., 2007)
have focused on the compilation of bases under the
possibilistic logic policy in order to be able to process
inference from it in a polynomial time. The combination of these methods allows us to propose a new
alternative approach to possibilistic inference. This is
justified by the fact that the possibilistic logic reasoning machinery can be applied to directed possibilistic
networks (Benferhat et al., 2002).
The idea is to encode the possibilistic knowledge base
Σmin into a classical propositional base (CNF). Let
A = {a1 , ..., an } with a1  ...  an the different
weights used in Σmin . A set of additional propositional variables, denoted by Ai , which correspond exactly to the number of different weights, are incorporated and for each formula φi , ai will correspond the
propositional formula φi ∨Ai . Hence, the propositional

encoding of Σmin , denoted by KΣ is defined by:
KΣ = {φi ∨ Ai : (φi , ai ) ∈ Σmin }

(11)

The following proposition shows that the CNF encoding KΣ recovers the min-joint possibility distribution.
Proposition 9. Let πmin be the joint possibility
distribution obtained using the chain rule with the
minimum-based conditioning and let KΣ be the propositional base associated with the possibilistic network
given by equation 11. Let φi be a propositional formula associated with a degree ai . Then ∀ω ∈ Ω,
Π(ω) = 1 iff {¬A1 , ..., ¬An } ∧ ω ∧ KΣ is consistent.
Π(ω) = ai iff {¬A1 , ..., ¬Ai } ∧ ω ∧ KΣ is inconsistent
and {¬A1 , ..., ¬Ai−1 } ∧ ω ∧ KΣ is consistent.
The CNF encoding KΣ is then compiled into a target
compilation language in order to compute a-posteriori
possibility degrees in an efficient way. Here, we are
interested in a particular query useful for possibilistic networks, namely what is the possibility degree of
an event A = a given an evidence E = e? Therefore, we propose to adapt the algorithm given in (Benferhat et al., 2007) in order to respond to this query
as shown by algorithm 4. Proposition 10 shows that
the possibility degree computed using algorithm 4 and
the one computed using the min-based joint possibility distribution are equal. Note that this approach is
qualified to be flexible since it takes advantage of existing propositional knowledge bases compilation methods (Benferhat et al., 2007). This method referred to
DNNF-PKB is outlined by algorithm 4.
Algorithm 4: Inference using DNNF
Data: ΠGmin , instance of interest x, evidence e
Result: Π(x|e)
begin
Transformation into KΣ
Transform ΠGmin into Σmin using definition 4
Transform Σmin into KΣ using equation 11
Inference
c
KΣ
← T arget(KΣ )
c
K ← KΣ
StopCompute ← false
i←1
Π(x|e) ← 1
while (K 2 Ai ∨ ¬e) and (i ≤ k) and (StopCompute=false) do
K ← condition (K, ¬Ai )
if K ¬x then
StopCompute← true
Π(x|e) ← 1-degree(i)
else
i←i+1
return Π(x|e)
end

Proposition 10. Let ΠGmin be a possibilistic network. Let πmin be a joint distribution obtained by

chain rule. Then for any a ∈ Da and e ∈ DE , we
have Π(A = a|E = e) = Πmin (A = a|E = e) where
Πmin (A = a|E = e) is obtained from πmin using equation 1 and Π(A = a|E = e) is obtained from Algo. 4.

Indeed, in Π-DNNFP F , we associate propositional
variables not only to possibility degrees (parameters),
but also to each value xi of Xi . While in DNNF-PKB
only m new variables are added (one variable per different degree).

Example 4. To illustrate algorithm 4 we will consider

Let us now analyze these three approaches from experimental points of view. Our experimentation is
performed on random possibilistic networks. More
precisely, we have compared DNNF-PKB and ΠDNNFP F on 100 possibilistic networks having from 10
to 50 nodes. As mentioned that the approaches focus
mainly on encoding the possibilistic network as a CNF
then compile it into the appropriate language, hence,
it should be interesting to compare the CNF parameters (the number of variables and clauses) and the
DNNF parameters (the number of nodes and edges)
for the two methods.

the min-based possibilistic network represented in figure 2.
The CNF encoding is as follows :
KΣ
=
(d2 ∨ f1 ∨ b2 ∨ A1 ) , (b1 ∨ A2 ) , (d1 ∨ f2 ∨ b2 ∨ A2 ) ,
(f2 ∨ A3 ) , (d2 ∨ f2 ∨ b1 ∨ A3 ) , (d2 ∨ f1 ∨ b1 ∨ A4 )
such
as A1 (0.8), A2 (0.6), A3 (0.3) and A4 (0.2) are
propositional variables followed by their weights under
c
brackets. Compiling KΣ into DNNF results in: KΣ
=
((b2 ∧ A2 ) ∧ [(A3 ∧ f1 ) ∨ (f2 ∧ [d2 ∨ (A4 ∧ d1 )])]) ∨ (b1 ∧
[[f2 ∧ (d2 ∨ (A1 ∨ d1 ))] ∨ [(f 1 ∧ A3 ) ∧ (d1 ∨ (A2 ∧ d2 ))]]).
c
The computation of Π(f2 |d1 ) using KΣ
requires two
iterations. Therefore, Π(f2 |d1 ) = 1 − degree(2) = 0.4.

Due to the compilation step, this algorithm runs in
polynomial time. Moreover, the number of additional
variables is low since it corresponds exactly to the
number of priority levels existing in the base.

5

COMPARATIVE AND
EXPERIMENTAL STUDIES

The paper analyzes three compilation-based methods,
namely DNNF-PKB, Π-DNNF and Π-DNNFP F . The
first dimension that differentiates the three approaches
proposed in this paper is the CNF encoding. It consists
of specifying the number of variables and clauses per
approach.
The CNF of DNNF-PKB is based on encoding ¬x
where x is an instance of interest having a possibility degree different from 1. In Π-DNNF, we write
implications relative to instances having 1 as possibility degree. We can notice that the local structure
in both methods is exploited in semantically different
ways. In DNNF-PKB, the encoding uses the number
of different weights as the number of additional variables while the Π-DNNF encoding uses the number of
the non-redundant possibility degrees different from 1
in the distributions. Regarding the number of clauses,
both methods handle possibility degrees different from
1. This leads us to the following proposition:

5.1

CNF PARAMETERS

First we propose to test the CNF encodings characterized by the number of variables and the number of
clauses. Regarding DNNF-PKB, the number of additional variables correspond to the number of weights
which are different. While in Π-DNNFP F , variables
are both those associated to the possibility degrees
of each distribution and those to variable’s instances.
The number of clauses for each method is related to
the CNF encoding itself. Figure 4 shows the results of
this experimentation. Each approach is characterized
by a curve for the average number of variables and a
curve for the average number of clauses. It is clear
that the higher the number of nodes considered in the
possibilistic network, the higher the number of variables and clauses. Figure 4 shows that DNNF-PKB
has the lower number of variables and clauses comparing to Π-DNNFP F , which confirms the theoretical
results detailed above.

Proposition 11. The CNF encodings of DNNF-PKB
and Π-DNNF have the same number of variables and
clauses.
The CNF encoding of Π-DNNFP F is different from
the ones of DNNF-PKB and Π-DNNF. Proposition 12
shows the difference between Π-DNNFP F and DNNFPKB in terms of number of variables and clauses.
Proposition 12. The number of variables and clauses
in Π-DNNFP F is more important than those in
DNNF-PKB.

Figure 4: CNF parameters.

5.2

DNNF PARAMETERS

Once we obtain the CNF encodings, it is important
to compare the number of nodes and edges for each
compiled base. Figure 5 represents the average size of
the compiled bases for the two methods in terms of
nodes and edges numbers. We remark that the number of nodes and edges depends deeply on CNF parameters. More precisely, the number of nodes and
edges in DNNF-PKB is considered narrow comparing
to Π-DNNFP F . This can be explained by the lower
number of variables and clauses on CNFs and the local structure which shrinks the sizes of compiled bases.
Comparing DNNF-PKB to Π-DNNFP F , the behavior
of DNNF-PKB is important.

(Pearl, 2000) (Benferhat and Smaoui, 2007).
Acknowledgements
We thank the anonymous reviewers for many interesting
comments and suggestions. Also, we wish to thank Mark
Chavira for our valuable discussions on this subject. The
third author would like to thank the project ANR Placid.


