
Continuous-time Bayesian networks is a natural structured representation language for multicomponent stochastic processes that evolve continuously over time. Despite the compact representation, inference in such models is intractable
even in relatively simple structured networks.
Here we introduce a mean field variational approximation in which we use a product of inhomogeneous Markov processes to approximate
a distribution over trajectories. This variational
approach leads to a globally consistent distribution, which can be efficiently queried. Additionally, it provides a lower bound on the probability of observations, thus making it attractive for
learning tasks. We provide the theoretical foundations for the approximation, an efficient implementation that exploits the wide range of highly
optimized ordinary differential equations (ODE)
solvers, experimentally explore characterizations
of processes for which this approximation is suitable, and show applications to a large-scale realworld inference problem.

1

Introduction

Many real-life processes can be naturally thought of as
evolving continuously in time. Examples cover a diverse range, including server availability, changes in socioeconomic status, and genetic sequence evolution. To realistically model such processes, we need to reason about systems that are composed of multiple components (e.g., many
servers in a server farm, multiple residues in a protein sequence) and evolve in continuous time. Continuous-time
Bayesian networks (CTBNs) provide a representation language for such processes, which allows to naturally exploit
sparse patterns of interactions to compactly represent the
dynamics of such processes [9].
Inference in multi-component temporal models is a notoriously hard problem [1]. Similar to the situation in dis-

Raz Kupferman
Institute of Mathematics
The Hebrew University
raz@math.huji.ac.il

crete time processes, inference is exponential in the number of components, even in a CTBN with sparse interactions [9]. Thus, we have to resort to approximate inference
methods. The recent literature has adapted several strategies from discrete graphical models to CTBNs. These include sampling-based approaches, where Fan and Shelton
[5] introduced a likelihood-weighted sampling scheme, and
more recently we [4] introduced a Gibbs-sampling procedure. Such sampling-based approaches yield more accurate answers with the investment of additional computation. However, it is hard to bound the required time in advance, tune the stopping criteria, or estimate the error of
the approximation. An alternative class of approximations
is based on variational principles.
Recently, Nodelman et al. [11] introduced an Expectation Propagation approach, which can be roughly described as a local message passing scheme, where each
message describes the dynamics of a single component
over an interval. This message passing procedure can automatically refine the number of intervals according to the
complexity of the underlying system [14]. Nonetheless,
it does suffer from several caveats. On the formal level,
the approximation has no convergence guaranties. Second,
upon convergence, the computed marginals do not necessarily form a globally consistent distribution. Third, it
is restricted to approximations in the form of piecewisehomogeneous messages on each interval. Thus, the refinement of the number of intervals depends on the fit of
such homogeneous approximations to the target process.
Finally, the approximation of Nodelman et al does not provide a provable approximation on the likelihood of the
observation—a crucial component in learning procedures.
Here, we develop an alternative variational approximation, which provides a different trade-off. We use the strategy of structured variational approximations in graphical
models [8], and specifically by the variational approach of
Opper and Sanguinetti [12] for approximate inference in
Markov Jump Processes, a related class of models (see below). The resulting procedure approximates the posterior
distribution of the CTBN as a product of independent components, each of which is an inhomogeneous continuous-

92

COHN ET AL.

time Markov process. As we show, by using a natural representation of these processes, we derive a variational procedure that is both efficient, and provides a good approximation both for the likelihood of the evidence and for the
expected sufficient statistics. In particular, the approximation provides a lower-bound on the likelihood, and thus is
attractive for use in learning.

2

UAI 2009
probability of various events given the evidence (e.g., that
the state of the system at time t is x), and to compute conditional expectations (e.g., the expected amount of time Xi
was in state xi ). Direct computations of these quantities
involve matrix exponentials of the rate matrix Q, whose
size is exponential in the number of components, making
this approach infeasible beyond a modest number of components. We therefore have to resort to approximations.

Continuous-Time Bayesian Networks

Consider a D-component Markov process X (t) =
(t)
(t)
(t)
(X1 , X2 , . . . XD ) with state space S = S1 × S2 × · · · ×
SD . A notational convention: vectors are denoted by boldface symbols, e.g., X, and matrices are denoted by blackboard style characters, e.g., Q. The states in S are denoted
by vectors of indexes, x = (x1 , . . . , xD ). We use indexes
1 ≤ i, j ≤ D for enumerating components and X (t) and
(t)
Xi to denote the random variable describing the state of
the process and its i’th components at time t.
The dynamics of a time-homogeneous continuous-time
Markov process are fully determined by the Markov transition function,
px,y (t) = Pr(X (t+s) = y|X (s) = x),
where time-homogeneity implies that the right-hand side
does not depend on s. These dynamics are fully captured by a matrix Q—the rate matrix with non-negative
offP
diagonal entries qx,y and diagonal qx,x = − y6=x qx,y .
This rate matrix defines the transition probabilities
px,y (h) = δx,y + qx,y · h + o(h)
where δx,y is a multivariate Kronecker delta and o(·)
means decay to zero faster than its argument. Using the
rate matrix Q, we can express the Markov transition function as px,y (t) = [exp(tQ)]x,y where exp(tQ) is a matrix
exponential [2, 7].
A continuous-time Bayesian network is defined by assigning each component i a set of components Pai ⊆
{1, . . . , D} \ {i}, which are its parents in the network [9].
With each component i we then associate a set of condii|Pa
tional rate matrix Q·|ui i for each state ui of Pai . The
i|Pa
off-diagonal entries qxi ,yii|ui represent the rate at which Xi
transitions from state xi to state yi given that its parents are
in state ui . The dynamics of X (t) are defined by a rate

matrix Q with entries qx,y , which amalgamates the conditional rate matrices as follows:
 i|Pa
i

δ(x, y) = {i}
 qxi ,yi |ui
P i|Pai
qx,y =
(1)
x=y
i qxi ,xi |ui


0
otherwise,
where δ(x, y) = {i|xi 6= yi }. This definition implies that
changes are one component at a time.
Given a continuous-time Bayesian network, we would
like to evaluate the likelihood of evidence, to compute the

3

Variational Principle for Continuous Time
Markov Processes

We start by defining a variational approximations principle in terms of a general continuous-time Markov process
(that is, without assuming any network structure). For convenience we restrict our treatment to a time interval [0, T ]
with end-point evidence X (0) = e0 and X (T ) = eT . We
discuss more general types of evidence below. Here we
aim to define a lower bound on ln PQ (eT |e0 ) as well as to
approximate the posterior probability PQ (· | e0 , eT ).
Marginal Density Representation Variational approximations cast inference as an optimization problem of a
functional which approximates the log probability of the
evidence by introducing an auxiliary set of variational parameters. Here we define the optimization problem over a
set of mean parameters [15], representing possible values
of expected sufficient statistics.
As discussed above, the prior distribution of the process
can be characterized by a time-independent rate matrix Q.
It is easy to show that if the prior is a Markov process, then
the posterior is also a Markov process, albeit not necessarily a homogeneous one. Such a process can be represented
by a time-dependent rate matrix that describes the instantaneous transition rates. Here, rather than representing the
target distribution by a time-dependent rate matrix, we consider a representation that is more natural for variational
approximations. Let Pr be the distribution of a Markov
process. We define a family of functions:
µx (t) = Pr(X (t) = x)
Pr(X (t) = x, X (t+h) = y)
, x 6= y (2)
γx,y (t) = lim
h↓0
h
X
γx,x (t) = −
γx,y (t).
y6=x

The function µx (t) is the probability that X (t) = x.
The function γx,y (t) is the probability density that X transitions from state x to y at time t. Note that this parameter is not a transition rate, but rather a product of a pointwise probability with the point-wise transition rate of the
approximating probability, i.e., γx,y (t)/µx (t) is the x, y
entry of the time-dependent rate matrix. Hence, unlike the
(inhomogeneous) rate matrix at time t, γx,y (t) takes into
account the probability of being in state x and not only the

UAI 2009

COHN ET AL.

rate of transitions. This definition implies that
Pr(X (t) = x, X (t+h) = y) = µx (t)δx,y +γx,y (t)h+o(h),

93
terms in the continuous functional correspond to an entropy,
Z

We aim to use the family of functions µ and γ as a
representation of a Markov process. To do so, we need
to characterize the set of constraints that these functions
should satisfy.
Definition 3.1: A family η = {µx (t), γx,y (t) : 0 ≤ t ≤
T } of continuous functions is a Markov-consistent density
set if the following constraints are fulfilled:
X
µx (t) ≥ 0,
µx (0) = 1,
x

γx,y (t) ≥ 0
γx,x (t)

∀y 6= x,
X

= −

γx,y (t),

y6=x

d
µx (t)
dt

=

X

γy,x (t).

y

Let M be the set of all Markov-consistent densities.
Using standard arguments we can show that there exists a correspondence between (generally inhomogeneous)
Markov processes and density sets η. Specifically:
Lemma 3.2: Let η = {µx (t), γx,y (t)}. If η ∈ M, then
there exists a continuous-time Markov process Pη for which
µx and γx,y satisfy (2).
The processes we are interested in, however, have additional structure, as they correspond to the posterior distribution of a time-homogeneous process with end-point evidence. This additional structure implies that we should
only consider a subset of M:
Lemma 3.3: Let Q be a rate matrix, and e0 , eT be states
of X. Then the representation η corresponding to the posterior distribution PQ (·|e0 , eT ) is in the set Me ⊂ M that
contains Markov-consistent density sets satisfying µx (0) =
δx,e0 , µx (T ) = δx,eT .
Thus, from now on we can restrict our attention to density sets from Me . The constraint that µx (0) and µx (T )
also has consequences on γx,y at these points.
Lemma 3.4: If η ∈ Me then γx,y (0) = 0 for all x 6= e0
and γx,y (T ) = 0 for all y 6= eT .
Variational Principle We can now state the variational
principle for continuous processes, which closely tracks
similar principles for discrete processes.
We define a free energy functional,
F(η; Q) = E(η; Q) + H(η),

T

XX

H(η) =
0

γx,y (t)[1 + ln µx (t) − ln γx,y (t)]dt,

x y6=x

and an energy,
Z
E(η; Q) =
0

T


X



µx (t)qx,x +

x

X

γx,y (t) ln qx,y  dt.

y6=x

Theorem 3.5: Let Q be a rate matrix, e = (e0 , eT ) be
states of X, and η ∈ Me . Then
F(η; Q) = ln PQ (eT |e0 ) − ID(Pη (·)||PQ (·|e))
where ID(Pη (·||PQ (·|e)) is the KL divergence between the
two processes.
We conclude that F(η; Q) is a lower bound of the loglikelihood of the evidence, and that the closer the approximation to the target posterior, the tighter the bound.
Proof Outline The basic idea is to consider discrete approximations of the functional. Let K be an integer. We
define the K-sieve X K to be the set of random variables
X (t0 ) , X (t1 ) , . . . , X (tK ) where tk = kT
K . We can use
the variational principle [8] on the marginal distributions
PQ (X K |e) and Pη (X K ). More precisely, define


PQ (X K , eT | e0 )
,
FK (η; Q) = EPη ln
Pη (X K )
which can, by using simple arithmetic manipulations, be
recast as
FK (η; Q) = ln PQ (eT |e0 ) − ID(Pη (X K )||PQ (X K |e)).
We get the desired result by letting K
→
∞. By definition limK→∞ ID(Pη (X K )||PQ (X K |e)) is
ID(Pη (·)||PQ (·|e)). The crux of the proof is in proving the
following lemma.
Lemma 3.6: F(η; Q) = limK→∞ FK (η; Q).
Proof: Since both PQ and Pη are Markov processes,
FK (η; Q) =

K−1
X

h
i
EPη ln PQ (X (tk+1 ) |X (tk ) )

k=0

−

K−1
X

h
i
EPη ln Pη (X (tk ) , X (tk+1 ) )

k=0

+

K−1
X

h
i
EPη ln Pη (X (tk ) )

k=1

which, as we will see, measures the quality of η as an approximation of PQ (·|e). (For succinctness, we will assume
that the evidence e is clear from the context.) The two

We now express these terms as functions of µx (t), γx,y (t)
and qx,y . By definition, Pη (X (tk ) = x) = µx (tk ). Each

94

COHN ET AL.

of the expectations either depend on this term, or on the
joint distribution Pη (X (tk−1 ) , X (tk ) ). Using the continuity of γx,y (t) we write
Pη (X (tk ) = x, X (tk+1 ) = y) = δx,y µx (tk )
+ ∆K · γx,y (tk ) + o(∆K )
where ∆K = T /K. Similarly, we can also write
PQ (X (tk+1 ) = y|X (tk ) = x) = δx,y +∆K ·qx,y +o(∆K )

UAI 2009
Q
\i
where µx (t) = j6=i µjxj (t) is the joint distribution at time
t of all the components other than the i’th. (It is not hard to
see that if η i ∈ Mie for all i, then η ∈ Me .) We define the
set MF
e to contain all factored density sets. From now on
we assume that η = η 1 × · · · × η D ∈ MF
e.
Assuming that Q is defined by a CTBN, and that η is a
factored density set, we can rewrite
E(η; Q) =

ln (1 + ∆K · z + o(∆K ))

=

+

∆K · z + o(∆K ).

XZ
i

Using these relations, we can rewrite after tedious yet
straightforward manipulations,

EK (η; Q) =

∆K eK (tk ), HK (η) =

k=0

∆K hK (tk ),

k=0

and
eK (t) =

XX

γx,y (t)[1 + ln µx (t) − ln γx,y (t)] + o(∆K )


X
x

µx (t)qxx +

X



γxi i ,yi (t)Eµ\i (t) ln qxi ,yi |U i dt,

xi ,yi 6=xi

and
X

H(η i ).

This decomposition involves only local terms that either
include the i’th component, or include the i’th component
and its parents in the CTBN
 defining Q. Note that terms
such as Eµ\i (t) qxi ,xi |U i involve only µj (t) for j ∈ Pai .
To make the factored nature of the approximation explicit in the notation, we write henceforth,


X

γx,y (t) log qx,y  + o(∆K )

y6=x

P
Letting K → ∞ we have that k ∆k [f (tk ) + o(∆K )] →
RT
f (t)dt, hence EK (η; Q) and HK (η) converge to
0
E(η; Q) and H(η), respectively.

4

xi

F(η; Q) = F F (η 1 , . . . , η D ; Q).

x y6=x

hK (t) =



µixi (t)Eµ\i (t) qxi ,xi |U i dt

i

where
K−1
X

0

T

X

H(η) =

FK (η; Q) = EK (η; Q) + HK (η),
K−1
X

0

i

Finally, using properties of logarithms we have that

T

XZ

Factored Approximation

The variational principle we discussed is based on a representation that is as complex as the original process—the
number of functions γx,y (t) we consider is equal to the size
of the original rate matrix Q. To get a tractable inference
procedure we make additional simplifying assumptions on
the approximating distribution.
Given a D-component process we consider approximations that factor into products of independent processes. More precisely, we define Mie to be the continuous Markov-consistent density sets over the component
Xi , that are consistent with the evidence on Xi at times
0 and T . Given a collection of density sets η 1 , . . . , η D
for the different components, the product density set η =
η 1 × · · · × η D is defined as
Y
µx (t) =
µixi (t)
i
\i

δ(x, y) = {i}
 γxi i ,yi (t)µx (t)
P i
\i
γx,y (t) =
γ
(t)µ
(t)
x=y
x
x ,x

 0 i i i
otherwise

Fixed Point Characterization We can now pose the optimization problem we wish to solve:
Fixing i, and given η 1 , . . . , η i−1 , η i+1 , . . . , η D ,
D
i+1
respecin M1e , . . . Mi−1
e , Me , . . . , Me ,
F
1
tively, find arg maxηi ∈Mie F (η , . . . , η D ; Q).
If for all i, we have a µi ∈ Mie , which is a solution
to this optimization problem with respect to each component, then we have a (local) stationary point of the energy
functional within MF
e.
To solve this optimization problem, we define a Lagrangian, which includes the constraints in the form of
Def. 3.1. The Lagrangian is a functional of the functions
µixi (t) and γxi i ,yi (t) and Lagrange multipliers (which are
functions of t as well). The stationary point of the functional satisfies the Euler-Lagrange equations, namely the
functional derivatives of L vanish. Writing these equations
in explicit form we get a fixed point characterization of the
solution in term of the following set of ODEs:
X

d i
γyi i ,xi (t) − γxi i ,yi (t)
µxi (t) =
dt
yi 6=xi

d i
ρ (t) = −ρixi (t)(q̄xi i ,xi (t) + ψxi i (t))
dt xi
X
−
ρiyi (t)q̃xi i ,yi (t)
yi 6=xi

where ρi are the exponents of the Lagrange multipliers.

(3)

UAI 2009

COHN ET AL.

In addition we have the following algebraic constraint
ρixi (t)γxi i ,yi (t) = µixi (t)q̃xi i ,yi (t)ρiyi (t), xi 6= yi .

(4)

In these equations we use the following shorthand notations
for the average rates
h
i
i|Pa
q̄xi i ,yi (t) = Eµ\i (t) qxi ,yii|U i
h
i
i|Pa
q̄xi i ,yi |xj (t) = Eµ\i (t) qxi ,yii|U i | xj ,
Similarly, we have the following shorthand notations for
the geometrically-averaged rates,
h
io
n
i|Pa
q̃xi i ,yi (t) = exp Eµ\i (t) ln qxi ,yii|U i
h
io
n
i|Pa
q̃xi i ,yi |xj (t) = exp Eµ\i (t) ln qxi ,yii|U i | xj ,
The last auxiliary term is
X X
µjxj (t)q̄xj j ,xj |xi (t)+
ψxi i (t) =
j∈Childreni xj

X

X

γxj j ,yj (t) ln q̃xj j ,yj |xi (t).

j∈Childreni xj 6=yj

The two differential equations (3) for µixi (t) and ρixi (t)
describe, respectively, the progression of µixi forward, and
the progression of ρixi backward. To uniquely solve these
equations we need to set the boundary conditions. The
boundary condition for µixi is defined explicitly in MF
e as
µixi (0)

= δxi ,ei,0

(5)

The boundary condition at T is slightly more involved. The
i
constraints in MF
e imply that µxi (T ) = δxi ,ei,T . As stated
i
by Lemma 3.4, we have that γei,T ,xi (T ) = 0 when xi 6=
ei,T . Plugging these values into (4), and assuming that Q
is irreducible we get that ρxi (T ) = 0 for all xi 6= ei,T .
In addition, we notice that ρei,T (T ) 6= 0, for otherwise the
whole system of equations for ρ will collapse to 0. Finally,
notice that the solution of (3) for µi and γ i is insensitive
to the multiplication of ρi by a constant. Thus, we can
arbitrarily set ρei,T (T ) = 1, and get the boundary condition
ρixi (T ) = δxi ,ei,T .

(6)

Theorem 4.1: η i ∈ Mie is a stationary point (e.g., local
maxima) of F F (η 1 , . . . , η D ; Q) subject to the constraints
of Def. 3.1 if and only if it satisfies (3–6).
It is straightforward to extend this result to show that at a
maximum with respect to all the component densities, this
fixed-point characterization must hold for all components
simultaneously.
Example 4.2: Consider the case of a single component,
for which our procedure should be exact, as no simplifying
assumptions are made on the density set. In this case, the

95
averaged rates q̄ i and the geometrically-averaged rates q̃ i
both reduce to the unaveraged rates q, and ψ ≡ 0. Thus,
the system of equations to be solved is
X
d
µx (t) =
(γy,x (t) − γx,y (t))
dt
y6=x

X
d
ρx (t) = −
qx,y ρy (t),
dt
y
along with the algebraic equation
ρx (t)γx,y (t) = qx,y µx (t)ρy (t),

y 6= x.

In this case, it is straightforward to show that the backward propagation rule for ρx implies that
ρx (t) = Pr(eT |X (t) ).
This system of ODEs is similar to forward-backward propagation, except that unlike classical forward propagation
(which would use a function such as αx (t) = Pr(X (t) =
x|e0 )), here the forward propagation already takes into account the backward messages, to directly compute the posterior. Given this interpretation, it is clear that integrating
ρx (t) from T to 0 followed by integrating µx (t) from 0 to
T computes the exact posterior of the processes.
This interpretation of ρx (t) also allows us to understand
the role of γx,y (t). Recall that γx,y (t)/µx (t) is the instantaneous rate of transition from x to y at time t. Thus,
ρy (t)
γx,y (t)
= qx,y
.
µx (t)
ρx (t)
That is, the instantaneous rate combines the original rate
with the relative likelihood of the evidence at T given y and
x. If y is much more likely to lead to the final state, then
the rates are biased toward y. Conversely, if y is unlikely
to lead to the evidence the rate of transitions to it are lower.
This observation also explains why the forward propagation of µx will reach the observed µx (T ) even though we
did not impose it explicitly.
Example 4.3: We define an Ising chain to be a CTBN
X1 ↔ X2 ↔ · · · ↔ XD such that each binary component prefers to be in the same state as its neighbor. These
models are governed by two parameters: a coupling parameter β which determines the strength of the coupling
between two neighboring components, and a rate parameter τ that determines the propensity of each component to
change its state. More formally, we define the conditional

−1
P
i|Pa
rate matrices as qxi ,yii|ui = τ 1 + e−2yi β ∈Pai xj
where xj ∈ {−1, 1}.
As an example, we consider a two-component Ising
(0)
(0)
chain with initial state X1 = −1 and X2 = 1, and a
(T )
(T )
reversed state at the final time, X1 = 1 and X2 = −1.
For a large value of β, this evidence is unlikely as at

96

COHN ET AL.

UAI 2009
imation behaves in this manner, we notice that in the single
component case we have
qx,y =

ρx (t)γx,y (t)
,
ρy (t)µx (t)

which should be constant. Consider the analogous quantity
in the multi-component case: q̃xi i ,yi (t), the geometric average of the rate of Xi , given the probability of parents state.
Not surprisingly, this is exactly a mean field approximation,
where the influence of interacting components is approximated by their average influence. Since the distribution of
the parents (in the two-component system, the other component) changes in time, these rates change continuously,
especially near the end of the time interval. This suggests
that a piecewise homogeneous approximation cannot capture the dynamics without a loss in accuracy.
Optimization Procedure If Q is irreducible, then ρixi
and µxi are non-zero throughout the open interval (0, T ).
As a result, we can solve (4) to express γxi i ,yi as a function
of µi and ρi , thus eliminating it from (3) to get evolution
equations solely in terms of µi and ρi . Abstracting the details, we obtain a set of ODEs of the form

Figure 1: Numerical results for the two-component Ising chain
described in Example 4.3 where the first component starts in state
−1 and ends at time T = 1 in state 1. The second component
has the opposite behavior. (top) Two likely trajectories depicting
the two modes of the model. (middle) Exact (solid) and approximate (dashed/dotted) marginals µi1 (t). (bottom) The log ratio
log ρi1 (t)/ρi0 (t).

both end points the components are in a undesired configurations. The exact posterior is one that assigns higher
probabilities to trajectories where one of the components
switches relatively fast to match the other, and then toward
the end of the interval, they separate to match the evidence.
Since the model is symmetric, these trajectories are either
ones in which both components are most of the time in
state −1, or ones where both are most of the time in state 1
(Fig. 1 top). Due to symmetry, the marginal probability of
each component is around 0.5 throughout most of the interval (Fig. 1 middle). The variational approximation cannot
capture the dependency between the two components, and
thus converges to one of two local maxima, corresponding
to the two potential subsets of trajectories. Examining the
value of ρi , we see that close to the end of the interval they
bias the instantaneous rates significantly (Fig. 1 bottom).
This example also allows to examine the implications
of modeling the posterior by inhomogeneous Markov processes. In principle, we might have used as an approximation Markov processes with homogeneous rates, and conditioned on the evidence. To examine whether our approx-

d i
µ (t) = α(µi (t), ρi (t), µ\i (t)) µi (0) = given
dt
d i
ρ (t) = −β(ρi (t), µ\i (t))
ρi (T ) = given.
dt
where α and β can be inferred from (3) and (4). Since the
evolution of ρi does not depend on µi , we can integrate
backward from time T to solve for ρi . Then, integrating
forward from time 0, we compute µi . After performing a
single iteration of backward-forward integration, we obtain
a solution that satisfies the fixed-point equation (3) for the
i’th component. (This is not surprising once we have identified our procedure to be a variation of a standard forwardbackward algorithm for a single component.) Such a solution will be a local maximum of the functional w.r.t. to η i
(reaching a local minimum or a saddle point requires very
specific initialization points).
This suggests that we can use the standard procedure
of asynchronous update, where we update each component in a round-robin fashion. Since each of these singlecomponent updates converges in one backward-forward
step, and since it reaches a local maximum, each step improves the value of the free energy over the previous one.
Since the free energy functional is bounded by the probability of the evidence, this procedure will always converge.
Another issue is the initialization of this procedure.
Since the iteration on the i’th component depends on µ\i ,
we need to initialize µ by some legal assignment. To do
so, we create a fictional rate matrix Q̃i for each component
and initialize µi to be the posterior of the process given
the evidence ei,0 and ei,T . As a reasonable initial guess,
we choose at random one of the conditional rates in Q to
determine the fictional rate matrix.

UAI 2009

COHN ET AL.

97

Figure 2: (a) Relative error as a function of the coupling parameter β (x-axis) and transition rates τ (y-axis) for an 8-component Ising
chain. (b) Comparison of true vs. estimated likelihood as a function of the rate parameter τ . (c) Comparison of true vs. likelihood as a
function of the coupling parameter β.

The continuous time update equations allow us to use
standard ODE methods with an adaptive step size (here we
use the Runge-Kutta-Fehlberg (4,5) method). At the price
of some overhead, these procedure automatically tune the
trade-off between error and time granularity.

spent in state x, and Mx,y , the number of transitions from
state x to y. In a discrete-time model, we can capture the
statistics for every random variable. In a continuous-time
model, however, we need to consider the time derivative of
the statistics. Indeed, it is not hard to show that

5

d
E [Tx (t)] = µx (t)
dt

Perspective & Related Works

Variational approximations for different types of
continuous-time processes have been recently proposed [12, 13]. Our approach is motivated by results of
Opper and Sanguinetti [12] who developed a variational
principle for a related model. Their model, which they call
a Markov jump process, is similar to an HMM, in which
the hidden chain is a continuous-time Markov process and
there are (noisy) observations at discrete points along the
process. They describe a variational principle and discuss
the form of the functional when the approximation is a
product of independent processes. There are two main
differences between the setting of Opper and Sanguinetti
and ours. First, we show how to exploit the structure of the
target CTBN to reduce the complexity of the approximation. These simplifications imply that the update of the i’th
process depends only on its Markov blanket in the CTBN,
allowing us to develop efficient approximations for large
models. Second, and more importantly, the structure of
the evidence in our setting is quite different, as we assume
deterministic evidence at the end of intervals. This setting
typically leads to a posterior Markov process in which
the instantaneous rates used by Opper and Sanguinetti
diverge toward the end point—the rates of transition into
the observed state go to infinity, leading to numerical
problems at the end points. We circumvent this problem by
using the marginal density representation which is much
more stable numerically.
Taking the general perspective of Wainwright and Jordan [15], the representation of the distribution uses the natural sufficient statistics. In the case of a continuous-time
Markov process, the sufficient statistics are Tx , the time

and

d
E [Mx,y (t)]
dt

= γx,y (t).

Thus, our marginal density sets η provide what we consider a natural formulation for variational approaches to
continuous-time Markov processes.
Our presentation focused on evidence at two ends of
an interval. Our formulation easily extends to deal with
more elaborate types of evidence: (1) If we do not observe
the initial state of the i’th component, we can set µix (0)
to be the prior probability of X (0) = x. Similarly, if we
do not observe Xi at time T , we set ρix (T ) = 1 as initial data for the backward step. (2) In a CTBN where one
(or more) components are fully observed, we simply set µi
for these components to be a distribution that assigns all
the probability mass to the observed trajectory. Similarly,
if we observe different components at different times, we
may update each component on a different time interval.
Consequently, maintaining for each component a marginal
distribution µi throughout the interval of interest, we can
update the other ones using their evidence patterns.

6

Experimental Evaluation

To gain better insight into the quality of our procedure, we
performed numerical tests on models that challenge the approximation. Specifically, we use Ising chains where we
explore regimes defined by the degree of coupling between
the components (the parameter β) and the rate of transitions
(the parameter τ ). We evaluate the error in two ways. The
first is by the difference between the true log-likelihood and
our estimate. The second is by the average relative error in
the estimate of different expected sufficient statistics deP |θ̂ −θ |
fined by j jθj j where θj is exact value of the j’th ex-

98

COHN ET AL.

pected sufficient statistics and θ̂j is the approximation.
Applying our procedure on an Ising chain with 8
components, for which we can still perform exact inference, we evaluated the relative error for different
choices of β and τ . The evidence in this experiment is
e0 = {+, +, +, +, +, +, −, −}, T = 0.64 and eT =
{−, −, −, +, +, +, +, +}. As shown in Fig. 2a, the error
is larger when τ and β are large. In the case of a weak coupling (small β), the posterior is almost independent, and
our approximation is accurate. In models with few transitions (small τ ), most of the mass of the posterior is concentrated on a few canonical “types” of trajectories that can
be captured by the approximation (as in Example 4.3). At
high transition rates, the components tend to transition often, and in a coordinated manner, which leads to a posterior that is hard to approximate by a product distribution.
Moreover, the resulting free energy landscape is rough with
many local maxima. Examining the error in likelihood estimates (Fig. 2b,c) we see a similar trend.
Next, we examine the run time of our approximation
when using fairly standard ODE solver with few optimizations and tunings. The run time is dominated by the time
needed to perform the backward-forward integration when
updating a single component, and by the number of such
updates until convergence. Examining the run time for different choices of β and τ (Fig. 3), we see that the run time
of our procedure scales linearly with the number of components in the chain. Moreover, the run time is generally
insensitive to the difficulty of the problem in terms of β.
It does depend to some extent on the rate τ , suggesting
that processes with more transitions require more iterations
to converge. Indeed, the number of iterations required to
achieve convergence in the largest chains under consideration are mildly affected by parameter choices. The scalability of the run time stands in contrast to the Gibbs sampling procedure [4], which scales roughly with the number
in transitions in the sampled trajectories. Comparing our
method to the Gibbs sampling procedure we see (Fig. 4)
that the faster Mean Field approach dominates the Gibbs
procedure over short run times. However, as opposed to
Mean Field, the Gibbs procedure is asymptotically unbiased, and with longer run times it ultimately prevails. This
evaluation also shows that adaptive integration procedure
in our methods strikes a better trade-off than using a fixed
time granularity integration.

7

Inference on Trees

The abovementioned experimental results indicate that our
approximation is accurate when reasoning about weaklycoupled components, or about time intervals involving few
transitions (low transition rates). Unfortunately, in many
domains we face strongly-coupled components. For example, we are interested in modeling the evolution of biological sequences (DNA, RNA, and proteins). In such systems,
we have a phylogenetic tree that represents the branching

UAI 2009

Figure 3: Evaluation of the run time of the approximation versus
the run time of exact inference as a function of the number of
components.

process that leads to current day sequences (see Fig. 5a). It
is common in sequence evolution to model this process as
a continuous-time Markov process over a tree [6]. More
precisely, the evolution along each branch is a standard
continuous-time Markov process, and branching is modeled by a replication, after which each replica evolves independently along its sub-branch. Common applications
are forced to assume that each character in the sequence
evolves independently of the other.
In some situations, assuming an independent evolution
of each character is highly unreasonable. Consider the evolution of an RNA sequence that folds onto itself to form
a functional structure. This folding is mediated by complementary base-pairing (A-U, C-G, etc) that stabilizes the
structure. During evolution, we expect to see compensatory
mutations. That is, if a A changes into C then its basedpaired U will change into a G soon thereafter. To capture
such coordinated changes, we need to consider the joint
evolution of the different characters. In the case of RNA
structure, the stability of the structure is determined by
stacking potentials that measure the stability of two adjacent pairs of interacting nucleotides. Thus, if we consider
a factor network to represent the energy of a fold, it will
have structure as shown in Fig. 5b. We can convert this
factor graph into a CTBN using procedures that consider
the energy function as a fitness criteria in evolution [3, 16].
Unfortunately, inference in such models suffers from computational blowup, and so the few studies that deal with it
explicitly resort to sampling procedures [16].
To consider trees, we need to extend our framework to
deal with branching processes. In a linear-time model, we
view the process as a map from [0, T ] into random variables
X (t) . In the case of a tree, we view the process as a map
from a point t = hb, ti on a tree T (defined by branch b
and the time t within it) into a random variable X (t) . Similarly, we generalize the definition of the Markov-consistent
density set η to include functions on trees. We define continuity of functions on trees in the obvious manner.
The variational approximation on trees is thus similar

UAI 2009

COHN ET AL.

Figure 4: Evaluation of the run time vs. accuracy trade-off for
several choices of parameters for Mean Field and Gibbs sampling
on the branching process of Fig. 5(a).

99

Figure 5: (a) An example of a phylogenetic tree. Branch lengths
denote time intervals between events with the interval used for
the comparison in Fig. 6a highlighted. (b) The form of the energy
function for encoding RNA folding, superimposed on a fragment
of a folded structure; each gray box denotes a term that involves
four nucleotides. (c) Illustration of the ODE updates on a directed
tree.

Figure 6: (a) Comparison of exact vs. approximate inference along the branch from C to D in the tree of Fig. 5(a) with and without
additional evidence at other leaves. Exact marginals are shown in solid lines, whereas approximate marginal are in dashed lines. The
two panels show two different components. (b) Evaluation of the relative error in expected sufficient statistics for an Ising chain in
branching-time; compare to Fig. 2(a). (c) Evaluation of the estimated likelihood on a tree; compare to Fig. 2(b).

to the one on intervals. Within each branch, we deal with
the same update formulas as in linear time. We denote by
µixi (b, t) and ρixi (b, t) the messages computed on branch b
at time t. The only changes occur at vertices. Suppose we
have a branch b1 of length T1 incoming into vertex v, and
two outgoing branches b2 and b3 (see Fig. 5c). Then we
use the following updates for µixi and ρixi
µixi (bk , 0) = µixi (b1 , T1 ) k = 2, 3,
ρixi (b1 , T1 ) = ρixi (b2 , 0)ρixi (b3 , 0).
The forward propagation of µi simply uses the value at the
end of the incoming branch as initial value for the outgoing
branches. In backward propagation of ρi the value at the
end of b1 is the product of the values at the start of the
two outgoing branches. This is the natural operation when
we recall the interpretation of ρi as the probability of the
downstream evidence given the current state.
When switching to trees, we increase the amount of evidence about intermediate states. Consider for example the

tree of Fig. 5a. We can view the span from C to D as an
interval with evidence at its end. When we add evidence at
the tip of other branches we gain more information about
intermediate points between C and D. To evaluate the impact of these changes on our approximation, we considered the tree of Fig. 5a, and compared it to inference in
the backbone between C and D (Fig. 2). Comparing the
true marginal to the approximate one along the main backbone (see Fig. 6a) we see a major difference in the quality
of the approximation. The evidence in the tree leads to a
much tighter approximation of the marginal distribution. A
more systematic comparison (Fig. 6b,c) demonstrates that
the additional evidence reduces the magnitude of the error
throughout the parameter space.
As a more demanding test, we applied our inference
procedure to the model introduced by Yu and Thorne [16]
for a stem of 18 interacting RNA nucleotides in 8 species in
the phylogeny of Fig. 5a. We compared our estimate of the
expected sufficient statistics of this model to these obtained

100

COHN ET AL.

UAI 2009
our variational procedure to generate initial distribution
for Gibbs sampling skip the initial burn-in phase and produce accurate samples. Another attractive aspect of this
new variational approximation is its potential use for learning model parameters from data. It can be easily combined with the EM procedure for CTBNs [10], to obtain
a Variational-EM procedure for CTBNs, which monotonically increases the likelihood by alternating between steps
that improve the approximation η (the updates discussed
here) and steps that improve the model parameters θ.

Figure 7: Comparison of estimates of expected sufficient statistics in the evolution of 18 interacting nucleotides, using a realistic
model of RNA evolution. Each point is an expected statistic value;
the x-axis is the estimate by the variational procedure, whereas
the y-axis is the estimate by Gibbs sampling.

by the Gibbs sampling procedure. The results, shown in
Fig. 7, demonstrate that over all, the two approximate inference procedures are in good agreement about the value
of the expected sufficient statistics.

8

Discussion

In this paper we formulate a general variational principle
for continuous-time Markov processes (by reformulating
and extending the one proposed by Opper and Sanguinetti
[12]), and use it to derive an efficient procedure for inference in CTBNs. In this mean field-type approximation, we
use a product of independent inhomogeneous processes to
approximate the multi-component posterior. Our procedure
enjoys the same benefits encountered in discrete time mean
field procedure [8]: it provides a lower-bound on the likelihood of the evidence and its run time scales linearly with
the number of components. Using asynchronous updates
it is guaranteed to converge, and the approximation represents a consistent joint distribution. It also suffers from expected shortcomings: there are multiple local maxima, and
it cannot captures certain complex interactions in the posterior. By using a time-inhomogeneous representation, our
approximation does capture complex patterns in the temporal progression of the marginal distribution of each component. Importantly, the continuous time parametrization enables straightforward implementation using standard ODE
integration packages that automatically tune the trade-off
between time granularity and approximation quality. We
show how to extend it to perform inference on phylogenetic
trees, and show that it provides fairly accurate answers in
the context of a real application.
One of the key developments here is the shift from
(piecewise) homogeneous parametric representations to
continuously inhomogeneous representations based on
marginal density sets. This shift increases the flexibility
of the approximation and, somewhat surprisingly, also significantly simplifies the resulting formulation.
A possible extension of the ideas set here is to use

Acknowledgments
We thank the anonymous reviewers for helpful remarks on
previous versions of the manuscript. This research was
supported in part by a grant from the Israel Science Foundation. Tal El-Hay is supported by the Eshkol fellowship
from the Israeli Ministry of Science.



A central task in many applications is reasoning about processes that change over continuous time. Recently, Nodelman et al. introduced
continuous time Bayesian networks (CTBNs), a
structured representation for representing Continuous Time Markov Processes over a structured
state space. In this paper, we introduce continuous time Markov networks (CTMNs), an alternative representation language that represents a
different type of continuous-time dynamics, particularly appropriate for modeling biological and
chemical systems. In this language, the dynamics of the process is described as an interplay
between two forces: the tendency of each entity to change its state, which we model using a
continuous-time proposal process that suggests
possible local changes to the state of the system
at different rates; and a global fitness or energy
function of the entire system, governing the probability that a proposed change is accepted, which
we capture by a Markov network that encodes
the fitness of different states. We show that the
fitness distribution is also the stationary distribution of the Markov process, so that this representation provides a characterization of a temporal
process whose stationary distribution has a compact graphical representation. We describe the
semantics of the representation, its basic properties, and how it compares to CTBNs. We also
provide an algorithm for learning such models
from data, and demonstrate its potential benefit
over other learning approaches.

1

Introduction

In many applications, we reason about processes that
evolve over time. Such processes can involve short time
scales (e.g., the dynamics of molecules) or very long ones
(e.g., evolution). In both examples, there is no obvious discrete “time unit” by which the process evolves. Rather, it is
more natural to view the process as changing in a continuous time: the system is in some state for a certain duration,

Raz Kupferman
Institute of Mathematics
The Hebrew University
raz@math.huji.ac.il

and then transitions to another state. The language of continuous time Markov processes (CTMPs) provides an elegant mathematical framework to reason about the probability of trajectories of such systems. Unfortunately, when we
consider a system with multiple components, this representation grows exponentially in the number of components.
Thus, we aim to construct a representation language for
CTMPs that can compactly encode natural processes over
high-dimensional state spaces. Importantly, the representation should also facilitate effective inference and learning.
Recently, Nodelman et al. [8, 9, 10, 11] introduced the
representation language of continuous time Bayesian networks (CTBNs), which provides a factorized, componentbased representation of CTMP: each component is characterized by a conditional CTMP dynamics, which describes
its local evolution as a function of the current state of its
parents in the network. This representation is natural for
describing systems with a sparse structure of local influences between components. Nodelman et al. provide algorithms for efficient approximate inference in CTBNs, and
for learning them from both complete and incomplete data.
In this paper, we introduce continuous time Markov networks, which have a different representational bias. Our
motivating example is modeling the evolution of biological
sequences such as proteins. In this example, the state of the
system at any given time is the sequence of amino acids encoded by the gene of interest. As evolution progresses, the
sequence is continually modified by local mutations that
change individual amino acids. The mutations for different
amino acids occur independently, but the probability that
these local mutations survive depends on global aspects of
the new sequence. For example, a mutation may be accepted only if the new sequence of amino acids folds properly into a functional protein, which occurs only if pairs of
amino acids that are in contact with each other in the folded
protein have complementary charges. Thus, although the
modifications are local, global constraints on the protein
structure and function introduce dependencies.
To capture such situations, we introduce a representation where we specify the dynamics of the process using
two components. The first is a proposal process that attempts to change individual components of the system. In
our example, this process will determine the rate of random

mutations in protein sequences. The second is an equilibrium distribution, which encodes preferences over global
configurations of the system. In our example, an approximation of the fitness of the folded protein. The equilibrium distribution is a static quantity that encodes preferences among states of the system, rather than dynamics of
changes. The actual dynamics of the system are determined
by the interplay between these two forces: local mutations
and global fitness. We represent the equilibrium distribution compactly using a Markov network, or, more generally, a feature-based log-linear model.
Importantly, as we shall see, the equilibrium distribution parameter is indeed the equilibrium distribution of
the process. Thus, our representation provides an explicit
representation of both the dynamics of the system and its
asymptotic limit. Moreover, this representation ensures
that the equilibrium distribution has a pre-specified simple structure. Thus, we can view our framework as a
continuous-time Markov network (CTMN) — a Markov
network that evolves over continuous time. From a different perspective, our representation allows us to capture a
family of temporal processes whose stationary distribution
has a certain locality structure. Such processes occur often
in biological and physical systems. For example, recent results of Socolich et al. [13] suggest that pairwise Markov
networks can fairly accurately capture the fitness of protein
sequences.
We provide a reduction from CTMNs to CTBNs, allowing us to use CTBN algorithms [7, 11] to perform effective
approximate inference in CTMNs. More importantly, we
also provide a procedure for learning CTMN parameters
from data. This procedure allows us to estimate the stationary distribution from observations of the system’s dynamics. This is important in applications where the stationary
distribution provides insight about the domain of application. In the protein evolution example, the stationary distribution provides a description of the evolutionary forces
that shape the protein and thus gives important clues about
protein structure and function.

2

Reversible Continuous Time Markov
Processes

We now briefly summarize the relevant properties of continuous time Markov processes that will be needed below.
We refer the interested reader to Taylor and Karlin [14] and
Chung [2] for more thorough expositions. Suppose we have
a family of random variables {X(t) : t ≥ 0} where the
continuous index t denotes time. A joint distribution over
these random variables is a homogeneous continuous time
Markov process (CTMP) if it satisfies the Markov property

Pr(X(tk+1 )|X(tk ), . . . , X(t0 )) = Pr(X(tk+1 )|X(tk ))

for all tk+1 > tk > . . . > t0 , and time-homogeneity,
Pr(X(s + t) = y|X(s) = x) =
Pr(X(s0 + t) = y|X(s0 ) = x)
for all s, s0 and t > 0.
The dynamics of a CTMP are fully determined by the
Markov transition function,
px,y (t) = Pr(X(s + t) = y|X(s) = x),
where time-homogeneity implies that the right hand side
does not depend on s. Provided that the transition function
satisfies certain analytical properties (see [2]) the dynamics
are fully captured by a constant matrix Q — the rate, or
intensity matrix — whose entries qx,y are defined by
qx,y = lim
h↓0

px,y (h) − 1 {x = y}
,
h

(1)

where 1 {} is the indicator function which takes the value 1
when the condition in the argument holds and 0 otherwise.
The Markov process can also be viewed as a generative process: The process starts in some state x. After spending a
finite amount of time at x, it transitions, at a random time,
to a random state y 6= x. The transition times to the various states are exponentially distributed, with rate parameters qx,y . The diagonal elements of Q are set to ensure the
constraint that each row sums up to zero.
If the process satisfies certain conditions (reachability)
then the limit
π x = lim py,x (t)
t→∞

exists and is independent of the initial state y. That is, in
the long time limit, the probability of visiting state x is independent of the initial state at time 0. The distribution
π x is called the stationary distribution of the process. A
CTMP is called stationary if P (X(0) = x) = π x , that
is, if the initial state is sampled from the stationary distribution. A stationary CTMP is called reversible if for every
x, y, and t > 0
Pr(X(t) = y|X(0) = x) = Pr(X(0) = y|X(t) = x).
This condition implies that the process is statistically
equivalent to itself running backward in time. Reversibility is intrinsic to many physical systems where the microscopic dynamics are time-reversible. Reversibility can be
formulated as a property on the Markov transition function,
where for every x, y, and t > 0
π x px,y (t) = π y py,x (t).
This identity is known as the detailed balance condition.
To better understand the constraint, we can examine the
implications of reversibility on the rate matrix Q.

Proposition 2.1: A CTMP is reversible if and only if its rate
matrix can be expressed as
qx,y = π y sx,y ,
where sx,y are the entries of a symmetric matrix (that is,
sx,y = sy,x ).
In other words, in a reversible CTMP, the asymmetry in
transition rates can be interpreted as resulting entirely from
preferences of the stationary distribution.

3

Continuous Time Metropolis Processes

We start by considering a reformulation of reversible
CTMPs as a continuous time version of the Metropolis
sampling process. We view the process as an interplay between two factors. The first is an unbiased random process that attempts to transition between states of the system,
and the second is the tendency of the system to remain in
more probable states. This latter probability is taken to be
the stationary distribution of the process. The structure of
the process can be thought of as going through iterations
of proposed transitions that are either accepted or rejected,
similar to the Metropolis sampler [6].
To formally describe such a process, we need to describe these two components. The first is the unbiased proposal of transitions. These proposals occur at fixed rates.
We denote by rx,y the rate at which proposals to transition
x → y occur. This in effect defines a CTMP process with
rate matrix R. To ensure an unbiased proposal, we require
R to be symmetric. (The stationary distribution of a symmetric rate matrix is the uniform distribution.)
The second component is a decision whether to accept
or reject the proposed transition. The decision whether to
accept the transition x → y depends on the probability
ratio of these states at equilibrium. We assume that we are
given a target distribution, which should coincide with the
equilibrium distribution π. As we shall see, to reach the
target equilibrium distribution, the acceptance probability
should satisfy a simple condition. To make this precise, we
assume we have an acceptance function f that takes as an
argument the ratio π y /π x and returns the probability of
accepting transition x → y. This function should return a
value between 0 and 1, and satisfy the functional relation
 
1
f (z) = zf
.
(2)
z
Two functions that satisfy these conditions are
fMetropolis (z)

=

flogistic (z)

=

min(1, z)
1
.
1 + z1

The function fMetropolis is the standard one used in Metropolis sampling. The function flogistic is closely linked to logistic regression. It is continuously differentiable, which, as
we shall see, facilitates the subsequent analysis.

Formally, a continuous time Metropolis process is defined by a symmetric matrix R, a distribution π, and a
real-valued function f . The semantics of the process are
defined in a generative manner. Starting at an initial state
x, the system remains in the state until receiving a proposed
transition x → y with rate rx,y . This proposal is then accepted with probability f (π y /π x ). If it is accepted, the
system transitions to state y; otherwise it remains in state
x. This process is repeated indefinitely.
To formulate the statistical dynamics of the system,
consider a short time interval h. In this case, the probability
of a proposal of the transition x → y is roughly h · rx,y .
Since the proposed transition is accepted with probability
f (π y /π x ), we have:


πy
.
px,y (h) ≈ h · rx,y · f
πx
Plugging this into Eq. (1) we conclude that the off-diagonal
elements of Q are


πy
qx,y = rx,y · f
.
(3)
πx
Proposition 3.1: Consider a continuous time Metropolis
process defined as in Eq. (3). Then, this CTMP is reversible, and its stationary distribution is π.
Proof: The condition on f implies that




πy
1
πx
1
f
=
f
,
πy
πx
πx
πy
Thus, it follows that qx,y is of the form sx,y π y , i.e., that
the process is reversible. Moreover, it implies that the stationary distribution of the process is π.
The inverse result is also easy to obtain.
Proposition 3.2: Any reversible CTMP can be represented
as a continuous time Metropolis process.
Proof: According to Proposition 2.1 we can write qx,y =
π y sx,y for a symmetric matrix sx,y . Define
πy
rx,y = sx,y   ,
π
f πxy
 
π
so that qx,y = rx,y · f πxy . Together, sx,y = sy,x and
Eq. (2) imply that rx,y = ry,x . Thus, R is symmetric and
together with π defines a continuous time Metropolis process which is equivalent to the original reversible CTMP.
We conclude that continuous time Metropolis processes
are a general reparameterization of reversible CTMPs.

4

Continuous Time Markov Networks

We are interested in dealing with structured, multicomponent systems, whose state description can be viewed

as an assignment to some set of state variables X =
hX1 , X2 , . . . , Xn i, where each Xi assumes a finite set of
values. The main challenge is dealing with the large state
space (exponential in n). We aim to find succinct representations of the system’s dynamics within the framework
of continuous time Metropolis processes. We do so in two
stages, first dealing with the proposal rate matrix R, and
then with the equilibrium distribution π.
Our first assumption is that proposed transitions are local. Specifically, we require that, for x 6= y
 i
rxi ,yi
(xj = yj ) ∀j 6= i
rx,y =
(4)
0
otherwise
where Ri = {rxi i ,yi } are symmetric local transition rates
for Xi . Thus, we allow only one component to change at
a time and the proposal rates do not depend on the global
state of the system.
The second assumption concerns the structure of the
stationary distribution π. Log-linear models or Markov
networks provide a general framework to describe structured distributions. A log-linear model is described by a set
of features, each one encoding a local property of the system that involves few variables. For example, the function
1 {X1 = X2 } is a feature that only involves two variables.
A feature-based Markov network is defined by a vector of features, s = hs1 , . . . , sK i, where each feature sk
assigns a real number to the state of the system. We further assume that each feature sk is a function of a (usually
small) subset D k ⊆ X of variables. We use the notation x|Dk to denote the projection of x on the subset of
variables D k . Thus, sk is a function of x|Dk ; however,
for notational convenience, we sometimes use sk (x) as a
shorthand for sk (x|Dk ).
Based on a set of features, we define a distribution by
assigning different weights to each feature. These weights
represent the relative importance of each feature. We use
the notation θ = hθ1 , . . . , θK i ∈ IRK to denote the vector of weights or parameters. The equilibrium distribution
represented by s and θ takes the log-linear form
(
)
X
1
πx =
exp
θk · sk (x|Dk ) ,
(5)
Z(θ)
k

where the partition function Z(θ) is the normalizing factor.
The structure of the equilibrium distribution can be represented as an undirected graph G — the nodes of G represent the variables {X1 , . . . , Xn }. If Xi , Xj ∈ D k for
some k, then there is an edge between the corresponding
nodes. Thus, for every feature sk , the nodes that represent
the variables in D k form a clique in the graph G. We define
the Markov Blanket, NG (i), of the variable Xi as the set of
neighbors of Xi in the graph G [12].
Example 4.1 :
Consider a four-variable process
{X1 , X2 , X3 , X4 }, where each variable takes binary

X1

X1
X4

(a)

X4

X2
X3

(b)

X2
X3

Figure 1: (a) The Markov network structure for Example 4.1. (b) The corresponding CTBN structure.
values, with the following set of features:
s1 (X1 ) = 1 {X1 = 1}

s5 (X1 , X2 ) = 1 {X1 = X2 }

s2 (X2 ) = 1 {X2 = 1}

s6 (X2 , X3 ) = 1 {X2 = X3 }

s3 (X3 ) = 1 {X3 = 1}

s7 (X3 , X4 ) = 1 {X3 = X4 }

s4 (X4 ) = 1 {X4 = 1}

s8 (X1 , X4 ) = 1 {X1 = X4 }

Note that all these features involve at most two variables.
The corresponding graph structure is shown in Figure 1(a).
In this example N(1) = {X2 , X4 }, N(2) = {X1 , X3 },
N(3) = {X2 , X4 }, and N(4) = {X1 , X3 }.
We now take advantage of the structured representation
of both R and π to get a more succinct representation of
the rate matrix Q of the process. We exploit the facts that
π appears explicitly in the rate only as a ratio π y /π x , and
moreover that the proposal process includes only transitions that modify a single variable. Thus, we only examine
ratios where y and x agree on all variables but one. It is
straightforward to show that if x and y are two states that
are identical except for the value of Xi and ui = x|N(i) ,
then
π y /π x = gi (xi → yi |ui ),
where
gi (xi → yi |ui ) =
(
)
X
exp
θk [sk (yi , ui ) − sk (xi , ui )] .
k:Xi ∈D k

Note that, if Xi ∈ D k , then D k ⊆ N(i) ∪ {Xi }. Thus, the
function gi is well defined.
Thus, the acceptance probability of a change in Xi depends only on the state of variables in its Markov blanket.
This property is heavily used for Gibbs sampling in Markov
networks. Depending on the choice of features, these dependencies can be very sparse, or involve all the variables
in the process.
To summarize, assuming a local form for R and a loglinear form for π, we can further simplify the definition of
the rate matrix Q. If x and y are two states that differ only
in the i’th variable, then
qx,y = rxi i ,yi f (gi (xi → yi |ui )),

(6)

where ui = x|N(i) . All other off-diagonal entries are 0,
and the diagonal entries are set to ensure that the sum of

each row is 0. We call a process with a Q matrix of the
form Eq. (6) a Continuous time Markov Network (CTMN).
One consequence of the form of the CTMN rate matrix
Eq. (6) is that the dynamics of the i’th variable depend directly only on the dynamics of its neighbors. As we can
expect, we can use this property to discuss independencies
among variables in the network. However, since we are
examining a continuous process, we need to consider independencies between full trajectories (see also [8]).
Theorem 4.2: Consider a CTMN with a stationary distribution represented by a graph G. If A, B, C are subsets
of X such that C separates A from B in G, then the trajectories of A and B are conditionally independent, given
observation of the full trajectory of C.
Proof: (sketch) Using the global independence properties
of a Markov network (see for example, [12]), we have that
π can be written as a product of two function each with its
own domain X 1 and X 2 such that X 1 ∩ X 2 = C and
A ⊆ X 1 and B ⊆ X 2 . Once the trajectories of variables in C are given, the dynamics of variables in X 1 − C
and X 2 − C are two independent CTMNs, each with its
own stationary distribution. As a consequence we get the
desired independence.
That is, the usual conditional separation criterion in
Markov networks [12] applies in a trajectory-wise fashion
to CTMNs.
It is important to note that although we can represent
any reversible CTMP as a continuous time Metropolis process, once we move to CTMNs this is no longer the case.
The main restriction is that, in CTMNs as we have defined them, each transition involves a change in the state
of exactly one component. Thus, although the language
of Markov networks allow to describe arbitrary equilibrium distributions (potentially with an exponential number
of features), the restrictions on R limit the range of processes we can describe as CTMNs. As an example of a
domain where CTMNs are not suitable, consider reasoning
about biochemical systems, where each component of the
state is the number of molecules of a particular species and
transitions correspond to chemical reactions. For example,
a reaction might be one that takes an OH molecule and
an H molecule and replace them by an H2 O molecule. If
reactions are reversible (i.e., H2 O can break into OH and
H molecules), then this process might be described by a
reversible CTMP. However, since reactions change several
components at once, we cannot describe such system as a
CTMN.

5

Connection to CTBNs

The factored form of Eq. (6) allows us to relate CTMNs
with CTBNs. A CTBN is defined by a directed (often
cyclic) graph whose nodes correspond to variables of the
process, and whose edges represent direct influences of
one variable on the evolution of another. More precisely,

a CTBN is defined by a collection of conditional rate matrices (also called conditional intensity matrices). For each
Xi , and for each possible value ui of its direct parents in
the CTBN graph, the matrix QXi |ui is a rate matrix over
the state space of Xi . These conditional rate matrices are
combined into a global rate matrix by a process Nodelman
et al. [9] call amalgamation. Briefly, if x and y are identical
except for the value of Xi , then
qx,y = qxXii,y|ui i

(7)

where ui = x|Pai is the assignment to Xi ’s parents in
the state x. That is, the rate of transition from x to y is
the conditional rate of Xi changing from xi to yi given the
state of its parents. Again, all other off-diagonal elements,
where more than one variable changes, are set to 0.
This form is similar to the rate matrix of CTMNs shown
in Eq. (6). Thus, given a CTMN, we can build an equivalent
CTBN by setting the parents of each Xi to be N(i), and
using the conditional rates:
qxXii,y|ui i = rxi i ,yi gi (xi → yi | x|N(i) )

(8)

Figure 1(b) shows the CTBN structures corresponding to
the CTMN of Example 4.1. In general, the CTBN graph
corresponding to a given CTMN is built by replacing each
undirected arc by a pair of directed ones. This matches the
intuition that if Xi and Xj appear in the context of some
feature, then they mutually influence each other.
As this transformation shows, the class of processes
that can be encoded using CTMNs is a subclass of CTBNs.
In a sense, this is not surprising, as a CTBN can encode any
Markov process where at most one variable can transition
at a time. However, the CTMN representation imposes a
particular parametrization of the system dynamics in terms
of the local proposal process and the global equilibrium
distribution. This parametrization violates both local and
global parameter independence [5] in the resulting CTBN.
In particular, a transition between xi and yi is proposed at
the same rate, regardless of whether it is globally advantageous (in terms of equilibrium preferences). As we shall
see, this property is important for our ability to effectively
estimate these rate parameters.
Moreover, as we have seen, this parametrization guarantees that the stationary distribution of the process factorizes as a particular Markov network. In general, even
a fairly sparse CTBN gives rise to a fully entangled stationary distribution that cannot be factorized. Indeed, even
computing the stationary distribution of a given CTBN is a
hard computational problem. By contrast, we have defined
a model of temporal dynamics that gives rise to a natural
and interpretable form for the stationary distribution. This
property is critical in applications where the stationary distribution is the key element in understanding the system.
Yet, the ability to transform a CTMN into a CTBN allows us to harness the recently developed approximate in-

ference methods for CTBNs [11, 7], including for the Estep used when learning CTMNs for partially observable
data.

6

Parameter Learning

We now consider the problem of learning the parametrization of CTMNs from data. Thus, we assume we are given
the form of π, that is, the set of features s, and need to
learn the parameters θ governing π and the local rate matrices Ri that govern the proposal rates for each variable.
We start by considering this problem in the context of complete data, where our observations consist of full trajectories of the system. As we show, we define a gradient ascent
procedure for learning the parameters from such data.
This result also enables us to learn from incomplete
data using the standard EM procedure. Namely, we can use
existing CTBNs inference algorithms to perform the E-step
effectively when learning from partially observable data to
compute expected sufficient statistics. The M-step is then
an application of the learning procedure for complete data
with these expected sufficient statistics. This combination
is quite standard and follows the lines of similar procedure
for CTBNs [10], and therefore we do not expand on it here.
6.1

(a)

The Likelihood Function

A key concept in addressing the learning problem is the
likelihood function, which determines how the probability
of the observations depends on the parameters.
We assume that the data is complete, and thus our observations consist of a trajectory of the system that can be
described as a sequence of intervals, where in each interval
the system is in one state. Using the relationship to CTBNs,
we can use the results of Nodelman et al. [9] to write the
probability of the data as a function of sufficient statistics
and entries in the conditional rate matrices of Eq. (8). A
problem with this approach is that the entries in the conditional rate matrix involve both parameters from Ri and
parameters from θ. Thus, the resulting likelihood function
couples the estimation of these two sets of parameters.
However, if we had additional information, we could
decouple these two sets of parameters. Suppose we observe
not only the actual trajectories, but also the rejected proposals; see Figure 2. With this additional information, we can
estimate the rate of different proposals, independently of
whether they were accepted or not. Similarly, we can estimate the equilibrium distribution from the accepted and
rejected proposals. Thus, we are going to view our learning problem as a partial data problem where the annotation
of rejected proposals is the missing data.
To formalize these ideas, assume that our evidence is a
trajectory annotated with proposal attempts. We describe
such a trajectory using three vectors; see Figure 2. The first
vector, τ = hτ [1], . . . , τ [M + 1]i, represents the time intervals between consecutive proposals. Thus, the first pro-

(b)
τ[8]

x[8]

y[8]
1

2 3

4 5

6

7

8

9

10

11 12 13

14

15

Figure 2: An illustration of training data. (a) A complete
trajectory. The x-axis denotes time and the y-axis denotes
the state at each time. Filled circles denote transitions. (b)
A trajectory annotated with accepted and rejected proposals (closed and open circles, respectively). (Remember that
accepted proposals lead to a transition.) The marks on the
x-axis denote the index of the proposal. We illustrate the
notation we use in the text, where τ [i] denotes the time interval before the i’th proposal, x[i] denote the actual state
after the i’th proposal, and y[i] denote the proposed state
in the i’th proposal.

posal took place at time τ [1], the second at time τ [1] + τ [2],
and so on. The last entry in this vector is the time between the last proposal and the end of the observed time
interval. The second vector, Ξ = hx[0], x[1], . . . , x[M ]i,
denotes the actual state of the system after each proposal
was made. Thus, x[0] is the initial state of the system,
x[1] is the state after the first proposal, and so on. Finally, Υ = hy[1], . . . , y[M ]i denotes the sequence of proposed states. Clearly, the m’th proposal was accepted if
y[m] = x[m] and rejected otherwise. We denote these
event using the indicators S[m] = 1 {x[m] = y[m]}.
The likelihood of these observations is the product of
the probability density of the duration between proposals,
and the probability of accepting or rejecting each proposal.
Plugging in the factored form of R and π we can write this
likelihood in a compact form.
Proposition 6.1: Given an augmented data set, τ , Ξ, and
Υ, the log-likelihood can be decomposed as
`(θ, {Ri } : τ , Ξ, Υ) =

n
X

`r,i (Ri : τ ) + `s (θ : Ξ, Υ),

i=1

such that
`r,i (Ri : τ ) =

X
xi 6=yi


M [xi , yi ] ln rxi i ,yi − rxi i ,yi T [xi ]

and
`s (θ : Ξ, Υ) =
n X X
X
i=1 ui xi 6=yi
n X X
X

M a [xi , yi |ui ] ln f (gi (xi → yi |ui )) +
M r [xi , yi |ui ] ln(1 − f (gi (xi → yi |ui )))

i=1 ui xi 6=yi

where M a [xi , yi |ui ] is the number of accepted transitions
of Xi from xi to yi when N(i) = ui , M r [xi , yi |ui ] is the
count of rejected proposals to make the same transition,
M [xi , yi |ui ] = M a [xi , yi |ui ]+M r [xi , yi |ui ], and T [xi ]
is the time spent in states where Xi = xi .
Note that if we use flogistic , then, as ln((1 + e−x )−1 )
is concave, the likelihood function `s (θ : Ξ, Υ) is concave
and has a unique maximum.
6.2

Maximizing the Likelihood Function

Under the Maximum Likelihood Principle, our estimated
parameters are the ones that maximize the likelihood function given the observations. We now examine how to maximize the likelihood. The decoupling of the likelihood into
several terms allows us to estimate each set of parameters
separately.
The estimation of Ri is straightforward: imposing the
symmetry condition, the maximum likelihood estimate is
rxi i ,yi =

M [xi , yi ] + M [yi , xi ]
.
T [xi ] + T [yi ]

Finding the maximum likelihood parameters of π is
somewhat more involved. Note that the likelihood `s (θ :
Ξ, Υ) is quite different from the likelihood of a log-linear
distribution given i.i.d. data [3]. The probability of acceptance or rejection involves ratios of probabilities. Therefore, the partition function Z(θ) cancels out, and does not
appear in the likelihood.
In a sense, our likelihood is closely related to the
pseudo-likelihood for log-linear models [1]. Recall that
pseudo-likelihood is a technique for estimating the parameters of a Markov network (or log-linear model) that uses
a different objective function. Rather than optimizing the
joint likelihood, one optimizes a sum of log conditional
likelihood terms, one for each variable given its neighbors.
By considering the conditional probability of a variable
given its neighbors, the partition function cancels out, allowing the parameters to be estimated without the use of inference. At the large sample limit, optimizing the pseudolikelihood criterion is equivalent to optimizing the joint
likelihood, but the results for finite sample sizes tend to
be worse. In our setting, the generative model is defined in
terms of ratios only. Thus, in this case the exact likelihood
turns out to take a form similar to the pseudo-likelihood
criterion. As for pseudo-likelihood, this form allows us to

perform parameter estimation without requiring inference
in the underlying Markov network.
In the absence of an analytical solution for this equation we learn the parameters using a gradient-based optimization procedure to find a (local) maximum of the likelihood. The derivation of the gradient is a standard exercise;
for completeness, we provide the details in the appendix.
When using flogistic we are guaranteed that such a procedure finds the unique global maximum.
6.3

Completing the Data

Our derivation of the likelihood and the associated optimization procedure relies on the assumption that rejected
transition attempts are also observed in the data. As we can
see from the form of the likelihood, these failures play an
important role in estimating the parameters. The question
is how to adapt the procedure to the case where rejected
proposals are not observed. Our solution to this problem is
to use Expectation Maximization, where we view the proposal attempts as the unobserved variables.
In this approach, we start with an initial guess of the
model parameters. We use these to estimate the expected
number of rejected proposals; we then treat these expected
counts as though they were real, and maximize the likelihood using the procedure described in the previous section.
We repeat these iterations until convergence.
The question is how to compute the expected number
of rejected attempts. It turns out that this computation can
be done analytically.
Proposition 6.2: Given a CTMN, and an observed trajectory τ , Ξ. Then,
IE [M r [xi , yi |ui ] |D]
=T

[xi |ui ] rxi i ,yi

(9)
(1 − f (g(xi , yi |ui )))

where T [xi |ui ] is the total amount of time the system was
in states where Xi = xi and N(i) = ui .
We see that, in this case, the E-step of EM is fairly
straightforward. The harder step is the M-step which requires an iterative gradient-based optimization procedure.
To summarize the procedure, to learn from complete
data we perform the following steps: We first collect sufficient statistics T [xi |ui ] and M a [xi , yi |ui ]. We then initialize the model with some set of parameters (randomly,
or using prior knowledge). We then iterate over the two
steps of EM until convergence: in the E-step, we complete the sufficient statistics with the expected number of
rejected attempts, as per Eq. (9); in the M-step, we perform
maximum likelihood estimation using the expected sufficient statistics, using gradient descent with the gradient of
Eq. (10).

7

A Numerical Example

To illustrate the properties of our CTMN learning procedure, we evaluated it on a small synthetic data set. We

Correct structure
0.14

0.14
CTMN
CTBN
Markov Network

0.12

0.1

0.08

(c)

0.06

KL distance

(b)

0.06

0.08
0.06

0.04

0.04

0.04

0.02

0.02

0.02

0

125

250
500
1000
Expected number of transitions

0

2000

CTMN
CTBN
Markov Network

0.12

0.1

0.08

KL distance

KL distance

0.1

(a)

0.14
CTMN
CTBN
Markov Network

0.12

125

250
500
1000
Expected number of transitions

0

2000

125

250
500
1000
Expected number of transitions

2000

Partial structure
CTMN
CTBN
Markov Network

0.25

(e)

0.2

0.15

(f)

KL distance

0.15

0.15

0.1

0.1

0.1

0.05

0.05

0.05

0

125

250
500
1000
Expected number of transitions

Equilibrium distribution

2000

0

CTMN
CTBN
Markov Network

0.25

0.2
KL distance

KL distance

0.2

(d)

CTMN
CTBN
Markov Network

0.25

125

250
500
1000
Expected number of transitions

2000

Uniform distribution (long)

0

125

250
500
1000
Expected number of transitions

2000

Uniform distribution (short)

Figure 3: Comparison of estimates of the equilibrium distribution by the CTMN learning procedure (solid lines), the CTBN
learning procedure (dashed lines) and a Markov Network parameter learning procedure applied to the frequency of time
spent in each state (dotted lines). The x-axis denotes the total length of training trajectories (measured in units of expected
number of observed transitions). The y-axis denotes the KL-divergence between the equilibrium distribution of the true
model and the estimated model. The curves report the median performance among 50 data sets, and the error bars report
25% − 75% percentiles. (a-c) report performance when learning with the true structure from which the data was generated,
and (d-f) report results when learning the parameters of a structure without the edges between X1 and X4 . In (a) and (d)
p(X(0)) is the equilibrium distribution. In (b) and (e) p(X(0)) is uniform and each trajectory is of length 25 time units.
In (c) and (f) p(X(0)) is uniform and each trajectory is of length 10 time units.
generated data from the CTMN model of Example 4.1
with θ = h−0.2, −2.3, 0.7, 0.7, −1.2, −1.2, −1.2, −1.2i
1
2
3
and proposal rates r0,1
= 1, r0,1
= 2, r0,1
= 3, and
4
r0,1 = 4.
The goal of our experiments is to test the ability of the
CTMN learning procedure to estimate stationary distributions from data in various conditions. As a benchmark, we
compared our procedure to two alternative methods:
• A procedure that estimates the stationary distribution
directly from the frequency of visiting each state. This
procedure is essentially the standard parameter learning method for Markov networks, where the weight of
each state (instance) is proportional to the duration in
which the process spends in that state. This procedure
uses gradient ascent to maximize the likelihood [3].
When the process is sampling from the stationary distribution, the relative time in each state is proportional
to its stationary probability, and in such situations we
expect this procedure to perform well.
• A procedure that estimates the Q-matrix of the associated CTBN shown in Figure. 1. Here we used the
methods designed for parameter learning of CTBNs
in [9]. Once that the Q-matrix has been estimated, the

estimated stationary distribution is the only normalized vector in its null space.
We examined these three procedures in three sets of
synthetic trajectories. The first set was generated by sampling the initial state X(0) of each trajectory from the stationary distribution and then sampling further states and
durations from the target model. In this data set the system is in equilibrium throughout the trajectory. The second
data set was generated by sampling the initial state from a
uniform distribution, and so the system starts in a distribution that is far from equilibrium. However, the trajectory is
long enough to let the system equilibrate. The third data set
is similar to the second, except that trajectories are shorter
and thus do not have sufficient time to equilibrate. To evaluate the effect of training set size, we repeated the learning
experiments with different numbers of trajectories. We report the size of the training set in terms of the total length of
training trajectories. Time is reported in units of expected
transition number. That is, one time unit is equal to the average time between transitions when the process is in equilibrium. The short and long trajectories in our experiments
are of length 10 and 25 expected transitions, respectively.
To evaluate the quality of the learned distribution, we

measured the Kullback-Leibler divergences from the true
stationary distribution to the estimated ones. Figures 3(ac) show the results of these experiments. When sampling
from the stationary distribution, the three procedures tend,
as the data size increases, toward the correct distribution.
For small data size, the performance of the CTMN learning procedure is consistently superior, although the error
bars partially overlap. We start seeing a difference between
the estimation procedures when we modify the initial distribution. As expected, the Markov network learning procedure suffers since it is learning from a biased sample. On
the other hand, the performance of the CTMN and CTBN
learning procedures is virtually unchanged, even when we
modify the length of the trajectories. These results illustrate
the ability of the CTMN and CTBN learning procedures to
robustly estimate the equilibrium distribution from the dynamics even when the sampled process is not at equilibrium.
To test the robustness to the network structure, we also
tested the performance of these procedures when estimating using a wrong structure. As we can see in Figures 3(df), while the three procedures converge to the wrong distribution, their relative behavior remains similar to the previous experiment, and the performance of the CTMN learning procedure is still not affected by the nature of the data.

8

Discussion and Future Work

In this paper, we define the framework of continuous time
Markov networks, where we model a dynamical system as
being governed by two factors: a local transition model,
and a global acceptance/rejection model (based on an equilibrium distribution). By using a Markov network (or
feature-based log-linear model) to encode the equilibrium
distribution, we naturally define a temporal process guaranteed to have an equilibrium distribution of a particular, factored form. We showed a reduction from CTMNs
to CTBNs that illustrates the differences in the expressive
powers of the two formalisms. Moreover, this reduction
allows us to reason in CTMNs by exploiting the efficient
approximate inference algorithms for CTBNs. Finally, we
provided learning algorithms for CTMNs, which allow us
to learn the equilibrium distribution in a way that exploits
our understanding about the system dynamics. We demonstrated on that this learning procedure is able to robustly estimate the equilibrium distribution even when the sampled
process is not at equilibrium. These results can be combined for learning from partial observations, by plugging
in the learning procedure as the M-step in the EM procedure for CTBNs [10].
This work opens many interesting questions. A key
goal in learning these models is to estimate the stationary
distribution. It is interesting to analyze, both theoretically
and empirically, the benefit gained in this task by accounting for the process dynamics, as compared to learning the
stationary distribution directly from a set of snapshots of

the system (e.g., a set of instances of a protein sequence
in different species). Moreover, so far, we have tackled
only the problem of parameter estimation in these models. In many applications, the model structure is unknown,
and of great interest. For example, in models of protein
evolution, we want to know which pair of positions in the
protein are directly correlated, and therefore likely to be
structurally interacting. Of course, tackling this problem
involves learning the structure of a Markov network, a notoriously difficult task. From the perspective of inference,
our reduction to CTBNs can lose much of the structure of
the model. For example, if the stationary distribution is
a pairwise Markov network, the fact that the interaction
model decomposes over pairs of variables is lost in the induced CTBN. It is interesting to see whether one can construct inference algorithms that better exploit this structure.
Finally, one important limitation of the CTMN framework
is the restriction to an exponential distribution on the duration between proposed state changes. Although such a
model is a reasonable one in many systems (e.g., biological sequence evolution), there are other settings where it is
too restrictive. In recent work, Nodelman et al. [10] show
how one can expand the framework of CTBNs to allow a
richer set of duration distributions. Essentially, their solution introduces a “hidden state” internal to a variable, so
that the overall transition model of the variable is actually
the aggregate of multiple transitions of its internal state. A
similar solution can be applied in our setting, but the resulting model would not generally encode a reversible CTMP.
One major potential field of application for this class of
models is sequence evolution. The current state of the art in
phylogenetic inference is based on continuous time probabilistic models of evolution [4]. Virtually all of these models assume that sequence positions evolve independently of
each other (although in some models, there are global parameters that induce weak dependencies). Our models provide a natural language for modeling such dependencies. In
this domain, the proposal process corresponds to mutation
rates within the sequence, and the equilibrium distribution
is proportional to the relative fitness of different sequences.
The latter function is of course very complex, but there is
empirical evidence that modeling pairwise interactions can
provide a good approximation [13]. Thus, in these systems,
both the local mutation process and a factored equilibrium
distribution are very appropriate, making CTMNs a potentially valuable tool for modeling and analysis. We hope to
incorporate this formalism within phylogenetic inference
tools, and to develop a methodology to leverage these models to provide new insights about the structure and function
of proteins.
Acknowledgments
We thank A. Jaimovich, T. Kaplan, M. Ninio, I. Wiener,
and the anonymous reviewers for comments on earlier versions of this manuscript. This work was supported by

grants from the Israel Science Foundation and the USIsrael Binational Science Foundation, and by DARPA’s
CALO program, under sub-contract to SRI International.

with a discontinuity when gi (xi → yi |ui ) = 1. We see
that, in this case, the updates are asymmetric, with maximal
weight to updates of accepted transitions.

A



A central task in many applications is reasoning about processes that change over continuous time. Continuous-Time Bayesian Networks
is a general compact representation language
for multi-component continuous-time processes.
However, exact inference in such processes is exponential in the number of components, and thus
infeasible for most models of interest. Here we
develop a novel Gibbs sampling procedure for
multi-component processes. This procedure iteratively samples a trajectory for one of the components given the remaining ones. We show how to
perform exact sampling that adapts to the natural
time scale of the sampled process. Moreover, we
show that this sampling procedure naturally exploits the structure of the network to reduce the
computational cost of each step. This procedure
is the first that can provide asymptotically unbiased approximation in such processes.

1

Introduction

In many applications, we reason about processes that
evolve over time. Such processes can involve short time
scales (e.g., the dynamics of molecules) or very long ones
(e.g., evolution). In both examples, there is no obvious discrete “time unit” by which the process evolves. Rather, it
is more natural to view the process as changing in a continuous time: the system is in some state for a certain duration, and then transitions to another state. The language of
continuous-time Markov processes (CTMPs) provides an
elegant mathematical framework to reason about the probability of trajectories of such systems (Gardiner, 2004). We
consider Markov processes that are homogeneous in time
and have a finite state space. Such systems are fully determined by the state space S, the distribution of the process
at the initial time, and a description of the dynamics of the
process. These dynamics are specified by a rate matrix Q,
whose off-diagonal entries qa,b are exponential rate intensities for transitioning from state a to b. Intuitively, we can
think of the entry qa,b as the rate parameter of an exponen-

Raz Kupferman
Institute of Mathematics
The Hebrew University
raz@math.huji.ac.il
tial distribution whose value is the duration of time spent in
state a before transitioning to b.
In many applications, the state space is of the form of a
product space S = S1 × S1 × · · · × SM , where M is the
number of components (such processes are called multicomponent). Even if each of the Si is of low dimension,
the dimension of the state space is exponential in the number of components, which poses representational and computational difficulties. Recently, Nodelman et al. (2002)
introduced the representation language of continuous-time
Bayesian networks (CTBNs), which provides a factorized,
component-based representation of CTMPs: each component is characterized by a conditional CTMP dynamics,
which describes its local evolution as a function of the current state of its parents in the network. This representation
is natural for describing systems with a sparse structure of
local influences between components.
For most applications of such CTMP models, we need
to perform inference to evaluate the posterior probability
of various queries given evidence. Exact inference requires
exponentiation of the rate matrix Q. As the rate matrix is
exponential in the number of components, exact computations are infeasible for more than a few components. Thus,
applications of factored CTMPs require the use of approximate inference.
In two recent works Nodelman et al. (2005) and Saria
et al. (2007) describe approximate inference procedures
based on Expectation Propagation, a variational approximation method (Minka, 2001; Heskes and Zoeter, 2002).
These approximation procedures perform local propagation of messages between components (or sub-trajectories
of components) until convergence. Such procedures can be
quite efficient, however they can also introduce a systematic error in the approximation (Fan and Shelton, 2008).
More recently, Fan and Shelton (2008) introduced a
procedure that employs importance sampling and particle
filtering to sample trajectories from the network. Such a
stochastic sampling procedure has anytime properties as
collecting more samples leads to more accurate approximation. However, since this is an importance sampler, it
has limited capabilities to propagate evidence “back” to influence the sampling of earlier time steps. As a result, when
the evidence is mostly at the end of the relevant time inter-

val, and is of low probability, the procedure requires many
samples. A related importance sampler was proposed by
Ng et al. (2005) for monitoring a continuous time process.
In this paper we introduce a new stochastic sampling
procedure for factored CTMPs. The goal is to sample
random system trajectories from the posterior distribution.
Once we have multiple independent samples from this distribution we can approximate the answer to queries about
the posterior using the empirical distribution of the samples. The challenge is to sample from the posterior. While
generative sampling of a CTMP is straightforward, sampling given evidence is far from trivial, as evidence modifies the posterior probability of earlier time points.
Markov Chain Monte Carlo (MCMC) procedures circumvent this problem by sampling a stochastic sequence
of system states (trajectories in our models) that will eventually be governed by the desired posterior distribution.
Here we develop a Gibbs sampling procedure for factored
CTMPs. This procedure is initialized by setting an arbitrary trajectory which is consistent with the evidence. It
then alternates between randomly picking a component Xi
and sampling a trajectory from the distribution of Xi conditioned on the trajectories of the other components and
the evidence. This procedure is reminiscent of block Gibbs
sampling (Gilks et al., 1996) as we sample an entire trajectory rather than a single random variable in each iteration.
However, in our approach we need to sample a continuous
trajectory.
The crux of our approach is in the way we sample a trajectory for a single component from a process that is conditioned on trajectories of the other components. While
such a process is Markovian, it is not homogeneous as its
dynamics depends on trajectories of its Markov Blanket as
well as on past and present evidence. We show that we can
perform exact sampling by utilizing this Markovian property, and that the cost of this procedure is determined by the
complexity of the current trajectories and the sampled one,
and not by a pre-defined resolution parameter. This implies
that the computational time adapts to the complexity of the
sampled object.

2

Continuous-Time Bayesian Networks

In this section we briefly review the CTBN model (Nodelman et al., 2002). Consider an M -component Markov process
(t)
(t)
(t)
X (t) = (X1 , X2 , . . . XM )
with state space S = S1 × S2 × · · · × SM .
A notational convention: vectors are denoted by boldface symbols, e.g., X, a, and matrices are denoted by
blackboard style characters, e.g., Q. The states in S are
denoted by vectors of indexes, a = (a1 , . . . , aM ). The indexes 1 ≤ i, j ≤ M are used to enumerate the components.
(t)
We use the notation X (t) and Xi to denote a random variable at time t. We will use X [s,t] , X (s,t] , X [s,t) , to denote

the state of X in the closed and semi-open intervals from s
to t.
The dynamics of a time-homogeneous continuous-time
Markov process are fully determined by the Markov transition function,
pa,b (t) = Pr(X (t+s) = b|X (s) = a),
where time-homogeneity implies that the right-hand side
does not depend on s. Provided that the transition function satisfies certain analytical properties (continuity, and
regularity; see Chung (1960)) the dynamics are fully captured by a constant matrix Q—the rate, or intensity matrix—whose entries qa,b are defined by
qa,b = lim
h↓0

pa,b (h) − δa,b
,
h

where δa,b is a multivariate Kronecker delta.
A Markov process can also be viewed as a generative
process: The process starts in some state a. After spending a finite amount of time at a, it transitions, at a random
time, to a random state b 6= a. The transition times to the
various states are exponentially distributed, with rate parameters qa,b . The diagonal elements of Q are set such that
each row sums up to zero.
The time-dependent probability distribution, p(t),
whose entries are defined by
pa (t) = Pr(X (t) = a),

a ∈ S,

satisfies the so-called forward, or master, equation,
dp
= QT p.
dt

(1)

Thus, using the Q matrix, we can write the Markov transition function as
pa,b (t) = [exp(tQ)]a,b ,
that is, as the a, b entry in the matrix resulting from exponentiating Q (using matrix exponentiation).
It is important to note that the master Eq. (1) encompasses all the statistical properties of the Markov process.
There is a one-to-one correspondence between the description of a Markov process by means of a master equation, and by means of a “pathwise” characterization (up to
stochastic equivalence of the latter; see Gikhman and Skorokhod (1975)).
Continuous-time Bayesian Networks provide a compact representation of multi-component Markov processes
by incorporating two assumptions: (1) every transition involves a single component; (2) each component undergoes
transitions at a rate which depends only on the state of a
subsystem of components.
Formally, the structure of a CTBN is defined by assigning to each component i a set of indices Par(i) ⊆

{1, . . . , M } \ {i}. With each component i, we associate
i| Par(i)
a conditional rate matrix Qi| Par(i) with entries qai ,bi |ui
where ai and bi are states of Xi and ui is a state of Par(i).
This matrix defines the rate of Xi as a function of the state
of its parents. Thus, when the parents of Xi change state,
the rates governing its transition can change.
The formal semantics of CTBNs is in terms of a joint
rate matrix for the whole process. This rate matrix is defined by combining the conditional rate matrices


M
X
Y
i|
Par(i)
q
qa,b =
δaj ,bj  .
(2)
ai ,bi | Pi (a)
i=1

j6=i

where Pi (a) is a projection operator that project a complete assignment a to an assignment to the Par(i) components. Eq. (2) is, using the terminology of Nodelman et al.
(2002), the “amalgamation” of the M conditional rate matrices. Note the compact representation, which is valid for
both diagonal and off-diagonal entries. It is also noteworthy that amalgamation is a summation, rather than a product; indeed, independent exponential rates are additive. If,
for example, every component has d possible values and k
parents, the rate matrix requires only M dk+1 (d − 1) parameters, rather than dM (dM − 1).
The dependency relations between components can be
represented graphically as a directed graph, G, in which
each node corresponds to a component, and each directed
edge defines a parent-child relation. A CTBN consists of
such a graph, supplemented with a set of M conditional
rate matrices Qi| Par(i) . The graph structure has two main
roles: (i) it provides a data structure to which parameters
are associated; (ii) it provides a qualitative description of
dependencies among the various components of the system. The graph structure also reveals conditional independencies between sets of components (Nodelman et al.,
2002).
Notational conventions: Full trajectories and observed
pointwise values of components are denoted by lower case
(t)
letters indexed by the relevant time intervals, e.g., xi ,
[s,t]
(t)
[s,t]
xi . We will use Pr(xi ) and Pr(xi ) as shorthands
(t)
(t)
[s,t]
[s,t]
for Pr(Xi = xi ) and Pr(Xi = xi ).
It should be emphasized that even though CTBNs provide a succinct representation of multi-component processes, any inference query still requires the exponentiation of the full dM × dM dimensional rate matrix Q. For
example, given the state of the system at times 0 and T , the
Markov bridge formula is
Pr(X (t) = a|x(0) , x(T ) ) =
[exp(tQ)]x(0) ,a [exp((T − t)Q)]a,x(T )
.
[exp(T Q)]x(0) ,x(T )
It is the premise of this work that such expressions cannot
be computed directly, thus requiring approximation algorithms.

3
3.1

Sampling in a Two Component Process
Introduction

We will start by addressing the task of sampling from
a two components process. The generalization to multicomponent processes will follow in the next section.
Consider a two-component CTBN, X = (X, Y ),
whose dynamics is defined by conditional rates QX|Y and
QY |X (that is, X is a parent of Y and Y is a parent of X).
Suppose that we are given partial evidence about the state
of the system. This evidence might contain point observations, as well as continuous observations in some intervals,
of the states of one or two components. Our goal is to sample a trajectory of (X, Y ) from the joint posterior distribution.
The approach we take here is to use a Gibbs sampler
(Gilks et al., 1996) over trajectories. In such a sampler, we
initialize X and Y with trajectories that are consistent with
the evidence. Then, we randomly either sample a trajectory
of X given the entire trajectory of Y and the evidence on
X, or sample a trajectory of Y given the entire trajectory of
X and the evidence on Y . This procedure defines a random
walk in the space of (X, Y ) trajectories. The basic theory
of Gibbs sampling suggests that this random walk will converge to the distribution of X, Y given the evidence.
To implement such a sampler, we need to be able to
sample the trajectory of one component given the entire
trajectory of the other component and the evidence. Suppose, we have a fully observed trajectory on Y . In this case,
observations on X at the extremities of some time interval
statistically separate this interval from the rest of trajectory.
Thus, we can restrict our analysis to the following situation:
the process is restricted to a time interval [0, T ] and we are
given observations X (0) = x(0) and X (T ) = x(T ) , along
with the entire trajectory of Y in [0, T ]. The latter consists
of a sequence of states (y0 , . . . , yK ) and transition times
(τ0 = 0, τ1 , . . . , τK , τK+1 = T ). An example of such scenario is shown in Figure 1(a). The entire problem is now
reduced to the following question: how can we sample a
trajectory of X in the interval (0, T ) from its posterior distribution?
To approach this problem we exploit the fact that the
sub-process X given that Y [0,T ] = y [0,T ] is Markovian (although non-homogeneous in time):
Proposition 3.1: The following Markov property holds for
all t > s,
Pr(X (t) | x[0,s] , x(T ) , y [0,T ] ) = Pr(X (t) | x(s) , x(T ) , y [s,T ] ).
3.2

Time Granularized Process

Analysis of such process requires reasoning about a continuum of random variables. A natural way of doing so is to
perform the analysis in discrete time with a finite time granularity h, and examine the behavior of the system when we
take h ↓ 0.

To do so, we introduce some definitions. Suppose Pr
is the probability function associated with a continuoustime Markov process with rate matrix Q. We define the
h-coarsening of Pr to be Prh , a distribution over the random variables X (0) , X (h) , X (2h) , . . . which is defined by
the dynamics
Prh (X (t+h) = b | X (t) = a) = δa,b + h · qa,b ,
which is the Taylor expansion of [exp(tQ)]a,b , truncated
at the linear term. When h < mina (−1/qa,a ), Prh is a
well-defined distribution.
We would like to show that the measure Prh (A) of an
event A converges to Pr(A) when h ↓ 0. To do so, however, we need to define the h-coarsening of an event. Given
a time point t, define btch and dteh to be the rounding down
and up of t to the nearest multiple of h. For point events
we define [[X (t) = a]]h to be the event X (btch ) = a, and
+
[[X (t ) = a]]h to the event X (dteh ) = a. For an interval event, we define [[X (s,t] = a(s,t] ]]h to be the event
X (dseh ) = adseh , X (dseh +h) = adseh +h , . . . , X (btch ) =
abtch . Similarly, we can define the coarsening of events
over only one component and composite events.
Note that the probability of any given trajectory tends
to zero as h → 0. The difficulty in working directly in the
continuous-time formulation is that we condition on events
that have zero probability. The introduction of a granularized process allows us to manipulate well-defined conditional probabilities, which remain finite as h → 0.
Theorem 3.2: Let A and B be point, interval, or a finite
combination of such events. Then
lim Prh ([[A]]h | [[B]]h ) = Pr(A | B)
h↓0

From now on, we will drop the [[A]]h notation, and assume
it implicitly in the scope of Prh ().
A simple minded approach to solve our problem is to
work with a given finite h and use discrete sampling to
sample trajectories in the coarsened model (thus, working
with a dynamical Bayesian network). If h is sufficiently
small this might be a reasonable approximation to the desired distribution. However, this approach suffers from
sub-optimality due to this fixed time granularity — a too
coarse granularity leads to inaccuracies, while a too fine
granularity leads to computational overhead. Moreover,
when different components evolve at different rates, this
trade-off is governed by the fastest component.
3.3

Sampling a Continuous-Time Trajectory

To avoid the trade-offs of fixed time granularity we exploit
the fact that while a single trajectory is defined over infinite
time points, it involves only a finite number of transitions
in a finite interval. Therefore, instead of sampling states
at different time points, we only sample a finite sequence

of transitions. The Markovian property of the conditional
process X enables doing so using a sequential procedure.
Our procedure starts by sampling the first transition
time. It then samples the new state the transition leads to.
As this new sample point statistically separates the remaining interval from the past, we are back with the initial problem yet with a shorter interval. We repeat these steps until
the entire trajectory is sampled; it terminates once the next
transition time is past the end of the interval.
Our task is to sample the first transition time and the
next state, conditioned on X (0) = x(0) , X (T ) = x(T ) as
well as the entire trajectory of Y in [0, T ]. To sample this
transition time, we first define the conditional cumulative
distribution function F (t) that X stays in the initial state
for a time less than t:


(3)
F (t) = 1 − Pr X (0,t] = x(0) |x(0) , x(T ) , y [0,T ]
If we can evaluate this function, then we can sample the
first transition time τ by inverse transform sampling — we
draw ξ from a uniform distribution in the interval [0, 1], and
set τ = F −1 (ξ); see Figure 1a,b.
The Markov property of the conditional process allows
us to decompose the probability that X remains in its initial
state until time t. Denoting the probability of Y ’s trajectory
and of X remaining in its initial state until time t by
ppast (t) = Pr(X (0,t] = x(0) , y (0,t] |x(0) , y (0) ),
and the probability of future observations given the state of
(Xt , Yt ) by
pfuture
(t) = Pr(x(T ) , y (t,T ] |X (t) = x, y (t) ).
x
We can then write the probability that X is in state x(0)
until t as

 ppast (t) · pfuture (t)
x(0)
.
Pr X (0,t] = x(0) |x(0) , x(T ) , y [0,T ] =
future
px(0) (0)
(4)
Lamentably, while the reasoning we just described is
seemingly correct, all the terms in Eq. (4) are equal to
0, since they account for the probability of Y ’s trajectory.
However, as we shall see, if we evaluate this equation carefully we will be able to define it with terms that decompose
the problem in a similar manner.
To efficiently compute these terms we exploit the fact
that although the process is not homogeneous, the dynamics of the joint process within an interval [τk , τk+1 ) , in
which Y has a fixed value yk , is characterized by a single unnormalized rate matrix whose entries depend on yk .
This allows us to adopt a forward-backward propagation
scheme. We now develop the details of these propagations.
3.4

Computing ppast (t)

We begin with expressing ppast (t) as a product of local
terms. Recall that ppast (t) is the probability that X is constant until time t. We denote by ppast
h (t) the h-coarsened
version of ppast (t).

(a) Sampling first transition

(b) Sampling second transition

(c) Initial propagators

(d) Propagators in second step

Figure 1: Illustration of sampling of a single component with three states. (a) Top panel: sampling scenario, with a
complete trajectory for Y , that has four transitions at τ1 , . . . , τ4 , and point evidence on X at times 0 and T . Bottom panel:
the cumulative distribution F (t), that X changes states before time t given this evidence. We sample the next transition
time by drawing ξ from a uniform distribution and setting τ = F −1 (ξ). Note that as x(0) 6= x(T ) , F (T ) = 1. The bar
graph represents the conditional distribution of the next state, given a transition at time τ . (b) Same sampling procedure
for the second transition. Here F (T ) < 1 since it is not necessary for X to change its state. (c and d) The two components
used in computing 1 − F (t): p̃past (t) the probability that X stays with a constant value until time t and Y has the observed
trajectory until this time; and p̃future
(x) the probability that X transition’s from state x at t to its observed state at time T
t
and Y follows its trajectory from t to T .
To characterize the dynamics within intervals
(τk , τk+1 ) we define constant propagator functions
ppast
h (t)

=

"k−1
Y

#
l
φyh,x
(0) (∆l )

·

Y |X
qyl ,yl+1 |x(0)

k
· h φyh,x
(0) (t−τk )

l=0

φyh,x (∆t) =

Prh (X (t,t+∆t] = x, Y (t,t+∆t] = y|X (t) = x, Y (t) = y) where ∆l = τl+1 − τl .
To compute the constant propagator functions, we realize that in each step within the interval (s, t] the state does
not change. Thus,
These functions determine the probability that X = x and
Y = y throughout an interval of length ∆t if they start with
these values.
At time τk+1 the Y component changes it value from
yk to yk+1 . The transition probability at this point is h ·
Y |X
qyk ,yk+1 |x(0) . Thus, from the Markov property of the joint
process it follows that for t ∈ (τk , τk+1 )

Y |X

X|Y

φyh,x (∆t) = [1 + h · (qx,x|y + qy,y|x )]

b∆tch
h

We define
X|Y

Y |X

(∆t)(qx,x|y +qy,y|x )

φyx (∆t) = lim φyh,x (∆t) = e
h↓0

We conclude that if
"k−1
#
Y y
Y |X
past
l
p̃ (t) =
φx(0) (∆l ) · qyl ,yl+1 |x(0) φyxk(0) (t − τk ),
l=0

then for t ∈ (τk , τk+1 )
lim
h↓0

3.5

ppast
h (t)
= p̃past (t)
hk

"

Computing pfuture
(t)
x

future

p̃

We now turn to computing pfuture
(t). Unlike the previous
x
case, here we need to compute this term for every possible
value of x. We do so by backward dynamic programing
(reminiscent of backward messages in HMMs).
We denote by pfuture
(t) a vector with entries pfuture
h
h,x (t).
future
Note that, ph (T ) = ex(T ) where ex is the unit vector
with 1 in position x. Next, we define a propagator matrix
Syh (∆t) with entries
syh,a,b (∆t) =
Prh (X (t+∆t) = b, Y (t,t+∆t] = y|X (t) = a, Y (t) = y)
This matrix provides the dynamics of X in an interval
where Y is constant. We can use it to compute the probability of transitions between states of X in the intervals
(τk , τk+1 ], for every τk < s < t < τk+1
pfuture
(s)
h

=

Syhk (t

This terms is similar to transition matrix of a Markov process. Note, however that R is not a stochastic rate matrix,
as the rows do not sum up to 0. In fact, the sum of the
rows in negative, which implies that the entries in Syh (∆t)
tend to get smaller with ∆t. This matches the intuition that
this term should capture the probability of the evidence that
Y = y for the whole interval.
To summarize, if we define for t ∈ (τk , τk+1 )

−

lim
h↓0

3.6

y

K
Y

hTyl ,yl+1 Syh (∆l ) ex(T )

l=k+1

It remains to determine the form of the propagator matrix. At time granularity h, we can write the probability of
transitions between states of X while Y = y as a product
of transition matrices. Thus,
Syh (∆t) = (I + h · RX|y )

b∆tch
h

where RX|y is the matrix with entries
 X|Y

a 6= b
 qa,b|y
X|y
ra,b =

 q X|Y + q Y |X a = b
a,a|y
y,y|a
We now can define
Sy (∆t) = lim Syh (∆t) = e(∆t)R
h↓0

X|y

T

S (∆l ) ex(T ) ,

pfuture
(t)
h
= p̃future (t)
hK−k

Putting it All Together

Based on the above arguments.

 ppast (t)pfuture
(t)
h
h,x(0)
Prh X (0,t] = x(0) |x(0) , x(T ) , y [0,T ] =
future
ph,x(0) (0)
Now, if t ∈ (τk , τk+1 ), then


Pr X (0,t] = x(0) |x(0) , x(T ) , y [0,T ]
=

#

(τk+1 −t)

#
yl ,yl+1 y

then

s)pfuture
(t)
h

pfuture
(t) =
h
Shk−1 (τk+1 − t)

(t) = S

K
Y
l=k+1

At transition points τk we need to take into account the
probability of a change. To account for such transitions, we
0
Y |X
define a diagonal matrix Ty,y whose (a, a) entry is qy,y0 |a .
Using this notation and the Markov property of the joint
process the conditional probability of future observations
for τk ≤ t ≤ τk+1 is
"

yk−1

lim
h↓0

future
ppast
h (t)ph,x(0) (t)

pfuture
(0)
h,x(0)
−(K−k) future
[h−k ppast
ph,x(0) (t)]
h (t)][h

=

lim

=

p̃past (t)p̃future
(t)
x(0)
p̃future
(0)
x(0)

h↓0

h−K pfuture
(0)
h,x(0)

Thus, in both numerator and denominator we must account
for the observation of K transitions of Y , which have probability of o(hK ). Since these term cancels out, we remain
with the conditional probability over the event of interest.
3.7

Forward Sampling

To sample an entire trajectory we first compute p̃future (t)
only at transition times from the final transition to the start.
We sample the first transition time by drawing a random value ξ from a uniform distribution in [0, 1]. Now
we find τ such that F (τ ) = ξ in two steps: First, we
sequentially search for the interval [τk , τk+1 ] such that
F (τk ) ≤ F (τ ) ≤ F (τk+1 ) by propagating p̃past (t) forward through transition points. Second, we search the exact time point within [τk , τk+1 ] using binary search with L
steps to obtain accuracy of 2−L ∆k . This step requires computation of Syk (2−L ∆k ) and its exponents Syk (2−l ∆k ),
l = 1, . . . , L − 1.
Once we sample the transition time t, we need to compute the probability of the new state of X. Using similar

arguments as the ones we discussed above, we find that


+
+
Pr X (t ) = x|X [0,t) = x(0) , X (t ) 6= x(0) , y [0,T ] =
X|Y

qx(0) ,x · p̃future
(t)
x
P

x0 6=x(0)

X|Y

qx(0) ,x0 · p̃future
x0 (t)

.

Thus, we can sample the next state by using the precomputed value of p̃future
(t) at t.
x
Once we sample a transition (time and state), we can
sample the next transition in the interval [τ, T ]. The procedure proceeds while exploiting propagators which have
already been computed. It stops when F (T ) < ξ, i.e., the
next sampled transition time is greater than T . Figure 1
illustrates the conditional distributions of the first two transitions.

4

Sampling in a Multi-Component Process

The generalization from a two-component process to a general one is relatively straightforward. At each step, we need
to sample a single component Xi conditioned on trajectories in Y = (X1 , . . . , Xi−1 , Xi+1 , . . . , XM ). To save
computations we exploit the fact that given complete trajectories over the Markov blanket of Xi , which is the component set of Xi ’s parents, children and its children’s parents, the dynamics in Xi is independent of the dynamics of
all other components (Nodelman et al., 2002).
Indeed, the structured representation of a CTBN allows
computations using only terms involving the Markov blanket. To see that, we first notice that within an interval whose
state is Y t = y the propagator matrix involves terms which
Xi |Y
Xi |Par(i)
and
depend only on the parents of Xi qa,b|y
= qa,b|u
i
terms which depend on the other members of the Markov
blanket,
X
X |Par(j)
Y |X
qxjj,xj |uj + cy
qy,y|xii =
j∈Child(i)

where cy does not depend on the state of Xi . Therefore,
we define the reduced rate matrix RXi |v :
 X |Par(i)
i

a 6= b
 qa,b|ui
Xi |MB(i)
ra,b|v
=

Xj |Par(j)
 q Xi |Par(i) + P
a=b
j∈Child(i) qxj ,xj |uj
a,a|ui
where, v is the projection of y to the Markov blanket. Consequently the local propagator matrix becomes
Sv (t) = exp(t · RXi |v )

(5)

Importantly, this matrix differs from Sy (t) by a scalar factor of exp(t · cy ). The same factor arise when replacing the
term in the exponent of the constant propagator. Therefore,
these terms cancel out upon normalization.
This development also shows that when sampling Xi
we only care about transition points of one of the trajectories in MB(i). Thus, the intervals computed in the

Figure 2: Relative error versus burn-in and number of samples.
initial backward propagation are defined by these transitions. Therefore, the complexity of the backward procedure
scales with the rate of Xi and its Markov blanket.

5

Experimental Evaluation

We evaluate convergence properties of our procedure on a
chain network presented in Fan and Shelton (2008), as well
as on related networks of various sizes and parametrizations. The basic network contains 5 components, X0 , →
X1 → . . . X4 , with 5 states each. The transition rates
of X0 suggest a tendency to cycle in 2 possible loops:
s0 → s1 → s2 → s0 and s0 → s3 → s4 → s0 ; whereas
for i > 0, Xi attempts to follow the state of Xi−1 — the
Xi |Xi−1
transition qa,b|c
has higher intensity when c = b. The
intensities of X0 in the original network are symmetric relative to the two loops. We slightly perturbed parameters to
break symmetry since the symmetry between the two loops
tends to yield untypically fast convergence.
To obtain a reliable convergence assessment, we should
generate samples from multiple independent chains which
are initialized from an over-dispersed distribution. Aiming to construct such samples, our initialization procedure
draws for each component a rate matrix by choosing an assignment to its parents from a uniform distribution and taking the corresponding conditional rate matrix. Using these
matrices it samples a trajectory that is consistent with evidence independently for every component using the backward propagation-forward sampling strategy we described
above.
A crucial issue in MCMC sampling is the time it takes
the chain to mix — that is, sample from a distribution that
is close to the target distribution rather than the initial distribution. It is not easy to show empirically that a chain has
mixed. We examine this issue from a pragmatic perspective
by asking what is the quality of the estimates based on sam-

Figure 3: Error versus burn-in for different evidence sets.
For each set we specify the average log-likelihood of the
samples after convergence.

ples taken at different number of “burn-in” iterations after
the initialization, where a single iteration involves sampling
each of the components once. We examine the estimates of
expected sufficient statistics that are required for learning
CTBN’s — residence time of components in states and the
number of transitions given the state of the component’s
parent (Nodelman et al., 2003). We measure estimation
P |θ̂ −θ |
quality by the average relative error j jθj j where θj
is exact value of the j’th sufficient statistics calculated using numerical integration and θ̂j is the approximation.
To make the task harder, we chose an extreme case
by setting evidence X (0) = ~s0 (the vector of s0 ), and
X (3) = (s0 , s1 , s3 , s0 , s1 ). We then sampled the process
using multiple random starting points, computed estimated
expected statistics, and compared them the exact expected
statistics. Figure 2 shows the behavior of the average relative error taken over all θ > 0.05 versus the sample size for
different number of burn-in iterations. Note that when
√using longer burn-in, the error decreases at a rate of O( n),
where n is the number of samples, which is what we would
expect from theory, if the samples where totally independent. This implies that at this long burn-in the error due to
the sampling process is smaller than the error contributed
by the number of samples.
To study further the effect of evidence’s likelihood,
we measured error versus burn-in using 10,000 samples in
our original evidence set, and four additional ones. The
first additional evidence, denoted by e2 is generated by
setting X (0) = ~s0 , forward sampling a random trajectory
and taking the complete trajectory of X4 as evidence.
Additional sets are:
e3 = {X (0) = ~s0 , X (3) = ~s0 };
(0)
e4 = {X = ~s0 } and an extremely unlikely case
(0,3)
e5 = {X (0) = ~s0 , X0
= s0 , X (3) = (s0 , s1 , s3 , s0 , s1 )}.
Figure 3 illustrates that burn-in period may vary by an

Figure 4: Effect of conditional transition probability sharpness on mixing time.

order of magnitude, however it is not correlated with the
log-likelihood. Note that in this specific experiment slower
convergence occurs when continuous evidence is absent.
The reason for this may be the existence of multiple possible paths that cycle through state zero. That is, the posterior
distribution is , in a sense, multi-modal.
To further explore the effect of the posterior’s landscape, we tested networks with similar total rate of transitions, but with varying level of coupling between components. Stronger coupling of components leads to a sharper
joint distribution. To achieve variations in the coupling
we consider variants
of the chain CTBN where we set
(q
)α
π̂a,b|y = P a,b|y
α and q̂a,b|y = qa,a|y · π̂a,b|y where
c6=a (qa,c|y )
α is a non-negative sharpness parameter As α → 0 the network becomes smoother, which reduces coupling between
components. However, the stationary distribution is not
tending to a uniform one because we do not alter the diagonal elements. Figure 4 shows convergence behavior for
different values of α where estimated statistics are averaged
over 1,000 samplers. As we might expect, convergence is
faster as the network becomes smoother.
Next we evaluated the scalability of the algorithm
by generating networks containing additional components
with an architecture similar to the basic chain network.
As exact inference is infeasible in such networks we measured relative error versus estimations taken from long
runs. Specifically, for each N , we generated 1000 samples by running 100 independent chains and taking samples after 10,000 rounds as well as additional 9 samples
from each chain every 1,000 rounds. Using these samples
we estimated the target sufficient statistics. To avoid averaging different numbers of components, we compared the
relative error in the estimate of 5 components for networks
of different sizes. Figure 5 shows the results of this experiment. As we can see, convergence rates decay moderately

Figure 5: Convergence of relative error in statistics of first
five components in networks of various sizes. Errors are
computed with respect to statistics that are generated with
N = 10, 000 rounds.

Figure 7: The effect of different time scales on the sampling. In this network Xi ’s rate is twice as fast than Xi+1 ’s
rate. (top) The number transitions sampled for each of the
first four components as a function of iteration number.
(bottom) The number of intervals of Markov neighbors of
each component as a function of iteration number.
Figure 6: Relative error versus run-time in seconds for various network sizes.

with the size of the network.
While for experimental purposes we generate many
samples independently. A practical strategy is to run a
small number of chains in parallel and then collect take a
large number of samples from each. We tested this strategy
by generating 10 independent chain for various networks
and estimating statistics from all samples except the first
20%. Using these, we measured how the behavior of error
versus CPU run-time scales with network size. Average results of 9 independent tests are shown in Figure 6. Roughly,
the run-time required for a certain level of accuracy scales
linearly with network size.
Our sampling procedure is such that the cost of sampling a component depends on the time scales of its Markov
neighbors and its own rate matrix. To demonstrate that, we

created a chain network where each component has rates
that are of half the magnitude of its parent. This means that
the first component tends to switch state twice as fast as the
second, the second is twice as fast as the third, and so on.
When we examine the number of transitions in the sampled
trajectories Figure 7, we see that indeed they are consistent
with these rates, and quickly converge to the expected number, since in this example the evidence is relatively weak.
When we examine the number of intervals in the Markov
blanket of each components, again we see that neighbors
of fast components have more intervals. In this graph X1
is an anomaly since it does not have a parent.

6

Discussion

In this paper we presented a new approach for approximate inference in Continuous-Time Bayesian Networks.
By building on the strategy of Gibbs sampling. The core

of our method is a new procedure for exact sampling of a
trajectory of a single component, given evidence on its end
points and the full trajectories of its Markov blanket components. This sampling procedure adapts in a natural way
to the time scale of the component, and is exact, up to a
predefined resolution, without sacrificing efficiency.
This is the first MCMC sampling procedure for this
type of models. As such it provides an approach that
can sample from the exact posterior, even for unlikely evidence. As the current portfolio of inference procedures
for continuous-time processes is very small, our procedure
provides another important tool for addressing these models. In particular, since the approach is asymptotically unbiased in the number of iterations it can be used to judge
the systematic bias introduced by other, potentially faster,
approximate inference methodologies, such as the one of
Saria et al. (2007).
It is clear that sampling complete trajectories is not useful in situations where we expect a very large number of
transitions in the relevant time periods. However, in many
applications of interest, and in particular our long term goal
of modeling sequence evolution (El-Hay et al., 2006), this
is not the case. When one or few components transitions
much faster than neighboring components, then we are essentially interested in its average behavior (Friedman and
Kupferman, 2006). In such situations, it would be useful to
develop a Rao-Blackwellized sampler that integrates over
the fast components.
As with many MCMC procedures, one of the main concerns is the mixing time of the sampler. An important direction for future research is the examination of methods
for accelerating the mixing - such as Metropolis-coupled
MCMC or simulated tempering (Gilks et al., 1996) - as well
as a better theoretic understanding of the convergence properties.
Acknowledgments
We thank Ido Cohn and the anonymous reviewers for helpful remarks on previous versions of the manuscript. This
research was supported in part by grants from the Israel
Science Foundation and the US-Israel Binational Science
Foundation. Tal El-Hay is supported by the Eshkol fellowship from the Israeli Ministry of Science.




tiona! approximation. This method approximates the pos­
terior by a distribution composed of independent substruc­

Global variational approximation methods in graphical

tures of random variables. This idea can be generalized for

models allow efficient approximate inference of com­

various factored forms for

plex posterior distributions by using a simpler model.

and Markov networks

The choice of the approximating model determines a

Q,

such as Bayesian networks

[1, 1 1 ]. Jaakkola and Jordan [5] ex­

tradeoff between the complexity of the approximation

plore another direction for improving the mean field ap­

procedure and the quality of the approximation. In this

proximation. They propose to use a mixture of

paper, we consider variational approximations based

approximations in order to approximate multi-modal pos­

on two classes of models that are richer than standard

mean field

teriors. Both structured variational approximation and the

Bayesian networks, Markov networks or mixture mod­

mixture approximation methods allow for a more refined

els. As such, these classes allow to find better tradeoffs
in the spectrum of approximations. The first class of

trade-off between accuracy and computational complexity.

that are partially directed. The second class of mod­

by adding structure, while in the mixture approximation we

models are elwin graphs, which capture distributions

In the structured approximations more accuracy is gained

els are directed graphs (Bayesian networks) with addi­

can increase the number of mixture components.

tional latent variables. Both classes allow representa­

In this paper, we generalize and improve on these two

tion of multi-variable dependencies that cannot be eas­

methods in order to achieve greater accuracy given the

ily represented within a Bayesian network.

available computational resources. The resulting approx­
imation results enhance the range of approximating distri­

1

Introduction

butions and increase the ability to trade-off accuracy for

A central task in using probabilistic graphical models is

ference.

in­

Exact inference algorithms exploit the structure

of the model to decompose the task. In general, although
the problem is NP-hard, some structures (e.g., these with
bounded tree width) allow efficient inference. W hen the
model is intractable for exact inference, we can still hope
to perform approximate

inference (although that problem

is also known to be generally intractable).

One class of

approximations that received recent attention is the class

of variational approximation algorithms [6]. These algo­

rithms attempt to approximate the posterior
where

o is

P(T I o),
T are the
0) that has

an observation of some variable and

remaining variables, by a distribution

Q(T

:

tractable structure. Using this approximating distribution,
we can define a lower bound on the likelihood
parameters

P(o).

The

e that define Q are adapted by trying to maxi­

mize this lower bound.
The simplest variational approximation is the

mean-field

approximation [8, 9] that approximates the posterior distri­
bution with a network in which all the random variables are
independent. As such, it is unsuitable when there are strong
dependencies in the posterior. Saul and Jordan [ 1 0] sug­
gest to circumvent this problem by using

structured varia-

complexity.
We start by considering extensions of structured approx­
imations. Current structured approximation use Bayesian
networks

or

Markov networks as approximating distribu­

tions. These two classes of models have different expres­
sive power. We provide uniform treatment of both classes
by examining

chain graphs

-

a class of models that is

more expressive than Bayesian and Markov networks, and
includes each one of them as a special case.
We then consider how to add
approximating model.

extra hidden variables to the

This method generalizes both the

structured approximation and the mixture model approxi­
mation. It enables us to control the complexity of the ap­
proximating model both through the structure and through
the number

of values of the hidden variables. The extra hid­

den variables also enable us to maintain the dependency be­

tween different variables but control the level of complex­
ity, thus keeping the dependencies in a compressed man­
ner. As straightforward insertion of extra hidden variables
to the variational approximation framework results in an
intractable optimization problem, we need to combine ad­
ditional approximation steps. We present a natural gener­
alization of methods suggested by Jaakkola and Jordan [5]

UA12001

137

EL-HAY & FRIEDMAN

for mixtures of mean field models.
2

Variational Approximation with Directed
Networks

Structured approximations approximate a probability dis­
tribution using probability distribution with non-trivial de­
pendency structure. We re-derive standard structured ap­
proximation schemes with Bayesian networks (such as the
ones in [4, 10, 11]) using tools that will facilitate later de­
velopments.
Suppose we are given a distributionP over the set of ran­
dom variables X= {X1, ... ,Xn}· Let 0 C X be the
subset of observed variables. We denote by T = X \ 0
the set of hidden variables. Our task is to approximate the
distribution P(T I o) by another distribution Q(T : e),
where e is the set of parameters for the approximating dis­
tribution Q.
In this paper, we focus on approximating distributions
represented by discrete graphical models such as Bayesian
networks and Markov networks. That is, we assume that
{X1 , ... , Xn} are discrete random variables and thatP has
a factorized form
P(x)

1

=-IT
1/Ji{d;)
Zp .

logP(o) = F[Q] + D(QIIP) � F[Q ]
The inequality is true because the KL divergence is non­
negative. Hence, F[Q] is a lower bound on the log­
likelihood. The difference between F[Q] and the true
log-likelihood is the KL-divergence. Minimizing the KL­
divergence is equivalent to finding the tightest lower bound.
A simple approximation of this form uses a distribution
Q that is a Bayesian network
j

where Ui denotes the parents of Xi in the approximating
network, and ()x3 1u; are the parameters of the distribution.
The computational complexity of calculating the lower
bound depends on the complexity of inference in Q and
on the domain size of the factors of P. To see that let us
rewrite F(Q] in a factorized form.
Lemma2.1: /fQ(t) = Ti j ()x;lu;• then
F[Q I c]

=

(1 )

t

Where D1, Dk are subsets of X. This representation can
be a Bayesian network (in which case, each tPi is a con­
ditional distribution) or a Markov network (in which case,
each rp; is a potential over some subset of X).
The approximating distribution will be represented as an­
other graphical model Q(T : e). Once we specify the
form of this model, we wish to find the set of parameters
e that minimizes the distance between Q(T : e) and the
posterior distribution P(T I o). A common measure of
distance is the KL divergence [2] between Q(T : 8) and
the posterior distribution P(T I o). This is defined as

j

2: Eqclc) [logrp;(D;, o)]-log Zp

- L EQ(.Ic) [logBx;iuJ)+logQ(c)
j

..•

[

D(Q(T)IIP(T I o))= Eq(T) log

Q(T)
P(T I o)

]

(2)

Finding the parameters for Q will allow us to compute a
lower bound for log P( o) . To see this, we define a func­
tional F of the general form:

[

F[Q I cl = EQ(·Icl log

P(T, c, o)
Q(T I c)

]

where cis an evidence vector assigned to a subset C <:;:; T,
and QC I c) is a shorthand for Q(T I c). (The reasons
for using additional evidence in the definition will be clear
shortly.) In the special case where C = 0, F becomes
F[Q]

=

EQ

We can easily verify that

]
[1og P(T,o)
Q(T)

where cPi(Di, o) represents a random variable whose value
is tPi ( d;) if d; is consistent with o; otherwise it is 0.

Our goal is to find a set of parameters maximizing F
while conforming to the local normalization constraints.
The optimal parameters for Q are found by writing the La­
grangian for this problem and differentiating it with respect
to them. The Lagrangian is

To differentiate the Lagrangian we shall use the following
technical result.
Lernrna2.2: LetQ(t) = TijBx;lu;• then

8Eq (!(C)]
80x;lu;

=

Q(uj)·Eq(·lx;,u;) [/(C)]+Eq

[ 8f(C) ]
80 x;lu;

Corollary 2.3:

Note that logQ(xj, uj) = log Q(uj) +logBx;lur Equat­
ing the derivative of the Lagrangian to zero and dividing
both sides by Q(ui) and then rearranging, we get
(4)

138

EL-HAY & FRIEDMAN

UAI2001

where Zu; is a normalization constant, and
EsN(Xj , uJ)

=

F[Q I Xj,uJ]-logQ(ui)

EQ(·I•;,u;)

�

[�

)og¢; (D,, o) -

(5)

ft l g0
o

i

, 1 , u1,

]

-logZp

(b)

(c)

Figure 1: (a) A Bayesian network with an observed variable
(01). (b) A representation of the posterior distribution as a

To better understand this characterization, we examine
the term

(a )

F [Q I xi, Uj ].

chain graph. (c) an approximating chain graph network.

It is easy to verify that

D(Q(T I Xj, uj)IIP(T I Xj, uj, o) )
-F[Q I Xj, uj] +logP(xj, Uj, o)

We can then redefineEBN to be

=

Thus,

F[Q I Xj, Uj]

EBN(Xj,Uj)

L

=

iEFj

is a lower bound on logP(xj , u1,o).

This suggests that Eq. 4 can be thought of as approximat­

L

EQ(-!x;,u;) [log(Pi(Di,o)]­
EQ(lxi,u;) [IogQ(Xj' I Uj')]

P(xj I uj ,o)byBxilui· If we replaceEsN(x;,uj)
by logP(xj,Uj,o) in this equation, we would get that
Bx,Ju ; = P (xj I Uj, o).1

Depending on the decomposition of Q, this formula might

ative procedure that updates the parameters of one family

field approximation,

ing

In order to find optimal parameters, we can use an iter­

j'EFJ

involve much fewer terms then Eq. 5. For example, in mean

on each iteration. An asynchronous update of the parame­

clude X1, and

the lower bound F[Q] and converges to a local maximum.

work.

ters according to Eq. 4 guarantees a monotonic increase in

This is a consequence of the fact that, for every i and every

u1, F is a concave function of
{Bx;Ju; I Xj E dom(Xj)}. There­

assignment to the parents
the set of parameters

fore, the stationary point is a global maximum with respect
to those parameters. The concavity ofF follows from the
fact that the second order partial derivatives are negative

82F
--....,.2
8Bxilu;

=

1
--e-Q(uJ)

x;Ju;

<

o

and the mixed partial derivatives are all zero.
The complexity of calculatingEBN as defined in Eq. 5 is
determined by the number of variables, the size of the fam­
ilies in

P

and by the complexity of calculating marginal

probabilities in

Q.

It is important to realize that not all the

terms in this equation need to be computed. To see this
we need to consider conditional independence properties

X is independent of Y given Z in Q,
denoted Q F Ind(X; Y I Z), if Q(X I y, z ) = Q(X I z )
for all values y and z of Y and Z. If Q is a Bayesian
in

Q.

We say that

network, we can determine such independencies using d­

separation [7] Now, suppose that Q
.

Q(c I Xj, ui)

i.e.,

EQ(c!x;,u;)

=

Q(c I uj).

F Ind(Xj; C I Uj),
Terms of the form

[/(c)] can be ignored in the update equations

since they change the new parameters by a constant factor
which will be absorbed in Zu;. Therefore we can reduce
the amount of computations by defining the sets of indices
of the factors that depend on Xj given

pP

Similar derivation can be made when

Q is a Markov net­

The main difference is that in Markov networks

there is a global constraint (defined by the partition func­
tion) rather then local ones for each conditional distribu­
tion. Due to space considerations we omit the details, and
refer the interested reader to

3

[11].

Chain Graph Approximations

As is well known, the classes of distributions that can
be represented by Markov networks and by Bayesian net­
works are not equivalent. Therefore, for some distributions
the best tractable approximations might be represented by
Bayesian networks while for other distributions the best ap­
proximation is a Markov network. We can gain more flex­
ibility in choosing an approximating distribution by using
a more general class of probability models that can cap­
ture the dependency models implied by Bayesian networks,
Markov networks and dependency models that can be cap­
tured by neither of them.
To consider a concrete example, suppose that

P

is a

Bayesian network. W hat is the form of the posterior P(T

o)?

I

For a concrete example, consider the network of Fig­

ure l(a). When, we observe the value of 01, we create de­
pendencies among the variables T1, T2, and T3. The poste­

rior distribution is neither a Bayesian network nor a Markov
network (because of the v-structure in the parents of

T5).

Instead, we can write this posterior in the form:

Uj as follows:

{i: Q � Ind(Xj;Di I Uj)}
F�
{j' =/; j: Q � Ind(Xj;Xj', uj' I Uj)}
----"
'------"
J

FJ includes only potentials that in­
Fl is empty.

=

Note that the term log Q(uj) in EsN (xj, Uj) can be ignored,

1

since it is absorbed by the normalizing constant. We include it

above to simplify the decomposition of EBN ( xh Uj).

where

'f/I(TI . T2,T3 )

=

P(�1 )P(ol

I T�,T2,T3) is a poten­

tial that is induced by the observation of

o1.

A natural class of models that has this general form are
chain graphs [3].

Such a model factorizes to a product

of conditional distributions and potentials. Formally, we

UAI2001

EL-HAY & FRIEDMAN

define a chain graph to have for each variable a (possibly
empty) set of parents, and in addition to have a set of po­

139

Lemma 3.1: IJQ is a chain graph overT, then

tentials on some subsets of variables.
When we represent

Q as a chain graph, we will have the

general form:

Ui are the directed parents of Xi. In
'lj!k are potential functions on subsets ofT, and
l::t fli Q(xi I Uj) flk 'lj!k(Ck) is a normalizing

where, as before,
addition,

ZQ

=

function that ensures that the distribution sums to 1. Fig­

ure l(b) shows the chain graph that represents this factor­
ization.
It is easy to check that if

P(T I o)

P

is a Bayesian network, then

can be represented as a chain graph (for each

variable Xi in

0, add a potential over the parents of Xj).

.F[Q] we get two terms.
I Xj, Uj)] as before, and the other is :F[Q].
since .F[Q] does not depend on the value of Xj,

Note that when we differentiate

The first, is ..F[Q
However,

it is a absorbed in the normalizing constant

Zu; .

Thus,

the general structure of the solution remains similar to the
simpler case of Bayesian networks:

In contrast, it is easy to build examples where the posterior
distribution cannot be represented by a Bayesian network

=

without introducing unnecessary dependencies. Thus, this
class of models is, in some sense, a natural representation

l

e E co

l

eE co (co )

_
_

Zu;
_
_

Zq

of conditional distributions in Bayesian networks.

(x ; u; )
,

This argument suggests that by considering chain graphs
we can represent approximate distributions that are more
tractable than the original distribution, yet are closer to
the posterior we want to approximate. For example, Fig­
ure l(c) shows a simple example for a possible approxi­
mate network for representing the posterior of the network
of Figure l(a). In this network there are two potentials with
two variables each, rather than one with three variables.
Given the structure of the approximating chain graph, we
wish to find the set of parameters that maximizes

F[Q],

the lower bound on the log-likelihood. As usual, we need
to define a Lagrangian that capture the constraints on the
model. These constraints contain the constraints that ap­
peared in the Bayesian network case, and, in addition, we

where

Ecc(xj, ui)
Eca(ck)

F[Q I Xj, Uj]-logQ(xj, Uj ) + logBx; lu;
:F[Q I ck]-logQ(ck) + log'lj!k(Ck)

To get an explicit form of these equations, we simply
write the chain-graph analogue of Lemma

2.1 which has

similar form but includes additional terms. As in the case
of Bayesian network, we can easily identify terms that can
depend on the value of

Xj, and focus the computation only

on these. This is a straightforward extension of the ideas in
Bayesian networks, and so we omit the details.

4

require that each potential sums up to one:

=

Adding Hidden Variables

Structured approximations were the first method proposed
for improving the mean field approximation. Jaakkola and
Jordan [5] proposed another way of improving the mean
To understand this constraint, note that the each potential

field approximation: to use mixture distributions, where

can be scaled without changing

each mixture component is represented by a factorized dis­

stant is absorbed

Q,

since the scaling con­

in ZQ. Thus, without constraining the

scale of each potential there is a continuum of solutions,
and the magnitude of values in the potentials can explode.
Putting these together, the Lagrangian has the form:

tribution.

The motivation for using mixture distribution

emerges from the fact that in many cases the posterior dis­
tribution is multi-modal, i.e. there are several distinct re­
gions in the domain of the distribution with relatively high
probability values. If the location of the different modes of
the distribution depends on the values of several variables
than the mean field approximation can not capture more
than one mode.

The main difference from the Bayesian network approxi­
mation is in the form of the analogue of Lemma

2.2. In the

case of chain graphs, we also have to differentiate
so we get slightly more complex derivatives.

ZQ, and

Recal1 that the mean field approximation uses a graphical
model in which all the variables in

T

are independent of

each other. Thus, we can think of it as a Bayesian network
without edges. The mixture distribution approximation can

140

EL-HAY & FRIEDMAN

(a)

UAI2001

(b)

(c)

Figure 2: (a) A simple dynamic Bayesian network that describe a temporal process. Time progresses to the right. Each
vertical "slice" describe variables that exist in the same instance. (b) and (c) are two approximating networks for the
distribution represented by the network (a) with extra hidden variables. (b) Edges within a time slice are maintained.
Correlations between time slice are modeled through the introduction of the hidden variable set
(c) Edges
between time slices are maintained. Correlations between the three chains are modeled through the hidden variables
and

{Vn};;'=l·

V2

V1,

V3•

be viewed as one that uses a Bayesian network over the
variables T and an extra variable
such that
is the
parent node of each Xj E T. As before, the parameters
of the mixture distribution could be found by maximizing
the lower bound of the log likelihood as presented in Sec­
tion 2. Unfortunately, using this technique in a straightfor­
ward manner would not help us since the extra hidden vari­
ables introduces correlations, which leave us with an opti­
mization problem whose complexity is at least as great as
this of the original inference problem. Jaakkola and Jordan
overcame this problem by introducing another variational
transformation resulting in another lower bound to the log
likelihood [5].
In this section, we generalize the ideas of Jaakkola and
Jordan, and show a method where we can perform struc­
tured approximation with distributions Q that are defined
over TUV, where V is a set of hidden variables that did not
appear in the original distribution. (For clarity, we focus on
the case of Bayesian networks, although similar extension
can be applied to chain graphs as well.)
Given the distribution P(X) and evidence owe shall ap­
proximate the posterior P(T I o) with another distribution
Q(T) = l:v Q(T, v ) . This distribution is defined over
the variable set T U V where V is a set of extra hidden
variables. Our task is to find the parameters of Q that will
maximize the lower bound F[Q].
Figure 2(b) and (c) are two examples of possible approx­
imations for the distribution that is represented by the net­
work in Figure 2(a). Recall the structured approximation
for this network modeled the approximating distribution by
a network with three independent chains. In the networks
presented here, the correlations are maintained through the
hidden variables. In Figure 2(a) we added an extra hid­
den variable for every time slice. The correlations between
time slices are maintained through those hidden variables.
The edges within a time slice are maintained in order to
preserve intra-time dependencies. In Figure 2(b) we main-

V,

V

tained the edges between the time slices and added extra
hidden variables for every chain. Correlations among the
chains are maintained by the connections between the hid­
den variables.
Another perspective of the potential of extra hidden vari­
ables was suggested by Jaakkola and Jordan [5]. We can
easily extend it to our case. This is done by reexamination
of the lower bound F[Q].

Lenuna 4.1: Let Q(T)

F[Q]

l:v Q(T, v ), then

EQ(V) [F[Q I V]] + l(T; V)

[ g Qb�J..�l]

EQ to
T and V.

where I(T; V)
tion between

=

=

=

is the mutual informa­

The first term is an average on lower bounds that are gained
without introducing extra hidden variables. The improve­
ment arises from the second term. Given the structure of
the approximating network without extra hidden variables,
the lower bound can be improved if there are several dif­
ferent configurations of the parameters of the sub-network
defined on T that achieve lower bounds that are near opti­
mal. Using an extra hidden variable set to combine these
configuration, will improve the lower bound by the amount
of the mutual information between T and V.
As described above, in the presence of hidden variable,
the optimization of the functional F[Q] is more complex.
The source of these complications is the fact that
Q(t)
does not decompose. Therefore we shall relax the lower
bound. We start by rewriting F[Q] as

log

F[Q]

=

EQ

[log�(�:�)]- H(V IT)

This first term does decompose. The remaining term is the

conditional entropy

UAI2001

EL-HAY & FRIEDMAN

Instead of decomposing this term, we can calculate a lower
bound for it by introducing extra variational parameters.
The new parameters are based on the convexity bound [6)
-

l og(x) � -Ax+ log( ).) + 1

(6)

We can use the convexity bound by adding an extra vari­
ational parameter R(t, v) for every assignment toT U V.
Applying Equation 6 for every term in the summation of
the conditional entropy, we get a lower bound for the con­
ditional entropy:

the expression £ H is similar to the one obtained for the sim­
pler structural approximation, except for the last two terms
that arise from the lower bound on the negative conditional
entropy. To evaluate the term Lv Eq(Tix;,u;) [R (T, v)]
we perform variable-elimination like dynamic program­
ming algorithm.
To complete the story, we need to consider the update
equations for the parameters of R. Simple differentiation
results in the equation

Px;,u;

-H(V I T)
>

Eq -R(t, v)

=

-l:R(t, v)Q(t)

[

Q����)

+

logR(t, v)

+

Eq[logR(t, v)]

+

+1

Obviously, if we add a distinct variational parameter for ev­
ery assignment t, v, the conditional entropy can be recov­
ered accurately. Unfortunately, this setting leaves us with
an intractable computation. In order to reduce the compu­
tational complexity of the lower bound, we assume that R
has a similar structure to that of Q
=

IJ Px;,u;
j

We define the lower bound on F as a new functional:

g[Q,R I c]

EQ ( · Ic}

=

[log �i�'�0�)]

-2: Eq(Tic) (R(T, v) ]

v
+EQ(-Ic) [logR(T, V)]

=

g[Q,RJ-l:l:Au;l:Ox;lu;
x;
j u;

Using Lemma 2.2, and then applying the constraints, we
get the update equations for 8x 11 u; :
ex; I
u; =

1

-.

zu;

e£H(x;,u;)

(7)

Where

£H(Xj, ui )

=

g[Q,R I Xj, Uj]- log(uj)

As usual, we can decompose this term to a sum of terms:

2: EQ ( · Ix; ,u ) [logQ(Xj'

I Uj')]

2: EQ(· I x ,u; ) [logR(Xj'

I uj' )]

;

j'#-j
+

j'

;

L Eq(Tix;,u;) [R(T , v)]
v

Px;,u;
Q XJ, UJ
Lt,vt=x;,u; R(t, v)Q(t) ( . )

(S)

does not appear in the right hand side (since it cancels out
in the fraction). Again, we can efficiently compute such
equations using dynamic programming.
The Lagrangian is a convex function of both Bx;lu; and
Px;,u; . Therefore, asynchronous iterations of Equation 8
and Equation 7 improve the lower bound and will eventu­
ally converge to a stationary point.

5

Examples

To evaluate our methods we performed a preliminary test
with synthetic data. We created dynamic Bayesian net­
works with the general architecture shown in Figure 2(a).
All the variables in these networks are binary. We con­
trolled two parameters: the number of time slices ex­
panded, and the number of variables in each slice. The pa­
rameters of networks were sampled from a Dirichlet prior
with hyperparameter
Thus, there was some bias toward
skewed distributions. Our aim was to compute the like­
lihood of the observation in which all observed variables
were set to be 0. We repeated these tests for sets of 20 net­
works sampled for each combination of the two parameters
(number of time slices and number of variables per slice).
We performed variational approximation to the posterior
distribution using three types of networks with hidden vari­
ables: The first two types are based on the "vertical" and
"horizontal" architectures shown in Figure 2(b) and (c). We
considered networks with 1, 2, and 3 values for the hidden
variable. (Note that when we consider a hidden variable
with one value, we essentially apply the Bayesian network
structured approximation.) The third type are networks that
represent mixture of mean field approximations. For this
type we considered networks with 1, 4 and 6 mixture com­
ponents (When there is one mixture component the approx­
imation is simply mean field). We run each procedure for
10 iterations of asynchronous updates. This seems to con­
verge on most runs. To avoid local maxima, we tried 10
different random starting points in each run and returned
the best scoring one.
The figure of merit for our approximations is the reported
upper-bound on the KL-divergence between the approxi­
mation and the true posterior. This is simply log P{o ) -

t.

+ l

We now can define the Lagrangian with the desired con­
straints:

JH

=

Where the term t, v f= x1,u1 denotes assignments to
{T, V} that are consistent with Xj, Uj. Note that Px;,u;

1]

t,v

R(t, v)

141

EL-HAY & FRIEDMAN

142

Mixture of mean fields

UAI2001

"Vertical" approximation

"Horizontal" approximation

7

Figure 3: Comparison of the two appro x imating structures of Figure 2 and mixture of mean fields. The figures on the left
column report results for the mixture of mean fields approximation, with 1, 4, and 6 mixture components. The figures
on the middle column report results for the network structure containing additional hidden variable for each time slice
(Figure 2(b)) with hidden variables with 1, 2, and 3 values. The figures on the right column report results for the network
structure containing additional hidden variable for each temporal chain (Figure 2(c)) with hidden variables with 1, 2, and
3 values. The figures on the top row report on approximation to networks with 3 va riab les per time slice and the figures
on the bottom row report on networks with 4 variables per time slice. The x-axis corresponds to the number of time slices
in the network. They-axis corresponds to the upper-bound on the KL-divergence log P(o) - 9Q [Q, R] normalized by the
number of time slices in the network. Lines describe to median performance among 20 inference problems, and error bars
describe 25-75 perc entile s.

9Q[Q, R]. (The examples are sufficient ly small, so that we
can compute log P( o). ) We need to examine this quantity
since

different random networks have different values of

P(o) and so we cannot compare lower bounds.

Figure 3 describes the results of these runs. As we can see
the differences grow with the number of time slices. This
is expected as the problem becomes harder with additional
slices. The general trend we see is that runs with more
hidden values perform better. These differences are mostly

pronounced in the larger networks. This is probably due to
the higher complexity of these networks.
The comparison to mixture of mean fi el ds approximation
shows that simple mean field ( 1 component) is much worse
than all the other methods. Second, we see that although
mixtures of mean field improve with larger number of com­
ponents, they are still worse th an the structured approxima­
tions on the network with 3 variables per slices. We believe
that these toy examples are not sufficiently large to high­
light the differences between the different methods. For
e x ample , differences start to emerge when we examine 6

and 7 time slices.
Our implementation of these variational methods is not
optimized and thus we do not believe that running times
are informative on these small examples. Nonetheless, we
note that running mixtures of mean fields with 6 compo ­
nents took roughly the same time as running the structured
approximations with hidden variables of cardinality 3.
One caveat of this experiment is that it is based on random
networks, for which the depenencies between variables is
often quite weak. As such it is hard to gauge how hard
is inference in this networks. We are currently starting to
apply these m ethod s to real-life problems, where we expect
to improvement over mean field type methods to be more
pronounced.
6

Discussion

this paper we presented two extensions of structured
variational methods-based on chain graphs and additional
hidden variables. Each extension exploits a representaIn

UAI2001

EL-HAY & FRIEDMAN

tiona! feature that allows to better match a tractable ap­
proximating network to the posterior.

In Michael I. Jordan, editor, Learning in Graphical

Models. Kluwer, 1997.

By perusing such

extensions we hope to find better tradeoffs between net­
work complexity on one hand and the approximation of the

[6] M. I. Jordan,

methods for graphical models. In M. I.Jordan, editor,

We demonstrated the effect of using hidden variables in

Learning in Graphical Models. Kluwer, 1998.

synthetic examples and showed that learning non-trivial

[7] J. P earl. Probabilistic Reasoning in Intelligent Sys­
tems. Morgan Kauffman, San Francisco, 1988.

We are currently starting larger scale experiments on hard
real-life problems.
We put emphasis on presenting uniform machinery in the

[8] C. Peterson and J.R. Anderson. A mean field theory
learning algorithm for neural networks. Complex Sys­

derivations of the three variants we considered. This uni­
form presentation allows for better insights into the work­
ings of such approximations and simplifies the process of

tems, 1:995-1019, 1987.

[9] L. Saul, T. Jaakkola, and M. Jordan. Mean field the­

ory for sigmoid belief networks. Journal of Artificial

deriving new variants for other representations.

Intelligence Research, 4:61-76, 1996.

One issue that we did not address here for lack of space is
efficient computations on the network Q. The usual analy­

z. Ghahramani, T. Jaakkola, and L. K.

Saul. An introduction to variational approximations

posterior distribution on the other.

network with hidden variables helps the approximation.

143

[10] L. K. Saul and M. I. Jordan.

Exploiting tractable

sis focuses on the maximal tree width of the network. How­

substructures in intractable networks. In Advances

ever, much computation (up to a quadratic factor) can be

in Neural Information Processing Systems, volume 8,

saved by conscious planning of order of asynchronous up­

pp.486-492, 1996.

dates and the propagation of messages in Q's join tree.
The grand challenge for applications of such variational
methods is to build automatic procedures that can deter­
mine what structure matches best a given network with
a given query.

This is a non-trivial problem.

We hope

that some of the insights we got from our derivations can
provide initial clues that will lead to development of such
methods.

Acknowledgements
We thank Gal Elidan, Tommy Kaplan, Iftach Nachman,
Matan Ninio, and the anonymous referees for comments on
earlier drafts of this paper. This work was supported in part
by Israeli Science Foundation grant number 224/99-1. Nir
Friedman was also supported by an Alon fellowship and by
the Abe

& Harry Sherman Senior Lectureship in Computer

Science. Experiments reported here were run on equipment
funded by an ISF Basic Equipment Grant.


