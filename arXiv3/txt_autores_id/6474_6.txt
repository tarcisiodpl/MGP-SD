

A pseudo independent (PI) model is a proba­
bilistic domain model (PDM) where proper
subsets of a set of collectively dependent
variables display marginal independence. PI
models cannot be learned correctly by many
algorithms that rely on a single link search.
Earlier work on learning PI models has sug­
gested a straightforward multi-link search al­
gorithm. However, when a domain contains
recursively embedded PI submodels, it may
escape the detection of such an algorithm.
In this paper, we propose an improved al­
gorithm that ensures the learning of all em­
bedded PI submodels whose sizes are upper
bounded by a predetermined parameter. We
show that this improved learning capability
only increases the complexity slightly beyond
that of the previous algorithm. The perfor­
mance of the new algorithm is demonstrated
through experiment.
Keywords:

Belief networks, probabilistic domain
model, learning, search.
1

INT RODUCTION

Learning belief networks has been researched actively
by many as an alternative to elicitation in knowledge
acquisition [3, 1, 4, 2]. A pseudo-independent (PI)
model is a probabilistic domain model (PDM) where
proper subsets of a set of collectively dependent vari­
ables display marginally independence (hence pseudo­
independent) [8, 6]. Commonly used algorithms for
learning belief networks rely on a single link locka­
head search to identify local dependence among vari­
ables. These algorithms cannot learn correctly when
the domain model unknown to us is a P I model [7].
If an incorrectly learned model is used for subsequent

inference, it will cause decision mistakes. Worse yet,
the mistakes will be made without even knowing. The
pseudo-independent property of PI models requires
multi-link lookahead search in order to detect the col­
lective dependency [8]. As the computational complex­
ity increases exponentially with the number of links to
lookahead, a multi-link search must be performed cau­
tiously. In order to manage the increased complexity,
it is suggested [6] that the single link search should
be performed first and then the number of links to
lookahead should be increased one-by-one.
Several issues remain open. A straightforward multi­
link lookahead search as suggested in [8] will perform a
single link lookahead search, then a double link locka­
head search, and then a triple link lookahead search,
etc. It turns out that some PI models will escape such
a multi-link search (to be detailed below). Therefore,
Xiang [6] suggested to perform a single link locka­
head search first, followed by a combination of dou­
ble link lookahead and single link lookahead search,
followed by a combination of triple, double and single
link lookahead search, etc. However, it is unclear what
is the most effective way to combine lookahead search
of different number of links.
In this paper, we propose an algorithm for learning be­
lief networks from PI domains. We focus on learning
decomposable Markov networks [8), although the algo­
rithm can be extended to learning Bayesian networks.
We show that our algorithm will ensure correct learn­
ing of PI models that contain no embedded submodels
beyond a predetermined size. The time complexity of
the algorithm is analyzed.
We assume that readers are familiar with commonly
used graph-theoretic terminologies such as connected
graph, component of a graph, chordal graph, clique,
I-map, Bayesian networks, Markov networks, etc.
The rest of the paper is organized as follows: In section
2, we briefly introduce PI models. In section 3, we
present the algorithm. The property of the algorithm

Learning in Domains with Recursively Embedded PI Submodels

is analyzed in section 4. The complexity is analyzed
in section 5. We present our experimental results in
section 6.
2

BACKGROUND

To make this paper self-contained, we introduce the
basic concepts on PI models briefly in this section.
We will use freely the formal definitions in [6]. More
detailed discussions and examples can be found in the
above reference.
If each variable X in a subset A is marginally inde­
pendent of A\ {X}, we shall say that variables in
A are marginally independent. A set N of variables
are collectively dependent if for each proper subset
A C N, there exists no proper subset C C N\ A
such that P(AIN\A) = P(AIC). A set N of vari­
ables are generally dependent if for any proper subset
A, P(AIN\A)# P(A).
A pseudo-independent (PI) model is a probabilistic do­
main model (PDM) where proper subsets of a set of
collectively dependent variables display marginal inde­
pendence. PI models can be classified into three types.
In a full PI model, every proper subset of variables are
marginally independent.
Definition 1 (Full PI model) A PDM over a set
N (]NI 2:: 3} of variables is a full PI model if the
following two conditions hold:
(51) For each X E N, variables zn N\ {X} are
marginally independent.
(52) Variables in N are collectively dependent.

In a partial PI model, not every proper subset of vari­
ables are marginally independent.
Definition 2 ( Partial PI model) A PDM over a
set N (]NI 2:: 3} of variables is a partial PI model if
the following three conditions hold:
(51') There exists a partition {Nt, ... ,Nk} {k 2:: 2}
of N such that variables in each subset N; are
generally dependent, and for each X E N; and
each Y E Nj {i # j), X andY are marginally
independent.
{52) Variables in N are collectively dependent.

In a PI model, it may be the case that not all vari­
ables in the domain are collectively dependent. An
embedded PI submodel displays the same dependence
pattern of the previous PI models but involves only a
proper subset of domain variables.

259

Definition 3 (Embedded PI submodel)
Let a PDM be over a set N of generally dependent
variables. A proper subset N' C N (]N'I 2:: 3} of vari­
ables forms an embedded PI submodel if the following
two conditions hold:
(54) N' forms a partial PI model.
(55) The partition {N1, ... , Nk} of N' by 51' extends
into N. That is, there is a partition { A1, ... , Ak}
of N such that N; s; A;, {i = 1, .. , k), and for
each X E A; and each Y E Aj {i # j), X and Y
are marginally independent.

In general, a PI model can contain one or more PI
submodels, and this embedding can occur recursively
for any finite number of times.
P DMs can often be concisely represented by a graph
called an !-map [5} of the PDM. In this paper, we shall
mainly use undirected I-maps. In particular, we focus
on learning an I-map that is a decomposable Markov
network (DMN). A DMN consists of a graphical struc­
ture and a probability distribution factorized accord­
ing to the structure. The structure is a chordal graph
whose nodes are labeled by domain variables.
Since variables in a PI submodel are collectively de­
pendent, in a minimal I-map of the PDM, the vari­
ables in the submodel is completely connected. The
marginal independence between subsets in the sub­
model is thus unrepresented. The undirected 1-maps
can be extended into coloredI-maps [6). The marginal
independence between subsets are highlighted in a col­
ored I-map by coloring the corresponding links.
Definition 4 An undirected gmph G is a colored
I-map of a PDM M over N if {1} G is a minimal
/-map of M, and {2) for each PI submodel m, links
between each pair of nodes from distinct marginally
independent subsets in m are colored. Other links are
referred to as black.

A partial P I model is shown in Table 1 . The PDM has
four variables, which are partitioned into three inde­
pendent subsets. The PDM contains three embedded
PI submodels over
N1 ={ a, b, c}, N2 = {d, a , c}, N3 ={d, b, c} .
Figure 1 shows the colored I-map of this model. The
colored links are drawn as dotted. For example, from
the distribution P(a, c, d), it is easy to verify that N2
forms a partial PI submodel with the marginally in­
dependent partition { {a} , { c,d}} (S4) . This partition
extends into a marginally independent partition {{a},
{b, c, d}} (85).

260

Hu and Xiang

Table 1 : A model with embedded P I submodels.
(d, a, b, c)
(0,0,0,0)
(0,0,0, 1)
(0,0, 1,0)
(0,0, 1, 1)
(0, 1,0,0)
(0, 1,0, 1)
(0, 1, 1,0)
(0, 1, 1, 1)

P(.)
0. 02
0.02
0. 06
0
0. 1
0.06
0. 1 4
0. 1

(d, a, b, c)
(1,0,0,0)
(1,0,0,1)
(1, 0, 1,0)
(1, 0, 1, 1)
(1, 1,0,0)
(1, 1,0, 1)
(1, 1, 1,0)
(1, 1, 1, 1)

P(.)
0.03
0.01
O.Dl

0.05
0.0 9
0.07
0. 1 5
0.09

a •.::· ··

Figure 1 : Colored 1-map of the model in Table 1 .
It has been shown (7] that common algorithms for
learning belief networks cannot learn a P I model cor­
rectly because they rely on a single link lookahead
search to identify local dependence among variables.
For example, if these algorithms are used to learn the
above model (assuming learning starts with an empty
graph) only the link (d, c ) can be connected and the
returned graph is not an I-map of the PDM.
3

THE LE A RNING ALGORITHM

The pseudo independence property of PI models re­
quires more sophisticated search procedures in learn­
ing. Suppose a PI submodel over N' C N is parti­
tioned into k marginally independent subsets. If we
lookahead by multiple links at each search step such
that N' is completely connected by a set of new links,
and test P(XIY, N' \X,Y) = P(XIN' \X, Y), where
(X, Y) is one of the new links, we will get a negative
answer. This prompts the completion of N' in the
learned graph. Based on this observation, a straight­
forward multi-link search is suggested in [8]. Such a
search will perform a single link lookahead, followed
by a double link lookahead, followed by a triple link
lookahead, etc.
A multi-link search is more expensive than a single link
search since O(INI2 i) sets of links need to be tested
before one set of links is adopted. Since the complex­
ity increases exponentially with the number of links
to lookahead, an multi-link search must be performed
cautiously. Three strategies are proposed in [6] to
manage the computational complexity: ( 1) perform­
ing single link search first, (2) increasing the number

of links to search one-by-one, and (3) making learning
inference-oriented.
Although the previous straightforward multi-link
search can learn correctly many PI models, it was
found that some PI submodels may still escape the
learning algorithm. For example, if we apply such
a search to the P I model in Table 1, the single link
search will add the link (d, c). The following double
link search will first discover the PI submodel over N2
and add links (d,a) and (a,c). It then discovers the
P I submodel N3 and add links (d, b) and (b, c). But
the P I submodel over N 1 will never be learned by the
double link lookahead or lookahead with higher num­
ber of links, since only a single link ( a, b) is uncon­
nected. Consequently, the learning outcome will not
be an I-map.
Realizing this deficiency of the straightforward multi­
link search, an improved multi-link search algorithm
was proposed in [6]. In addition to the incorporation
of the above three strategies, the search is performed
in the following manner: A single link lookahead is
performed first, followed by a combination of double
link lookahead and single link lookahead, followed by
a combination of triple, double and single link looka­
head, etc. We shall refer to such a systematic search
that lookaheads by no more than i > 1 links as an
i-link search. We refer to a multi-link search which
examines only j � 1 links at each step until no more
links can be learned as an j-link-only search.
The algorithm proposed in [6), however, did not specify
what is the most effective way to combine lookahead
search of different number of links. This is the issue we
address in this paper. We start by asking the question
why some P I models may escape the straightforward
multi-link search. The previous example shows that
the main reason is the recursive embedding of P I sub­
models. If a P I submodel M1 is embedded in another
P I model M2 , M1 will be learned first. After that, if
the number of unlearned links in M2 is less than the
current number of links to lookahead, M2 will not be
learned correctly in the later search steps. In order to
learn M2, backtracking to lower number of lookahead
links is necessary. Hence the problem translates to a
proper arrangement of backtracking during learning.
We propose a multi-link search algorithm (ML) which
overcomes the deficiency mentioned above. The learn­
ing outcome is represented as DMN. The algorithm
focus on learning the chordal structure. Once the
chordal graph is obtained, the numerical probability
distribution can be estimated from the data.
ML starts with an empty graph. It performs a single
link search first. The first stage of the search now ends.

Learning in Domains with

ML then performs a double-link-only search. If some
links are learned during the double-link-only search,
ML backtracks to perform another single link search.
Afterwards, it performs double-link-only search again
and backtracks if necessary as before. The combi­
nation of double-link-only and single link search will
continue until no link is learned in a double-link-only
search. We shall refer to this repeated combination of
the double-link-only search and the single link search
as a combined-double-link search. Now the second
stage of the search ends.

Next, ML will perform a triple-link-only search. If
some links are learned during the search, ML back­
tracks to repeat the previous two stages. Afterwards,
it performs another triple-link-only search and back­
tracks if necessary as before. We shall refer to this
repeated combination of the triple-link-only, double­
link-only and single link search as a combined-triple­
link search. Note that a combined-triple-link search
can include several combined-double-link search. Now
the third stage of the search ends.
ML continues with a combined-four-link search, fol­
lowed by a combined-five-link search, etc., until a
combined-k-link search, where k > 1 is a predeter­
mined integer. The pseudo-code of this algorithm is
presented below.
Algorithm ML
Input: A dataset D over a set N of variables, a
maximum number k of lookahead links.
Return: The learned graph.
Comment: lookahead(i) is the function for
an i-link-only search.
begin
1
initialize a graph G = (N, E = ¢>);
2
for j = 1 to k do
3
� :::::: ];
while i :::; j do
4
5
modified := lookahead( i );
6
if (i > 1) AND (modified= true)
7
then i := 1;
{backtracking}
8
else i := i + 1;
9
return G and halts.
end

:

In algorithm ML, the search stages are indexed by j
(line 2) and each iteration of the outer for loop corre-­
sponds to one stage. The first iteration has i = j = 1
(lines 2 and 3). The single link search lookahead(l)
(line 5) will be performed. The test in line 6 will fail
and i becomes 2 (line 8). This terminates the while
loop as well as the first iteration of the for loop. It
corresponds to the first stage of search.
The next iteration of for loop has i = j = 2.
The double-link-only search lookahead(2) will be per­
formed. If some links have been added, the test in
line 6 will succeed and i becomes 1. This causes the

Recursively Embedded PI

261

Submodels

execution of another single link search lookahead(l).
Afterwards, i becomes 2 and another double-link-only
search will be performed. If nothing has been added,
modified is false and i becomes 3. This terminates
the while loop and the second iteration of the for loop.
It corresponds to the second stage of search.
The next iteration of for loop has i = j = 3. The
triple-link-only search lookahead(3) will be performed.
If some links have been added, the test in line 6 will
succeed and i becomes 1. This causes the repeti­
tion of the previous two stages. This execution of
lookahead(3) and repetition of stages 1 and 2 contin­
ues until an execution of lookahead(3) returns false.
Afterwards, i becomes 4 and the while loop will be
terminated. It will also terminate the third iteration
of the for loop and end the third stage of search.
The function lookahead( i) performs an i-link-only
search. It consists of multiple passes and each pass is
composed of multiple steps. Each step tests one set of
i links. Each pass learns one set of i links after testing
all distinct and legal combinations, one at each search
step, of i links. This function may be implemented
using different scoring metrics. We defer the presen­
tation of our implementation using the cross-entropy
scoring metric to section 6.

..

r

.

. ...

h
(a)

Stage I

il

•.

r

Stage Z

•

b

---·

(b)

a •

/1"
·.
...
,

.

c

h
. •

(c)

Stage 2

Figure 2: The process of learning the model in Table 1.
Figure 2 shows the execution of ML in learning the PI
model in Table 1 with the value of k set as k = 2. ML
starts with a single link search (The first stage). After
all links are examined, one set of links L1 = {(d, c)}
is learned. The learned graph is shown in Figure 2
(a). In the second stage, ML performs the double­
link-only search first, which learns two sets of links
L2 = {(d, a), ( a, c)}, L3 = {(d, b), (b, c)}. These links
are contained in the P I submodels over N2 and N3.
The corresponding graph is shown in Figure 2 (b).
Since some new links are added after the double­
link-only search, ML backtracks to perform the sin­
gle link search again. During this search one set of
links L4 = { (a, b)} is added and Figure 2 (c) is ob­
tained. ML continues to perform another double-link­
only search but no more links can be learned. The ML
halts with a complete graph which is a correct I-map.
4

PROPERTY

Can ML learn any P I model correctly? Clearly the an­
swer is no as ML only searchs up to a predetermined

262

Hu and Xiang

number i of lookahead links. A PI submodel that con­
tains more than i colored links may escape ML. Then
what is the characteristics of the PI models that can
be learned by ML? The following theorem answers this
question.
Theorem 5 Let M be a PI model such that each em­
bedded PI submodel in M contains no more than i col­
ored links, the algorithm ML with parameter i will re­
turn an !-map of M.

5

COMPLEXITY ANALYSIS

For each pass in ani-link-only search, O(N2i) sets of i
links need to be tested, one set at each step. Therefore
each pass contains O(N2i) steps. Since each pass adds
one set of i links, an i-link-only search contains 0( �l)
passes.
Table 2 shows the relation among the index i, the num­
ber of steps per pass and the number of passes in an
i-link-only search.

Proof:
Let GM be the minimal colored I-map of M. All black
links in GM can be learned by the initial single link
search lookahead(1) in the first stage. We show that
ML will learn colored links in every embedded PI sub­
model.
When i = 1, M contains no embedded PI submodels
(no colored links) and the result is trivially true.
When i = 2, each embedded PI submodel in M con­
tains only three variables. There are two colored links
and one black link among these three variables. The
black link will be learned by the initial single link
search as mentioned above. The two colored links can
be learned by lookahead(2) in the second stage.
Now we assume that when i = k, if an embedded PI
submodel has no more than k unlearned colored links,
then these links can be learned by the first k stages.
Suppose i = k + 1. Every PI submodel with no more
than k colored links in M can be learned by assump­
tion. For each PI submodel x with k + 1 colored links
in M, x either contains one or more embedded PI sub­
models or contains none.
If x contains at least one embedded PI submodel y of
j � 2 colored links, then we have j � k and y must
have been learned in the first k stages by assumption.
Since the number of remaining colored links in x is
k + 1 j :::; k 1, these links must also have been
learned in the first k stages by assumption.
-

-

If x contains no embedded PI submodel, then it can be
learned by lookahead (k + 1) at the beginning of stage
D
k + 1. The theorem is proven.
Given the parameter k for ML, some PI submodels
with more than k colored links may still be learned.
Suppose a PI submodel x has more thank colored links
and has two other PI submodels y and z embedded
in it. If the number of colored links in y or z is no
more than k, then y and z can be learned by ML. If
the number of remaining colored links in x is no more
than k, then x can also be learned by ML. A formal
treatment of such cases will be included in a longer
version of this paper.

Table 2: The relation among i, number of steps per
pass and number of passes in an i-link-only search.
1
2
3

# o f stepsfpass
O(N2)
O(N2*2)
O(N2*3)

# o f passes
O(N2)
0( ";l)
0( "';2)

k-1
k

O(N2•(k-1))
O(N2•k)

0( .!:£:...
k-1 .)

o(Jr)

In order to derive the upper bound of the total number
of passes in a k-link search, we construct a directed
graph such that each node in the graph corresponds
to one pass during the search and each arrow indi­
cates the chronological order of successive passes. We
shall label each node by the number of links to locka­
head in the pass. For example, a pass in a single link
search will be labeled by 1, and a pass in a double­
link-only search will be labeled by 2, etc. A graph so
constructed will be a directed chain. For the purpose
of a later conversion, nodes with the same label will
be drawn at the same level and levels are arranged
in the decreasing order of the labels. Figure 3 shows
such a graph for the execution of a 3-link-search. The
four nodes in the bottom left correspond to the four
passes in the first stage during the search. The next
three nodes (labeled 2) correspond to the three passes
in the first double-link-only search. Since links are
learned, they are followed by backtracking to a single
link search, shown by the three nodes labeled 1 in the
middle bottom of the graph.

Figure 3: The execution chain of a 3-link search.
Once we obtain such a chain, it can be converted into a
set of trees (a forest) as follows. Each node not at the
top level will be assigned a parent at the next higher

Learning in Domains with Recursively Embedded PI Submodels

level, and the child and the parent will be connected by
an undirected link. The parent of a node is assigned as
the first node in the next higher level down the chain.
For example, the first node labeled 2 in the chain will
be the parent of the first four nodes labeled 1 in the
chain. The first three nodes labeled 2 in the chain will
have the first node labeled 3 as their parent. After each
node not at the top level has been assigned a parent,
we remove all arrows from the graph. The resultant
graph is shown in Figure 4. Each component of the
graph is a tree. This is because each node not at the
top level has a unique parent. We shall refer to the
graph as an execution forest.

0 0

I
�

Figure 4: The execution forest of a 3-link search.
We now use the execution forest to analyze the com­
plexity of an i-link-search. For each node at level i
(1 < i � k), some child nodes correspond to learning
passes each of which adds a set of i - 1 links. Other
child nodes correspond to non-learning passes that add
no links. The number of non-learning passes can not
be more than the number of! earning passes. The num­
ber of learning passes is bounded by 0 ( {!_21 ) according
to Table 2. Hence each node at level i (1 < i � k ) has
1
ren.
0( i-I
N') ch'ld
Next, we derive the number of passes at each level.
The number of passes at the level k ( top level) is
0( 'i,' ) . The number of passes at the level k- 1 is

The number of passes at the level 2 is

Finally, the number of passes at the level 1 is

Therefore, according to Table 2, the total number of
search steps is
+
+
+
+

263

Since the factor (t + h(Ll) + . . . +
) is
..... 2•1
upper-bounded by 1, the total number of search steps
in a k-link-search is

h(k-1)

O(N2•(k+1l).

In order to complete the complexity analysis, we need
to take into account of the complexity of each search
step, which is dependent on the choice of scoring met­
ric used in lookahead(i). Our implementation, to be
detailed in the next section, is based on the algorithm
in [8]. The complexity of one search step is

O(n + ry(ry log 77 + 211)),
where n I S the number of cases in the dataset and
77 is the maximum size of cliques. Hence the overall
complexity of the algorithm is

Compared with the complexity of a straightforward
multi-link search algorithm [8]

the complexity of a k-link-search using ML is higher
but not much higher. The benefit of the slightly in­
creased complexity is the capability of learning recur­
sively embedded PI models.
6

IMPLEMENTATION AND
EXPERIMENTAL RESULTS

Given the algorithm ML, the only missing detail in
implementation is the function lookahead( i). Our im­
plementation of this function is based on the algo­
rithm in [8]. Instead of testing the conditional inde­
pendence directly, a test of whether new links decrease
the Kullback-Leibler cross entropy is performed. This
is justified the following shown in [8]. ( 1 ) Minimiz­
ing the K-L cross entropy between a dataset D and
a DMN obtained from D is equivalent to minimizing
the entropy of the DMN. (2) A learning process start­
ing with an empty DMN structure and driven by the
minimization of the above K-L cross entropy is par­
alleled by the process of removing false independence
(missing links relative to some minimal 1-map) in the
intermediate DMNs.
The pseudo code of the lookahead(i) function is shown
below. A threshold d is used to differentiate between
a strong dependence and a weak one ( may be due to
noise) . A greedy search can thus be applied { line 4
through 9) to avoid adding unnecessary links and links
due to weak dependence [8]. The condition that L is
implied by a single clique C means that all links in
L are contained in the subgraph induced by C. This
requirement helps to reduce the search space.

264

Hu and Xiang

Function BOOL lookahead( int i );
Input: i is the number of lookahead links.
Comment: &his a threshold.
begin
1 modified:= false;
2 repeat
3
initialize the entropy decrement dh' := 0;
4 for each set L of links (ILl = i, L n E = 4>), do
5
if G" = ( N, E u L) is chordal and L is implied by a
clique, then compute the entropy decrement dh";
6
7
if dh* > dh', then dh' := dh*, G' :== a•;
8 if dh' > oh, then G := G', done:= false,
9
modified :== true;
10 else done := true;
11 until done = true;

12 return
end

The PDM contains five embedded PI submodels over
Nt

The following demonstrates our implementation with
two datasets. Our primary emphasis is the capability
of learning correctly PDMs with recursively embedded
PI submodels. First, a dataset of 1000 cases was gen­
erated from the PDM shown in Table 1. The successful
run used k = 2, Jh = 0.001. The learning process is
the same as Figure 2. It is summarized in Table 3.

N2 = {ball2,ball3, music_box},

N3 = {ball1, ball2, ball3, musicJJOx},
N4 = {lightl, light2, dog},

Ns = {music_box, dog, John}.
Note that the first two PI submodels are recursively
embedded in the third PI submodel.

bl. bl b3
m

Table 3: Summary of learning the PDM in Table 1
i

learned
#graphs cross entropy
tested
decrement
link set
6
0.0033
{(d,c)}
26
0.0139
, a, c)}
{(d, a)J
0.0022
29
{(d ,b), (b,c)}
0.0389
30
{(a, b)}

-lmk1

2
2

1

{ball1,ball3,music_.box},

We generated a dataset of 2000 cases from the music­
box-dog-John domain. Using k = 3 and 6h = 0.004,
the algorithm learned the 1-map successfully. The
learning process is shown in Figure 7.

modified;

only search

=

Next, we use a PDM from [6] described below:
Three balls are drawn each from a different urn. Urn
1 has 20% white balls and the rest of the balls black.
Urn 2 and urn 3 have 60% and 50% of white balls,
respectively. A music box plays if all three balls are
white or exactly one is white. A dog barks if two
random lights are both on or both off. John complains
if it's too quiet (neither the box plays nor the dog
barks) or too noisy (both the box plays and the dog
barks).
The model is specified as a Bayesian network shown in
Figure 5. Its colored 1-map is shown in Figure 6.

II

I \
Jo

b.2·•v· \.:-�·

.12

bl ..

. ·:

.

b3 II

m

(a)

J•

Stage I

12

bl.?:>.. _b3 11 12
··.:··

(c)

Stage 2

�v ... '): .
�·
·

t·

(d)

Stage 3

Figure 7: The process of learning the music-box model.
The algorithm started by performing the single link
search. In the first pass, one link was learned:
Lt

={(light!, dog)}

It took 28 steps (28 candidate graphs tested). In the
second pass, after 27 steps, another link was learned:
L2 = {(ball3, music_box)}.

Note that a standard single-link search learning algo­
rithm will halt and returns this graph which is not
an 1-map of the domain. Since nothing was learned
in the third pass, a 2-link-only search was performed
next. After 884 steps, three sets of links were learned
in the following order:
Ls = {(light 1, light2), (light2, dog)},
£4 = {(ball2, ball3), (ball2, music_box)},

ball2e
b.:r

· •·• \/_..," \;�;r"

music_box.·····....

...

.··

. e·}�hn

Figure 6: Colored 1-map of the music-box example.

£5

= {(ball1, ball3), (balll, music_box)}.

Then the algorithm backtracked to perform a single
link search with one link learned:
Ls

= {(balll, ball2)}.

During the next single link search and the following
2-link-only search, no link was added. Hence a 3-link­
only search was performed, which learned the links:

Learning in Domains with Recu rsively Embedded PI Submodels

ball!

b all2 bal l3 l ight I light2

m"'

'� ):f

dog£ { b ark,quiet}
music_box £ {play,quiet}
John £ ( complain,satisfied}

ball£ {white,bl ack} P(b alll =w)=0.2
light£ ( on,off}
P(ball2=w)=0.6
P(b all3=w)=0.5
P(lightl=on)=0.5
P(light2=on)=0. 7

P(John=clmusic_box=p,dog=b)=l
P(John=clmusic_box=q,dog=b)=O
P(John=clmusic_box=p,dog=q)=O
P{John=clmusic_box=q,dog=q)=l

P(dog=bllight 1 :;:an,light2=on)= 1

P(music_box=plall balls=w)=l
P(music_box=plone b all=w)=l
P(music_box=plall balls=b)=0
P(music_box=plone b all=b)=O

P(dog=bllight 1 =on,light2=off)=0

P(dog:;:bllight 1 =off,light2=on)=0
P(dog=bllight1=off,light2=off)=l

265

Figure 5: The specification of the music-box model.
L1

=

{(music.hox, dog), (dog, John), (John, music.hox)}.

The backtracking occurred afterwards, but no more
links was learned. Finally, the algorithm halted and
returned the correct I-map. A total of 3583 candidate
graphs were tested. A summary of the experiment is
shown in Table 4.

- -

Table 4: Summary of learning result

i

link

only
search
1
1

2
2
2
1
3

learned
link set
£1
£2
£3

£4

Ls

Ls

£7

# graphs cr oss entropy
tested
28
55
432
708
939
1149
2327

decrement
0.0822
0.0069
0.6109
0.1922
0.0146
0.4802
0.6895

We believe that no search steps in the improved algo­
rithm may be deleted without jeopardizing the above
learning capability. We are currently working to for­
mally establish this result.
Acknowledgements
This work is supported by grant OGP0155425 from the
Natural Sciences and Engineering Research Council and
grant from the Institute for Robotics and Intelligent Sys­
tems in the Networks of Centres of Excellence Program of
Canada.


A Robust Quantum Random Access Memory
Fang-Yu Hong,1 Yang Xiang,2 Zhi-Yan Zhu,1 Li-zhen Jiang,3 and Liang-neng Wu4
1

arXiv:1201.2250v1 [quant-ph] 11 Jan 2012

Department of Physics, Center for Optoelectronics Materials and Devices,
Zhejiang Sci-Tech University, Hangzhou, Zhejiang 310018, China
2
School of Physics and Electronics, Henan University, Kaifeng, Henan 475004, China
3
College of Information and Electronic Engineering,
Zhejiang Gongshang University, Hangzhou, Zhejiang 310018,China
4
College of Science, China Jiliang University, Hangzhou, Zhejiang 310018, China
(Dated: January 12, 2012)
A “bucket brigade” architecture for a quantum random memory of N = 2n memory cells needs
n(n+5)/2 times of quantum manipulation on control circuit nodes per memory call. Here we propose
a scheme, in which only average n/2 times manipulation is required to accomplish a memory call.
This scheme may significantly decrease the time spent on a memory call and the average overall
error rate per memory call. A physical implementation scheme for storing an arbitrary state in a
selected memory cell followed by reading it out is discussed.
PACS numbers: 03.67.Lx, 03.65.Ud, 89.20.Ff
Keywords: quantum random memory, bucket brigade, microtoroidal resonator

A random access memory (RAM) is a fundamental
computing device, in which information (bit) can be
stored in any memory cell and be read out at discretion
[1, 2]. A RAM is made up of an input address register, a
data register, an array of memory cells, and a controlling
circuit. A unique address is ascribed to each memory
cell. When the address of a memory cell is loaded into
the address register, the memory cell is selected and the
information in the data register can be stored in it or the
information of the cell can be read out to the data register. Like its classic counterpart, quantum random access
memory (QRAM) is the building block of large quantum computers. A QRAM is a RAM working in a way
with quantum characteristic: the address and data registers are comprised of qubits instead of bits, and every
node of the controlling circuit is composed of a quantum
P object. When the address state is in superposition,
Pi αi |xi i, the read-out operation gives the output state
i αi |qi i in the data register, where |qi i is the quantum information stored in the memory cell i associated
with the address |xi i. Quantum random access memories storing classic data can exponentially speed up the
pattern recognition [3–6], discrete logarithm [7, 8], and
quantum Fourier transform, and quantum searching on
a classical database [9]. A general QRAM is an indispensable for the performance of many algorithms, such
as quantum searching [10], element distinctness [11, 12],
collision finding [13], general NAND-tree evaluation [14],
and signal routing [15].
In a seminal paper [15, 16], Giovannetti et al. (GLM)
proposed a promising bucket-brigade architecture for
QRAMs, which exponentially reduce the requirements
for a memory call. However, in GLM scheme, n times
of quantum unitary transformations per memory call is
required to turn one quantum trit initialized in |waiti in
each node of the control circuit into |lef ti or |righti, and
all flying qubits including address qubit and bus qubit
can pass through an arbitrary node of the controlling cir-

cuit only if a successful quantum manipulation has been
performed on the trits, leading to the times of manipulations on the nodes Nc = n(n + 5)/2 per memory call for
2n memory cells, where n is the number of bits in the address register. Here we present a QRAM scheme, where
the quantum object in every node have only two possible
states |lef ti and |righti. On average the times of quantum manipulations on the nodes per memory call can be
reduced to Nc = n/2, significantly decreasing both the
decoherence rate and the time spent on a QRAM call.
A physical implementation for information storage and
read-out on a QRAM is presented.
The main idea is shown in Fig.1. The N memory cells
are positioned at the end of a bifurcation control circuit
with n = log2 N levels. At each node of the control circuit there is a qubit with two states |lef ti and |righti.
The state of the jth qubit in the address register controls
which route to follow when a signal arrives at a node in
the jth level of the circuit: if the node qubit is |0i, the left
path is chosen; if it is |1i, the right path is chosen. For
example, an address register |001i means that left at the
0th level, left at the next, and right at the second. Illuminated by a control laser pulse a node qubit in state |lef ti
will flip to |righti if the incoming address qubit is |1i, or
remain in |lef ti if the address qubit is |0i. Without the
control pulse, a node qubit in |lef ti (|righti)will deviate
any incoming signal along the left(right) side route.
First, all the node qubits are initialized in state |lef ti.
Then the first qubit of the address register is dispatched
through the circuit. At the first node, the address qubit
incurs a unitary transformation U on the node qubit with
the help of a control pulse Ω(t): U |0i|lef ti = |0i|lef ti
and U |1i|lef ti = |0i|righti. Next the second qubit of the
address register is dispatched through the circuit, follow
left or right route relying on the state of the first node
qubit, and arrives at one of the two nodes on the second level of the circuit. The node qubit illuminated by
the control pulse will make a corresponding state change

2
according to the state of the second address qubit, and
so on. Note that the ith control pulse Ω(t) address all
the nodes of the ith level control circuit simultaneously.
After all the n qubits of the address register have gone
through the whole circuit, a unique path of n qubits has
been singled out from the circuit (see Fig.1). Subsequently, a single photon is sent along the selected path
to single out a memory cell. After that an arbitrary unknown state in the data register can be transferred to
the selected memory cell along the selected path, or the
state of the selected memory cell can be read out to the
data register along the path with black squares in Fig.1.
Finally, all the node qubits are reset to |lef ti for a next
memory address.
Because the state of a node qubit |lef ti will not be affected by the control pulse illuminating should the qubit
of the address register be in state |0i, on average there
are n/2 node qubits will flip to |righti in each memory
call. This means that on average only n/2 times of control manipulations are really performed in each memory
call. As a result, the mean comprehensive error rate per
memory address is nǫ/2 = 12 log2 N ǫ with the assumed
error rate ǫ per node qubit flip event. In contrast, the
GLM scheme requires n times state flip for a memory
call. In addition, in the GLM scheme a photon may pass
through a node only when a control pulse is applied on
the quantum trit, resulting the overall times of quantum
manipulations on the quantum trits per memory call be
n(n + 5)2, which has included 2n times of manipulations
for a signal photon going to a memory cell and back to a
data register along a same selected path. Here a singlephoton can pass through a node without any quantum
manipulation, therefore the average times of quantum
manipulation really performed on node qubits per memory call is n/2. Thus this scheme may significantly decrease the average overall error rate and shorten the time
required for a memory call.
Now we discuss a physical implementation. The node
qubit is encode on an atom with level |lef ti, |righti, and
an intermediate state |ei (see Fig.2 A). The transition
between |lef ti and |ei is coupled to the evanescent fields
of modes a and b of of frequency ωc of a microtoroidal
resonator. State |righti is coupled to |ei by classical
control field Ω(t). A tapered fiber and the resonator are
assumed to be in critical coupling where the input photons of frequency ωp = ωc are all reflected back and the
forward flux in the fiber drops to zero when the atom
transition (|lef ti → |ei) is far detuned from the resonator
frequency ωc [17]. If the atomic transition (|righti → |ei)
is on resonant with the resonator, the input photons can
transmit the resonator and travel forward one by one
[18]. A single-photon can be coherently stored in the
atom initialized in |lef ti by applying the control pulse
Ω(t) simultaneous with the arrive of the photon which is
equally divided and incident from both sides of the tapered fiber simultaneously (see Fig.2) [19]. The photon
storage results in a state flip of the atom to |righti. If
no single-photon is contained in the incoming field, the

data register

0

0

1

address register

left
left
right

left
left

left

left

memory cells

FIG. 1. (color online). Schematics for a quantum random
access memory. In each node of the binary control circuit, a
qubit in |righti(|lef ti) routes the approaching signals right
(left). A single photon can excite the qubit from |lef ti to
|righti with the aid of a classical impedance-matched control field Ω(t). Here the third level memory cell |001i is addressed through the selected path marked with red circles.
The read-out state is transferred to a data register along the
path marked with black squares.

atom does not affected by the control pulse illuminating
and remains in |lef ti [19].
The switch function of a node qubit in a QRAM can be
realized as follows: first, the address qubit is encoded as
α|0ip +β|1ip with Fock state |nip (n = 0, 1) and arbitrary
unknown complex coefficients α and β; the first qubit of
the address register is sent out along the control circuit
and is coherently stored in the atom in the first node
by applying the control pulse Ω(t) simultaneous with the
arrival of the address qubit equally split and incident
from both sides of the tapered fiber simultaneously. This
storing process will incur a state flip of the node atom to
|righti if the address qubit is |1i, or make no change in
the atom state |lef ti if the address qubit is |0i. When
the second address qubit is sent out and meet the first
node, it will be reflected back and travel along the left
path by applying an optical circulator in one side of the
tapered fiber (see Fig.2 b) if the atom in the first node
is in |lef ti, or will transmit the resonator and go along
the right path if the atom is in |righti. When the second
address qubit arrive at one of the two nodes on the second
level, it will be coherently stored in the node atom and
left the atom in |lef ti or |righti dependent on the photon
number contained in the address qubit, and so on.
We assume that each quantum memory cell in the
memory array consists of a memory atom m and an ancillary atom a, which are confined in two harmonic traps
and positioned inside a high quality cavity (see Fig.3A).
The ancillary atom has a three-level structure: |gia is
coupled to |eia by the field of the cavity mode with
strength ga ; |sia is coupled to |eia by a classic control
field Ω1 (t), where the subscript a denotes an ancillary
atom. After a path to the memory array is singled out, a
single-photon is sent along the path to the selected memory cell. A control laser pulse Ω1 (t) is applied to the ancillary atoms initialized in state |gia at the moment when

3
A

B

|eÚ
Ω ( t )

g

|rightÚ

|leftÚ
|leftÚ

atom
a

|rightÚ

microtoroidal
b resonator

optical
circulator

a
b

left

right

FIG. 2. (Color online). Schematic diagram of a node consisting of a three-level atom and a microtoroidal resonator.
(A) An address qubit consisting of zero or one photon is
split equally in counter-propagating directions and coherently
stored using an impedance-matched control field Ω(t), leading to a state flip of the atom conditioned on the photon
number. (B) By employing an optical circulator an photon
travels along a tapered fiber being in critical coupling to the
resonator will go along the left (right) path if the atom is in
state |lef ti(|righti).

the photon arrives at the memory cell, resulting a state
flip of atom a to |sia [20, 21]. To avoid to be involved
into the quantum operations aimed on the selected memory cell, the ancillary atoms non-selected are excited to a
stable state |tia by a π pulses on transition |gia → |tia .
Next we can do some quantum manipulations either to
save an arbitrary unknown quantum state in the selected
memory cell or to read out its content.
In the first place, we discuss how to save an arbitrary
unknown quantum state in the memory cell identified
by the address register. First, an initialization operation on the memory cell array is performed. This can
be realized as follows: atom a in state |sia is excited
to a Rydberg state |ria by two-photon stimulated Raman (TWSR) pulses [22]; TWSR pulses on the transition
|sim → |rim (in terms of the perturbed state ) of the
memory atoms are applied, resulting the selected memory atom being excited to |rim and immediately flipping
to the ground state |gim through spontaneous radiation
(see Fig.3B). In this way only the memory atom in the
selected memory cell is reset in state |gim , leaving the
content of the others unchanged, because the states |rim
of the non-selected memory atoms are off resonant with
the TWSR pulses due to the absence of the strong Rydberg dipole interactions [23]. The ancillary atom in Rydgerg |ria is brought to the ground state |gia by applying
a π pulse on its transition |ria → |gia .
Second, an arbitrary unknown state α|0ip + β|1ip with
Fock bases |nip (n = 0, 1) is transferred along the selected
path to the memory cells. On the arrival of the signal
a classic control pulse Ω2 (t) is applied on the ancillary
atoms initialized in the ground state |gia , leading to a
state map (α|0ip + β|1ip )|gia → |0ip (α|gia + β|sia ) [20,
21].
State α|gia + β|sia is then transferred to a memory
atom by employing the strong dipole interaction between

two Rydberg atoms [24, 25]. When both of the memory
atom and the ancillary atom are in Rydberg states, the
strong dipole interaction between them will couple their
motion, which is best described in the basis of normal
modes |jin (j = 0, 1, 2, ...). All the motion modes of the
atoms are initially cooled to near their ground state by
Raman sideband cooling on them [26].
Third, we drive a π pulse on transition on |gim → |rim
on memory atoms to excite the memory atoms from
state |gim to state |rim . Fourth, a π pulse on transition |gia → |ria |0in and a blue sideband (BSB) pulse
on transition |sia → |ria |1in are applied on the ancillary
atoms, leading to state
|ψi1 = α|ria |rim |0in + β|ria |rim |1in

(1)

for the selected memory cell (see Fig. 3bB) and state
|ψi2 = (αx |rim + βx |sim )|tia for other memory cells with
their initial states state αx |gim + βx |sim (see Fig.3C).
Here we have used the fact that the normal mode of
motion is shared by the memory atom and the ancillary atom, both of which are in Rydberg states. Fifth,
a π pulse on the transition |rim → |gim unperturbed by
the strong dipole interaction between two Rydberg atoms
is applied on the non-selected memory atoms to restore
them to their initial states αx |rim + βx |sim .
Sixth, a π pulse on transition |rim |1in → |r′ im |0in
illuminates the memory atoms, resulting a state mapping
|ψi1 → |ψi2 = α|ria |rim |0in + β|ria |r′ im |0in .

(2)

′

Seventh, two π pulses on |r im |0in → |si and |rim |0in →
|gim , respectively, are applied on the memory atom,
leading to an unitary transformation |ψi2 → |ψi3 =
(α|gim + β|sim )|ria ; the unknown state α|0i + β|1i has
been stored on the selected memory atom. Note that this
two pulses will not affect the states of the non-selected
memory atoms since the pulses is detuned from the transitions |r′ im → |sim and |rim → |gim , which are free
of the influence of strong dipole interaction between two
Rydberg atoms. Eighth, the ancillary atoms is restored
to the ground state |gia by a a π pulse on the transition
|ria → |gia (see Fig.3E), and the non-selected ancillary
atoms are restored to the ground state |gi for the next
memory call by flipping to a intermediate level with a
pulse and then falling to |gi through spontaneous radiation. In this way an arbitrary unknown state can be
transferred to the selected memory atom, leaving the
states of other memory atoms unchanged.
Now we discuss how to read out the content of the selected memory atom αx |gim + βx |sim . First, a π pulse
on transition |sia → |ria is employed to excite the selected ancillary atom to state |ria . Second, we employ two π pulses on transition |gim → |rim |0in and
|sim → |rim |1in , respectively (see Fig.3F), driving the
system of the selected atoms m and a into state
|ψi4 = αx |rim |ria |0in + βx |rim |ria |1in .

(3)

Third, a π pulse on transition |ria |1in → |r′ ia |0in
drive the selected system into state |rim (αx |ria +

4
$

βx |r′ ia )|0in (see Fig.3G). Fourth, the selected memory
atom is sent to the ground state by a π pulse on the
transition |rim |0in → |gim . Fifth, two pulses on the
transition |ria → |gia and |r′ ia → |sia , respectively, set
the selected ancillary atom in state αx |gia + βx |sia (see
Fig.3H); the content of the select memory atom is transferred to the ancillary atom. Note that these pulses do

not influence the non-selected atoms a and m because
they have no strong dipole interaction. By applying a
classical impedance-matched control field Ω(t), a matterphoton mapping (αx |gia + βx |sia )|0ip → |gia (αx |0ip +
βx |1ip ) can be accomplished [20, 21], transferring the
state of the selected memory cell to the flying qubit and
leaving the ancillary atom in state |gia . The flying qubit
goes along the path with black squares to the data register (see Fig.1). Finally, the non-selected ancillary atoms
are initialized to state |gia for a next task.
In summary, we have presented a scheme for a quantum random access memory. With three-level memory
system been substituted by a qubit in every node of the
control circuit, this structure may significantly reduce
overall error rate per memory address and the memory
address time. In addition, we have discussed a physical implementation based on microtoroidal resonator and
strong-dipole interaction between two Rydberg atoms for
a QRAM writing and read-out. The microtoroidal resonator and the tapered fiber may be replaced by a surface plasmon propagating on the surface of a nanowireconductor-dielectric interface [27, 28].
This work was supported by the National Natural Science Foundation of China ( 11072218, and 11005031), by
Zhejiang Provincial Natural Science Foundation of China
(Grant No. Y6110314 and Y6100421), and by Scientific
Research Fund of Zhejiang Provincial Education Department (Grant No. Y200909693 and Y200906669).

[1] R. Feynman, Feynman Lectures on Computation
(Perseus Books Group, New York, 2000).
[2] R. C. Jaeger and T. N. Blalock, Microelectronic Circuit
Design (McGraw-Hill, Dubuque, 2003), p. 545.
[3] G. Schaller and R. Schützhold, Phys. Rev. A 74, 012303
(2006).
[4] R. Schützhold, Phys. Rev. A 67, 062311 (2003).
[5] C. A. Trugenberger, Phys. Rev. Lett. 87, 067901 (2001);
89, 277903 (2002).
[6] D. Curtis and D. A. Meyer, Proc. SPIE 5161, 134 (2004).
[7] A. Ambainis, Proceedings of the 45th IEEE Symposium
on Foundations of Computer Science (FOCS04) (2004),
p. 22;SIAM J. Comput. 37, 210 (2007).
[8] A. M. Childs, A. W. Harrow, and P. Wocjan, Proceedings
of the 24th Symposium on Theoretical Aspects of Computer Science (STACS 2007), Lecture Notes in Computer
Science vol. 4393 (2007), p. 598.
[9] M. A. Nielsen and I. L. Chuang, Quantum Computation
and Quantum Information (Cambridge University Press,
Cambridge, 2000).
[10] L. K. Grover, in Proceedings of the 28th Annual Symposium on the Theory of Computing(ACM Press, New
York, 1996), p.212.
[11] A. Ambainis, in Proceedings of the 45th IEEE
FOCS04 (IEEE Computer Society, Rome, 2004), p. 22;
arXiv:quant-ph/0311001.
[12] A. M. Childs, A.W. Harrow, and P. Wocjan, in Proceedings of the 24th Symposium on Theoretical Aspects of
Computer Science (STACS 2007), Lecture Notes in Com-

puter Science (Springer, New York, 2007), Vol. 4393, p.
598; arXiv:quant-ph/0609110.
G. Brassard, P. Høer, and A. Tapp, ACM SIGACT
News (Cryptology column) 28, 14 (1997), e-print
arXiv:quant-ph/9705002.
A. M. Childset al. in Proceedings of the 48th IEEE Symposium on Foundations of Computer Science (FOCS07)
(to be published); arXiv:quant-ph/0703015.
V. Giovannetti, S. Lloyd, and L. Maccone, Phys. Rev. A
78, 052310 (2008).
V. Giovannetti, S. Lloyd, and L. Maccone, Phys. Rev.
Lett. 100, 160501 (2008).
T. Aoki, B. Dayan, E. Wilcut, W. P. Bowen, A. S.
Parkins, T. J. Kippenberg, K. J. Vahala, and H. J. Kimble, Nature( London) 443, 671 (2006).
B. Dayan, A. S. Parkins, T. Aoki, E. P. Ostby, K. J.
Vahala, H. J. Kimble, Science 319, 4062 (2008).
F.-Y. Hong and S.-J. Xiong, Phys. Rev. A 78, 013812
(2008).
W. Yao, R.-B. Liu, and L. J. Sham, Phys. Rev. Lett. 95,
030504 (2005).
W. Yao, R.-B. Liu, and L. J. Sham, J.Opt.B 7, S318
(2005).
E. Urban, T. A. Johnson, T. Henage, L. Isenhower, D.
D. Yavuz, T. G.Walker, and M. Saffman, Nature Phys.
5, 110 (2009).
D. Jaksch, J. I. Cirac, P. Zoller, S. L. Rolston, R. Côté,
and M. D. Lukin Phys. Rev. Lett. 85, 2208 (2000).
F.-Y. Hong, Y. Xiang, Z.Y. Zhu, and W.H. Tang, Quan-

|eÚa

m
Ω1 ( t )

ga

a

|sÚa |gÚ
a

%

&

'

(

)

*

+

|rÚ |1|0ÚÚ
|r'Ú

n
n

|sÚ
|gÚ

|1Ún
|0Ún
|1Ún
|0Ún

m

a

m

a

m

a

m

a

m

a

m

a

m

a

FIG. 3. (color online). Schematics for writing to and reading
out of a memory cell. (A) A single-photon can be coherently
stored in a memory cell consisting of two atoms m and a
positioned inside of a high-Q cavity with a time-dependent
control pulse Ω2 (t). Diagrams of energy levels and pulses
sequences for storing unknown state to the selected memory
cell (B to E) and for reading out of the content of the selected
memory cell (F to H).

[13]
[14]

[15]
[16]
[17]

[18]
[19]
[20]
[21]
[22]

[23]
[24]

5
tum Inf. Comput. 11, 0925 (2011).
[25] P. O. Schmidt, T. Rosenband, C. Langer, W. M. Itano,
J. C. Bergquist, and D. J. Wineland, Science 309, 749
(2005).
[26] C. Monroe, D. M. Meekhof, B. E. King, W. M. Itano,
and D. J. Wineland, Phys. Rev. Lett. 75, 4714(1995).

[27] D.E. Chang, A.S. Sørensen, E.A. Demler, and M.D.
Lukin, Nature Phys. 3, 807 (2007).
[28] F.-Y. Hong, S.J. Xiong, Nanoscale Res. Lett. 3, 361
(2008).





these algorithms, a single-link lookahead search is com­
monly adopted for efficiency. In a single-link lookahead

It has been shown that a class of probabilistic
domain models cannot be learned correctly
by several existing algorithms which employ
a single-link lookahead search. When a multi­
link lookahead search is used, the computa­
tional complexity of the learning algorithm
increases.

We study how to use parallelism

to tackle the increased complexity in learn­
ing such models and to speed up learning in
large domains. An algorithm is proposed to
decompose the learning task for parallel pro­
cessing. A further task decomposition is used

search, consecutive network structures adopted differ
by only one link.

However, it has been shown that

there exists a class of domain models termed pseudo­
independent

(PI)

models which cannot be learned cor­

rectly by a single-link lookahead search (Xiang et al.

1996).

One alternative for learning PI models is to

use multi-link lookahead search (Xiang et a!.

1997),

where consecutive network structures may differ by
more than one link. Increasing the number of links to
lookahead, however, increases the complexity of learn­
ing computation.
In this work, we study parallel learning of belief net­

to balance load among processors and to in­

works. Parallel learning not only can be used to tackle

crease the speed-up and efficiency. For learn­

the increased complexity during multi-link lookahead

ing from very large datasets, we present a re­

search, but also can speed up learning computation

grouping of the available processors such that

during single-link lookahead search in a large domain.

slow data access through file can be replaced

Although parallel learning of rules have been studied

by fast memory access. Our implementation
in a parallel computer demonstrates the ef­
fectiveness of the algorithm.

(Cook & Holder 1990; Provost & Aronis 1996; Shaw
& Sikora 1990), we do not realize other works on par­
allel learning of belief networks.

Our study focuses

on learning decomposable Markov networks (DMNs),
although our result can be generalized to learning

1

INTRODUCTION

As the applicability of belief networks has been demon­

Bayesian networks. We study the parallelism using a
message passing MIMD (multiple instruction multiple
data) parallel computer.

strated in different domains, and many effective infer­

We shall assume that readers are familiar with com­

ence techniques have been developed, the acquisition

monly used graph-theoretic terminologies such

of such networks from domain experts through elicita­

cle, connected graph, DAG, chordal graph, clique,

tion becomes a bottleneck. As an alternative to man­

junction tree (JT), sepset in a JT, I-map, etc. A junc­

as

cy­

ual knowledge acquisition, many researchers have ac­

tion forest (JF) F of chordal graph G is a set of JTs,

tively investigated methods for learning such networks

each of which is a JT of one component of G.

from data (Cooper & Herskovits 1992; Beckerman et
al. 1995; Herskovits & Cooper 1990; Lam & Bacchus

1994;

Spirtes et al. 1991; Xiang et al. 1997).

The paper is organized

as

follows: In Section 2, we in­

troduce PI models and multi-link lookahead search. In
section

3,

we propose parallel algorithms for learning

Since learning belief networks in general is NP-hard

belief networks. We also analyze the problems of load

(Chickering et al. 1995). it is justified to use heuristic

balancing and local memory limitation, and present

search in learning. Many algorithms developed use a

our solutions. In section
tal results.

scoring metric combined with a search procedure. In

4,

we present our experimen­

Exploring Parallelism in Learning Belief Networks

B ACKGROUND

2

Xl

,v
!\ '�y,,�-l

To make this paper self-contained, we give a brief
introduction of PI models and multi-link lookahead
search.
2.1

XI

PSEUDO-INDEPENDENT MODELS

(a) Structure define<! by
given PI model

It has been shown (Xiang et a!. 1997) that there exists
a class of probability domain models where proper sub­
sets of a set of collectively dependent variables display
marginal independence. Examples of PI models are
parity problems and Modulus addition problems (Xi­
ang 1996). Several algorithms for learning belief net­
works have been shown being unable to learn correctly
when the underlying domain model is a PI model. A
simple PI model with four variables is shown in Ta­
ble 1. More examples can be found in (Xiang et a!.
1996).
Table 1: An example of PI models
(X,, X2, Xa, X4)
0,0,0,0
0,0,0,1
0,0,1,0
0,0,1,1
0,1,0,0
0,1,0,1
0,1,1,0
0,1,1,1

P(N)
0.0225
0.2025
0.005
0.02
0.0175
0.0075
0.135
0.09

(X,,X2,X3,X4)
1,0,0,0
1,0,0,1
1,0,1,0
1,0,1,1
1,1,0,0
1,1,0,1
1,1,1,0
1,1,1,1

P(N)
0.02
0.18
0.01
0.04
0.035
0.015
0.12
0.08

It can be verified that X1 and X4 are condition­
ally independent given X2 and X3. In the sub­
set {X2, X3,X4}, each pair is marginally dependent,
e.g., P(X2, X3) # P(X2)P(X3), and is still depen­
dent given the third, e.g., P(X2IX3,X4) 'I P(X2IX4).
However, a special dependence relationship exists
in the subset {X1,X2,X3}. Although each pair is
dependent given the third, e.g., P(XdX2, X3) 'f:.
P(XtiX2), X1 and X2 are marginally independent,
i.e., P(X�o X2) = P(Xl)P(X2), so are Xt and X3.
{X1,X2,X3} are said to be pairwise independent but
collectively dependent. They form an embedded PI
submodel. The minimal I-map of this model is shown
in Figure 1 (a).
Suppose learning starts with an empty graph or struc­
ture (with all nodes but without any link). A single
link lookahead search will not connect X1 and X2 since
the two variables are marginally independent. Nei­
ther will X1 and X3 be connected. This results in the
learned DMN structure in Figure 1 (b), which is incor­
rect. On the other hand, if we perform a double link
search after the single-link search, which can effectively
test whether P(X1IX2,X3) = P(XtiX2) holds, then
the answer will be negative and the two links (Xt, X2)

(b) Structure learned by
•ingle-link search

91

XI

(c) Structure learned by
double-link <earch

Figure 1: Comparison of learning results.
and ( X1 , X3) will be added. The learned DMN struc­
ture is shown in Figure 1 (c).
2.2

A MULTI-LINK LOOKAHEAD
SEARCH ALGORITHM

As our parallel learning algorithm is developed based
on a multi-link lookahead search algorithm (Xiang et
al. 1997), the latter is briefly introduced below.
Algorithm

Input:

(Sequential)

A

dataset D over a set N of variables, a maximum
size '7 of clique, a maximum number K. ::; '7('7- 1 )/2
of lookahead links, and a threshold 8h.

begin
initialize an empty graph G = ( N, E);
G':=G;
for i 1 to r;., do
repeat
initialize the entropy decrement dh' := 0;
for each set L of i links ( Ln E = ¢), do
if G" = (N, E u L) is chordal and Lis im­
plied by a single clique of size ::; '7, then
compute the entropy decrement dh*;
if dh* > dh', then dh' := dh*, G' := G*;
if dh' > 8h, then G := G', done := false;
else done := true;
until done true;
return G;
end
=

=

The search is structured into levels and the number
of lookahead links is identical in the same level. Each
level consists of multiple passes. Each pass at the same
level tries to add the same number i of links, that is, al­
ternative structures that differ from the current struc­
ture by i links are evaluated. For instance, level one
search adds a single link in each pass, level two search
adds two links, and so on. Search at each pass selects
i links that decrease the cross entropy maximally after
testing all distinct and legal combinations of i links.
If the corresponding entropy decrement is significant
enough, the i links will be adopted and search contin­
ues at the same level. Otherwise, the next higher level
of search starts.
Note that each intermediate graph is chordal as indi­
cated by the if statement in the inner-most loop. The

Chu and Xiang

92

condition that L is implied by a single clique C means
that all links in L are contained in the subgraph in­
duced by C. This requirement helps to reduce the
search space.
PARALLEL LEARNING OF

3

BELIEF NETWORKS

Learning a belief network using a single link lookahead
search requires checking of O(N2) alternative struc­
tures before a link is added. In an m link lookahead
search, O(N2m) structures must be checked before m
links can be added. We view parallel learning as an
alternative to tackle the increased complexity in multi­
link search, as well as to speed up the single link search
when the domain is large.
3.1

alternative graphs based on the current graph. It then
partitions these graphs into n sets and distributes one
set to each explorer. Each explorer executes Algorithm
(Explorer-1). It checks chordality for each graph re­
ceived and computes the cross entropy decrement dh*
for each valid chordal graph. It then chooses the best
graph a• and reports dh* and a• to manager. Man­
ager collects the reported graphs from all explorers, se­
lects the best, and then starts the next pass of search.
Algorithm

begin
receive D, Nand
repeat
receive G and

PARALLEL LEARNING

Algorithm

(Manager-1)
Input: A dataset D over a set N of variables, a maximum
size rJ of clique, a maximum number� ::; rJ(TJ- 1)/2
of lookahead links, the total number n of explorers,
and a threshold oh for the cross entropy decrement.
begin
send D, N and '1 to each explorer;
initialize an empty graph G = (N, E);
G':=G;

for i =

1 to

"'•

do

repeat

end

initialize the cross entropy decrement dh' := 0;
partition all graphs that differ from G by i links
into n sets;
send one set of graphs and G to each explorer;
for each explorer
receive dh" and G";
if dh" > dh' then dh' := dh", G' := G*;
if dh' > oh, then G := G', done:= false;
else done := true;
until done = true;
send a termination signal to each explorer;
return G;

As mentioned earlier, our study is performed in an
environment where processors communicate through
message passing only (vs. shared memory). We par­
tition the processors as follows. One processor is des­
ignated as the search manager and the others are net­
work structure explorers. The manager executes Al­
gorithm (Manager-1). It is responsible for generating

end

TJ

from the manager;

set of graphs from the manager;
0 and G* :== G;
for each received graph G' = (N, L U E), do
if G' is chordal and L is implied by a single
clique of size ::; r], then compute dh' locally;
if dh' > dh*, then dh* := dh', G" :== G';
send dh. and c· to the manager;
until termination signal is received;
initialize dh*

ALGORITHMS

We extend Algorithm (Sequential) to parallel learning
based on the following observation: at each pass of
search, the exploration of alternative structures are
coupled only through the current structure, i.e., given
the current structure, tests of alternative structures
are independent of each other. Hence the tests can be
performed in parallel.

(Explorer-1)

a

:=

Figure 2 illustrates the parallel learning process with
two explorers and a dataset of four variables u, v, x
and y. Only a single-link search is performed for sim­
plicity. Manager starts with an empty current graph
in (a). It sends six alternative graphs in (b) through
(g) to explorer 1 and 2. Explorer 1 checks graphs in
(b), (c) and (d), selects the one in (b), and reports
to manager. Explorer 2 reports the one in (e) to man­
ager. After collecting the two graphs, manager chooses
the one in (b) as the new current graph. It then sends
graphs in (i) through (m). Repeating the above pro­
cess, manager finally gets the graph in (n) and sends
graphs in (o) and (p) to all explorers. Since none of
them decreases the cross entropy significantly, man­
ager chooses the graph in (n) as the final result and
terminates explorers.

tomanager .2-J :

fj)

tomanage
�

(n)

terminal signal�

M
�

v

�··· ··· · y
(0)

tennination

(<)

�

to manager
to manager

(m)
•

-,

I :!21: I
(p)

lenninalion

Figure 2: An example of parallel learning of

DMN.

Exploring Parallelism in Learning Belief Networks

3.2

LOAD BALANCING

3.2.1

3.2.2

The Need of Load Balancing

Balancing load among processors is critical to the ef­
ficiency of parallel learning. In Algorithm {Manager­
l), alternative graphs are evenly allocated to explor­
ers. However, the amount of computation in checking
each graph tends to switch between two extremes. If a
graph is non-chordal, it is ignored immediately with­
out having to compute the cross entropy decrement.
For example, suppose the current graph is shown in
Figure 3 (a). There are six graphs that differ from it
by only one link. If any of the dotted links in (b) is
added to (a), the resultant graph is non-chordal. Since
the complexity of checking chordality is O(INI +lEI),
where N is the number of variables and E is the
number of edges in the graph, the amount of com­
putation is very small. On the other hand, if any of
the dashed links in (c) is added to (a), the resultant
graph is chordal. Since the complexity of comput­
ing cross entropy decrement by local computation is
0( n + 17 ( 7J log 7J + 2'�)) (Xiang et a!. 1997), where n is
the number of cases the dataset and 17 is the maximum
size of the cliques involved, the amount of computa­
tion is much larger. As a result, even job allocation
may require significantly different amount of computa­
tion among explorers. As manager must collect reports
from all explorers before a decision on the new current
graph can be made, some explorers will be idle while
other explorers are completing their jobs.
u

(J (;:] n\/
..

l

· ···
'::<:�
·:::::·
::�:.
.............

{a)

y

X

--.:_:.-:·---- :.·::.:_--

y

w

(c)

(b)

Figure 3: Two types of alternative structures.
Figure 4 shows the time taken by each of the six ex­
plorers in a particular search step. Explorer 1 takes
much longer than others. This illustrates the needs
for more sophisticated job allocation strategy in order
to improve the efficiency of the parallel system.
13

t (s)

12
11
10
9
8
7
6
:s
4
3
2

N• -·
1L_�L_--�--�--�L_�L_--�-g�-·----2

3

4

5

6

Figure 4: The time needed for each explorer.

93

Two-stage Loading Method

To improve load balancing, we modify Algorithms
(Manager- I) and (Explorer- I) such that jobs are a1lo­
cated in two stages. In the first stage, manager parti­
tions alternative graphs evenly and distributes one set
to each explorer. Each explorer checks the chordal­
ity for each graph received and reports to manager
valid candidates (chordal graphs). Since the amount
of computation for checking chordality is small, this
stage can be completed quickly and the computation
among explorers tends to be even. In the second stage,
manager partitions all received graphs evenly and dis­
tributes one set to each explorer. Each explorer com­
putes cross entropy decrement for each graph received.
It then chooses the best graph c· and reports dh*
and G* to manager. Manager collects the reported
graphs, selects the best, and then starts the next pass
of the search. Since all graphs are chordal in the sec­
ond stage, the degree of load balance mainly depends
on the variability of the sizes of the largest cliques.
3.3

MARGINAL SERVER

During learning, each explorer needs to extract
marginal probabilities (marginals) for cliques from the
dataset. If each processor must extract marginals by
file access each time, the file system will become a bot­
tleneck. One alternative is to compress the dataset and
download one copy at each processor's local memory.
This allows us to handle a dataset up to about 500MB
in our parallel computer. However, when the size of
dataset further increases, more sophisticated methods
are needed.
According to the size of dataset, we partition the avail­
able processors into one manager, n explorers and m
marginal servers. Each server's task is to compute
marginals from the data stored in its local memory
based on the request of explorers. Servers are con­
nected logically into a pipeline indexed from 1 to m.
The dataset is partitioned into m + 1 sets. Each server
stores one distinct set in its local memory. The last set
is duplicated at each explorer's local memory. Algo­
rithms (Manager-1) and (Explorer-1) are modified into
Algorithms (Manager-2), (Explorer-2) and (Server).
The manger executes Algorithm (Manager-2). It per­
forms data distribution as mentioned above. It then
initializes an empty graph and starts the search. It
generates alternative graphs based on the current
graph, partitions into m + n sets and distributes one
set to each explorer and each server. It receives the
valid candidates from explorers and servers, partitions
them into n sets, and send one to each explorer. It
then collects the reported graphs from explorers, se­
lects the best, sends a signal to each server, and starts

Chu and Xiang

94

the next pass of search.
Algorithm

(Manager-2)
D over N variables,

Input: A dataset

a maximum size '1

'7('7- 1)/2

of clique, a maximum number"' ::;
of
lookahead links, the total num ber n of explorers,
the total numb er m of servers and a threshold oh.

begin
partition D into m + 1 set s , send one distinct set to
each server and broadcast the last set to explorers;
initialize an empty graph G = (N, E);
G':= G;
fori= 1 to "'• do
repeat
initialize the cross entropy decrement dh' := 0;
partition all graphs t hat differ from G by i links
into m + n sets;
send one set of graphs and G to each explorer
and each server ;
for each explorer and server, do
receive a set of valid graphs;
partition all received graphs into n sets;
send one set of graphs to each explorer;
for each explorer
receive dh" and G";
if dh" > dh' then dh' := dh", G' := G";
if dh' > oh, then a:= G', done:= false;
else done :::::: true;
send a signal to each server;
until done = true;
send a signal to each explorer and server;
return a;
end

Algorithm

Each server executes Algorithm (Server). It does the
same as an explorer for checking chordality. It then
processes requests from explorers until a signal is re­
ceived to start the next pass of search. After rec eiving
a set of nodes (variables) from an explorer, it computes
the sub-marginal using its local data, sums with the
sub-marginal from its preceding server, and passes the
sum to the next server or the requesting explorer.
Algorithm ( Ser v er )
begin

receive a set of data over a set
maximum size '7 of clique;

receive a set of data over a set
maximum size '7 of clique;

N of variables

receive G and a set of graphs from the manager;
do
for each received graph a'= (N, L U
if G' is chordal and L is implied by a single
clique of size $ t], then mark it valid;
send th e all valid candidates to manager;
repeat

E),

receive a set of nodes from an explorer;
compute the sub-marginal of the nodes received;
if the server is not server 1, then

receive the sub-marginal from its predecessor;
sum the su b- marginal;
if the server is not server m, then

send the sub-marginal to the next server;
else send the marginal to the req ues t ed explorer ;
until received signal;
end

Since each server has to serve n ex plorers , the pro­
cessing of each server must be n times as fast as an
explorer. This means nTm = Te, where Tm and Te are
the computing time of each marginal server and each
explorer, respectively. Let IDml (IDel) be the size of
local data at a s erv er (explorer). Tm and Te can be
expressed as Tm = kdiDm I and Te :::: kgN + kdiDel,
where kd and kg are coefficients, N is the total number
of variables in the domain, and k9N is the computa­
tion time to get the subgraph for computing the cross
entropy decrement. Therefore, we have

receive a and a set of graphs from the manager;
initialize dh" := 0 and G* := G;

(N, L U E), do
if G' is chordal and L is implied by a single
clique of size $ !'], then mark it valid;
send the valid candidates to manager;
receive a set of graphs from manager;
for each received graph a' = (N, L U E), do
for each clique involved in computing dh',
se nd the nodes of the clique to serve rs ;
compute marginal based on its local data;
receive marginal from the server m;
sum the two marginals up;
compute the entropy decrement dh' locally;
if dh' > dh", then dh* := dh', a• := G';
send dh" and a· t o manager;
until received signal;
for each r e ce iv ed graph a'=

Al gorithm

(Explorer-2).

of variables and a

until received signal;

and a

repeat

Each explorer executes

N

repeat

(Explorer-2)

begin

end

based on its local data, receives a sub-marginal from
the last server and sums them up. It then computes
the cross entropy decrement dh. Finally, it reports the
best graph c· to manager.

( 1)
Recall that

we parti tion the dataset D into

m+

1 sets,
(2)

It

checks chordality for each graph received and reports
to manager the chordal candidates. Next, it receives
a set of chordal gr aphs from manager. For each g r aph
received, it sends the nodes of each clique involved in
computing the cross entropy decrement dh to servers.
After sending a request, it computes a sub-marginal

Solving equation 1 and 2 and W' = m + n, where
W = W' + 1 is the total number of processors available,
we get
W'(aN + IDel)
n=
aN+IDI
W'(IDI-ID el)
m=
aN+IDI

Exploring Parallelism in Learning Belief Networks

N DI
IDm I = o: + I
W'
'

95

the root of the tree) and an explorer is Tmaz =
flog�2W+l)l 1. The maximum number of links be­
tween an explorer and a server 2Tmax. In general,
Tmax is smaller than Dma:r::· For example, if W = 25,
then Dmax = 8 but Tmaz = 3. Therefore, the best
topology is a ternary tree for our parallel learning al­
gorithms. Figure 5 illustrates such a configuration.
W hen there is no need for servers, all non-root proces­
sors are explorers.
-

where a is a coefficient presented by a = kg/ kd,
whose value is between 0.003 to 0.006 in our exper­
imental environment. Furthermore, IDe I has to sat­
isfy IDel ::::; Md, where Md is the maximum local
memory available to store the data. For example,
T. ::::: 0.024sec., Tm ::::: 0.005sec., and a ::::: 0.003 in
learning the ALARM network with n = 4, m = 4,
IDel = O.OBMB, and IDml::::: 0.04MB. If learning is
performed on a large domain with a very large dataset,
more marginal servers are needed. As an example,
suppose IDI = lOOMB, N = 1000, W' = 30 and
o: = 0.005.
If we choose IDel ::::20MB, we obtain
n = 7, m= 23, IDml::::: 3.48MB, where nand mare
rounded to the nearest integers.

Host
Computer

-------------

Explorers

EXPERIMENTAL RESULTS

4

Figure 5: A ternary tree configuration

4.1

CONFIGURATION

T he previous algorithms are implemented on an
ALEX AVX Series 2 parallel computer with a MIMD
distributed-memory architecture. It has 64 40M Hz
processors each with 32 MB local memory and can be
directly linked to at most four others. Communication
among processors are through message passing at 10
Mbps for simplex and 20 Mbps for duplex communi­
cation.
Message passing time increases as the number of links
between the communicating processors and the length
of the message. Table 2 shows the relation of the mes­
sage passing time with the message length and the
number of links between the communicating proces­
sors. It is important to link the processors such that
the number oflinks involved in each message passing is
minimized. Since each processor has up to four links
and communication is to be performed among man­
ager, explorers and servers, candidate topologies are a
2D mesh and a ternary tree.

4.2

RESULTS AND PERFORMANCE
ANALYSIS

Our experiments are intended to check the correctness
of the parallel algorithms and the speed up through
parallelism.
A network structure learned from a dataset generated
from the Alarm network (Beinlich et a!. 1989) is shown
in Figure 6. The result obtained using the parallel
algorithms is identical with that obtained using the
sequential algorithm.

Table 2: Message passing time
Length(bytes)

256

1link

O.QlS

2 links

0.016
0.017
0.021
0.030
0.051

3 links
7links

15 links
31 links

1024
0.016
0.020
0.022
0.032
0.057
0.105

4096
0.023
0.035
0.044
0.081
0.160
0.328

16384
0.096
0.129
0.125
0.165
0.241
0.409

In a 2D mesh with W processors, the maximum num­
ber of links between any two processors is Dmax =
2( vW 1). In a ternary tree with W processors,
the maximum number of links between manager (as
-

Figure 6: The learned Markov network: Alarm
We generated four control PI models and tested us­
ing the parallel algorithms with single-link and multi­
link lookahead search. The model PIMl has 26 vari­
ables and contains one PI sub-model of three variables.
PIM2 and PIM3 have 30 and 35 variables, respec­
tively. Each contains two PI sub-models each of which
has three variables. PIM4 has 16 variables and con­
tains a PI sub-model of four variables. The datasets

Chu and Xiang

96

of cases are generated by sampling these models with
20000, 25000, 30000 and 10000 cases, respectively.
For each dataset, our parallel algorithms were able to
learn an approximate I-map of the control model. The
network learned from PIM3 is shown in Figure 7.
The two subsets of variables involved in the two P I
sub-models are { x6, XB, xg } and {x14, Xts, Xi6} , respec­
tively. Using a single link lookahead search, the dashed
links (corresponding to the two P I sub-models) are
missing (hence not an I-map) in the learning outcome.
Using a triple-link lookahead search, the structure is
learned correctly.

!)!_,
0

I

4

3

16\ ,.-

11
1

19
20

25

26

network in 12800 seconds. The parallel program took
9260 seconds using 2 processors, 8512 seconds using 3
processors, and 8706 seconds using 4 processors. The
speed-up is very small as the number of processors
increases and the efficiency is very low. The speed-up
with four processors was even lower than with three
processors. This appears to be due to the bottleneck in
file access and the corresponding increase in overhead.
The result suggests that the file access method should
be avoided due to the extensive data access needed
during learning.
When the entire dataset can be loaded into the local
memory (about 20 MB is available in our environ­
ment) of each processor, loading the dataset to the
memory is performed once for all and each marginal
can be extracted directly from the memory. We refers
to this as the memory access method. We conducted a
comparison between even job allocation and two-stage
job allocation using the memory access method.
Table 3: Results on even and two-stage job allocation
n
1

Figure 7: The structure learned from a P I model P IM3
Next, we present the performance result. The perfor­
mance of a parallel program are commonly measured
by speed-up (S) and efficiency (E). Given a particular
task, let T(l) be the execution time of a sequential
program and T(W) be that of a parallel program with
W processors. The two measurements are defined as
S = T(l)/T(W) and E = SjW.
In practice, the performance of a parallel program is
affected by many factors. For our learning problem,
the size of the input data is not trivial. Hence how
the dataset is accessed by processors affects the per­
formance significantly. Our experiments were intended
to test the learning program using different data access
methods. We also tested different job allocation meth­
ods which also affect the performance significantly.
For efficient I/0 and storage, the original dataset of
cases are converted into a compressed frequency table
where the value of each variable is represented by one
byte. This file is used as the input to the learning
program. In the parallel computer we used, file access
by all processors must be performed through a special
processor. The simplest way for a learning program
to extract marginals from the data is to access the file
directly for each marginal. This file access method was
tested using a dataset of 10000 cases generated from
the ALARM network.
The sequential program (one processor) learned the

2
3
4
5
6
7
8
9
10
11
12

Even loading
tJmels)
::;
3160
1750
1201
957
764
712
604
558
528
486
467
454

1
1.81
2.63
3.30
4.14
4.44
5.23
5.66
5.98
6.50
6.77
6.96

t;
1

0.903
0.877
0.825
0.827
0.740
0.747
0.708
0.665
0.650
0.615
0.580

two-stage loading
t;
time ls)
::;
3160
1614
1090
830
686
578
525
471
448
410
381
378

1

1.95
2.86
3.72
4.50
5.19
5.92
6.69
7.31
8.04
8.43
9.00

1
0.977
0.952
0.929
0.900
0.865
0.845
0.837
0.813
0.804
0.766
0.750

The experiment used a dataset of 10000 cases gener­
ated from the ALARM network. The result is shown
in Table 3. Each row is the result obtained by using
n explorers as indicated in the first column. Columns
2 through 4 present the result obtained with even job
allocation and columns 5 through 7 present the result
obtained with two-stage job allocation.
Columns 3 and 6 show that as the number of explor­
ers increases, the speed-up increases as well with either
job allocation method. It demonstrates that our paral­
lel algorithm can effectively reduce the learning time.
This provides positive evidence that parallelism is an
alternative to tackle the computational complexity in
learning large belief networks.
Comparing column 3 with 6 and column 4 with 7,
it can be seen that the two-stage allocation further
speeds up the learning process and improves the effi­
ciency compared with even job allocation. For exam­
ple, when eight explorers are used, the speed-up is 5.66

97

Exploring Parallelism in Learning Belief Networks

and efficiency is 0.708 for even allocation, and 6.69 and
0.837 for two-stage allocation.

creased computational complexity in learning PI mod­
els.

The result also shows a gradual decrease in efficiency
as the number of explorers increases. This efficiency
decrease is mainly due to the job allocation overhead.
Manager must allocate job to each explorer sequen­
tially at the beginning of each search step. Therefore,
each explorer is idle after its report in the previous
search step is submitted and before the next job is
assigned to it.

When the size of dataset is beyond the available lo­
cal memory of each processor, we suggest the use of
marginal servers. Comparison of using different num­
ber of servers was performed in learning ALARM net­
work. Figure 8 shows the speed-up comparison and
Figure 9 shows the efficiency comparison. The verti­
cal axis is labelled by S for speed-up or E for efficiency.
The horizontal axis is labelled by W = m + n, where
m is the number of marginal servers and n is the num­
ber of explorers. De is the size of data stored in the
local memory of each explorer, and Dm is that of each
server. The speed up is calculated using sequential
learning with file access as this is considered the alter­
native when marginal server is not used.

Table 4: Results on learning PI
n
1

12

T, S, E
T(min.)
T(min.)
�

t;

24

36

PIMl
262.4
26.8
9.8
0.82

T(min.)

17.2

T(min.)

15.3
0.64
12.5
21.0
0.58

s
J::

s
E

PIM2
868.6
89.3
9.7
0.81
54.2
16.0
0.67
37.7
23.0
0.64

models

PIM3
3555.4
352.2
10.1
0.84
179.4
19.8
0.83
124.5
28.6
0.79

PIM4
36584
3382
10.8
0.90
1735
21.1
0.88
1197
30.6
0.85

20

s

18

•

16

m=4, De=2Dm

The second row shows the computation time in sequen­
tial learning. It increases from PIM 1 to PIM4. This
is because the increased size of the domain for PIM 1
through P IM 3 (26, 30, 3 5 variables) and the increased
size of the dataset (20000, 25000, 30000). PIM4 used
the most computation time because it has a PI sub­
model of 4 variables, which requires six-link lookahead
search.
For all models, the speed-up increases as more explor­
ers are employed. On the other hand, when more ex­
plorers are used, PIM1 has the fastest decrease in ef­
ficiency and PIM4 has the slowest decrease with the
other two models in-between. This is highly correlated
with the increase of computation time from PIM 1 to
PIM4. This is because as the search space becomes
larger, the number of alternative graphs to be explored
in each job allocation becomes larger. The conse­
quence is that the message passing overhead becomes
less significant compared with the search time and
hence the efficiency improves. This result shows that
parallel learning are quite suited for tackling the in-

�

o m= 6, De = 2 Dm

14

•
D

12

�

10

Table 4 lists the experimental result in learning the
four PI models mentioned above. Triple-link locka­
head search is used for learning PIM 1, PIM2 and
P IM3, respectively. Six-link lookahead search is used
for learning PIM4. The first column indicates the
number of explorers used. Each row shows computa­
tion time, speed up or efficiency as indicated by the
second column. Each of the last four columns shows
the result for learning one PI model.

0

D rn=4, De = Drn

8

�

D

9

10

0
•

•

0

D

II

12

0

•

6

0

•

4

w
5

6

7

8

Figure 8: Speed-up by using marginal servers

2.0

E

1.8
1.6

�

1.4

�

1.2
1.0
0.8

D
•

•
D

�
0

0
•
D

0

0

0.6

�
D

m=4, De= Dm

•

m=4, De= 2 Dm

0

0.4

0
•
D

m=6,De=2 Dm
w

5

6

7

8

9

10

II

12

Figure 9: Efficiency by using marginal servers
In the Figure 9, the maximum efficiency is 1.528 for
m
4, n = 3 and De = Dm, 1.574 form= 4, n = 4
and De = 2Dm, and 1.623 for m = 6, n = 6 and
De = 2Dm. The corresponding speed-up is 10.69,
12.59 and 19.48, respectively. The speed up is more
than the number of processors since marginal servers
allow much faster memory access compared with the
file access when a single processor is used.
=

98

5

Chu and Xiang

REMARKS

We have studied parallelism in learning to tackle the
increased computational complexity in learning belief
networks in difficult domains (PI models) as well as in
learning from large domains. Parallel algorithms were
proposed that decompose the learning task such that
multiple processors can be used without incurring ad­
ditional error. In order to improve the efficiency of the
parallel system, we proposed a two-stage job allocation
method to handle the variation in computation time
in searching different candidate networks. In order to
overcome the bottleneck by file access, we proposed
the parallel learning algorithm using marginal servers.
This allows fast memory access of data when the size
of the dataset is much larger than the local memory of
each processor.
The parallel learning algorithms are implemented on
an AVX Series 2 parallel computer with a MIMD
distributed-memory architecture. Our experimental
result showed that parallel learning can effectively
speed up learning PI models as well as learning non-PI
models in large domains.
Acknowledgements

This work is supported by grants OGP0155425,
CRD193296 from the Natural Sciences and Engineer­
ing Research Council of Canada, and by the Institute
for Robotics and Intelligent Systems in the Networks
of Centres of Excellence Program of Canada.


Real World

Current Bayesian net representations do not
consider structure in the domain and include
all variables in a homogeneous network. At
any time, a human reasoner in a large do­
main may direct his attention to only one of
a number of natural subdomains, i.e., there
is 'localization' of queries and evidence. In
such a case, propagating evidence through
a homogeneous network is inefficient since
the entire network has t o be updated each
time. This paper presents multiply sectioned
Bayesian networks that enable a (localization
preserving) representation of natural subdo­
mains by separate Bayesian subnets. The
subnets are transformed into a set of perma­
nent junction trees such that evidential rea­
soning takes place at only one of them at a
time. Probabilities obtained are identical to
those that would be obtained from the homo­
geneous network. We discuss attention shift
to a different junction tree and propagation of
previously acquired evidence. Although the
overall system can be large, computational
requirements are governed by the size of only
one junction tree.
1
1.1

Domain
Gather
Evidence

Evidence

&

Figure

Recommen-

dations

1:

An illustration of the context

a property of large domains with respect to human
cognitive activity.
There are fixed 'natural' subdomains. At
any time, a human reasoner focuses attention
at only one of them. He can acquire evidence
from and form queries about only one of them
at a time. He may shift attention from one
to another from time to time.

WHAT IS LOCALIZATION?

localization

t

System
User

System

LOCALIZATION

define informally what we will call

~t

Actions

Expert

We consider the following general context where an ex­
pert system is to be used (Figure 1): The human user
plays the central role between an expert system and a
real world domain. To know the state o f the domain
{e.g. diagnosis), the user gathers evidence from the
domain and enters the evidence along with queries to
the expert system. The system may provide a recom­
mendation or may prompt further gathering of infor­
mation. We would like the system to be efficient in
inference computation. How can this aim be realized
when the domain is large?
We

Queries

� t

as

Localization can be seen in many large domains. For
example, in many medical domains (e.g. neuromuscu­
lar diagnosis [Xiang et al. 19911) medical practitioners
acquire information about a patient by history taking,
physical examination, and performing a set of special­
ized tests. Each such activity involves a subdomain
which contains possibly dozens of alternative ques­
tions, procedures, test sites, etc., and diseases which
can be differentiated. Each such subdomain can hold
the person's attention for a period of time. During
this period, he updates his belief on disease hypothe­
ses based on acquired evidence, carefully weighs the

Exp l ori ng Localization

importance of alternative means to gather information
under the current situation, and selects the best alter­
native to perform next. Although one subdomain may
have an influence on another, the influence is summa­
rized by the common disease hypotheses which they
both can (partially) differentiate.
We distinguish 'interesting' variables from 'relevant'
variables. Call the set U of variables in a subdomain
'interesting' to the human reasoner if this subdomain
captures his current attention. A set V of variables
outside the current subdomain may have a bearing on
U due to two reasons. First, the background knowl­
edge on U may provide partial background knowledge
on V. Second, obtained evidence on U may change
one's belief on V. Thus V is 'relevant' to U but is
not currently 'interesting' to the reasoner. However,
we can often find a set I (I C U) of variables which
summarizes all the influence on U from V such that V
can be discarded from the reasoner's current attention.
This issue is treated more formally in Section 4.1.
1.2

IS LOCALIZATION USEFUL?

W hen localization exists, a large domain can be rep­
resented in an expert system according to the natu­
ral subdomains. Each subdomain is represented by a
subsystem. During a consultation session, only the
subsystem corresponding to the current subdomain
consumes computational resources. This subsystem is
called active. When a user's attention shifts to a differ­
ent subdomain, the evidence acquired in the previously
active subsystem can be absorbed by the newly ac­
tive subsystem through summarizing variables. Since
no computational resources are consumed by the in­
active subsystems, computational savings can be ob­
tained without loss of inference accuracy.
Our observation of localization was made based on our
experience in the domain of neuromuscular diagnosis
(ib.]. We believe that it is common in many large do­
mains. How to exploit it when it arises, how to con­
struct the above ideal representation in the context of
Bayesian networks, and how to guarantee the correct­
ness of inference in the representation is the subject of
this paper.
2

in Bayesian Networks for Large Expert Systems

halter 1988; Jensen, Lauritzen and Olesen 1990a;
Baker and Boult 1990; Suermondt, Cooper and Heck­
erman 1990).
This paper takes the exact approach. For general but
sparse nets, efficient computation has been achieved by
creating a secondary directed [Lauritzen and Spiegel­
halter 1988], or undirected clique tree (junction tree)
!Jensen, Lauritzen and Olesen 1990a) structure, which
also offers the advantage of trading compile time with
running time for expert systems. Both methods and
many others are based on a net representation which
does not consider domain structure and lumps all vari­
ables into a homogeneous network.
Pruning Bayesian nets with respect to each query in­
stance is another exact method with savings in com­
putational cost [Baker and Boult 1990). The method
does not support incremental evidence (i.e. all evi­
dence must be entered at one time).
Heckerman [1990b] partitions Bayesian nets into small
groups of naturally related variables to ease the con­
struction of large networks. But once the construction
is finished, the run time representation is still homo­
geneous.
Suermondt, Cooper and Heckerman (1990] combine
cutset conditioning with the clique tree method and
convert the original net into a set of clique trees to
obtain computational savings. The cutset is chosen
mainly based on net topology. It does not lead to the
exploration of localization in general.
2.2

'OBVIOUS' WAYS TO EXPLORE
LOCALIZATION

Splitting homogeneous nets

One obvious way to explore localization is to split a
homogeneous Bayesian net into a set of subnets ac­
cording to localization. Each subnet can then be used
as a separate computational object. This is not always
workable as is shown by the following example.

EXPLORE LOCALIZATION IN
BAYESIAN NETS

2.1

B ACKGROUND

Cooper (1990] has shown that probabilistic inference in
a general Bayesian net is NP-hard. Several different
approaches have been pursued to avoid combinatorial
explosion for typical cases, and thus to reduce compu­
tational cost. Two classes of approaches can be identi­
fied. One class explores approximation [Hention 1988;
Pearl 1988; Jensen and Andersen 1990]. Another class
explores specificity in computing exact probabilities
[Pearl 1986; Heckerman 1990a; Lauritzen and Spiegel-

Figure 2: Left: a DAG D. Right: A set of subnets
formed by sectioning D.
Suppose the directed acyclic graph (DAG) D in
Figure 2 is split according to localization into
{Dl, D2, D3}. Suppose variable G is instantiated by
evidence. According to d-separation [Pearl 1988], now

345

346

Xiang, Poole, and Beddoes

both paths between E and F are active. Therefore,
in order to pass a new piece of evidence on E to F,
the joint distribution on { B, C} needs to be passed
from {D1,D2} to D3 1• However, this is not possible
because neither D1 nor D2 contains this joint distribu­
tion. This shows that arbitrarily partitioning Bayesian
nets causes loss of information and is incorrect in gen­
eral.

Figure

3:

An unsectioned Bayesian net.

Splitting the junction tree

Another obvious way to explore localization is to pre­
serve localization within subtrees of a junction tree
[Jensen, Lauritzen and Olesen 1990a] by clever choice
in triangulation and junction tree construction. If this
can be done, the junction tree can be split and each
subtree can be used as a separate computational ob­
ject. The following example shows that this is also not
always workable. Consider the DAG 0 in Figure 3.
Suppose variables in the DAG form three naturally
related groups which satisfy localization:
Gr
G2
Ga

=
=

=

{Ar,A2,Aa,Hr,H2,Ha,H4}
{FI,F2,HI,H2}
{E1,E2, £3,H2, H3,Ho�,}

We would like to construct a junction tree which would
preserve the localization within three subtrees. The
graph <pin Figure 4 is the moral graph of 0. Only the
cycle A3 - H3 - E3 E1 - H4 - A3 needs to be trian­
gulated. There are six distinct ways of triangulation
out of which only two do not mix nodes in different
groups. The two triangulations have the link (Ha,H4)
in common but they do ;wt make a significant differ­
ence in the following analysis. The graph A in Figure 4
shows one of the two triangulations. All the cliques in
A appear as nodes of graph r.
-

The junction tree r does not preserve localization since
cliques 3, 4, 5 and 8 correspond to group G1 but are
connected via cliques 6 and 7 which contain Ea from
group Ga. This is unavoidable. When there is evi­
dence for A1 or A2 in A, updating the belief in group
G3 requires passing the joint distribution of H2 and
1 Passing only the marginal distributions on B and on
C is not correct.

r

S-�
Figure 4: .P: the moral graph of 0 in Figure 3. A:
a triangulated graph of ci>. r: the junction tree con­
structed from A.
H3• But updating the belief in Aa only requires pass­
ing the marginal distribution of H3. That is to say,
updating the belief in A3 needs less information than
group G3• In the junction tree representation, this be­
comes a path from cliques 3, 4 and 5 to clique 8 via
cliques 6 and 7.

In general, let X and Y be two sets of variables in the
same natural group, and let Z be a set of variables in
a distinct group. Suppose the information exchange
between pairs of them requires the exchange of distri­
bution on sets lxy, lxz and lyz of variables respec­
tively. Sometime lxy is a subset of both lxz and ]yz.
When this is the case, a junction tree representation
will always indirectly connect cliques corresponding to
X and Y through cliques corresponding to Z if the
method in Jensen, Lauritzen and Olesen [ 1990a] is fol­
lowed.
A brute force method

There is, however, a way around the problem with a
brute force method. In the above example, when there
is evidence for A1 or A2, the brute force method pre­
tends that updating the belief in A3 needs as much
information as Ga. What one does is to add a dummy
link (H2,A3) to the moral graph <I> in Figure 4. Then

Exploring Localization in Bayesian Networks for Large Expert Systems

triangulating the augmented graph gives the graph A'
in Figure 5. The resultant junction tree r' in Fig­
ure 5 does have three subtrees Which correspond to
the three groups desired. However, the largest cliques
now have size four instead of three as before. In the bi­
nary case, the size of the total state space is 84 instead
of 76 as before.
In general, the brute force method preserves natural lo­
calization by congregation of a set of interfacing nodes
(nodes H2, H3, H4 above) between natural groups. In
this way, the joint distribution on interfacing nodes
can be passed between groups, and preservation of lo­
calization and preservation of tree structure can be
compatible. However, in a large application domain
with the original network sparse, this will greatly in­
crease the amount of computation in each group due to
the exponential enlargement of the clique state space.
The required increase of computation could outweigh
the savings gained by exploring localization in general.

3

MULTIPLY SECTIONED
BAYESIAN NETS

This section introduces a knowledge representation
formalism, Multiply Sectioned Bayesian Networks
(MSBNs), as our solution to explore localization.
We want to partition a large domain according to nat­
ural localization into subdomains such that each can
be represented separately by a Bayesian subnet. Each
subnet then stands as a computational object, and dif­
ferent subnets cooperate with each other during atten­
tion shift by exchanging a small amount of information
between them. We call such a set of subnets a MSBN.
The construction of a MSBN can be formulated con­
ceptually in the opposite direction. Suppose the do­
main has been represented with a homogeneous net­
work. We frequently refer to a homogeneous net as an
UnSectioned Bayesian network (USBN). A MSBN is a
set of Bayesian subnets resulted from the sectioning of
the corresponding USBN.
For example the DAG 0 in Figure 3 is sectioned into
{81,82, 03} in Figure 6 according to the localization
described in Section 2.2. A variable shared by 'ad­
jacent' subnets appears in both subnets. The set of
shared variables is subject to a technical constraint,
in addition to localization, as will be discussed in Sec­
tion 4.
,

Figure 5: A' is a triangulated graph. r' is a junction
tree of A'.
The trouble illustrated in the above two situations can
be traced to the tree structure of a junction tree repre­
sentation which requites a single path between any two
cliques in the tree. In the normal triangulation case,
one has small cliques but one loses localization. In
the brute force case, one preserves localization but one
does not have small cliques. To summarize, the preser­
vation of natural localization and small cliques can not
coexist by the method of Andersen et al. [1989J and
Jensen, Lauritzen and Olesen [ 1990a]. It is claimed
here that this is due to a single information channel
between local groups of variables. This paper present
a representation which, by introducing multiple infor­
mation channels between groups and by exploring con­
ditional independence, allows passing the joint distri­
bution on a set of interfacing variables b e tween groups
by passing only marginal distributions on subsets of
the set.

Figure 6: The set {81, 82, 03} forms a MSBN for the
USBN 0 in Figure 3.

3.1

TRANSFORMATION OF MSBNS
INTO JUNCTION FORES TS

The junction tree representation [Andersen et al. 1989;
Jensen, Lauritzen and Olesen 1990a] allows efficient
computation for general but sparse networks in expert
systems. Thus it is desirable to transform each subnet
of a MSBN into a junction tree. The resultant set of
junction trees is called a junction forest.
For example, the MSBN {8\ 82, 03} in Fi�ure 6 is
transformed into the junction forest {r1' r 'r3} in
Figure 7. Omit the ribbed bands for the moment which
will be introduced shortly.
In order to propagate evidence between junction trees
during attention shift, information channeJs need to

be created between them. As discussed in Section 2.2,
multiple channels are required in general to preserve

347

348

Xiang, Poole, and Beddoes

probabilities in this tree are the same as in a globally
consistent junction forest [Xiang, Poole and Beddoes
1992]. It is this feature of MSBNsfjunction forests
that allows the exploitation of localization.

Figure 7: The set {f1, rz, f3} is a junction forest
transformed from the MSBN {81, 82, e3} in Figure 6.
Linkages between junction trees are shown by ribbed
bands.
both localization and a small clique size. These chan­
nels are called linkages between the two junction trees
being connected. Each linkage connects two cliques in
different trees. The two cliques being connected are
called the host cliques of the linkage. A linkage is the
intersection of its two host cliques. Host cliques are
selected such that the union of linkages is the set of in­
terfacing variables between the corresponding subnets,
and each linkage is maximal. With multiple linkages
created between junction trees, we have a linked junc­
tion forest.

For example, in Figure 7, linkages between junction
trees are indicated with ribbed bands connecting the
corresponding host cliques. There is only one linkage
between clique 1 of f1 and clique 2 of f2' namely,
{H1,H2}. The two linkages between f1 and f3 are
{H2, H3} and {H3, H4}.
Up to here, we have only discussed the manipula­
tions of graphical structures of a MSBN. As other
approaches based on secondary structures [Lauritzen
and Spie&elhalter 1988; Jensen, Lauritzen and Ole­
sen 1990aj, there needs to be a corresponding conver­
sion from the probability distribution of the MSBN to
the belief table (potential) of the linked junction for­
est. Readers are refened to Xiang, Poole and Beddoes
[1992] for details regarding the conversion.
3.2

EVIDENTIAL REASONING

Afte1· a l inked junction fores t is created, it becomes the
permanent representation of the corresponding MSBN.
The evidential reasoning during a consultation session
will be performed solely in the junction forest.
Due to localization, only one junction tree in a junc­
tion forest is active during evidential reasoning. When
new evidence becomes available to the currently active
junction tree, it is entered and the tree is made consis­
tent. The operations to enter evidence and to main­
tain consistency within a junction tree are the same as
Jensen, Lauritzen and Olesen [1990a]. We only main­
tain consistency in the currently active tree. All the

When the user shifts attention from the currently ac­
tive tree to a 'destination' tree, all previously acquired
evidence is absorbed through an operation ShiftAt­
tention. The operation swaps in and out sequentially
a chain of 'intermediate' junction trees between the
currently active tree and the destination tree. It has
been shown (ib.} that, with a properly structured junc­
tion forest, the following is true.
Start with any active junction tree in a
globally consistent junction forest. Repeat
the following cycle a finite number of times:
1. Enter evidence to the currently active
tree and make the tree consistent a fi­
nite number of times.
2. Use ShiftAttention to shift attention
to any destination tree.
The marginal distributions obtained in
the final active tree are identical to those of
a globally consistent forest.
The above property shows the most important charac­
terization of MSBNs and junction forests, namely, the
capability of exploiting localization to reduce the com­
putational cost. Note that the above statement only
requires the initial global consistency of the junction
forest.
With localization, the user's interest and new evidence
remain in the sphere of one junction tree for a pe­
riod of time. Thus the time and space requirement,
while reasoning within a junction tree, is bounded
above by what is required by the largest junction tree.
The judgments obtained take into account all the rele­
vant background knowledge and evidence. Compared
to the USBN and the single junction tree representa­
tion where each piece of evidence has to be propagated
through the entire system, this leads to computational
savings.
When the user shifts interest to another set of vari­
ables contained in a different destination tree, only
the intermediate trees need to be updated. The time
required is linear to the number of intermediate trees
and to the number of linkages between each pair of
neighbours [ib.]. No matter how large the entire junc­
tion forest, the time requirement for attention shift
is fixed once the destination tree and mediating trees
are fixed. The space requirement is upper bounded
by what is needed by the largest junction tree. With
localization, the computational cost for attention shift
is incurred only occasionally.
Given the above analysis, the computational complex­
ity of evidential reasoning in a MSBN with jJ subnets
of equal size is about 1//3 of the corresponding USBN
system given localization. The actual time require-

Exploring Localization in Bayesian Networks for Large Expert Systems

ment is a little more than 1/ f3 due to the computation
required for attention shift. The actual space require­
ment is a little more than 1 / f3 due to the repetition of
interfacing nodes.

Section 2.2 has shown that we cannot divide a homo­
geneous Bayesian net or its junction tree arbitrarily
in order to explore localization. This section discusses
major technical issues in the MSBN/junction forest
representation.

For example, the sectioning in Figure 2 satisfies the
d-sepset condition, but the resultant MSBN does not
guarantee correct inference. This is because the sec­
tioning has an unsound overall organization of subnets.
Intuitively, the overall structure of a MSBN should en­
sure that evidence acquired in any subnet be able to
propagate to a different subnet by a unique chain of
subnets. In the example of Figure 2, after a piece of
evidence is available on G, a new piece of evidence on
E has to propagate to F through two different chains
of subnets D2 - D3 and D2 - D1 - D3• This violates
the above requirement and causes the problem. The
issue of overall structure is treated formally in (ib.J.

4.1

4.3

4

TECHNICAL ISSUES

INTERFACE BETWEEN S UBNETS

Localization does not dictate exactly what should be
the boundary between different subnets. The intuitive
criterion is that the interface should allow evidence
acquired to be propagated to adjacent subnets dur­
ing attention shift by a small amount of information
exchange. We define d-sepset as the criterion of inter­
face, which makes use of Pearl's d-separation concept
[Pearll988]. We denote the union D of DAGs D1 and
D2 by D = D1 UD2 = (N1 uN2,E1 u E2).
Definition 4.1 (d-sepset) Let D = D1 U D2 be a
DAG. The set of nodes I = N1 n N2 is a d-sepset
between subDAG D1 and D2 if the following condition
holds.
For every A; E I with its parents 1!"; in
either 1!"; <;:;; N1, or 1!"; <;:;; N2•

D,

Elements of a d-sepset are called d-sepnodes. When
the above condition holds, D is said to be sectioned
into {Dl,D2}.

The following theorem and corollary [Xiang, Poole and
Beddoes 1992)2 say that a d-sepset d-separates subnets
in a MSBN and is a sufficient information channel.

MORAL I-TRIANGULATION BY
LOCAL COMPUTATION

Transformation of a MSBN into a junction forest
requires moralization and triangulation conceptually
the same way as the other approaches based on sec­
ondary structures [Lauritzen and Spiegelhalter 1988;
Andersen et al. 1989; Jensen, Lauritzen and Olesen
1 990a.}. However, in the MSBN context, the transfor­
mation can be performed globally or by local compu­
tation at the level of the subnets. The global compu­
tation performs moralization and triangulation in the
same way as the other approaches with care not to mix
the nodes of distinct subnets into one clique. An addi­
tional mapping of the resultant moralized and triangu­
lated graph into subgraphs corresponding to the sub­
nets is needed. But where space saving is concerned,
local computation is desired.
Since the number of parents for a d-sepnode may
be different for different subnets, the moralization in
MSBN cannot be achieved by 'pure' local computation
in each subnet. Communication between the subnets
is required to ensure that the parents of d-sepnodes
are moralized identically in different subnets.

The d-sepset criterion concerns with the interface be­
tween each pair of subnets. This is not sufficient for a
workable MSBN.

The criterion of triangulation in a MSBN is to en­
sure the 'int ac tness ' of a resulting hypergra.ph f rom
the corresponding homogeneous net. Problems arise
if one insists on triangulation by local computation at
the level of subnets. One problem is that an inter­
subnet cycle will be triangulated in the homogeneous
net, but the cycle cannot be identified by examining
each of the subnets involved individually. Another
problem is that cycles involving d-sepnodes may be
triangulated differently in different subnets. The so­
lution is to let the subnets communicate during tri­
angulation. Since moralization and triangulation both
involve adding links and both require communication
between subnets, the corresponding local operations in
each subnet can be performed together and messages
to other subnets can be sent together. Therefore, oper­
ationally, moralization and triangulation in MSBN are
not separate steps as in the single junction tree repre­
sentation. The corresponding integrated operation is
t ermed morali-triangulation to reflect this fac t.

2They are simplified here to the MSBN of two subnets.

For example, the MSBN in Figure 6 is morali-

Theorem 4.2 Let a DAG
{ D1, D2} and I = N1 nN2 be
N1 \ I from N2 \ I.

a

D be sectioned into
d-sepset. I d-separates

Co r ollary 4.3 Let (D, P) be a Bayesian net, D be
sectioned into { D1, D2}, and I = N1 n N2 be the d­
sepset. When evidence is available at variables in N1,
the propagation of the joint distribution on I from D1
to D 2 is sufficient in order to obtain posterior distri­
bution on N2•
4.2

OVERALL S TRUCTURE OF MSBNS

349

350

Xiang, Poole,

and Beddoes

Figure 8: Morali-triangulated graphs of the MSBN in Figure 6. The meaning of the different line-types is
explained in Section 4.3.
triangulated to the graphs in Figure 8. Thin solid
lines ( e.g. (A1, HI)) are from the original arcs by drop­
ping directions. Thin dotted lines (e .g. ( A1, A2)) are
links added by local moralization. Thick dotted lines
(e.g. (H1,H2)) are 'moral' links added through com­
munication. Thin dashed lines (e.g. (H3,H4)) are
added through communication for triangulation. The
thick solid line ((Ea, H4)) is added by local triangula­
tion. A formal treatment and an algorithm for morali­
triangulation are given in Xiang, Poole and Beddoes
[1992].
4.4

PROPAGATING INFORMATION
THROUGH MULTIPLE L INKAGES

Propagating information between junction trees of a
junction forest is required in two different situations:
belief initialization and evidential reasoning. In both
cases, information needs to be propagated between
junction trees of a junction forest through multiple
linkages. Care is to be taken against potential errors.

Belief initialization serves the same purpose as in other
approaches based on secondary structures [Lauritzen
and S iegelhalter 1988; Jensen, Lauritzen and Olesen
1990a . It establishes the global consistency before any
evidence is available. This requires the propagation
of knowledge stored in each junction tree to the rest
of the forest. When doing so, redundant information
could be passed through multiple linkages. We must
make sure that the information is passed only once.

f

In evidential reasoning, evidence acquired in one junc­
tion tree needs to be propagated to the destination
tree during attention shift. The potential error in this
case takes a different form from the case of initializa­
tion. Passing information through multiple linkages
from one junction tree to another can 'confuse' there­
ceiving tree such that the correct consistency between

the two cannot be established.
Detailed illustrations of these potential problems and
the operations which avoid them are given in Xiang,
Poole and Beddoes [1992].
5

CONCLUSION

This paper overviews MSBNs and junction forests as
a flexible knowledge representation and as an efficient
inference formalism. This formalism is suitable for ex­
pert systems which reason about uncertain knowledge
in large domains where localization exists.
MSBNs allow partitioning of a large domain into
smaller natural subdomains such that each of them can
be represented as a Bayesian subnet, and can be tested
and refined individually. This makes the representa­
tion of a complex domain easier for knowledge engi­
neers and may make the resultant system more natural
and more understandable to system users. The mod­
ularity facilitates implementation of large systems in
an incremental fashion. When partitioning, a knowl­
edge engineer has to take into account the technical
constraints imposed by MSBNs which are not very re­
strictive.
Each subnet in the MSBN is transformed into a junc­
tion tree such that the MSBN is transformed into a
junction forest where evide ntial reasoning takes place.
Each subnet/junction tree in the MSBN /junction for­
est stands as a separate computational object. Since
the representation allows transformation by local com­
putation at the level of subnets, and allows reason­
ing to be conducted with junction trees, the space re­
quirement is governed by the size of the largest sub­
net/junction tree. Hence large applications can be
built and run on relatively small computers wherever
hardware resources are of concern. This was, in fact,

Exploring Localization in Bayesian Networks for Large Expert Systems

our original motivation for developing the MSBN rep·
resentation.
During a consultation session, the MSBN representa·
tion allows only the 'interesting' junction tree to be
loaded while the rest of the forest remains inactive and
uses no computational resources. The judgments made
on variables in the active tree are consistent with all
the knowledge available, including both prior knowl­
edge and all the evidence contained in the entire forest.
When the user's attention shifts, inactive trees can be
made active and previous accumulation of evidence is
preserved. This is achieved by passing the joint beliefs
on d-sepsets. The overall computational resources re­
quired are governed by the size of the largest subnet,
and not by the size of the application domain.
The MSBN has been applied to an expert system
PAINULIM for diagnosis of neuromuscular diseases
characterized
a painful or impaired upper limb [Xi­
ang et al. 1991 .

br.

The MSBN representation makes the localization as­
sumption about the large domain being represented.
Our justification of the generality of localization has
been intuitive and has been based on our experience
in PAINULIM. We are prepared to test its generality
in other large domains.
Acknowledgements

This work is supported by Operating Grants A3290,
OGP0044121 and OGP0090307 from NSERC, and
CRD3474 from the Centre for Systems Science at SFU.
We are grateful to Stefan Joseph and anonymous re­
viewers for helpful comments to an earlier draft.

