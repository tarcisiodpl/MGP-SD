
Poker is a challenging problem for artificial intelligence, with non-deterministic dynamics, partial observability, and the added difficulty of unknown adversaries. Modelling all of the uncertainties in this domain is not an easy task. In
this paper we present a Bayesian probabilistic
model for a broad class of poker games, separating the uncertainty in the game dynamics
from the uncertainty of the opponent’s strategy.
We then describe approaches to two key subproblems: (i) inferring a posterior over opponent
strategies given a prior distribution and observations of their play, and (ii) playing an appropriate
response to that distribution. We demonstrate the
overall approach on a reduced version of poker
using Dirichlet priors and then on the full game
of Texas hold’em using a more informed prior.
We demonstrate methods for playing effective responses to the opponent, based on the posterior.

1 Introduction
The game of poker presents a serious challenge to artificial intelligence research. Uncertainty in the game stems
from partial information, unknown opponents, and game
dynamics dictated by a shuffled deck. Add to this the large
space of possible game situations in real poker games such
as Texas hold’em, and the problem becomes very difficult
indeed. Among the more successful approaches to playing poker is the game theoretic approach, approximating
a Nash equilibrium of the game via linear programming
[5, 1]. Even when such approximations are good, Nash
solutions represent a pessimistic viewpoint in which we
face an optimal opponent. Human players, and even the
best computer players, are certainly not optimal, having
idiosyncratic weaknesses that can be exploited to obtain
higher payoffs than the Nash value of the game. Opponent
modelling attempts to capture these weaknesses so they can

be exploited in subsequent play.
Existing approaches to opponent modelling have employed
a variety of approaches including reinforcement learning
[4], neural networks [2], and frequentist statistics [3]. Additionally, earlier work on using Bayesian models for poker
[6] attempted to classify the opponent’s hand into one of a
variety of broad hand classes. They did not model uncertainty in the opponent’s strategy, using instead an explicit
strategy representation. The strategy was updated based
on empirical frequencies of play, but they reported little
improvement due to this updating. We present a general
Bayesian probabilistic model for hold ’em poker games,
completely modelling the uncertainty in the game and the
opponent.
We start by describing hold’em style poker games in general terms, and then give detailed descriptions of the casino
game Texas hold’em along with a simplified research game
called Leduc hold’em for which game theoretic results are
known. We formally define our probabilistic model and
show how the posterior over opponent strategies can be
computed from observations of play. Using this posterior
to exploit the opponent is non-trivial and we discuss three
different approaches for computing a response. We have
implemented the posterior and response computations in
both Texas and Leduc hold’em, using two different classes
of priors: independent Dirichlet and an informed prior provided by an expert. We show results on the performance of
these Bayesian methods, demonstrating that they are capable of quickly learning enough to exploit an opponent.

2 Poker
There are many variants of poker.1 We will focus on hold
’em, particularly the heads-up limit game (i.e., two players
with pre-specified bet and raise amounts). A single hand
consists of a number of rounds. In the first round, players are dealt a fixed number of private cards. In all rounds,
1
A more thorough introduction of the rules of poker can be
found in [2].

1
r

c

2
f

r

2
c

r

1
f

c

1
c

f

r

c

2
f

c

Figure 1: An example decision tree for a single betting
round in poker with a two-bet maximum. Leaf nodes with
open boxes continue to the next round, while closed boxes
end the hand.
some fixed number (possibly zero) of shared, public board
cards are revealed. The dealing and/or revealing of cards is
followed by betting. The betting involves alternating decisions, where each player can either fold (f), call (c), or raise
(r). If a player folds, the hand ends and the other player
wins the pot. If a player calls, they place into the pot an
amount to match what the other player has already placed
in the pot (possibly nothing). If a player raises, they match
the other player’s total and then put in an additional fixed
amount. The players alternate until a player folds, ending
the hand, or a player calls (as long as the call is not the first
action of the round), continuing the hand to the next round.
There is a limit on the number of raises (or bets) per round,
so the betting sequence has a finite length. An example
decision tree for a single round of betting with a two-bet
maximum is shown in Figure 1. Since folding when both
players have equal money in the pot is dominated by the
call action, we do not include this action in the tree. If
neither player folds before the final betting round is over,
a showdown occurs. The players reveal their private cards
and the player who can make the strongest poker hand with
a combination of their private cards and the public board
cards wins the pot.
Many games can be constructed with this simple format
for both analysis (e.g., Kuhn poker [7] and Rhode Island
hold’em [9]) and human play. We focus on the commonly
played variant, Texas hold ’em, along with a simplified and
more tractable game we constructed called Leduc hold ’em.
Texas Hold ’Em. The most common format for hold ’em
is “Texas Hold’em”, which is used to determine the human
world champion and is widely considered the most strategically complex variant. A standard 52-card deck is used.
There are four betting rounds. In the first round, the players
are dealt two private cards. In the second round (or flop),
three board cards are revealed. In the third round (turn)

and fourth round (river), a single board card is revealed.
We use a four-bet maximum, with fixed raise amounts of
10 units in the first two rounds and 20 units in the final two
rounds. Finally, blind bets are used to start the first round.
The first player begins the hand with 5 units in the pot and
the second player with 10 units.
Leduc Hold ’Em. We have also constructed a smaller
version of hold ’em, which seeks to retain the strategic elements of the large game while keeping the size of the game
tractable. In Leduc hold ’em, the deck consists of two suits
with three cards in each suit. There are two rounds. In the
first round a single private card is dealt to each player. In
the second round a single board card is revealed. There is
a two-bet maximum, with raise amounts of 2 and 4 in the
first and second round, respectively. Both players start the
first round with 1 already in the pot.
Challenges. The challenges introduced by poker are
many. The game involves a number of forms of uncertainty,
including stochastic dynamics from a shuffled deck, imperfect information due to the opponent’s private cards, and,
finally, an unknown opponent. These uncertainties are individually difficult and together the difficulties only escalate.
A related challenge is the problem of folded hands, which
amount to partial observations of the opponent’s decisionmaking contexts. This has created serious problems for
some opponent modelling approaches and our Bayesian approach will shed some light on the additional challenge that
fold data imposes. A third key challenge is the high variance of payoffs, also known as luck. This makes it difficult
for a program to even assess its performance over short periods of time. To aggravate this difficulty, play against human opponents is necessarily limited. If no more than two
or three hundred hands are to be played in total, opponent
modelling must be effective using only very small amounts
of data. Finally, Texas hold’em is a very large game. It has
on the order of 1018 states [1], which makes even straightforward calculations, such as best response, non-trivial.

3 Modelling the Opponent
We will now describe our probabilistic model for poker. In
all of the following discussion, we will assume that Player 1
(P1) is modelling its opponent, Player 2 (P2), and that all
incomplete observations due to folding are from P1’s perspective.
3.1 Strategies
In game theoretic terms, a player makes decisions at information sets. In poker, information sets consist of the
actions taken by all players so far, the public cards revealed
so far, and the player’s own private cards. A behaviour
strategy specifies a distribution over the possible actions

for every information set of that player. Leaving aside the
precise form of these distributions for now, we denote P1’s
complete strategy by α and P2’s by β.
We make the following simplifying assumptions regarding
the player strategies. First, P2’s strategy is stationary. This
is an unrealistic assumption but modelling stationary opponents in full-scale poker is still an open problem. Even the
most successful approaches make the same assumption or
use simple methods such as decaying histories to accommodate opponent drift. However, we believe this framework can be naturally extended to dynamic opponents by
constructing priors that explicitly model changes in opponent strategy. The second assumption is that the players’ strategies are independent. More formally, P (α, β) =
P (α)P (β). This assumption, implied by the stationarity,
is also unrealistic. Hower, modelling opponents that learn,
and effectively deceiving them, is a difficult task even in
very small games and we defer such efforts until we are
sure of effective stationary opponent modelling. Finally,
we assume the deck is uniformly distributed, i.e., the game
is fair. These assumptions imply that all hands are i.i.d.
given the strategies of the players.

3.3 Probability of Observations
Suppose a hand is fully observed, i.e., a showdown occurs.
The probability of a particular showdown hand Hs occurring given the opponent’s strategy is, 2

P (Hs |β)
= P (C, D, R1:k , A1:k , B1:k |β)
= P (D|C)P (C)

k
Y


i=1

= P (D|C)P (C)

k
Y


i=1

= pshowcards

k
Y

P (Bi |D, R1:i , A1:i , B1:i−1 , β)
P (Ai |C, R1:i , A1:i−1, B1:i−1 )
P (Ri |C, D, R1:i−1 )
αYi ,C,Ai βZi ,D,Bi 
P (Ri |C, D, R1:i−1 )

αYi ,C,Ai βZi ,D,Bi

i=1

∝

k
Y

βZi ,D,Bi ,

i=1

3.2 Hands
The following notation is used for hand information. We
consider a hand, H, with k decisions by each player. Each
hand, as observed by an oracle with perfect information, is
a tuple H = (C, D, R1:k , A1:k , B1:k ) where,
• C and D denote P1 and P2’s private cards,

where for notational convenience, we separate the information sets for P1 (P2) into its public part Yi (Zi ) and its
private part C (D). So,

Yi = (R1:i , A1:i−1 , B1:i−1 )
Zi = (R1:i , A1:i , B1:i−1 ).

• Ri is the set (possibly empty) of public cards dealt
before either player makes their ith decision, and
• Ai and Bi denote P1 and P2’s ith decisions (fold, call
or raise).
We can model any limit hold’em style poker with these
variables. A hand runs to at most k decisions. The fact
that particular hands may have fewer real decisions (e.g., a
player may call and end the current betting round, or fold
and end the hand) can be handled by padding the decisions
with specific values (e.g., once a player has folded all subsequent decisions by both players are assumed to be folds).
Probabilities in the players’ strategies for these padding decisions are forced to 1. Furthermore, the public cards for a
decision point (Ri ) can be the empty set, so that multiple
decisions constituting a single betting round can occur between revealed public cards. These special cases are quite
straightforward and allow us to model the variable length
hands found in real games with fixed length tuples.

In addition, αYi ,C,Ai is the probability of taking action Ai
in the information set (Yi , C), dictated by P1’s strategy,
α. A similar interpretation applies to the subscripted β.
pshowcards is a constant that depends only on the number of
cards dealt to players and the number of public cards revealed. This simplification is possible because the deck
has uniform distribution and the number of cards revealed
is the same for all showdowns. Notice that the final unnormalized probability depends only on β.
Now consider a hand where either player folds. In this
case, we do not observe P2’s private cards, D. We must
marginalize away this hidden variable by summing over all
possible sets of cards P2 could hold.

2
Strictly speaking, this should be P (H|α, β) but we drop the
conditioning on α here and elsewhere to simplify the notation.

The probability of a particular fold hand Hf occurring is,
P (Hf |β)
= P (C, R1:k , A1:k , B1:k |β)
X

k
Y


P (Bi |D, R1:i , A1:i , B1:i−1 , β)
i=1 P (Ai |C, R1:i , A1:i−1 , B1:i−1 )
D

P (Ri |C, D, R1:i−1 )
" k
#
k
Y
XY
= pfoldcards (Hf )
βZi ,D0 ,Bi
αYi ,C,Ai

= P (C)

P (D|C)

D 0 i=1

i=1

∝

k
XY

βZi ,D0 ,Bi

We start with a simple objective,
αBBR = argmax EH|O V (H)
α
X
= argmax
V (H)P (H|O, α)
α

= argmax
α

= argmax
α

D 0 i=1

3.4 Posterior Distribution Over Opponent Strategies
Given a set O = Os ∪ Of of observations, where Os are the
observations of hands that led to showdowns and Of are the
observations of hands that led to folds, we wish to compute
the posterior distribution over the space of opponent strategies. A simple application of Bayes’ rule gives us,
P (O|β)P (β)
P (O)
Y
P (β) Y
=
P (Hs |β)
P (Hf |β)
P (O)
Hs ∈Os
Hf ∈Of
Y
Y
P (Hs |β)
P (Hf |β)
∝ P (β)

P (β|O) =

Hs ∈Os

Hf ∈Of

4 Responding to the Opponent
Given a posterior distribution over the opponent’s strategy
space, the question of how to compute an appropriate response remains. We present several options with varying
computational burdens. In all cases we compute a response
at the beginning of the hand and play it for the entire hand.
4.1 Bayesian Best Response
The fully Bayesian answer to this question is to compute
the best response to the entire distribution. We will call this
the Bayesian Best Response (BBR). The objective here is
to maximize the expected value over all possible hands and
opponent strategies, given our past observations of hands.

X

V (H)

H∈H

X

V (H)

X

V (H)

H∈H

= argmax
α

where D0 are sets of cards that P2 could hold given the
observed C and R (i.e., all sets D that do not intersect with
C ∪ R), and pfoldcards (Hf ) is a function that depends only on
the number of cards dealt to the players and the number of
public cards revealed before the hand ended. It does not
depend on the specific cards dealt or the players’ strategies.
Again, the unnormalized probability depends only on β.

H∈H

H∈H

Z
Z

Z

P (H|α, β, O)P (β|O)
β

P (H|α, β, O)P (O|β)P (β)
β

β



P (H|α, β, O)P (β)
k
Y Y
βZi ,D,Bi
Hs ∈Os i=1

k
Y XY

βZi ,D,Bi

Hf ∈Of D 0 i=1



where H is the set of all possible perfectly observed hands
(in effect, the set of all hands that could be played). Although not immediately obvious from the equation above,
one algorithm for computing Bayesian best response is a
form of Expectimax [8], which we will now describe.
Begin by constructing the tree of possible observations in
the order they would be observed by P1, including P1’s
cards, public cards, P2’s actions, and P1’s actions. At the
bottom of the tree will be an enumeration of P2’s cards for
both showdown and fold outcomes. We can backup values
to the root of the tree while computing the best response
strategy. For a leaf node the value should be the payoff to
P1 multiplied by the probability of P2’s actions reaching
this leaf given the posterior distribution over strategies. For
an internal node, calculate the value from its children based
on the type of node. For a P2 action node or a public card
node, the value is the sum of the children’s values. For a
P1 action node, the value is the maximum of its children’s
values, and the best-response strategy assigns probability
one to the action that leads to the maximal child for that
node’s information set. Repeat until every node has been
assigned a value, which implies that every P1 information
set has been assigned an action. More formally Expectimax
computes the following value for the root of the tree,
X
R1

max
A1

X
B1

···

X

Z Y
k

Rk

max
Ak

XX
Bk

V (H)

D

βZi ,D,Bi P (O|β)P (β)

β i=1

This corresponds to Expectimax, with the posterior inducing a probability distribution over actions at P2’s action
nodes.
It now remains to prove that this version of Expectimax

computes the BBR. This will be done by showing that,
Z
X
V (H) P (H|α, β, O)P (O|β)P (β)
max
α

≤

β

H∈H

X
R1

max
A1

Z Y
k

X

X

···

B1

Rk

max
Ak

XX
Bk

V (H)

D

βZi ,D,Bi P (O|β)P (β)

β i=1

P
First we rewrite maxα H as,
XXXX
XXX
,
···
max · · · max
α(1)

α(k)

Rk Ak Bk

R1 A1 B1

D

where maxα(i) is a max over the set of all parameters inPα that govern
Pthe ith decision. Then, because
maxx y f (x, y) ≤ y maxx f (x, y), we get,
max · · · max
α(1)

α(k)

≤ max · · · max
≤

XXX

X

α(k)

X

XX

max
α(1)

···

R1 A1 B1

α(2)

R1

XXXX

R1

max
α(1)

···

max
α(k)

D

XXXX

···

Rk Ak Bk

A1 B1

X
Rk

A1 B1

Rk Ak Bk

XX

D

XXX
Ak Bk

D

Second, we note that,
Z
P (H|α, β, O)P (O|β)P (β)
β

∝

k
Y

αYi ,C,Ai

Z Y
k

βZi ,D,Bi

β i=1

i=1

We can distribute parameters from α to obtain,
X
X
X
···
αY1 ,C,A1
max
R1

X
Rk

α(1)

max
α(k)

Z Y
k

B1

A1

X

αYk ,C,Ak

XX
Bk

Ak

D

βZi ,D,Bi P (O|β)P (β)

parameter setting is to take the highest-valued action with
probability 1.
Computing the integral over opponent strategies depends
on the form of the prior but is difficult in any event. For
Dirichlet priors (see Section 5), it is possible to compute
the posterior exactly but the calculation is expensive except for small games with relatively few observations. This
makes the exact BBR an ideal goal rather than a practical
approach. For real play, we must consider approximations
to BBR.
One straightforward approach to approximating BBR is to
approximate the integral over opponent strategies by importance sampling using the prior as the proposal distribution:
Z
X
P (H|α, β̃, O)P (O|β̃)
P (H|α, β, O)P (O|β)P (β) ≈
β

β̃

where the β̃ are sampled from the prior, β̃ ∼ P (β). More
effective Monte Carlo techniques might be possible, depending on the prior used.
Note that P (O|β̃) need only be computed once for each β̃,
while the much smaller computation P (H|α, β̃, O) must
be computed for every possible hand. The running time
of computing the posterior for a strategy sample scales
linearly in the number of samples used in the approximation and the update is constant time for each hand
played. This tractability facilitates other approximate response techniques.
4.2 Max A Posteriori Response
An alternate goal to BBR is to find the max a posteriori
(MAP) strategy of the opponent and compute a best response to that strategy. Computing a true MAP strategy for
the opponent is also hard, so it is more practical to approximate this approach by sampling a set of strategies from the
prior and finding the most probable amongst that set. This
sampled strategy is taken to be an estimate of a MAP strategy and a best response to it is computed and played. MAP
is potentially dangerous for two reasons. First, if the distribution is multimodal, a best response to any single mode
may be suboptimal. Second, repeatedly playing any single
strategy may never fully explore the opponent’s strategy.

β i=1

=

X
R1

max
A1

Z Y
k

X
B1

···

X
Rk

max
Ak

XX
Bk

D

βZi ,D,Bi P (O|β)P (β),

β i=1

which is the Expectimax algorithm. This last step is possible because parameters in α must sum to one over all possible actions at a given information set. The maximizing

4.3 Thompson’s Response
A potentially more robust alternative to MAP is to sample
a strategy from the posterior distribution and play a best
response to that strategy. As with BBR and MAP, sampling
the posterior directly may be difficult. Again we can use
importance sampling, but in a slightly different way. We
sample a set of opponent strategies from the prior, compute
their posterior probabilities, and then sample one strategy
according to those probabilities.

P (β̃i |H, O)
P (i) = P
j P (β̃j |H, O)

This was first proposed by Thompson [10]. Thompson’s
has some probability of playing a best-response to any nonzero probability opponent strategy and so offers more robust exploration.

5 Priors
As with all Bayesian approaches, the resulting performance
and efficiency depends on the choice of prior. Obviously
the prior should capture our beliefs about the strategy of
our opponent. The form of the prior also determines the
tractability of (i) computing the posterior, and (ii) responding with the model. As the two games of hold ’em are considerably different in size, we explore two different priors.
Independent Dirichlet. The game of Leduc hold ’em is
sufficiently small that we can have a fully parameterized
model, with well-defined priors at every information set.
Dirichlet distributions offer a simple prior for multinomials, which is a natural description for action probabilities.
Any strategy (in behavioural form) specifies a multinomial
distribution over legal actions for every information set.
Our prior over strategies, which we will refer to as an independent Dirichlet prior, consists of independent Dirichlet distributions for each information set. We are using
Dirichlet(2, 2, 2) distributions, whose mode is the multinomial (1/3, 1/3, 1/3) over fold, call, and raise.
Informed. In the Texas hold ’em game, priors with independent distributions for each information set are both
intractable and ineffective. The size of the game virtually
guarantees that one will never see the same information set
twice. Any useful inference must be across information
sets and the prior must encode how the opponent’s decisions at information sets are likely to be correlated. We
therefore employ an expert defined prior that we will refer
to as an informed prior.
The informed prior is based on a ten dimensional recursive
model. That is, by specifying values for two sets of five
intuitive parameters (one set for each player), a complete
strategy is defined. Table 1 summarizes the expert defined
meaning of these five parameters. From the modelling perspective, we can simply consider this expert abstraction to
provide us with a mapping from some low-dimensional parameter space to the space of all strategies. By defining a
density over this parameter space, the mapping specifies a
resulting density over behaviour strategies, which serves as
our prior. In this paper we use an independent Gaussian
distribution over the parameter space with means and variances chosen by a domain expert. We omit further details

Table 1: The five parameter types in the informed prior
parameter space. A corresponding set of five are required
to specify the opponent’s model of how we play.
Parameter
Description
Fraction of opponent’s strength distribution that must be exceeded to
r0
raise after $0 bets (i.e., to initiate
betting).
Fraction of opponent’s strength disr1
tribution that must be exceeded to
raise after >$0 bets (i.e., to raise).
Fraction of the game-theoretic opb
timal bluff frequency.
Fraction of the game-theoretic opf
timal fold frequency.
t
Trap or slow-play frequency.
of this model because it is not the intended contribution of
this paper but rather a means to demonstrate our approach
on the large game of Texas hold’em.

6 Experimental Setup
We tested our approach on both Leduc hold’em with the
Dirichlet prior and Texas hold’em with the informed prior.
For the Bayesian methods, we used all three responses
(BBR, MAP, and Thompson’s) on Leduc and the Thompson’s response for Texas (BBR has not been implemented
for Texas and MAP’s behaviour is very similar to Thompson’s, as we will describe below). For all Bayesian methods, 1000 strategies were sampled from the prior at the beginning of each trial and used throughout the trial.
We have several players for our study. Opti is a Nash (or
minimax) strategy for the game. In the case of Leduc, this
has been computed exactly. We also sampled opponents
from our priors in both Leduc and Texas, which we will
refer to as Priors. In the experiments shown, a new opponent was sampled for each trial (200 hands), so results
are averaged over many samples from the priors. Both Priors and Opti are static players. Finally, for state-of-the-art
opponent modelling, we used Frequentist, (also known as
Vexbot) described fully in [3] and implemented for Leduc.
All experiments consisted of running two players against
each other for two hundred hands per trial and recording
the bankroll (accumulated winnings/losses) at each hand.
These results were averaged over multiple trials (1000 trials for all Leduc experiments and 280 trials for the Texas
experiments). We present two kinds of plots. The first is
simply average bankroll per number of hands played. A
straight line on such a plot indicates a constant winning
rate. The second is the average winning rate per number of hands played (i.e., the first derivative of the aver-

700

5
BBR
MAP
Thompson
Freq
Opti
Best Response

600

BBR
MAP
Thompson
Freq
Opti
Best Response

4.5

4

Average Winning Rate

Average Bankroll

500

400

300

3.5

3

2.5

2

200
1.5
100
1

0

0.5
0

20

40

60

80

100
120
Hands Played

140

160

180

200

0

20

40

60

80

100
120
Hands Played

140

160

180

200

Figure 2: Leduc hold’em: Avg. Bankroll per hands played
for BBR, MAP, Thompson’s, Opti, and Frequentist vs. Priors.

Figure 3: Leduc hold’em: Avg. Winning Rate per hands
played for BBR, MAP, Thompson’s, Opti, and Frequentist
vs. Priors.

age bankroll). This allows one to see the effects of learning more directly, since positive changes in slope indicate
improved exploitation of the opponent. Note that winning
rates for small numbers of hands are very noisy, so it is difficult to interpret the early results. All results are expressed
in raw pot units (e.g., bets in the first and second rounds of
Leduc are 2 and 4 units respectively).

Figures 4 and 5 show bankroll and winning rate results
for BBR, MAP, Thompson’s, Opti, and Frequentist versus
Opti on Leduc hold’em. Note that, on average, a positive
bankroll again Opti is impossible, although sample variance allows for it in our experiments. From these plots
we can see that the three Bayesian approaches behave very
similarly. This is due to the fact that the posterior distribution over our sample of strategies concentrates very rapidly
on a single strategy. Within less than 20 hands, one strategy dominates the rest. This means that the three responses
become very similar (Thompson’s is almost certain to pick
the MAP strategy, and BBR puts most of its weight on the
MAP strategy). Larger sample sizes would mitigate this
effect. The winning rate graphs also show little difference
between the three Bayesian players.

7 Results
7.1 Leduc Hold’em
Figures 2 and 3 show the average bankroll and average winning rate for Leduc against opponents sampled from the
prior (a new opponent each trial). For such an opponent, we
can compute a best response, which represents the best possible exploitation of the opponent. In complement, the Opti
strategy shows the most conservative play by assuming that
the opponent plays perfectly and making no attempt to exploit any possible weakness. This nicely bounds our results
in these plots. Results are given for Best Response, BBR,
MAP, Thompson’s, Opti, and Frequentist.
As we would expect, the Bayesian players do well against
opponents drawn from their prior, with little difference between the three response types in terms of bankroll. The
winning rates show that MAP and Thompson’s converge
within the first ten hands, whereas BBR is more erratic
and takes longer to converge. The uninformed Frequentist
is clearly behind. The independent Dirichlet prior is very
broad, admitting a wide variety of opponents. It is encouraging that the Bayesian approach is able to exploit even this
weak information to achieve a better result. However, it is
unfair to make strong judgements on the basis of these results since, in general, playing versus its prior is the best
possible scenario for the Bayesian approach.

Frequentist performs slightly worse than the Bayes approaches. The key problem with it is that it can form models of the opponent that are not consistent with any behavioral strategy (e.g., it can be led to believe that its opponent
can always show a winning hand). Such incorrect beliefs,
untempered by any prior, can lead it to fold with high probability in certain situations. Once it starts folding, it can
never make the observations required to correct its mistaken belief. Opti, of course, breaks even against itself. On
the whole, independent Dirichlet distributions are a poor
prior for the Opti solution, but we see a slight improvement
over the pure frequentist approach.
Our final Leduc results are shown in Figure 6, playing
against the Frequentist opponent. These results are included for the sake of interest. Because the Frequentist
opponent is not stationary, it violates the assumptions upon
which the Bayesian (and, indeed, the Frequentist) player
are based. We cannot drawn any real conclusions from
this data. It is interesting, however, that the BBR response is substantially worse than MAP or Thompson’s.

10

100

0

90
80

-10

70

Average Bankroll

-20
Average Bankroll

BBR
MAP
Thompson
Freq
Opti

-30
-40
-50

60
50
40
30

-60

20

-70

10

BBR
MAP
Thompson
Freq
Opti

-80

0

-90

-10
0

20

40

60

80

100
120
Hands Played

140

160

180

200

Figure 4: Leduc hold’em: Avg. Bankroll per hands played
for BBR, MAP, Thompson’s, Opti, and Frequentist vs.
Opti.

0

20

40

60

80

100
120
Hands Played

140

160

180

200

Figure 6: Leduc hold’em: Avg. Bankroll per hands played
for BBR, MAP, Thompson’s, and Opti vs. Frequentist.
1
BBR
MAP
Thompson
Freq
Opti

0.9

-0.2

0.8
0.7
Average Winning Rate

Average Winning Rate

-0.3

-0.4

-0.5

0.6
0.5
0.4
0.3
0.2

-0.6
0.1
0

-0.7
BBR
MAP
Thompson
Freq

-0.1
0

20

40

-0.8
0

20

40

60

80

100
120
Hands Played

140

160

180

60

80

100
120
Hands Played

140

160

180

200

200

Figure 5: Leduc hold’em: Avg. Winning Rate per hands
played for BBR, MAP, Thompson’s, Opti, and Frequentist
vs. Opti.
It seems likely that the posterior distribution does not converge quickly against a non-stationary opponent, leading
BBR to respond to several differing strategies simulataneously. Because the prior is independent for every information set, these various strategies could be giving radically different advice in many contexts, preventing BBR
from generating a focused response. MAP and Thompson’s necessarily generate more focused responses. We
show winning rates in Figure 7 for the sake of completeness, with the same caveat regarding non-stationarity.
7.2 Texas Hold’em
Figure 8 show bankroll results for Thompson’s, Opti, and
Frequentist versus opponents sampled from the informed
prior for Texas hold’em. Here Thompson’s and Frequentist
give very similar performance, although there is a small

Figure 7: Leduc hold’em: Avg. Winning Rate per hands
played for BBR, MAP, Thompson’s, and Opti vs. Frequentist.
advantage to Thompson’s late in the run. It is possible that
even with the more informed prior, two hundred hands does
not provide enough information to effectively concentrate
the posterior on good models of the opponent in this larger
game. It may be that priors encoding strong correlations
between many information sets are required to gain a substantial advantage over the Frequentist approach.

8 Conclusion
This research has presented a Bayesian model for hold’em
style poker, fully modelling both game dynamics and opponent strategies. The posterior distribution has been
described and several approaches for computing appropriate responses considered. Opponents in both Texas
hold’em and Leduc hold’em have been played against using Thompson’s sampling for Texas hold’em, and approximate Bayesian best response, MAP, and Thompson’s for

telligence, 134(1–2):201–240, 2002.

300
Thompson
Freq
Opti

[3] Darse Billings, Aaron Davidson, Terrance Schauenberg, Neil Burch, Michael Bowling, Rob Holte,
Jonathan Schaeffer, and Duane Szafron. Game Tree
Search with Adaptation in Stochastic Imperfect Information Games. In Nathan Netanyahu and Jaap van
den Herik Yngvi Bjornsson, editor, Computers and
Games’04. Springer-Verlag, 2004.

250

Average Bankroll

200

150

100

50

0
0

20

40

60

80

100
120
Hands Played

140

160

180

200

[4] Fredrik A. Dahl. A reinforcement learning algorithm applied to simplified two-player Texas Hold’em
poker. In Proceedings of the 12th European Conference on Machine Learning (ECML-01), pages 85–96,
September 2001.

Figure 8: Texas hold’em: Avg. Bankroll per hands played
for Thompson’s, Frequentist, and Opti vs. Priors.

[5] D. Koller and A. Pfeffer. Representations and solutions for game-theoretic problems. Artificial Intelligence, 94(1):167–215, 1997.

Leduc hold’em. These results show that, for opponents
drawn from our prior, the posterior captures them rapidly
and the subsequent response is able to exploit the opponent, even in just 200 hands. On Leduc, the approach
performs favourably compared with state-of-the-art opponent modelling techniques against prior-drawn opponents
and a Nash equilibrium. Both approaches can play quickly
enough for real-time play against humans.

[6] K. Korb, A. Nicholson, and N. Jitnah. Bayesian
poker. In Uncertainty in Artificial Intelligence, pages
343–350, 1999.

The next major step in advancing the play of these systems is constructing better informed priors capable of modelling more challenging opponents. Potential sources for
such priors include approximate game theoretic strategies,
data mined from logged human poker play, and more sophisticated modelling by experts. In particular, priors that
are capable of capturing correlations between related information sets would allow for generalization of observations
over unobserved portions of the game. Finally, extending
the approach to non-stationary approaches is under active
investigation.
Acknowledgements
We would like to thank Rob Holte, Dale Schuurmanns,
Nolan Bard, and the University of Alberta poker group for
their insights. This work was funded by the Alberta Ingenuity Centre for Machine Learning, iCore, and NSERC.




incomplete on each step, still efficiently computes optimal
actions in a timely manner.

We consider the problem of efficiently learning
optimal control policies and value functions over
large state spaces in an online setting in which
estimates must be available after each interaction
with the world. This paper develops an explicitly
model-based approach extending the Dyna architecture to linear function approximation. Dynastyle planning proceeds by generating imaginary
experience from the world model and then applying model-free reinforcement learning algorithms to the imagined state transitions. Our
main results are to prove that linear Dyna-style
planning converges to a unique solution independent of the generating distribution, under natural conditions. In the policy evaluation setting,
we prove that the limit point is the least-squares
(LSTD) solution. An implication of our results
is that prioritized-sweeping can be soundly extended to the linear approximation case, backing
up to preceding features rather than to preceding
states. We introduce two versions of prioritized
sweeping with linear Dyna and briefly illustrate
their performance empirically on the Mountain
Car and Boyan Chain problems.

The Dyna architecture (Sutton 1990) provides an effective
and flexible approach to incremental planning while maintaining responsiveness. There are two ideas underlying the
Dyna architecture. One is that planning, acting, and learning are all continual, operating as fast as they can without
waiting for each other. In practice, on conventional computers, each time step is shared between planning, acting,
and learning, with proportions that can be set arbitrarily according to available resources and required response times.

Online learning and planning

Efficient decision making when interacting with an incompletely known world can be thought of as an online learning
and planning problem. Each interaction provides additional
information that can be used to learn a better model of the
world’s dynamics, and because this change could result in a
different action being best (given the model), the planning
process should be repeated to take this into account. However, planning is inherently a complex process; on large
problems it not possible to repeat it on every time step without greatly slowing down the response time of the system.
Some form of incremental planning is required that, though

The second idea underlying the Dyna architecture is that
learning and planning are similar in a radical sense. Planning in the Dyna architecture consists of using the model
to generate imaginary experience and then processing the
transitions of the imaginary experience by model-free reinforcement learning algorithms as if they had actually occurred. This can be shown, under various conditions, to
produce exactly the same results as dynamic-programming
methods in the limit of infinite imaginary experience.
The original papers on the Dyna architecture and most subsequent extensions (e.g., Singh 1992; Peng & Williams
1993; Moore & Atkeson 1993; Kuvayev & Sutton 1996)
assumed a Markov environment with a tabular representation of states. This table-lookup representation limits the
applicability of the methods to relatively small problems.
Reinforcement learning has been combined with function
approximation to make it applicable to vastly larger problems than could be addressed with a tabular approach.
The most popular form of function approximation is linear function approximation, in which states or state-action
pairs are first mapped to feature vectors, which are then
mapped in a linear way, with learned parameters, to value
or next-state estimates. Linear methods have been used
in many of the successful large-scale applications of reinforcement learning (e.g., Silver, Sutton & Müller 2007;
Schaeffer, Hlynka & Jussila 2001). Linear function approximation is also simple, easy to understand, and possesses some of the strongest convergence and performance
guarantees among function approximation methods. It is

natural then to consider extending Dyna for use with linear
function approximation, as we do in this paper.
There has been little previous work addressing planning
with linear function approximation in an online setting.
Paduraru (2007) treated this case, focusing mainly on sampling stochastic models of a cascading linear form, but
also briefly discussing deterministic linear models. Degris,
Sigaud and Wuillemin (2006) developed a version of Dyna
based on approximations in the form of dynamic Bayes networks and decision trees. Their system, SPITI, included
online learning and planning based on an incremental version of structured value iteration (Boutilier, Dearden &
Goldszmidt 2000). Singh (1992) developed a version of
Dyna for variable resolution but still tabular models. Others
have proposed linear least-squares methods for policy evaluation that are efficient in the amount of data used (Bradtke
& Barto 1996; Boyan 1999, 2002; Geramifard, Bowling &
Sutton 2006). These methods can be interpreted as forming and then planning with a linear model of the world’s
dynamics, but so far their extensions to the control case
have not been well suited to online use (Lagoudakis &
Parr 2003; Peters, Vijayakumar & Schaal 2005; Bowling,
Geramifard, & Wingate 2008), whereas our linear Dyna
methods are naturally adapted to this case. We discuss
more specifically the relationship of our work to LSTD
methods in a later section. Finally, Atkeson (1993) and others have explored linear, learned models with off-line planning methods suited to low-dimensional continuous systems.

2

Notation

We use the standard framework for reinforcement learning with linear function approximation (Sutton & Barto
1998), in which experience consists of the time indexed
stream s0 , a0 , r1 , s1 , a1 , r2 , s2 , . . ., where st ∈ S is a state,
at ∈ A is an action, and rt ∈ R is a reward. The actions are selected by a learning agent, and the states and rewards are selected by a stationary environment. The agent
does not have access to the states directly but only through
a corresponding feature vector φt ∈ Rn = φ(st ). The
n
agent selects actions
P according to a policy, π : R × A →
[0, 1] such that a∈A π(φ, a) = 1, ∀φ. An important step
towards finding a good policy is to estimate the value function for a given policy (policy evaluation). The value function is approximated as a linear function with parameter
vector θ ∈ Rn :
(∞
)
X
θ> φ(s) ≈ V π (s) = Eπ
γ t−1 rt | s0 = s ,
t=1

where γ ∈ [0, 1). In this paper we consider policies that are
greedy or -greedy with respect to the approximate statevalue function.

Algorithm 1 : Linear Dyna for policy evaluation, with random sampling and gradient-descent model learning
Obtain initial φ, θ, F, b
For each time step:
Take action a according to the policy. Receive r, φ0
θ ← θ + α[r + γθ> φ0 − θ> φ]φ
F ← F + α(φ0 − F φ)φ>
b ← b + α(r − b> φ)φ
temp ← φ0
Repeat p times (planning):
Generate a sample φ from some distribution µ
φ0 ← F φ
r ← b> φ
θ ← θ + α[r + γθ> φ0 − θ> φ]φ
φ ← temp

3

Theory for policy evaluation

The natural place to begin a study of Dyna-style planning
is with the policy evaluation problem of estimating a statevalue function from a linear model of the world. The model
consists of a forward transition matrix F ∈ Rn × Rn (incorporating both environment and policy) and an expected
reward vector b ∈ Rn , constructed such that F φ and b> φ
can be used as estimates of the feature vector and reward
that follow φ. A Dyna algorithm for policy evaluation goes
through a sequence of planning steps, on each of which a
starting feature vector φ is generated according to a probability distribution µ, and then a next feature vector φ0 = F φ
and next reward r = b> φ are generated from the model.
Given this imaginary experience, a conventional modelfree update is performed, for example, according to the linear TD(0) algorithm (Sutton 1988):
θ ← θ + α(r + γθ> φ0 − θ> φ)φ,

(1)

or according to the residual gradient algorithm (Baird
1995):
θ ← θ + α(r + γθ> φ0 − θ> φ)(φ − γφ0 ),

(2)

where α > 0 is a step-size parameter. A complete algorithm using TD(0), including learning of the model, is
given in Algorithm 1.
3.1

Convergence and fixed point

There are two salient theoretical questions about the Dyna
planning iterations (1) and (2): Under what conditions on
µ and F do they converge? and What do they converge
to? Both of these questions turn out to have interesting answers. First, note that the convergence of (1) is in question
in part because it is known that linear TD(0) may diverge
if the distribution of starting states during training does not
match the distribution created by the normal dynamics of

the system, that is, if TD(0) is used off-policy. This suggests that the sampling distribution used here, µ, might
have to be strongly constrained in order for the iteration
to be stable. On the other hand, the data here is from the
model, and the model is not a general system: it is deterministic1 and linear. This special case could be much better
behaved. In fact, convergence of linear Dyna-style policy
evaluation, with either the TD(0) or residual-gradient iterations, is not affected by µ, but only by F , as long as µ exercises all directions in the full n-dimensional vector space.
Moreover, not only is the fact of convergence unaffected by
µ, but so is the value converged to. In fact, we show below
that convergence is to a deterministic fixed point, a value
of θ such that the iterations (1) and (2) leave it unchanged
not just in expected value, but for every individual φ that
could be generated by µ. The only way this could be true is
if the TD error (the first expression in parentheses in each
iteration) were exactly zero, that is, if
0

=

r + γθ> φ0 − θ> φ
>

>

b φ + γθ F φ − θ φ

=

(b + γF > θ − θ)> φ.

And the only way that this can be true for all φ is for the
expression in parenthesis above to be zero:
0

Before verifying the conditions of this result, let us
rewrite (4) in terms of the matrix G = I − γF :
θk+1

= θk + αk (b> φk + θk> (γF − I)φk )φk

= b + (γF > − I)θ,

= θk + αk sk .

=

(I − γF > )−1 b,

(4)

where θ0 ∈ Rn is P
arbitrary. AssumeP
that (i) the step-size
∞
∞
sequence satisfies k=0 αk = ∞, k=0 αk2 < ∞, (ii)
r(F ) ≤ 1, (iii) (φk ) are uniformly
 bounded
 i.i.d. random
variables, and that (iv) C = E φk φ>
is non-singular.
k
Then the parameter vector θk converges with probability
one to (I − γF > )−1 b.

= θk + αk (b> φk − θk> Gφk )φk

Here sk is defined by the last equation.
(3)

assuming that the inverse exists. Note that this expression
for the fixed point does not depend on µ, as promised.
If I − γF > is nonsingular, then there might be no fixed
point. This could happen for example if F were an expansion, or more generally if the limit (γF )∞ were not
zero. These cases correspond to world models that say the
feature vectors diverge to infinity over time. Failure to converge in these cases should not be considered a problem for
the Dyna iterations as planning algorithms; these are cases
in which the planning problem is ill posed. If the feature
vectors diverge, then so too may the rewards, in which case
the true values given the model are infinite. No real finite
Markov decision process could behave in this way.
It remains to show the conditions on F under which the iterations converge to the fixed point if one exists. We prove
next that under the TD(0) iteration (1), convergence is guaranteed if the numerical radius of F is less than one,2 and
1

θk+1 = θk + αk (b> φk + γθk> F φk − θk> φk )φk ,

= b + γF > θ − θ

which immediately implies that
θ

Theorem 3.1 (Convergence of linear TD(0) Dyna for policy evaluation). Consider the TD(0) iteration with a nonnegative step-size sequence (αk ):

Proof. The idea of the proof is to view the algorithm as a
stochastic gradient descent method. In particular, we apply
Proposition 4.1 of (Bertsekas & Tsitsiklis 1996).

>

=

then that under the residual-gradient iteration (2), convergence is guaranteed for any F as long as the fixed point exists. That F ’s numerical radius be less than 1 is a stronger
condition than nonsingularity of I − γF > , but it is similar
in that both conditions pertain to the matrix trending toward
expansion when multiplied by itself.

The model is deterministic because it generates the expectation of the next feature vector; the system itself may be stochastic.
2
The numerical radius of a real-valued square matrix A is defined by r(A) = maxkxk2 =1 xT Ax.

The cited proposition requires the definition of a potential function J(θ) and will allow us to conclude that
limk→∞ ∇J(θk) = 0 with probability one. Let
 us choose
J(θ) = 1/2 E (b> φk + γθ> F φk − θ> φk )2 . Note that
by our i.i.d. assumptions on the features, J(θ) is welldefined. We need to check four conditions (because the
step-size conditions are automatically satisfied): (i) The
nonnegativity of the potential function; (ii) The Lipschitz
continuity of ∇J(θ); (iii) The pseudo-gradient property of
the expected update direction; and (iv) The boundedness of
the expected magnitude of the update, more precisely that
E ksk k22 |θk ≤ O(k∇J(θk )k22 ). Nonnegativity is satisfied
by definition and the boundedness condition (iv) is satisfied
thanks to the boundedness of the features.
Let us show now that the pseudo-gradient property (iii) is
satisfied. This condition requires the demonstration of a
positive constant c such that
ck∇J(θk )k22 ≤ −∇J(θk )> E [sk |θk ] .

(5)

Define sk = E [sk |θk ] = Cb − CG> θk . A simple calculation gives ∇J(θk ) = −Gsk . Hence k∇J(θk )k22 =
>
>
>
s>
k G Gsk and −(∇J(θk )) sk = sk Gsk . Therefore (5)
> >
>
is equivalent to c sk G Gsk ≤ sk Gsk . In order to make
this true with a sufficiently small c, it suffices to show that

s> Gs > 0 holds for any non-zero vector s. An elementary
reasoning shows that this is equivalent to 1/2(G + G> ) being positive definite, which in turn is equivalent to r(F ) ≤
1, showing that (iii) is satisfied.
Hence, we have verified all the assumptions of the
cited proposition and can therefore we conclude that
limk→∞ ∇J(θk ) = 0 with probability one. Plugging in the
expression of ∇J(θk ), we get limt→∞ (Cb−CG> θk ) = 0.
Because C and G are invertible (this latter follows from
r(F ) ≤ 1), it follows that the limit of θk exists and
limk→∞ θk = (G> )−1 b = (I − γF > )−1 b.

verges with probability one to (I − γF > )−1 b, assuming
that (I − γF > ) is non-singular.
Proof. As all the conditions of Proposition 4.1 of (Bertsekas & Tsitsiklis 1996) are trivially satisfied with the
choice J(θ) = E [J(θ, φk )], we can conclude that θk converges w.p.1 to the minimizer of J(θ). In the previous theorem we have seen that the minimizer of J(θ) is indeed
θ = (I − γF > )−1 b, finishing the proof.
3.2

Convergence to the LSTD solution

Several extensions of this result are possible. First, the requirement of i.i.d. sampling can be considerably relaxed.
With an essentially unchanged proof, it is possible to show
that the theorem remains true if the feature vectors are generated by a Markov process given that they satisfy appropriate ergodicity conditions. Moreover, building on a result by Delyon (1996), one can show that the result continues to hold even if the sequence of features is generated in an algorithmic manner, again provided that some
ergodicity conditions are met.
PKThe major assumption then
is that C = limK→∞ 1/K k=1 φk φ>
k exists and is nonsingular. Further, because there is no “noise” to reject, there
is noP
need to decay the step-sizes towards zero (the condi∞
tion k=0 αk2 < +∞ in the proofs is used to “filter out
noise”). In particular, we conjecture that sufficiently small
constant step-sizes would work as well (for a result of this
type see Proposition 3.4 by Bertsekas & Tsitsiklis 1996).

So far we have discussed the convergence of planning given
a model, but we have said nothing about the relationship
of the model to data, or about the quality of the resultant
solution. Suppose the model were the best linear fit to a
finite dataset of observed feature-vector-to-feature-vector
transitions with accompanying rewards. In this case we can
show that the fixed point of the Dyna updates is the least
squares temporal-difference solution. This is the solution
for which the mean TD(0) update is zero and is also the solution found by the LSTD(0) algorithm (Barto & Bradtke
1996).

On the other hand the requirement on the numerical radius of F seems to be necessary for the convergence of the
TD(0) iteration. By studying the ODE associated with (4),
we see that it is stable if and only if CG is a positive stable
matrix (i.e., iff all its eigenvalues have positive real part).
From this it seems necessary to require that G is positive
stable. However, to ensure that CG is positive stable the
strictly stronger condition that G + G> is positive definite must be satisfied. This latter condition is equivalent
to r(F ) ≤ 1.

Proof. It suffices to show that the respective solution sets
of the equations

We turn now to consider the convergence of Dyna planning
using the residual-gradient Dyna iteration (2). This update
rule can be derived by taking the gradient of J(θ, φk ) =
(b> φk + γθ> φk − θ> φk )2 w.r.t. θ. Thus, as an immediate
consequence of Proposition 4.1 of (Bertsekas & Tsitsiklis
1996) we get the following result:
Theorem 3.2 (Convergence of residual-gradient Dyna for
policy evaluation). Assume that θk is updated according to
θk+1 = θk + αk (b> φk + γθk> F φk − θk> φk )(φk − γF φk ),
where θ0 ∈ Rn is arbitrary. Assume that the non-negative
step-size sequence (αk ) satisfies the summability condition
(i) of Theorem 3.1 and that (φk ) are uniformly bounded
i.i.d. random variables. Then the parameter vector θk con-

Theorem 3.3. Given a training dataset of feature, reward,
next-state feature triples D = [φ1 , r1 , φ01 , . . . , φn , rn , φ0n ],
let F, bPbe the least-squares model built on D. Assume that
n
C = k=1 φk φ>
k has full rank. Then the solution (3) is
the same as the LSTD solution on this training set.

0

=

n
X

φk (rk + γ(φ0k )> θ − φ>
k θ),

(6)

k=1

0

=

b + (γF > − I)θ

(7)

are the same. This is because the LSTD parameter vectors
are obtained by solving the first equation and the TD(0)
Dyna solutions are derived from the second equation.
Pn
Pn
Let D = k=1 φk (φ0k )> , and r = k=1 φk rk . A standard calculation shows that
F>

=

C −1 D

and b = C −1 r.

Plugging in C, D into (6) and factoring out θ shows that
any solution of (6) also satisfies
0

=

r + (γD − C) θ.

(8)

If we multiply both sides of (8) by C −1 from the left we
get (7). Hence any solution of (6) is also a solution of (7).
Because all the steps of the above derivation are reversible,
we get that the reverse statement holds as well.

Algorithm 2 : Linear Dyna with PWMA prioritized
sweeping (policy evaluation)
Obtain initial φ, θ, F, b
For each time step:
Take action a according to the policy. Receive r, φ0
δ ← r + γθ> φ0 − θ> φ
θ ← θ + αδφ
F ← F + α(φ0 − F φ)φ>
b ← b + α(r − b> φ)φ
For all i such that φ(i) 6= 0:
For all j such that F ij 6= 0:
Put j on the PQueue with priority |F ij δφ(i)|
Repeat p times while PQueue is not empty:
i ← pop the PQueue
δ ← b(i) + γθ> F ei − θ(i)
θ(i) ← θ(i) + αδ
For all j such that F ij 6= 0:
Put j on the queue with priority |F ij δ|
φ ← φ0

4

Algorithm 3 : Linear Dyna with MG prioritized sweeping
(policy evaluation)
Obtain initial φ, θ, F, b
For each time step:
Take action a according to the policy. Receive r, φ0
δ ← r + γθ> φ0 − θ> φ
θ ← θ + αδφ
F ← F + α(φ0 − F φ)φ>
b ← b + α(r − b> φ)φ
For all i such that φ(i) 6= 0:
Put i on the PQueue with priority |δφ(i)|
Repeat p times while PQueue is not empty:
i ← pop the PQueue
For all j such that F ij 6= 0:
δ ← b(j) + γθ> F ej − θ(j)
θ(j) ← θ(j) + αδ
Put j on the PQueue with priority |δ|
φ ← φ0

Linear prioritized sweeping

We have shown that the convergence and fixed point of policy evaluation by linear Dyna are not affected by the way
the starting feature vectors are chosen. This opens the possibility of selecting them cleverly so as to speed the convergence of the planning process. One natural idea—the
idea behind prioritized sweeping—is to work backwards
from states that have changed in value to the states that
lead into them. The lead-in states are given priority for being updated because an update there is likely to change the
state’s value (because they lead to a state that has changed
in value). If a lead-in state is updated and its value is
changed, then its lead-in states are in turn given priority
for updating, and so on. In the table-lookup context in
which this idea was developed (Moore & Atkeson 1993;
Peng 1993; see also Wingate & Seppi 2005), there could
be many states preceding each changed state, but only one
could be updated at a time. The states waiting to be updated were kept in a queue, prioritized by the size of their
likely effect on the value function. As high-priority states
were popped off the queue and updated, it would sometimes give rise to highly efficient sweeps of updates across
the state space; this is what gave rise to the name “prioritized sweeping”.
With function approximation it is not possible to identify
and work backwards from individual states, but alternatively one could work backwards feature by feature. If
there has just been a large change in θ(i), the component of
the parameter vector corresponding to the ith feature, then
one can look backwards through the model to find the features j whose components θ(j) are likely to have changed
as a result. These are the features j for which the elements
F ij of F are large. One can then preferentially construct

starting feature vectors φ that have non-zero entries at these
j components. In our algorithms we choose the starting
vectors to be the unit basis vectors ej , all of whose components are zero except the jth, which is 1. (Our theoretical
results assure us that this cannot affect the result of convergence.) Using unit basis vectors is very efficient computationally, as the vector matrix multiplication F φ is reduced
to pulling out a single column of F .
There are two tabular prioritized sweeping algorithms in
the literature. The first, due simultaneously to Peng and
Williams (1993) and to Moore and Atkeson (1993), which
we call PWMA prioritized sweeping, adds the predecessors
of every state encountered in real experience to the priority queue whether or not the value of the encountered state
was significantly changed. The second form of prioritized
sweeping, due to McMahan and Gordon (2005), and which
we call MG prioritized sweeping, puts each encountered
state on the queue, but not its predecessors. For McMahan and Gordon this resulted in a more efficient planner.
A complete specification of our feature-by-feature versions
of these two forms of prioritized sweeping are given above,
with TD(0) updates and gradient-descent model learning,
as Algorithms 2 and 3. These algorithms differ slightly
from previous prioritized sweeping algorithms in that they
update the value function from the real experiences and not
just from model-generated experience. With function approximation, real experience is always more informative
than model-generated experience, which will be distorted
by the function approximator. We found this to be a significant effect in our empirical experiments (Section 6).

Algorithm 4: Linear Dyna with MG prioritized sweeping
and TD(0) updates (control)
Obtain initial φ, θ, F, b
For each time step:

>
a ← arg maxa b>
(or -greedy)
a φ + γθ Fa φ
Take action a, receive r, φ0
δ ← r + γθ> φ0 − θ> φ
θ ← θ + αδφ
Fa ← Fa + α(φ0 − Fa φ)φ>
ba ← ba + α(r − b>
a φ)φ
For all i such that φ(i) 6= 0:
Put i on the PQueue with priority |δφ(i)|
Repeat p times while PQueue is not empty:
i ← pop the PQueue
ij
For all j s.t. there
 exists an>a s.t. F
 a 6= 0:
δ ← maxa ba (j) + γθ Fa ej − θ(j)
θ(j) ← θ(j) + αδ
Put j on the PQueue with priority |δ|
φ ← φ0

5

Theory for Control

We now turn to the full case of control, in which separate
models Fa , ba are learned and are then available for each
action a. These are constructed such that Fa φ and b>
a φ can
be used as estimates of the feature vector and reward that
follow φ if action a is taken. A linear Dyna algorithm for
the control case goes through a sequence of planning steps
on each of which a starting feature vector φ and an action
a are chosen, and then a next feature vector φ0 = Fa φ and
next reward r = ba φ are generated from the model. Given
this imaginary experience, a conventional model-free update is performed. The simplest case is to again apply
(1). A complete algorithm including prioritized sweeping
is given in Algorithm 4.
The theory for the control case is less clear than for policy evaluation. The main issue is the stability of the “mixture” of the forward model matrices. The corollary below
is stated for an i.i.d. sequence of features, but by the remark after Theorem 3.1 it can be readily extended to the
case where the policy to be evaluated is used to generate
the trajectories.
Corollary 5.1 (Convergence of linear TD(0) Dyna with
action models). Consider the Dyna recursion (4) with
the modification that in each step, instead of F φk ,
we use Fπ(φk ) φk , where π is a policy mapping feature vectors to actions and {Fa } is a collection of
forward-model matrices. Similarly, b> φk is replaced by
b>
π(φk ) φk . As before, assume that φk is an unspecified
i.i.d. process. Let (F, b)
 be the least squares
 model of
π: F = harg minG E kGφk − iFπ(φk ) φk k22 and b =
2
arg minu E (u> φk − b>
If the numerical radius
π(φk ) φk )
of F is bounded by one, then the conclusions of Theo-

-3

N

1
0
.
.
0
0

-3

N-1

-3

-3

.75
.25
.
.
0
0

-3

3

0
0
.
.
.5
.5

-3

-3

2

0
0
.
.
.25
.75

-2

1

0
0
.
.
0
1

0

0

0
0
.
.
0
0

Figure 1: The general Boyan Chain problem.
rem 3.1 hold: the parameter vector θk converges with probability one to (I − γF > )−1 b.
Proof. The proof is immediate
from
equation

 the normal


for F , which states that E F φk φ>
= E Fπ(φk ) φk φ>
k
k ,
and once we observe that, in the proof of Theorem 3.1, F
appears only in expressions of the form E F φk φ>
k .
As in the case of policy evaluation, there is a corresponding
corollary for the residual gradient iteration, with an immediate proof. These corollaries say that, for any policy with a
corresponding model that is stable, the Dyna recursion can
be used to compute its value function. Thus we can perform a form of policy iteration—continually computing an
approximation to the value function for the greedy policy.

6

Empirical results

In this section we illustrate the empirical behavior of the
four Dyna algorithms and make comparisons to model-free
methods using variations of two standard test problems:
Boyan Chain and Mountain Car. Our Boyan Chain environment is an extension of that by Boyan (1999, 2002)
from 13 to 98 states, and from 4 to 25 features (Geramifard, Bowling & Sutton 2006). Figure 1 depicts this environment in the general form. Each episode starts at state
N = 98 and terminates in state 0. For all states s > 2,
there is an equal probability of transitioning to states s − 1
or s − 2 with a reward of −3. From states 2 and 1, there are
deterministic transitions to states 1 and 0 with respective
rewards of −2 and 0. Our Mountain Car environment is exactly as described by Sutton (1996; Sutton & Barto 1998),
re-implemented in Matlab. An underpowered car must be
driven to the top of a hill by rocking back and forth in a
valley. The state variables are a pair (position,velocity) initialized to (−0.5, 0.0) at the beginning of each episode. The
reward is −1 per time step. There are three discrete actions
(accelerate, reverse, and coast). We used a value function
representation based on tile-coding feature vectors exactly
as in Sutton’s (1996) experiments, with 10 tilings over the
combined (position, velocity) pair, and with the tiles hashed
down to 10,000 features. In the policy evaluation experiments with this domain, the policy was to accelerate in

4

2

Boyan chain

10

9

Mountain Car

x 10

Dyna-Random
TD

7

1

Loss

Dyna-Random
Dyna-PWMA

Loss

10

5

TD

0

10

Dyna-MG

Dyna-PWMA

3

Dyna-MG
−1

10

0

20

40

60
Episode

80

100

1
0

200

400

600
Episode

800

1000

Figure 2: Performance of policy evaluation methods on the Boyan Chain and Mountain Car environments
the direction of the current velocity, and we added noise to
the domain that switched the selected action to a random
action with 10% probability. Complete code for our test
problems as standard RL-Glue environments is available
from the RL-Library hosted at the University of Alberta.
In all experiments, the step size parameter α took the form
0 +1
αt = α0 NN0 +t
1.1 , in which t is the episode number and
the pair (N0 , α0 ) was selected based on empirically finding the best combination out of α0 ∈ {.01, .1, 1} and
N0 ∈ {100, 1000, 106 } separately for each algorithm and
domain. All methods observed the same trajectories in policy evaluation. All graphs are averages of 30 runs; error
bars indicate standard errors in the means. Other parameter
settings were  = 0.1, γ = 1, and λ = 0.
We performed policy evaluation experiments with four algorithms: Dyna-Random, Dyna-PWMA, Dyna-MG (as in
Algorithms 1–3), and model-free TD(0). In the case of
the Dyna-Random algorithm, the starting feature vectors
in planning were chosen to be unit basis vectors with the 1
in a random location. Figure 2 shows the policy evaluation
performance of the four methods in the Boyan Chain and
Mountain Car environments. For the Boyan Chain domain,
the loss was the root-mean-squared error of the learned
value function compared to the exact analytical value, averaged over all states. In the Mountain Car domain, the
states are visited very non-uniformly, and a more sophisticated measure is needed. Note that all of the methods
drive θ toward an asymptotic value in which the expected
TD(0) update is zero; we can use the distance from this
as a loss measure. Specifically, we evaluated each learned
value function by freezing it and then running a fixed set
of 200,000 episodes with it while running the TD(0) algorithm (but not allowing θ to actually change). The norm of
the sum of the (attempted) update vectors was then computed and used as the loss. In practice, this measure can be
computed very efficiently as ||A∗ θ − b∗ || (in the notation of

LSTD(0), see Bradtke & Barto 1996).
In the Boyan Chain environment, the Dyna algorithms generally learned more rapidly than model-free TD(0). DynaMG was initially slower than the other algorithms, then
caught up and surpassed them. The relatively poor early
performance of Dyna-MG was actually due to its being
a better planning method. After few episodes the model
tends to be of very high variance, and so therefore is the
best value-function estimate given it. We tested this hypothesis by running the Dyna methods starting with a fixed,
well-learned model; in this case Dyna-MG was the best of
all the methods from the beginning. All of these data are
for one step of planning for each real step of interaction
with the world (p = 1). In preliminary experiments with
larger values of p, up to p = 10, we found further improvements in learning rate of the Dyna algorithms over TD(0),
and again Dyna-MG was best.
The results for Mountain Car are less clear. Dyna-MG
quickly does significantly better than TD(0), but the other
Dyna algorithms lag initially and never surpass TD(0).
Note that, for any value of p, Dyna-MG does many more θ
updates than the other two Dyna algorithms (because these
updates are in an inner loop, cf. Algorithms 2 and 3). Even
so, because of its other efficiencies Dyna-MG tended to run
faster overall in our implementation. Obviously, there is a
lot more interesting empirical work that could be done here.
We performed one Mountain Car experiment with DynaMG as a control algorithm (Algorithm 4), comparing it
with model-free Sarsa (i.e., Algorithm 4 with p = 0). The
results are shown in Figure 3. As before, Dyna-MG showed
a distinct advantage over the model-free method in terms
of learning rate. There was no clear advantage for either
method in the second half of the experiment. We note
that, asymptotically, model-free methods are never worse
than model-based methods, and are often better because the
model does not converge exactly to the true system because

7

−120
−140

Return

−160

Dyna-MG

−180
−200

Sarsa
−220
−240
−260
0

20

40

60
Episode

80

100

Figure 3: Control performance on Mountain Car

Conclusion

In this paper we have taken important steps toward establishing the theoretical and algorithmic foundations of
Dyna-style planning with linear function approximation.
We have established that Dyna-style planning with familiar
reinforcement learning update rules converges under weak
conditions corresponding roughly, in some cases, to the existence of a finite solution to the planning problem, and
that convergence is to a unique least-squares solution independent of the distribution used to generate hypothetical experience. These results make possible our second
main contribution: the introduction of algorithms that extend prioritized sweeping to linear function approximation,
with correctness guarantees. Our empirical results illustrate
the use of these algorithms and their potential for accelerating reinforcement learning. Overall, our results support
the conclusion that Dyna-style planning may be a practical
and competitive approach to achieving rapid, online control
in stochastic sequential decision problems with large state
spaces.
Acknowledgements

of structural modeling assumptions. (The case we treat
here—linear models and value functions with one-step TD
methods—is a rare case in which asymptotic performance
of model-based and model-free methods should be identical.) The benefit of models, and of planning generally, is in
rapid adaptation to new problems and situations.

The authors gratefully acknowledge the substantial contributions of Cosmin Paduraru and Mark Ring to the early
stages of this work. This research was supported by
iCORE, NSERC and Alberta Ingenuity.

These empirical results are not extensive and in some cases
are preliminary, but they nevertheless illustrate some of the
potential of linear Dyna methods. The results on the Boyan
Chain domain show that Dyna-style planning can result in
a significant improvement in learning speed over modelfree methods. In addition, we can see trends that have been
observed in the tabular case re-occurring here with linear
function approximation. In particular, prioritized sweeping can result in more efficient learning than simply updating features at random, and the MG version of prioritized
sweeping seems to be better than the PWMA version.

Atkeson, C. (1993). Using local trajectory optimizers to
speed up global optimization in dynamic programming.
Advances in Neural Information Processing Systems, 5,
663–670.
Baird, L. C. (1995). Residual algorithms: Reinforcement
learning with function approximation. In Proceedings of the Twelfth International Conference on Machine Learning, pp. 30–37.
Bertsekas, Dimitri P., Tsitsiklis. J. (1996). Neuro-Dynamic
Programming. Athena Scientific, 1996.
Boutilier, C., Dearden, R., Goldszmidt, M. (2000).
Stochastic dynamic programming with factored representations. Artificial Intelligence 121: 49–107.
Bowling, M., Geramifard, A., Wingate, D. (2008). Sigma
point policy iteration. In Proceedings of the Seventh
International Conference on Autonomous Agents and
Multiagent Systems.
Boyan, J. A. (1999). Least-squares temporal difference
learning. In Proceedings of the Sixteenth International
Conference on Machine Learning, 49–56.
Boyan, J. A. (2002). Technical update: Least-squares temporal difference learning. Machine Learning, 49:233–
246.
Bradtke, S., Barto, A. G. (1996). Linear least-squares al-

Finally, we would like to note that we have done extensive experimental work (not reported here) attempting to
adapt least squares methods such as LSTD to online control domains, in particular to the Mountain Car problem. A
major difficulty with these methods is that they place equal
weight on all past data whereas, in a control setting, the policy changes and older data becomes less relevant and may
even be misleading. Although we have tried a variety of
forgetting strategies, it is not easy to obtain online control
performance with these methods that is superior to modelfree methods. One reason we consider the Dyna approach
to be promising is that no special changes are required for
this case; it seems to adapt much more naturally and effectively to the online control setting.


