
Computing the probability of evidence even with
known error bounds is NP-hard. In this paper we
address this hard problem by settling on an easier
problem. We propose an approximation which
provides high confidence lower bounds on probability of evidence but does not have any guarantees in terms of relative or absolute error. Our
proposed approximation is a randomized importance sampling scheme that uses the Markov inequality. However, a straight-forward application
of the Markov inequality may lead to poor lower
bounds. We therefore propose several heuristic
measures to improve its performance in practice.
Empirical evaluation of our scheme with stateof-the-art lower bounding schemes reveals the
promise of our approach.

1

Introduction

Computing the probability of evidence even with known
error bounds is NP-hard [Dagum and Luby, 1993]. In this
paper we address this hard problem by proposing an approximation that gives high confidence lower bounds on
the probability of evidence but does not have any guarantees of relative or absolute error.
Previous
work
on
bounding
the
probability
of
evidence
comprises
of
deterministic
approximations
[Dechter and Rish, 2003,
Leisink and Kappen, 2003, Bidyuk and Dechter, 2006a]
and sampling based randomized approximations
[Cheng, 2001, Dagum and Luby, 1997].
An approximation algorithm for computing the lower bound is
deterministic if it always outputs an approximation that
is a lower bound. On the other hand, an approximation
algorithm is randomized if the approximation fails with
some probability δ > 0. The work in this paper falls under
the class of randomized approximations.

Randomized
approximations
[Cheng, 2001,
Dagum and Luby, 1997] use known inequalities such
as the Chebyshev and the Hoeffding inequalities
[Hoeffding, 1963] for lower (and upper) bounding
the probability of evidence. The Chebyshev and Hoeffding
inequalities provide bounds on how the sample mean
of N independently and identically distributed random
variables deviates from the actual mean. The main idea
in [Cheng, 2001, Dagum and Luby, 1997] is to express
the problem of computing the probability of evidence as
the problem of computing the mean (or expected value)
of independent random variables and then use the mean
over the sampled random variables to bound the deviation
from the actual mean. The problem with these previous
approaches is that the number of samples required to
guarantee high confidence lower (or upper) bounds is
inversely proportional to the probability of evidence (or
the actual mean). Therefore, if the probability of evidence
is arbitrarily small (e.g. < 10−20 ), a large number of
samples (approximately 1019 ) are required to guarantee
the correctness of the bounds.
We alleviate this problem, which arises from the dependence of the Hoeffding and Chebyshev inequalities on
the number of samples N, by using the Markov inequality which is independent of N. Recently, the Markov inequality was used to lower bound the number of solutions of a Satisfiability formula [Gomes et al., 2007] showing good empirical results. We adapt this scheme to compute lower bounds on probability of evidence and extend
it in several ways. First, we show how importance sampling can be used to obtain lower bounds using the Markov
inequality. Second, we address the difficulty associated
with the approach presented in [Gomes et al., 2007] in that
with the increase in number of samples the lower bound is
likely to decrease by proposing several parametric heuristic methods. Third, we show how the probability of evidence of belief networks with zero probabilities can be efficiently estimated by using the Markov inequality in conjunction with a recently proposed SampleSearch scheme
[Gogate and Dechter, 2007]. Finally, we provide empirical
results demonstrating the potential of our new scheme by

142

GOGATE ET AL.

comparing against state-of-the-art bounding schemes such
as bound propagation [Leisink and Kappen, 2003] and its
improvements [Bidyuk and Dechter, 2006b].
The rest of the paper is organized as follows. In section 2,
we discuss preliminaries and related work. In section 3, we
present our lower bounding scheme and propose various
heuristics to improve it. In section 4, we describe how the
SampleSearch scheme can be used within our lower bounding scheme. Experimental results are presented in section
5 and we end with a summary in section 6.

2

Preliminaries and Previous work

We represent sets by bold capital letters and members of a
set by capital letters. An assignment of a value to a variable
is denoted by a small letter while bold small letters indicate
an assignment to a set of variables.
Definition 1. (belief networks) A belief network (BN) is a
graphical model P = hZ, D, Pi, where Z = {Z1 , . . . , Zn }
is a set of random variables over multi-valued domains
D = {D1 , . . . , Dn }. Given a directed acyclic graph G over
Z, P = {Pi }, where Pi = P(Zi |pa(Zi )) are conditional probability tables (CPTs) associated with each Zi . The set
pa(Zi ) is the set of parents of the variable Zi in G. A belief network represents a probability distribution over Z,
P(Z) = ∏ni=1 P(Zi |pa(Zi )). An evidence set E = e is an instantiated subset of variables.
Definition 2 (Probability of Evidence). Given a belief network P and evidence E = e, the probability of evidence
P(E = e) is defined as:
n

P(e) =

∑ ∏ P(Z j |pa(Z j ))|E=e

(1)

Z\E j=1

To compute the probability of evidence by importance sampling, we use the substitution:
n

f (x) = P(z, e) =

∏ P(Z j |pa(Z j ))|E=e ,
j=1

X = Z\E

(4)

For the rest of the paper, assume M = P(e) and f (x) =
∏nj=1 P(Z j |pa(Z j ))|E=e .
Several choices are available for the proposal distribution Q(x) ranging from the prior distribution as in likelihood weighting to more sophisticated alternatives such as
IJGP-Sampling [Gogate and Dechter, 2005] and EPIS-BN
[Yuan and Druzdzel, 2006] where the output of belief propagation is used to compute the proposal distribution.
As in prior work [Cheng and Druzdzel, 2000], we assume
that the proposal distribution is expressed in a factored
product form dictated by the belief network: Q(X) =
∏ni=1 Qi (Xi |X1 , . . . , Xi−1 ) = ∏ni=1 Qi (Xi |Yi ), where Yi ⊆
{X1 , . . . , Xi−1 }, Qi (Xi |Yi ) = Q(Xi |X1 , . . . , Xi−1 ) and |Yi | < c
for some constant c. When Q is given in a product form ,
we can generate a full sample from Q as follows. For i
= 1 to n, sample Xi = xi from the conditional distribution
Q(Xi |X1 = x1 , . . . , Xi−1 = xi−1 ) and set Xi = xi . This is often referred to as an ordered Monte Carlo sampler.
2.2

Related Work

[Dagum and Luby, 1997] provide an upper bound on the
number of samples N required to guarantee that for any
b computed using Equation 3 apε , δ > 0, the estimate M
proximates M with relative error ε with probability at least
1 − δ . Formally,
b ≤ M(1 + ε )] > 1 − δ
Pr[M(1 − ε ) ≤ M

(5)

The specific bound on N that the authors derive is:
The notation h(Z)|E=e stands for a function h over Z \ E
with the assignment E = e.
2.1

N≥

Importance sampling [Rubinstein, 1981] is a simulation
technique commonly used to evaluate the following sum:
M = ∑x∈X f (x) for some real function f . The idea is to
generate samples x1 , . . . , xN from a proposal distribution Q
(satisfying f (x) > 0 ⇒ Q(x) > 0) and then estimate M as
follows:

b= 1
M
N

N

∑ w(xi ) , where

i=1

w(xi ) =

f (xi )
Q(xi )

4
2
ln
Mε 2 δ

(6)

These bounds were later improved by [Cheng, 2001] to
yield:

Computing Probability of Evidence Using
Importance Sampling

f (x)
f (x)
Q(x) = EQ [
]
M = ∑ f (x) = ∑
Q(x)
Q(x)
x∈X
x∈X

N≥

(2)

(3)

w is often referred to as the sample weight. It is known
b = M [Rubinstein, 1981].
that the expected value E(M)

1
1
2
ln
M (1 + ε )ln(1 + ε ) − ε δ

(7)

In both these bounds (Equations 6 and 7) N is inversely
proportional to M and therefore when M is small, a large
number of samples are required to achieve an acceptable
confidence level (1 − δ ) > 99%.
A bound on N is required because [Dagum and Luby, 1997,
Cheng, 2001] use Chebyshev and Hoeffding inequalities
which depend on N for correctness. Instead, we could use
the Markov inequality which is independent of N and still
achieve high confidence lower bounds. The independence
from N allows us to use even a single sample to derive
lower bounds. The only caveat is that our proposed method
does not have any guarantees in terms of relative error ε .
We describe our method in the next section.

GOGATE ET AL.

3

Markov Inequality to lower bound P(e)

Definition 3 (Markov Inequality). For any random variable
X and k > 1, Pr(X ≥ kE[X]) ≤ 1k
[Gomes et al., 2007] show how the Markov inequality can
be used to obtain probabilistic lower bounds on the number
of solutions of a satisfiability/constraint satisfaction problem. Using the same approach, we present a small modification of importance sampling for obtaining lower bounds
on the probability of evidence (see Algorithm 1). The algorithm generates k independent samples from a proposal
(x)
distribution Q and returns the minimum αfQ(x)
(minCount
in Algorithm 1) over the k-samples.
Algorithm 1 Markov-LB ( f , Q, k, α > 1)
1:
2:
3:
4:
5:
6:

minCount ← ∞
for i = 1 to k do
Generate a sample xi from Q
f (xi )
IF minCount > α ∗Q(xi ) THEN minCount =
end for
Return minCount

f (xi )
α ∗Q(xi )

T HEOREM 1 (Lower Bound). With probability of at least
1 − 1/α k , Markov-LB returns a lower bound on M = P(e)
Proof. Consider an arbitrary sample xi .
It is clear
from the discussion in section 2 that the expected value
of f (xi )/Q(xi ) is equal to M.
Therefore, by the
f (xi )
Pr( α ∗Q(x
i)

Markov inequality, we have
> M) < 1/α .
Since, the generated k samples are independent, the
f (xi )
k
probability Pr(minki=1 α ∗Q(x
i ) > M) < 1/α and therefore
i

f (x )
1
Pr(minki=1 [ α ∗Q(x
i ) ] ≤ M) > 1 − α k .

The problem with Algorithm 1 is that unless the variance
of f (x)/Q(x) is very small, we expect the lower bound to
decrease with increase in the number of samples k. In practice, given a required confidence of δ = α k , one can decrease α as k is increased.
Note that each sample in Algorithm 1 provides a lower
bound with probability > (1 − 1/α ). We can replace the
sample by any procedure that provides a lower bound with
probability > (1 − 1/α ) and therefore in the following
we propose several heuristic methods to compute a lower
bound with probability > (1 − 1/α ).
3.1

3.2

143

Using the maximum over N samples

We can even use the maximum instead of the average over
the N i.i.d samples and still achieve a confidence of 1 −
1/α . Given a set of N independent events such that each
event occurs with probability > (1 − 1/β ), the probability
that all events occur is > (1 − 1/β )N . Consequently, we
can prove that:
Proposition 1. Given N i.i.d. samples generated from Q,
(xi )
N
Pr(maxNi=1 [ βfQ(x
i ) ] < M) > (1 − 1/β ) .
Therefore, by setting (1 − 1/β )N = 1 − 1/α (i.e. β =
1/[1 − (1 − 1/α )1/N ]) and recording the maximum value of
f (xi )/β Q(xi ) over the N samples, we can achieve a lower
bound on M with confidence (1 − 1/α ).
Again the problem with this method is that increasing the
number of samples increases β and consequently the lower
bound decreases. However, when the variance of the random variables f (xi )/Q(xi ) is large, the maximum value is
likely to be larger than the sample average. Another approach to utilize the maximum over the N samples is to use
the martingale inequalities.
3.3

Using the martingale Inequalities

In this subsection, we show how the martingale theory can
be used to obtain lower bounds on P(e).
Definition 4 (Martingale). A sequence of random variables
X1 , . . . , XN is a martingale with respect to another sequence
Z1 , . . . , ZN defined on a common probability space Ω iff
E[Xi |Z1 , . . . , Zi−1 ] = Xi−1 for all i.
Given i.i.d. samples (x1 , . . . , xN ) generated from Q, note
i

p
f (x )
that the sequence Λ1 , . . . , ΛN , where Λ p = ∏i=1
MQ(xi )
forms a martingale as shown below:

E[Λ p |x1 , . . ., x p−1 ]

E[Λ p−1 ∗

=
p

f (x )
1
p−1 ] = 1,
E[ M∗Q(x
p ) |x , . . . , x

Because,

we

have

|x1 , . . . , x p−1 ]

E[Λ p
= Λ p−1 as required. The expected
value E[Λ1 ] = 1 and for such martingales which have
a mean of 1, [Breiman, 1968] provides the following
extension of the Markov inequality:
Pr(maxN
i=1 Λi > α ) ≤

Using Average over N samples

One obvious way is to use the importance sampling estimab Because E[M]
b = M, by Markov inequality M/
b α is a
tor M.
lower bound of M with confidence 1 − 1/α . As the number
of samples increases, the average becomes more stable and
is likely to increase the minimum value over the k iterations
of Algorithm 1.

f (x p )
|x1 , . . ., x p−1 ]
M ∗ Q(x p )
f (x p )
Λ p−1 ∗ E[
|x1 , . . ., x p−1 ]
M ∗ Q(x p )

=

1
α

(8)

and therefore,
i

f (x j )
1
) > α) ≤
j)
MQ(x
α
j=1

Pr((maxN
i=1 ∏

(9)

From
Inequality
9,
we
can
see
that
f (x j ) 1/i
1
i
N
maxi=1 ( α ∏ j=1 [ Q(x j ) ]) is a lower bound on M with a

144

GOGATE ET AL.

confidence of (1 − 1/α ). In general one could use any
randomly selected permutation of the samples (x1 , . . . , xN )
and apply inequality 9.
Another related extension of Markov inequality for martingales deals with the order statistics of the sample. Let
f (x(1) )
MQ(x(1) )

(N)

(2)

f (x )
f (x )
≤ MQ(x
(2) ) ≤ . . . ≤ MQ(x(N) ) be the order statistics
of the sample. Using martingale theory, [Kaplan, 1987]
proved that the random variable

f (x(N− j+1) )

i

Θ∗ = maxN
i=1 ∏

j=1

N
i

M ∗ Q(x(N− j+1) ) ∗

satisfies the inequality Pr(Θ∗ > α ) ≤ 1/α . Therefore,
f (x(N− j+1) )

i

Pr((maxN
i=1 ∏

j=1

From
maxNi=1 ( α1

M ∗ Q(x(N− j+1) ) ∗

Inequality

10,

f (x(N− j+1) )
∏ij=1 [ Q(x(N− j+1) ) N ])1/i
( )

N
i

we
is

) > α) ≤

can
a

1
α

(10)

see

that

lower

bound

i

on M with a confidence of (1 − 1/α ).
To summarize in this section, we have proposed four
heuristic ways to improve Algorithm 1 (1) The average
method, (2) The max method and (3) The martingale random permutation method and (4) The martingale order
statistics method.

4

In the following example, we show how constraints can be
extracted from the CPTs.

Overcoming Rejection: Using
SampleSearch with Markov-LB

One problem with importance sampling based algorithms
is the so-called rejection problem and in this section
we discuss how to alleviate this problem in MarkovLB by using the recently proposed SampleSearch scheme
[Gogate and Dechter, 2007].
4.1

The rejection problem has been largely ignored in the importance sampling community except the work on adaptive
importance sampling techniques [Hernández et al., 1998,
Cheng and Druzdzel, 2000, Yuan and Druzdzel, 2006]. In
[Gogate and Dechter, 2005], we initiated a new approach
of reducing the amount of rejection by using constraint processing methods. The main idea is to express the zero probabilities in the belief network using constraints.
Definition 5 (constraint network). A constraint network
(CN) is defined by a 3-tuple, R = hZ, D, Ci, where Z is
a set of variables Z = {Z1 , . . . , Zn }, associated with a set
of discrete-valued domains, D = {D1 , . . . , Dn }, and a set of
constraints C = {C1 , . . . ,Cr }. Each constraint Ci is a relation RSi defined on a subset of variables Si ⊆ Z. The
relation denotes all compatible tuples of the cartesian product of the domains of Si . A solution is an assignment of
values to all variables z = (Z1 = z1 , . . . , Zn = zn ), zi ∈ Di ,
such that z belongs to the natural join of all constraints i.e.
z ∈ RS1 ⊲⊳ . . . ⊲⊳ RSr . The constraint satisfaction problem
(CSP) is to determine if a constraint network has a solution,
and if so, to find one. When we write R(z), we mean that
z satisfies all the constraints in R.

Rejection Problem

Given a positive belief network that expresses the probability distribution P(Z) = ∏ni=1 P(Zi |Z1 , . . . , Zi−1 ) and an
empty evidence set, all full samples generated by the ordered Monte Carlo sampler along the ordering Z1 , . . . , Zn
are guaranteed to have a non-zero weight. However, in
presence of both zero probabilities and evidence the ordered Monte Carlo sampler may generate samples which
have zero weight because the sample may conflict with the
evidence and zero probabilities. Formally, if the proposal
distribution Q is such that the probability of sampling an assignment from the set {x| f (x) = 0} is substantially larger
than the probability of sampling an assignment from the set
{x| f (x) > 0}, a large number of samples generated from Q
will have zero weight. In fact, in the extreme case if no
positive weight samples are generated, the lower bound reported by the Markov-LB scheme will be trivially zero.

Figure 1: An example Belief Network.
Example 1. Figure 1 presents a belief network over 6
binary variables. The CPTs associated with C and G
have zero probabilities. The constraint extracted from
the CPT of C is the relation RA,C = {(0, 0), (1, 0), (1, 1)}
while the CPT of G yields the constraint relation RD,F,G =
{(0, 0, 0)(0, 1, 0), (1, 0, 1), (1, 1, 0)}. Namely, each ”0” tuple in a CPT corresponds to a no-good, and therefore does
not appear in the corresponding relation.
Our importance sampling scheme called IJGP-Sampling
[Gogate and Dechter, 2005] uses constraint propagation to
reduce rejection. Given a partial sample (x1 , . . . , x p ), constraint propagation prunes values in the domains of future variables Xp+1 , . . . , Xn which are inconsistent with
(x1 , . . . , x p ).
However, we observed recently that when a substantial number of zero probabilities are present or when

GOGATE ET AL.
there are many evidence variables, the level of constraint
propagation achieved by IJGP is not effective and often
few/no consistent samples will be generated. Therefore in
[Gogate and Dechter, 2007], we proposed a more aggressive approach that searches explicitly for a non-zero weight
sample yielding the SampleSearch scheme.
Algorithm 2 SampleSearch
Input: The proposal distribution Q(x) = ∏ni=1 Qi (xi |x1 , . . ., xi−1 ),
hard constraints R that represent zeros in f (x)
Output: A sample x = (x1 , . . ., xn ) satisfying all constraints in R
1: i = 1, D′i = Di (copy domains), Q′i (Xi ) = Qi (Xi ) (copy distribution), x = φ
2: while 1 ≤ i ≤ n do
3: if D′i is not empty then
4:
Sample Xi = xi from Q′i and remove it from D′i
5:
if (x1 , . . ., xi ) violates any constraints in R then
6:
set Qi (xi |x1 , . . ., xi−1 ) = 0 and normalize Q′i
7:
Goto Step 3
8:
end if
9:
x = x ∪ xi , i = i + 1, D′i = Di , Q′i (Xi |x1 , . . ., xi−1 ) =
Qi (Xi |x1 , . . ., xi−1 )
10: else
11:
x = x\xi−1 .
12:
set Q′i−1 (Xi−1 = xi−1 |x1 , . . ., xi−2 ) = 0 and normalize.
13:
set i = i − 1
14: end if
15: end while
16: Return x

4.2

The SampleSearch scheme

An ordered Monte Carlo sampler samples variables in the
order hX1 , . . . , Xn i from the proposal distribution Q and rejects a partial or full sample (x1 , . . . , xi ) if it violates any
constraints in R (R models zero probabilities in f ). Upon
rejecting a (partial or full) sample, the sampler starts sampling anew from the first variable in the ordering. Instead, we propose the following modification. We can set
Qi (Xi = xi |x1 , . . . , xi−1 ) = 0 (to reflect that (x1 , . . . , xi ) is not
consistent), normalize Qi and re-sample Xi from the normalized distribution. The newly sampled value may be
consistent in which case we can proceed to variable Xi+1
or it may be inconsistent. If we repeat the process we
may reach a point where Qi (Xi |x1 , . . . , xi−1 ) is 0 for all values of Xi . In this case, (x1 , . . . , xi−1 ) is inconsistent and
therefore we need to change the distribution at Xi−1 by setting Qi−1 (Xi−1 = xi−1 |x1 , . . . , xi−2 ) = 0, normalize and resample Xi−1 . We can repeat this process until a globally
consistent full sample that satisfies all constraints in R is
generated. By construction, this process always yields a
globally consistent full sample.
Our proposed SampleSearch scheme is described as Algorithm 2. It is a depth first backtracking search (DFS) over
the state space of consistent partial assignments searching
for a solution to a constraint satisfaction problem R, whose
value selection is guided by Q.

145

It can be proved that SampleSearch generates independently and identically distributed samples from the
backtrack-free distribution which we define below.
Definition 6 (Backtrack-free distribution ). Given a distribution Q(X) = ∏Ni=1 Qi (Xi |X1 , . . . , Xi−1 ), an ordering O =
hx1 , . . . , xn i and a set of constraints R, the backtrack-free
distribution QR is the distribution:
n

QR (x) = ∏ QRi (xi |x1 , . . ., xi−1 )

(11)

i=1

where QRi (xi |x1 , . . . , xi−1 ) is given by:
QRi (xi |x1 , . . ., xi−1 ) =

Qi (xi |x1 , . . ., xi−1 )
1 − ∑x′i ∈Bi Qi (x′i |x1 , . . ., xi−1 )

(12)

where Bi = {x′i ∈ Di |(x1 , . . . , xi−1 , x′i ) can not be extended
to a solution of R} and xi ∈
/ Bi . Note that by definition,
f (x) = 0 ⇒ QR (x) = 0 and vice versa.

T HEOREM 2. [Gogate and Dechter, 2007] SampleSearch
generates independently and identically distributed samples from the backtrack-free distribution.
Given that the backtrack-free distribution is the sampling
distribution of SampleSearch, we can use SampleSearch
within the importance sampling framework as follows. Let
(x1 , . . . , xN ) be a set of i.i.d samples generated by SampleSearch. Then we can estimate M = ∑x∈X f (x) as:
N
i
b = 1 ∑ f (x )
M
N i=1 QR (xi )

(13)

Although SampleSearch was described using the naive
backtracking algorithm, in principle we can integrate any
systematic CSP/SAT solver that employs advanced search
schemes with sampling through our SampleSearch scheme.
Since the current implementations of SAT solvers are very
efficient, we represent the zero probabilities in the belief network using cnf (SAT) expressions and use Minisat
[Sorensson and Een, 2005] as our SAT solver.
Computing QR (x) given a sample x:
From Definition 6, we notice that to compute the components QRi (xi |x1 , . . . , xi−1 ) for a sample x = (x1 , . . . , xn ), we
have to determine the set Bi = {x′i ∈ Di |(x1 , . . . , xi−1 , x′i ) can
not be extended to a solution of R}. The set Bi can be
determined by checking for each x′i ∈ Di if the partial assignment (x1 , . . . , xi−1 , x′i ) can be extended to a solution of
R. To speed up this checking at each branch point, we use
the Minisat SAT solver [Sorensson and Een, 2005]. Minisat should be invoked a maximum of O(n ∗ (d − 1)) times
where n is the number of variables and d is the maximum
domain size.
In [Gogate and Dechter, 2007], we found that the SampleSearch based importance sampling scheme outperforms all
competing approaches when a substantial number of zero

146

GOGATE ET AL.

probabilities are present in the belief network. Therefore,
we employ SampleSearch as a sampling technique within
Markov-LB when a substantial number of zero probabilities are present. It should be obvious that when Samplei
Search is used, we should use QfR(x(x)i ) as a random variable
instead of

5
5.1

f (xi )
Q(xi )

P(c1 , ..., cq , e), q < |C|, for a polynomial number of partially instantiated tuples of subset C, resulting in:
h

k′

i=1

i=1

L
P(e) ≥ ∑ P(ci , e) + ∑ PBP
(ci1 , ..., ciq , e)

(15)

L (c , ..., c , e) is obtained using
where lower bound PBP
q
1
bound propagation. Although bound propagation bounds
marginal probabilities, it can be used to bound any joint
probability P(z) as follows:

in the Markov-LB scheme.

Empirical Evaluation
Competing Algorithms

L
L
PBP
(z) = ∏ PBP
(zi |z1 , ..., zi−1 )

(16)

i

Markov-LB with SampleSearch and IJGP-sampling:
The performance of importance sampling based algorithms is highly dependent on the proposal distribution
[Cheng and Druzdzel, 2000, Yuan and Druzdzel, 2006]. It
was shown that computing the proposal distribution from
the output of a Generalized Belief Propagation scheme
of Iterative Join Graph Propagation (IJGP) yields better empirical performance than other available choices
[Gogate and Dechter, 2005]. Therefore, we use the output
of IJGP to compute the proposal distribution Q. The complexity of IJGP is time and space exponential in its i-bound,
a parameter that bounds cluster sizes. We use a i-bound of 3
in all our experiments. The preprocessing time for computing the proposal distribution using IJGP (i = 3) was negligible (< 2 seconds for the hardest instances).
We experimented with four versions of Markov-LB (a)
Markov-LB as given in Algorithm 1, (b) Markov-LB with
the average heuristic, (c) Markov-LB with the martingale
random permutation heuristic and (d) Markov-LB with the
martingale order statistics heuristic. In all our experiments,
we set α = 2 and k = 7 which gives us a correctness confidence of 1 − 1/27 ≈ 99.2% on our lower bounds. Finally,
we set N = 100 for the heuristic methods. Also note that
when the belief network is positive we use IJGP-sampling
but when the belief network has zero probabilities, we use
SampleSearch whose initial proposal distribution Q is computed from the output of IJGP.
Bound Propagation with Cut-set Conditioning We also
experimented with the state of the art any-time bounding
scheme that combines sampling-based cut-set conditioning
and bound propagation [Leisink and Kappen, 2003] and
which is a part of Any-Time Bounds framework for bounding posterior marginals [Bidyuk and Dechter, 2006a].
Given a subset of variables C ⊂ X\E, we can compute
P(e) exactly as follows:
k

P(e) = ∑ P(ci , e)

(14)

i=1

The lower bound on P(e) is obtained by computing
P(ci , e) for h high probability tuples of C (selected
through sampling) and bounding the remaining probability mass by computing a lower bound PL (c1 , ..., cq , e) on

L (z |z , ..., z
where lower bound PBP
i 1
i−1 ) is computed
directly by bound propagation.
We use here the
same variant of bound propagation described in
[Bidyuk and Dechter, 2006b] that is used by the AnyTime Bounds framework. The lower bound obtained by
Eq. 15 can be improved by exploring a larger number of
tuples h. After generating h tuples by sampling, we can
stop the computation at any time after bounding p < k′ out
of k′ partially instantiated tuples and produce the result.

In our experiments we run the bound propagation with cutset conditioning scheme until convergence or until a stipulated time bound has expired. Finally, we should note that
the bound propagation with cut-set conditioning scheme
provides deterministic lower and upper bounds on P(e)
while our Markov-LB scheme provides only a lower bound
and it may fail with a probability δ ≤ 0.01.
5.1.1 Evaluation Criteria
We experimented with six sets of benchmark belief networks (a) Alarm networks (b) CPCS networks, (c) Randomly generated belief networks, (d) Linkage networks,
(e) Grid networks and (f) Two-layered deterministic networks. Note that only linkage, grid and deterministic networks have zero probabilities.
On each network instance, we compare log relative error
between the exact probability of evidence and the lower
bound reported by the competing techniques. Formally, if
Pexact is the actual probability of evidence and Papp is the
approximate probability of evidence, we compute the logrelative error as follows:
∆ = Abs(

log(Pexact ) − log(Papp )
)
log(Pexact )

(17)

Note that the exact P(e) for most instances is available from
the UAI competition web-site 1 . The exact P(e) for the
two layered deterministic networks was computed using
AND/OR search [Dechter and Mateescu, 2004].
We compute the log relative error instead of the usual relative error because when the probability of evidence is
1 http://ssli.ee.washington.edu/∼bilmes/uai06InferenceEvaluation/

GOGATE ET AL.

Table 1: Results on various benchmarks. The columns Min, Avg,
Per and Ord give the log-relative-error ∆ for the minimum, the
average, the martingale random permutation and the martingale
order statistics heuristics respectively. The last two columns provide log-relative-error ∆ and time for the bound propagation with
cut-set conditioning scheme. In the first column N is the number
of variables, D is the maximum domain size and E is the number
of evidence variables. Time is in seconds. The column best LB
reports the best lower bound reported by all competing scheme
whose log-relative error is highlighted in each row.

(N, D, |E|)
Alarm
(100,2,36)
(100,2,51)
(125,2,55)
(125,2,71)
(125,2,46)
CPCS
(360,2,20)
(360,2,30)
(360,2,40)
(360,2,50)
(422,2,20)
(422,2,30)
(422,2,40)
(422,2,50)
Random
(53,50,6)
(54,50,5)
(57,50,6)
(58,50,8)
(76,50,15)

Exact
P(e)

IJGP-sampling-Markov-LB
Bound
Min Avg Per Ord
Propagation
∆
∆
∆
∆ Time ∆ Time

Best
LB

2.8E-13
3.6E-18
1.8E-19
4.3E-26
8.0E-18

0.157 0.031 0.040 0.059
0.119 0.023 0.040 0.045
0.095 0.020 0.021 0.030
0.124 0.016 0.024 0.030
0.185 0.023 0.061 0.064

0.2
0.1
0.2
0.2
0.1

0.090
0.025
0.069
0.047
0.102

22.3
5.6
36.0
19.3
31.6

1.1E-13
1.4E-18
7.7E-20
1.6E-26
3.3E-18

1.3E-25
7.6E-22
1.2E-33
3.4E-38
7.2E-21
2.7E-57
6.9E-87
1.4E-73

0.012 0.012 0.000 0.001
0.045 0.015 0.010 0.010
0.010 0.009 0.000 0.000
0.022 0.009 0.002 0.000
0.028 0.016 0.001 0.001
0.005 0.005 0.000 0.000
0.003 0.003 0.000 0.000
0.007 0.004 0.000 0.000

1.2
1.2
1.2
1.2
8.4
8.3
8.1
8.5

0.002
0.000
0.000
0.000
0.002
0.000
0.001
0.001

13.2
16.3
26.8
19.2
120
120
120
120

1.3E-25
7.6E-22
1.2E-33
3.4E-38
6.8E-21
2.7E-57
6.9E-87
1.3E-73

0.235 0.029 0.063 0.025 0.8
0.408 0.036 0.095 0.013 0.6
0.131 0.024 0.013 0.024 0.8
0.521 0.022 0.079 0.041 0.9
0.039 0.007 0.007 0.012 2.0
SampleSearch-Markov-LB
Exact Min Avg Per Ord
P(e)
∆
∆
∆
∆ Time

4.0E-11
2.1E-09
1.9E-11
1.6E-14
1.5E-26

0.028 1.5
0.131 4.6
0.147 5.9
0.134 13.0
0.056 19.1
Bound
Propagation
∆ Time

(N, D, |E|)
Grid
(1156,2,120) 9.1E-12 0.256 0.040 0.106 0.047 3.5 0.946
(1444,2,150) 2.4E-12 0.208 0.094 0.111 0.107 5.3 3.937
(1444,2,150) 3.5E-15 0.269 0.090 0.131 0.097 5.3 3.067
(1444,2,150) 4.9E-10 0.243 0.093 0.090 0.108 4.3 5.380
(1444,2,150) 4.6E-11 0.103 0.086 0.065 0.069 5.7 4.458
(1444,2,150) 5.2E-14 0.127 0.100 0.079 0.098 3.7 3.456
Linkage
(777,36,78) 2.8E-54 0.243 0.176 0.169 0.153 8.6 1.022
(2315,36,159) 8.8E-72 0.390 0.340 0.347 0.340 40.7 1.729
(1740,36,202) 1.4E-111 0.438 0.235 0.323 0.235 30.8 0.984
(2155,36,252) 7.5E-151 0.196 0.128 0.196 0.128 41.6 0.298
(2140,36,216) 6.1E-114 0.419 0.311 0.354 0.311 48.3 1.560
(749,36,66) 2.2E-45 0.954 0.780 0.949 0.761 9.3 5.314
(1820,36,155) 2.1E-91 0.258 0.215 0.236 0.208 45.8 2.209
(2155,36,169) 1.4E-110 0.475 0.374 0.435 0.374 128.2 0.712
(1020,36,135) 2.8E-79 0.262 0.198 0.225 0.185 18.4 1.385
Two-layered
(1000,2,800) 8.8E-26 0.059 0.024 0.032 0.029 12.2 11.01
(1000,2,800) 3.2E-28 0.076 0.030 0.045 0.042 10.5 9.95
(1000,2,800) 1.2E-27 0.061 0.019 0.020 0.024 7.7 10.19
(1000,2,800) 4.3E-26 0.109 0.050 0.060 0.061 13.0 10.87
(1000,2,800) 1.2E-26 0.115 0.036 0.055 0.046 20.5 10.61

2.2E-11
1.6E-09
1.4E-11
8.1E-15
9.4E-27
Best
LB

33.9
600
600
600
600
600

3.3E-12
2.0E-13
1.7E-16
7.2E-11
9.6E-12
4.6E-15

3600
3600
3600
3600
3600
3600
3600
3600
3600

1.8E-62
6.3E-96
1.2E-137
4.2E-170
4.0E-149
2.4E-79
3.0E-110
1.2E-151
8.9E-94

3600
3600
3600
3600
3600

2.2E-26
4.9E-29
3.9E-28
2.3E-27
1.4E-27

extremely small the relative error between the exact and
the approximate probability of evidence will be arbitrarily
close to 1 and we would need a large number of digits to
determine the best performing competing scheme.
5.2

Results

Our results are summarized in Table 1. We see that our
new strategy of Markov-LB scales well with problem size
and provides good quality high-confidence lower bounds

147

on most problems. It clearly outperforms the bound propagation with cut-set conditioning scheme. We discuss the
results in detail below.
The Alarm networks The Alarm networks are one of the
earliest belief networks designed by medical experts for
monitoring patients in intensive care. The evidence in these
networks was set at random. These networks have between 100-125 binary nodes. We can see that Markov-LB
is slightly superior to the bound propagation based scheme
accuracy-wise, but is far more efficient time-wise. Among
the different versions of Markov-LB, the average heuristic
performs better than the martingale heuristics. The minimum heuristic is the worst performing heuristic.
The CPCS networks The CPCS networks are derived
from the Computer-based Patient Case Simulation system
[Pradhan et al., 1994]. The nodes of CPCS networks correspond to diseases and findings and conditional probabilities
describe their correlations. The CPCS360b and CPCS422b
networks have 360 and 422 variables respectively. We report results on the two networks with 20,30,40 and 50 randomly selected evidence nodes. We see that the lower
bounds reported by the bound propagation based scheme
are slightly better than Markov-LB on the CPCS360b networks but they take far more time. However, on the
CPCS422b networks, Markov-LB has better performance
than the bound propagation based scheme. The martingale
heuristics (the random permutation and the order statistics)
have better performance than the average heuristic. Again,
the minimum heuristic has the worst performance. Note
that we stop each algorithm after 2 mins of run-time if the
algorithm has not terminated by itself.
Random networks The random networks are randomly
generated graphs available from the UAI competition website. The evidence nodes are generated at random. The
networks have between 53 and 76 nodes and the maximum
domain size is 50. We see that Markov-LB is better than the
bound propagation based scheme on all random networks.
The random permutation and the ordered statistics martingale heuristics are slightly better than the average heuristic
on most instances.
Grid Networks The Grid instances are also available from
the UAI competition web-site. All nodes in the Grid are
binary and evidence nodes are selected at random. The
Grid networks have substantial amount of determinism and
therefore we employ the SampleSearch based importance
sampling scheme to compute the lower bound. Here, we
stop each algorithm after 10 minutes if it has not terminated by itself. We notice that the performance of MarkovLB is significantly better than the bound propagation based
scheme on all instances.
Linkage networks The linkage instances are generated by converting a Pedigree to a Bayesian network
[Fishelson and Geiger, 2003]. These networks have be-

148

GOGATE ET AL.

tween 777-2315 nodes with a maximum domain size of
36 and are much larger than the Alarm, the CPCS, and
the random networks. On these networks, we ran each
algorithm until termination or until a time-bound of 1hr
expired. The Linkage instances have a large number of
zero probabilities which makes them hard for traditional
importance sampling based schemes because of the rejection problem. Therefore, in all our experiments on linkage instances we used the SampleSearch based importance
sampling scheme. On Linkage instances, IJGP-sampling
did not return a single non-zero weight sample (not shown
in Table 1) in more than one-hour of run-time yielding a
lower bound of 0. We see that the bound propagation based
scheme yields inferior lower bounds as compared to the
SampleSearch based Markov-LB scheme. However, we
notice that the log-relative-error is significantly higher for
Markov-LB on the linkage instances than the Alarm, the
CPCS, and the random instances. We suspect that this is
because the quality of the proposal distribution computed
from the output of IJGP is not as good on the linkage instances as compared to other instances. Finally, we notice
that the average and the martingale-heuristics perform better than the min heuristic on all instances with the martingale order statistics heuristic being the best performing
heuristic.

using the SampleSearch scheme. Our experimental results
on a range of benchmarks show that our new lower bounding scheme outperforms the state-of-the-art bound propagation scheme and provides high confidence good quality
lower bounds on most instances.

Deterministic two-layered networks Our final domain
is that of completely deterministic two-layered networks.
Here, the first layer is a set of root nodes connected to a second layer of leaf nodes. The CPTs of the root node are such
that each value in the domain is equally likely while the
CPTs associated with the leaf nodes are deterministic i.e.
each CPT entry is either one or a zero. All nodes are binary.
The evidence set is all the leaf nodes instantiated to a value.
We experimented with 5 randomly generated 1000-variable
two-layered networks each with 800 leaf nodes which are
set to true (evidence). We employ the SampleSearch based
importance sampling scheme for these networks because
these instances have zero probabilities. We see that the average and the martingale heuristics are the best performing heuristics while the min-heuristic performs the worst.
The SampleSearch based Markov-LB scheme shows significantly better performance than the bound propagation
based scheme.

[Dechter and Mateescu, 2004] Dechter, R. and Mateescu, R. (2004). Mixtures of
deterministic-probabilistic networks and their and/or search space. In UAI.

6

Conclusion and Summary

In this paper, we proposed a randomized approximation algorithm, Markov-LB for computing high confidence lower
bounds on probability of evidence. Markov-LB is based on
importance sampling and the Markov inequality. A straight
forward application of the Markov inequality may lead to
poor lower bounds and therefore we suggest various heuristic measures to improve Markov-LB’s performance. We
also show how the performance of Markov-LB can be improved further on belief networks with zero probabilities by

ACKNOWLEDGEMENTS
This work was supported in part by the NSF under award
numbers IIS-0331707 and IIS-0412854.




width of the network whose instantiated variables (evidence and sampled) are removed.

The paper extends the principle of cutset
sampling over Bayesian networks, presented
previously for Gibbs sampling, to likelihood
weighting (LW). Cutset sampling is motivated by the Rao-Blackwell theorem which
implies that sampling over a subset of variables requires fewer samples for convergence
due to the reduction in sampling variance.
The scheme exploits the network structure
in selecting cutsets that allow efficient computation of the sampling distributions. In
particular, as we show empirically, likelihood weighting over a loop-cutset (abbreviated LWLC), is time-wise cost-effective. We
also provide an effective way for caching
the probabilities of the generated samples
which improves the performance of the overall scheme. We compare LWLC against regular liklihood-weighting and against Gibbsbased cutset sampling.

We defined previously an efficient parametrized Gibbs
cutset sampling scheme, called w-cutset sampling
[2, 3], where the complexity of generating a single
sample is bounded exponentially by w. In this paper,
we extend the cutset sampling principle to likelihood
weighting (LW) [11, 21], which is a form of importance
sampling [21], focusing on sampling from a loop-cutset.
The resulting scheme, which we call LWLC, computes
a sample over a loop-cutset C in O((|C| + |E|) · N ),
where E is evidence and N is the size of the input
network. While we present our scheme for LW, it is
applicable to other importance sampling schemes.

Introduction

Stochastic sampling is a popular approach for estimating answers to Bayesian queries when exact inference
is intractable. Based on the generated samples, we
can obtain estimates that converge to the exact values
as the number of samples increases. However, convergence may be slow in large networks due to increase
in sampling variance. This is the problem we address
in this paper.
Based on Rao-Blackwell theorem, we can reduce sampling variance and speed up convergence by sampling
only a subset of the variables (a cutset). However, the
efficiency of sampling from lower-dimensional spaces is
hindered by the overhead of computing the sampling
distributions. The latter is equivalent to performing
exact inference which is exponential in the induced

While both cutset schemes, one based on Gibbs sampling and one based on likelihood weighting, exploit
the network structure to manage the complexity of exact inference, they compute different sampling distributions. Gibbs sampler draws a new value of variable
Xi from distribution P (Xi |x\xi ). Likelihood weighting samples a new value from P (Xi |x1 , ..., xi−1 ). Furthermore, while both schemes benefit from reducing
the size of the sampling space, it is hard to predict
which of the two schemes is superior. The convergence speed of Gibbs sampling depends on the maximum correlation between the sampled variables. The
convergence of likelihood weighting is affected by the
distance between the sampling and the target distributions and, thus, depends also on the nature of evidence. Finally, Gibbs estimates converge only when
all Markov Chain transition probabilities are positive.
The advantages of cutset-based importance sampling,
also known as Rao-Blackwellised importance sampling,
were demonstrated previously in a few special cases
[9, 8, 1]. Our scheme automates the cutset selection
process based on the Bayesian network structure. We
demonstrate empirically that LWLC is efficient timewise and has a lower rejection rate in networks with
determinism. We achieve additional improvements by
caching the probabilities of the generated samples.

Our scheme can be generalized to other importance
sampling schemes.

2

Background

Definition 2.1 (belief networks)
Let X={X1 , ..., Xn } be a set of random variables over
multi-valued domains D(X1 ), ..., D(Xn ). A belief network is a pair <G, P> where G is a directed acyclic
graph on X and P={P (Xi |pai )|Xi ∈ X} is the set
of conditional probability tables (CPTs), conditioned
on parents pai of Xi . An evidence e is an instantiated subset of variables E. A network is singlyconnected (also called a poly-tree), if its underlying undirected graph has no cycles. Otherwise, it is
multiply-connected.
Definition 2.2 (loop-cutset) A loop in G is a
subgraph of G whose underlying graph is a cycle. A
vertex v is a sink with respect to a loop L if the two
edges adjacent to v in L are directed into v. Every
loop contains at least one vertex that is not a sink with
respect to that loop. Each vertex that is not a sink with
respect to a loop L is called an allowed vertex with respect to L. A loop-cutset of a directed graph G is a
set of vertices that contains at least one allowed vertex
with respect to each loop in G.
The queries over a singly-connected network can be
processed in time linear in the size of the network [18].
In general, the complexity of queries can be reduced
by restricting G to a relevant subnetwork.
Definition 2.3 (Relevant Subnetwork) A
variable Xi in DAG G over X is irrelevant (barren)
w.r.t. a subset Z⊂X if Xi ∈Z
/ and Xi only has irrelevant descendants (if any). The relevant subnetwork of
G w.r.t. a subset Z is the subgraph of G obtained by
removing all variables that are irrelevant w.r.t Z.
2.1

Likelihood Weighting

Likelihood weighting [11, 21] belongs to a family of
importance sampling schemes that draw independent
samples from a trial distribution Q(X). The trial
distribution is different from the target distribution
P (X). Generally, Q(X) is selected so that it is easy
to compute. A typical query in Bayesian networks is
to estimate the posterior marginals P (xi |e) which can
be obtained from the sampling estimates of P (e) and
P (xi , e). Let Y = X\E. Then:

Consequently, the sampling estimate P̂ (e) of P (e),
based on T samples from Q(X), is obtained by:
P̂ (e) =

T
T
1 X (t)
1 X P (y (t) , e)
=
w
T t=1 Q(y (t) )
T t=1

(1)

(t)

(y ,e)
(t)
. In a
where w(t) = PQ(y
(t) ) is the weight of sample y
similar manner, but counting only those samples where
Xi = xi , we can obtain an expression for the sampling
estimate P̂ (xi , e) of P (xi , e) for Xi ∈ X\E by:
PT
P̂ (xi , e) = T1 t=1 w(t) δ(xi , x(t) )
(2)
(t)

where δ(xi , x(t) )=1 iff xi =xi and δ(xi , x(t) )=0 otheri ,e)
wise. Since P (xi |e) = PP(x(e)
, we get:
PT

T
X
w(t) δ(xi , x(t) )
w(t) δ(xi , x(t) )
=α
PT
(t)
w
t=1
t=1
(3)
where α is a normalization constant. These sampling
estimates are guaranteed to converge to their target
values as T increases as long as the support for Q(X)
includes all support for P (X). Namely, the condition
∀x ∈ X, P (x) 6= 0 ⇒ Q(x) 6= 0 must hold. Eq. (3)
yields a biased estimate of P̂ (xi |e). However, when the
sample size is large enough, bias can be ignored [10].

P̂ (xi |e) =

t=1

Likelihood weighting draws samples from a distribution Q(X) that is close to the prior distribution. It
begins with a network without evidence and assigns
values to nodes in topological order. First, root nodes
are sampled from their prior distributions. Then, the
values of all other nodes Xi ∈X\E are sampled from
the distribution P (Xi |pai ). Evidence variables Ei ∈E
are assigned their observed value. Thus, the sampling
distribution of likelihood weighting can be described
as follows:
Q
Q(X) = Xi ∈X\E P (Xi |pai ) |E=e
(4)
We therefore compute the weight w(t) of sample t by:
Q
(t)
(t)
P (x(t) )
Xi ∈X P (xi |pai )
(t)
(5)
=
w =
Q
(t)
(t)
Q(x(t) )
P (x |pa )
Xi ∈X\E

i

i

All factors in the numerator and denominator of the
fraction cancel out except for P (ei |pai ), leaving:
w(t) =

Q

(t)

Ei ∈E

P (ei |pai )

(6)

Thus, during sampling, we compute the weight w(t)
of sample t by initializing w(t) ←1 and updating
(t)
w(t) ←w(t) · P (ei |pa ) whenever we encounter an eviX
X P (y, e)
P (y, e) dence E = e . Thei posterior marginals estimates are
i
i
Q(y) = EQ [
]
EP [P (e)] =
P (y, e) =
Q(y)
Q(y) obtained by plugging the sample weights in Eq.(3).
y
y

The convergence of importance sampling schemes can
be slow when Q(X) is very different from P (X). Consequently, many importance sampling schemes focus
on finding an improved sampling distribution by either
changing the variable sampling order [12] or updating
the sampling distribution based on previously generated samples [21, 5, 22]. We can also improve convergence by reducing the dimensionality of the sampling
space as implied by Rao-Blackwell theorem.

3

Rao-Blackwellised Likelihood
Weighting

Give a Bayesian network over a set of variables X with
evidence E⊂X, E=e,
S let C⊂X\E be a subset of variables in X, Z=C E, and m=|Z|. Let o={Z1 , ..., Zm }
be a topological ordering of the variables. We can
define likelihood weighting over Z as follows. Processing variables in order o, we sample value z1 from distribution P (Z1 ), z2 from P (Z2 |z1 ), and so on. For
each Zi ∈C, we sample a value zi from the distribution
P (Zi |z1 , ..., zi−1 ). If Zi ∈E, we assign Zi its observed
value. The sampling distribution Q(C) is:
Y
Q(C) =
P (Zi |z1 , ..., zi−1 ) |E=e
(7)
Zi ∈C

The weight w(t) of sample t is given by:
Q
(t) (t)
(t)
P (z (t) )
Zi ∈Z P (zi |z1 , ..., zi−1 )
(t)
(8)
=Q
w =
(t) (t)
(t)
Q(z (t) )
P (z |z1 , ..., z )
Zi ∈Z\E

i

i−1

After cancelling out the common factors in denominator and numerator, we get:
Q
(t)
(t)
(9)
w(t) = Zi ∈E P (ei |z1 , ..., zi−1 )
During sampling, the weight (initialized to 1) is updated every time we encounter an evidence variable
Zi ∈ E with observed value ei using:
w(t) ← w(t) · P (ei |z1 , ..., zi−1 )

Theorem 3.1 Given Bayesian network over X, evidence E ⊂ X, and cutset C ⊂ X\E, let Z = C ∪ E
be a loop-cutset. If Z is topologically ordered, then
∀Zj ∈ Z the relevant subnetwork of Z1 , ..., Zj is singlyconnected when Z1 , ..., Zj are observed.
Proof. Proof by contradiction. Assume that the relevant subnetwork of Z1 , ..., Zj contains a loop L with
sink S. Then, either S = Zq or S has a descendant
Zq , 1≤q≤j, (otherwise S is irrelevant). By definition
of loop-cutset, ∃Cm ∈L s.t. Cm 6=S and Cm ∈ C ⊂ Z.
Threfore, Cm is an ancestor of Zq . Since variables are
topologically ordered and all loop-cutset nodes preceding Zq are observed, Cm must be observed, thus,
breaking the loop, yielding a contradiction. 2
Conclusion: if C is a loop-cutset, we can compute the
distributions P (Zi |z1 , ..., zi−1 ) for every Zi ∈Z over the
relevant subnetwork of Zi in linear time and space.
Therefore, the complexity of computing a new sample
is proportional to the number of variables in Z and the
size of the input N . In summary:
Theorem 3.2 (Complexity) Given a Bayesian network over X, evidence E, and a loop-cutset C⊂X\E,
the complexity of generating one sample using likelihood weighting over a cutset C is O(|Z| · N ) where
Z = C ∪ E and N is the size of the input network.
Once a sample c(t) is generated, we apply belief propagation algorithm one more time to obtain the posterior
marginals, P (Xi |c(t) , e), for each remaining variable.
Once T samples are generated, we obtain the posterior marginals estimates, similar to Eq. (3), by:
P̂ (ci |e)

= α

T
X
t=1

P̂ (xi |e)

= α

T
X

Consider the special case when C ∪ E is a loopcutset. In this case, we can compute the probability
P (z)=P (c, e) in linear time and space using Pearl’s
belief propagation algorithm. We can show that we
can also compute P (Zi |z1 , ..., zi−1 ) efficiently if we order the variables in Z topologically and restrict our
attention to the relevant subnetwork of Z1 , ..., Zi .

w(t) P (xi |c(t) , e), ∀Xi ∈ X\C, E

t=1

(10)

The main difference between likelihood weighting over
cutset C and sampling over all variables X is in
computing the sampling distributions. In the latter
case, the distribution P (Xi |x1 , ..., xi−1 ) = P (Xi |pai )
is readily available in the conditional probability
table of Xi . However, the sampling distribution
P (Zi |z1 , ..., zi−1 ) for LWLC needs to be computed.

w(t) δ(ci , c(t) ), ∀Ci ∈ C

3.1

Convergence

Likelihood weighting on a loop-cutset (LWLC) has
a higher overhead in computing the distributions
P (Zi |z1 , ..., zi−1 ) for ∀Zi ∈ Z, compared with sampling on a full variable set. However, as mentioned
earlier, it converges faster. In general, importance
sampling convergence rate is affected by the sampling
variance and the distance between the sampling and
the target distributions. The estimates obtained by
sampling from a lower-dimensional space have lower
variance due to Rao-Blackwell theorem. That is:
V ar{

P (C)
P (Y, C)
} ≥ V ar{
}
Q(Y, C)
Q(C)

P
P
where P (C) = y P (Y, C) and Q(C) = y Q(Y, C)
[9, 16] A proof can be found in [9] and [16]. Consequently, fewer LWLC samples are needed to achieve
the same accuracy as LW.
The information distance between target distribution
P (C|e) and sampling distribution Q(C) in LWLC is
smaller than the distance between P (X|e) and sampling distribution Q(X). We can show this for the
KL-distance [15]:
KL(P (X), Q(X)) =

X
x

P (x) log

P (x)
Q(x)

(11)

Theorem 3.3 (Reduced Information Distance)
Given a Bayesian network expressing probability distribution P (X), evidence E=e, and a cutset C ⊂ X\E,
let Q(X) and Q(C, E) denote the likelihood weighting
sampling distribution over X and over C, E respectively. Then:
KL(P (C|e), Q(C, E)) ≤ KL(P (X|e), Q(X))
We outline the proof in the Appendix. The details are
available in [4].
3.2

Caching Sampling on a Cutset

Often, we can reduce the computation time of a sampling scheme by caching the generated samples and
their probabilities. Caching LW values is of limited benefit since it uses probabilities stored in CPTs.
However, in the case of LWLC, caching may compensate in part for the computation overhead. A suitable
data structure for caching is a search-tree over the cutset C with a root node C1 . As new variable values
are sampled and a partial assignment to the variables
C1 , ..., Ci is generated, LWLC traverses the search tree
along the path c1 , ..., ci . Whenever a new value of Ci
is sampled, the corresponding tree branch is expanded
and the current sample weight and the sampling distribution P (Ci |z1 , ..., zi−1 ) are saved in the node Ci .
In the future, when generating the same partial assignment c1 , ..., ci , LWLC saves on computation by
reading saved distributions from the tree. We will use
LWLC-BUF to denote LWLC sampling scheme that
uses a memory buffer to cache previously computed
probabilities. LWLC-BUF can also update the sampling distributions P (Ci |z1 , ..., zi−1 ) when dead-ends
are discovered. Namely, if the algorithm finds that
a partial instantiation z1 , ..., zi , cannot be extended
to a full tuple with non-zero probability, then we set
P (Ci |z1 , ..., zi−1 ) = 0 and normalize the updated distribution.

4
4.1

Experiments
Methodology

In this section, we compare empirically the performance of full likelihood weighting (LW), sampling over
all the variables, against likelihood weighting on a
loop-cutset (LWLC) and buffered likelihood weighting
on a loop-cutset (LWLC-BUF). In networks with positive distributions, we compare likelihood weighting
side by side with Gibbs sampling (Gibbs) and Gibbsbased loop-cutset sampling (LCS) [2]. For reference,
we also compare with the estimates obtained by Iterative Belief Propagation (IBP). Belief propagation
computes the exact posterior marginals in poly-trees
[18]. When applied to networks with loops, it computes approximate marginals when it converges. IBP
is fast and often produces good estimates [17, 20].
The quality of the approximate posterior marginals is
measured by the Mean Square Error (MSE):
P
P
2
Xi ∈X\E
D(Xi ) [P (xi |e) − P̂ (xi |e)]
P
M SE =
Xi ∈X\E |D(Xi )|
The exact posterior marginals P (Xi |e) are obtained
by bucket-tree elimination [7, 6]. We also measure the
rejection rate R of each sampling scheme.
Table 1: Benchmarks’ characteristics: N -number of
nodes, w∗ -induced width, |LC|-loop-cutset size, P (e)average probability of evidence (over 30 instances),
TBE -exact computation time by bucket elimination.
N w∗ |LC|
P(e)
TBE
cpcs360b
360
21
26
5E-8
20 min
cpcs422b
422
22
47 1.5E-6
50 min
Pathfinder1 109
6
9
0.07
1 sec
Pathfinder2 135
4
4
0.06 0.01 sec
Link
724
15
142
0.07 325 sec
Our benchmarks are taken from Bayesian network
repository. They include two subsets of Pathfinder
network, Pathfinder1 and Pathfinder2, Link, and
two CPCS networks, cpcs360b and cpcs422b. The
benchmarks’ properties are summarized in Table 1.
Pathfinder is an expert system for identifying disorders from lymph node tissue sections [13]. Link is a
model for the linkage between two genes [14]. The exact posterior marginals for those networks were easy
to compute by bucket elimination. However, they are
hard for sampling because of the large number of deterministic relationships. cpcs360b and cpcs422b are
derived from the Computer-Based Patient Care Simulation system [19]. They are more challenging for
exact inference because of their large induced widths.
All experiments were performed on a 1.8 GHz CPU.

4.2
4.2.1

Results
Sampling Speed

We generated 30 instances of each network with different random observations among the leaf nodes. In
Table 2, we report the speed of generating samples using LW, LWLC, and LWLC-BUF sampling schemes.
As expected, LWLC generates far fewer samples than
LW. Notably, the relative speed of LW and LWLC remains the same in the two Pathfinder networks and
in Link network. By the time LW generates 100, 000
samples, LWLC generates 1200 samples. Table 2 also
shows an order of magnitude improvement in the speed
of generating samples by LWLC-BUF in cpcs360b,
Pathfinder1, and Pathfinder2, a factor of 2 improvement in cpcs422b, and no change in the Link network.
The improvement depends on the ratio of unique samples. The number of unique tuples in Pathfinder networks is only ≈1% of the total number of samples and,
thus, 99% of the computation is redundant. However,
in Link network, nearly all samples are unique. Hence,
buffering was not beneficial.
Table 2: Average # of samples generated by LWLC
and LWLC-BUF by the time LW generates 100, 000
samples.
LW LWLC LWLC-BUF
cpcs360b
100000
2400
24000
cpcs422b
100000
25
50
Pathfinder1 100000
1200
12000
Pathfinder2 100000
1200
12000
Link
100000
1200
1200

the rejection rate R to denote the percentage of samples of weight 0. When the evidence is rare, we may
need to generate a very large number of samples before we find a single sample of non-zero weight. When
all samples are rejected, we will say that the rejection
rate is 100% and call the network instance unresolved.
The rejection rates of the three likelihood weighting
schemes over Pathfinder1, Pathfinder2, and Link are
summarized in Table 3. For each benchmark, we report the number of instances k (out of 30), where the
rejection rate <100%. As we can see, LW resolved all
30 instances of Pathfinder1 but only 28 instances of
Pathfinder2 and only 17 instances of Link. LWLC and
LWLC-BUF resolved all network instances.
Table 3 also reports the rejection rate R averaged over
those instances where all three algorithms generated
some samples with non-zero probabilities. As we can
see, LW has high rejection rates in all benchmarks.
The corresponding LWLC rejection rates are a factor
of 3 or more smaller. Although lower rejection rate
alone does not guarantee faster convergence, it helps
compensate for generating fewer samples. The rejection rate of LWLC-BUF is two orders of magnitude
lower than LWLC in Pathfinder networks but it is the
same as LWLC in Link network (also because most of
the samples are unique).
The rejection rate of LW and LWLC does not change
with time. However, as LWLC-BUF learns zeros of
the target distribution, its rejection rate may decrease
as the number of samples increases. Figure 1 demonstrates this on the example of Pathfinder networks.
LWLC-BUF Rejection Rates

Rejection Rates

Table 3: Average rejection rates for different benchmarks: k -# instances, out of 30, where rejection rate
<100%, R - average rejection rate.
LW
LWLC
LW-BUF
k R(%)
k R(%)
k R(%)
PF1
30
47 30
6 30
0.01
PF2
28
77 30
26 30
0.05
Link 17
67 30
16 30
16
When target distribution P (X) has many zeros where
sampling distribution Q(X) remains positive, many
samples with weight 0 may be generated which do not
contribute to the sampling estimates. Hence, we call
them “rejected.” This is not an issue in cpcs360b and
cpcs422b where all probabilities are positive. However, in deterministic networks, many samples may be
rejected, contributing to slow convergence. We will use

Rejection Rate

4.2.2

Pathfinder1

0.3

Pathfinder2

0.25
0.2
0.15
0.1
0.05
0
0

5000

10000

15000

20000

25000

30000

35000

40000

# samples

Figure 1: LWLC-BUF average rejection rate over 30
network instances in Pathfinder1 and Pahfinder2 as a
function of the number of samples.
4.2.3

Accuracy of the Estimates

The MSE results for PathFinder1, Pathfinder2, and
Link are shown in Figure 2 as a function of time.
The comparative behavior of LW, LWLC, and LWLCBUF sampling schemes is similar in all three networks.
LWLC consistenly converges faster than LW and out-

LW

LW

PathFinder 1, N=109, w*=6, |LC|=9, |E|=11

cpcs360b, N=360, |E|=18, |LC|=26, w*=21

LWLC

0.002

LWLC
Gibbs

LWLC-BUF
1.E-03

IBP

0.0016

LCS

MSE

MSE

IBP

0.0012
0.0008

1.E-04

0.0004
0
0

2

4

6

8

10

12

1.E-05
0

Time (sec)

5

10

15

20

25

30

35

Time (sec)
LW

PathFinder2, N=135, |LC|=4, |E|=17

LW
LWLC
Gibbs
LCS
IBP

cpcs422b, N=422, |LC|=47, |E|=30

LWLC

0.002

1.E-02

LWLC-BUF

1.E-03
0.001

MSE

MSE

IBP

1.E-04

0.000
0

2

4

6

8

1.E-05

10

0

10

20

30

Time (sec)

40

50

60

70

80

Time (sec)
LW

Link, N=724, w*=15, |LC|=142, |E|=10

LWLC

0.003

IBP

MSE

0.002

Figure 3: MSE as a function of time for full Gibbs
sampling (Gibbs), Gibbs loop-cutset sampling (LCS),
LW, LWLC, and IBP in cpcs360b and cpcs422b.

0.001

0

2

4

6

8

10

12

Time (sec)

Figure 2: MSE as a function of time for LW, LWLC,
LWLC-BUF, and IBP over 30 network instances of
Pathfinder1 (top), 28 instances of Pathfinder2 (middle), and 17 instances of Link (bottom).
performs IBP within 2 seconds. LW outperforms IBP
within 2 seconds in Pathfinder1 and within 8 seconds
in Pathfinder2. However, LW is considerably worse
than IBP in Link network. LWLC-BUF converges
faster than LWLC in Pathfinder1 and Pathfinder2 because it generates more samples and has a lower rejection rate. In Link network, their performance is the
same and, thus, we only show the LWLC curve.
The PathFinder2 network was also used as a benchmark in the evaluation of AIS-BN algorithm [5], an
adaptive importance sampling scheme. Although we
experimented with different network instances, we can
make a rough comparison. Within 60 seconds, AIS-BN
computes MSE ≈ 0.0005. Adjusting for the difference
in processor speed, the corresponding MSEs of LWLC
and LWLC-BUF are ≈0.004 and ≈0.00008, obtained
in 6 seconds. Hence, AIS-BN and LWLC-BUF produce comparable results.

The accuracy of LW and LWLC for cpcs360b and
cpcs422b networks is shown in Figure 3. Overall, results are similar. The LWLC outperforms LW by a
wide margin in both benchmarks. Since all probabilities are positive, we also show the results for two Gibbs
sampling schemes. Gibbs outperforms full likelihood
weighting. Gibbs-based loop-cutset sampling (LCS)
outperforms LWLC. Figure 4 focuses on the buffered
cutset sampling schemes. Both LWLC-BUF and LCSBUF improve substantially over the plain LWLC and
LCS. And again, the Gibbs-based LCS-BUF is better
than LWLC-BUF.

cpcs360b, N=360, |E|=18, |LC|=26, w*=21
1.E-03

LWLC
LWLC-BUF
LCS
LCS-BUF

1.E-04

MSE

0.000

1.E-05

1.E-06
0

5

10

15

20

25

30

35

Time (sec)

Figure 4: MSE in cpcs360b as a function of time for
LCS and LWLC vs. buffered LCS and LWLC-BUF.

Although Gibbs sampling schemes outperformed likelihood weighting methods in cpcs360b and cpsc422b,
where evidence was selected among leaf nodes, the two
methods are likely to switch places when fewer leaf
nodes are observed. In particular, likelihood weighting
outperforms Gibbs sampling in cpcs360b and cpcs422b
without evidence [4].

5

Related Work and Conclusions

In this paper we presented a cutset-based likelihood
weighting. By reducing the dimensionality of the sampling space, we achieve reduction in th esampling variance and also reduce the information distance (KLdistance) between the sampling and the target distributions. Therefore, the cutset sampling scheme requires fewer samples to converge.
In the past, Rao-Blackwellised importance sampling
was made efficient by exploiting the properties of the
conditional probability distributions, e.g., when the
distributions for the marginalised variables could be
computed analytically using a Kalman filter [9, 8, 1]
or when the marginalised variables in a factored
HMM became conditionally independent (when sampled variables are observed) due to the numerical
structure of the CPTs [8]. In contrast, our method
bounds the complexity of computing the sampling distributions by exploiting the structure of the network.
We demonstrated empirically that cutset-based likelihood weighting is time-wise effective. Namely, it computes more accurate estimates than likelihood weighting as a function of time. We improve the convergence
of cutset-based likelihood weighting by caching previously computed samples. The buffered scheme reduces
the average sample computation time since it does not
re-compute the probabilities of previously generated
tuples and since it allows modifying the cached distributions dynamically.
In this paper, we only updated the saved distributions when a partially-instantiated cutset tuple could
not be extended to a full cutset tuple with non-zero
probability. However, we can additionally update
cached distributions based on the weight of previously
generated samples as adaptive importance sampling
techniques do. The proposed cutset-based likelihood
weighting can be generalized to other importance sampling schemes.




of our approach on a complex dynamic domain of a person’s transportation routines.

This paper describes a general framework called
Hybrid Dynamic Mixed Networks (HDMNs)
which are Hybrid Dynamic Bayesian Networks
that allow representation of discrete deterministic
information in the form of constraints. We propose approximate inference algorithms that integrate and adjust well known algorithmic principles such as Generalized Belief Propagation,
Rao-Blackwellised Particle Filtering and Constraint Propagation to address the complexity of
modeling and reasoning in HDMNs. We use
this framework to model a person’s travel activity over time and to predict destination and
routes given the current location. We present a
preliminary empirical evaluation demonstrating
the effectiveness of our modeling framework and
algorithms using several variants of the activity
model.

Focusing
on
algorithmic
issues,
the
most
popular
approximate
query
processing
algorithms
for
dynamic
networks
are
Expectation
propagation(EP)
[Heskes and Zoeter, 2002]
and
Rao-Blackwellised
Particle
Filtering
(RBPF) [Doucet et al., 2000].
We therefore extend
these algorithms to accommodate and exploit discrete
constraints in the presence of continuous probabilistic
functions. Extending Expectation Propagation to handle
constraints is easy, extension to continuous variables is a
little more intricate but still straightforward. The presence
of constraints introduces a principles challenge for Sequential Importance Sampling algorithms, however. Indeed the
main algorithmic contribution of this paper in presenting
a class of Rao-Blackwellised Particle Filtering algorithm,
IJGP-RBPF for HDMNs which integrates a Generalized
Belief Propagation component with a Rao-Blackwellised
Particle Filtering scheme.

1 INTRODUCTION
Modeling sequential real-life domains often requires the
ability to represent both probabilistic and deterministic information. Hybrid Dynamic Bayesian Networks (HDBNs)
were recently proposed for modeling such phenomena
[Lerner, 2002]. In essence, these are factored representation of Markov processes that allow discrete and continuous variables. Since they are designed to express uncertain
information they represent constraints as probabilistic entities which may have negative computational consequences.
To address this problem [Dechter and Mateescu, 2004,
Larkin and Dechter, 2003] introduced the framework of
Mixed Networks. In this paper we extend the Mixed Networks framework to dynamic environments, allow continuous Gaussian variables, yielding Hybrid Dynamic Mixed
Networks (HDMN). We address the algorithmic issues that
emerge from this extension and demonstrate the potential

Our motivation for developing HDMNs as a modeling
framework is a range of problems in the transportation literature that depend upon reliable estimates of the prevailing demand for travel over various time scales. At one
end of this range, there is a pressing need for accurate
and complete estimation of the global origins and destinations (O-D) matrix at any given time for an entire urban
area. Such estimates are used in both urban planning applications [Sherali et al., 2003] and integrated traffic control systems based upon dynamic traffic assignment techniques [Peeta and Zilaskopoulos, 2001]. Even the most advanced techniques, however, are hamstrung by their reliance upon out-dated, pencil-and-paper travel surveys and
sparsely distributed detectors in the transportation system.
We view the increasing proliferation of powerful mobile
computing devices as an opportunity to remedy this situation. If even a small sample of the traveling public
agreed to collect their travel data and make that data publicly available, transportation management systems could
significantly improve their operational efficiency. At the

other end of the spectrum, personal traffic assistants running on the mobile devices could help travelers replan their
travel when the routes they typically use are impacted by
failures in the system arising from accidents or natural disasters. A common starting point for these problems is to
develop an efficient formulation for learning and inferring
individual traveler routines like traveler’s destination and
his route to destination from raw data points.
The rest of the paper is organized as follows. In the next
section, we discuss preliminaries and introduce our modeling framework. We then describe two approximate inference algorithms for processing HDMN queries: an Expectation Propagation type and a Particle Filtering type.
Subsequently, we describe the transportation modeling approach and present preliminary empirical results on how
effectively a model is learnt and how accurately its predictions are given several models and a few variants of the
relevant algorithms.
We view the contribution of this paper in addressing a complex and highly relevant real life domain using a general
framework and domain independent algorithms, thus allowing systematic study of modeling, learning and inference in a non-trivial setting.

2 PRELIMINARIES AND DEFINITIONS
Hybrid Bayesian Networks (HBN) [Lauritzen, 1992] are
graphical models defined by a tuple B = (X, G, P), where X
is the set of variables
partitioned into discrete and continuS
ous ones X = Γ ∆, respectively, G is a directed acyclic
graph whose nodes corresponds to the variables. P =
{P1 , ..., Pn } is a set of conditional probability distributions
(CPDs). Given variable xi and its parents in the graph
pa(xi ), Pi = P(xi |pa(xi )). The graph structure G is restricted in that continuous variables cannot have discrete
variables as their child nodes. The conditional distribution of continuous variables are given by a linear Gaussian
model: P(xi |I = i, Z = z) = N(α(i) + β(i) ∗ z, γ(i))xi ∈ Γ
where Z and I are the set of continuous and discrete parents
of xi , respectively and N(µ, σ) is a multi-variate normal distribution. The network represents a joint distribution over
all its variables given by a product of all its CPDs.
A Constraint Network [Dechter, 2003] is a graphical model
R = (X, D,C), where X = {x1 , . . . , xn } is the set of variables, D = {D1 , . . . , Dn } is their respective discrete domains and C = {C1 ,C2 , . . . ,Cm } is the set of constraints.
Each constraint Ci is a relation Ri defined over a subset of
the variables Si ⊆ X and denotes the combination of values that can be assigned simultaneously. A Solution is an
assignment of values to all the variables such that no constraint is violated. The primary query is to decide if the
constraint network is consistent and if so find one or all
solutions.

The recently proposed Mixed Network framework [Dechter and Mateescu, 2004] for augmenting
Bayesian Networks with constraints, can immediately be
applied to HBNs yielding the Hybrid Mixed Networks
(HMNs). Formally, given a HBN B = (X, G, P) that
expresses the joint probability PB and given a constraint
network R = (X, D,C) that expresses a set of solutions ρ,
an HMN is a pair M = (B , R ). The discrete variables and
their domains are shared by B and R and the relationships
are those expressed in P and C. We assume that R is
consistent. The mixed network M = (B , R ) represents the
conditional probability PM (x) = PB (x|x ∈ ρ) i f x ∈ ρ and
0 otherwise.
Dynamic Bayesian Networks are Markov models whose
state-space and transition functions are expressed in a factored form using Bayesian Networks. They are defined by
a prior P(X0 ) and a state transition function P(Xt+1 |Xt ).
Hybrid Dynamic Bayesian Networks (HDBNs) allow continuous variables while Hybrid Dynamic Mixed Networks
(HDMNs) also permit explicit discrete constraints.
D EFINITION 2.1 A Hybrid Dynamic Mixed Network
(HDMN) is a pair (M0 , M→ ), defined over a set of variables X = {x1 , ..., xn }, where M0 is an HMN defined over
X representing P(X0 ). M→ is a 2-slice network defining
the stochastic process P(Xt+1 |Xt ). The 2-time-slice Hybrid
00
Mixed network (2-THMN) is an HMN defined over X 0 ∪ X
0
00
such that X and X are identical to X. The acyclic graph
0
of the probabilistic portion is restricted so that nodes in X
are root nodes and have no CPDs associated with them.
The constraints are defined the usual way. The 2-THMN
00
0
represents a conditional distribution P(X |X ).
The semantics of any dynamic network can be understood by unrolling the network to T time-slices. Namely,
T
P(X0:t ) = P(X0 ) ∗ ∏t=1
P(Xt |Xt−1 ) where each probabilistic component can be factored in the usual way, yielding a
regular HMN over T copies of the state variables.
The most common task over Dynamic Probabilistic Networks is filtering and prediction Filtering is the task of determining the belief state P(Xt |e0:t ) where Xt is the set of
variables at time t and e0:t are the observations accumulated
at time-slices 0 to t. Filtering can be accomplished in principle by unrolling the dynamic model and using any stateof-the art exact or approximate reasoning algorithm. The
join-tree-clustering algorithm is the most commonly used
algorithm for exact inference in Bayesian networks. It partitions the CPDs and constraints into clusters that interact
in a tree-like manner (the join-tree) and applies messagepassing between clusters. The complexity of the algorithm is exponential in a parameter called treewidth, which
is the maximum number of discrete variables in a cluster. However, the stochastic nature of Dynamic Networks
restricts the applicability of join-tree clustering considerably. In the discrete case the temporal structure implies

tree-width which equals to the number of state variables
that are connected with the next time-slice, thus making the
factored representation ineffective. Even worse, when both
continuous and discrete variables are present the effective
treewidth is O(T ) when T is the number of time slices, thus
making exact inference infeasible. Therefore the applicable approximate inference algorithms for Hybrid Dynamic
Networks are either sampling-based such as Particle Filtering or propagation-based such as Expectation Propagation.
In the next two sections, we will extend these algorithms to
HDMNs.

3 EXPECTATION PROPAGATION
In this section we extend an approximate inference algorithm called Expectation Propagation
(EP) [Heskes and Zoeter, 2002] from HDBNs to HDMNs.
The idea in EP (forward pass) is to perform Belief Propagation by passing messages between slices t and t + 1
along the ordering t = 0 to T . EP can be thought of as
an extension of Generalized Belief Propagation (GBP)
to HDBNs [Heskes and Zoeter, 2002]. For simplicity of
exposition, we will extend a GBP algorithm called Iterative
Join graph propagation [Dechter et al., 2002] to HDMNs
and call our technique IJGP(i)-S where ”S” denotes that
the process is sequential. The extension is rather straightforward and can be easily derived by integrating the results
in [Murphy, 2002, Dechter et al., 2002, Lauritzen, 1992,
Larkin and Dechter, 2003].
IJGP [Dechter et al., 2002] is a Generalized Belief Propagation algorithm which performs message passing on a
join-graph. A join-graph is collection of cliques or clusters
such that the interaction between the clusters is captured
by a graph. Each clique in a join-graph contains a subset
of variables from the graphical model. IJGP(i) is a parameterized algorithm which operates on a join-graph which
has less than i + 1 discrete variables in each clique. The
complexity of IJGP(i) is bounded exponentially by i , also
called the i-bound. In the message-passing step of IJGP(i),
a message is sent between any two nodes that are neighbors of each other in the join-graph. A message sent by
node Ni to N j is constructed by multiplying all the functions and messages in a node (except the message received
from N j ) and marginalizing on the common variables between N j and Ni (see [Dechter et al., 2002]).
IJGP(i) can be easily adapted to HDMNs (which we call
IJGP(i)-S) and we describe some technical details here
rather than a complete derivation due to lack of space. Note
that because we are performing online inference, we need
to construct the join-graph used by IJGP(i)-S in an online
manner rather than recomputing the join-graph every time
new evidence arrives. Murphy [Murphy, 2002] describes
a method to compute a join-tree in an online manner by
pasting together join-trees of individual time-slices using

special cliques called the interface. [Dechter et al., 2002]
describe a method to compute join-graphs from join-trees.
The two methods can be combined in a straightforward way
to come up with an online procedure for constructing a
join-graph. In this procedure, we split the interface into
smaller cliques such that the new cliques have less than
i + 1 variables. This construction procedure is shown in
Figure 1.
Message-passing is then performed in a sequential way as
follows. At each time-slice t, we perform message-passing
over nodes in t and the interface of t with t − 1 and t + 1
(shown by the ovals in Figure 1). The new functions computed in the interface of t with t + 1 are then used by t + 1,
when we perform message passing in t + 1.
Three important technical issues remain to be discussed.
First, message-passing requires the operations of multiplication and marginalization to be performed on functions
in each node. These operators can be constructed for
HDMNs in a straightforward way by combining the operators by [Lauritzen, 1992] and [Larkin and Dechter, 2003]
that work on HBNs and discrete mixed networks respectively. We will now briefly comment on how the multiplication operator can be derived. Let us assume we
want to multiply a collection of probabilistic functions P0
and a set of constraint relations C0 (which consist of only
discrete tuples allowed by the constraint) to form a single function PC. Here, multiplication can be performed
on the functions in P0 and C0 separately using the operators in [Lauritzen, 1992] and [Dechter, 2003] respectively
to compute a single probabilistic function P and a single
constraint relation C. These two functions P and C can be
multiplied by deleting all tuples in P that are not present in
C to form the required function PC.
Second, because IJGP(i)-S constructs join-graphs sequentially, the maximum-i-bound for IJGP(i)-S is bounded by
the treewidth of the time slice and its interfaces and not the
treewidth of the entire HDMN model (see Figure 1).

Figure 1: Schematic illustration of the Procedure used for
creating join-graphs and join-trees of HDMNs

Algorithm IJGP-RBPF
• Input: A Hybrid Dynamic Mixed Network (X, D, G, P,C)0:T and a observation sequence e0:T Integer N, w and i.
• Output: P(XT |e0:T )
• For t = 0 to T do
• Sequential Importance Sampling step:
1. Generalized Belief Propagation step
Use IJGP(i) to compute the proposal distribution Ωapp
2. Rao-Blackwellisation step
Partition the Variables Xt into Rt and Zt such that the treewidth of a join-tree of
Zt is w.
3. Sampling step
For i = 1 to N do
(a) Generate a Rti from Ωapp .
(b) reject sample if rti is not a solution. i=i-1;
(c) Compute the importance weights wti of Rti .
ci .
4. Normalize the importance weights to form w
t
• Selection step:
– Resample N samples from Rbti according to the normalized importance weights
ci to obtain new N random samples.
w
t

• Exact step:
– for i = 1 to N do
i , e , Rbi and
Use join-tree-clustering to compute the distribution on Zti given Zt−1
t t
d
i
R .
t−1

Figure 2: IJGP-RBPF for HDMNs
Third, IJGP(i) guarantees that the computations will be exact if i is equal to the treewidth. This is not true for IJGP(i)S in general as shown in [Lerner, 2002]. It can be proved
that:
T HEOREM 3.1 The complexity of IJGP(i)-S is O(((|∆t | +
n) ∗ d i ∗ Γt3 ) ∗ T ) where |∆t | is the number of discrete variables in time-slice t, d is the maximum-domain size of the
discrete variables, i is the i-bound used, n is the number
of nodes in a join-graph of the time-slice, Γt is the maximum number of continuous variables in the clique of the
join-graph used and T is the number of time-slices.

4 RAO-BLACKWELLISED PARTICLE
FILTERING
In this section, we will extend the Rao-Blackwellised Particle filtering algorithm [Doucet et al., 2000] from HDBNs
to HDMNs. Before, we present this extension, we will
briefly review Particle Filtering and Rao-Blackwellised
Particle Filtering (RBPF) for HDBNs.
Particle filtering uses a weighted set of samples or particles to approximate the filtering distribution. Thus, given
a set of particles Xt1 , . . . , XtN approximately distributed according to the target-filtering distribution P(Xt = M|e0:t ),
the filtering distribution is given by P(Xt = M|e0:t ) =
1/N ∑Ni=1 δ(Xti = M) where δ is the Dirac-delta function.
Since we cannot sample from P(Xt = M|e0:t ) directly, Parti-

cle filtering uses an appropriate (importance) proposal distribution Q(X) to sample from. The particle filter starts by
generating N particles according to an initial proposal distribution Q(X0 |e0 ). At each step, it generates the next state
i for each particle X i by sampling from Q(X
i
Xt+1
t+1 |Xt , e0:t ).
t
It then computes the weight of each particle based given by
wt = P(X)/Q(X) to compute a weighted distribution and
then re-samples from the weighted distribution to obtain a
set of un-biased or un-weighted particles.
Particle filtering often shows poor performance in highdimensional spaces and its performance can be improved
by sampling from a sub-space by using the RaoBlackwellisation (RB) theorem (and the particle filtering
is called Rao-Blackwellised Particle Filtering (RBPF)).
Specifically, the state Xt is divided into two-sets: Rt and
Zt such that only variables in set Rt are sampled (from a
proposal distribution Q(Rt ) ) while the distribution on Zt
is computed analytically given each sample on Rt (assuming that P(Zt |Rt , e0:t , Rt−1 ) is tractable). The complexity
of RBPF is proportional to the complexity of exact inference step i.e. computing P(Zt |Rt , e0:t , Rt−1 ) for each sample Rtk . w-cutset [Bidyuk and Dechter, 2004] is a parameterized way to select Rt such that the complexity of computing P(Zt |Rt , e0:t , Rt−1 ) is bounded exponentially by w. Below, we use the w-cutset idea to perform RBPF in HDMNs.
Since exact inference can be done in polynomial time if
a HDBN contains only continuous variables, a straightforward application of RBPF to HDBNs involves sampling
only the discrete variables in each time slice and exactly
inferring the continuous variables [Lerner, 2002].
Extending this idea to HDMNs, suggests that in each time
slice t we sample the discrete variables and discard all particles that violate the constraints in the time slice. Let us assume that we select a proposal distribution Q that is a good
approximation of the probabilistic filtering distribution but
ignores the constraint portion. The extension described
above can be inefficient because if the proposal distribution Q is such that it makes non-solutions to the constraint
portion highly probable, most samples from Q will be rejected (because these samples Rti will have P(Rti ) = 0 and
so the weight will be zero). Thus, on one extreme sampling
only from the Bayesian Network portion of each time-slice
may lead to potentially high rejection-rate.
On the other extreme, if we want to make the sample rejection rate zero we would have to use a proposal distribution Q0 such that all samples from this distribution
are solutions. One way to find this proposal distribution
is to make the constraint network backtrack-free (using
adaptive-consistency [Dechter, 2003] or exact constraint
propagation) along an ordering of variables and then sample along the reverse ordering. Another approach is to
use join-tree-clustering which combines probabilistic and
deterministic information and then sample from the join-

tree. However, both join-tree-clustering and adaptiveconsistency are time and space exponential in treewidth and
so they are costly when the treewidth is large. Thus on one
hand, zero-rejection rate implies using a potentially costly
inference procedure while on the other hand sampling from
a proposal distribution that ignores constraints may result
in a high rejection rate.
We propose to exploit the middle ground between the two
extremes by combining the constraint network and the
Bayesian Network into a single approximate distribution
Ωapp using IJGP(i) which is a bounded inference procedure. Note that because IJGP(i) has polynomial time complexity for constant i, we would not eliminate the samplerejection rate completely. However, by using IJGP(i) we
are more likely to reduce the rejection-rate because IJGP(i)
also achieves Constraint Propagation and it is well known
that Constraint Propagation removes many inconsistent
tuples thereby reducing the chance of sampling a nonsolution. [Dechter, 2003].
Another important advantage of using IJGP(i) is that
it yields very good approximations to the true posterior [Dechter et al., 2002] thereby proving to be an ideal
candidate for proposal distribution. Note that IJGP(i)
can be used as a proposal distribution because it can be
proved using results from [Dechter and Mateescu, 2003]
that IJGP(i) includes all supports of P(Xt |e0:t , Xt−1 ) (i.e.
P(Xt |e0:t , Xt−1 ) > 0 implies that the output of IJGP(i) viz.
Q > 0)
Note that IJGP(i) we use here is different from the algorithm IJGP(i)-S that we described in the previous section.
This is because in our RBPF procedure, we need to comk ,e )
pute an approximation to the distribution P(Rt |Rt−1
0:t
k
given the sample Rt−1
on variables Rt−1 and evidence e0:t .
IJGP(i) as used in our RBPF procedure works on HMNs
and can be derived using the results in [Dechter et al., 2002,
Lauritzen, 1992, Larkin and Dechter, 2003]. For lack of
space we do not describe the details of this algorithm (see a
companion paper [Gogate and Dechter, 2005] for details).
The integration of the ideas described above into a formal
algorithm called IJGP-RBPF is given in Figure 2. It uses
the same template as in [Doucet et al., 2000] and the only
step different in IJGP-RBPF from the original template is
the implementation of the Sequential Importance Sampling
step (SIS).
SIS is divided into three-steps: (1) In the Generalized
Belief Propagation step of SIS, we first perform Belief Propagation using IJGP(i) to form an approximation of the posterior (say Ωapp ) as described above.
(2) In the Rao-Blackwellisation step, we first partition the variables in a 2THMN into two sets Rt and
Zt using a method due to [Bidyuk and Dechter, 2004].
This method [Bidyuk and Dechter, 2004] removes minimal
variables Rt from Xt such that the treewidth of the remain-

Figure 3: Car Travel Activity model of an individual
ing network Zt is bounded by w. (3) In the sampling
step, the variables Rt are sampled from Ωapp . To generate a sample from Ωapp , we use a special data-structure
of ordered buckets which is described in a companion paper [Gogate and Dechter, 2005]. Importance weights are
computed as usual [Doucet et al., 2000].
Finally, the exact-step computes a distribution on Zt using join-tree-clustering for HMNs (see a companion paper [Gogate and Dechter, 2005] for details on join-treeclustering for HMNs). It can be proved that:
T HEOREM 4.1 The complexity of IJGP-RBPF(i,w) is
O([NR ∗ d w+1 + ((|∆| + n) ∗ (d i ∗ |Γ|3 ))] ∗ T ) where |∆| is
the number of discrete variables, d is the maximum-domain
size of the discrete variables, i is the adjusted-i-bound, w is
defined by w-cutset, n is the number of nodes in a joingraph, Γ is the number of continuous variables in a 2THMN, NR is the number of samples actually drawn and
T is the number of time-slices.

5

THE TRANSPORTATION MODEL

In this section, we describe the application of HDMNs to
a real-world problem of inferring car travel activity of individuals. The major query in our HDMN model is to predict where a traveler is likely to go and what his/her route
to the destination is likely to be, given the current location of the traveler’s car. This application was described
in [Liao et al., 2004] in a different context for detecting abnormal behavior in Alzheimer’s patients and they use a Abstract Hierarchical Markov Models (AHMM) for reasoning
about this problem. The novelty in our approach is not only
a more general modeling framework and approximate inference algorithms but also a domain independent implementation which allows an expert to add and test variants
of the model.
Figure 3 shows a HDMN model for modeling the car travel
activity of individuals. Note that the directed links express
the probabilistic relationships while the undirected (bold)

edges express the constraints.
We consider the roads as a Graph G(V, E) where the vertices V correspond to intersections while the edges E correspond to segments of roads between intersections. The
variables in the model are as follows. The variables dt and
wt represent the information about time-of-day and dayof-week respectively. dt is a discrete variable and has four
values (morning, a f ternoon, evening, night) while the variable wt has two values (weekend, weekday). Variable gt
represents the persons next goal (e.g. his office, home etc).
We consider a location where the person spends significant
amount of time as a proxy for a goal [Liao et al., 2004].
These locations are determined through a preprocessing
step by noting the locations in which the dwell-time is
greater than a threshold (15 minutes). Once such locations
are determined, we cluster those that are in close proximity
to simplify the goal set. A goal can be thought of as a set of
edges E1 ⊂ E in our graph representation. The route level
rt represents the route taken by the person to move from
one goal to other. We arbitrarily set the number of values
it can take to |gt |2 . The person’s location lt and velocity
vt are estimated from GPS reading yt . ft is a counter (essentially goal duration) that governs goal switching. The
Location lt is represented in the form of a two-tuple (a, w)
where a = (s1 , s2 ),a ∈ E and s1 , s2 ∈ V is an edge of the
map G(V, E) and w is a Gaussian whose mean is equal to
the distance between the person’s current position on a and
one of the intersections in a.
The probabilistic dependencies in the model are straightforward and can be found by tracing the arrows (see Figure 3). The constraints in the model are as follows. We
assume that a person switches his goal from one time slice
to another when he is near a goal or moving away from a
goal but not when he is on a goal location. We also allow a forced switch of goals when a specified maximum
time that he is supposed to spend at a goal is reached. This
is modeled by using a constant D. These assumptions of
switching between goals is modeled using the following
constraints between the current location, the current goal,
the next goal and the switching counters: (1)If lt−1 = gt−1
and Ft−1 = 0 Then Ft = D, (2) If lt−1 = gt−1 and Ft−1 > 0
Then Ft = Ft−1 − 1, (3) If lt−1 6= gt−1 and Ft−1 = 0 Then
Ft = 0 and (4) If lt−1 6= gt−1 and Ft−1 > 0 Then Ft = 0, (5)
If Ft−1 > 0 and Ft = 0 Then gt is given by P(gt |gt−1 ), (6) If
Ft−1 = 0and Ft = 0 Then gt is same as gt−1 , (7) If Ft−1 > 0
and Ft > 0 gt is same as gt−1 and (8) If Ft−1 = 0 and Ft > 0
gt is given by P(gt |gt−1 ).

6 EXPERIMENTAL RESULTS
The test data consists of a log of GPS readings collected
by one of the authors. The test data was collected over
a six month period at intervals of 1-5 seconds each. The
data consist of the current time, date, location and veloc-

ity of the person’s travel. The location is given as latitude
and longitude pairs. The data was first divided into individual routes taken by the person and the HDMN model
was learned using the Monte Carlo version of the EM algorithm [Liao et al., 2004, Levine and Casella, 2001].
We used the first three months’ data as our training
set while the remaining data was used as a test set.
TIGER/Line files available from the US Census Bureau
formed the graph on which the data was snapped. As specified earlier our aim is two-fold: (a) Finding the destination
or goal of a person given the current location and (b) Finding the route taken by the person towards the destination or
goal.
To compare our inference and learning algorithms, we use
three HDMN models. Model-1 is the model shown in Figure 3. Model-2 is the model given in Figure 3 with the
variables wt and dt removed from each time-slice. Model3 is the base-model which tracks the person without any
high-level information and is constructed from Figure 3 by
removing the variables wt , dt , ft , gt and rt from each timeslice.
We used 4 inference algorithms. Since EM-learning uses
inference as a sub-step, we have 4 different learning algorithms. We call these algorithms as IJGP-S(1), IJGPS(2) and IJGP-RBPF(1,1,N) and IJGP-RBPF(1,2,N) respectively. Note that the algorithm IJGP-S(i) (described
in Section 3) uses i as the i-bound. IJGP-RBPF(i,w,N) (described in Section 4) uses i as the i-bound for IJGP(i), w as
the w-cutset bound and N is the number of particles at each
time slice. Three values of N were used: 100, 200 and 500.
For EM-learning, N was 500. Experiments were run on a
Pentium-4 2.4 GHz machine with 2G of RAM. Note that
for Model-1, we only use IJGP-RBPF(1,1) and IJGP(1)-S
because the maximum i-bound in this model is bounded by
1 (see section 3).
6.1 FINDING DESTINATION OR GOAL OF A
PERSON
The results for goal prediction with various combinations
of models, learning and inference algorithms are shown
in Tables 1, 2 and 3. We define prediction accuracy as
the number of goals predicted correctly. Learning was
performed offline. Our slowest learning algorithm based
on GBP-RBPF(1,2) used almost 5 days of CPU time for
Model-1, and almost 4 days for Model-2—significantly
less than the period over which the data was collected. The
column ’Time’ in Tables 1, 2 and 3 shows the time for inference algorithms in seconds while the other entries indicate
the accuracy for each combination of inference and learning algorithms.
In terms of which model yields the best accuracy, we can
see that Model-1 achieves the highest prediction accuracy

of 84% while Model-2 and Model-3 achieve prediction accuracies of 77% and 68% respectively or lower.
For Model-1, to verify which algorithm yields the best
learned model we see that IJGP-RBPF(1,2) and IJGP(2)S yield an accuracy of 83% and 81% respectively while
for Model-2, we see that the average accuracy of IJGPRBPF(1,2) and IJGP(2)-S was 76% and 75% respectively.
From these two results, we can see that IJGP-RBPF(1,2)
and IJGP(2)-S are the best performing learning algorithms.
For Model-1 and Model-2, to verify which algorithm yields
the best accuracy given a learned model, we see that
IJGP(2)-S is the most cost-effective alternative in terms
time versus accuracy while IJGP-RBPF yields the best accuracy.
Table 1: Goal-prediction: Model-1
N
100
100
200
200
500
500

Inference
IJGP-RBPF(1,1)
IJGP-RBPF(1,2)
IJGP-RBPF(1,1)
IJGP-RBPF(1,2)
IJGP-RBPF(1,1)
IJGP-RBPF(1,2)
IJGP(1)-S
IJGP(2)-S
Average

Time
12.3
15.8
33.2
60.3
123.4
200.12
9
34.3

LEARNING
IJGP-RBPF
IJGP-S
(1,1)
(1,2)
(1)
(2)
78
80
79
80
81
84
78
81
80
84
77
82
80
84
76
82
81
84
80
82
84
84
81
82
79
79
77
79
74
84
78
82
79.625
82.875
78.25
81.25

6.2 FINDING THE ROUTE TAKEN BY THE
PERSON
To see how our models predict a person’s route, we use
the following method. We first run our inference algorithm
on the learned model and predict the route that the person
is likely to take. We then super-impose this route on the
actual route taken by the person. We then count the number
of roads that were not taken by the person but were in the
predicted route i.e. the false positives, and also compute
the number of roads that were taken by the person but were
not in the actual route i.e. the false negatives. The two
measures are reported in Table 4 for the best performing
learning models in each category: viz GBP-RBPF(1,2) for
Model-1 and Model-2 and GBP-RBPF(1,1) for Model-3.
As we can see Model-1 and Model-2 have the best route
prediction accuracy (given by low false positives and false
negatives).

Table 3: Goal Prediction Model-3
N
100
200
500

Inference
IJGP-RBPF(1,1)
IJGP-RBPF(1,1)
IJGP-RBPF(1,1)
IJGP(1)-S
Average

Time
2.2
4.7
12.45
1.23

LEARNING
IJGP-RBPF(1,1)
IJGP(1)-S
68
61
67
64
68
63
66
62
67.25
62.5

7 RELATED WORK
[Liao et al., 2004] and [Patterson et al., 2003] describe a
model based on AHMEM [Bui, 2003] and Hierarchical
Markov Models (HMMs) respectively for inferring highlevel behavior from GPS-data. Our model goes beyond
their model by representing two new variables day-of-week
and time-of-day which improves the accuracy in our model
by about 6%.
A mixed network framework for representing deterministic and uncertain information was presented
in [Dechter and Larkin, 2001, Larkin and Dechter, 2003,
Dechter and Mateescu, 2004]. These previous works also
describe exact inference algorithms for mixed networks
with the restriction that all variables should be discrete.
Our work goes beyond these previous works in that we describe approximate inference algorithms for the mixed network framework, allow continuous Gaussian nodes with
certain restrictions in the mixed network framework and
model discrete-time stochastic processes. The approximate inference algorithms called IJGP(i) described in
[Dechter et al., 2002] handled only discrete variables. In
our work, we extend this algorithm to include Gaussian
variables and discrete constraints. We also develop a sequential version of this algorithm for dynamic models.
Particle Filtering is a very attractive research
area [Doucet et al., 2000]. Particle Filtering in HDMNs
can be inefficient if non-solutions of constraint portion
have high probability of being sampled. We show how
to alleviate this difficulty by performing IJGP(i) before
sampling. This algorithm IJGP-RBPF yields the best
performance in our settings and might prove to be useful
in applications in which particle filtering is preferred.

Table 2: Goal Prediction: Model-2
N
100
100
200
200
500
500

Inference
IJGP-RBPF(1,1)
IJGP-RBPF(1,2)
IJGP-RBPF(1,1)
IJGP-RBPF(1,2)
IJGP-RBPF(1,1)
IJGP-RBPF(1,2)
IJGP(1)-S
IJGP(2)-S
Average

Time
8.3
14.5
23.4
31.4
40.08
51.87
6.34
10.78

LEARNING
IJGP-RBPF
IJGP-S
(1,1)
(1,2)
(1)
(2)
73
73
71
73
76
76
71
75
76
77
71
75
76
77
71
76
76
77
71
76
76
77
71
76
71
73
71
74
76
76
72
76
75
75.75
71.125
75.125

Table 4: False positives (FP) and False negatives for routes
taken by a person (FN)
N

100
200
100
200

INFERENCE
IJGP(1)
IJGP(2)
IJGP-RBPF(1,1)
IJGP-RBPF(1,1)
IJGP-RBPF(1,2)
IJGP-RBPF(1,2)

Model1
FP/FN
33/23
31/17
33/21
33/21
32/22
31/22

Model2
FP/FN
39/34
39/33
39/33
39/33
42/33
38/33

Model3
FP/FN
60/55
60/54
58/43

8 CONCLUSION AND FUTURE WORK
In this paper, we introduced a new modeling framework
called HDMNs, a representation that handles discrete-timestochastic processes, deterministic and probabilistic information on both continuous and discrete variables in a systematic way. We also propose a GBP-based algorithm
called IJGP(i)-S for approximate inference in this framework. The main algorithmic contribution of this paper
is presenting a class of Rao-Blackwellised particle filtering algorithm, IJGP-RBPF for HDMNs which integrates
a generalized belief propagation component with a RaoBlackwellised Particle Filtering scheme for effective sampling in the presence of constraints. Another contribution
of this paper is addressing a complex and highly relevant
real life domain using a general framework and domain independent algorithms. Directions for future work include
relaxing the restrictions made on dependencies between
discrete and continuous variables and developing an efficient EM-algorithm.

ACKNOWLEDGEMENTS
The first and third author were supported in part by National Science Foundation under award numbers 0331707
and 0331690. The second-author was supported in part by
the NSF grant IIS-0412854.


