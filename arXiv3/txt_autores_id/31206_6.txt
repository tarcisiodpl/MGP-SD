

very computationally-efficient Monte­
Carlo algorithm for the calculation of
Dempster-Shafer belief is described. If Bel
is the combination using Dempster's Rule of
belief functions Bel1, . . . , Belm then, for sub­
set b of the frame 8, Bel(b) can be calcu­
lated in time linear in 181 and m (given that
the weight of conflict is bounded). The algo­
rithm can also be used to improve the com­
plexity of the Shenoy-Shafer algorithms on
Markov trees, and be generalised to calculate
Dempster-Shafer Belief over other logics.
A

1

INTRODUCTION

One of the major perceived problems with application
of the Dempster-Shafer Theory [Shafer, 76] has been
its apparent computational complexity e.g., [ Kyburg,
87], [Bonissone, 87]. This is because the Dempster­
Shafer theory as usually implemented involves re­
peated application of Dempster's Rule of Combina­
tion, keeping a record at each stage of each subset of
8 with a non-zero mass. For example the combination
of m simple support functions can have as many as 2q
non-zero masses where q is the minimum of m and 181,
thus making the approach computationally infeasible
for large m and 181.
There have been a number of schemes to deal with this;
[ Barnett, 81] showed how calculation of Dempster­
Shafer belief in a very special case, when all the ev­
idence sets are either singletons or complements of
singletons, belief could be calculated in linear time.
[ Gordon and Shortliffe, 85] extended this with an effi­
cient approximation to Dempster-Shafer belief for hier­
archically related evidences, and it was shown in both
[ Shafer and Logan, 87] and [Wilson, 87] that the hi­
erarchical case could be dealt with exactly in a com­
putationally efficient manner. The Shafer-Logan al­
gorithm was generalised to propagation of belief func­
tions in Markov Trees [ Shafer and Shenoy, 88] but,

although this is a very important contribution, it still
requires that the product space associated with the
largest clique is small, a condition which will by no
means always be satisfied. The hierarchical evidence
algorithm in [ Wilson, 87] was generalised to arbitrary
evidence sets [Wilson, 89] and, because it calculates
belief directly without first calculating the masses, it
leads to very substantial increases in efficiency (see sec­
tion 4). However this algorithm appears to have com­
plexity worse than polynomial, which is not surprising
since Dempster's Rule is #?-complete [Orponen, 90],
[ P rovan, 90].
This paper describes the Monte-Carlo algorithm given
in [Wilson, 89] which also calculates belief directly (or,
more accurately, it approximates belief up to arbi­
trary accuracy). This calculation has very low com­
plexity, showing that the general pessimism about
the complexity of Dempster-Shafer Theory is mis­
guided. The use of Monte-Carlo algorithms for calcu­
lating Dempster-Shafer belief has also been suggested
in [P earl, 88], [ Kampke, 88] and [ Kreinovich and Bar­
rett, 90].
2

THE MONTE-CARLO
ALGORITHM

Let Belt, . .., Belm be belief functions on a finite frame
8, and let Bel= Belt Etl· ·EtlBelm be their combination
using Dempster's Rule. Using the model of [ Dempster,
67] Bel; is represented by a probability function P; (on
a finite set rl;) and a compatibility function f; : rl; >-->
2° where the meaning of f; is 'for r E rl;, if r is true
then so is f;( r)'.
·

The mass function m; is given by:
m;(f;(c;))= P;(ci), and, for b S::: 8,
Bel;(b)= P;(f;(c;) s;; b), that is,

for c; E rl;,

I:

P;(c;).

,,,r,(,,)�b

Let n = nl X . X Slm and for c = (c), ... ,c,)
define f (c) = n::1 f;(c; ). Define the 'independent
.

.

nte C arlo Algorithm for Dempster-Shafer Belief

A Mo

probability function' P' on f! by P'((c:1, . . ,c:m))
.

n::1 P;(c:;).

Using [ Dempster, 67] it can be seen that
Bel(b)= P'(r(c:) <;; blf(c:) =1 0),
where e.g. P'(r(c:) # 0) just means L<:r(<);>!0 P'(c:).
r(c:) can be viewed as a random set [Nguyen, 78].
The Monte-Carlo algorithm just simulates the last
equation.
A large number, N, of trials are performed. For each
trial:
Randomly pick c: such that f(c:) # 0:
a. For i = 1,... , m
randomly pick an element of f!;, i.e.
pick c; with probability P;(c;)
Let c = (c:1, . . ,em )
b. If r(c:) = 0 then restart trial;
2. If f(c:) <;; b then trial succeeds, letT= 1
else trial fails, let T = 0

1.

.

The proportion of trials that succeed converges to
Bel( b):
E[T] = P'(r(c:) <;; bll'(c:) # 0) = Bel(b).
Var[T] = E[T2]-(E[T])2 = E[T]-(E[T])2 = Bel(b)(1Bel(b)) :S: t·
Let f be the average value ofT over the N trials, i.e.,
the proportion of trials that succeed.
N �T)

= Bel(b)
NVar[TJ < ...!.._
- 4N
N2

E[f]

=

and Var[t]

_

Therefore the variance (an<!_ so also the standard de­
viation) for the estimate, T, of Bel(b) can be made
arbitrarily small independently of 181 and m.
Let us say that the estimate t_ of Bel(b) 'has accuracy
k' if 3 standard deviations ofT is less than or equal to
k. Then f has accuracy k if N 2: -,&,-.
Testing separately if r(c:) = 0 and if f( c:) <;; b wastes
time; these tests can be combined within the same
algorithm (where Xj denotes the jth element of 8):
For each trial:
repeat

pick c: with probability P'(c:)
: 1; T := 1
T0 =
for j = 1 to 181
if

f(c:)

3 Xj then

T0 =
:

if Xj

0;

tf_ b

(since f(c:) # 0)
:= 0; exit trial;

then T

-

end if
end if
next j
until T0 = 0

3

(since r(c:) �b)

COMPUTATION TIME

Picking c; involves m random numbers so takes less
than Am where A is constant, approximately the time
it takes to generate a random number (with efficient
storing of the P;s). Testing if r(c:) 3 Xj takes less than
Bm for constant B. For a given trial there is a prob­
ability K = P'(r(c:) = 0) that the repeat-until loop
will be entered a second time. The expected number
of repeat-until loops per trial is 1��; K is a measure
of the conflict of the evidences [Shafer, 76, p65].
Thus the expected time the algorithm takes is less than
1�� m(A+ Bl81), and so the expected time to achieve
accuracy k is less than 4(1 9�)pm(A + Bl81).
At least for the case where the Bel;s are simple sup­
port functions, the condition f(c) 3 Xj can be tested
more efficiently; under weak conditions this leads to
expected time of less than 4(1 9�)k2 (Am+ Cl81) for
constant C [Wilson, 89].
4

EXPERIMENTAL RESULTS

The algorithm for the case where the Bel;s are simple
support functions has been implemented and tested
using the language Modula-2 on a SUN 3/60 worksta­
tion. The results showed that the value of A is much
bigger than the value of C in this implementation, A
being roughly 40�0 seconds and C roughly 50 �00 sec­
time taken to ge�erate a
onds. A is essentially the
1
random number, and 4000 seconds seems rather slow
for that. This suggests that very substantial speed­
ups (of perhaps an order of magnitude or two) could
be achieved by careful choice and use of the random
number generator and the use of antithetic runs (so
that the random number generator is only used once
for several different data items).
The results indicate that, unless the evidences are ex­
tremely conflicting, the Monte-Carlo algorithm is prac­
tical for problems with large m and 8. For example,
with K = 0.5, m = 181 = 40, and with 1000 tri­
als, the calculation of the approximate value of Bel(b)
would be expected to take 20.6 seconds. The 1000
trials mean that the standard deviation is less than
0.016, and so the confidence interval for the correct
value of belief corresponding to 3 standard deviations
would be roughly [ b- 0.05, b+ 0.05] . If instead we did
10,000 trials this would take a little over 3 minutes,
and give a standard deviation of 0.005. Extrapolating
the figures (which seems unlikely to cause problems in

415

416

Wilson

this case) gives an approximate time of 1 minute for
m = 101 = 120, with 1000 trials, and 5 minutes for
m = 101 = 600.
Also in [Wilson, 89] an exact algorithm for calcu­
lating belief is described (related to those described
in [P rovan, 90]) which involves expressing the event
f(E) s;; b as a boolean expression and then calculat­
ing the probability of this using the laws of boolean
algebra. Again this avoids explicit calculation of the
masses. The complexity for the simple support func­
tion case appears to be approximately of the form

l0llogm.

The usual approaches for calculating belief are mass­
based: they calculate the combined mass function and
use this to calculate the appropriate belief (a good one
of these is the fast Mobius transform in [Kennes and
Smets, 90]). For large m and 0 this is of necessity very
computationally expensive, since if q = min(m, 101),
there can be as many as 2q masses. For simplicity it is
assumed that the calculation of belief then just does 2q
REAL multiplications. The speed of REAL multipli­
cation was tested on the same workstation and within
the same language that the Exact and Monte-Carlo al­
gorithms were tested and implemented on and it was
found that it did just over 104 REAL multiplications
per second. This gives the following results:
m,n
15 X
20 X
25 X
30 X
35 X
50 X

15
20
25
30
35
50

MC
7 sees
11 sees
13 sees
15 sees
17 sees
25 sees

Mass-based 2:
3 sees
1 min
1 hour
1 day
1 month
3000 years

Exact
9 sees
13 sees
46 sees
3 mins
8 mins
2 hours

The values for the Monte-Carlo algorithm were based
on doing 1000 trials and the contradiction being 0.5.
The figure of 2 hours for the Exact in the 50 case is a
very rough upper bound derived from insufficient data.
Details of the experiments and the full results and
analysis are given in [Wilson, 90b].

5

THE GENERALISED

(or f((El, . . . , Em)) = the set {f;(E;)
the logic doesn't have conjunction).

:

i

=

1, ... , m} if

For each trial:
1. Randomly pick E such that
f(E) is not contradictory:
a. For i = 1, . . . , m
randomly pick an element of ll;, i.e.
pick E; with probability P;(t:;)
Let E = (<!, . . . ,Em)

b. If f(c:) is contradictory then restart trial;
2. If b can be deduced from r(c:)
then trial succeeds, let T = 1
else trial fails, let T = 0

Undecidability and semi-decidability would clearly
cause problems, in which case trials which went on
for too long would have to be cut short; if T for these
trials was given the value 0 then this would lead to a
lower bound for Bel(b). This technique of prematurely
halting trials that take too long could be used to in­
crease the efficiency for other cases as well, at the cost
of only finding lower and upper bounds for Bel(b).
The time this algorithm takes is then approximately
���(Am + R) where R is the average time it takes to

see if f(c:) is contradictory, and if f(c:) allows b to be
deduced. Given that the weight of conflict of the ev­
idences is bounded this means that the complexity is
proportional to that of proof in the logic; it is hard to
see how any sensible uncertainty calculus could do bet­
ter than this (although the complexity for this Monte­
Carlo algorithm has a very large constant term if high
accuracy is required).
As Shafer points out [Shafer, 90] 101 can be a large
product space, making the first algorithm impractical.
The generalised algorithm can also be used to greatly
improve the complexity of the algorithms for calcu­
lating Belief in Markov trees [Shafer and Shenoy, 88].
For each trial, propositions (i.e. belief functions with
a single focal element) must be propagated through
the Markov tree. The complexity is then proportional
to that of propagating propositions, rather than the
whole belief functions. Some other propositional cases
have been dealt with in [Wilson, 89].

ALGORITHM
The algorithm can be generalised to deal with arbi­
trary logics [Wilson, 90a]. Let L be the language of
some logic. For each i, Bel; is now a function from L
to [0, 1] saying how much the evidence warrants belief
in propositions in L and the compatibility function is
a function f; : ll; f-> L. The combined compatibility
function r is now defined by

f((El, ... ,Em)) =

m

1\ f;(E;),
i=l

6

DISCUSSION

There are two obvious drawbacks with the Monte­
Carlo algorithm:
(i) if very high accuracy is required then the Monte­
Carlo algorithm will require a large number of trials
(quadratic in the reciprocal of accuracy) so giving a
very high constant factor to the complexity;
(ii) when the evidence is highly conflicting the Monte­
Carlo algorithm loses some of its efficiency. I don't see

A Monte-Carlo Algorithm for Dempster-Shafer Belief

this as a great problem since an extremely high weight
of conflict would suggest, except in exceptional circum­
stances, that Dempster's Rule is being applied when
it is not valid, e.g. updating a Bayesian prior with a
Dempster-Shafer belief function [see Wilson, 91]. I also
argue there that, although Dempster's Rule has strong
justifications for the combination of a finite number of
simple support functions, the more general case has
not been convincingly justified: the Monte-Carlo algo­
rithm is guaranteed to give results in accordance with
Dempster's Rule, but it remains to be seen if these are
always sensible.
It may be important to know which relatively small
sets have relatively high beliefs: the Monte-Carlo algo­
rithm can be easily applied to deal with this problem.
Dempster's Rule makes particular independence as­
sumptions, using a single probability function on 0.
By modifying step 1 of the algorithms the beliefs cor­
responding to other probability functions on n can be
calculated.

Acknowledgements
I am currently supported by the ESPRIT basic re­
search action DRUMS (3085). Most of the material
in this paper was produced in the period Summer '87Summer '88 when I was employed by the The Hotel
and Catering Management, and Computing and Math­
ematical Sciences Departments of Oxford Polytechnic.
Thanks also to Bills Triggs and Boatman for their help
during this period, and more recently to Mike Clarke.



We investigate the computational complexity of testing dominance and consistency in CP-nets.
Previously, the complexity of dominance has been determined for restricted classes in which the
dependency graph of the CP-net is acyclic. However, there are preferences of interest that define
cyclic dependency graphs; these are modeled with general CP-nets. In our main results, we show
here that both dominance and consistency for general CP-nets are PSPACE-complete. We then
consider the concept of strong dominance, dominance equivalence and dominance incomparability,
and several notions of optimality, and identify the complexity of the corresponding decision problems. The reductions used in the proofs are from STRIPS planning, and thus reinforce the earlier
established connections between both areas.

1. Introduction
The problems of eliciting, representing and computing with preferences over a multi-attribute domain arise in many fields such as planning, design, and group decision making. However, in a
multi-attribute preference domain, such computations may be nontrivial, as we show here for the
CP-net representation. Natural questions that arise in a preference domain are, “Is this item preferred to that one?”, and “Is this set of preferences consistent?” More formally, a set of preferences
is consistent if and only if no item is preferred to itself. We assume that preferences are transitive,
i.e., if α is preferred to β, and β is preferred to γ, then α is preferred to γ.
An explicit representation of a preference ordering of elements, also called outcomes, of such
multi-variable domains is exponentially large in the number of attributes. Therefore, AI researchers
have developed languages for representing preference orderings in a succinct way. The formalism
of CP-nets (Boutilier, Brafman, Hoos, & Poole, 1999) is among the most popular ones. A CP-net
c 2008 AI Access Foundation. All rights reserved.

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

provides a succinct representation of preference ordering on outcomes in terms of local preference
statements of the form p : xi > x j , where xi , x j are values of a variable X and p is a logical condition.
Informally, a preference statement p : xi > x j means that given p, xi is strictly preferred to x j ceteris
paribus, that is, all other things being equal. The meaning of a CP-net is given by a certain ordering relation, called dominance, on the set of outcomes, derived from such reading of preference
statements. If one outcome dominates another, we say that the dominant one is preferred.
Reasoning about the preference ordering (dominance relation) expressed by a CP-net is far from
easy. The key problems include dominance testing and consistency testing. In the first problem,
given a CP-net and two outcomes α and β, we want to decide whether β dominates α. The second
problem asks whether there is a dominance cycle in the dominance ordering defined by an input
CP-net, that is, whether there is an outcome that dominates (is preferred to) itself.
We study the computational complexity of these two problems. The results obtained prior to this
work concerned only restricted classes of CP-nets, all requiring that the graph of variable dependencies implied by preference statements in the CP-net be acyclic. Under certain assumptions, the
dominance-testing problem is in NP and, under some additional assumptions, even in P (Domshlak
& Brafman, 2002; Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004a). We show that the complexity in the general case is PSPACE-complete, and this holds even for the propositional case, by
exhibiting in Section 4 a PSPACE-hardness proof for dominance testing.
We then turn to consistency testing. While acyclic CP-nets are guaranteed to be consistent, this
is not the case with general CP-nets (Domshlak & Brafman, 2002; Brafman & Dimopoulos, 2004).
In Section 5, we show that consistency testing is as hard as dominance testing.
In the following two sections we study decision problems related to dominance and optimality
in CP-nets. First, we consider the complexity of deciding strict dominance, dominance equivalence
and dominance incomparability of outcomes in a CP-net. Then, we study the complexity of deciding
the optimality of outcomes, and the existence of optimal outcomes, for several notions of optimality.
To prove the hardness part of the results, we first establish the PSPACE-hardness of some problems related to propositional STRIPS planning. We then show that these problems can be reduced
to CP-net dominance and consistency testing by exploiting connections between actions in STRIPS
planning and preference statements in CP-nets.
The complexity results in this paper address CP-nets whose dominance relation may contain
cycles. Most earlier work has concentrated on the acyclic model. However, as argued earlier, for
instance by Domshlak and Brafman (2002), acyclic CP-nets are not sufficiently expressive to capture human preferences on even some simple domains.1 Consider, for instance, a diner who has
to choose either red or white wine, and either fish or meat. Given red wine, they prefer meat, and
conversely, given meat they prefer red wine. On the other hand, given white wine, they prefer fish,
and conversely, given fish they prefer white wine. This gives a consistent cyclic CP-net, and there is
no acyclic CP-net giving rise to the same preferences on outcomes. So, such cyclicity of preference
variables does not necessarily lead to a cyclic order on outcomes.

1. We do not mean to say that cyclic CP-nets are sufficient to capture all possible human preferences on simple domains
– this is obviously not true. However, we note that every preference relation extends the preference relation induced
by some CP-net with possibly cyclic dependencies. Not only is this property no longer true when cyclic dependencies
are precluded but, in the case of binary variables, the number of linear orders that extends some acyclic CP-net is
exponentially smaller than the number of all linear orders (Xia, Conitzer, & Lang, 2008).

404

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

We assume some familiarity with the complexity class PSPACE. We refer to Papadimitriou
(1994) for details. In particular, we later use the identities NPSPACE = PSPACE = coPSPACE.
In several places, we will consider versions of decision problem, in which input instances are
assumed to have some additional property. Such problems are usually formulated in the following
way: “Q, given R”2 . We first note that “Q, given R” is not the same problem as “Q and R”. Let
us recall the definition of a decision problem as presented by Ausiello et al. (1999). A decision
problem is a pair P = hIP ,YP i where IP is a set of strings (formally, a subset of Σ∗ , where Σ is a
finite alphabet), The decision problem P = hIP ,YP i reads as follows: given a string x ∈ IP , decide
whether x ∈ YP . A problem hIP ,YP i is in a complexity class C if the language YP ⊆ Σ∗ is in C (this
does not depend on IP ). A problem hIQ ,YQ i is reducible to hIP ,YP i if there is a polynomial-time
function F such that (1) for every x ∈ IQ , F(x) ∈ IP , and (2) for every x ∈ IQ , x ∈ YQ if and only
if F(x) ∈ YP . Thus, if P is the decision problem “Q, given R”, then IP is the set of all strings
satisfying R, while YP is the set of all strings satisfying R ∩ Q. For all such problems, it is granted
that the input belongs to R; to solve them we do not have to check that the input string is indeed
an element of R. Such problems “Q, given R” are widespread in the literature. However, in most
cases, R is a very simple property, that can be checked in polynomial (and often linear) time, such
as “decide whether a graph possesses a Hamiltonian cycle, given that every vertex has a degree at
most 3”. Here, however, we will consider several problems “Q, given R” where R itself is not in the
class P (unless the polynomial hierarchy collapses). However, as we said above, the complexity of
recognizing whether a given string is in R does not matter. In other words, the complexity of “Q,
given R” is the same, whether R can be recognized in unit time or is PSPACE-complete. We will
come back to this when the first such problem appears in the paper (cf. the proof of Proposition 5).
In no case that we consider is the complexity of R greater than the complexity of Q.
A part of this paper (up to Section 5) is an extended version of our earlier conference publication
(Goldsmith, Lang, Truszczyński, & Wilson, 2005). Sections 6 and 7 are entirely new.

2. Generalized Propositional CP-Nets
Let V = {x1 , . . . , xn } be a finite set of variables. For each variable x ∈ V , we assume a finite domain
Dx of values. An outcome is an n-tuple (d1 , . . . , dn ) of Dx1 × · · · × Dxn .
In this paper, we focus on propositional variables: variables with binary domains. Let V be a
finite set of propositional variables. For every x ∈ V , we set Dx = {x, ¬x} (thus, we overload the
notation and write x both for the variable and for one of its values). We refer to x and ¬x as literals.
Given a literal l we write ¬l to denote the dual literal to l. The focus on binary variables makes the
presentation clearer and has no impact on our complexity results.
We also note that in the case of binary domains, we often identify an outcome with the set of
its values (literals). In fact, we also often identify such sets with the conjunctions of their elements.
Sets (conjunctions) of literals corresponding to outcomes are consistent and complete.
A conditional preference rule (sometimes, a preference rule or just a rule) over V is an expression p : l > ¬l, where l is a literal of some atom x ∈ V and p is a propositional formula over V that
does not involve variable x.
2. In the literature one often finds the following formulation: “Q, even if R”, which does not have exactly the same
meaning as “Q, given R”. Specifically, when saying “Q is NP-complete, even if R”, one means “Q is NP-complete,
and Q, given R is NP-complete as well”.

405

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

In the rest of the paper, we need to refer to two different languages: a conditional preference
language where for every (binary) variable x, the conditional preference table for x needs to specify
a preferred value of x for every possible assignment of its parent variables, and a more general
language where the tables may be incomplete (for some values of its parents, the preferred value of
x may not be specified) and/or locally inconsistent (for some values of its parents, the table may both
contain the information that x is preferred and the information that ¬x is preferred). We call these
languages respectively CP-nets and GCP-nets (for “generalized CP-nets”). Note that GCP-nets are
not new, as similar structures have been discussed before (Domshlak, Rossi, Venable, & Walsh,
2003). The reason why we use this terminology (“CP-nets” and “GCP-nets”) is twofold. First, even
if the assumptions of completeness and local consistency for CP-nets are sometimes relaxed, most
papers on CP-nets do make them. Second, we could have used “CP-nets” and “locally consistent,
complete CP-nets” instead of “GCP-nets” and “CP-nets”, but we felt our notation is simpler and
more transparent.
Definition 1 (Generalized CP-net) A generalized CP-net C (for short, a GCP-net) over V is a
set of conditional preference rules. For x ∈ V we define pC+ (x) and pC− (x), usually written just:
p+ (x) and p− (x), as follows: pC+ (x) is equal to the disjunction of all p such that there exists a rule
p : x > ¬x in C; pC− (x) is the disjunction of all p such that there exists a rule p : ¬x > x in C. We
define the associated directed graph GC (the dependency graph) over V to consist of all pairs (y, x)
of variables such that y appears in either p+ (x) or p− (x).
In our complexity results we will also need the following representation of GCP-nets: a GCPnet C is said to be in conjunctive form if C only contains rules p : l > ¬l such that p is a (possibly
empty) conjunction of literals. In this case all formulas p− (x), p+ (x) are in disjunctive normal form,
that is, a disjunction of conjunctions of literals (including ⊤ – the empty conjunction of literals).
GCP-nets determine a transitive relation on outcomes, interpreted in terms of preference. A
preference rule p : l > ¬l represents the statement “given that p holds, l is preferred to ¬l ceteris
paribus”. Its intended meaning is as follows. If outcome β satisfies p and l, then β is preferred to
the outcome α which differs from β only in that it assigns ¬l to variable x. In this situation we say
that there is an improving flip from α to β sanctioned by the rule p : l > ¬l.
Definition 2 If α0 , . . . , αm is a sequence of outcomes with m ≥ 1 and each next outcome in the
sequence is obtained from the previous one by an improving flip, then we say that α0 , . . . , αm is an
improving sequence from α0 to αm for the GCP-net, and that αm dominates α0 , written α0 ≺ αm .
Finally, a GCP-net is consistent if there is no outcome α which is strictly preferred to itself, that
is, such that α ≺ α.
The main objective of the paper is to establish the complexity of the following two problems
concerning the notion of dominance associated with GCP-nets (sometimes under restrictions on the
class of input GCP-nets).
Definition 3
GCP - DOMINANCE : given a GCP-net C and two outcomes α and β, decide whether α ≺ β in C, that
is, whether β dominates α in C.
GCP - CONSISTENCY : given a GCP-net C, decide whether C is consistent.
406

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

GCP-nets extend the notion of CP-nets (Boutilier et al., 1999). There are two properties of
GCP-nets that are essential in linking the two notions.
Definition 4
A GCP-net C over V is locally consistent if for every x ∈ V , the formula pC− (x) ∧ pC+ (x) is unsatisfiable. It is locally complete if for every x ∈ V , the formula pC− (x) ∨ pC+ (x) is a tautology.
Informally, local consistency means that there is no outcome in which both x is preferred over
¬x and ¬x is preferred over x. Local completeness means that, for every variable x, in every outcome
either x is preferred over ¬x or ¬x is preferred over x.
Definition 5 (Propositional CP-net) A CP-net over the set V of (propositional) variables is a locally consistent and locally complete GCP-net over V .
It is not easy to decide whether a GCP-net is actually a CP-net. In fact, the task is coNPcomplete.
Proposition 1 The problem of deciding, given a GCP-net C, whether C is a CP-net is coNPcomplete.
Proof: Deciding whether a GCP-net C is a CP-net consists of checking local consistency and local
completeness. Each of these tasks amounts to n validity tests (one for each variable). It follows that
deciding whether a GCP-net is a CP-net is the intersection of 2n problems from coNP. Hence, it is
in coNP, itself. Hardness comes from the following reduction from UNSAT. To any propositional
formula ϕ we assign the CP-net C(ϕ), defined by its set of variables Var(ϕ)∪{z}, where z 6∈ Var(ϕ),
and the following tables:
−
+
• for any variable x 6= z: pC(ϕ)
(x) = ⊤; pC(ϕ)
(x) = ⊥;
−
+
(z) = ⊥.
(z) = ¬ϕ; pC(ϕ)
• pC(ϕ)
−
−
+
+
(z) = ⊥. There(z) ∧ pC(ϕ)
(x) = ⊥; moreover, pC(ϕ)
(x) ∧ pC(ϕ)
For any variable x 6= z, we have pC(ϕ)
−
+
fore, C(ϕ) is locally consistent. Now, for any variable x 6= z, we have pC(ϕ) (x) ∨ pC(ϕ)
(x) = ⊤.
−
+
Moreover, pC(ϕ) (z) ∨ pC(ϕ) (z) = ¬ϕ. Thus, C(ϕ) is locally complete if and only if ϕ is unsatisfiable.
It follows that C(ϕ) is a CP-net if and only if ϕ is unsatisfiable.


Many works on CP-nets make use of explicit conditional preference tables that list every combination of values of parent variables (variables on which x depends) exactly once, each such combination designating either x or ¬x as preferred.3 Clearly, CP-nets in this restricted sense can be
regarded as CP-nets in our sense that, for every variable x, satisfy the following condition:
if y1 , . . . , yk are all the atoms appearing in p+ (x) and p− (x) then every complete and
consistent conjunction of literals over {y1 , . . . , yn } appears as a disjunct in exactly one
of p+ (x) and p− (x).
3. There are exceptions. Some are discussed for instance by Boutilier et al. (2004a) in Section 6 of their paper.

407

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

Under this embedding, the concepts of dominance and consistency we introduced here for GCP-nets
generalize the ones considered for CP-nets as defined by Boutilier et al. (2004a).
Problems CP - DOMINANCE and CP - CONSISTENCY are defined analogously to Definition 3. In
the paper we are interested in the complexity of dominance and consistency problems for both GCPnets and CP-nets. Therefore, the matter of the way in which these nets (especially CP-nets, as for
GCP-nets there are no alternative proposals) are represented is important. Our representation of
CP-nets is often more compact than the one proposed by Boutilier et al. (2004a), as the formulas
p+ (x) and p− (x) implied by the conditional preference tables can often be given equivalent, but
exponentially smaller, disjunctive normal form representations. Thus, when defining a decision
problem, it is critical to specify the way to represent its input instances, as the representation may
affect the complexity of the problem. Unless stated otherwise, we assume that GCP-nets (and thus,
CP-nets) are represented as a set of preference rules, as described in Definition 1. Therefore, the
size of a GCP-net is given by the total size of the formulas p− (x), p+ (x), x ∈ V .
We now note a key property of consistent GCP-nets, which we will use several times later in the
paper.
Proposition 2 If a GCP-net C is consistent then it is locally consistent.
Proof: If C is not locally consistent then there exists a variable x and an outcome α satisfying
pC− (x) ∧ pC+ (x). Then α ≺ α can be shown by flipping x from its current value in α to the dual value
and then flipping it back: since α satisfies pC− (x) ∧ pC+ (x), and since pC− (x) ∧ pC+ (x) does not involve
any occurrences of x, both flips are allowed.

Finally, we conclude this section with an example illustrating the notions discussed above.
Example 1 Consider a GCP-net C on variables V = {x, y} with four rules, defined as follows:
x : y > ¬y; ¬x : ¬y > y; y : ¬x > x; ¬y : x > ¬x. We have p+ (y) = x, p− (y) = ¬x, p+ (x) = ¬y and
p− (x) = y. Therefore C is locally consistent and locally complete, and so is a CP-net.
There is a cycle of dominance between outcomes: x ∧ y ≺ ¬x ∧ y ≺ ¬x ∧ ¬y ≺ x ∧ ¬y ≺ x ∧ y,
and so C is inconsistent. This shows that consistency is a strictly stronger property than local
consistency.

3. Propositional STRIPS Planning
In this section we derive some technical results on propositional STRIPS planning which form the
basis of our complexity results in Sections 4 and 5. We establish the complexity of plan existence
problems for propositional STRIPS planning under restrictions on input instances that make the
problem of use in the studies of dominance and consistency in GCP-nets.
Let V be a finite set of variables. A state over V is a complete and consistent set of literals over
V , which we often view as the conjunction of its members. A state is therefore equivalent to an
outcome, defined in a CP-nets context.
Definition 6 (Propositional STRIPS planning) By a propositional STRIPS instance we mean a
tuple hV, α0 , γ, ACTi, where
1. V is a finite set of propositional variables;
408

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

2. α0 is a state over V , called the initial state;
3. γ is a state called the goal;4
4. ACT is a finite set of actions, where each action a ∈ ACT is described by a consistent conjunction of literals pre(a) (a precondition) and a consistent conjunction of literals post(a) (a
postcondition, or effect).5
An action a is executable in a state α if α |= pre(a). The effect of a in state α, denoted by eff (a, α),
is the state α′ containing the same literals as α for all variables not mentioned in post(a), and
the literals of post(a). We assume that an action can be applied to any state, but that it does not
change the state if its preconditions do not hold: if α 6|= pre(a) (given that states are complete,
this is equivalent to α |= ¬pre(a)) then eff (a, α) = α. This assumption has no influence as far as
complexity results are concerned.
The PROPOSITIONAL STRIPS PLAN EXISTENCE problem, or STRIPS PLAN for short, is to decide whether for a given propositional STRIPS instance hV, α0 , γ, ACTi there is a finite sequence
of actions leading from the initial state α0 to the final state γ. Each such sequence is a plan for
hV, α0 , γ, ACTi. A plan is irreducible if every one of its actions changes the state.
We assume, without loss of generality, that for any action a, no literal in post(a) appears also
in pre(a); otherwise we can omit the literal from post(a) without changing the effect of the action;
if post(a) then becomes an empty conjunction, the action a can be omitted from ACT as it has no
effect.
We have the following result due to Bylander (1994).
Proposition 3 (Bylander, 1994)

STRIPS PLAN

is PSPACE-complete.

Typically, propositional STRIPS instances do not require that goals be states. Instead, goals are
defined as consistent conjunctions of literals that do not need to be complete. In such a setting, a
plan is a sequence of actions that leads from the start state to a state in which the goal holds. We
restrict consideration to complete goals. This restriction has no effect on the complexity of the plan
existence problem: it remains PSPACE-complete under the goal-completeness restriction (Lang,
2004).
3.1 Acyclic STRIPS
Definition 7 (Acyclic sets of actions) A set of actions ACT (we use the same notation as in Definition 6) is acyclic if there is no state α such that hV, α, α, ACTi has a non-empty irreducible plan,
that is to say, if there are no non-trivial directed cycles in the graph on states induced by ACT.
We will now establish the complexity of the following problem:
ACTION - SET ACYCLICITY :

given a set ACT of actions, decide whether ACT is acyclic.

Proposition 4
ACTION - SET ACYCLICITY is PSPACE-complete.
4. Note that in standard STRIPS the goal can be a partial state. This point is discussed just after Proposition 3.
5. We emphasize that we allow negative literals in preconditions and goals. Some definitions of STRIPS do not allow
this. This particular variant of STRIPS is sometimes called PSN (propositional STRIPS with negation) in the literature.

409

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

Proof: The argument for the membership in PSPACE is standard; we nevertheless give some details.
We will omit such details for further proofs of membership in PSPACE. The following nondeterministic algorithm decides that ACT has a cycle:
guess α0 ;
α := α0 ;
repeat
guess an action a ∈ ACT ;
α′ := eff (a, α);
α := α′
until α = α0 .
This algorithm works in nondeterministic polynomial space (because we only need to store α0 ,
α and α′ ), which shows that ACTION - SET ACYCLICITY is in NPSPACE, and therefore in PSPACE,
since NPSPACE = PSPACE. Thus, ACTION - SET ACYCLICITY is in coPSPACE, hence in PSPACE,
since coPSPACE = PSPACE.
We will now show that the complement of the ACTION - SET ACYCLICITY problem is PSPACEhard by reducing the ACYCLIC STRIPS PLAN problem to it.
Let PE = hV, α0 , γ, ACTi be an instance of the ACYCLIC STRIPS PLAN problem. In particular,
we have that ACT is acyclic. Let a be a new action defined by pre(a) = γ and post(a) = α0 . It is easy
to see that ACT ∪ {a} is not acyclic if and only if there exists a plan for PE. Thus, the PSPACEhardness of the complement of the ACTION - SET ACYCLICITY problem follows from Proposition
5. Consequently, the ACTION - SET ACYCLICITY problem is coPSPACE-hard. Since PSPACE =
coPSPACE, the ACTION - SET ACYCLICITY problem is PSPACE-hard, as well.

Next, we consider the STRIPS planning problem restricted to instances that have acyclic sets of
actions. Formally, we consider the following problem:
ACYCLIC STRIPS PLAN : Given a propositional STRIPS instance hV, α0 , γ, ACTi such
that ACT is acyclic and α0 6= γ, decide whether there is a plan for hV, α0 , γ, ACTi

This is the first of our problems of the form “Q, given R” that we encounter and it illustrates
well the concerns we discussed at the end of the introduction. Here, R is the set of all propositional
STRIPS instances hV, α0 , γ, ACTi such that ACT is acyclic, and Q is the set of all such instances for
which there is a plan for hV, α0 , γ, ACTi. Checking whether a given propositional STRIPS instance
is actually acyclic is itself PSPACE-complete (this is what Proposition 4 states), but this does not
matter when it comes to solving ACYCLIC STRIPS PLAN: when considering an instance of ACYCLIC
STRIPS PLAN , we already know that it is acyclic (and this is reflected in the reduction below).
Proposition 5
ACYCLIC STRIPS PLAN

is PSPACE-complete.

Proof: The argument for the membership in PSPACE is standard (cf. the proof of Proposition 4). To
prove PSPACE-hardness, we first exhibit a polynomial-time reduction F from STRIPS PLAN. Let
PE = hV, α0 , γ, ACTi be an instance of STRIPS PLAN. The idea behind the reduction is to introduce
a counter, so that each time an action is executed, the counter is incremented. The counter may
count up to 2n , where n = |V |, making use of n additional variables. The counter is initialized to
410

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

0. Once it reaches 2n − 1 it can no longer be incremented and no action can be executed. Hence,
the set of actions in the resulting instance of STRIPS PLAN is acyclic: we are guaranteed to produce
an instance of R. To describe the reduction, we write V as {x1 , . . . , xn }. We define F(PE) = PE′ =
hV ′ , α′0 , γ′ , ACT ′ i as follows:
• V ′ = {x1 , . . . , xn , z1 , . . . , zn }, where zi are new variables we will use to implement the counter;
• α′0 = α0 ∧ ¬z1 ∧ · · · ∧ ¬zn ;
• γ′ = γ ∧ z1 ∧ · · · ∧ zn ;
• for each action a ∈ ACT, we include in ACT ′ n actions ai , 1 ≤ i ≤ n, such that:

pre(ai ) = pre(a) ∧ ¬zi ∧ zi+1 ∧ · · · ∧ zn
– for i ≤ n − 1 :
post(ai ) = post(a) ∧ zi ∧ ¬zi+1 ∧ · · · ∧ ¬zn , and

pre(an ) = pre(a) ∧ ¬zn
– for i = n :
post(an ) = post(a) ∧ zn .
• Furthermore, we include in ACT ′ n actions bi , 1 ≤ i ≤ n, such that:

pre(bi ) = ¬zi ∧ zi+1 ∧ · · · ∧ zn
– for i ≤ n − 1 :
post(bi ) = zi ∧ ¬zi+1 ∧ · · · ∧ ¬zn , and

pre(bn ) = ¬zn
– for i = n :
post(bn ) = zn .
We will denote states over V ′ by pairs (α, k), where α is a state over V and k is an integer, 0 ≤
k ≤ 2n − 1. We view k as a compact representation of a state over variables z1 , . . . , zn : assuming that
the binary representation of k is d1 . . . dn (with dn being the least significant digit), k represents the
state which contains zi if di = 1 and ¬zi , otherwise. For instance, let V = {x1 , x2 , x3 }. Then we have
V ′ = {x1 , x2 , x3 , z1 , z2 , z3 }, and the state ¬x1 ∧ x2 ∧ x3 ∧ z1 ∧ ¬z2 ∧ z3 is denoted by (¬x1 ∧ x2 ∧ x3 , 5).
We note that the effect of ai or bi on state (α, k) is either void, or increments the counter:
eff (ai , (α, k)) =



(eff (a, α), k + 1) if ai is executable in (α, k)
(α, k)
otherwise

eff (bi , (α, k)) =



(α, k + 1) if bi is executable in (α, k)
(α, k)
otherwise

Next, we remark that at most one ai and at most one bi are executable in a given state (α, k).
More precisely,
• if k < 2n − 1, then exactly one bi is executable in (α, k); denote by i(k) the index such that bi(k)
is executable in (α, k) (this index depends only on k). We also have that ai(k) is executable in
(α, k), provided that a is executable in α.
• if k = 2n − 1, then no ai and no bi is executable in (α, k).
411

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

Now we show that PE′ is acyclic. Assume π is an irreducible plan for hV ′ , α′ , α′ , ACT ′ i. Let
= (α, k). If k < 2n − 1, then π is empty, since any action in ACT ′ in any state either is nonexecutable or increments the counter, and an irreducible plan contains only actions whose effect is
non-void. If k = 2n − 1, then no action of ACT ′ is executable in α′ and again π is empty. Thus, there
exists no non-empty irreducible plan for hV ′ , α′ , α′ , ACT ′ i, and this holds for all α′ . Therefore PE′
is acyclic.
We now claim that there is a plan for PE if and only if there is a plan for PE′ . First, assume that
there is a plan in PE. Let π be a shortest plan in PE and let m be its length (the number of actions
used). We have m ≤ 2n − 1, since no state along π repeats (otherwise, shorter plans than π for PE
would exist). Let α0 , α1 , . . . , αm = γ be the sequence of states obtained by executing π. Let a be the
action used in the transition from αk to αk+1 . Since k < 2n − 1 (because m ≤ 2n − 1 and k ≤ m − 1),
there is exactly one i, 1 ≤ i ≤ n, such that the action ai applies at the state (α, k) over V ′ . Replacing
a with ai in π yields a plan that when started at (α0 , 0) leads to (αm , m) = (γ, m). Appending that
plan with appropriate actions bi to increment the counter to 2n − 1 yields a plan for PE′ . Conversely,
if τ is a plan for PE′ , the plan obtained from τ by removing all actions of the form b j and replacing
each action ai with a is a plan for PE, since ai has the same effect on V as a does. Thus, the claim
follows.

α′

We emphasize that this reduction F from STRIPS PLAN to ACYCLIC STRIPS PLAN (or, equivalently, to STRIPS PLAN given ACTION - SET ACYCLICITY) works because it satisfies the following
two conditions:
1. for every instance PE of STRIPS PLAN, F(PE) is an instance of ACYCLIC STRIPS PLAN (this
holds because for every PE, F(PE) is acyclic);
2. for every PE of STRIPS PLAN, F(PE) is a positive instance of ACYCLIC
only if PE is a positive instance of STRIPS PLAN.

STRIPS PLAN

if and

3.2 Mapping STRIPS Plans to Single-Effect STRIPS Plans
Versions of the STRIPS PLAN and ACYCLIC STRIPS PLAN problems that are important for us allow only actions with exactly one literal in their postconditions in their input propositional STRIPS
instances. We call such actions single-effect actions.6 We refer to the restricted problems as SE
STRIPS PLAN and ACYCLIC SE STRIPS PLAN , respectively.
To prove PSPACE-hardness of both problems, we describe a mapping from STRIPS instances to
single-effect STRIPS instances.7
Consider an instance PE = hV, α0 , γ, ACTi of the STRIPS PLAN problem, where ACT is not necessarily acyclic. For each action a ∈ ACT we introduce a new variable xa , whose intuitive meaning
is that action a is currently being executed.
V
We set X = a∈ACT ¬xa . That is, X is the conjunction of negative literals of all the additional
V
variables. In addition, for each a ∈ ACT we set Xa = xa ∧ b∈ACT−{a} ¬xb . We now define an
instance PE′ = hV ′ , α′0 , γ′ , S(ACT)i of the SE STRIPS PLAN problem as follows:
6. Such actions are also called “unary” actions in the planning literature. We stick to the terminology “single-effect”
although it is less commonly used, simply because it is more explicit.
7. PSPACE-completeness of propositional STRIPS planning with single-effect actions was proved already by Bylander
(1994). However, to deal with acyclicity we need to give a different reduction than the one used in that paper.

412

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

• Set of variables: V ′ = V ∪ {xa : a ∈ ACT};
• initial state: α′0 = α0 ∧ X;
• goal state: γ′ = γ ∧ X;
• set of actions: S(ACT) = {ai : a ∈ ACT, i = 1, . . . , 2|post(a)| + 1}.
Let a be an action in ACT such that post(a) = l1 ∧ · · · ∧ lq , where l1 , . . . , lq are literals.
– For i = 1, . . . , q, we define an action ai by setting:
pre(ai ) = pre(a) ∧ X ∧ ¬li ; post(ai ) = xa .
The role of ai is to enforce that Xa holds after ai is successfully applied, and in this
way to enable “starting the execution of a”, provided that no action is currently being
executed, that the ith effect of a is not already true, and that the precondition of a is true.
– For i = q + 1, . . . , 2q, we define action ai by setting:
pre(ai ) = Xa ; post(ai ) = li .
The role of ai is to make the ith effect of a true.
– Finally, we define a2q+1 by setting:
pre(a2q+1 ) = Xa ∧ l1 ∧ · · · ∧ lq ; post(a2q+1 ) = ¬xa .
Thus, a2q+1 is designed so that X holds after a2q+1 is successfully applied; that is, a2q+1
“closes” the execution of a, thus allowing for the next action to be executed.
Let π be a sequence of actions in ACT. We define S(π) to be the sequence of actions in S(ACT)
obtained by replacing each action a in π by a1 , . . . , a2q+1 , where q = |post(a)|. Now consider a
sequence τ of actions from S(ACT). Remove from τ every action ai such that i 6= 2|post(a)| + 1,
and replace actions of the form a2|post(a)|+1 by a. We denote the resulting sequence of actions from
ACT by S′ (τ). We note that S′ (S(π)) = π. The following properties then hold.
Lemma 1 With the above definitions,
(i) if π is a plan for PE then S(π) is a plan for PE′ ;
(ii) if τ is an irreducible plan for PE′ then S′ (τ) is an irreducible plan for PE;
(iii) ACT is acyclic if and only if S(ACT) is acyclic.
Proof: (i) Let a ∈ ACT be an action, let α be a state and let β be the state obtained from α by
applying a. Let θ be the V ′ -state obtained by applying the sequence of actions ha1 , . . . , a2q+1 i
(where q = |post(a)|) to the state α ∧ X of PE′ . We will show that θ = β ∧ X.
We note that if for each i = 1, . . . , q, state α ∧ X does not satisfy pre(ai ) then the sequence of
actions ha1 , . . . , a2q+1 i has no effect, so the state is still α ∧ X. For this to happen, either α doesn’t
satisfy pre(a), or all of l1 , . . . , lq already hold in α so post(a) holds in α. In either case, α = β, and
so θ = β ∧ X.
413

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

Suppose now that for some i ∈ {1, . . . , q}, α does satisfy pre(ai ). Then the first such action
causes xa and hence Xa to hold. After applying actions aq+1 , . . . , a2q , l1 ∧ · · · ∧ lq holds, and so
post(a) holds. After applying a2q+1 both post(a) and X hold. No other variable in V has changed,
so θ = β ∧ X, as required.
Applying this result iteratively implies that if π is a plan for PE then S(π) is a plan for PE′ .

ai

(ii) Let τ be an irreducible plan for PE′ , so that every action in τ changes the state, which implies
that every action in τ is performed in a state where its precondition is true. We will show that S′ (τ)
/ When τ = 0,
/ S′ (τ) = 0,
/ too, and the assertion follows.
is a plan for PE. We will assume that τ 6= 0.
j
′
Write the first action in τ as a , where a ∈ ACT, and let τ be the maximal initial subsequence of
τ consisting of all actions of the form ai . We must have j ≤ |post(a)|, since X holds in α′0 (by our
assumption above, action a j does apply) and X is inconsistent with the precondition of ai for each
i > |post(a)|. Also, pre(a j ) and ¬l j hold in α′0 and so, in α0 as well. Thus, α0 satisfies pre(a), and
applying a changes the state, since ¬l j holds in α0 and post(a) |= l j . Let us denote by β the state
resulting from applying a to α0 . As we noted, β 6= α0 ,
Let β′ be the state resulting after applying τ′ to α′0 . If β′ is the goal state γ′ then X holds in β′ . If
β′ is not the goal state then τ 6= τ′ . Let bi be the action in τ directly following the last action in τ′ .
By the definition of τ′ , a 6= b. After applying a j , Xa holds, so in β′ either Xa holds or X holds. Thus,
Xb does not hold, as a 6= b. Since bi changes the state, i must be in {1, . . . , |post(b)|}, so X holds in
β′ in this case, too.
Hence the last action in τ′ is a2q+1 , where q = |post(a)|. Since the only variables in V which can
be affected by actions ai are those that appear in the literals in post(a) and since the action a2q+1
can be executed (otherwise it would not belong to τ), it follows that β′ = β ∧ X.
Applying this reasoning repeatedly, we show that applying S′ (τ) to α0 yields γ, and that each
action in S′ (τ) changes the state, so S′ (τ) is an irreducible plan for PE, which is non-empty if and
only if τ is non-empty.
(iii) Suppose ACT is not acyclic, so that there exists state α and a non-empty irreducible plan π for
PEα = hV, α, α, ACTi. Then, by (i), S(π) is a plan for PE′α = hV ′ , α ∧ X, α ∧ X, S(ACT )i. Because
π is non-empty and irreducible, it changes some state, so S(π) also changes some state, and hence
can be reduced to a non-empty irreducible plan for PE′α . Therefore S(ACT) is not acyclic.
Conversely, suppose that S(ACT) is not acyclic. Then there exists a state α′ and a non-empty
irreducible plan τ for hV ′ , α′ , α′ , S(ACT)i. We will first prove that X holds at some state obtained
during the execution of this plan.
/ By
Suppose that X holds at no such state, and let a j be the first action in τ. We note that τ 6= 0.
our assumption, X does not hold either before or after applying a j . Therefore q + 1 ≤ j ≤ 2q, where
q = |post(a)|. Since τ is irreducible, a j changes the state. Thus, ¬l j holds in α′ and l j holds in the
state resulting from α′ after applying a j .
By our assumption, Xa holds before and after applying a j . Thus, the next action, if there is one,
must also be of the form ai for q + 1 ≤ i ≤ 2q. Repeating this argument implies that all actions in
τ are of the form ai where q + 1 ≤ i ≤ 2q. Since the set of literals in post(a) is consistent, l j is
never reset back to ¬l j . Thus, the state resulting from α′ after applying τ is different from α′ , a
contradiction.
Thus, X holds at some state reached during the execution of τ. Let us consider one such state.
It can be written as β ∧ X, for some state β over V . We can cyclically permute τ to generate a
non-empty irreducible plan τ′ for hV ′ , β ∧ X, β ∧ X, S(ACT)i. By part (ii), S′ (τ′ ) is a non-empty
414

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

irreducible plan for hV, β, β, ACTi. Therefore ACT is not acyclic.



Proposition 6
SE STRIPS PLAN

and ACYCLIC SE STRIPS PLAN are PSPACE-complete.

Proof: Again, the argument for the membership in PSPACE is standard. PSPACE-hardness of
ACYCLIC SE STRIPS PLAN is shown by reduction from ACYCLIC STRIPS PLAN . The same construction shows that STRIPS PLAN is reducible to SE STRIPS PLAN, and thus SE STRIPS PLAN is
PSPACE-complete.
Let us consider an instance PE = hV, α0 , γ, ACTi of ACYCLIC STRIPS PLAN. We define PE′ =
′
hV , α′0 , γ′ , S(ACT)i, which by Lemma 1(iii) is an instance of the ACYCLIC SE STRIPS PLAN problem. By Lemma 1(i) and (ii) there exists a plan for PE if and only if there exists a plan for PE′ . This
implies that ACYCLIC SE STRIPS PLAN is PSPACE-hard.


4. Dominance
The goal of this section is to prove that the GCP - DOMINANCE problem is PSPACE-complete, and
that the complexity does not go down even when we restrict the class of inputs to CP-nets. We
use the results on propositional STRIPS planning from Section 3 to prove that the general GCP DOMINANCE problem is PSPACE-complete. We then show that the complexity does not change if
we require the input GCP-net to be locally consistent and locally complete.
The similarities between dominance testing in CP-nets and propositional STRIPS planning were
first noted by Boutilier et al. (1999). They presented a reduction, discussed later in more detail by
Boutilier et al. (2004a), from the dominance problem to the plan existence problem for a class
of propositional STRIPS planning specifications consisting of unary actions (actions with single
effects). We prove our results for the GCP - DOMINANCE and GCP - CONSISTENCY problems by constructing a reduction in the other direction.
This reduction is much more complex than the one used by Boutilier et al. (1999), due to the
fact that CP-nets impose more restrictions than STRIPS planning. Firstly, STRIPS planning allows
multiple effects, but GCP-nets only allow flips x > ¬x or ¬x > x that change the value of one
variable; this is why we constructed the reduction from STRIPS planning to single-effect STRIPS
planning in the last section. Secondly, CP-nets impose two more restrictions, local consistency and
local completeness, which do not have natural counterparts in the context of STRIPS planning.
For all dominance and consistency problems we consider, the membership in PSPACE can be
demonstrated similarly to the membership proof of Proposition 4, namely by considering nondeterministic polynomial space algorithms consisting of repeatedly guessing appropriate improving flips
and making use of the fact that PSPACE = NPSPACE = coPSPACE. Therefore, from now on we
only provide arguments for the PSPACE-hardness of problems we consider.
4.1 Dominance for Generalized CP-Nets
We will prove that the GCP - DOMINANCE problem is PSPACE-complete by a reduction from the
problem SE STRIPS PLAN, which we now know to be PSPACE-complete.
415

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

4.1.1 M APPING S INGLE -E FFECT STRIPS P ROBLEMS
P ROBLEMS

TO

GCP-N ETS D OMINANCE

Let hV, α0 , γ, ACTi be an instance of the SE STRIPS PLAN problem. For every action a ∈ ACT
we denote by la the unique literal in the postcondition of a, that is, post(a) = la . We denote by
pre′ (a) the conjunction of all literals in pre(a) different from ¬la (we recall that by a convention we
adopted earlier, pre′ (a) does not contain la ). We then define ca to be the conditional preference rule
pre′ (a) : la > ¬la and define M(ACT) to be the GCP-net C = {ca : a ∈ ACT}, which is in conjunctive
form.
A sequence of states in a plan corresponds to an improving sequence from α0 to γ, which leads
to the following result.
Lemma 2 With the above notation,
(i) there is a non-empty irreducible plan for hV, α0 , γ, ACTi if and only if γ dominates α0 in
M(ACT);
(ii) ACT is acyclic if and only if M(ACT) is consistent.
Proof: We first note the following equivalence. Let a be an action in ACT, and let α and β be
different outcomes (or, in the STRIPS setting, states). The action a applied to α yields β if and only
if the rule ca sanctions an improving flip from α to β. This is because a applied to α yields β if and
only if α satisfies pre(a) and α and β differ only on literal la , with β satisfying la and α satisfying
¬la . This is if and only if α satisfies pre′ (a) and α and β differ only on literal la , with β satisfying
la , and α satisfying ¬la . This, in turn, is equivalent to say that rule ca sanctions an improving flip
from α to β.
Proof of (i): Suppose first that there exists a non-empty irreducible plan a1 , . . . , am for hV, α0 , γ, ACTi.
Let α0 , α1 , . . . , αm = γ be the corresponding sequence of outcomes, and, for each i = 1, . . . , m, action ai , when applied in state αi−1 , yields different state αi . By the above equivalence, for each
i = 1, . . . , m, cai sanctions an improving flip from αi−1 to αi , which implies that α0 , α1 , . . . , αm is an
improving flipping sequence in M(ACT), and therefore γ dominates α0 in M(ACT).
Conversely, suppose that γ dominates α0 in M(ACT), so that there exists an improving flipping
sequence α0 , α1 , . . . , αm with αm = γ, and m ≥ 1. For each i = 1, . . . , m, let cai be an element of
M(ACT) which sanctions the improving flip from αi−1 to αi . Then, by the above equivalence,
action ai , when applied to state αi−1 yields αi (which is different from αi−1 ), and so a1 , . . . , am is a
non-empty irreducible plan for hV, α0 , γ, ACTi.
Proof of (ii): ACT is not acyclic if and only if there exists a state α and a non-empty irreducible
plan for hV, α, α, ACTi. By (i) this is if and only if there exists an outcome α which dominates itself
in M(ACT), which is if and only if M(ACT) is not consistent.


Theorem 1 The GCP - DOMINANCE problem is PSPACE-complete. Moreover, this remains so under
the restrictions that the GCP-net is consistent and is in conjunctive form.
Proof: PSPACE-hardness is shown by reduction from ACYCLIC SE STRIPS PLAN (Proposition 6).
Let hV, α0 , γ, ACTi be an instance of the ACYCLIC SE STRIPS PLAN problem. By Lemma 2(ii),
M(ACT) is a consistent GCP-net in conjunctive form. Since α0 6= γ (imposed in the definition of
416

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

the problem ACYCLIC SE STRIPS PLAN), there is a plan for hV, α0 , γ, ACTi if and only if there is a
non-empty irreducible plan for hV, α0 , γ, ACTi, which, by Lemma 2(i), is if and only if γ dominates
α0 in C.

Theorem 1 implies the PSPACE-completeness of dominance in the more general conditional
preference language introduced by Wilson (2004b), where the conditional preference rules are written in conjunctive form.
4.2 Dominance in CP-Nets
In this section we show that GCP - DOMINANCE remains PSPACE-complete under the restriction to
locally consistent and locally complete GCP-nets, that is, CP-nets. We refer to this restriction of
GCP - DOMINANCE as CP - DOMINANCE .
Consistency of a GCP-net implies local consistency (Proposition 2). Therefore, the reduction in the proof of Theorem 1 (from ACYCLIC SE STRIPS PLAN to GCP - DOMINANCE restricted
to consistent GCP-nets) is also a reduction to GCP - DOMINANCE restricted to locally consistent
GCP-nets. PSPACE-hardness of ACYCLIC SE STRIPS PLAN (Proposition 6) then implies that GCP DOMINANCE restricted to locally consistent GCP-nets is PSPACE-hard, and, in fact, PSPACEcomplete since membership in PSPACE is easily obtained with the usual line of argumentation.
We will show PSPACE-hardness for CP - DOMINANCE by a reduction from GCP - DOMINANCE
for consistent GCP-nets.
4.2.1 M APPING L OCALLY C ONSISTENT GCP-N ETS

TO

CP-N ETS

Let C be a locally consistent GCP-net. Let V = {x1 , . . . , xn } be the set of variables of C. We define
/ We define a GCP-net C′ over V ′ , which we
V ′ = V ∪ {y1 , . . . , yn }, where {y1 , . . . , yn } ∩ V = 0.
will show is a CP-net. To this end, for every z ∈ V ′ we will define conditional preference rules
q+ (z) : z > ¬z and q− (z) : ¬z > z to be included in C′ by specifying formulas q+ (z) and q− (z).
First, for each variable xi ∈ V , we set
q+ (xi ) = yi and q− (xi ) = ¬yi .
Thus, xi depends only on yi . We also note that the formulas q+ (xi ) and q− (xi ) satisfy local consistency and local completeness requirements.
Next, for each variable yi , 1 ≤ i ≤ n, we define
ei = (x1 ↔ y1 ) ∧ · · · ∧ (xi−1 ↔ yi−1 ) ∧ (xi+1 ↔ yi+1 ) ∧ · · · ∧ (xn ↔ yn ),
fi+ = ei ∧ p+ (xi ) and fi− = ei ∧ p− (xi ).
Finally, we define
q+ (yi ) = fi+ ∨ (¬ fi− ∧ xi )
and
q− (yi ) = fi− ∨ (¬ fi+ ∧ ¬xi ).
Thus, yi depends on every variable in V ′ but itself.
We note that by the local consistency of C, formulas fi+ ∧ fi− , 1 ≤ i ≤ n, are unsatisfiable.
Consequently, formulas q+ (yi ) ∧ q− (yi ), 1 ≤ i ≤ n, are unsatisfiable. Thus, C′ is locally consistent.
417

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

Finally, q+ (yi ) ∨ q− (yi ) is equivalent to fi+ ∨ ¬xi ∨ fi− ∨ xi , so is a tautology. Thus, C′ is locally
complete and hence a CP-net over V ′ .
Let α and β be outcomes over {x1 , . . . , xn } and {y1 , . . . , yn }, respectively. By αβ we denote the
outcome over V ′ obtained by concatenating n-tuples α and β. Conversely, every outcome for C′ can
be written in this way.
Let α be an outcome over V . We define α to be the outcome over {y1 , . . . , yn } obtained by
replacing in α every component of the form xi with yi and every component ¬xi with ¬yi . Then for
every i, 1 ≤ i ≤ n, αα |= ei .
Let s be a sequence α0 , . . . , αm of outcomes over V . Define L(s) to be the sequence of V ′ outcomes: α0 α0 , α0 α1 , α1 α1 , α1 α2 , . . . , αm αm . Further, let t be a sequence ε0 , ε1 , . . . , εm of V ′ outcomes with ε0 = αα and εm = ββ. Define L′ (t) to be the sequence obtained from t by projecting
each element in t to V and iteratively removing elements in the sequence which are the same as their
predecessor (until any two consecutive outcomes are different).
Lemma 3 With the above definitions,
(i) if s is an improving sequence for C from α to β then L(s) is an improving sequence for C′ from
αα to ββ;
(ii) if t is an improving sequence from αα to ββ then L′ (t) is an improving sequence from α to β;
(iii) C is consistent if and only if C′ is consistent.
Proof: Let e = ni=1 (xi ↔ yi ). The definitions have been arranged so that the GCP-net C and the
CP-net C′ have the following properties:
(a) If e does not hold in an outcome γ over V ′ , then every improving flip applicable to γ changes the
value of some variable xi or yi so that xi ↔ yi holds after the flip.
Indeed, let us assume that there is an improving flip from γ to some outcome γ′ over V ′ . If the
flip concerns a variable xi , then xi ↔ ¬yi holds in γ. Consequently, xi ↔ yi holds in γ′ .
Thus, let us assume that the flip concerns a variable yi . If ei holds in γ then, since e does not,
xi ↔ ¬yi holds in γ. Thus, xi ↔ yi holds in γ′ . If ei does not hold in γ then neither fi+ nor fi− does.
Thus, if xi (¬xi , respectively) holds in γ, yi (¬yi , respectively) holds in γ′ . Since the flip concerns yi ,
it follows that xi ↔ yi holds in γ′ .
(b) No improving flip from αα changes any variable xi .
Indeed, for any variable xi , since e holds in αα, xi ↔ yi holds in αα, too. Thus, no improving
flip changes xi .
(c) There is an improving flip in C′ that changes variable yi in an outcome αα if and only if there is
an improving flip for the GCP-net C from outcome α that changes variable xi . After applying the
improving flip (changing variable yi ) to αα, there is exactly one improving flip possible. It changes
xi and results in an outcome ββ, where β is the outcome over V resulting from applying to α the
improving flip changing the variable xi .
To prove (c), let us first assume that ¬yi holds in αα and observe that in such case ¬xi holds in
αα, too. It follows that q+ (yi ) holds in αα if and only if p+ (xi ) holds in α. Consequently, changing
yi in αα is an improving flip in C′ if and only if changing xi in α is an improving flip in C. The
argument in the case when yi holds in αα is analogous (but involves q− (yi ) and p− (xi )). Thus, the
first part of (c) follows.
V

418

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

Let β be the outcome obtained by applying an improving flip to xi in α. It follows that the
improving flip changing the value of yi in αα results in the outcome αβ. In this outcome, by (a),
an improving flip must concern x j or y j such that x j ↔ y j holds after the flip. Since for every j 6= i,
x j ↔ y j holds in αβ, the only improving flips in αβ concern either xi or yi . By the local consistency
of C′ , yi cannot be flipped right back. Clearly, changing xi is an improving flip that can be applied
to αβ. By our discussion, it is the only improving flip applicable in αβ and it results in the outcome
ββ. This proves the second part of (c).
Proof of (i): The assertion follows by iterative application of (c).
Proof of (ii): Suppose that t is an improving sequence ε0 , ε1 , . . . , εm of V ′ -outcomes with ε0 = αα
and εm = ββ. Since e holds in ε0 , (b) implies that the first flip changes some variable yi , and (c)
implies that the second flip changes variable xi to make xi ↔ yi hold again. Hence ε2 can be written
as δδ. By (c) there is an improving flip in C from outcome α changing variable xi , that is, leading
from α to δ. Iterating this process shows that L′ (t) is an improving sequence from α to β.
Proof of (iii): Suppose that C is inconsistent. Then there exists some outcome α and an improving
sequence s in C from α to α. By (i), L(s) is an improving sequence from αα to αα, proving that C′
is inconsistent.
Conversely, suppose that C′ is inconsistent, so there exists an improving sequence t for C′ from
some outcome to itself. By (a), any improving flip applied to an outcome in which e does not hold
increases (by one) the number of i such that xi ↔ yi holds. This implies that e must hold in some
outcome in t, because t is not acyclic. Write this outcome as αα. We can cyclically permute t to
form an improving sequence t2 from αα to itself. Part (ii) then implies that L′ (t2 ) is an improving
flipping sequence for C from α to itself, showing that C is inconsistent.


Theorem 2 CP - DOMINANCE is PSPACE-complete. This holds even if we restrict the CP-nets to
being consistent.
Proof: We use a reduction from PSPACE-hardness of the GCP - DOMINANCE problem when the
GCP-nets are restricted to being consistent (Theorem 1). Let C be a consistent, and hence locally
consistent, GCP-net over V , and let α and β be outcomes over V . Consider the CP-net C′ over
variables V ′ constructed above. Lemma 3(i) and (ii) imply that β dominates α in C if and only if ββ
dominates αα in C′ . Moreover, C′ is consistent by Lemma 3(iii). Consequently, the hardness part
of the assertion follows.

Note that PSPACE-hardness obviously remains if we require input outcomes to be different,
because the reduction for Theorem 1 uses a pair of different outcomes.
Notice the huge complexity gap with the problem of deciding whether there exists a nondominated outcome, which is “only” NP-complete (Domshlak et al., 2003, 2006).

5. Consistency of GCP-Nets
In this section we show that the
from Sections 3 and 4.

GCP - CONSISTENCY

419

problem is PSPACE-complete, using results

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

Theorem 3
GCP - CONSISTENCY is PSPACE-complete. This holds even under the restriction to GCP-nets in
conjunctive form.
Proof: PSPACE-hardness is shown by reduction from ACTION - SET ACYCLICITY. We apply function S from Section 3.2 followed by M from Section 4.1. This maps instances of ACTION - SET
ACYCLICITY to instances of GCP - CONSISTENCY in conjunctive form. By Lemma 1(iii) and Lemma
2 (ii), an instance of ACTION - SET ACYCLICITY is acyclic if and only if the corresponding instance
of GCP - CONSISTENCY is consistent, proving the result.

We now show that consistency testing remains PSPACE-complete for CP-nets (GCP-nets that
are both locally consistent and locally complete).
Theorem 4

CP - CONSISTENCY

is PSPACE-complete.

Proof: We use a reduction from GCP - CONSISTENCY under the restriction that the GCP-net is in
conjunctive form. Let C be a GCP-net in conjunctive form. We define a CP-net C′ as follows. Because C is in conjunctive form, local consistency can be decided in polynomial time, as it amounts
to checking the consistency of a conjunction of conjunctions of literals. If C is not locally consistent
we set C′ to be a predetermined inconsistent but locally consistent CP-net, such as in the example
in Section 2. Otherwise, C is locally consistent and for C′ we take the CP-net we constructed in
Section 4.2. The mapping from locally consistent GCP-nets to CP-nets, described in Section 4.2,
preserves consistency (Lemma 3 (iii)). Since local inconsistency implies inconsistency (Proposition 2), we have that the GCP-net C is consistent if and only if the CP-net C′ is consistent. Thus,
PSPACE-hardness of the CP - CONSISTENCY problem follows from Theorem 3.


6. Additional Problems Related to Dominance in GCP-Nets
Having proved our main results on consistency of and dominance in GCP-nets, we move on to
additional questions concerning the dominance relation. Before we state them, we introduce more
terminology.
Let α and β be outcomes in a GCP-net C. We say that α and β are dominance-equivalent in C,
written α ≈C β, if α = β, or α ≺C β and β ≺C α. Next, α and β are dominance-incomparable in C
if α 6= β, α⊀C β and β⊀C α. Finally, α strictly dominates β if β ≺C α and α6≺C β.
Definition 8
We define the following decision problems:
SELF - DOMINANCE : given a GCP-net C and an outcome α, decide whether α ≺C α, that is, whether
α dominates itself in C.
STRICT DOMINANCE : given a GCP-net C and outcomes α and β, decide whether α strictly dominates β in C.
DOMINANCE EQUIVALENCE : given a GCP-net C and outcomes α and β, decide whether α and β
are dominance-equivalent in C.
DOMINANCE INCOMPARABILITY : given a GCP-net C and outcomes α and β, decide whether α
and β are dominance-incomparable in C.
420

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

When establishing the complexity of these problems, we will use polynomial-time reductions
from the problem GCP - DOMINANCE. Let H be a GCP-net with the set of variables V = {x1 , . . . , xn },
and let β be an outcome. We define a GCP-net G = Θ1 (H, β) with the set of variables W = V ∪ {y}
by setting the conditions for flips on variables xi , i = 1, . . . , n, and y as follows:
1. if xi ∈ β:
+
p+
G (xi ) = pH (xi ) ∨ ¬y
−
p−
G (xi ) = pH (xi ) ∧ y
2. if ¬xi ∈ β:
+
p+
G (xi ) = pH (xi ) ∧ y
−
p−
G (xi ) = pH (xi ) ∨ ¬y
3. p+
G (y) = β
4. p−
G (y) = ¬β.
The mapping Θ1 can be computed in polynomial time. Moreover, one can check that if H is a
locally consistent GCP-net, Θ1 (H, β) is also locally consistent. Finally, if H is a CP-net, Θ1 (H, β)
is a CP-net, as well.
For every V -outcome γ, we let γ+ = γ ∧ y and γ− = γ ∧ ¬y. We note that every W -outcome is of
the form γ+ or γ− . To explain the structure of the GCP-net G, we point out that there is an improving
flip in G from γ+ into δ+ if and only if there is an improving flip in H from γ to δ (thus, G restricted
to outcomes of the form γ+ forms a copy of the GCP-net H). Moreover, there is an improving flip
in G from γ− into δ− if and only if δ agrees with β on exactly one more variable xi than γ does.
Finally, an improving flip moves between outcomes of different type if and only if it transforms β−
to β+ , or γ+ to γ− for some γ 6= β.
We now formalize some useful properties of the GCP-net G = Θ1 (H, β). We use the notation
introduced above.
Lemma 4 For every V -outcome γ, γ− ≺G β+ and, if γ 6= β, γ+ ≺G β+ (in other words, β+ dominates
every other W -outcome).
Proof: Consider any V -outcome γ 6= β. Then γ ∧ ¬y ≺C β ∧ ¬y since, given ¬y, changing a literal
to the form it has in β is an improving flip. By the definition, we also have β ∧ ¬y ≺C β ∧ y and
γ ∧ y ≺G γ ∧ ¬y (as γ 6= β). It follows that β− ≺G β+ and γ+ ≺G γ− ≺G β+ . Thus, the assertion
follows.


Lemma 5 For arbitrary V -outcome α different from β, the following statements are equivalent:
1. β ≺H α;
2. β+ ≺G α+ ;
3. β+ ≈G α+ .
421

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

Proof: By Lemma 4, α+ ≺G β+ . Thus, the conditions (2) and (3) are equivalent.
[(1)⇒(2)] Clearly (recall our discussion about the structure of G), if there is an improving flip from
γ to δ in H, then there is an improving flip from γ+ to δ+ in G. Thus, if there is an improving
sequence in H from β to α, there is an improving sequence in G from β+ to α+ .
[(2)⇒(1)] Let us assume β+ ≺G α+ , and let us consider an improving sequence of minimum length
from β+ to α+ . By the minimality, no internal element in such a sequence is β+ . Thus, no internal
element equals β− either (as the only improving flip from β− leads to β+ ). Since an improving flip
from γ− to γ+ requires that γ = β, all outcomes in the sequence are of the form γ+ . By dropping
y from each outcome in this sequence, we get an improving flipping sequence from α to β in H.
Thus, β ≺H α.

Lemma 6 Let H be consistent and let α and β be different V -outcomes. Then, α+ ≺G α+ if and
only if β ≺H α.
Proof: Suppose there exists an improving sequence from α+ to itself. There must be an outcome
in the sequence of the form γ ∧ ¬y (otherwise, dropping y in every outcome yields an improving
sequence from α to α in H, contradicting the consistency of H). To perform an improving flip from
¬y to y we need β to hold, which implies that β+ appears in the sequence. Thus, β+ ≺G α+ . By
Lemma 5, β ≺H α.
Conversely, let us assume that β ≺H α. Again by Lemma 5, β+ ≺G α+ . By Lemma 4, α+ ≺G β+ .
Thus, α+ ≺G α+ .

The next construction is similar. Let H be a GCP-net on variables V = {x1 , . . . , xn }, and let α
be an outcome. We define a GCP-net F = Θ2 (H, α) as follows. As before, we set W = V ∪ {y} to
be the set of variables of F. We define the conditions for flips on variables xi , i = 1, . . . , n, and y as
follows:
+
1. p+
G (xi ) = pH (xi ) ∧ y
−
2. p−
G (xi ) = pH (xi ) ∧ y

3. p+
G (y) = ¬α
4. p−
G (y) = α.
Informally, outcomes of the form γ+ form in F a copy of H. There are no improving flips between
outcomes of the form γ− . There is an improving flip from α+ to α− and, for every γ 6= α, from γ− to
γ+ . In particular, if F is consistent then Θ2 (H, α) is consistent, The mapping Θ2 can be computed
in polynomial time and we also have the following property.
Lemma 7 Let β be a V -outcome different from α. Then the following conditions are equivalent:
1. β ≺H α
2. α− strictly dominates β− in F
3. α− and β− are not dominance-incomparable in F.
422

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

Proof: If there exists an improving sequence from β− to α− then the first improving flip in the sequence changes β− to β+ . Moreover, there is an improving flip from γ+ to γ− if and only if γ = α.
Thus, β− ≺F α− if and only if β ≺H α. Since α− ⊀F β− all three conditions are equivalent.


Proposition 7 The following problems are PSPACE-complete: SELF - DOMINANCE, STRICT
INANCE , DOMINANCE EQUIVALENCE , and DOMINANCE INCOMPARABILITY .

DOM -

Proof: For all four problems, membership is proven easily as for the problems in earlier sections.
For the PSPACE-hardness proofs, we use the problem CP - DOMINANCE in a version when we
required that the input CP-net be consistent and the two input outcomes different. The problem is
PSPACE-hard by Theorem 2.
Let H be a consistent CP-net on a set V of variables, and let α and β be two different V -outcomes.
By Lemma 5, β ≺H α can be decided by deciding the problem DOMINANCE EQUIVALENCE for α+
and β+ in the GCP-net Θ1 (H, β). Thus, the PSPACE-hardness of DOMINANCE EQUIVALENCE
follows.
Next, the equivalence of Lemma 6, α+ ≺G α+ ⇔ β ≺H α, which holds due to consistency of H,
shows that the problem SELF - DOMINANCE is PSPACE-hard.
Finally, by Lemma 7, β ≺H α can be decided either by deciding the problem STRICT DOMI NANCE for outcomes α− and β− in Θ2 (H, α), or by deciding the complement of the problem DOM INANCE INCOMPARABILITY for α− and β− in the GCP-net Θ2 (H, α). It follows that STRICT DOM INANCE and DOMINANCE INCOMPARABILITY (the latter by the fact that coPSPACE=PSPACE) are
PSPACE-complete.8


Corollary 1 The problems SELF - DOMINANCE and DOMINANCE EQUIVALENCE are PSPACE-complete under the restriction to CP-nets. The problems STRICT DOMINANCE and DOMINANCE IN COMPARABILITY remain PSPACE-complete under the restriction to consistent CP-nets.
Proof: Since in the proof of Proposition 7 we have that H is a CP-net, the claim for the first two
problems follows by our remarks that the mapping Θ1 preserves the property of being a CP-net.
For the last two problems, we observe that since H in the proof of Proposition 7 is assumed to
be consistent, F = Θ2 (H, α) is consistent, too. Thus, it is also locally consistent and the mapping
F to F ′ we used for the proof of Theorem 2 applies. In particular, F ′ is a consistent CP-net and has
the following properties (implied by Lemma 3):
1. α strictly dominates β in F if and only if αα strictly dominates ββ in F ′
2. α and β are dominance-incomparable in F if and only if αα and ββ are dominance-incomparable in F ′ .
Since F ′ is a consistent CP-net, the claim for the last two problems follows, too.



8. For STRICT DOMINANCE, the result could have been also obtained as a simple corollary of Theorem 2, since in
consistent GCP-nets dominance is equivalent to strict dominance.

423

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

7. Problems Concerning Optimality in GCP-Nets
The dominance relation ≺C of a GCP-net C determines a certain order relation, which gives rise to
several notions of optimality. We will introduce them and study the complexity of corresponding
decision problems.
We first observe that the dominance equivalence relation is indeed an equivalence relation (reflexive, symmetric and transitive). Thus, it partitions the set of all outcomes into non-empty equivalence classes, which we call dominance classes. We denote the dominance class of an outcome α
in a GCP-net C by [α]C .
The relation ≺C induces on the set of dominance classes a strict order relation (a relation that is
irreflexive and transitive). Namely, we define [α]C ≺Cdc [β]C if [α]C 6= [β]C (equivalently, α 6≈C β) and
α ≺C β. One can check that the definition of the relation ≺Cdc on dominance classes is independent
of the choice of representatives of the classes.
Definition 9 (Non-dominated class, optimality in GCP-nets) Let C be a GCP-net. A dominance
class [α]C is non-dominated if it is maximal in the strict order ≺Cdc (there is no dominance class
[β]C such that [α]C ≺Cdc [β]C ). A dominance class is dominating if for every dominance class [β]C ,
[α]C = [β]C or [β]C ≺Cdc [α]C .
An outcome α is weakly non-dominated if it belongs to a non-dominated class. If α is weakly
non-dominated and is the only element in its dominance class, then α is non-dominated.
An outcome α is dominating if it belongs to a dominating class. An outcome α is strongly
dominating if it is dominating and non-dominated.
Outcomes that are weakly non-dominated, non-dominated, dominating and strongly dominating
capture some notions of optimality. In the context of CP-nets, weakly non-dominated and nondominated outcomes were proposed and studied before (Brafman & Dimopoulos, 2004). They were
referred to as weakly and strongly optimal there. Similar notions of optimality were also studied
earlier for the problem of defining winners in partial tournaments (Brandt, Fischer, & Harrenstein,
2007). We will study here the complexity of problems to decide whether a given outcome is optimal
and whether optimal outcomes exist.
First, we note the following general properties (simple consequences of properties of finite strict
orders).
Lemma 8 Let C be a GCP-net.
1. There exist non-dominated classes and so, weakly non-dominated outcomes.
2. Dominating outcomes and nondominated outcomes are weakly non-dominated.
3. A strongly dominating outcome is dominating and non-dominated.
4. The following conditions are equivalent:
(a) C has a unique non-dominated class;
(b) C has a dominating outcome;
(c) weakly non-dominated and dominating outcomes in C coincide.
For consistent GCP-nets only two different notions of optimality remain.
424

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

Lemma 9 Let C be a consistent GCP-net. Then:
1. Each dominance class is a singleton, ≺C is a strict order, and ≺C and ≺Cdc coincide (modulo
the one-to-one and onto correspondence α 7→ [α]C )
2. If α is a weakly non-dominated outcome, α is non-dominated (weakly non-dominated and
non-dominated outcomes coincide)
3. If α is a dominating outcome, α is strongly dominating (strongly dominating and dominating
outcomes coincide).
4. Finally, α is a unique (weakly) non-dominated outcome if and only if α is strongly dominating.
Next, we observe that all concepts of optimality we introduced are different. To this end, we will
show GCP-nets with a single non-dominated class that is a singleton, with multiple non-dominated
classes, each being a singleton, with a single non-dominated class that is not a singleton, and with
multiple non-dominated classes, each containing more than one element. We will also show a GCPnet with two non-dominated classes, one of them a singleton and the other one consisting of several
outcomes.
Example 2 Consider the following GCP-net C with two binary variables a and b
: a > ā
: b > b̄
This GCP-net determines a strict preorder on the dominance classes, in which {ab} is the only
maximal class (in fact, all dominance classes are singletons). Thus, ab is both non-dominated and
dominating and so, it is strongly dominating.
Example 3 Consider the following GCP-net C with two binary variables a and b
b : a > ā
b̄ : ā > a
a : b > b̄
ā : b̄ > b
This GCP-net determines a strict preorder, in which {ab} and {āb̄} are two different non-dominated
classes. Thus, ab and āb̄ are non-dominated and there is no dominating outcome.
Example 4 Consider a GCP-net with variables a, b and c, defined as follows:
a : b > b̄
ā : b̄ > b
b̄ : a > ā
b : ā > a
ab : c > c̄

425

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

There are two dominance classes: Sc = {abc, ab̄c, ābc, āb̄c} and Sc̄ = {abc̄, ab̄c̄, ābc̄, āb̄c̄}. Every
outcome in Sc strictly dominates every outcome in Sc̄ , therefore, Sc is the unique non-dominated
class and every outcome in Sc is dominating. Because Sc is not a singleton, there are no nondominated outcomes (and so, no strongly dominating outcome, either).
Example 5 Let us remove from the GCP-net of Example 4 the preference statement ab : c > c̄. Then
Sc and Sc̄ are still the two dominance classes, but now every outcome is Sc is incomparable with
any outcome in Sc̄ . Thus, Sc and Sc̄ are both non-dominated. Since there are two non-dominated
classes, there is no dominating outcome. Since each class has more than one element, there are no
non-dominated outcomes. All outcomes are weakly non-dominated, though.
Example 6 Let us modify the GCP-net of Example 4 by changing the preference statement b̄ : a > ā
into b̄c : a > ā. The dominance relation ≺ of this GCP-net satisfies the following properties: (i)
the four outcomes in Sc dominate each other; (ii) āb̄c̄ ≻ ābc̄ ≻ abc̄ ≻ ab̄c̄; (iii) any outcome in Sc
dominates abc̄ (and, a fortiori, ab̄c̄). One can check that there are five dominance classes: Sc , {abc̄},
{ābc̄}, {ab̄c̄} and {āb̄c̄}. Two of them are non-dominated: Sc and {āb̄c̄}. Since there are two nondominated classes, there is no dominating outcome. On the other hand, {āb̄c̄} is a non-dominated
outcome (a unique one).
We will consider the following decision problems corresponding to the notions of optimality we
introduced.
Definition 10
For a given GCP-net C:
WEAKLY NON - DOMINATED OUTCOME : given an outcome α, decide whether α is weakly nondominated in C
NON - DOMINATED OUTCOME : given an outcome α, decide whether α is non-dominated in C
DOMINATING OUTCOME : given an outcome α, decide whether α is dominating in C
STRONGLY DOMINATING OUTCOME : given an outcome α, decide whether α is strongly dominating in C
EXISTENCE OF A NON - DOMINATED OUTCOME : decide whether C has a non-dominated outcome
EXISTENCE OF A DOMINATING OUTCOME : decide whether C has a dominating outcome
EXISTENCE OF A STRONGLY DOMINATING OUTCOME : decide whether C has a strongly dominating outcome.
In some of the hardness proofs, we will again use the reductions Θ1 and Θ2 , described in the
previous section. We note the following additional useful properties of the GCP-net G = Θ1 (H, β).
Lemma 10 For arbitrary V -outcome α different from β, the following statements are equivalent:
1. β+ ≺G α+
2. α+ is weakly non-dominated in G
3. α+ is a dominating outcome in G.
426

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

Proof: Since β+ is dominating in G (Lemma 4), weakly non-dominated outcomes and dominating
outcomes coincide (Lemma 8). It follows that the conditions (1)-(3) are equivalent to each other. 

Proposition 8 The following problems are PSPACE-complete: WEAKLY NON - DOMINATED OUTCOME and DOMINATING OUTCOME . The result holds also for the problems restricted to CP-nets.
Proof: The membership is easy to prove by techniques similar to those we used earlier.
For the PSPACE-hardness proofs, we use reductions from CP - DOMINANCE for consistent CPnets (in the version where the two input outcomes are different). Let H be a CP-net, and α and
β two different V -outcomes. By Lemmas 5 and 10, β ≺H α can be decided by deciding either of
the problems WEAKLY NON - DOMINATED OUTCOME and DOMINATING OUTCOME for the GCPnet G = Θ1 (H, β) and the outcome α+ . We observed earlier, that if H is a CP-net, then so is
G = Θ1 (H, β). Thus, the second part of the assertion follows.

Next, we will consider the problem STRONGLY DOMINATING OUTCOME. We will exploit the
reduction F = Θ2 (H, α), which we discussed in the previous section. We observe the following
property of F.
Lemma 11 Let H be a GCP-net and F = Θ2 (H, α). Then α− is strongly dominating in F if and
only if α is dominating in H.
Proof: Let us assume that α is dominating in H. From the definition of F, it follows that for every
V -outcome γ 6= α, γ+ ≺F α+ and γ− ≺F γ+ . Since α+ ≺F α− , α− is dominating in F. Since there
is no improving flip leading out of α− , α− is strongly dominating.
Conversely, let us assume that α− is strongly dominating in F and let γ be a V -outcome different from α. Let us consider an improving sequence from γ+ to α− . All outcomes in the sequence
other than the last one, α− , are of the form δ+ . Moreover, the outcome directly preceding α− is
α+ . Dropping y from every outcome in the segment of the sequence between γ+ and α+ yields an
improving sequence from γ to α in H.

We now have the following consequence of this result.
Proposition 9 The problem STRONGLY
stricted to CP-nets.

DOMINATING OUTCOME

is PSPACE-complete, even if re-

Proof: Let H be a CP-net (over the set V of variables) and α an outcome. By Lemma 11, the problem DOMINATING OUTCOME can be decided by deciding the problem STRONGLY DOMINATING
OUTCOME for F = Θ2 (H, α) and α− . Thus, the PSPACE-hardness of STRONGLY DOMINATING
OUTCOME follows by Proposition 8. The membership in PSPACE is, as in other cases, standard and
is omitted.
Since H is a CP-net, it is locally consistent and so, F is locally consistent, too. As in the proof
of Corollary 1 we use the mapping from GCP-net F to CP-net F ′ defined in Section 4.2. By Lemma
3, α is a strongly dominating outcome in F if and only if αα dominates every outcome of the form
γγ, which is if and only if αα is a strongly dominating outcome in F ′ , since any F ′ -outcome is
dominated by an outcome of the form γγ (using the rules q+ (xi ) = yi and q− (xi ) = ¬yi ). Therefore
427

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

for F and α can be decided by deciding
for F ′ and αα. Thus, the second part of the claim follows.

STRONGLY DOMINATING OUTCOME
NATING OUTCOME

STRONGLY DOMI 

The problem NON - DOMINATED OUTCOME is easier. It is known to be in P for CP-nets (Brafman
& Dimopoulos, 2004). The result extends to GCP-nets. Indeed, if H is a GCP-net and α an outcome,
α is non-dominated if and only if there is no improving flip that applies to α. The latter holds if and
only if for every variable x in H, if x (respectively, ¬x) holds in α, then p− (x) (respectively, p+ (x))
does not hold in α. Since the conditions can be checked in polynomial the claim holds and we have
the following result.
Proposition 10 The problem NON - DOMINATED

OUTCOME

for GCP-nets is in P.

Next, we will consider the problems concerning the existence of optimal outcomes. Let H be a
GCP-net on the set of variables V = {x1 , . . . , xn }, and let α and β be two different V -outcomes. For
every i = 1, 2, . . . , n, we define formulas αi as follows. If xi ∈ α, then αi is the conjunction of all
literals in α, except that instead of xi we take ¬xi . Similarly, if ¬xi ∈ α, then αi is the conjunction of
all literals in α, except that instead of ¬xi we take xi . Thus, αi is the outcome that results in α when
the literal in corresponding to xi is flipped into its dual.
We now define a GCP-net E = Θ3 (H, α, β) by taking W = V ∪ {y} as the set of variables of E
and by defining the flipping conditions as follows:
+
1. p+
E (xi ) = (pH (xi ) ∧ y) ∨ (¬y ∧ ¬α ∧ ¬αi )
−
−
pE (xi ) = pH (xi ) ∧ y

2. p+
E (y) = β
3. p−
E (y) = ¬β.
The GCP-net Θ3 (H, α, β) has the following properties. The outcomes of the form γ+ (= γ ∧ y)
form a copy of H. There is no improving flip for the outcome α− (= α ∧ ¬y). Next, there is no
improving flip into α− from an outcome of the form γ− . To see this, let us assume that such a flip
exists and concerns a variable, say, xi . It follows that γ = αi . By the definition of flipping conditions,
an improving flip for γ− that involves xi is impossible, a contradiction. Thus, the only improving
flip that leads to α− originates in α+ .
We also have that for every outcome γ other than α and β, γ− ≺E β− . It follows from the fact
that for every outcome γ other than α and β, γ− has an improving flip. Indeed, for each such γ there
is a variable xi such that (i) xi is false in γ, and (ii) flipping the literal of xi to its dual does not lead to
α (that is, γ is not αi ). (For even if γ = αi for some i, then, because γ, α 6= β, there exists i′ 6= i such
that γ and β differ on xi′ , so that xi′ satisfies (i) and (ii).) Thus, a flip on that variable is improving.
As all improving flips between outcomes containing ¬y result in one more variable xi assigned to
true, thus having the same status as it has in β, γ− ≺E β− follows.
Finally, we have β− ≺E β+ and, for every outcome γ other than β, γ+ ≺E γ− . This leads to the
following property of E = Θ3 (H, α, β).
Lemma 12 Let H be a GCP-net and let α and β be two different outcomes. Then β ≺H α if and
only if Θ3 (H, α, β) has a (strongly) dominating outcome.
428

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

Proof: (Only if) Based on our earlier remarks, α+ ≺E α− . Moreover, since β ≺H α, we have
β+ ≺E α+ . In addition, for every γ different from α and β, γ+ ≺E γ− ≺E β− ≺E β+ . Thus, α− is
both dominating and strongly dominating (the latter follows from the fact that no improving flips
lead out of α− ).
(If) Let us assume that α− is dominating (and so, the argument applies also when α− is strongly
dominating). Then there is an improving sequence from β+ to α− . Let us consider a shortest such
sequence. Clearly, α+ is the outcome just before α− in that sequence (as we pointed out, no improving flip from an outcome of the form γ− to α− is possible). Moreover, by the definition of
Θ3 (H, α, β) and the fact that we are considering a shortest sequence from β+ to α− , every outcome
in the sequence between β+ and α+ is of the form γ+ . By dropping y from each of these outcomes,
we get an improving sequence from β to α.


Proposition 11 The problem EXISTENCE OF DOMINATING OUTCOME and the problem EXISTENCE
OF STRONGLY DOMINATING OUTCOME are PSPACE-complete, even if restricted to CP-nets.
Proof: We show the hardness part only, as the membership part is straightforward. To prove hardness we notice that by Lemma 12, given a consistent CP-net H and two outcomes α and β, β ≺H α
can be decided by deciding either of the problems EXISTENCE OF DOMINATING OUTCOME and
EXISTENCE OF STRONGLY DOMINATING OUTCOME for Θ3 (H, α, β). To prove the second part of
the assertion, we note that if H is consistent, E = Θ3 (H, α, β) is consistent, too and so, the mapping
from locally consistent GCP nets to CP-nets applies. Let us denote the result of applying the mapping to E by E ′ . Then, using the same argument as in the proof of Proposition 9, E has a (strongly)
dominating outcome if and only if E ′ has a strongly dominating outcome. Thus, one can decide
whether β ≺H α in a consistent CP-net H by deciding either of the problems EXISTENCE OF DOM INATING OUTCOME and EXISTENCE OF STRONGLY DOMINATING OUTCOME for E ′ .

We also note that the problem EXISTENCE
standard complexity theory assumptions).

OF NON - DOMINATED OUTCOME

Proposition 12 The problem EXISTENCE OF NON - DOMINATED

OUTCOME

is easier (under

is NP-complete.

Proof: We note that in the case of GCP-nets in conjunctive form the problem is known to be NP-hard
(Domshlak et al., 2003, 2006). Thus, the problem is NP-hard for GCP-nets. The membership in the
class NP follows from Proposition 10.

If we restrict to consistent GCP-nets, the situation simplifies. First, we recall (Lemma 9) that if
a GCP-net is consistent then weakly non-dominated and non-dominated outcomes coincide, and the
same is true for dominating and strongly dominating outcomes. Moreover, for consistent GCP-nets,
non-dominated outcomes exist (and so, the corresponding decision problem is trivially in P). Thus,
for consistent GCP-nets we will only consider problems DOMINATING OUTCOME and EXISTENCE
OF DOMINATING OUTCOME .
Proposition 13 The problems DOMINATING OUTCOME and
COME restricted to consistent GCP-nets are in coNP.
429

EXISTENCE OF DOMINATING OUT-

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

Proof: Using Lemmas 8 and 9, α is not a dominating outcome if and only if there exists an outcome
β 6= α which is non-dominated. Similarly, there is no dominating outcome in a consistent GCP-net
if and only if there are at least two non-dominated outcomes. Thus, guessing non-deterministically
an outcome β 6= α, and verifying that β is non-dominated, is a non-deterministic polynomial-time
algorithm deciding the complement of the problem DOMINATING OUTCOME. The argument for the
other problem is similar.

We do not know if the bounds in Proposition 13 are tight, that is, whether these two problems
are coNP-complete. We conjecture they are.

8. Concluding Remarks
We have shown that dominance and consistency testing in CP-nets are both PSPACE-complete. Also
several related problems related to dominance and optimality in CP-nets are PSPACE-complete, too.
The repeated use of reductions from planning problems confirms the importance of the structural similarity between STRIPS planning and reasoning with CP-nets. This suggests that the welldeveloped field of planning algorithms for STRIPS representations, especially for unary operators
(Brafman & Domshlak, 2003), could be useful for implementing algorithms for dominance and
consistency in CP-nets.
Our theorems extend to CP-nets with non-binary domains, and to extensions and variations of
CP-nets, such as TCP-nets (Brafman & Domshlak, 2002; Brafman, Domshlak, & Shimony, 2006)
that allow for explicit priority of some variables over others, and the more general language for
conditional preferences (Wilson, 2004a, 2004b), where the conditional preference rules are written
in conjunctive form.
The complexity result for dominance is also relevant for the following constrained optimisation
problem: given a CP-net and a constraint satisfaction problem (CSP), find an optimal solution (a
solution of the CSP which is not dominated by any other solution of the CSP). This is computationally complex, intuitively because a complete algorithm involves many dominance checks when
the definition of dominance under constraints allows for dominance paths to go through outcomes
violating the constraints (Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004b).9 The problem of
checking whether a given solution of a CSP is non-dominated can be seen to be PSPACE-complete
by a reduction from CP-dominance that uses a CSP that has exactly two solutions.
Our results reinforce the need for work on finding special classes of problems where dominance
and consistency can be tested efficiently (Domshlak & Brafman, 2002; Boutilier et al., 2004a),
and for incomplete methods for checking consistency and constrained optimisation (Wilson, 2004a,
2006).
Several open problems remain. We do not know the complexity of deciding whether the preference relation induced by a CP-net is complete. We do not know whether dominance and consistency
testing remain PSPACE-complete when the number of parents in the dependency graph is bounded
by a constant. We also do not know whether these two problems remain PSPACE-complete for
CP-nets in conjunctive form (the reduction used to prove Theorems 2 and 4 yields CP-nets that are
not in conjunctive form). Two additional open problems are listed at the end of Section 7.
9. With another possible definition, where going through outcomes violating the constraints is not allowed (Prestwich,
Rossi, Venable, & Walsh, 2005), dominance testing is not needed to check whether a given solution is non-dominated.

430

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

Acknowledgments
Jérôme Lang’s new address is: LAMSADE, Université Paris-Dauphine, 75775 Paris Cedex 16,
France. The authors are grateful to the reviewers for their excellent comments, and to Pierre Marquis
for helpful discussions. This work was supported in part by the NSF under Grants ITR-0325063,
IIS-0097278 and KSEF-1036-RDE-008, by the ANR Project ANR–05–BLAN–0384 “Preference
Handling and Aggregation in Combinatorial Domains”, by Science Foundation Ireland under Grants
No. 00/PI.1/C075 and 05/IN/I886, and by Enterprise Ireland Ulysses travel grant FR/2006/36.




This paper examines the concept of a combi­
nation rule for belief functions. It is shown
that two fairly simple and apparently reason­
able assumptions determine Dempster's rule,
giving a new justification for it.
Keywords: Dempster-Shafer Theory, be­
lief functions, Dempster's rule, foundations
of uncertain reasoning.
1

INTRODUCTION

Dempster's rule is the cornerstone of Dempster-Shafer
Theory, the theory of uncertainty developed by Shafer
[76a] from the work of Dempster [67]. The rule is used
to combine the representations of a number of inde­
pendent evidences, to achieve a combined measure of
belief. For the theory to be able give meaningful con­
clusions, it is essential that Dempster's rule is con­
vincingly justified. The rule and its justifications have
been criticised from many angles, a common criticism
being that it can be hard to know when evidences are
independent, and indeed, what 'independence' means
here.
In this paper an axiomatic approach to the combina­
tion of belief functions is taken. The concept of a com­
bination rule is formulated precisely, and assumptions
are made which determine a unique rule, Dempster's
rule. A benefit of this approach is that it makes the in­
dependence or irrelevance assumptions explicit. Since
the assumptions are arguably reasonable this gives a
justification of the rule. This justification is quite dif­
ferent from previous justifications of the complete rule,
though it is related to the justification in [Wilson, 89,
92c] of Dempster's rule for a collection of simple sup­
port functions.
In section 2, the mathematical framework is intro­
duced; in section 3, the concept of a combination rule is
defined; section 4 discusses Dempster's rule and some

of the problems with previous justifications of the rule;
section 5 defines Bayesian conditioning, used for rep­
resenting one of the assumptions; the assumptions on
rules of combination are defined and discussed in sec­
tion 6, and the main result of the paper, that they
determine Dempster's rule, is given.
2

SOURCE STRUCTURESAND
BELIEF FUNCTIONS

In this section the basic concepts are introduced. The
mathematical framework is essentially that of [Demp­
ster, 67] with different notation (and minor differences)
but some fundamental issues are considered in greater
detail.
2.1

SOME BASIC CONCEPTS

We will be interested in sets of propositions and con­
sidering measures of belief over these.
Definition: Frame

A frame is defined to be a finite set1•
Without loss of generality, it will be assumed that
frames are subsets of the set of natural numbers,2 IN.
The intended interpretation of a frame is a set of mu­
tually exclusive and exhaustive propositions. Then the
set of subsets of a frame 0, written as 2°, is a boolean
algebra of propositions.
1'Frame' is an abbreviation for Shafer's term 'frame of
discernment' [Shafer, 76a]; [Dempster 67, 68] and [Shafer,
79] allow frames to be infinite; however the results here
only apply to finite frames of discernment.
2 Actually any other infinite set would do; this is just to
ensure that the collection of all multiple source structures
(defined later) is a set.

528

Wilson

Definition: ( Additive) Probability Function

Let 0 be a frame. P is said to be a probability function
over 0 if P is a function from 2° to [ 0, 1] such that
(i) P(O) = 1, and (ii) (additivity) for all A, B � 0
such that An B = 0, P(A U B) = P(A) + P(B).
We are interested in the propositions in 2°, for frame
e. Dempster, in his key paper [Dempster, 67] consid­
ers a situation where we have a probability function
over a related frame 0 representing Bayesian beliefs.
Definition: Source Structure

A source structure3Sover frame e is a triple (0, P, I),
where n and e are frames (known as the underlying
frame and frame of interest respectively) P (known
as the underlying probability function) is a proba­
bility function over 0, and compatibility function I
(Dempster's multi-valued mapping) is a function from
0 to 2°. Furthermore, for w E 0, if I(w) :::: 0, then
P(w) == 0.
The interpretation ofSis as follows. The set of propo­
sitions we are interested in is 2°, but we have no un­
certain information directly about e. Instead we have
a subjective additive measure of belief P over 0, and a
logical connection between the frames given by I: we
know that, for w E 0, if w is true, then I(w) is also
true. Here it is assumed that P is made with knowl­
edge of I.
The reason for the last condition in the definition is
that if w is true then I(w) is true; however, if I(w) :::: 0
then, since 0 is the contradictory proposition, w cannot
be true, so must be assigned zero probability.
Since it is frame e that we are interested in, we need
to extend our uncertain information about 0 to 2°.
Associated with the source structure Sis a belief func­
tion and mass function over e (see [Shafer, 76a] for the
definitions of these terms) defined, for X � e by
m8 (X) =
L P(w)
wE!l:J(w)=X

Be18 (X) =

L

P(w).

wE!l:J(w)!";X

Bel8 is the extension of the uncertain information
given by P, via the compatibility function I, to the
frame e. It is viewed as a subjective measure of belief
over e, and is generally non-additive.

work, and focuses on belief functions (the lower prob­
abilities in Dempster's framework). The relationship
between Dempster's and Shafer's frameworks is fairly
straight-forward, but for clarity the connection will be
described here. Although this paper deals primarily
with source structures, and justifies Dempster's rule
within Dempster's framework, these results also ap­
ply to Shafer's framework, using the correspondence
between the two.
Proposition

Function Bel : 2° --> [ 0, 1] is a belief function if and
only if there exists a source structure S over e with
Bel8 = Bel.
Each belieffunction has a unique associated mass func­
tion, and vice versa. The focal elements of a belief
function are the subsets of the frame which have non­
zero mass. Let us define the focal elements of a source
structureS= (0, P, I) over e to be the subsets A of e
such that I(w) ==A for some wE 0 such that P(w)#O.
It can easily be seen that the set of focal elements of
S is the same as the set of focal elements of Bel8.
From any belief function Bel, one can generate a source
structure by letting 0 be a set in 1-1 correspondence
with the set of focal elements, and defining the under­
lying probability function and compatibility function
in the obvious way.
Though the underlying frame may be more abstract
than the frame of interest, the natural occurrences of
belief functions generally seem to have an intrinsic un­
derlying frame. Even Shafer, who in his book does
away with the underlying frame, uses a Dempster-type
framework in later work, for example in his random
codes justification of Dempster's rule.
2.3

It is assumed here that all the source structures we
are interested in combining are over the same frame.
This is not really a restriction since if they are over
different frames, we can take a common refinement e
of all the frames (see [Shafer, 76a, chapter 6]). All the
source structures can then be re-expressed as source
structures over e' and we can proceed as before.
3

2.2

THE CONNECTION BETWEEN
SOURCE STRUCTURES AND BELIEF
FUNCTIONS

In his book, a mathematical theory of of evidence
[Shafer, 76a], Shafer re-interprets Dempster's frame3See also 'Dempster spaces' in [Hajek et al., 9'2).

EXTENSION TO DIFFERENT
FRAMES OF INTEREST

COMBINATION RULES

Crucial to Shafer's and Dempster's theories is combi­
nation of belief functions/source structures. The idea
is that the body of evidence is broken up into small,
(intuitively) independent pieces, the impact of each
individual piece of evidence is represented by a belief
function, and the impact of the whole body of evidence

The Assumptions Behind Demp ster's Rule

is calculated by combining these belief functions using
Dempster's rule.
Informally, a combination rule is a mapping which
takes a collection of source structures and gives a
source structure, which is intended to represent the
combined effect of the collection; the combined mea­
sures of belief in propositions of interest can then be
calculated. If possible we would like to make natural
assumptions that determine a uniquely sensible com­
bination rule.
3.1

COMBINING SOURCE STRUCTURES

First a collection of source structures must be for­
mally represented. This is done using a multiple source
structure.
Definition: Jl4ultiple Source Structures

A multiple source structure s over frame 0 is defined
to be a function with finite domain '1/J" C IN, which
maps each i E '1/J" to a source structure over 0; we
write s(i) as the triple (Qi, Pi, I!).
There are some collections of source structures that
give inconsistent information. This leads to the fol­
lowing definition, which is justified in section 6.
Definition: Combinable

Multiple source structures (over some frame) is said to
be combinable if there exist Wi En: (for each i E '1/J")
with P:(w;):f:O and niE'I/I' If(w;):f:0.
Definition: Combination Rule

(i) The Underlying Fl.·ame

Let us interpret element w E n· as meaning that Wi is
true for all i E ,p•. n• is exhaustive, since each Qi is
exhaustive, and every combination is considered; the
elements of n• are mutually exclusive since any two
different ws differ in at least one co-ordinate i, and the
elements of Of are mutually exclusive. Therefore we
can use n• as the underlying frame for the combina­
tion. (Some of the elements of the product space may
well be known to be impossible, using the compatibil­
ity functions, so a smaller underlying frame could be
used, but this makes essentially no difference).
(ii) The Combined Compatibility Function

For w E n•) if w is true, then Wi (E nn is true for
each i E ¢•, which implies It (w;) is true for each i, so
nie..P· It is true (since intersection of sets in 2e corre­
sponds to conjunction of propositions). Assuming we
have no other information about dependencies between
underlying frames, this is the strongest proposition we
can deduce from w. Thus compatibility functions It
generate compatibility function I" on n·.
(iii) The Combined Underlying Probability
Function

This is the hard part of the combination rule so it is
convenient to consider this part on its own, defining
a C-rule to be the third component of a combination
rule.
Definition: C-rule

Let C be the set of all combinable multiple source
structures (over any frames). A combination rule II
is defined to be a function with domain C such that,
for sEC over frame 0, II(s) is a source structure over
e.

3.2

529

THE DIFFERENT COMPONENTS OF
A COMBINATION RULE

A C-rule 1r is defined to be a function, with domain the
set of all combinable multiple source structures, which
acts on a combinable multiple source structure s over
some frame 0 and produces an additive probability
function over Q$. We write the probability function
1r ( s ) as 1!'3•

It turns out that there are easy, natural choices for two
of the three components of a combination rule, . the two
logical components.

4

Definition

In this section Dempster's rule is expressed within the
framework of this paper, and previous justifications
are discussed.

For multiple source structure

s,

(i) ns is defined to be niE.P' n:. An element w of n·
is a function with domain '1/J" such that w(i) E Of. The
element w( i) will usually be written w;.
(ii) The compatibility function I8 is given by P(w)
niE,P' lf (w;).

=

DEMPSTER'S RULE OF
COMBINATION4

4This refers to the rule described in [Shafer, 76a] and
the combination rule in [Dempster, 67), not the amended
non-normalised version of the rule, suggested in [Smets,
88), which is sometimes, confusingly, also referred to as
'Dempster's rule'.

530

4.1

Wilson

DEMPSTER'S COMBINATION RULE
AND C-RULE

Definition: the Dempster C-rule

The Dempster C-rule 7rDs is defined as follows. For
combinable multiple source structure s, and w E 06, if
P(w) = 0 then 7riJ8(w) = 0, else
7r.b8(w)

=

K

IT P:(wi),

iE1fJ•

where K is a constant (i.e., independent of w) chosen
such that 7riJ8(n•) = 1 (as it must for 7rvs to be a
probability function).
Definition: the Dempster Combination Rule

The Dempster Combination Rule acts on multi­
ple source structure s to give source structure
(06' 7rns• J6).
It is easy to see that this is the combination rule used
in [Dempster, 67] and corresponds to 'Dempster's rule'
in (Shafer, 76a].
Justification of Dempster's rule therefore amounts to
justifying the Dempster C-rule 7rDS·
In section 6 the Dempster C-rule is justified by consid­
ering a set of constraints and assumptions on C-rules
that determine a unique C-rule.
4.2

DISCUSSION OF JUSTIFICATIONS
OF DEMPSTER'S RULE

Dempster's explanation of his rule in (Dempster, 67]
amounts to assuming independence (so that for any
w E 06' the propositions represented by Wj for i E 1/J'
are considered to be independent) thus generating the
product probability function P(w) == TiiE1fJ• Pi(wi), for
w E n•. If I' ( w) is empty then w cannot be true, so P
is then conditioned on the set {w : J'(w)¥:0}, leading
to Dempster's rule.
This two stage process. of firstly assuming indepen­
dence, and then conditioning on I'(w) being non­
empty, needs to be justified. The information given
by J• is a dependence between Wi for i E 1/;6, so they
clearly should not be assumed to be independent if this
dependence is known. Ruspini's justification [Ruspini,
87] also appears not to deal satisfactorily with this
crucial point.
A major weakness of a mathematical theory of evidence
is that the numerical measures of belief are not given a
clear interpretation, and Dempster's rule is not prop­
erly justified. This is rectified in (Shafer, 81] with his
random codes canonical examples.

Shafer's Random Codes Canonical Examples

Here the underlying frame n is a set of codes. An
agent randomly picks a particular code w with chance
P(w) and this code is used to encode a true statement,
which is represented by a subset of some frame e. We
know the set of codes and the chances of each being
picked, but not the particular code picked, so when we
receive the encoded message we decode it with each
code w' E n in turn to yield a message J(w') (which is
a subset of e for each w'). This situation corresponds
to a source structure (0, P, J) over e.
This leads to the desired two stage process: for if there
are a number of agents picking codes stochastically in­
dependently and encoding true (but possibly different)
messages then the probability distributions are (at this
stage) independent. Then if we receive all their mes­
sages and decode them we may find certain combina­
tions of codes are incompatible, leading to the second,
conditioning, stage.
To use Shafer's theory to represent a piece of evidence,
we choose the random codes canonical example (and
associated source structure) that is most closely anal­
ogous to that piece of evidence. Two pieces of ev­
idences are considered to be independent if we can
satisfactorily compare them to the picking of indepen­
dent random codes. However, in practice, it will often
be very hard to say whether our evidences are analo­
gous to random codes canonical examples, and judging
whether these random codes are independent may also
be very hard, especially if the comparison is a rather
vague one.5
Shafer's justification applies only when the underlying
probability function has meaning independently of the
compatibility function, that is, when the compatibil­
ity function is transitory (Shafer, 92] (see also [Wilson,
92b] for some discussion of this point). Many occur­
rences of belief functions are not of this form. The
justification given in this paper opens up the possibil­
ity of justifying Dempster's rule for other cases.
The Non-Normalised Version of Dempster's
Rule

The non-normalised version of Dempster's rule [Smets,
88, 92) is simpler mathematically so it is less hard
to find mathematical assumptions that determine it.
However, whether these assumptions are reasonable or
not is another matter. Smets considers that the un­
normalised rule applies when the frame is interpreted
as a set of mutually exclusive propositions which are
not known to be exhaustive. Such a frame can be rep­
resented by a conventional frame, by adding an extra
element representing the proposition which is true if
and only if all the other propositions (represented by
50ther criticisms of this justification are given in the
various comments on [Shafer, 82a, 82b], and in [Levi, 83].

The Assumptions Behind Demp ster's Rule

other elements of the frame) are false, thus restoring
exhaustivity. Therefore Smets' non-exhaustive frames
are unnecessary (and are restrictive).
Smets also attempts to justify (the normalised) Demp­
ster's rule using the unnormalised rule by 'closed-world
conditioning' [Smets, 88], i.e., combining the belief
functions as if the frame was not known to be exhaus­
tive, and then conditioning on the frame being exhaus­
tive after all. This suffers from a similar problem to
that faced by Dempster's justification (see above dis­
cussion), and seems very unsatisfactory: if we know
that the frame is exhaustive then this information
should be taken into account at the beginning (and
then Smets' justification does not apply)-pretending
temporarily that the frame is not exhaustive is per­
verse and liable to lead to unreliable results.
See also [Dubois and Prade, 86; Hajek, 92; Klawonn
and Schwecke, 92].
5

531

Definition: Product Subsets

Let s be a multiple source structure. A is said to
be a product subset of n• (with respect to s) if A=
TiiEtfJ• Ai for some 0;iAi � Of (iE ,P').
Note that such a representation, if it exists, is unique.
For product subset A of 03 and i E ,P', we will write
Ai as the projection of A into Of.
The following is a straight-forward extension of the
Bayesian conditioning of a source structure.
Definition: Bayesian Conditioning of a
Multiple Source Structure

Let s be a multiple source structure and let A be a
product subset of Q3 such that Pt(A;);iO for all iE 'lj;'.
Then the multiple source structure s� is defined as
follows: s� has domain '1/J$ and, for i E '1/J', s�(i) =
(s (i))�.

BAYESIAN CONDITIONING
6

In this section Bayesian conditioning of source
structures6 is defined; these are used to simply express
assumption (A) in section 6.
Definition: Bayesian Conditioning of a
Probability Function

Let P be an (additive) probability function over set
n, and let A � n be such that P(A);iO. Then the
probability function P � over A. is defined by
P�(f)=

P(f)
P(A)

for

r

� A.

This is used for conditioning on certain evidence A.
Note that if A is considered to be certain, and n is a
frame, then A is also a frame.
Definition: Bayesian Conditioning of a Source
Structure

Let S= (Q, P, I) be a source structure over frame 0
and let A � n (representing certain evidence) be such
that P(A);iO. Then s� is defined to be the source
structure (A, P �,I�), where I� is I restricted to A.
This should be uncontroversial, given that the judge­
ment of the underlying epistemic probability P is made
with knowledge of the compatibility function.
Incidentally if, for source structure S = (0, P, I) over
0 and A � 0, we let A = {wEn : I(w) � A}
then S� corresponds to geometric conditioning by A
[Shafer, 76b; Suppes and Zanotti, 77].
6This is not closely related to Bayesian updating of a
belieffunction [Kyburg, 87; Jatfray, 92]

CONSTRAINTS AND
ASSUMPTIONS ON C-RULES

In this section we introduce two clearly natural con­
straints on C-rules, and two arguably reasonable as­
sumptions. It is shown that together these determine
a unique C-rule, which turns out to be Dempster's C­
rule, hence justifying Dempster's rule.
Constraint: Respecting Contradictions

C-rule 1r is said to respect contradictions if for any
combinable multiple source structure s and w E n•, if
P(w) = 0 then 7r3(w)= 0.
If I' ( w) :::: 0 then w cannot be true since w true im­
plies J• (w) true, and 0 represents the contradictory
proposition. Therefore any sensible C-rule must re­
spect contradictions.
Constraint: Respecting Zero Probabilities

C-rule 1r is said to respect zero probabilities if for any
combinable multiple source structure s and w E 0', if
Pi (w;) = 0 for some iE '1/J', then 1!'1(w) = 0.
If Pf ( w;) = 0 for some i then w; is considered impossi­
ble (since frames are finite), so, since w is the conjunc­
tion of the propositions w;, w should clearly have zero

probability.

Note that if we missed out the condition that the mul­
tiple source structure had to be 'combinable' in these
two constraints and in the definition of a C-rule then
these two constraints are inconsistent: for any C-rule 1r
and any multiple source structure s which is not com­
binable, if 1r respects contradictions and zero proba-

532

Wilson

bilities then 7r8(w) = 0 for any wE !.l", which is incon­
sistent with 1r8 being a probability function.
Definition

Let s be a multiple source structure, k E 1/J", and I E
!.lt. Then

Ei is defined to be {wE n• : w(k) =I}, and -.Ei is
defined to be {wE n• : w(k)fl}, i.e., !.l"\Ei. The set
Ei is the cylindrical extension in n• of I (E !.lt), and
can be thought of as expressing the event that variable
k takes the value l.
Definition: Assumption (A)

C-rule 1r is said to satisfy assumption (A) if 7r respects
zero probabilities and, for any combinable multiple
source structure s, for any k E 1/J", I E !.lt such that
7r8(.6.)f0, where .6. = -.EL,
(7r")a = 7r•a.
Note that since 7r respects zero probabilities, if
7r"(.6.)f0 then Pk(.6.k)f0, so s� is defined.
In fact it can be shown that if 7r satisfies assumption
(A) then it satisfies a more general form of the assump­
tion where .6. is allowed to be an arbitrary product
subset of n·.
Assumption (A) can be thought of as postulating that
Bayesian conditioning commutes with source structure
combination.
Bayesian conditioning by -.Ei can be viewed (roughly
speaking) as omitting the lth focal element from the
kth Belief function (and scaling up the other masses).
Assumption (A) amounts to saying that it should not
make any difference whether we omit that focal ele­
ment before, or after, combination.
Definition: Assumption (B)

Let s be a combinable multiple source structure such
that, for some kE 1/J",
lOti= 2 and lOti= 1 foriE 1/J" \ {k},
and l" (w)f0 for wE n•.
Then for IE !.lt,

The notation hides the simplicity of this assumption.
The multiple source structures referred to are of a very
simple kind: one of the component source structures
has just two elements in its underlying space, and so
leads to a belief function with at most two focal ele­
men�s, and all the other component source structures
give belief functions with just one focal element, so

they can be viewed as just propositions, i.e., certain
evidences; furthermore there is no conflict in the evi­
dences. In terms of belief functions this is the situation
where we are conditioning a belief function with two
masses by a subset of e.
Assumption (B) is just that adding all the other cer­
tain sources does not change the probabilities of com­
ponent k. The rationale behind this assumption is that
the certain evidences are not in conflict with the in­
formation summarised by the kth source structure, so
why should they change the probabilities?
Theorem

7rDS is the unique C-rule respecting contradictions,
zero probabilities and satisfying (A) and (B).
This means that Dempster's rule of combination
uniquely satisfies our constraints and assumptions,
hence justifying it.
Sketch of Proof

Unfortunately the proof of this theorem is far too long
to be included here. To give the reader some idea of
the structure of the proof, it will be briefly sketched.
It can easily be checked that 7rDS satisfies the con­
straints and assumptions. Conversely, let 7r be an ar­
bitrary C-rule satisfying the constraints and assump­
tions. First, it is shown that 7r satisfies a more gen­
eral form of (A), where .6. is allowed to be an arbi­
trary product subset of n•. This is then applied to
the case of .6. = {w, w '} where w and w' differ in only
one co-ordinate. In conjunction with assumption (B)
this enables us to show that, when the denominators
are non-zero,
1r"(w)
1r8(w')
.
7r_bs(w)
7r.bs(w ')
_

A source structure over e is said to be discounted
if e is a focal element of it, and a multiple source
structure is said to be discounted if each of the source
structures of which it is composed is discounted. It
is then shown that, for any discounted multiple source
structure s, 7r8 = 7r_b8, using the last result repeatedly.
The theorem is then proved by taking an arbitrary
combinable multiple source structure t, discounting it
to form s (see [Shafer, 76a]) and using the more general
form of assumption (A) again to relate 7rt and 7r8
7r.bs.
7

DISCUSSION

Both assumptions (A) and (B) seem fairly reasonable.
(A) appears to be an attractive property of a C-rule,
but is a rather strong one, and it is not currently clear
to me in which situations it should hold (it is conceiv­
able that there are other reasonable-seeming principles

The Assump tions Behind Dempster's Rule

with which it is sometimes in conflict). Further work
should attempt to clarify exactly when both assump­
tions are reasonable.
There are cases where Dempster's rule can seem un­
intuitive, for example, I argued in [Wilson, 92b] that
Dempster's rule is unreasonable at least for some in­
stances of Bayesian belief functions, and there has been
much criticism of certain examples of the use of the
rule e.g., [Pearl, 90a, 90b; Walley, 91; Voorbraak, 91;
Zadeh, 84]_7
If it does turn out that there are certain types of belief
functions where assumption (A) or (B) is not reason­
able, then the above theorem, as it stands, is not use­
ful. However, an examination of its proof reveals that
only two operations on belief functions/source struc­
tures are used-Bayesian conditioning (i.e, omitting
focal elements and scaling the others up) and discount­
ing (i.e, adding a focal element equal to the frame
e, and scaling the others down). This means that
the proof could be used to justify Dempster's rule for
any sub-class of belief functions/source structures (for
which (A) and (B) may be more reasonable) which is
closed under these operations, for example the set of
simple support functions or the set of consonant sup­
port functions. Also, for the same reason, the proof
could be used to justify Dempster's rule for collections
of belief functions/multiple source structures s such
that l3(w)-:j:.0 for all w E f23, if (A) and (B) were con­
sidered reasonable here.
It might also be interesting to investigate alternatives
to (B), which give different values for 11'3(Ek) than
those given in (B). The proof of the theorem can be
modified to show that there is at most one C-rule
satisfying the constraints and assumptions, though of
course it will not be the Dempster C-rule.
Acknowledgements

I am very grateful to an anonymous referee for pointing
out a minor error.
This work was supported by a SERC postdoctoral fel­
lowship, based at Queen Mary and Westfield College.
Thanks also to Oxford Brookes University for use of
their facilities.


We take a general approach to uncertainty
on product spaces, and give sufficient condi­
tions for the independence structures of un­
certainty measures to satisfy graphoid prop­
erties. Since these conditions are arguably
more intuitive than some of the graphoid
properties, they can be viewed as explana­
tions why probability and certain other for­
malisms generate graphoids. The conditions
include a sufficient condition for the Inter­
section property which can still apply even if
there is a strong logical relationship between
the variables. We indicate how these results
can be used to produce theories of qualita­
tive conditional probability which are semi­
graphoids and graphoids.
Keywords: Graphoids, Conditional Inde­
pendence, Qualitative P robability.

This means that then independence assumptions can
be propagated using the graphoid inference rules, and
can be represented and propagated using the (both
directed and undirected) graphical methods of [Pearl,

88].
In section 2 we define independence structures, semi­
graphoids and graphoids. GCPPs are defined in sec­
tion 3, with examples and we show how they give rise
to independence structures. Section 4 considers the
Intersection property. In the literature it seems to be
generally assumed that this only holds for probability
distributions which are always non-zero; we show here
that it holds much more generally, a sufficient con­
dition being a connectivity property on the non-zero
values of the probability; exactly the same condition
is sufficient for other GCPPs to satisfy Intersection.
Section 5 considers different sufficient conditions for
GCPPs to give rise to semi-graphoids. These are use­
ful for constructing uncertainty calculi which generate
graphoids and might also be used for showing that an
uncertainty calculus gives rise to a graphoid.

1

INTRODUCTION

The importance of the qualitative features of prob­
abilistic reasoning has often been emphasised in the
recent AI literature, especially by Judea Pearl. An
i mportan t qualitative aspect of probability is given by
the graphoid properties, defined in [Pearl, 88] (see also
[Dawid, 79; Smith, 90]) which sum up many of the
properties of probabilistic conditional independence.
In this paper we look at the reasons why probabil­
ity obeys these properties, with an eye to generating
other uncertainty theories which share much of the
same structure as probability, but represent different
types of information, perhaps of a more qualitative
nature.

A fairly general family of uncertainty calculi on prod­

uct spaces is introduced, which we call Generalised
Conditional Probability on Product Spaces (GCPP),
and define two different types of conditional indepen­
dence for GCPPs. We show that under simple (and
apparently fairly weak) conditions, conditional inde­
pendence for GCPPs satisfies the graphoid properties.

In section 6 we consider another view of GCPPs: as
qualitative conditional probabilities. This view allows
graphoids to be constructed from qualitative compar­
ative judgements of probability. Section 7 briefly con­
siders computation of GCPPs, and section 8 highlights
some areas for further study.

2

INDEPENDENCE STRUCTURES

Let U be a finite set. An independence structure I on

U is defined to be a set of triples (X, Z, Y) where X, Y
and Z are disjoint1 subsets of U. We write I(X, Z, Y)
for (X, Z, Y) E I. For disjoint subsets X, Y � U, their

union

XU Y

will be written

XY.

U is intended to be a set of variables, and I(X, Z, Y) is
intended to mean that variables X are independent of
variables Y given we know the values of the variables

z.
1 A collection A of sets is sa.id to be disjoint if for ea.ch
E A, X n Y
0.

X, Y

=

584

Wilson

The Graphoid Properties of Independence
Structures

I(X, Z, 0)

(Trivial Independence)

If I(X,Z, Y) then I( Y, Z, X)

(Symmetry)

If I(X,Z, YW) then I(X, Z, Y)

If I(X,Z, YW) then I(X,ZY, W)

(Decomposition)

(Contraction)

If I{X,ZY, W) and I(X, ZW, Y) then I(X , Z, YW)
(Intersection)
where W, X, Y, Z are arbitrary disjoint subsets of U
(so, for example, I satisfies symmetry if and only if
the above property holds for all disjoint X, Y and Z).
If an

independence structure satisfies all these proper­
ties then it is said to be a graphoid; if it satisfies the
first five (i.e, all except Intersection) then it is said to
be a semi-graphoid. As we shall see in section 5, prob­
abilistic conditional independence is a semi-graphoid,
and in certain situations a graphoid.
The definitions given here for semi-graphoid and
graphoid differ from that given in [Pearl, 88], in that
we require Trivial Independence to hold. However, our
definition seems to be what Pearl intended2; it is not
implied by other properties (consider the empty inde­
pendence structure) and it is satisfied by probabilis­
tic conditional independence so it is a (rather triv­
ial) counter-example to the Completeness Conjecture3
[Pearl, 88]; also without Trivial Independence, Markov
boundaries don't necessarily exist (consider the empty
independence structure again) which makes Theorem
4 of [Pearl, 88] incorrect.
The intersection of a family of (semi-)graphoids is a
(semi-)graphoid. Hence, for any independence struc­
ture I, there is a unique smallest (semi-)graphoid con­
taining I.

GENERALISED CONDITIONAL
PROBABILITY ON PROD UCT
SPACES (GCPPs)

Uncertainty measures are usually defined on boolean
algebras. However, for our purposes of studying in­
dependence structures generated by the uncertainty
measure, a different domain is natural.
3.1

The set u· is defined to be

(Weak Union)

If I(X,ZY, W) and I(X,Z, Y) then I(X,Z, YW)

3

disjoint X, Y � U, an element of XY may be written
xy for x EX, y E Y. For disj oi nt X, Y � U we define
XIY to be the set of all pairs {xly : x EX, y E Y}.
The set ]1 is defined to be a singleton {T}. An element.
xiT of X I� will usually be abbreviated to x (we are
identifying Xl]l. with X).

THE B ASIC DEFINITIONS

U = {X1, ... , Xn} is said to be a set of variables if
associated with each variable X; E U is a finite set of
values X;. For X� U define X to be Ilx,E X;. For

X

2See, for example, the sentence before Theorem 4, p97
of [Pearl, 88].
3Fatal counter-examples are given in [Studeny, 92].

u

X,YCU
XnY =�

X IY.

A GCPP p over set of variables U is defined to be a
function p: u• _. D for some set D containing differ­
ent distinguished elements 0 an d oo such that for any
disjoint X, Y � U and x EX,
(i) p(x) = 0 if and only if for all y E Y,
and

p(xy)

= 0,

(ii) for any y E Y, p(xlv) = oo if and only if p(y) = 0.
GCPPs will be viewed as measures of uncertainty;
p(xly) may be thought of as some sort of measure
of how plausible it is that the composite variable X
takes the value x , given that Y takes the value y. The
assignment p(x IY) = 0 is intended to mean that :c is
impossible given y, i.e., X cannot take the value :1: if
Y takes the value y. The inclusion of element oo in
D is not strictly necessary; it is u sed as a notational
convenience, and can be read as 'undefined'. We re­
quire (i) because: x is possible if and only if there is
some value of Y which is possible when X takes the
value x. We require (ii) because: if y is impossible
then conditioning on y doesn't make much sense.
Note that no structure on Dis assumed; for example,
we do not assume that D \ { oo} � IR, or even that D
has an ordering on it.
The definition implies that p(T) ::}; 0, oo and
any X� U and x EX, p(J.;)::}; oo .

that

for

Definition

For GCPP p over U and disjoint X, Y � U define pXIY
to be p restricted to X IY and define px to be pXI0.
For Z � U and z E Z define Pz: (U \ Z)" -> D by,
for disjoint X, Y � U \ Z, x E X, y E Y, Pz( xly) =
p(xlyz). For disjoint X, Y � U \ Z, p;'JY is defined to
be Pz restricted to X IY, and P7 is defined to be p;'10.
GCPP p over U is said to be a full GCPP over U if
for every Z � U and z E Z such that p(z)::}; 0, p, is a
GCPP over U \ Z.
The function P.z may be thought of as p conditioned
on Z = z. It turns out that, for GCPP p, p is a full
GCPP if and only if for all disjoint X, Y � U, x E X,
y E Y , [p(xly) = 0 ¢::=::> p(xy) = 0 and p(y)::}; 0].
3.2

EXAMPLES OF GCPPS

A probability function over set of variables U is de­
fined to be a function p: u· - [ 0, 1] u { 00} such that

585

Generating Graphoids from Generalised Conditional Probability

for xJy E U*, (i) P(xJy)
(ii) if P(y) # 0, P(xJy)
P(x)::: LweU\X P(xw).

¢::::::}
P(y) == 0;
P(xy)/P(y); and (iii)

oo

The definition implies that P is a full GCPP over U
and P{T) = 1. The latter follows since P(T) is equal,
by definition, to P(TJT) so by (i) above, P(T) = 0
if and only if P(T) = oo, which implies that P (T) is
neither 0 or oo. We can now apply (ii) to get P(T) =
P(TIT) = P(T)/P(T) which implies that P(T) = 1 as
re quired.
For any (finite) set of variables U, there is a one-to-one
correspondence between probability functions P over
U and probability functions f on U, i.e., functions
f:U--> [0,1] such that LuEuf(u) = 1; P restricted
to U is a probability function on U, and conversely,
a probability function f on U extends uniquely to a
probability function over U using (i), (ii) and (iii).

A Dempster possibility function over set of variables U
[ 0, 1) u { 00} such
is defined to be a function 11": u·
that for xJy E U*, {i) 1r(xly) = oo <==:> 1r(y)
0;
(ii) if 1r(y) # 0, 1r(xly) = 1r(xy)/1r(y); and (iii) 1r(x) =
maxwEU\X 1r(xw).

include a maximal element in the range of OCFs be­
cause he desired belief change to be reversible [Spohn,
88, p130}.
Kappa f u nct ion x: can be transformed into a Demp­
ster possibility function 1r" by 1r"(¢) == oo ¢::::::}
.�e(1/;) = oon and 1r"("l/J) = 2-�<(;J;) otherwise, for
1/J E U*, where 2-oo is taken to be 0. For 1/J, ¢ E U*,
x:( 1/;) == x:( ¢) ¢::::::} 1r" (1/;) = 1r"' ( ¢). This means that,
for our purposes, kappa functions can be viewed as
s p eci al cases of Dempster p ossibili ty functions.

Shafer's pla usi bil ity functions [Shafer, 76], also give
full GCPPs; their dual functions, belief functions, and
ne ce ssi ty functions, the dual of possibility functions,
do not give GCPPs, since, for these, a value of 0 means
a lack of e vid ence , rather than 'impossible'.
3.3

INDEPENDENCE STRUCTURES OF
GCPPS

-+

=

Again, the definition implies that 1r is a full GCP P
over U and 1r(T) = 1. De mps ter possibility functions
are essentially Zadeh's possibility measures [Zadeh,78;
Dubois and P r ade , 88] and consonant plausibility fun c­
tions [Shafer, 76]; the definition of conditional possibil­
i ty is obtained from Dempster's rule, and is not the one
most commonly used, partly because it means that the
range of 1T cannot be viewed as an ordinal scale (and
most justifications of possibility theory require this).
A special case of Demps ter possibility functions are
consistency functions over U, where 1T only takes the
values 0, 1 and oo. 1r(xjy) = 1 is then int ended to mean
that, given that y is the true value of variables Y, it is
possible that x is the true value of variables X. Every
full GCPP p over U gives rise to a consistency function
p* over U defined, for 1/; E U* , by p*('l/;) = p('I/J) if
p("l/J) = 0 or oo, and p*(l/J) = 1 otherwise. Consistency
functions appear in the theory of relational databases
[Fagin, 77}, and also in [Shafer et a/., 87].
A kappa function over set of variables U is defined
to be a function K.:U*-+ {0,1, 2 , .. . ,oo,oon } (where
oo n is different from the other elements) such that
for xiy E U*, (i) K.(xly) = DOn {::::::} ��:(y) = oo;
(ii) if K(y) # oo, K(xjy) = x:(xy) - K(y); and (i ii )
x:(x) = minwEU\X K(xw).
The definition impli es that K.(T) = 0 and ,.., is a full
GCPP over U (however, the labelling of the elements
in the range of "' is confusing: the zero of D in the
definition of a GCPP is oo and the element meaning
'undefined' is oo n not oo ) . Kappa f unctions are based
on Spohn's Ordinal Conditional Fu nctions [Spohn, 88].
An important difference is that the range of kappa
f unctions has a maximum element oo; Spohn did not

For GCPP p over set of variab les U, independence
structures Ip a n d I� are defined as follows. Let X,
Y and Z be disjoint subsets of U .
Ip(X, Z, Y) if and only if p(:I:jyz) = p(J:Iz) for all
X, y EY, z E Z such that p(yz) # 0.

x

E

I�(X, Z, Y) if and only if p(:z:Jyz) = p(xly'z) for all
x EX, y,y' E Y, z E Z s11ch that p( yz) =P 0 and.
p(y'z)#O.
For set S and functions g, h: S -+ D write g =00 h if
they are equal when they are both defined, i.e., if for
all s ES, [g(s) = h(s) or g(s) = oo or h(s) = oo] . This
gives a simpler way of expressing; the two independence
structures. For disjoint. subsets X, Y and Z of U,

Ip(X, Z, Y) if and only if for ally E Y, p;jiZ

and

I�(X,Z,Y) if and
XIZ
XIZ
py
Py'
==

'
only i ffor all y,y

= ""

pXIZ,

EY,

00

To underst and the definitions, first consider the case
when Z == 0. Then Ip(X, Z, Y) if the degree of plau­
sibility of x, p(x), does not change when we condition
by any (possible) value y of Y. Thus our uncertainty
about variable X does not change by learning the value
of variable Y I�(X, Z, Y) holds if the degree of plau­
sibility of x gi ven y, p(xiy) does not depend on the
choice of (possible) value y of Y. The same 1·emarks
apply for general Z, except that. now we must consider
the degrees of plau sibilit y conditional on each value z
of Z.
We shall see in s ectio n 5 that for any GCPP p, lp
satisfies Trivial Independence and Contraction, and, if
Ip = I;, it satisfies Decomposition and Weak Union
al so .

586

Wilson

If GCPP p is non-zero on U then, trivially, U
connected, and so
satisfies Intersection.

THE INTERSECTION
PROPERTY

4

I;

This is the only one of the graphoid properties that
does not hold for all probability functions. [Pe ar l, 88,
p87] appears to suggest that it only holds for proba­
bility functions which are every where non-zero. This
turns out not to be the case, and we will see that a
sufficient condition for Intersection to hold sometimes
allows very strong logical dependencies between the
variables.
Set n is said t o be connected under relation R � n X n
if the smallest equivalence relation on 0 containing R
is the relation 0 X 0.
Let p be a GCPP over set of variables U and let Y,
W and Z be disjoint subsets of U. For z E Z.. define
(YW)t,z = {yw E YW : p(ywz) =/; 0}. We say that
(Y, W) is p, Z-connected4 if for all z E £.., (YW)t,,. is
connected under the relation R defined by
1
'
{::::::} y = y or w = w .

yw R y' w'

For GCPP p over set of variables U, we say that U if
p-connected if for all disjoint subsets Y, W, Z of U, the
pair {Y, W) is p, Z-connected.
Note that these properties only depend on the set of
elements of U for which pis zero (that is, those which
are known to be impossible).
The above concepts are not quite as obscure as they
appear at first sight. (YW)�. is the set of yw which
are not known to be impossible when we know that
Z = z. If we label Y as Y1, ... , Ym and W as
W1, ... , Wn then YW = Y x W can be viewed as the
squares of am x n chess board. Then y;Wj R Yi'Wj' iff
i = i' or j = j', i.e., iff the rook chesspiece could move
between the squares (i,j) and (i',j'). Let N, be the
set of squares corresponding to (YW)t,.. We there­
fore have that (Y, W) is p, Z-connected iff for all z, it
is possible to move between any two elements of N, us­
ing a sequence of rook moves, where each intermediate
square is also in Nz.

Let pbe a GCPP over a set of variables U.
(i) For disjoint subsets X, Y, Z and W of U,
suppose that {Y, W) is p, Z-connected and also
that 1;(x, ZY, W) and I;(x, ZW, Y).
Then
holds.

(ii) If GCPP p over set of variables U is such that U
if p-connec.ted then
satisfies Intersection. 5

I�

similar concept is important in
in [Moral and Wilson,
it guar­
antees the convergence of Markov Chain Monte-Carlo algo­
rithms, and in
ilson,
it is relevant to the justification
of Dempster's rule.
4Interestingly, a

very

Dempster-Shafer theory:

(W

94)

93)

(89)

5Milan Studeny
has found a similar result (for the
of probability functions).

case

p­

Example
Let U = {S, H1, H2}. Variable S ranges over shoe
sizes, and the correct value is the shoe size of the (un­
known) next person to walk into my office. H1 and H2
both take integer values between 0 and 3000. The cor­
rect value of H 1 is the height in millimetres rounded
down of this unknown person and the correct value of
H2 is their height to the nearest millimetre.
Let P be a Bayesian probability function on U, rep­
resenting our Bayesian beliefs about the variables. As
described above, P extends uniquely to a GCPP over
U. Now, P(ij) = 0 unless i = j or i = j- 1, where
ij means H1 = i and H2 = j. Despite the very strong
logical relationship between H1 and H2, ({HI},{H2})
is P, 0-connected, and so if we considered S to be
logically independent of {H1 , H2}, in the sense that
P(sh1h2) = 0 if and only if P(s) = 0 or P(h1h2) = 0,
then U would be P-connected. This implies that Ip
( = If, by the results of the next section) would satisfy
the Intersection axiom, and so would be a graphoid.
In any case, given knowledge of height to the near­
est millimetre, one will learn almost nothing more
about shoe size by learning height in millimetres
rounded down, so one might be tempted to make
the subjective conditional independence judgement
/p({S}, {H2}, {HI}). ( Alternatively, if one did not
know the precise meaning of H1 and H2, but had
the values of the variables for previous visitors then
it would take a vast amount of data to detect a de­
pendency.) Similarly one might be tempted to say
/p({S}, {HI}, {H2}). But these lead, using the last
proposition, to Ip({S}, 0, {H1, H2}) which is certainly
unreasonable since there is a definite dependency be­
tween shoe size and height.

This example illustrates how careful one must be
with subjective independence judgements for Bayesian
probability (or any other GCPP). It also seems to sug­
gest that GCPPs, with definition
cannot represent
'approximate independence' .

1;,

Proposition 1

I�(X, Z, YW)

IS

5

SUFFICIENT CONDITIONS FOR
GCPPS TO GENERATE
GRAPHOIDS

Here we consider simple sufficient conditions on
GCPPs for its associated independence structures t.o
satisfy semi-graphoid properties. Since probability
functions, Dempster possibility functions, kappa func­
tions and consistency functions sat isf y these proper­
ties with the exception of the conditions of proposi­
tion 4(v), these could be v iewed as explanations for
why they are semi-graphoids.

Generating Graphoids from Generalised Conditional Probability

5.1

CONDITIONAL COHERENCE

It is easy to see that for any GCPP p, Ip � I�, i.e.,
if Ip(X, Z,Y) holds then I�( X, Z,Y) must hold. Fur­
thermore if p is inten de d to represent a generalised
form of probability then a natural constraint is that
Ip = I�. T his is because a sufficient condition for
Ip = I� is the apparently reasonable condition:
Conditional-Coherence:

For any disjoint subsets

EX and z E Z, if for all y,y' EY ,
p(xiyz) = p(xly'z) (i.e., p(xlyz) does not vary with
y) then p(xlz) = p(xlyz) (i.e., p(xlz) is e qu al to that
constant v alue).
We say t hat p is weakly conditional-coherent if Ip = I�.
X,Y,Z of U,

x

Consider conditioning on a fixed z E Z. The idea
behind conditional coherence is that if the degree of
plausibility of x given y (i.e, Pz(xiy)) is not dependent
on which value y of Y we use, then one might expect
that the degree of plausibility of x ( i.e ., Pz(x)) wou ld
be equal to that constant value. The conditionals and
marginal then cohere in a particular sense.

Conditional-Coherence is a re stri cted version of the
S an d wich Prin cip le [Pearl, 90; IJAR, 92]. Pl ausi bi l­
ity /belief functions and upper /lower probability func­
tions have good reason not to obey conditional coher­
ence: see e.g., [W ilson, 92; Chunhai and Arasta, 94).
It is satisfied by probability and Dempster possibility
functions and hence by con si stency and kappa func­
ti ons.
Proposition 2

For any GCPP p over set of variables U the indepen­
dence s tru cture IP satisfies Trivial Independence and
Contraction, and lp satisfies Decomposition if and only
if it satisfies Weak Union.
Now suppose tha t p is weakly conditional-coherent.
We then have

(i) lp s at is fies Decomposition and Weak Union.
Therefore if Ip satisfies Symmetry then it is a semi­

graphoid.

(ii) If U is p-connected then Ip satisfies Intersection.
Therefore if lp satisfies Symmetry then it is a
graphoid.
Perhaps the only one of t he grap hoid properties, with
the exc epti on of Trivial Independence, which imme­
diately seems natural is Symmetry. Surprisingly, it
seems to be harde r to find natural sufficient conditions
on p for Ip t o satisfy Symmetry. The follo wi ng result
gives a fairly strong condition.
Proposition 3

Suppose that p is a full GCPP over U such that for all
Z � U and z E Z, there exists a fu ncti on o: R -+ D
for some R � D x D such that

587

(i) for al l xly E (U\Z)*, Pz(xy) = p,.(xly)<>pz(Y), and
(ii) if a o b = c <>a and

a

=f

0 t hen b

= c.

Then Ip satisfies Sy mme tr y.
5.2

DETERMINIST IC RELATIONS
BETWEEN JOINTS, CONDITIONALS
AND MARGIN ALS

If I is an independence structure let its reflection IR
be defined by IR(Y, Z,X) {::::} I(X, Z, Y). Clearly,

I satisfies Symmetry if and only if I = I R. Let
I5 = In JR be the symmetric part of I, so that for
disjoint subsets X,Y, Z of U, I5 (X, Z,Y) if and only
if I(X, Z, Y) and I(Y, Z,X).
Proposition 4

Suppose p is a GCPP o ve r set of variables U.

(i) If p(Tix) = p(T) for all X � U and x EX such
that p(x) =f 0, t h en (I�)R s ati sfies Trivial In depen ­
d en ce.
(ii) If for all disjoint W,Y,X � U there exists a func­
tion M su ch that for al l x E X, M(p!;YY) = p";
then (I�)R satisfies De com positi on .
(iii) If for all disjoint W,Y,X � U t here exists a fun c­
tion C s u ch that for all x EX, C{p!;YY) = p!;YIY
then (I�)R satisfei s Weak Union.
(iv) If p i s a full GCPP and for all disjoint W,Y,X �
U there exists a function J such that (a) for all
x E X, J(p!;YIY, p;;) = p!;YY and (b) J(g, h) =
J(g',h) whe n for all wand y, [g(wYi ) = g' ( wly)
or h(y) = 0], for functions g, g': WIY -+ D and
h: Y-+ D wit h (g,h) and (g',h) in the domain of
J. The n (I�)R sati s fies Contraction.
( v) If p is non-zero on U and for all disjoint W, Y, X �
U there exists a function S s uch that for all x EX,
S(p!;YIY, p�IW) = p!;YY then (I�)R satisfies Inter­

section.

The functi on Min (ii) can be thoughtofas a marginal­
isation op e r ator, and C in (ii i) as a conditioning op­
erator. J in (iv) gives a way of calculating the joint.
dist r ib u tion from the conditional and marginal distri­
butions; condition (b) in (iv) can be omitted if p is
non- ze ro on U; J is e ssen tia lly just pointwise multipli­
cation for probability and Dempster possibility.
The existence of M in (ii) means that for each x,
the joint distribution of Px on WY determines the
marginal distribution (of Px on Y). To see how
this condition is used , suppose I�(WY, 0,X) and
p(x), p(x') =f 0; then p!;YY = p !;I:Y sop� = p� which
leads to I�(Y, 0,X). Similar considerations apply to
(iii), (iv) and (v).
Pr op osi ti on 4 implies that if p is a full GCPP, satisfy­
ing the conditions of (i), (ii), (iii) and (iv) above, an d

Wilson

588

1;

is symmetric then

I;

is a semi-graphoid; further­

more if U is p-connected then

J� is a graphoid.

The result also leads to a way of constructing a semi­
graphoid from a GCPP even if I, and
are not sym­
metric.

I�

Proposition 5
If GCPP p over U is weakly conditional-coherent, and
satisfies the conditions of (i), (ii), (iii) and (iv) of
proposition 4 then
is a semi-graphoid. If, in ad-

If

dition, p satisfies the conditions of (v) then
graphoid.

6

If

IS

a

associated SQCPPs. The correspondences between
GCPPs and SQCPPs mean that the sufficient con­
ditions for independence structures to satisfy the
graphoid properties given in section 5 can be trans­
lated into sufficient conditions for SQCPPs (and hence
QCPPs) to generate independence structures with
those properties.

If consistent � is a full SQCPP (i.e, for all xly E u·
� 0 -¢:::::::} xy � 0 and y ';j:; 0), a sufficient condi­
I

xly

tion for I� to satisfy Symmetry is the following 'cross
multiplication' condition:
For all disjoint X, Y, Z <:;;: U, x E X, y E Y, z E �� if
xz ¢ 0 and xyzlyz �xzlz then xyzlxz � yziz,
where we have trivially extended � to elements :r:yly
of XYIY (for disjoint X, Y <:;;: U), by placing xyiy in
the same � e quivalence c las s as xly.

QUALITATIVE COND ITIONAL
PROBABILITY

-

Another way of viewing GCPPs is as Qualitative Con­
ditional Probabilities on Product spaces (QCPPs).
A Symmetric QCPP (abbreviated to SQCPP) �over
set of variables U is an equivalence relation on u· U
{O,oo} satisfying, for disjoint X, Y <:;;: U and x EX,
(i)

p(x)

� 0 if and only if for ally E Y,

p(xy) �

0,

We will ofte n want to cons truct a QCPP from a num­
ber of different types of information:

p(y) � 0.

(i) some qualitative probability relationships we ex­
pect always to hold, such as, perhaps 0 � xl11 for
all xiy E U*;

and
(ii) for any

y

E Y,

p(xly) � oo if

� is said to be consistent if 0 f.

and only if

oo.

Independence s t ru c tures I:: and 1{:, on U are defined
analogously to the definitions for GCPPs: Ir:::(X,Z, Y)
-¢:::::::} xlyz � xly for all x, y, z such that yz ¢ 0, and
I�(X, Z, Y) -¢:::::::} xlyz � xly' z for all x, y, y1, z such
that yz f. 0 f. y' z.
The framework of consistent SQCPPs is essentially
equivalent to the framework of GCPPs. For any GCPP
p over U we can define a consistent SQCPP �P over U,
by first ext endi ng p toW U{O, oo} by defining p(O) = 0
and p(oo) = oo, and then, for ¢,1/; E U"' U {O,oo},
defining t/J �P ¢ -¢:::::::} p(t/;) = p(¢). We then have
Ir:::. p = and I!.. = I '.

Ip

-p

Constructing QCPPs

p

Conversely, for consistent SQCPP � we can define

GCPP Pr:::. by letting D = (u•u{O, oo})/�, calling the
equivalence classes of 0 and oo by 0 and oo res pec ti v e ly
and, for xly E U*, defining Pr:::.(xly) = d-¢:::::::} d 3 xly.
We have I,, = Ir:::. and I� .. = I�- Also, for any
SQCPP �. we have �(p..,)= �Relation � is said to be a QCPP over set of variables
U i f it is a reflexive transitive relation on U* U {0, oo}

such that its symmetric part � (the intersection of �
and �) is a SQCPP over U.

QCPPs might be thought of as probability fun cti on s
without the numbers; a statement such as xlz � yiz
could mean 'given z, value y i s at least as prob able as

x'.

QCPPs generate independence s tru ctures via their

,

(ii) some desirable properties of=:::;, such as the above
sufficient condition for Symmetry of Ir:::., and other
conditions that imply graphoid properties;
(iii) an agent's comparative probability judgements,
e.g., statements of the form xlz =:::; x or xlz � yjz;
(iv) an agent's conditional independence judgements.
The obvious way to at t e mpting to construct a QCPP

for a particular si tu a t io n is to treat (i) and (iii) as
sets of axioms and (ii) and (iv) as sets of inference
rules, and generate the QCPP from these. H owever,
there is a technical problem: because of the condi­
tions yz ¢ 0, the conditional independence assump­
tions cannot quite be viewed as se ts of inference rules.
We can solve this by requiring that the user gives (ex­
plicitly or implicitly) a II the values u of U such that
u � 0 (that is, the set of all u which are considered im­
possible); the key point here is that the application of
the rules must not lead to any more zero values of U.
The conditional independence assumptions can now be
viewed as inference rules, since they are now closed un­
der intersection. For the same reason, we re q uire the
properties in (ii) also to be closed under intersection,
once the zeros of U are determined.
Naturally, if we have included in (ii) properties which
imply that Ir:::. is a semi-graphoid, then we can pr opa­
gate conditional independence assumptions using the
semi-graphoid prope rt ie s, or using the graphical meth­
od s described in (Pearl, 88].

Generating Graphoids from Generalised Conditional Probability

COMPUTATION OF GCPPS

7

Here we only consider computation of values of the
joint distribution of a GCPP, leaving other aspects of
computation for future work.

I

Let be an independence structure on U. For X; E U,
and W � U \{Xi} the set B � W is said to be an
/-Markov boundary of X; with respect to W if B is
minimal such that I({Xi}, B, W \B).

If I satisfies Trivial Independence then there is at least
one /-Markov boundary of X; with respect toW, and
if I satisfies Weak Union and Intersection then there
is at most one.

Proposition

6

589

to explore, as would its relationship with comparative
probability [Walley and Fine, 79]. T here may well also
be connections between GCPPs and the framework of
[Shenoy, 92] which uses a pro duct definition of inde­
pendence.
Acknowledgements

Thanks to Serafin Moral and Luis de Campos for some
useful discussions, and to Milan Studeny and the ref­
erees for their helpful comments. The author is sup­
ported by a SERC postdoctoral fellowship. I am also
grateful for the use of the computing facilities of the
school of Computing and Mathematical Sciences, Ox­
ford Brookes University.

443

I
I

Rules, Belief Functions
and Default Logic*

I

Nic Wilson
Department of Computer Science
Queen Mary and Westfield College
Mile End Rd., London El 4NS, UK

I

A b st ra ct

I

This paper describes a natural framework for rules ,
bas ed on belief functions , which includes a repre­
s entation of numerical rules, default rules and rules
allowing and rules not allowing contraposition. In
particular it jus tifies th e us e of th e Demps ter-Sh afer
Th eory for repres enting a particular class of rules ,
Belief calculated being a lower probability given cer­
tain independence assumptions on an underlying
s pace. It s h ows h ow a belief function framework can
be generalised to other logics, including a general
Monte-Car lo algorithm for calculating belief, and
h ow a version of Reiter's Default Logic can be s een
as a limiting case of a belief function formalism.

I
I
I
I

1.

Rules used by people are often not completely re­
liable so any attempt to represent them must cope
with the conclusion of the rule sometimes being in­
correct. Numerical approaches do this by giving
some kind of weighting to the conclusion of an un­
certain rule; non-monotonic reasoning, a symbolic
approach, ensures that these rules are defeasible, so
that their conclusions could later be retracted if nec­
essary.
There has been little work done, however, on relat­
ing numerical and symbolic techniques, an exception
being the work of Adams [Adams, 66] further devel­
oped by Geffner and Pearl [Geffner, 89; Pearl, 88]
where a. logic is produced from probability theory,
by tending the probabilities to 1.
This paper shows how a belief function approach can
represent numerical rules, both those allowing con­
traposition and those not allowing contra.position,
and how default rules may be viewed as the lim­
iting case of such rules, when the certainty of the
rule tends to 1. This allows the integration of the
Dempster-Shafer Theory (DST) [Shafer, 76] and Re­
iter's Default Logic [Reiter, 80], hence enhancing the
understanding of both.

I
I
I
I
I
I
I
I
I

Introduct ion

*

This research was carried out as part of
t he ESPRlT basic research action DRUMS
(3085)

Section 2 deals with the representation of numeri­
cal rules within DST, 2.1 giving an interpretation
of the type of rule that DST typically represents;
2.2 presents a belief function framework that allows
the theory to be generalised to other logics, and 2.3
shows how the framework can be applied to include
rules which don't allow contraposition. Section 3
deals with the representation of default rules within
the framework: 3.1 reformulates Reiter's Default
Logic and defines a modified extension (equivalent
to Lukaszewicz's); 3.2 shows how the belief function
framework can be turned into a logic and 3.3 shows
how to represent default rules within this logic. Sec­
tion 4 indicates how priorities between rules can be
represented, and Section 5 suggests how numerical
and default rules could be used together within the
framework.
2. Numerical Rule s

Expert Systems like MYCIN [Buchanan and Short­
liffe, 84) use uncertain rules of the form If a then c
: (a-), where a- is the some measure of how reliable
the rule is. There are many ways of interpreting such
a rule. We consider a natural interpretation which
leads to the standard Dempster-Shafer representa­
tion of rules.
2.1 J u stifyin g

DST Representation of Rules

The standard way of representing ·the rules If a;
then c; : (a-;) (for i= 1, . . . , m) with the Dempster­
Shafer Theory is, for each rule to produce a simple
support function with mass a-; allocated to the ma­
terial implication a; _. c; and the remaining mass
1 - o; allocated to the tautology, and then to com­
bine these simple support functions by repeated ap­
plication of Dempster's Rule. Pearl has criticised
this representation for its behaviour under chaining
and reasoning by cases. However it turns out that
this DST approach represents a very natural type of
rule.
The uncertain rule may in fact be an approximation
to the certain rule n/\a -+ c where n is an unknown
antecedent or one too complicated to be easily ex­
pressed by the expert but which they judge to be
true with probability o. After all '. . . uncertainty
measures characterise invisible facts, i.e., exceptions
not covered in the formulas' [Pearl, 88, p2].
Since n 1\ a -+ c is logically equivalent to n 1\ ...,c -+
...,a, such a rule allows contraposition, an d since it's
also logically equivalent to n _. (a -+ c) , this rule
may also be interpreted
In a proportion o o f worlds (or situations ) we know
th e material implication a -+ c is true.

444

If we represent such a rule by a simple support func­
tion, as described above, Belief is just the probability
that we know a -+ c to be true, so that it's a lower
probability for a -+ c. Similarly if we have a num­
ber of such rules, a; -+ c; (i = 1, ... , m) , represent
them as simple support functions and combine these
with Dempster's Rule, Belief is a lower probability,
given certain independence assumptions on the n;s.
Consider now the typical Reasoning by Cases situa­
tion: we're given two rules If a then c: {Ql) and If
-.a then c : (Q2) which we'll interpret as uncertain
material implications n1 1\ a -+ c and n2 1\ -.a -+ c
with Pr(n;) = Q;, i = 1, 2.
Pearl argues that any reasonable measure of belief
should obey the Sandwich Principle: deducing from
those two rules that belief in c should be between
Q1 and Q2; the Dempster-Shafer approach however
gives that Bel(c) = Ql Q2.
But it is clear why the Sandwich Principle is violated
for this approach: knowing either a or -.a increases
our knowledge and hence our belief. In worlds where
n1 1\ -.n2 is true, c may be always false if a is always
false; in the event -.n1/\ n2, c may be always false if
a is always true, and in the event -m1 1\ -.n2 there
is no constraint on c so c may again always be false.
Only in the event n1 1\ n2 can we be sure that c
is true, so making the assumption of independence
of n1 and n2 (which is reasonable without contrary
knowledge) we get Bel( c) , the probability that we're
in a world where we know c to be true, is Q1a2.
This type of rule can also be chained:
n1 1\ a -+ b and n2 1\ b -+ c with Pr(n;) = a;,
(i = 1, 2) leads to (n1 1\ n2) 1\ a -+ c, and again
assuming independence of n1 and n2 this gives
Pr(n 1 /\ n2) = a1a2. If we now learn that a is true we
get Pr(c) � a1 a2 and so Bel(c) = a1 a2, the result
given by application of Dempster's Rule.
Of course the assumption of independence of the n;s
will not always be valid-if correlations between the
rules are known they should be (and can be) incor­
porated.
The Dempster-Shafer approach is thus a natural,
formally justified as well as a computationally ef­
ficient way (see [Wilson, 89] and section 2.2) to rep­
resent If-Then rules.

logic of knowledge). A natural way to extend Demp­
ster's multi-valued mapping [Dempster, 67] is as fol­
lows:
We have a mutually exclusive and exhaustive set 0
with a probability function P on it, and we're inter­
ested in the truth of formulae in L, where L is the
language of some logic. With each 7] E n is associ­
ated a set K'1 (� L): the set of all formulae known
to be true given that '7 is true. For a formula dE L,
Bel(d) is defined to be the probability that we know
d to be true i.e.,
Bel(d) =

L

P(77)
'1"1:=>d

=

L

P(77).
!7=K�3d

Justifying Dempster's Rule for general belief func­
tions is problematic* so we restrict ourselves to the
combination of a finite number of simple support
functions and, to justify this, use the Sources of Ev­
idence framework (based on Shafer's random sources
canonical example [Shafer, 87]; see [Wilson, 89] for
details).
Suppose we have distinct propositions n;, i =
1, ... , m, (not in L) and for each we have a prior
probability Q;. Suppose also that we know that if n;
is true, some evidence Evd; is also true, where Evd;
is a statement about the logic (it might for example
be that the material implication a; -+ c; is true, as
in section 2.1). If n; is not true we know nothing
about the truth of Evd;.
We also allow there to be a set of facts W which are
known certainly to be true.
n; may, as the name of the framework suggests, rep­
resent the event that a source of evidence, which tells
us evidence Evd;, is reliable. Alternatively n; may
be an unknown antecedent of a rule, as described in
the last section; or ni may just be some event for
which, when it occurs, we are sure that the evidence
Evd; is true.
Let 'lu be the elementary event

1\ n; A 1\ -.n;

iEu

if"

and let n be the mutually exclusive and exhaus­
tive set of elementary events { T/u : u � {1, . . . m } } .
Take some probability function P on 0.

* For example there is the problem of the collaps­
ing of the Belief-Plausibility interval, e.g., [Pearl,
We will be interested in extending DST to other log­
89]; see also Shafer's presentations of his random
ics (see [Saffioti, 90] for other work on this, and see
codes canonical example, with discussion [Shafer,
[Ruspini, 87] for a justification of DST using a modal
82a, 82b].
2.2 The Sources of Evidence Framework

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

445

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

If we think ofP as saying, for each 1]11, the probability
that we are in a world in which f111 is true, then Bel(d)
is the probability that we are in a world in which we
know d is true.
Bel maybe viewed as a lower probability given the
probability function P on the underlying space n.
It is argued in [Wilson, 89] that, in the absence of
correlation information on the n;s, certain assump­
tions (A) and (B) are entirely reasonable. (A) is
roughly that, since an unreliable source/rule doesn't
give us any information, it shouldn't affect the prob­
abilities (an example of the application of this as­
sumption is given below in 2.3); (B) is that, if the
sources are not contradictory (i.e., /\';' n; is not
known to be impossible) then we take, for each i,
P(n;) to be a;, its prior value. These assumptions
determine a unique probability function P08 given
by
pDS(c:.,) =

where
and

{ 0, jk
p .,

k

p.,

'

if K., inconsistent,
otherwise,
p.,

=

I:
K.consistent

=IT a; ITC1- a; ) .
iEo

i�11

This is, in fact, the probability function that leads to
Dempster-Shafer belief when each Evd; is that some
proposition p; is true: the belief as defined above will
be the same as that calculated by using Dempster's
Rule to combine simple support functions with mass
a; attributed to the proposition p;, i = 1, ... , m.
Since Belief, as defined here, is just 'randomised
logic' the calculation of Belief inherits its compu­
tational efficiency from that of the underlying logic:
Bel( d) can be calculated, using the following Monte­
Carlo algorithm:
For each trial:-

(i) Pick u with probability P(q.,)
(ii) If K., 3 d then trial succeeds else trial fails.
The proportion of successful trials then converges to
Bel(d).
Given that P( '1u ) is not too hard to calculate, the
calculation takes time proportional to the time it
takes to check if d E Kt!, but with a fairly large
constant term corresponding to the number of trials
needed to get reasonable accuracy.
With the probability function P = P05, step (i) can
be performed very easily. Since any sensible measure

of belief should collapse to the logic for the extreme
case, its computational efficiency cannot hope to be
better than that of the underlying logic. Thus the
calculation of Dempster-Shafer belief is as fast, up to
a constant, as the calculation of a measure of belief
could possibly be. In particular it is shown in [Wil­
son, 89] that (up to arbitrary accuracy) Dempster­
Shafer Belief on a mutually exclusive and exhaustive
frame of discernment can be calculated in time ap­
proximately linear in the number of evidences and
size of the frame of discernment.
2.3 Rules Not Allowing Contraposition

Some rules do not allow contraposition. For exam­
ple the rule Typically males don't have long beards
seems reasonable, and even mildly informative, but
on meeting someone with a long beard, it would be
unreasonable to deduce that they were female. In
order to represent rules not allowing contraposition,
inference rules such as a / c will be used which, like
the rules used in many Expert Systems, given a, al­
low the deduction of c, but given -.c, do not allow
-.a to be deduced.
Suppose we have a set of rules If a; then c; :
(a;) for i = 1, . . . , m, (a; s and c;s closed wffs in
first order logic) for which we do not wish to allow
contraposition. Let I = { a; / c; : i = 1, ... , m}
where the (certain) inference rule a; I c; means 'if
we know a; we can deduce c/, and let I., = {a; I c; :
ieu}.

For some set U of closed wffs and set of inference
rules J we define Th1 (U) to be the logical closure of
U when all the inference rules in J are added to the
logic i.e., the set of formulae obtained by applying
all the inference rules in J repeatedly to U, so that
Th1 (U) is the smallest set r such that
(i) r 2 u,
(ii) Th(r) = r and
(iii) if a I c E J and a E r then c E r'

where Th(r) means the logical closure of r within
first order logic.

Abbreviate Th1.. (W) to Th17(W).
To represent this set of rules within the sources of
evidence framework we make the ith evidence be
that the inference rule ai I c; is added to the logic
(so that whenever a; is known, c; may be deduced).
To be precise, we set K., to be Tht! (W).
This includes the uncertain material implications,
described in 2.1, as a special case: make, for all i,
the ith inference rule equal T I (a; - c; ) .

44·6

I

Example

3. Default Rules

Whilst attempting to deduce information about our
acquaintance Nixon we learn that he is a quaker and
a republican, so that W = {quaker,republican}.
Two rules, If quaker then pacifist : Ca-1) and
(a-2) are also
If republican then -,pacifist :
known.
To represent these we take Evd1 to be that the first
rule is correct and that the corresponding inference
rule quaker /pacifist should be added to the logic,
and similarly for Evd2.
Thus if n1 then, if at any time we learn quaker, we
will deduce pacifist. This gives

An alternative to rules with numerical uncertainty
are default rules-in the absence of information in­
dicating that the circumstances are exceptional, the
rule is fired, though the consequence of the rule may
later have to be retracted, if it's discovered that cir­
cumstances are in fact exceptional.

Ke = Th(W) = Th( {quaker, republican})
K{l} = Th(W U {pacifist})
K{2} = Th(W u {-,pacifist})
K{l,2} = Th(W U {pacifist, ...,pacifist}).

Since K{1,2} is inconsistent, P(nt /\n2) must be 0. In
order to come up with a probability function P we
make certain independence assumptions. Knowing
only about one rule, the first, we would obviously
take P(n1) = a-1; adding an unreliable second rule
doesn't give us any information so shouldn't change
this probability i.e., we make the assumption that
P(n1l..,n2) = O't. Symmetrically we make the as­
sumption P(n2j-.nt) = a-2. Both these assumptions
are instances of assumption (A) mentioned above in
2.2. Only in worlds when n1 1\ -.n2 is true (when
u = {1}) do we know pacifist, and only in worlds
-.nl 1\ n2 ( u = {2}) do we know -.pacifist, so

3.1 Default Logic

Reiter's Default Logic [Reiter, 80) is a logic for rea­
soning with default rules. A default rule is a rule of
the form 'If we know a then deduce c, as long as b
is consistent', or a : b I c for short.
Let � = (D, W) be a closed default theory where L
is the language of a first order logic, W � L, a set of
closed wffs, are the facts and D is the set of default
rules
a; : b; .
;
: '= 1, . .. ,m
-c--

{

}

where a;, b; and c; are closed formulae, for each i.
It turns out that Reiter's default logic can be ex­
pressed in terms of inference rules. Let I = {a; I c; :
i = 1, . . . , m}. The behaviour of the defaults in D
will be mimicked by use of the corresponding infer­
ence rule in I.
Let S = {Th"Y(W): -y � {l, ... m}}. S contains all
the sets of formulae produced by applying different
subsets of the inference rules to W.
For some K E S an inference rule a; I c; may have
been applied even though b; is inconsistent (i.e.,
-.b; e K), in which case the inference rule was not
behaving like the corresponding Default rule. Then
we say that K is A-inconsistent. Formally this prop­
erty can be defined as follows:

I
I
I
I
I
I
I
I
I
I

K E S is �-consistent if and only if there exists a
� {1, ... m} with K = Th"Y(W) and K � -.b; for
all i e 1'·

I

In default logic the extensions are intended to be
the different possible completions, using the default
rules, of an incomplete set of facts about the world.

I

-y

If the reliabilities of the two rules are the same then
Bel(pac ifist) = Be l(-.pacif iat) . If, on the other
hand, the first rule is very reliable, but the second
isn't so reliable then Bel(pacifist) will be close to
1 and Bel(-.pacifist) will be close to 0.

I

Theorem 1:

Th"Y(W) where

E is an extension if and only if E
i = { i : -.b; fl. E }.

=

This shows that extensions are �-consistent sets in

S. In fact we have

If E is an extension of A then E is a
maximal �-consistent set in S.

Theorem 2:

E is said to be

M-extension of
if E is a maximal .6.-consistent set inS.
Definition:

an

.0.

I
I
I
I
I

447

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

M-extensions are formed by applying 8S many in­
ference rules as possible without contradicting �­
consistency. Theorem 2 showed that extensions are
always M-extensions.

to infinity, and given £ > 0 there exists N, such that
for all z > N, and for all p E L
Bel(g:(z),p)
Bel(g:(z),p)

Let � be a closed normal default
theory. Then E is an extension of Ll if and only if
E is a maximally �-consistent set in S.

Theorem 3:

Thus for closed normal default theories E is an ex­
tension if and only if E is an M-extension.
M-extensions have for general closed default theories
the nice properties extensions only have for closed
normal default theories:
Theorem 4:
M-extension.

Every closed default theory has an

Let Ll =
(D, W), �' = (D', W) be closed default theories
with D � D'. If E is an M-extension of � then
there exists an M-extension E' of�' with E' ;;2 E.

Theorem 5 (Semi-monotonicity):

We ca.n also define

an M-default proof, in a.n obvious
way, which is complete, that is for any dosed wff p
there is an M-default proof of p if and only if p E E
for some M-extension E.
It might be suggested that any M-extension of a de­
fault theory which is not an extension is not a sen­
sible completion of one's knowledge: this however
is not the case e.g., there are apparently coherent
default theories that allow no extension (see [Wil­
son, 90] for an example, and also for proofs of the
above results) but which, by Theorem 4, allow M­
extensions.
M-extensions turn out to be the modified extensions
defined in [Lukaszewicz, 84] (also see [Besnard, 89]).

3.2 The Sources of Evidence Framework
as

a Logic

To turn the Sources of Evidence Framework into a
Logic, we tend the reliabilities of the sources (the
a is) to 1. B-extensions are the sets of formulae
whose belief can be made to tend to 1. We can
consider Bel as a function Bel(g:,p) where .9: =
(at. a2, ... , am) is the vector consisting of the re­
liabilities of all the sources.
To use the Sources of Evidence framework to pro­
duce a logic we require that for any closed wff p,
Bel(p) tends to either 0 or 1. A B-extension is then
the set of formulae whose belief tends to 1.
Formally E is a B-extension if and only if
for i = 1, .. . , m there exist monotonic functions ai :
[1, oo) - [0,1) with ai ( z) tending to 1 as z tends

>

1- £ if pEE
if p �E.

< £

Example Co ntinued

In the case of Nixon we have 2 B-extensions. When
the reliability of the first rule tends to 1 much
faster than that of the second rule we get that
Bel(pacit ist) tends to 1, and Bel(-.paci:f ist)
tends to 0 so K{l} = Th(W U {pacifist}) is a B­
extension.
Similarly K{2} = Th(W U {-.pacifist}) is a B­
extension.
Theorem

for some

u.

6:

If E is a B-extension then E

=

Ku

If we don't have information about correlations be­
tween the sources we can reasonably make assump­
tions (A) and (B) giving p = p08 .
Theorem 7: With P = P08 , E is a B-extension
if and only if E = K17 for some u maximal with K17
consistent.
3.3 Representation of Default Rules in
Sources of Evidence Framework

Default rules will be represented in the Sources of
Evidence framework by treating them rather like nu­
merical rules with a high, but unknown, certainty:
roughly speaking we make the ith evidence Evd; be
that the inference rule ai I Ci is a correct rule, as we
did in 2.3, and take the limit as the reliabilities of
the sources (that is, the certainties of the rules) tend
to 1, to produce the B-extensions.
In the example we found that the B-extensions were
just the same as Reiter's extensions. This was no co­
incidence: when the probability function pDS on 0 is
used the B-extensions are exactly the M-extensions
of the default theory.
3.3.1 Closed Normal Default Theories

Let� be a closed normal default theory. We want
the ith evidence to be that the inference rule ai I Ci
17
is a correct rule, so, formally, we set Kt? = Th (W),
08
and also set P = P .
Theorem 8: Let� be a closed normal default the­

ory. With the above representation of Closed Nor­
mal Default rules within the Sources of Evidence
framework

448

E is a B-extension <=> E is an M-extension of .6.
is an extension of .6..

<=> E

condition

b;

P05. To represent the consistency

=

we have to be a little trickier. We first

add new distinct symbols q1, ... , qm to the alphabet
of the language to get a new language

L'.

We want the statement of the ith source to be that

inferences rules a; / q;, q; / c;, -.b; /-.q; are correct
rules. The idea is that knowing a; will enable us to
deduce c; unless

known, in which case we will

-.b; is

get an inconsistency since we'll know both q; and
-.q;.

To be precise we let

be the theorems of

K� � L'

when the inference rules

Ju

a;

q;

{-,

=

-

C;

q;

,

-.b;

-

:

-.q;

.
aE

W

K�

n

L,

so that

Ku

mentioning some q;.

is

u}

9:

rule), and

P(n2l....,n1)

sumption to make.
This gives P(n1

=

02

=

0,

A n2)

tent, P(n1 A -.n2)

o1 (since P(n1)

is still an intuitive

since

K{l,2}

01, P(-.nl A n2)

as­

is inconsis­

(1 - ol)a2,
P(-.nlA-.n2) = (1-ol)(l-o2), and Bel( -.:flies) =
o1, Bel(:fli es ) = (1- al)o2.
Here Th( {penguin, bird, -.flies}) is the only B­
=

=

extension.

5. Combining Numerical and Default Rules
It has been shown how the Sources of Evidence

to

1, default

rules. The next step is to combine both

within this framework.

Suppose that our knowledge includes both default

K� �rTh'"' (W).
wffs in L so let Ku

be

rules and numerical rules. F irst we represent both

as evidences, which add an inference rule to the logic,

in the sources of evidence framework (which includes
the contrapositioning rules as a special case).

This gives the following result:

Theorem

=

framework can represent either numerical rules, or,

stripped of all formulae

K�

In this case we specify P(nl)

reflect that pref­

taking the limit as the reliabilities of the rules tend

are added to the logic, that is,

We're only interested in the

P that

should be unaffected by the addition of a second

3.3.2 General Closed Default Theories
We again set P

on the probability function
erence.

I

W ith the representation of Default

rules in the Sources of Evidence framework described
above, for any set of closed wffs E

We

first consider only the default rules, and produce the

B-extensions. Then we add the other rules/sources
to get a belief function in each B-extension.

E is a B-extension if and only if E is an M-extension.

BEL (d) is defined to be the minimum
.
value of Bel(d) over the extensions, a rather conser­

4. Expressing Preferences between Rules

mum value of Bel(d). If BEL.(d) is high this gives

Suppose we have two rules,

-.:flies :

(al),

and

Co2), and two facts,

If penguin then

If bird then :flie1

:

W ={penguin, bird}.

Expressing these as inference rules gives, as in the
Nixon example,

For dE

L,

vative measure; BEL.(d) is defined to be the maxi­

at least some reason for believing d: there is some
combination of default rules which if correct lend

high support to d.

extensions could also be a useful measure.

Another way of looking at this is to consider Bel

as a function of the unknown, but high reliabilities

g_

BEL (d) is then info-1Bel(g_,p),
- .
and BEL.(d) is sup _1Bel(g_,p).
g_

= (alJ ..., am)·

6.

ansmg

from

Some average of Bel over the

assumptions

P(n21-.nl) = a2.

P(n1l....,n2)

=

o1,

But since we know that penguins are a subclass of

Concluding Comments

We have given counter-arguments to some of Pearl's
criticisms of the use of belief functions to represent

rules and argued that the Dempster-Shafer Theory

is a natural way to represent a type of If-Then rule.

birds it seems that the first rule should override the

If it is known that the rules are correlated, then the

get Bel(-.:flies)

which should not be changed

and so a more general belief function approach such

The preference of some sets of rules over others are

allow the dependencies between the rules to be in­

second: if we only knew about the first rule we would

= 01

on learning the second rule.

represented by making some different assumptions

independence assumptions may well not be justified,

as the sources of evidence framework is needed to

corporated in the underlying probability function P.

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

449

I
I
I
I
I
I
I
I
I
I

Dependencies must also be used, as described in sec­
tion 4, to represent dominance of certain rules (or
chains of rules) over others. We have shown that
this belief function approach also enables the repre­
sentation of default rules.
Another very natural type of rule H a then c :
( a) related to that described in 2.1 is where, again,
there is an unknown antecedent n with n 1\ a - c,
but instead of knowing the prior probability of n,
we know the conditional probability P ( n l a) = a.
With a number of such rules we can take, as before,
Belief as a lower probability, tend the a;s to 1 and
see which Beliefs tend to 1. This is effectively the
approach taken by Adams, Geffner and Pearl. It
would be interesting to explore whether progress can
be made by making independence assumptions on
the n; s, as we did for the type of rule described in
2.1.

It is clear that there is no single correct way of rep­
resenting numerical If-Then rules. Future research
in this area should attempt to clarify wha.t different
types of numerical rule there are, a.nd to represent
them within a single framework.

I
I
I
I
I
I
I

Pearl, Judea, 89, Reasoning with Belief Functions: An
Analysis of Compatibility, Technical Report R-136,
Computer Science Department, UCLA, Los Angeles,
CA. 90024-1596, November 1989.

e i

Ruspini, E. H., 87, Epist m c Logics, Probability and
the Calculus of Evidence,

Conf. on AI

Proc., lOth Inti. Joint

(IJCAI-87), Milan, 924-931.

Saffioti, A., 90, A Hybrid Framework for Representing
Uncertain Knowledge,

AI

Proc., 9th Nat/.

Con/ on

(AAAI-90), Boston, USA.

Shafer, G., 76,

A Mat hematical Theory of Evidence

(Princeton University Press, Princeton, NJ).

Shafer, G. 1982a, Belief Functions and Parametric Mod­
els (with discussion),

tical Society,

Journal of th e R oyal Statis­

series B, 44, No. 3, 322-352.

Shafer, G. 1982b, Lindley's paradox (with discussion),

Journal of the American Statistical Ass oc iation ,

Vol 7, No. 378, 325-351.

Shafer, G., 87, Probability Judgment in Artificial Intelli­
gence and Expert Systems,

Statistical

Science, Vol

2, No. 1, 3-44.

Acknowledgements

Reiter, R., 80, A Logic for Default Reasoning,

I am greatly indebted to Mike Clarke, for numerous use­
ful and interesting discussions, and without whom this
paper could not have been written. I have also enjoyed
many productive conversations with Mike Hopkins and
my colleagues on the DRUMS project.

I
I

Morgan Kaufmann Publishers Inc. 1988, Chapter 9,
in particular 455-457.

ed. J. Hintikka and

Default Logic,

Springer-Verlag,

Berlin, Heidelberg, r-;ew York.
Buchanan, B. G., and Shortliffe, E. H., 84.

expert systems.

Rule-based

Reading, Mass.: Addison Wesley.

Dempster, A. P., 67, Upper and Lower Probabilities In­

Ann. Math.
Statistics 38: 325-39.
Geffner, H., 89, Default Reasoning: Causal and Con­
ditional Theories, PhD thesis, Computer Science
duced by a Multi-valued Mapping.

Department, UCLA, Los Angeles, CA, November
1989.
Lukacewicz,
Proc.

84,

Considerations on Default Logic,

Non Monotonic Reasoning Workshop, New

also, 1988, Computational
Intelligence -1, pp1-16.
Pearl, Judea, 88, Probabilistic Reasoning in Intelli­
gent Systems: Networks of Plausible Inference,
Paltz NY, pp165-193;

Research Report no. 15, June 1989, Dept. of Com­
puting and Mathematical Sciences, Oxford Polytech­

Logic, Research Report, Dept.

P. Suppes. Amsterdam: North Holland.
Besnard, Philippe, 89,

and Generalisation of the Dempster-Shafer Theory,

W ilson, Nic, 90, Rules, Belief Functions and Default

Adams, E., 66, Probability and the logic of conditionals.

Aspects of inductive logic,

13 {1, 2), pp81-132.

Wilson, Nic, 89, Justification, Computational Efficiency

nic.



In this paper, we develop a qualitative theory of
influence diagrams that can be used to model
and solve sequential decision making tasks when
only qualitative (or imprecise) information is
available. Our approach is based on an orderof-magnitude approximation of both probabilities and utilities and allows for specifying partially ordered preferences via sets of utility values. We also propose a dedicated variable elimination algorithm that can be applied for solving
order-of-magnitude influence diagrams.

1

INTRODUCTION

Influence diagrams have been widely used for the past three
decades as a graphical model to formulate and solve decision problems under uncertainty. The standard formulation
of an influence diagram consists of two types of information: qualitative information that defines the structure of
the problem eg, the set of (discrete) chance variables describing the set of possible world configurations, the set of
available decisions, as well as the dependencies between
the variables, and quantitative information (also known as
the parametric structure) that, together with the qualitative
information, defines the model. The parametric structure
is composed of the conditional probability distributions as
well as the utility functions describing the decision maker’s
preferences. In general, the solution to an influence diagram depends on both types of information. Quite often,
however, we may have precise knowledge of the qualitative
information but only very rough (or imprecise) estimates
of the quantitative parameters. In such cases, the standard
solution techniques cannot be applied directly, unless the
missing information is accounted for.
In this paper, we propose a qualitative theory for influence
∗
This work was supported in part by the Science Foundation
Ireland under grant no. 08/PI/I1912

Nic Wilson
Cork Constraint Computation Centre
University College Cork, Ireland
n.wilson@4c.ucc.ie

diagrams in which such partially specified sequential decision problems can be modeled and solved. In particular, we
introduce the order-of-magnitude influence diagram model
that uses an order-of-magnitude representation of the probabilities and utilities. The model allows the decision maker
to specify partially ordered preferences via finite sets of
utility values. In this case, there will typically not be a
unique maximal value of the expected utility, but rather a
set of them. To compute this set and also the corresponding
decision policy we propose a dedicated variable elimination algorithm that performs efficient operations on sets of
utility values. Numerical experiments on selected classes
of influence diagrams show that as the quantitative information becomes more precise, the qualitative decision process becomes closer to the standard one.
The paper is organized as follows. Section 2 gives background on influence diagrams. In Section 3 we present
the order-of-magnitude calculus as a representation framework for imprecise probabilities and utilities. Sections 4
and 5 describe the main operations over sets of order-ofmagnitude values and introduce the order-of-magnitude influence diagram model. In Section 6 we present the results
of our empirical evaluation. Section 7 overviews related
work, while Section 8 provides concluding remarks.

2

INFLUENCE DIAGRAMS

An influence diagram is defined by a tuple hX, D, U, Gi,
where X = {X1 , . . . , Xn } is a set of oval-shaped nodes
labeled by the chance variables which specify the uncertain decision environment, D = {D1 , . . . , Dm } is a set of
rectangle-shaped nodes labeled by the decision variables
which specify the possible decisions to be made in the domain, U = {U1 , . . . , Ur } are diamond-shaped nodes labeled by the utility functions which represent the preferences of the decision maker, and G is a directed acyclic
graph containing all the nodes X ∪ D ∪ U. As in belief
networks, each chance variable Xi ∈ X is associated with
a conditional probability table (CPT) Pi = P (Xi |pa(Xi )),
where pa(Xi ) ⊆ X ∪ D \ {Xi } are the parents of Xi in

The wildcatter could do a seismic test that will help determine the geological structure of the site. The test results can
show a closed reflection pattern (indication of significant
oil), an open pattern (indication of some oil), or a diffuse
pattern (almost no hope of oil). The probabilistic knowledge consists of the CPTs P (O) and P (S|O, T ), while the
utility function is the sum of U1 (T ) and U2 (D, O). The optimal policy is to perform the seismic test and to drill only
if the test results show an open or a closed pattern. The
maximum expected utility of this policy is 42.75.

Figure 1: The oil wildcatter influence diagram.
G. Similarly, each decision variable Dk ∈ D has a parent
set pa(Dk ) ⊆ X ∪ D \ {Dk } in G, denoting the variables
whose values will be known at the time of the decision and
may affect directly the decision. Non-forgetting is typically
assumed for an influence diagram, meaning that a decision
node and its parents are parents to all subsequent decisions.
Finally, each utility node Uj ∈ U is associated with a utility function that depends only on the parents pa(Uj ) of Uj .
The decision variables in an influence diagram are typically
assumed to be temporally ordered. Let D1 , D2 , ..., Dm be
the order in which the decisions are to be made. The chance
variables can be partitioned into a collection of disjoint sets
I0 , I1 , . . . , Im . For each k, where 0 < k < m, Ik is the
set of chance variables that are observed between Dk and
Dk+1 . I0 is the set of initial evidence variables that are
observed before the first D1 . Im is the set of chance variables left unobserved when the last decision Dm is made.
This induces a partial order ≺ over X ∪ D, as follows:
I0 ≺ D1 ≺ I1 ≺ · · · ≺ Dm ≺ Im [5].
A decision policy (or strategy) for an influence diagram is
a list of decision rules ∆ = (δ1 , . . . , δm ) consisting of one
rule for each decision variable. A decision rule for the decision Dk ∈ D is a mapping δk : Ωpa(Dk ) → ΩDk , where
for a set S ⊆ X ∪ D, ΩS is the Cartesian product of the
individual domains of the variables in S. Solving an influence diagram is to find the optimal decision policy that
maximizes the expected utility. The maximum expected
utility (MEU) is equal to:


r
n
X
X
X
X Y
 Pi ×
max · · ·
Uj  (1)
max
I0

D1

Im−1

Dm

Im

i=1

j=1

Example 1 For illustration, consider the influence diagram displayed in Figure 1 which is based on the classic
oil wildcatter decision problem [9]. An oil wildcatter must
decide either to drill or not to drill for oil at a specific site.

Variable Elimination Several exact methods have been
proposed over the past decades for solving influence diagrams using local computations [10, 13, 11, 5, 3, 8]. These
methods adapted classical variable elimination techniques,
which compute a type of marginalization over a combination of local functions, in order to handle the multiple types
ofPinformation (probabilities and utilities), marginalization
( and max) and combination (× for probabilities, + for
utilities)
Pinvolved in influence diagrams. Since the alternation of and max in Eq. 1 does not commute in general, it
prevents the solution technique from eliminating variables
in any ordering. Therefore, the computation dictated by
Eq. 1 must be performed along a legal elimination ordering that respects ≺, namely the reverse of the elimination
ordering is some extension of ≺ to a total order [5, 3].

3

FOUNDATIONS

Our approach towards a qualitative theory for influence
diagrams is based on the qualitative decision theory proposed by Wilson [14]. Wilson’s theory defines a set of abstract quantities called extended reals, denoted by R∗ , that
are used to represent qualitative probabilities and utilities.
Each extended real is a rational function p/q where p and
q are polynomials in ǫ with coefficients in the rationals,
where ǫ is a very small but unknown quantity so that the
extended reals can be used to represent information up to ǫ
precision. For example, quantities such as 1−ǫ and ǫ might
be used for qualitative probabilities likely and unlikely respectively, and ǫ−1 for a high utility. These quantities can
then be combined using standard arithmetic operations between polynomials for computing expected qualitative utilities. The resulting utilities are then compared among each
other by means of a total order on R∗ that is defined in [14].
3.1

ORDER-OF-MAGNITUDE CALCULUS

Rather than using extended reals explicitly, we adopt a simpler calculus that allows us to reason about the “order of
magnitude” of the extended reals [14]. We start with the
definition of an order-of-magnitude value that represents a
qualitative probability or utility value.
D EFINITION 1 An order-of-magnitude value is a pair

hσ, ni, where σ ∈ {+, −, ±} is called the sign and n ∈ Z
is called the order of magnitude, respectively.
Intuitively, for each integer n we have an element h+, ni
meaning “of order ǫn ”, and an element h−, ni meaning “of
order −ǫn ”. Moreover, if we add something of order ǫn
to something of order −ǫn then the result can be of order
±ǫm , for any m ≥ n. To ensure closure of the calculus
under addition, we therefore add the element h±, ni representing this set of possibilities. In the following, we also
define O = {hσ, ni | n ∈ Z, σ ∈ {+, −, ±}} ∪ {h±, ∞i},
O± = {h±, ni | n ∈ Z ∪ {∞}} and O+ = {h+, ni | n ∈
Z ∪ {∞}}. The element h±, ∞i will sometimes be written
as 0, element h+, 0i as 1, and element h−, 0i as -1.
Standard arithmetic operations such as multiplication (×)
and addition (+) follow from the semantics of the order-ofmagnitude values [14] and are defined next.
D EFINITION 2 (multiplication) Let a, b ∈ O be such that
a = hσ, mi and b = hτ, ni. We define a × b = hσ ⊗
τ, m + ni, where ∞ + n = n + ∞ = ∞ for n ∈ Z ∪ {∞}
and ⊗ is the natural multiplication of signs, namely it is the
commutative operation on {+, −, ±} such that +⊗− = −,
+ ⊗ + = − ⊗ − = +, and ∀σ ∈ {+, −, ±}, σ ⊗ ± = ±.
This multiplication is associative and commutative, and
∀a ∈ O, a × 0 = 0 and a × 1 = a, respectively. Furthermore, for b ∈ O \ O± , we define b−1 to be the multiplicative inverse of b, namely hσ, mi−1 = hσ, −mi for
σ ∈ {+, −}. Given a ∈ O, we define a/b = a × b−1 .
D EFINITION 3 (addition) Let a, b ∈ O be such that a =
hσ, mi and b = hτ, ni. We define a + b to be: (1) hσ, mi
if m < n; (2) hτ, ni if m > n; (3) hσ ⊕ τ, mi if m = n,
where + ⊕ + = +, − ⊕ − = −, and otherwise, σ ⊕ τ = ±.
Addition is associative and commutative, and a + 0 = a,
∀a ∈ O. For a, b ∈ O, let −b = −1 × b and a − b =
a+(−b). Clearly, we can write −hσ, mi = h−σ, mi, where
−(+) = −, −(−) = + and −(±) = ±. We also have the
distributivity: ∀a, b, c ∈ O, (a + b) × c = a × c + b × c.
3.2

ORDERING ON SETS OF
ORDER-OF-MAGNITUDE VALUES

We will use the following ordering over the elements of O,
which is slightly stronger than that defined in [14].
D EFINITION 4 (ordering) Let a, b ∈ O be such that a =
hσ, mi and b = hτ, ni. We define the binary relation < on
O by a < b if and only if either: (1) σ = + and τ = +
and m ≤ n; or (2) σ = + and τ = ± and m ≤ n; or (3)
σ = + and τ = −; or (4) σ = ± and τ = − and m ≥ n;
or (5) σ = − and τ = − and m ≥ n.
Given a, b ∈ O, if a < b then we say that a dominates b.
For A, B ⊆ O, we say that A < B if every element of

B is dominated by some element of A (so that A contains
as least as large elements as B), namely if for all b ∈ B
there exists a ∈ A with a < b. As usual, we write a ≻ b
if and only if a < b and it is not the case that b < a. It is
easy to see that < is a partial order on O and the following
monotonicity property holds:
P ROPOSITION 1 Let a, b, c ∈ O. If a < b then a + c <
b + c, and if a < b and c ∈ O+ then a × c < b × c.
Any finite set of order-of-magnitude values can therefore
be represented by its maximal elements with respect to <.
D EFINITION 5 (maximal set) Given a finite set A ⊆ O,
we define the maximal set of A, denoted by max< (A), to be
the set consisting of the undominated elements in A, namely
max< (A) = {a ∈ A | ∄b ∈ A such that b ≻ a}.

4

OPERATIONS ON SETS OF
ORDER-OF-MAGNITUDE VALUES

We introduce now the main operations that can be performed over partially ordered finite sets of order-ofmagnitude values. In particular, we extend the addition (+)
and multiplication (×) operations from singleton to sets of
order-of-magnitude values as well as define a maximization
operation over such sets.
4.1

ADDITION, MULTIPLICATION AND
MAXIMIZATION

Given two finite sets A, B ⊆ O and q ∈ O+ , we define
the summation and multiplication operations as A + B =
{a + b | a ∈ A, b ∈ B} and q × A = {q × a | a ∈
A}, respectively. The maximization operation is defined
by max(A, B) = max< (A ∪ B).
In order to use the order-of-magnitude calculus to define
a qualitative version of influence diagrams we need to be
sure that each of +, × and max is commutative and associative, and also to give sufficient conditions such that the
following distributivity properties hold:
∀q, q1 , q2 ∈ O+ and ∀A, B, C ⊆ O
D1 q × (A + B) = (q × A) + (q × B)
D2 (q1 + q2 ) × A = (q1 × A) + (q2 × A)
D3 max(A, B) + C = max(A, C) + max(B, C)
It is easy to see that +, × and max are commutative and
associative, and the distributivity properties (D1) and (D3)
hold as well. Unfortunately, the distributivity property (D2)
does not always hold for sets of order-of-magnitude values.
To give a simple example, let q1 = h+, 2i, q2 = h+, 3i and
let A = {h±, 1i, h±, 4i}. Then, (q1 + q2 ) × A yields the set

{h±, 3i, h±, 6i}, whereas (q1 × A) + (q2 × A) is equal to
{h±, 3i, h±, 4i, h±, 6i}. This property does however hold
for convex sets, as we will show next.

We can show now that any finite subset of O is in fact ≡equivalent with a set of order-of-magnitude values containing one or two elements, namely:

4.2

T HEOREM 1 Let A be any finite subset of O. Then either
A ≡ {a} for some a ∈ O, or ∃ m, n ∈ Z with m < n and
σ ∈ {+, −, ±} such that A ≡ {h±, mi, hσ, ni}.

CONVEX SETS AND CONVEX CLOSURE

Based on Definition 5, every element of a finite set A ⊆
O is dominated by some maximal element in A. We can
therefore define an equivalence relation between finite sets
of order-of-magnitude values, as follows.
D EFINITION 6 (relation ≈) Given two finite sets A, B ⊆
O, we say that A is ≈-equivalent with B, denoted by A ≈
B, if and only if A < B and B < A.
Clearly, ≈ is an equivalence relation, namely it is reflexive,
symmetric and transitive. We then have that:
P ROPOSITION 2 Let A, B, C ⊆ O be finite sets and let
q ∈ O+ . The following properties hold: (1) A ≈ B if
and only if max< (A) = max< (B); (2) if A ≈ B then
A + C ≈ B + C and q × A ≈ q × B.
We introduce next the notions of convex sets and convex
closure of sets of order-of-magnitude values.
D EFINITION 7 A set A ⊆ O is said to be convex if
∀q1 , q2 ∈ O+ with q1 + q2 = 1, and ∀a, b ∈ A, we have
that (q1 × a) + (q2 × b) ∈ A. The convex closure C(A)
of a set AP⊆ O is defined to consist of every element of
k
the form i=1 (qi × ai ), where k is an arbitrary natural
Pk
number, each ai ∈ A, each qi ∈ O+ and i=1 qi = 1.
Consider two elements hσ, mi and hτ, ni in O, where we
can assume without loss of generality that m ≤ n. Any
convex combination of these two elements is of the form
hθ, li where l ∈ [m, n] and if l < n then θ = σ; if l = n
then θ = σ ⊕τ or θ = τ . This implies that the convex combination of a finite number of non-zero elements is finite
(since every element a in the convex combination has its
order restricted to be within a finite range), and so, in particular can be represented by its maximal set. In fact, this
property holds even if we allow the zero element h±, ∞i.
We can define now the following equivalence relation:
D EFINITION 8 (relation ≡) Given the finite sets A, B ⊆
O, we say that A is ≡-equivalent with B, denoted by A ≡
B, if and only if C(A) ≈ C(B).
Therefore, two sets of order-of-magnitude values are considered equivalent if, for every convex combination of elements of one, there is a convex combination of elements of
the other which is at least as good.
P ROPOSITION 3 Let A, B, C ⊆ O be finite sets and let
q ∈ O+ . The following properties hold: (1) A ≡ B if and
only if max< (C(A)) = max< (C(B)); (2) if A ≡ B then
A + C ≡ B + C, q × A ≡ q × B, and A ∪ C ≡ B ∪ C.

4.3

OPERATIONS ON EQUIVALENT SETS OF
ORDER-OF-MAGNITUDE VALUES

Theorem 1 allows us to efficiently perform the required
operations (ie, summation, multiplication and maximization) on sets of order-of-magnitude values. We assume that
the subsets O are either singleton sets or are of the form
{h±, mi, hσ, ni}, where m < n. We need to ensure that
the outputs are of this form as well. For a given a ∈ O, we
use the notation σ(a) and â to denote the sign and the order
of magnitude of a, respectively,
Multiplication Given A ⊆ O of the required form, and
q ∈ O+ , we need to generate a set A′ that is ≡-equivalent
with q × A. Write q as h+, li. If A = {hσ, mi} then q × A
is just equal to the singleton set {hσ, l + mi}. Otherwise, A
is of the form {h±, mi, hσ, ni}, where m < n. Then q × A
equals {h±, l + mi, hσ, l + ni}, which is of the required
form, since l + m < l + n.
Maximization Given the sets A1 , A2 , . . . , Ak ⊆ O, each
of them having the required form, we want to compute
a set A′ that is ≡-equivalent to max(A1 , . . . , Ak ). Let
A = A1 ∪ · · · ∪ Ak and, for σ ∈ {+, −, ±}, we define
mσ and nσ as follows: if there exists no element a ∈ A
with σ(a) = σ then we say that mσ and nσ are both undefined; otherwise we have that mσ = min{l : hσ, li ∈ A}
and nσ = max{l : hσ, li ∈ A}, respectively. The set A′ is
computed as follows: (1) if m+ and m± are both undefined
(there are only negative elements) then A′ = {h−, n− i};
(2) if m+ is defined and either m+ ≤ m± or m± is undefined then A′ = {h+, m+ i}; (3) if m+ > m± (and both
are defined) then A′ = {h±, m± i, h+, m+ i}; (4) if m+
is undefined (no positive elements) and either n± ≥ n−
or n− is undefined then A′ = {h±, m± i, h±, n± i} ; and
(5) if m+ is undefined (there are no positive elements) and
n± < n− then A′ = {h±, m± i, h−, n− i}.
Summation Given the sets A1 , A2 , . . . , Ak ⊆ O of required form as before, we want to compute a set A′ that
is ≡-equivalent to (A1 + · · · + Ak ). We can write Ai as
{ai , bi } where if ai 6= bi then σ(ai ) = ± and âi < b̂i .
Then, (A1 + · · · + Ak ) ≡ {a, b} where a = a1 + · · · + ak
and b = b1 + · · · + bk . We can write b more explicitly
as hσ(b), b̂i where b̂ = min(b̂1 , . . . , b̂k ), and σ(b) = + if
and only if all bi with minimum b̂i have σ(bi ) = +; else
σ(b) = − if all bi with minimum b̂i have σ(bi ) = −; else
σ(b) = ±. Similarly for a. If σ(a) 6= ± then {a, b} reduces
to a singleton because a = b.

Example 2 Consider the sets A1 = {h±, 3i, h±, 4i} and
A2 = {h±, 3i, h±, 6i}. To generate A′ ≡ max(A1 , A2 ),
we first compute m± = 3 and n± = 6, and then we have
that A′ = {h±, 3i, h±, 6i} which corresponds to the extreme points of the input sets. Similarly, we can compute
the set A′′ ≡ (A1 + A2 ) as {h±, 3i, h±, 4i}.
4.4

DISTRIBUTIVITY PROPERTIES REVISITED

In summary, we can show now that all three distributivity
properties hold with respect to the ≡-equivalence relation
between finite sets of order-of-magnitude values.
T HEOREM 2 ∀q, q1 , q2 ∈ O+ and ∀A, B, C ⊆ O finite
sets we have that: (D1) q × (A + B) ≡ (q × A) + (q × B);
and (D2) (q1 + q2 ) × A ≡ (q1 × A) + (q2 × A); and (D3)
max(A, B) + C ≡ max(A, C) + max(B, C).

5

ORDER-OF-MAGNITUDE INFLUENCE
DIAGRAMS

In this section, we introduce a new qualitative version of the
influence diagram model based on an order-of-magnitude
representation of the probabilities and utilities.
5.1

Table 1: Optimal policies sets for order-of-magnitude influence diagrams corresponding to the oil wildcatter problem.
decision rule
Test?
Drill?

S=closed, T=yes
S=open, T=yes
S=diffuse, T=yes
S=closed, T=no
S=open, T=no
S=diffuse, T=no
order-of-magnitude MEU

ǫ = 0.1
{yes,no}
yes
yes
no
yes
yes
yes
h+, −1i

OOM-ID
ǫ = 0.01
ǫ = 0.001
{yes, no}
{yes, no}
yes
{yes, no}
yes
{yes, no}
{yes, no}
{yes, no}
yes
{yes, no}
yes
{yes, no}
yes
{yes, no}
h+, 0i
{h±, 0i, h+, ∞i}

THE QUALITATIVE DECISION MODEL

An order-of-magnitude influence diagram (OOM-ID) is a
qualitative counterpart of the standard influence diagram
graphical model. The graphical structure of an OOM-ID
is identical to that of a standard ID, namely it is a directed
acyclic graph containing chance nodes (circles) for the random discrete variables X, decision nodes (rectangles) for
the decision variables D, and utility nodes (diamonds) for
the local utility functions U of the decision maker. The
directed arcs in the OOM-ID represent the same dependencies between the variables as in the standard model. Each
chance node Xi ∈ X is associated with a conditional probability distribution Pio that maps every configuration of its
scope to a positive order-of-magnitude probability value,
namely Pio : ΩXi ∪pa(Xi ) → O+ . The utility functions
Ujo ∈ U represent partially ordered preferences which
are expressed by finite sets of order-of-magnitude values,
namely Ujo : ΩQj → 2O , where Qj is the scope of Uj .
Solving an order-of-magnitude influence diagram is to find
the optimal policy ∆ = (δ1 , . . . , δmQ) that maximizes
Pr the
n
order-of-magnitude expected utility i=1 Pio × j=1 Ujo .
We define the optimal policies set of an order-of-magnitude
influence diagram to be the set of all policies having the
same maximum order-of-magnitude expected utility.
5.2

Figure 2: Order-of-magnitude probability and utility functions corresponding to the oil wildcatter influence diagram.

AN EXAMPLE

Figure 2 displays the order-of-magnitude probability and
utility functions of an OOM-ID corresponding to the oil

wildcatter decision problem from Example 1. For our
purpose, we used an extension of Spohn’s mapping from
the original probability distributions and utility functions
to their corresponding order-of-magnitude approximation
[12, 2]. Specifically, given a small positive ǫ < 1, the
order-of-magnitude approximation of a probability value
p ∈ (0, 1] is h+, ki such that k ∈ Z and ǫk+1 < p ≤ ǫk ,
while the order-of-magnitude approximation of a positive
utility value u > 0 is h+, −ki such that ǫ−k ≤ u <
ǫ−(k+1) (the case of negative utilities is symmetric). For
example, if we consider ǫ = 0.1 then the probability
P (S = closed|O = dry, T = yes) = 0.01 is mapped to
h+, 2i, while the utilities U2 (O = dry, D = yes) = −70
and U2 (O = soaking, D = yes) = 200 are mapped to
h−, −1i and h+, −2i, respectively.
Table 1 shows the optimal policies sets (including the maximum order-of-magnitude expected utility) obtained for
the order-of-magnitude influence diagrams corresponding
to ǫ ∈ {0.1, 0.01, 0.001}. When ǫ = 0.1, we can see
that there are two optimal policies having the same maximum order-of-magnitude expected utility, namely ∆1 (for
T = yes) and ∆2 (for T = no). Therefore, if the seismic
test is performed (T = yes) then drilling is to be done only
if the test results show an open or closed pattern. Otherwise
(T = no), the wildcatter will drill regardless of the test results. Ties like these at the decision variables are expected
given that the order-of-magnitude probabilities and utilities
represent abstractions of the real values. The expected utilities of ∆1 and ∆2 in the original influence diagram are

42.75 and 20.00, respectively.

Algorithm 1: ELIM-OOM-ID

When ǫ = 0.01, we also see that both drilling options are
equally possible if the seismic test is performed and the
test results show a diffuse pattern. In this case, there are
four optimal policies having the same maximum order-ofmagnitude expected utility. Finally, when ǫ = 0.001, we
can see that all decision options are possible and the corresponding optimal policies set contains 128 policies. The
explanation is that the order-of-magnitude influence diagram contains in this case only trivial order-of-magnitude
values such as h+, 0i, h−, 0i and h+, ∞i, respectively.

Data: An OOM-ID hX, D, U, Gi, bucket structure along a legal
elimination ordering of the variables o
Result: An optimal policy ∆
// top-down phase
for p = t downto 1 do
let Λp = {λ1 , ..., λj } and Θp = {θ1 , ..., θk } be the
probability and utility components in buckets[p]
if Yp is a chance variable then
P Q
λp ← Yp ji=1 λi
Q
P
P
θp ← (λp )−1 × Yp (( ji=1 λi ) × ( kj=1 θj ))

1
2
3
4
5

else if Yp is a decision variable then
P
if Λp = ∅ then θp ← maxYp kj=1 θj
else
Q
λp ← maxYp ji=1 λi
Q
P
p
θ ← maxYp (( ji=1 λi ) × ( kj=1 θj ))

6
7
8

5.3

VARIABLE ELIMINATION

9
10

Theorem 2 ensures the soundness and correctness of a variable elimination procedure using the summation (+), multiplication (×) and maximization (max) operations over
partially ordered sets of order-of-magnitude values, for
solving order-of-magnitude influence diagrams.
Therefore, a variable elimination algorithm that computes
the optimal policy of an order-of-magnitude influence diagram (and also the maximum order-of-magnitude expected
utility) is described by Algorithm 1. The algorithm, called
ELIM-OOM-ID, is based on Dechter’s bucket elimination
framework for standard influence diagrams [3] and uses a
bucket structure constructed along a legal elimination ordering o = Y1 , . . . Yt of the variables in X ∪ D. The bucket
data-structure, called buckets, associates each bucket with
a single variable. The bucket of Yp contains all input probability and utility functions whose highest variable is Yp .
The algorithm processes each bucket, top-down from the
last to the first, by a variable elimination procedure that
computes new probability (denoted by λ) and utility (denoted by θ) components which are then placed in corresponding lower buckets (lines 1–11). The λp of a chance
bucket is generated by multiplying all probability components and eliminating by summation the bucket variable.
The θp of a chance bucket is computed as the average utility of the bucket, normalized by the bucket’s compiled λp .
For a decision variable, we compute the λp and θp components in a similar manner and eliminate the bucket variable
by maximization. In this case, the product of the probability components in the bucket is a constant when viewed
as a function of the bucket’s decision variable [5, 15] and
therefore, the compiled λp is a constant as well.
In the second, bottom-up phase, the algorithm computes an
optimal policy. The decision buckets are processed in reverse order, from the first variable to the last. Each decision
rule is generated by taking the argument of the maximization operator applied over the combination of the probability and utility components in the respective bucket, for each
configuration of the variables in the bucket’s scope (ie, the
union of the scopes of all functions in that bucket minus the

15

place each λp and θp in the bucket of the highest-index
variable in its scope
// bottom-up phase
for p = 1 to t do
if Yp is a decision variable then
P
Q
δp ← arg maxYp (( ji=1 λi ) × ( kj=1 θj ))
∆ ← ∆ ∪ δp

16

return ∆

11

12
13
14

bucket variable Yp ).
T HEOREM 3 (complexity) Given an OOM-ID with n variables, algorithm ELIM-OOM-ID is time and space O(n ·
∗
k wo ), where wo∗ is the treewidth of the legal elimination
ordering o and k bounds the domain size of the variables.

6

EXPERIMENTS

In this section, we evaluate empirically the quality of the
decision policies obtained for order-of-magnitude influence
diagrams. All experiments were carried out on a 2.4GHz
quad-core processor with 8GB of RAM.
Methodology We experimented with random influence
diagrams described by the parameters hnc , nd , k, p, r, ai,
where nc is the number of chance variables, nd is the number of decision variables, k is the maximum domain size,
p is the number of parents in the graph for each variable, r
is the number of root nodes and a is the arity of the utility
functions. The structure of the influence diagram is created
by randomly picking nc + nd − r variables out of nc + nd
and, for each, selecting p parents from their preceding variables, relative to some ordering, whilst ensuring that the
decision variables are connected by a directed path. A single utility node with a parents picked randomly from the
chance and decision nodes is then added to the graph.
We generated two classes of random problems with parameters hn, 5, 2, 2, 5, 5i and having either positive utilities
only or mixed (positive and negative) utilities. They are

denoted by P : hn, 5, 2, 2, 5, 5i and M : hn, 5, 2, 2, 5, 5i,
respectively. In each case, 75% of the chance nodes were
assigned extreme CPTs which were populated with numbers drawn uniformly at random between 10−5 and 10−4 ,
whilst ensuring that the table is normalized. The remaining
CPTs were randomly filled using a uniform distribution between 0 and 1. For class P , the utilities are of the form 10u ,
where u is an integer uniformly distributed between 0 and
5. For class M , the utilities are of the form +10u or −10u ,
where u is between 0 and 5, as before, and we have an
equal number of positive and negative utility values. Each
influence diagram instance was then converted into a corresponding order-of-magnitude influence diagram using the
mapping of the probabilities and utilities described in Section 5.2, for some ǫ < 1. Intuitively, the smaller ǫ is, the
coarser the order-of-magnitude approximation of the exact
probability and utility values (ie, more information is lost).

Results Figure 3 displays the distribution of the relative
errors ηmed (top) and ηmax (bottom) obtained on orderof-magnitude influence diagrams derived from class P (ie,
positive utilities), as a function of the problem size (given
by the number of variables), for ǫ ∈ {0.5, 0.05, 0.005}.
Each data point and corresponding error bar represents the
25th , median and 75th percentiles obtained over 30 random problem instances generated for the respective problem size. We can see that ηmed is the smallest (less than
10%) for ǫ = 0.5. However, as ǫ decreases, the loss of
information due to the order-of-magnitude abstraction increases and the corresponding relative errors ηmed increase
significantly. Notice that the best policy ∆max derived
from the order-of-magnitude influence diagram was almost
identical to that of the corresponding standard influence diagram, for all ǫ (ie, the error ηmax is virtually zero).
Figure 4 shows the distribution of ηmed (top) and ηmax
(bottom) obtained on order-of-magnitude influence diagrams from class M (ie, mixed utilities). The pattern of
the results is similar to that from the previous case. However, in this case, the errors span over two or three orders of

relative error of OOM median policy (%)

ε=0.5
ε=0.05
ε=0.005

80

60

40

20

0

10

20

30

40
variables

50

60

30

40
variables

50

60

70

100
relative error of OOM best policy (%)

Measures of Performance To measure how close the decision policies derived from the optimal policy set of an
order-of-magnitude influence diagram are to the optimal
policy of the corresponding standard influence diagram, we
use two relative errors, defined as follows. Let I be an influence diagram and let Iǫ be the corresponding order-ofmagnitude approximation, for some ǫ value. We sample s
different policies, uniformly at random, from the optimal
policies set of Iǫ , and for each sampled policy we compute
its expected utility in I. Let ∆med be a policy corresponding to the median expected utility vmed amongst the samples. We define the relative error ηmed = |(v − vmed )/v|,
where v is the maximum expected utility of the optimal
policy in I. Similarly, we define ηmax = |(v − vmax )/v|,
where ∆max is the best policy having the highest expected
utility vmax amongst the samples.

100

ε=0.5
ε=0.05
ε=0.005
80

60

40

20

0

10

20

70

Figure 3: Results for class P influence diagrams. We show
the distribution of the relative errors ηmed (top) and ηmax
(bottom) for ǫ ∈ {0.5, 0.05, 0.005}. # of samples s = 100.
magnitude, especially for ǫ = 0.05 and 0.005. This is because the sampled policy space includes policies which are
quite different from each other and, although they have the
same maximum order-of-magnitude expected utility, their
expected utility in the corresponding standard influence diagram is significantly different. For this reason, we looked
in more detail at the distribution of the expected utility values of 100 policies sampled uniformly at random from the
optimal policies set of a class M OOM-ID instance with
45 variables, for ǫ ∈ {0.5, 0.05, 0.005}. As expected, we
observed that the smallest sample variance is obtained for
ǫ = 0.5. For ǫ = 0.05 and ǫ = 0.005, the samples are
spread out even more from the mean, and the variance of
the expected utility is significantly larger. This explains the
large variations of the relative errors ηmed and ηmax , especially for smaller ǫ values (eg, ǫ = 0.05 and ǫ = 0.005).

7

RELATED WORK

Several extensions of the standard influence diagram model
have been proposed in recent years to deal with imprecise
probabilistic and utility information. Garcia and Sabbadin
[4] introduced possibilistic influence diagrams to model
and solve decision making problems under qualitative uncertainty in the framework of possibilistic theory. Pralet et
al [8] considered a generalized influence diagram system

attribute utility allowing trade-offs.

relative error of OOM median policy (%)

104
103


