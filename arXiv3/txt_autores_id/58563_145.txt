

These values, referred to as query responses, clearly de­
pend on the training sample used to instantiate the param­

A Bayesian Belief Network (BN) is a model of

eter values - i.e., different training samples will produce

a joint distribution over a finite set of variables,

different parameters and hence different responses.

with a DAG structure to represent the immedi­

This paper investigates how sampling variability in the

ate dependencies between the variables, and a
set of parameters (aka CPTables) to represent the

training data is related to uncertainty about a query re­

local conditional probabilities of a node, given

sponse. We follow the Bayesian paradigm, where uncer­

each assignment to its parents. In many situa­

tainty is quantified in terms of random variation, and we

tions, the parameters are themselves treated as

present a technique for computing Bayesian credible in­

random variables- reflecting the uncertainty re­

tervals (aka "error-bars") for query responses. Our algo­

maining after drawing on knowledge of domain

rithm takes as inputs a belief net structure (which we as­

experts and/or observing data generated by the

sume is correct- i.e., an accurate /-map of true distribu­

network. A distribution over the CPtable param­

tion [Pea88]); a data sample generated from the true belief

eters induces a distribution for the response the

net distribution; and a specific query of the form "'What is

BN will return to any "W hat is

Q =

Pr{ HIE} ?"

Pr{ H

=

hIE

= e } ?". After determining the

This paper investigates the distribution

conditional (posterior) distribution of the belief net param­

of this response, shows that it is asymptotically

eters given the sample, the algorithm produces an estimate

query.

(posterior mean value) of Q: e.g., estimate Q to be

asymptotic variance. We show that this compu­

To quantify uncertainty about this estimate, the algorithm

tation has the same complexity as simply com­

computes an approximate posterior variance for Q and uses

puting the

this variance to construct error-bars (a Bayesian credible in­

(mean value of the) response -i.e.,

O(n exp(w)),
ables and

w

where

n

terval) for Q; e.g., assert that Q is in the interval

is the number of vari­

is the effective tree width.

with 90% probability.

We

also provide empirical evidence showing that the

0.3 ± 0.1

There are several obvious applications for these error-bars.

error-bars computed from our estimates are fairly

First, error-bars can help a user make decisions, especially

accurate in practice, over a wide range of belief

in safety-critical situations - e.g., take action if we are

net structures and queries.

99%

sure that Q =

Pr{ H

=

hIE

= e

}

is on one

side of a decision boundary. Second, error-bars can

1

Introduction

can make appropriate guarantees about the answers to cer­

model of a joint probability distribution, are used in
an ever increasing range of applications [Hec95].
nets

are

typically

built by

Be­

first finding an ap­

propriate structure (either by interviewing an expert,
or by

selecting

a

good model

from training

data),

then using a training sample to fill in the parame­
ters

sug­

gest that more training data is needed before the system

Bayesian belief nets (BNs), which provide a succinct

lief

0.3.

normal, and derives expressions for its mean and

[Hec98]. T he resulting belief net is then used to an­

swer questions, e.g., compute the conditional probability

tain queries. This information is especially valuable when
additional training data, while available, is costly, and its
acquisition needs to be justified. Similarly, the user might
decide that more evidence is needed about a specific in­
stance, before he can render a meaningful decision.

Fi­

nally, if an expert is available and able to provide "correct
answers" to some specific questions, error-bars can be used
to validate the given belief net structure. E.g., if the expert
claims that Q

=

0.5 but our algorithm asserts that Q is in

UAJ 2001

VAN ALLEN ET AL.

0
�
a
I

0

91,110
0.400

523

91,0!0

0.600

G(�l
"x@4�
x4

Figure

1:

o

1
0

Simple Example: Diamond Graph

the interval 0.30 ± 0.04 with 99.9% probability, then we
may question whether the structure provided is correct (as­
suming we believe the expert). By contrast, we might not
question this structure if our algorithm instead asserted that
Q is in the interval 0.30 ± 0.25 with 99.9% probability.
Section 2 provides background results and notation con­
cerning belief nets and Dirichlet distributions for belief net
parameters. Section 3 presents the theoretical results under­
lying our error-bars: a derivation of an approximate poste­
rior variance for a query probability Q, and a proof that the
posterior distribution of Q is asymptotically normal. Com­
putational issues related to calculation of the variance are
briefly discussed. Section 4 presents the results of an em­
pirical study using Monte Carlo simulations to validate our
error-bar methodology over a wide range of belief net struc­
tures and queries. Section 5 briefly surveys related work,
placing our results in context.
2

1
0
0

Belief nets and Dirichlet distributions

We encode the joint distribution of a vector of discrete ran­
dom variables X = (Xv}vEV as a belief net (aka Bayesian
network, probability net). A belief net (V, A, 8) is a
directed acyclic graph whose nodes V index the random
variables and whose arcs A represent dependencies. Let
Pa(v) C V be the immediate parents of node v, and let
Fv = (Xw)wEPa(v) be the corresponding vector of parent
variables. In a belief net, a variable Xv is independent of
its nondescendents, given Fv. The elements of the vector
8 are the CPtable entries

Let Xv and Fv = TiwEPa(v)Xw be the domains of Xv and
Fv· We assume that the domains are finite. The CPtable
for Xv contains IXvl X IFvl entries 0v,xJf·
Figure 1 provides a simple example of a belief network
with specific CPtable entries. Here X1 has no parents, so
we write F1 = (}.We have F2 :::��: {Xt), Fa= (Xt), F 4 =
{X2, Xa); and for each value a, b, c, d, we have 01,al0 =
Pr{ xl = a IE>}, e2,bJa = Pr{ x2 = b I Xt = a, E> },
and e4,djb,c = Pr { x4 = d I x2 = b, X a = c, e }.

(Hence, using Figure 1, we have 81,110 = 0.4.) Note that
the values in each row add up to 1. In general, the variables
need not be binary, but can have larger (finite) domains.
The CPtable entries are estimated using training data and
(possibly) expert opinion. The latter information is incor­
porated using the Bayesian paradigm, where 8 is mod­
eled as a random variable and expert opinion is expressed
through an a priori distribution for 8. We adopt indepen­
dent Dirichlet priors1 for the various CPtable rows. Specit1cally, let 8vl/ = (9v,zl/)xEX. denote the CPtable row for
Fv = f- e.g., e4J(1,0) = (04,1](1,0), 04,0](1,0}) denotes
the entries for the X4 variable associated with the parental
assignment X2 = 1 and X3 = 0. We assume that, be­
fore observing the training data, the evil are independent
"Dir( a::,,11, x E Xv )"random vectors, where a:;,,11 > 0.
An absence of expert opinion is often expressed by setting
a;,xlf = 1 for all (v,x, f)- e.g., 84J(1,o) ""Dir( I, 1)
- which yields a uniform (flat) prior. Stronger opinion is
expressed through larger values of a:*v,x 11. Expressions for
the mean and variance of a Dirichlet distribution are given
below.
Now suppose that the training data consist of m indepen­
dent replicates of vectors X, generated using the given
structure and a fixed set of CPtable entries e. Let
mv,xlf denote the number of cases in the training set with
(Xv, Fv) = (x, f). Under the posterior distribution (the
conditional distribution given the training data), the E> vlf
are independent Vir( O:v,zl/• x E Xv) random vectors, with
O:v,xlf = a;,xlf + m v,xlf [BFH95]. This posterior distri­
bution underlies our derivation of Bayesian credible inter­
vals. Several properties of the Dirichlet distribution will be
needed.
Setting O:v,.J/ = l::xEXv O:v,xJ f• the posterior means and
(co)variances for CPtable entries are [BFH95]:

1

E{G v,xJf}

=

Cov{0v,zJ/• 0v,yJ/}

=

f.lv,xlf

O:v,zl/

= --

(1)

O:v,-J/
llv,xjt(c5xy- llv,yJf)
(2)
O:v,-J/ + 1

Readers unfamiliar with these assumptions, or with Dirichlet

distributions, are referred to [Hec98]. Note that a Dirichlet distri­
bution over a binary variable is a Beta distribution.

VAN ALLEN ET AL.

524

1 if x == y and c5,y = 0 otherwise. The ran­
Elvlf are asymptotically normal, in the limit
as min, av,xl! --t oo [Aki96]. More precisely, the nor­
malized variables ..;av,.1f(8v,x1J - 1-Lv,xJJ) converge in

where

c5xy =

dom vectors

distribution to jointly normal random variables with mean
zero and covariances

J.tv,xJJ(o,y- J.tv,yJJ).

This asymp­

totic framework is applicable as the amount of training data
increases

8v,xJJ

(m --t oo) provided all of the CPtable entries

are positive. This condition occurs with probability

one under a Dirichlet prior.

3

Bayesian Credible Intervals for Query Re­
sponses

It is well-known that the CPtable entries determine the

joint distribution of X:

Pr{ Xv = Xv, v

E

vIe}

=

I1vEV ev,x. II.] where(! vlvEV is determined by (xv ) vEV ;
see [Pea88]. Users are typically interested in one or more

specific "queries" asked of this joint distribution, where a
query is expressed as a conditional probability of the form
Q

=

q(B)

=

Pr{H=hiE=e,e},

(3 )

UAI2001

partial derivatives. Let Pv (h, x,fIe) denote the probabil­

ity

Pr{ H =h, Xv = x, Fv =fIE= e, e
and let pv (x , f

=

f1. },

le),Pv(h,f le),pv(f le), andp(h le) be

defined in a similar manner. Note that the subscript

v

is

Xv or F u is involved,
and all probabilities are evaluated at e
/-£. Let q� /
.zl
denote the partial derivative 8q(B)/8Bv,zlf evaluated at
0 := IL· We will use the following identity, derived by
needed to identify the node when

=

[GGS97, DarOO]:
1

qv,zlf

Pv(h,x,f le) - p(h le)pv(x,f le)
J.tv,zlf

_
-

We now derive an expression for

i7�,

·

(S)

and demonstrate

asymptotic validity of the credible interval (Equation

4)

given a sufficiently large training sample.

1 We assume that 8 is a random vector with
posterior Dirichlet distribution described in Section 2, and
approximate the variance of Q
q(8) by

Theorem

=

where H and E are subvectors of X, and h and e are legal

assignments to these subvectors. Note also the dependency
one.
In our Bayesian context, Q is a random variable with a (the­
oretically) known distribution determined by the posterior

ij�

the posterior mean J.LQ =

E{ Q}.

the iden tity [CH92]:
=

E{ q(8)}
Set It =

defined

E{E>}

where the components

by Equation

J.tv,zJ/

of 11- are

1.

W hile a point estimate 11-Q

=

q(�t)

can be useful, one

often requires some information concerning the potential
error in the estimate.

In the Bayesian context, this can

be achieved by plotting the posterior distribution of Q.
Alternatively, one may construct a 100(1

-

r5)%

cally not analytically tractable, but simple approximations
We

ill show that the distribution of Q is

w

approximately normal, and derive an approximation i7Q for

the standard deviation of Q. We then propose the following
interval as an approximate 100(1

J.LQ

±

-

r5)% credible interval:

zo/2 ifQ,

where zo/2 = <I>-1 (1 - 8/2) is the upper
standard normal distribution.

(4)

J/2

value of the

Our derivation is based on a first-order Taylor expansion of

q(E>)

about

q(�J-).

Some notation is needed to express the

-

Bvf )/(av,.Jf

+

1),

(6)

{Pv(h,x,!Je)-p(hle)pv(x,f]e)P
,
J.t v, zlf

Consider an asymptotic framework where the poste­
rior means J.tv,xlf are fixed, positive values, and
min { nv,zl/}
T hen the random variable
-+
oo.
(Q- J.LQ )/i7Q converges in distribution to the standard
nonnal distribution.
Proof. Our proof uses the Delta method

sider the Taylor expansion

credi­

ble interval for Q; i.e., an interval (L, U) defined so that
Pr{ L :::; Q :::; U} = 1- o. Exact calculations are typi­

are available.

L

T his value can be calcu­

q( E{E>} ).

vEV !E:Fv

where

distribution of e. For a point estimate of Q, one may use

lated using

L L (Avt

=

q(8)

=

q(fJ.) + D

+

[BFH95]. Con­

R,

where

D

L L L q�,l:l/ (E>v,zlf - /Jv,z!f ).

vEV /E:Fv zEXv

(7)

and the remainder term R can be expressed in terms of the
matrix of second derivatives of q( e) evaluated at a pointe
between Band J.t. Since the variances for E>u,zJ! in EqUCJ­
tion 2 are of order 1/av,zlf --t 0, and since the second
derivatives remain bounded in a neighbourhood of f.i, the
remainder R is asymptotically negligible compared with D.

&b

We define
to be the variance of D (Equation 7). As
the CPtable rows El vlf are statistically independent, but

UAI2001

VAN ALLEN ET AL.

e ntries within a row are correlated, the variance of
be expressed as

D

can

525

Table 1: Gold Standard for Validity Estimates
d

After substituting Equation 2 for the covariances and sim­
plifying, we obtain Equation 6 with

L (q�,xlt)21Lx,vlf'

Avf

A substitution of Equation 5 then yields the equivalent ex­
pressions/or Avt and Bvt within Equation 6.

Dfuq

10%
20%
30%
40%

Mean
2.38

3.15
3.63

3.88

Std.Dev.
1.86

2.41
2.79
2.96

q�,xlf in time O(n2w);

see [DarOO]. Given these deriva­
tives, the summations in Equation 6 can be performed with
one additional pass over the values, of time 0 (n).

The extended paper [VGHOl] describes an algorithm for
computing UQ. The main challenge, computing all of
the derivatives q� xlf' is accomplished by "back propagat­
ing" intermediate' results obtained by the Bucket Elimina­
tion [Dec98] algorithm.

We observe that
is a random variable with mean 0
and variance 1. It remains to show that D / ijQ is asymp­
totically normal. This result follows from the asymptotic
multivariate normality of the components of 8 (after suit­
able standardization- see Section 2), and the fact that D
0
is a linear function ofEl.

[VGHOl] also provides additional comments on the proper
interpretation and application of this theorem.

There are exceptional situations where
the posterior distribution of q(9) is analytically tractable
and exact credible intervals are available. In the degener­
ate situation where the network structure has arcs connect­
ing all pairs of nodes (and hence imposes no assumptions
about conditional independence), the assumption of inde­
pendent Dirichlet distributions for CPtable rows is equiva­
lent to an assumption of a single Dirichlet distribution over
unconditional probabilities Pr{ Xv = Xv, v E V}. It is
then straightforward to derive the distribution of the query
probability using properties of the Dirichlet distribution;
see [Mus93].2 Note that this exact approach is not cor­
rect in general
i.e., it does not hold for networks with
non-trivial structure.3

(8)

4

Empirical Study

Theorem 1 proves that the interval /-LQ ± z6;2uq is asymp­
totically valid. More precisely, let

Degenerate Case:

-

The computational problem of
computing J.LQ = q(p,) is known to be NP-hard [Coo90];
when all variables Xv are binary, the most effective ex­
act algorithms require time O(n2w), where n = lVI is the
number of nodes and w is the induced tree width of the
graph [Dec98, LS99]. The variance uq can also be com­
puted in time O(n2w). This result follows from the exis­
tence of algorithms that can compute all of the derivatives
Computational Issues:

Assuming a uniform prior and a sample of size m, we can
compute the posterior variance of Pr{ HIE} as P{HIE} x (1F{HIE} )/((m x P{E})+3), where F(x) is the expected value
of x, wrt the given belief net.
3This follows from a dimensionality argument: in a non-trivial
structure, the 2n-dimensional vector of unconditional probabili­
ties is constrained to lie in a lower-dimensional submanifold of
t e 2n -!-dimensional simplex. This cannot be represented by a
smgle Dirichlet distribution because, wpl, the constraints would
not be satisfied.
2

�

be the probability that the query response Q falls outside of
the credible interval, based on our UQ estimate of standard
deviation, Equation 6. The values 1 o and 1 � are the
nominal and actual coverage probabilities for the credible
interval. The value� is a function of o, the graph (V, A),
the query q, and the posterior distribution of 0. The pos­
terior distribution depends on the prior distribution and the
training sample. Thus� typically varies from one applica­
tion to the next. While Theorem 1 implies that� � o when
the training sample is sufficiently large, it does not tell us
whether this approximation is valid in practice, particularly
for small samples. In general, the validity of the approxi­
mation depends on all of the factors determining �. We
carried out a number of experiments to assess how these
factors affect validity.
-

-

Given a fixed set of factors, we estimate the correspond­
ing� by a simple Monte Carlo strategy. Using the (fixed)
posterior distribution of 0, calculate ILQ and uq. Simulate
r replicates ei from the posterior distribution, calculate
Qi = q(0;), then let.&. be the proportion of the {Qi} with
IQ; /-LQI > Zof2UQ. In our experiments, each � was
based on r 100 replicates.
-

=

To quantify the validity of the approximation�
employ average absolute differences:
validity estimate

=

average I.&- Jl.

�

o, we

(9)

The absolute differences are averaged as we vary one or
more of the the factors determining �- The validity es­
timates are presented as percentages in our tables. When

VAN ALLEN ET AL.

526

Diamood Gqaph Qu,riat wit'! lilO% E'rorBa,.·· E�e (r"' 100

m

UAI2001

=50}

Analyiic &r11 •
Qyery�II)OnM +
Mon'le Carlo 81.1'1 D
..•

11

•..

lo
.,
i

•..

�-1

..•

-2

01

QO

02

Figure 2:

Results for the Diamond Graph

We studied the following inferential patterns in the dia­
mond graph (Figure I):
Pr{Xl

=

1 IE>}

Q2 = Pr{X1

=

1IX2

=

1 IX2X3

Qi

=

=

=

el,ll()
11
=

E>}

=

1,

E>}

Q3

=

Pr{Xt

Q4
Qs

=

IIX1 = 1, E>}
Pr{X2X3
1 IX4
11 E>}
Pr{X1

Q6

=

=

=

-.2

1

Standard Normal Ouanmea

-1

0

2

(B) QQ-plot showing relation to Normal
J E {10%, 20%, 30%, 40% }. The resulting validity es­
timates are listed in Table 2. Each cell in the table is an
average of 30 values.

Figure 2(A) shows the error-bars returned by our approx­
imation, and also the Monte Carlo system, on a random
network posterior, for the error-bars for 90% credible inter­
vals. We see the two methods give similar answers.
Figure 2(B) uses a quantile-quantile (QQ) plot to address
the validity of the normality assumption, independently of
the linear approximation. Each "line" in this figure corre­
sponds to z-scores of the 100 query responses generated
by our Monte Carlo simulation, plotted against standard
normal quantiles. This figure shows six such lines, each
corresponding to a single query in { Q1, . .. , Q6}, given a
sample of size m 10. A straight-line would correspond
to data produced by a "perfect" normal distribution; we see
each dataset is close. (Of course, this is only suggestive; the
real proof comes first from Theorem 1, and then from the
data (e.g., Table 2) which demonstrates that our approach,
which assumes normality, produces reasonable results.)
=

8�·'1' 9'·'1°
La e2,lla e,,,,o
=

=

e2.'1' e3,qt e,,,IO

La e2,tla e3,t!a e,,,,o

82.111 83,111

=

=

Lb1c e4,t!b,c e2.bll es.c11 e,,,IO
L,.,b,c e4,llb,c e2,bla e3,cla e,,,,o

=

Pr{X4

=

-3

(A) Examples of Error Bars;

viewing these values, it is helpful to have a gold standard
for comparison. Consider the validity estimate 1.& - 51 for
a single ..:l. The minimum expected value is obtained when
..:l == S; i.e., when 100.& has the Binomial(lOO, S) distribu­
tion. Table 1 presents means and standard deviations under
these ideal circumstances. Now suppose a validity estimate
is obtained by averaging k independent terms. Its standard
deviation is typically greater than the value Std.Dev./ v'k
suggested by Table 1 because there is usually variation in
the underlying � values.
4.1

""""

1IXt

=

1, E>}

=

L:o,c 84,tlb,c 82,olt 83,cll

The six queries cover a range of different inferential pat­
terns. The first is basically a "sanity check", as it is a triv­
ial inference; the fourth is also straightforward, although it
does involve a multiplication. The sixth is slightly more
complex, but it is still only a summation of a set of prod­
ucts. The remaining queries involve divisions of increas­
ingly complicated expressions.
For each m E {10, 20, 30, 40} , we carried out 30 trials
of the form: (1) generate E> from a uniform Dirichlet
prior distribution, (2) generate a training sample of size m
based on E> and use the result to obtain a posterior dis­
tribution, (3) generate 100 Monte Carlo replicates from
the posterior distribution and use these to obtain an esti­
mate 6. for each pair (Q, J), for Q E {QI. ... , Q6} and

4.2

Results for Alarm Network

The Alarm network [BSCC89] is a benchmark network
based on a medical diagnosis domain, commonly used in
belief network studies. The network variables are all dis­
crete, but many range over 3 or more values. The network
includes a CPtable for each node; i.e., a particular 0 is spec­
ified.
Table 3 summarizes the results for experiments on the
Alarm network, where we varied both J and m. For each
m, we generated a single random sample of size m from
0, and used this to determine a posterior distribution (as­
suming a uniform prior). Validity estimates were obtained
by averaging over randomly chosen queries. The queries
Pr{H =hIE= e, 0} were chosen by determining an as­
signment H = h to one randomly chosen query variable,
and assignments E = e to five randomly chosen evidence
variables. (Here, we used [HC91] to determine which vari-

527

VAN ALLEN ET AL.

UAI2001

Table 4: Results for Random Networks
#E
#H
2
3
4
5

Table 2: Results for Diamond Graph
m

Ql

Q2

10

2.37

2.77

20

2.67

3.33

30
40

2.60
2.60

3.03
2.97

10
20
30

3.50
4.60
2.90

5.00
6.27
5.07

40

4.07

5.27

3.97
5.20
4.50
4.90

5.63

4.70

30
40

3 . 70
5.13

7.20
5.80

3.33
3.90

6 = 20%
4.87
4.70
7.03
5.13
5.97
4.43
4.93
4.93

6 = 40%
7.53
5.33
9.27
5.73
6.47
5.00
6.73
5.97

3.63

30
40

2.00

3.10

6.97

10

4.33
4 .73

6 = 10%
3.27
3.10
3.37
2.50
3.13
2.40

6 = 30%
7.23
6.10
11.13
6.83
7.20
5.47
6.63
6.03

20

10
20

Q4

Q3

Q5

Q6

2.20
2.90
2.77

3.93
3 .50
3.70
2.90

3.43
4.03

5.57
4.53

3.97
4;03

4.87
3.87

2.60

5.13
5.30

6.27

3.50
4.27

5.30
4.27

1
2
3
4
5

1
2

3
4
5

6.03

4.40

6.63

5.20
4.27
4.43

5.97
4.97
4.47

1
2
3
4

5

fJ
10%

20%

30%

40%

50

2.47

4.37

4.48

4.07

100
150

2.66
3.04

4.95
5.35

5.97
6.45

4.87
5.66

200

2.65

4.80

5.43

5.42

abies could be query as opposed to evidence variables.)
Some or all of the evidence variables might have had no
effect on the query variable, others might have had a pro­
found effect. Each cell in Table 3 represents an average
from 100 queries on a single posterior distribution.

4.3

Results for Random Networks

Although random networks tend not to reflect typical (or
natural) domains, they complement more focussed studies
by exposing methods to a wide range of inputs and help to
support claims of generality. We carried out experiments
on networks with 10 binary variables and 20 links, gener­
ating gold models from a uniform prior distribution on e.
and generating random queries of various types. Here we
used sample size m = 1 00 throughout, and varied the type
of query. Table 4 displays the results of our experiments.
Each query was of the form Pr{H =hIE= e, 9}, with
varying dimensionalities forE and H. Let #E and #H
denote the number of variables comprising E and H, re­
spectively. Each cell of Table 4 is based on 100 trials: I 0
queries on 10 networks, with both structure and posterior
generated randomly.

2.72

2.59
2.49

2.50
2.59

2.57
2.72

2.26
2.53

0

6
3.06
3.60
3.50
4.12

4.16
3.63
4.16
4.46

4.67

5.76

0

4.4

4.56
5.64

5.07
6.31

6.63

7.85

8.17

30%
4.75
5.11
5.02
5.13

5.97

=

5.61

5.69

7.63

7.43
9.45

5.15
4.95

4.39
4.96
5.78
7.14

4.98

4.20

2.79

4.71

5.14

4.43
4.11

2.56
2.88

4.38

4.74

2

5

20%
4.41
4.19

3.00
2.45

=

4.7 8
4.28

1
3
4

= 10%
2.84
3.20
3.13
2.93
2.62
2.38
2.S4
2.58
2.61
3.05

4.17
4.46
4.12

0

Table 3: Results for Alarm Network
m

2.16

6.39
8.71

6.33
7.25
10.81

12.51

13.99

4.90
4.24

40%
4.56
5.00
4.81
5.44

4.62
4.52

4.80
6.47

5.95
8.36

7.14

8.95

13.05

=

5.62
6.43
7.12
10.99
16.15

Discussion

Our hypothesis was that our Bayesian error-bars algorithm
would be accurate for essentially all cases. We tried to falsify our hypothesis by varying the following experimental
factors:
•

Network structure (V, A)

•

Credibility level 1

•

Query type (Diamond network, Alarm)

•

Number of evidence variables (Random networks)

•

Number of query variables (Random networks)

-

c5

In no case did we observe a result where averagelto
81
exceeded 20%. In most cases, the validity estimate was less
than 8/3. As noted in Table 1, even if our error-bars were
exact, we would still get positive validity estimates due to
the variance in to about �- We therefore believe that these
results comfortably bound the expected error of our method
under the experimental conditions . None of the factors that
we manipulated had a profound effect. T he strongest ef­
fect, observed in Table 4, was that increasing the number
of variables assigned in a query tended to increase the er­
ror I� 81; see also [Kle96]. One possible explanation is
that, as #E and #H increase, the query function q tends
to become more complex, and the local linear approxima­
tion of q becomes less reliable. Another possibility is that
-

-

VAN ALLEN ET AL.

528

the query probability

Q

tends to become very small, mak­

UAI2001

bution over CPtables, but for different purposes. For exam­

ing the normal approximation less accurate. Further exper­

ple, Cooper and Herskovits [CH92] use it to compute the

iments could address this issue.

expected response to a query; by contrast, we also approx­

We found these results very encouraging.

Our method

appears to give reasonable error-bars for a wide range of
queries and network types.

This makes the technique a

promising addition to the array of data-analysis tools re­
lated to belief networks, especially as the algorithm is rea­
sonably efficient, (only) roughly doubling the computation
time per inference. W hile there may be pathological cases

imate the posterior variance in that response. Similarly,
while many BN-learning algorithms compute the posterior
distribution over CPtables [Hec98], most of these systems
seek a single set of CPtable entries that maximize the like­
lihood, which again is different from our task;

e.g.,

their

task is not relative to a specific query (but see [GGS97]).
Many other projects consider sensitivity analyses, provid­

where our method will not give reasonable results - per­

ing mechanisms for propagating ranges of CPtable values

haps because the local linear approximation and the asymp­

to produce a range in the response; cf., [BKRK97, Las95,

totic normality are far off the truth-

we

did not find such

CNKE93, DarOO]. W hile these papers assume the user is
explicitly specifying the range of a local CPtable value, our

cases in our experiments.

work considers the source of these variances based on a
Other Experiments:

We also ran a number of other ex­

periments. One set computed the average{ A

8}

data sample. This also means our system must propagate

scores

all of the "ranges"; most other analyses consider only prop­

in each situation, to determine if there was any systematic

agating a single range. The [DarOO] system is an excep­

-

bias. (Note this score differs from Equation 9 by not tak­

tion, as it can simultaneously produce all of the derivatives.

ing absolute values.) We found that our bounds were typi­

However, Darwiche does not consider our error-bar appli­

cally a bit too wide for most queries- i.e., we often found
the

1

-

a-interval included slightly more than

1

-

8 of the

cases. We are currently investigating this, to see if there are
straight-forward refinements we can incorporate.

cation, and so does not include the additional optimizations
we could incorporate.
Excluding the [DarOO] result, none of the other projects
provides an efficient way to compute that information.

We also computed error-bars based on the (incorrect!)

Also, some of those other papers focus on properties of

"complete structure" assumption, which implies the re­

this derivative - e.g., when it is

sponse will have a simple Dirichlet distribution; see Foot­

able entry.

note

ately from our expression (Equation 6). Finally, our anal­

2. We found that, as anticipated, the approach de­

scribed in this paper, using Equation 6, consistently out­

0

for some specific CPt­

Note that this information falls out immedi­

ysis holds for arbitrary structures; by constrast some other

performed that case, in that our approach was consistently

results (e.g., [CNKE93]) deal only with singly connected

closer to the Monte Carlo estimates.

networks (trees).

[VGHOl] discusses these results in detail.

It also inves­

Lastly, our analysis also connects to work on abstractions,

tigates techniques for dealing with extreme values, where

which also involves determining how influential a CPtable

the normal distribution may be sub-optimal.

entry is, with respect to a query, towards deciding whether

5

typically computational efficiency in computing that re­

to include a specific node or arc [GDSOl]. Their goal is

Related Work

sponse. By contrast, our focus is in computing the error­
Our results provide a way to compute the variance of

bars around the response, independent of the time required

a BN's response to a query,

to determine that result.

which depends on the

posterior distribution over the space of CPtable entries,
based on a data sample.
method" [BFH95]:

This is done using the "Delta

first determine the variance of each

CPtable row, then propagate this variance using a sensi­
tivity analysis

(i.e., the partial derivatives); see Equation 6.

Kleiter [Kle96] performs a similar computation; parts of his
analysis are more general, in that he considers incomplete

(1) discuss how to deal
structures, (2) show how to deal

data. However, he does not

with

general graphical

with

the correlations encountered with general Dirichlet distri­
butions, nor

(3)

provide an efficient way to compute this

information. Moreover, our empirical data provide addi­
tional evidence that the approximations inherent in this ap­
proach are appropriate, even for small sample sizes.
Several other researchers also consider the posterior distri-

6

Conclusion

Further Extensions:

Our current system has been im­

plemented, and works very effectively. There are several
obvious ways to extend it.

One set of extensions corre­

spond to discharging assumptions underlying Theorem

1:

computing error bars when the data was used to learn the
structure, as well as the parameters; dealing with param­
eters that are drawn from a distribution other than inde­
pendent Dirichlets, perhaps even variables that have con­
tinuous domains; dealing with a training sample whose
instances are

not completely specified.

Our work deals

with fully-parameterized CP tables. It would be interesting
to investigate techniques capable of dealing with CPtables

UAI2001

VAN ALLEN ET AL.

represented as, say, decision tree functions [BFGK96], etc.
Contributions:
Many real-world systems work by rea­
soning probabilistically, based on a given belief net modeL
When knowledge concerning model parameters is condi­
tioned on a random training sample, it is useful to view
the parameters as random variables; this characterizes our
uncertainty concerning the responses generated to specific
queries in terms of random variation. Bayesian error-bars
provide a useful summary of our current knowledge about
questions of interest, and so provide valuable guidance for
decision-making or learning.

This paper addresses the challenge of computing the error­
bars around a belief net's response to a query, from a
Bayesian perspective. We first motivated and formally de­
fined this task- finding the 100(1 - o)% credible interval
for a query response with respect to its posterior distribu­
tion, conditioned on a training sample. We then investi­
gated an application of the "Delta method" to derive these
intervals. This required determining both the covariance
matrix interrelating all of the parameters, and the derivative
of the query response with respect to each parameter. We
produced an effective system that computes these quanti­
ties, and then combines them to produce the error-bars.
The fact that our approximation is guaranteed to be cor­
rect in the limit does not mean it will work well in practice.
We therefore empirically investigated these claims, by test­
ing our system across a variety of different belief nets and
queries, and over a range of sample sizes and credibility
levels. We found that the method works well throughout.

Acknowledgements
We are grateful for the many comments and suggestions we
received from Adnan Darwiche and the anonymous review­
ers, and for the fairness of the UAI'0 1 programme chairs.
All authors greatfutly acknowledge the generous support
provided by NSERC, iCORE and Siemens Corporate Re­
search. Most of this work was done while the first author
was a student at the University of Alberta.


A Bayesian belief network models a joint
distribution with an directed acyclic graph
representing dependencies among variables
and network parameters characterizing conditional distributions. The parameters are
viewed as random variables to quantify uncertainty about their values. Belief nets are
used to compute responses to queries; i.e.,
conditional probabilities of interest. A query
is a function of the parameters, hence a random variable. Van Allen et al. (2001, 2008)
showed how to quantify uncertainty about a
query via a delta method approximation of
its variance. We develop more accurate approximations for both query mean and variance. The key idea is to extend the query
mean approximation to a “doubled network”
involving two independent replicates. Our
method assumes complete data and can be
applied to discrete, continuous, and hybrid
networks (provided discrete variables have
only discrete parents). We analyze several
improvements, and provide empirical studies
to demonstrate their effectiveness.

1

INTRODUCTION

Consider a simple example. Suppose A represents
presence/absence of a medical condition while B and
Y are test results. Variables B and Y are conditionally independent given A, with A and B binary
and Y continuous. The conditional independence assumption is represented by the directed acyclic graph
structure in Figure 1(a). Let θa = P (A = a),
θb|a = P (B = b | A = a), and let p(y | βa , σa ) be the
conditional density of Y given A = a, assumed normal with mean βa and variance σa2 . We want to estimate the probability that condition A is present given

specified results from the two tests B and Y . Let Θ
represent all of the parameters. If Θ were known, we
would use the formula:
θa θb|a p(y | βa , σa )
.
a1 θa1 θb|a1 p(y | βa1 , σa1 )

q(Θ) = qa|b,y (Θ) = P

(1)

In the Bayesian paradigm, uncertainty about Θ is
quantified by modeling parameters as random variables. It follows that query probabilities such as (1)
are also random. A query response is usually estimated
by approximating its posterior mean. This approximation is similar to expression (1), but with θa and θb|a
replaced by their posterior means and with the normal
densities replaced by Student’s t densities.
One may want more than just a point estimate. Van
Allen et al. (2001, 2008) showed (for discrete networks)
how one can approximate the variance and posterior
distribution of a query. Their variance derivation employs the delta method; i.e., a first-order Taylor series
expansion of the function q(Θ) about the posterior
mean of Θ. They provide asymptotic theory and empirical experiments supporting this approach. They
also showed how these approximations can be used
to construct a Bayesian credible interval (error bars)
for q(Θ). Guo and Greiner (2005) applied this delta
method approximation as part of a mean squared error (i.e., squared bias + variance) measure designed
to estimate the quality of different belief net structures when seeking a best classifier. Lee et al. (2006)
provide a technique for combining independent belief
net classifiers that involves weighting their respective
mean probability values by their inverse variances, and
they show that this works well in practice.
We propose new approximations for the mean and
variance based on a simple trick. Suppose (A1 , B1 , Y1 )
and (A2 , B2 , Y2 ) are replicates of the network variables,
conditionally independent given Θ. We represent the
paired replicates as nodes in a “doubled network” with
the same structure; see Figure 1. The squared query
q(Θ)2 can be expressed as a query in this doubled net-

UAI 2009

θb1 |a1
θb1 |a2

θb2 |a1
θb2 |a2

HOOPER ET AL.
#$
θa1 θa2
A
!"
! #
!
#
!
#
!
#
!
#
θ 1 1θ 1 1
"
!
$
#
#$ b |a b |a
#$
θb1 |a1 θb1 |a2
(βa1 , σa21 )
Y
B
2
!" (βa2 , σa2 ) !" θb1 |a2 θb1 |a1
θb1 |a2 θb1 |a2

233

θa1 θa1

θb1 |a1 θb2 |a1
θb1 |a1 θb2 |a2
θb1 |a2 θb2 |a1
θb1 |a2 θb2 |a2

θb2 |a1 θb1 |a1
θb2 |a1 θb1 |a2
θb2 |a2 θb1 |a1
θb2 |a2 θb1 |a2

θa1 θa2

θb2 |a1 θb2 |a1
θb2 |a1 θb2 |a2
θb2 |a2 θb2 |a1
θb2 |a2 θb2 |a2

θa2 θa1

θa2 θa2

'(

A1 , A2
%%&
'
%
'
%
'
%
'
%
'(
'(
(βa1 , σa21 )(βa1 , σa21 ) '
&
%
(
'
(βa1 , σa21 )(βa2 , σa22 )
Y1 , Y2
B1 , B2
(βa2 , σ 22 )(βa1 , σ 21 )
%& (β 2 , σa2 )(β 2 , σa2 ) %&
2
2
a
a
a
a

Figure 1: (a) A simple Bayesian network. (b) The corresponding doubled network.
Figure 1: A simple Bayesian net.

work:
P (A1 = A2 = a | B1 = B2 = b, Y1 = Y2 = y, Θ) .
The method used to approximate the mean of q(Θ)
can be extended to the doubled network to approximate the mean of q(Θ)2 and hence to approximate
the variance. Unlike the delta method, our approach
does not rely on approximate local linearity of q(Θ).
It does involve the addition of two incomplete observations to the data set when calculating the posterior
mean of q(Θ)2 . In some situations, this addition results in under-estimation of the desired variance. This
deficiency is largely eliminated by a simple adjustment.
A similar adjustment substantially improves the usual
query mean approximation.
Section 2 reviews pertinent models and methods for
belief networks. The network doubling technique is
described in Section 3 for discrete, continuous, and hybrid networks. Proposed adjustments and numerical
results are presented in Sections 4 and 5 for discrete
networks. Corresponding work for continuous and hybrid networks is ongoing. Computational issues are
discussed in Section 6. Contributions and plans for
further work are summarized in Section 7.

2
2.1

BACKGROUND
NETWORK VARIABLES

We assume network structure is known. Let B denote
a discrete network variable taking values b ∈ DomB .
Let Y denote a continuous network variable taking values y on the real line. Vectors of variables are denoted
by boldface: A for discrete and X for continuous. Let
Θ be a random vector comprising all unknown network
parameters; i.e., Θ determines all conditional distributions of variables given their parents.
We assume that discrete variables have only discrete
parents. Suppose pa(B) = A; i.e., the parents of B are
the variables comprising the vector A. The conditional
probability that B = b given A = a is denoted
θb|a = θB=b|A=a = P {B = b | A = a, Θ}.

1

Variables associated with values will be clear from context. We employ similar abbreviations for other parameters and hyperparameters. The θb|a parameters
are often presented in conditional probability tables
(CPtables) with rows indexed by a and columns by
b; e.g., see Figure 1. Note that we use superscripts
b1 , b2 to list the distinct values in DomB . We use subscripts b1 , b2 to denote arbitrary values in DomB , often
related to replicated variables B1 , B2 .
Continuous variables can have both discrete and continuous parents. Suppose pa(Y ) = hA, Xi with X =
hX1 , . . . , Xd i. The conditional distribution of Y is

(Y | A = a, X = x, Θ) ∼ N (1, xT )β a , σa2 ; (2)
i.e., normally distributed, conditional mean related to
x by a linear regression model with coefficients depending on a. Here xT is the transpose of the ddimensional column vector x while β a is an (d + 1)dimensional column vector of regression coefficients
(the first entry is the constant term).
2.2

PRIOR AND POSTERIOR

The network parameters represented by Θ consist of
CPtable parameters θb|a , regression coefficient vectors
β a , and variances σa2 . We assume the prior distribution for Θ has the following form; e.g., see Gelman et
al. (2003).
• CPtable rows follow Dirichlet distributions:
θB|a := hθb|a , b ∈ DomB i ∼ Dir(αB|a ),
where αB|a := hαb|a , b ∈ DomB i .
• The regression coefficients and variance together
have a normal-(inverse chi-square) distribution:

(β a | σa2 ) ∼ Nd+1 µa , σa2 (νa Ψa )−1 ,
σa−2

∼ (τa2 νa )−1 χ2νa .

I.e., dropping subscripts for a moment, β conditioned on σ 2 is multivariate normal with mean

234

HOOPER ET AL.
vector µ and covariance matrix σ 2 (νΨ)−1 ; and
ντ 2 /σ 2 has a χ2ν distribution with ν > 0 (not necessarily an integer). Note that τ 2 /σ 2 has mean 1
and variance 2/ν.

• Parameters are assumed to be statistically independent except where joint distributions are specified above. In particular, we assume global independence: the parameters determining the conditional distribution of one variable given its parents
are independent of all other parameters.
The prior is conjugate: given a data set D consisting
of n independent replicates of complete tuples of network variables, the prior hyperparameter values are
updated as follows. Let nab and na be the number of
tuples in D with (A, B) = (a, b) and A = a, respectively. Let (xi , yi ) be the observations of (X, Y ) for
the na tuples with A = a. Let X a be the na × (d + 1)
matrix with rows (1, xTi ). Let y a be the column vector
with entries yi . In the five equations below, the prior
hyperparameter values appear on the right-hand side
and are identified with tildes (e.g., α̃).
αb|a = α̃b|a + nab
νa = ν̃a + na
νa Ψa = ν̃a Ψ̃a + X Ta X a
νa Ψa µa = ν̃a Ψ̃a µ̃a + X Ta y a
i
h
 2

νa τa + µTa Ψa µa = ν̃a τ̃a2 + µ̃Ta Ψ̃a µ̃a + y Ta y a
P
P
The values
a,b αb|a and
a νa are called the effective sample sizes for variables B and Y , respectively.
Our adjustments developed in Section 4 are motivated
by large m asymptotics, where m is proportional to
the effective sample size for each of the variables; i.e.,
0
αb|a = mαb|a
and νa = mνa0
0
with (αb|a
, νa0 , Ψa , µa , τa2 ) fixed.

(3)

Large m asymptotics are similar to but not the same
as large n asymptotics. As the sample size n increases,
the posterior mean E{θb|a | D} = αb|a /α·|a varies and
converges to some value. (Here and elsewhere,
the dot
P
subscript indicates summation: α·|a = b αb|a .) Under assumption (3), the posterior mean remains fixed
as m varies.
2.3

APPROXIMATING A QUERY MEAN

Consider a query involving outcomes of hypothesis
variables H given values for evidence variables E.
It is convenient to represent the query in terms of a
function w(H). E.g., suppose H = A, E = (B, Y ),
e = (b, y), and
q(Θ)

= P (A = a | B = b, Y = y, Θ)
= E{w(A) | B = b, Y = y, Θ} ,

UAI 2009

where w(A) = 1 for A = a and w(A) = 0 otherwise.
For discrete networks, query responses q(Θ) are usually estimated by q(Θ̂), where Θ̂ := E{Θ | D} is the
posterior mean of the parameter vector. This plugin estimate usually differs slightly from the posterior query mean E{q(Θ) | D}. Cooper and Herskovits
(1992, expression 19) showed that the plug-in estimate
equals E{q(Θ) | D, e}; i.e., the posterior query mean
given an augmented data set consisting of D and an
additional partial observation of the evidence variables
E = e. Cooper and Herskovits (1991) derived a formula for E{q(Θ) | D, e} that is valid for discrete, continuous, and hybrid networks. This formula provides a
useful approximation of the less tractable E{q(Θ | D}.
The plug-in estimate is a special case of this formula
for discrete networks. The formula is important for
our network doubling technique, so is reviewed here.
In the integral expression below, Z represents all variables not included in (H, E); dh and dz refer to product measures allowing both integration for continuous
variables (Lebesgue measure) and summation for discrete variables (counting measure). Some manipulation yields
E{q(Θ) | D, e} = E{w(H) | E = e, D}
= E [ E{w(H) | E = e, Θ} | D ]
(4)
RR
R
w(h) p(h, e, z | θ)p(θ | D)dθdhdz
RRR
.
=
p(h, e, z | θ)p(θ | D)dθdhdz
Now p(h, e, z | θ) factors as a product of conditional
probabilities and densities, one for each variable in
the network.
Due to global independence, the inteR
gral p(h, e, z | θ)p(θ | D)dθ factors into a product of
integrals, one for each variable. The result is a product
of probabilities and densities described in Section 2.4
below. It follows that E{q(Θ | D, e} can be calculated
in essentially the same manner as the function q(Θ),
but with two modifications.
• For discrete variables, parameters θb|a are replaced by their posterior means. If all network
variables are discrete, then we have the plug-in
estimate:
E{q(Θ) | D, e} = q(E{Θ | D}).

(5)

• For continuous variables, the normal densities are
replaced by the St1 (η, ω 2 , ν) densities described
below. Note that this is not the same as replacing
β and σ 2 parameters with their posterior means.
2.4

PREDICTIVE DISTRIBUTIONS

The predictive distribution of the network variables is
obtained by integrating out their joint conditional dis-

UAI 2009

HOOPER ET AL.

tribution given Θ with respect to the posterior distribution of Θ. Global independence allows this integration to be carried out separately for each conditional
distribution of a variable given its parents.
The predictive distribution for a discrete variable B is
πb|a := P (B = b | A = a, D) = E{θb|a | D} =

αb|a
.
α·|a

The predictive distribution for a continuous variable is
a location-scale version of the Student’s t distribution
with ν degrees of freedom. We need the multivariate
form of this distribution in Section 3, so we define it
here. Suppose
T = η + U −1/2 (Z − η),
where Z and U are independent, Z ∼ Np (η, Ω), U ∼
(1/ν)χ2ν , and Ω is a nonsingular covariance matrix.
It follows that T has the following density function
(Johnson and Kotz, 1972, page 134):

(νπ)p/2 |Ω|1/2

Γ[(ν + p)/2] / Γ(ν/2)

(ν+p)/2 .
1 + ν1 (t − η)T Ω−1 (t − η)

We refer to this as the Stp (η, Ω, ν) distribution. For
p = 1, we write St1 (η, ω 2 , ν). Note that St1 (0, 1, ν) is
Student’s t distribution.
We claim that (Y | A = a, X = x, D) ∼ St1 (η, ω 2 , ν)
with ν = νa , η = (1, xT )µa , and

ω 2 = τa2 (1, xT )(νa Ψa )−1 (1, xT )T + 1 . (6)
To see this, let us suppress subscripts for a moment.
Let Z1 ∼ N (0, 1) be independent of (β, σ). Put Z 2 :=
σ −1 (β − µ) ∼ Nm+1 0, (νΨ)−1 . We then have
(Y | a, x, D) ∼ (1, xT )β + σZ1

∼ η + (σ/τ )τ (1, xT )Z 2 + Z1 .

3

NETWORK DOUBLING

In Section 2.3 we noted that E{q(Θ) | D} is usually
approximated by the more tractable E{q(Θ) | D, e}.
Here we propose approximating Var{q(Θ) | D} by
Var{q(Θ) | D, e, e}; i.e., the posterior variance given
D and additional replicates E 1 and E 2 of the vector
of evidence variables, both having the same value e.
We develop a formula for this latter variance by imagining a doubled network; see Figure 1(b). These mean
and variance approximations can be improved by adjustments described in Section 4.
Consider two replicated tuples of network variables,
conditionally independent and identically distributed
given Θ. Use these to replace each variable in the

235

original network by a pair of variables; e.g., B is replaced by B ∗ := (B1 , B2 ) with possible values b∗ =
(b1 , b2 ) ∈ DomB ∗ = DomB × DomB . If pa(B) = A,
then pa(B ∗ ) = A∗ := (A1 , A2 ). Conditional distributions of doubled variables given parents are obtained
by multiplying probabilities or densities for single variables.
For discrete variables, we have
P (B ∗ = b∗ | A∗ = a∗ , Θ) = θb1 |a1 θb2 |a2 .
E.g., if A = A, DomA = {a1 , a2 }, and DomB =
{b1 , b2 }, then the CPtable for B ∗ is the 4 × 4 array
shown in Figure 1(b). More generally, if a CPtable
in the original network involves dr × dc parameters,
then corresponding table in the doubled network has
d2r × d2c entries. Note that CPtable rows in the doubled network are not independent (local independence
does not hold) and do not have Dirichlet distributions.
Fortunately, these properties are not needed for the
factorization described following (4).
For continuous variables, the conditional density of
Y ∗ = (Y1 , Y2 ) given (A∗ = a∗ , X ∗ = x∗ , Θ) is the
product of the densities for two normal distributions
of the form (2) with subscript i = 1, 2 on a and x.
Put H ∗ = (H 1 , H 2 ), w∗ (H ∗ ) = w(H 1 )w(H 2 ), E ∗ =
(E 1 , E 2 ), and e∗ = (e, e). Some manipulation using
conditional independence yields
q(Θ)2 = E{w∗ (H ∗ ) | E ∗ = e∗ , Θ} ,
q(Θ) = E{w(H1 ) | E ∗ = e∗ , Θ} .
We thus have
Var{q(Θ) | D, e, e}
(7)
2
2
= E{q(Θ) | D, e, e} − [E{q(Θ) | D, e, e}]
= E{w∗ (H ∗ ) | e∗ , D} − [E{w(H1 ) | e∗ , D}]2 .
The doubled network satisfies global independence assumptions, so we can follow the approach of Section
2.3 to evaluate the two expected values in (7). To
accomplish this task, we need bivariate predictive distributions for the doubled network.
For discrete variables, the calculation follows from the
means and covariances of a Dirichlet distribution. Let
δb1 b2 be the Kronecker delta function. We have
πb∗∗ |a∗

:= P {B ∗ = b∗ | A∗ = a∗ , D}
= E{θb1 |a1 θb2 |a2 | D}
=

πb1 |a1 πb2 |a2 + δa1 a2

πb1 |a1 (δb1 b2 − πb2 |a1 )
.
α·|a1 + 1

If all network variables are discrete, then we have an
identity corresponding to (5). Let Θ∗ be the vector

236

HOOPER ET AL.

of all CPtable entries in the doubled network; e.g.,
θb1 |a1 θb2 |a2 appears in row a∗ and column b∗ for the
CPtable of B ∗ . We then have
E{q ∗ (Θ∗ ) | D, e, e} = q ∗ (E{Θ∗ | D})

(8)

with the entries in E{Θ∗ | D} given by the πb∗∗ |a∗ values above. The two expected values in the variance approximation (7) are calculated by applying (8) twice:
with q ∗ (Θ∗ ) = q(Θ)2 and with q ∗ (Θ∗ ) = q(Θ).
For continuous variables, we need the density for
{(Y1 , Y2 ) | a1 , a2 , x1 , x2 , D}. There are two cases to
consider.
• If a1 6= a2 , then the parameters (β a1 , σa2 1 ) and
(β a2 , σa2 2 ) are mutually independent. Consequently, the joint distribution factors as a product
of two St1 (η, ω 2 , ν) densities; see expression (6).
• If a1 = a2 ( = a, say), then the joint distribution
is St2 (η, Ω, ν) with ν = νa , η = X 2 µa , and
o
n
Ω = τa2 X 2 (νa Ψa )−1 X T2 + I 2 ,
where X 2 is the 2 × (1 + d) matrix whose rows
are each (1, xTi ) and I 2 is the 2 × 2 identity matrix. The derivation is similar to that following
(6). Note that (β a , σa2 ) is the same for both Y1
and Y2 in this case.

4

UAI 2009

Table 1: Summary of approximations for µq and σqq .
Means
q̂1 = E{q(Θ) | D, e}
q̂2 = E{q(Θ) | D, e, e}
q̂3 = q̂1 − (q̂2 − q̂1 )
q̂4 = q̂1 − σ̂qr /µr

√
verify that the distribution of m(Q − µq , R − µr )
converges to bivariate normal by modifying the proof
of Theorem 2 in Van Allen et al. (2008). Asymptotic
normality implies that
2
σqqrr − 2σqr
− σqq σrr → 0 at rate m−5/2

T

v̂1 = g Cg ,

E{R − µr | Q} ≈ (Q − µq )

We use approximations for higher moments motivated
by large m asymptotics; i.e., a sequence of posterior
distributions of the form (3) with m → ∞. One may

2σqr σqq (1 − 2µq )
.
µq (1 − µq ) + σqq

(11)

Switching the roles of Q and R gives
σqrr ≈

(9)

For conciseness we suppress D in our expressions; i.e.,
we implicitly assume that expectations are conditioned
on D. Put Q = q(Θ) = P (H = h | E = e, Θ) and R =
P (E = e | Θ). Note that R is an unconditional query,
with hypothesis E = e and no evidence variables. Let
µq , µr , σqq , σrr , and σqr denote the means, variances,
and covariance for (Q, R). We extend this notation to
higher moments; e.g., σqqr = E{(Q − µq )2 (R − µr )}.

σqr
σqq

and hence σqqr ≈ σqqq σqr /σqq . Now σqqq = 0 for normal distributions; however, Van Allen et al. (2008) argue that query distributions are usually better approximated by beta distributions. Substituting the third
moment of a beta distribution for σqqq , we obtain
σqqr ≈

where g is the gradient vector of q(Θ) and C is the
covariance matrix of Θ, both evaluated at E{Θ | D}.
The second variance approximation v̂2 is the doubling
method introduced in Section 3. The simple adjustments (q̂3 , v̂3 ) and more complex adjustments (q̂4 , v̂4 )
are developed in this section.

(10)

while σqrr and σqqr converge to zero at rate m−2 . We
considered approximating σqqr and σqrr by zero but
found that more accurate approximations give better
results. Asymptotic bivariate normality suggests

ADJUSTMENTS

We now narrow our focus to discrete networks and
consider the four mean and variance approximations
in Table 1. The delta method approximation is

Variances
v̂1 = delta method (9)
v̂2 = Var{q(Θ) | D, e, e}
v̂3 = expression (18)
v̂4 = expression (17)

2σqr σrr (1 − 2µr )
.
µr (1 − µr ) + σrr

(12)

Before proceeding, we observe that µr and σrr can
be calculated exactly because R can be expressed as
a sum of products of independent terms. For queries
with this property, all approximations (except v̂1 ) are
exact; i.e., additional observations of evidence variables have no effect on the posterior mean or variance. E.g., given a discrete network
with structure
P
E → B → H, we have q(Θ) = b θh|b θb|e . Since parameters in each product are independent, it follows
that q̂2 = q̂1 = µq and v̂2 = σqq .
We begin with adjustments to improve q̂1 . Bayes rule
and some manipulation yields
q̂1

=

q̂2

=

E(QR)
σqr
= µq +
(13)
E(R)
µr
E(QR2 )
2µr σqr + σqrr
= µq +
.
2
E(R )
µ2r + σrr

We approximate σqqrr using (10), σqqr by (11), σqr by
(14), µq by q̂4 , and replace σqq by v̂4 . Rearranging
terms yields the identity: v̂4 =
2
(µ2r + σrr ){v̂2 + (q̂2 − q̂4 )2 } − 2σ̂qr
. (17)
µ2r + σrr + 4µr σ̂qr (1 − 2q̂4 )/{q̂4 (1 − q̂4 ) + v̂4 }

Notice that v̂4 appears in the denominator of (17). We
initially set this value to v̂2 , then iteratively solve for
v̂4 . The values converge in a few iterations.
We observe that replacing σrr by zero has negligible
effect on (17) as m → ∞. By also replacing q̂4 by q̂3
and σ̂qr /µr by q̂2 − q̂1 , we obtain a simpler identity:
v̂3 =

v̂2 + 2(q̂2 − q̂1 )2
. (18)
1 + 4(q̂2 − q̂1 )(1 − 2q̂3 )/{q̂3 (1 − q̂3 ) + v̂3 }

We again initialize by v̂2 , then iteratively solve for v̂3 .
The approximations q̂3 and v̂3 may be preferred to q̂4
and v̂4 since µr and σrr are not required.
Rates of convergence are summarized in Proposition 1
below. The proof of this result follows easily from Van
Allen et al. (2008) and the development above.
Proposition 1. Assume a discrete network satisfying
(3) and let m → ∞. The query mean µq remains constant while the variance σqq approaches zero at rate
m−1 . The mean approximations have errors q̂j − µq
approaching zero at rate m−1 for j = 1 and 2, and at
the faster rate m−3/2 for j = 3 and 4. All four variance approximations have relative errors (v̂j −σqq )/σqq
approaching zero at rate m−1 .

-0.1
-0.3

Scaled Error
q3

q4

q1

q4

0.2
-0.6

-0.2

Scaled Error

0.0

Scaled Error

q3

(b) Diamond & m = 20

0.5

(a) NB & m = 20

(15)

µ2r σqq + 2µr σqqr + σqqrr
E{(Q − µq )2 R2 }
=
. (16)
E(R2 )
µ2r + σrr

0.0

q1

In trying to improve v̂2 , we began with the idea of
replacing q̂2 with µq :

This suggests an approximation v̂2 +4(q̂2 − q̂1 )2 , which
does help to reduce the under-estimation problem;
however, a greater improvement is obtained by further
analysis of (15):

-0.5

The formula for q̂4 in Table 1 follows from (13). Now
recall that, under condition (3), µr remains fixed while
σrr → 0 as m → ∞. It follows that setting σrr = 0
in (14) will have negligible effect for large m. We thus
obtain σ̂qr ≈ (q̂2 − q̂1 )µr , leading to the simpler q̂3
approximation.

E{(Q − µq )2 | e, e} = v̂2 + (q̂2 − µq )2 .

-0.4
-0.8

(14)

-0.5

(q̂2 − q̂1 )µr (µ2r + σrr ){µr (1 − µr ) + σrr }
.
2
µ3r (1 − µr ) + µr (1 − 2µr )σrr − σrr

Scaled Error

If µr = 1, then set σ̂qr = 0. Otherwise, substituting
(12) for σqrr and solving yields σ̂qr =

237

0.1

HOOPER ET AL.

-1.0

UAI 2009

q1

q3

q4

(c) NB & m = 500

q1

q3

q4

(d) Diamond & m = 500

Figure 2: Boxplots of scaled errors m(q̂j − q̂0 ) for j ∈
{1, 3, 4}, m ∈ {20, 500}, and network structures NB
and Diamond. Each boxplot shows variation in errors
for a set of distinct queries, 22 +24 = 20 for NB and 108
for Diamond. Errors for q̂3 and q̂4 are nearly identical.
Errors for q̂1 are often much larger. Results for q̂2 are
not plotted since q̂2 − q̂0 ≈ 2(q̂1 − q̂0 ).

5

NUMERICAL RESULTS

We evaluated accuracy of approximations q̂j and v̂j using highly accurate empirical estimates of µq and σqq .
These estimates q̂0 and v̂0 were obtained by simulating k = 106 replicates of Θ from the posterior distribution, evaluating q(Θ) for each replicate, then calculating the sample mean and sample variance. Computational
costs
preclude using empirical variance estipaper/R
figures
mates/Users/peterhooper/Documents/Research/Doubling
in practice. When m is large, asymptotic normality of q(Θ) implies that the distribution of v̂0 /σqq
is approximately (1/k)χ2k with variance 2/k.p Consequently v̂0 /σqq varies over the interval 1 ± 2 2/k for
roughly 95% of samples. Since our variance approximations have relative errors of order m−1 , it follows
that k should be of order at least m2 for v̂0 to have
substantially smaller relative error. When comparing
approximate relative errors (v̂j − v̂0 )/v̂0 with k = 106 ,
variation in v̂0 has a noticeable effect for m = 500; see
Figure 3(f).
Our examples differ with respect to network structure, posterior distribution, and query. All variables
are binary. All posterior distributions satisfy BDe
constraints (e.g., see Hooper 2008), so all variables
have the same effective sample size m. Hyperparameters are thus determined by m and the poste-

238

HOOPER ET AL.
3

E = all children of H, e varies over all combinations (22 for NB-2, 24 for NB-4).

0

1

&
• Diamond network with 4 variables .
& . , all 108
distinct queries with one hypothesis variable.

-2

-1

Scaled Relative Error

2

10
5
0
-5

Scaled Relative Error

-3

-10

v1

v2

v3

v4

v1

v3

v4

0
-2
-6

-4

Scaled Relative Error

10
0
-10
-20
-30

Scaled Relative Error

v2

(b) Diamond & m = 20

2

(a) NB & m = 20

v1

v2

v3

v4

v1

v2

v3

v4

(d) Diamond & m = 100

COMPUTATIONAL ISSUES

2
0
-2
-6

-4

Scaled Relative Error

0
-10
-20

Approximations for means are compared in Figure 2
and for variances in Figure 3. The errors and relative errors are multiplied by m in these figures to facilitate comparisons across a range of effective sample sizes. Boxplots for m = 20, 100, and 500 are
shown. Plots for other values of m are similar. By
Proposition 1, relative errors (v̂j − σqq )/σqq should approach zero at rates cj /m, where cj depends implicitly on the network, E(Θ | D), and the query. This
theory is supported by Figure 3 and additional plots
(not shown) comparing the four methods for individual queries. Our results suggest that c3 ≈ c4 while c1
and c2 tend to be further from zero. Relative errors
can be interpreted in terms of variances or standard
deviations. If (v̂j − σqq )/σqq = cj /m, then we have
p
r
v̂j
cj
cj
cj
v̂j
=1+
and √
= 1+
≈1+
.
σqq
m
σqq
m
2m

6

-40

Scaled Relative Error

10

20

(c) NB & m = 100

-30

UAI 2009

v1

v2

v3

(e) NB & m = 500

v4

v1

v2

v3

v4

(f) Diamond & m = 500

Figure 3: Boxplots of relative errors m(v̂j − v̂0 )/v̂0 for
j ∈ {1, 2, 3, 4}, m ∈ {20, 100, 500}, and network structures NB and Diamond. Each boxplot shows variation
among values for a set of distinct queries, 20 for NB
and 108 for Diamond. We observe that: relative errors
tend to be larger for NB compared with Diamond; v̂3
and v̂4 tend to over-estimate σqq for NB and are more
accurate than v̂2 ; the three methods v̂2 , v̂3 ,and v̂4 have
similar accuracy for Diamond; v̂1 is less accurate than
the other methods. The four methods appear to have
paper/R figures
similar
accuracy in (f), but these plots are mislead/Users/peterhooper/Documents/Research/Doubling
ing. Many of the Diamond queries have the property
described following (12), where v̂2 = v̂3 = v̂4 = σqq .
We would therefore expect the Diamond results for
m = 500 to be similar to those for m = 100. It appears
that the variation among relative errors for m = 500
is due in large part to variation in v̂0 .
rior means E{Θ | D}. Our examples are from three
small networks, each with one vector E{Θ | D} and
m ∈ {20, 50, 100, 200, 500}:
• Two naı̈ve Bayes networks (NB-2 and NB-4 with
2 and 4 features plus the root variable); H = root,

Inference in Bayesian networks is in general an NPcomplete problem (Cooper, 1990). For instance, the
complexity of the Variable Elimination (VE) Algorithm is O(dr ), where d is an upper bound on the
number of values that a variable can take and r is
an upper bound on the size of a factor generated by
the VE Algorithm (Koller and Friedman, 2008). Network doubling uses essentially the same technique to
calculate a variance as that used to evaluate a query,
resulting in corresponding computational complexity.
The doubled CPtables are larger (squared number of
rows and columns), so the computational complexity
of VE is increased to O(d2r ). The delta method retains O(dr ) complexity (Van Allen et al., 2008), so is
typically faster in large networks; see Table 2 below.
In some cases, we can exploit the structure of the network or query to achieve a polynomial time inference
algorithm. For poly-tree Bayesian networks (i.e. networks with at most one undirected path between any
pair of nodes), there exist inference algorithms with
linear time complexity. Reduced complexity is also
available when the query can be expressed in terms of
probabilities of hypothesis and evidence nodes conditioned on their Markov blanket; i.e., the parents, the
children and the parents of the children. Once again,
we have a polynomial time inference algorithm. These
techniques translate directly to efficient algorithms for
computing all of the variance approximations in Table 1.

UAI 2009

HOOPER ET AL.

Table 2: Timing results in seconds.
Network
NB-4
Diamond
Alarm

# Queries
100,000
108,000
100

Delta
37.837
50.052
11.390

Doubling
3.969
12.660
282.342

239

Acknowledgements
We are grateful for helpful comments from the anonymous reviewers. We acknowledge support provided by
NSERC, iCORE, and the Alberta Ingenuity Centre for
Machine Learning.

