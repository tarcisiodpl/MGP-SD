
Standard models of multi-agent modal logic do not capture the fact that information
is often ambiguous, and may be interpreted in different ways by different agents. We
propose a framework that can model this, and consider different semantics that capture
different assumptions about the agents’ beliefs regarding whether or not there is ambiguity.
We examine the expressive power of logics of ambiguity compared to logics that cannot
model ambiguity, with respect to the different semantics that we propose.

1 Introduction
In the study of multi-agent modal logics, it is implicitly assumed that all agents interpret all
formulas the same way. While they may have different beliefs regarding whether a formula ϕ
is true, they agree on what ϕ means. Formally, this is captured by the fact that the truth of ϕ
does not depend on the agent.
Of course, in the real world, there is ambiguity; different agents may interpret the same
utterance in different ways. For example, consider a public announcement p. Each player i
may interpret p as corresponding to some event Ei , where Ei may be different from Ej if i 6= j.
This seems natural: even if people have a common background, they may still disagree on how
1

to interpret certain phenomena or new information. Someone may interpret a smile as just a
sign of friendliness; someone else may interpret it as a “false” smile, concealing contempt; yet
another person may interpret it as a sign of sexual interest.
To model this formally, we can use a straightforward approach already used in [Halpern 2009;
Grove and Halpern 1993]: formulas are interpreted relative to a player. But once we allow such
ambiguity, further subtleties arise. Returning to the announcement p, not only can it be interpreted differently by different players, it may not even occur to the players that others may
interpret the announcement in a different way. Thus, for example, i may believe that Ei is
common knowledge. The assumption that each player believes that her interpretation is how
everyone interprets the announcement is but one assumption we can make about ambiguity. It
is also possible that player i may be aware that there is more than one interpretation of p, but
believes that player j is aware of only one interpretation. For example, think of a politician
making an ambiguous statement which he realizes that different constituencies will interpret
differently, but will not realize that there are other possible interpretations. In this paper, we
investigate a number of different semantics of ambiguity that correspond to some standard
assumptions that people make with regard to ambiguous statements, and investigate their relationship.
Our interest in ambiguity was originally motivated by a seminal result in game theory:
Aumann’s [1976] theorem showing that players cannot “agree to disagree.” More precisely, this
theorem says that agents with a common prior on a state space cannot have common knowledge
that they have different posteriors. This result has been viewed as paradoxical in the economics
literature. Trade in a stock market seems to require common knowledge of disagreement (about
the value of the stock being traded), yet we clearly observe a great deal of trading. One well
known explanation for the disagreement is that we do not in fact have common priors: agents
start out with different beliefs. In a companion paper [Halpern and Kets 2013], we provide a
different explanation, in terms of ambiguity. It is easy to show that we can agree to disagree
when there is ambiguity, even if there is a common prior.
Although our work is motivated by applications in economics, ambiguity has long been a
concern in philosophy, linguistics, and natural language processing. For example, there has
been a great deal of work on word-sense disambiguation (i.e., trying to decide from context
which of the multiple meanings of a word are intended); see Hirst [1988] for a seminal contribution, and Navigli [2009] for a recent survey. However, there does not seem to be much work
on incorporating ambiguity into a logic. Apart from the literature on the logic of context and
on underspecification (see Van Deemter and Peters [1996]), the only papers that we are aware
of that does this are ones by Monz [1999] and Kuijer [2013]. Monz allows for statements
that have multiple interpretations, just as we do. But rather than incorporating the ambiguity
directly into the logic, he considers updates by ambiguous statements.
Kuijer models the fact that ambiguous statements can have multiple meanings by using a
nondeterministic propositional logic, which, roughly speaking allows him to consider all the
meanings simultaneously. He then defines a notion of implication such that an ambiguous
statement A entails another ambiguous statement B if and only if every possible interpreta2

tions of A entails every possible interpretation of B. This idea of considering all possible
interpretations of an ambiguous statement actually has a long tradition in the philosophy literature. For example, Lewis [1982] considers assigning truth values to an ambiguous formula
φ by considering all possible disambiguations of φ. This leads to a semantics where a formula can, for example, have the truth value {true, false}. Lewis views this as a potential
justification for relevance logic (a logic where a formula can be true, false, both, or neither;
cf. [Rescher and Brandom 1979]). Our approach is somewhat different. We assume that each
agent uses only one interpretation of a given ambiguous formula φ, but an agent may consider
it possible that another agent interprets φ differently. In our applications, this seems to be the
most appropriate way to dealing with ambiguity (especially when it comes to considering the
strategic implications of ambiguity).
There are also connections between ambiguity and vagueness. Although the two notions
are different—a term is vague if it is not clear what its meaning is, and is ambiguous if it
can have multiple meanings, Halpern [2009] also used agent-dependent interpretations in his
model of vagueness, although the issues that arose were quite different from those that concern
us here.
Given the widespread interest in ambiguity, in this paper we focus on the logic of ambiguity.
We introduce the logic in Section 2. The rest of the paper is devoted to arguing that, in some
sense, ambiguity is not necessary. In Section 3, we show that a formula is satisfiable in a
structure with ambiguity (i.e., one where different agents interpret formulas differently) if and
only if it is satisfiable in a structure without ambiguity. Then in Section 4, we show that, by
extending the language so that we can talk explicitly about how agents interpret formulas, we
do not need structures with ambiguity. Despite that, we argue in Section 5 that we it is useful
to be able to model ambiguity directly, rather than indirectly.

2 Syntax and Semantics
2.1 Syntax
We want a logic where players use a fixed common language, but each player may interpret
formulas in the language differently. Although we do not need probability for the points we
want to make in this paper, for the applications that we have in mind it is also important for
the agents to be able to reason about their probalistic beliefs. Thus, we take as our base logic a
propositional logic for reasoning about probability.
The syntax of the logic is straightforward (and is, indeed, essentially the syntax already
used in papers going back to Fagin and Halpern [1994]). There is a finite, nonempty set N =
{1, . . . , n} of players, and a countable, nonempty set Φ of primitive propositions. Let LC
n (Φ) be
the set of formulas that can be constructed starting from Φ, and closing off under conjunction,
negation, the modal operators {CB G }G⊆N,G6=∅, and the formation of probability formulas. (We
omit the Φ if it is irrelevant or clear from context.) Probability formulas are constructed as

3

follows. If ϕ1 , . . . , ϕk are formulas, and a1 , . . . , ak , b ∈ Q, then for i ∈ N,
a1 pr i (ϕ1 ) + . . . + ak pr i (ϕk ) ≥ b
is a probability formula, where pr i (ϕ) denotes the probability that player i assigns to a formula
ϕ. Note that this syntax allows for nested probability formulas. We use the abbreviation Bi ϕ
1
for pr i (ϕ) = 1, EB 1G ϕ for ∧i∈G Bi ϕ, and EB m+1
ϕ for EB m
G EB G ϕ for m = 1, 2 . . .. Finally,
G
we take true to be the abbreviation for a fixed tautology such as p ∨ ¬p.

2.2 Epistemic probability structures
There are standard approaches for interpreting this language [Fagin and Halpern 1994], but
they all assume that there is no ambiguity, that is, that all players interpret the primitive propositions the same way. To allow for different interpretations, we use an approach used earlier
[Halpern 2009; Grove and Halpern 1993]: formulas are interpreted relative to a player.
An (epistemic probability) structure (over Φ) has the form
M = (Ω, (Πj )j∈N , (Pj )j∈N , (πj )j∈N ),
where Ω is the state space, and for each i ∈ N, Πi is a partition of Ω, Pi is a function that
assigns to each ω ∈ Ω a probability space Pi (ω) = (Ωi,ω , Fi,ω , µi,ω ), and πi is an interpretation
that associates with each state a truth assignment to the primitive propositions in Φ. That is,
πi (ω)(p) ∈ {true, false} for all ω and each primitive proposition p. Intuitively, πi describes
player i’s interpretation of the primitive propositions. Standard models use only a single interpretation π; this is equivalent in our framework to assuming that π1 = · · · = πn . We call a
structure where π1 = · · · = πn a common-interpretation structure; we call a structure where
πi 6= πj for some agents i and j a structure with ambiguity. Denote by [[p]]i the set of states
where i assigns the value true to p. The partitions Πi are called information partitions. While
it is more standard in the philosophy and computer science literature to use models where there
is a binary relation Ki on Ω for each agent i that describes i’s accessibility relation on states, we
follow the common approach in economics of working with information partitions here, as that
makes it particularly easy to define a player’s probabilistic beliefs. Assuming information partitions corresponds to the case that Ki is an equivalence relation (and thus defines a partition).
The intuition is that a cell in the partition Πi is defined by some information that i received,
such as signals or observations of the world. Intuitively, agent i receives the same information
at each state in a cell of Πi . Let Πi (ω) denote the cell of the partition Πi containing ω. Finally,
the probability space Pi (ω) = (Ωi,ω , Fi,ω , µi,ω ) describes the beliefs of player i at state ω, with
µi,ω a probability measure defined on the subspace Ωi,ω of the state space Ω. The σ-algebra
Fi,ω consists of the subsets of Ωi,ω to which µi,ω can assign a probability. (If Ωi,ω is finite, we
typically take Fi,ω = 2Ωi,ω , the set of all subsets of Ωi,ω .) The interpretation is that µi,ω (E) is
the probability that i assigns to event E ∈ Fi,ω in state ω.
Throughout this paper, we make the following assumptions regarding the probability assignments Pi , i ∈ N:
4

A1. For all ω ∈ Ω, Ωi,ω = Πi (ω).
A2. For all ω ∈ Ω, if ω ′ ∈ Πi (ω), then Pi (ω ′ ) = Pi (ω).
A3. For all j ∈ N, ω, ω ′ ∈ Ω, Πi (ω) ∩ Πj (ω ′) ∈ Fi,ω .
Furthermore, we make the following joint assumption on players’ interpretations and information partitions:
A4. For all ω ∈ Ω, i ∈ N, and primitive proposition p ∈ Φ, Πi (ω) ∩ [[p]]i ∈ Fi,ω .
These are all standard assumptions. A1 says that the set of states to which player i assigns
probability at state ω is just the set Πi (ω) of worlds that i considers possible at state ω. A2
says that the probability space used is the same at all the worlds in a cell of player i’s partition.
Intuitively, this says that player i knows his probability space. Informally, A3 says that player
i can assign a probability to each of j’s cells, given his information. A4 says that primitive
propositions (as interpreted by player i) are measurable according to player i.

2.3 Prior-generated beliefs
One assumption that we do not necessarily make, but want to examine in this framework, is
the common-prior assumption. The common-prior assumption is an instance of a more general
assumption, that beliefs are generated from a prior, which we now define. The intuition is that
players start with a prior probability; they then update the prior in light of their information.
Player i’s information is captured by her partition Πi . Thus, if i’s prior is νi , then we would
expect µi,ω to be νi (· | Πi (ω)).
Definition 2.1 An epistemic probability structure M = (Ω, (Πj )j∈N , (Pj )j∈N , (πj )j∈N ) has
prior-generated beliefs (generated by (F1 , ν1 ), . . . , (Fn , νn )) if, for each player i, there exist
probability spaces (Ω, Fi , νi ) such that
• for all i, j ∈ N and ω ∈ Ω, Πj (ω) ∈ Fi ;
• for all i ∈ N and ω ∈ Ω, Pi (ω) = (Πi (ω), Fi | Πi (ω), µi,ω ), where Fi | Πi (ω) is the
restriction of Fi to Πi (ω),1 and µi,ω (E) = νi (E | Πi (ω)) for all E ∈ Fi | Πi (ω) if
νi (Πi (ω)) > 0. (There are no constraints on νi,ω if νi (Πi (ω)) = 0.)

It is easy to check that if M has prior-generated beliefs, then M satisfies A1, A2, and A3.
More interestingly for our purposes, the converse also holds for a large class of structures. Say
that a structure is countably partitioned if for each player i, the information partition Πi has
countably many elements, i.e., Πi is a finite or countably infinite collection of subsets of Ω.
1

Recall that the restriction of Fi to Πi (ω) is the σ-algebra {B ∩ Πi (ω) : B ∈ Fi }.

5

Proposition 2.2 If a structure M has prior-generated beliefs, then M satisfies A1, A2, and
A3. Moreover, every countably partitioned structure that satisfies A1, A2, and A3 is one with
prior-generated beliefs, with the priors νi satisfying νi (Πi (ω)) > 0 for each player i ∈ N and
state ω ∈ Ω.
Proof. The first part is immediate. To prove the second claim, suppose that M is a structure
satisfying A1–A3. Let Fi be the unique algebra generated by ∪ω∈Ω Fi,ω . To define νi , if there
are Ni < ∞ cells in the partition Πi , define νi (ω) = N1i µi,ω (ω). Otherwise, if the collection
Πi is countably infinite, order the elements of Πi as p1i , p2i , . . .. Choose some state ωk ∈ pki for
each k, with associated probability space Pi (ωk ) = (Ωi,ωk , Fi,ωk , µP
i,ωk ). By A2, each choice of
ωk in pki gives the same probability measure µi,ωk . Define νi = k 21k µi,ωk . It is easy to see
that νi is a probability measure on Ω, and that M is generated by (F1 , ν1 ), . . . , (Fn , νn ).
Note that the requirement that that M is countably partitioned is necessary to ensure that
we can have νi (Πi (ω)) > 0 for each player i and state ω.
In light of Proposition 2.2, when it is convenient, we will talk of a structure satisfying
A1–A3 as being generated by (F1 , ν1 ), . . . , (Fn , νn ).
The common-prior assumption discussed in the introduction is essentially just the special
case of prior-generated beliefs where all the priors are identical.

2.4 Capturing ambiguity
We use epistemic probability structures to give meaning to formulas. Since primitive propositions are interpreted relative to players, we must allow the interpretation of arbitrary formulas
to depend on the player as well. Exactly how we do this depends on what further assumptions we make about what players know about each other’s interpretations. There are many
assumptions that could be made. We focus on two of them here, ones that we believe arise in
applications of interest, and then reconsider them under the assumption that there may be some
ambiguity about the partitions.
Believing there is no ambiguity The first approach is appropriate for situations where players may interpret statements differently, but it does not occur to them that there is another way
of interpreting the statement. Thus, in this model, if there is a public announcement, all players will think that their interpretation of the announcement is common knowledge. We write
(M, ω, i) out ϕ to denote that ϕ is true at state ω according to player i (that is, according to
i’s interpretation of the primitive propositions in ϕ). The superscript out denotes outermost
scope, since the formulas are interpreted relative to the “outermost” player, namely the player
i on the left-hand side of out . We define out , as usual, by induction.
If p is a primitive proposition,
(M, ω, i) out p iff πi (ω)(p) = true.
6

This just says that player i interprets a primitive proposition p according to his interpretation
function πi . This clause is common to all our approaches for dealing with ambiguity.
For conjunction and negation, as is standard,
(M, ω, i) out ¬ϕ iff (M, ω, i) 6 out ϕ,
(M, ω, i) out ϕ ∧ ψ iff (M, ω, i) out ϕ and (M, ω, i) out ψ.
Now consider a probability formula of the form a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b. The key
feature that distinguishes this semantics is how i interprets j’s beliefs. This is where we capture
the intuition that it does not occur to i that there is another way of interpreting the formulas
other than the way she does. Let
[[ϕ]]out
= {ω : (M, ω, i) out ϕ}.
i
Thus, [[ϕ]]out
is the event consisting of the set of states where ϕ is true, according to i. Note that
i
A1 and A3 guarantee that the restriction of Ωj,ω to Πi (ω) belongs to Fi,ω . Assume inductively
that [[ϕ1 ]]out
∩ Ωj,ω , . . . , [[ϕk ]]out
∩ Ωj,ω ∈ Fj,ω . The base case of this induction, where ϕ is
i
i
a primitive proposition, is immediate from A3 and A4, and the induction assumption clearly
extends to negations and conjunctions. We now define
(M, ω, i) out a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b iff
a1 µj,ω ([[ϕ1 ]]out
∩ Ωj,ω ) + . . . + ak µj,ω ([[ϕk ]]out
∩ Ωj,ω ) ≥ b.
i
i
Note that it easily follows from A2 that (M, ω, i) out a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b
if and only if (M, ω ′, i) out a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b for all ω ′ ∈ Πj (ω). Thus,
[[a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b]]i is a union of cells of Πj , and hence [[a1 pr j (ϕ1 ) + . . . +
ak pr j (ϕk ) ≥ b]]i ∩ Ωj,ω ∈ Fj,ω .
With this semantics, according to player i, player j assigns ϕ probability b if and only if the
set of worlds where ϕ holds according to i has probability b according to j. Intuitively, although
i “understands” j’s probability space, player i is not aware that j may interpret ϕ differently
from the way she (i) does. That i understands j’s probability space is plausible if we assume
that there is a common prior and that i knows j’s partition (this knowledge is embodied in the
assumption that i intersects [[ϕk ]]out
with Ωj,ω when assessing what probability j assigns to
i
2
ϕk ).
Given our interpretation of probability formulas, the interpretation of Bj ϕ and EB k ϕ follows. For example,
(M, ω, i) out Bj ϕ iff µj,ω ([[ϕ]]out
i ) = 1.
2

Note that at state ω, player i will not in general know that it is state ω. In particular, even if we assume that
i knows which element of j’s partition contains ω, i will not in general know which of j’s cells describes j’s
current information. But we assume that i does know that if the state is ω, then j’s information is described by
Ωj,ω . Thus, as usual, “(M, i, ω) out ϕ” should perhaps be understood as “according to i, ϕ is true if the actual
world is ω”. This interpretational issue arises even without ambiguity in the picture.

7

For readers more used to belief defined in terms of a possibility relation, note that
P if the probability measure µj,ω is discrete (i.e., all sets are µj,ω -measurable, and µj,ω (E) = ω′ ∈E µj,ω (ω ′)
for all subsets E ⊂ Πj (ω)), we can define Bj = {(ω, ω ′) : µj,ω (ω ′ ) > 0}; that is, (ω, ω ′) ∈ Bj
if, in state ω, agent j gives state ω ′ positive probability. In that case, (M, ω, i) out Bj ϕ iff
(M, ω ′ , i) out ϕ for all ω ′ such that (ω, ω ′) ∈ Bj . That is, (M, ω, i) out Bj ϕ iff ϕ is true
according to i in all the worlds to which j assigns positive probability at ω.
It is important to note that (M, ω, i)  ϕ does not imply (M, ω, i)  Bi ϕ: while (M, ω, i) out
ϕ means “ϕ is true at ω according to i’s interpretation,” this does not mean that i believes ϕ
at state ω. The reason is that i can be uncertain as to which state is the actual state. For i to
believe ϕ at ω, ϕ would have to be true (according to i’s interpretation) at all states to which i
assigns positive probability.
Finally, we define
(M, ω, i) out CB G ϕ iff (M, ω, i) out EB kG ϕ for k = 1, 2, . . .
for any nonempty subset G ⊆ N of players.
Awareness of possible ambiguity We now consider the second way of interpreting formulas.
This is appropriate for players who realize that other players may interpret formulas differently.
We write (M, ω, i) in ϕ to denote that ϕ is true at state ω according to player i using this
interpretation, which is called innermost scope. The definition of in is identical to that of out
except for the interpretation of probability formulas. In this case, we have
(M, ω, i) in a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b iff
in
a1 µj,ω ([[ϕ1 ]]in
j ∩ Ωj,ω ) + . . . + ak µj,ω ([[ϕk ]]j ∩ Ωj,ω ) ≥ b,
in
ϕ. Hence, according to player i,
where [[ϕ]]in
j is the set of states ω such that (M, ω, j) 
player j assigns ϕ probability b if and only if the set of worlds where ϕ holds according to j
has probability b according to j. Intuitively, now i realizes that j may interpret ϕ differently
from the way that she (i) does, and thus assumes that j uses his (j’s) interpretation to evaluate
the probability of ϕ. Again, in the case that µj,ω is discrete, this means that (M, ω, i) in Bj ϕ
iff (M, ω ′ , j) in ϕ for all ω ′ such that (ω, ω ′) ∈ Bj .
Note for future reference that if ϕ is a probability formula or a formula of the form CB G ϕ′ ,
then it is easy to see that (M, ω, i) in ϕ if and only if (M, ω, j) in ϕ; we sometimes write
(M, ω) in ϕ in this case. Clearly, out and in agree in the common-interpretation case, and
we can write . There is a sense in which innermost scope is able to capture the intuitions
behind outermost scope. Specifically, we can capture the intuition that player i is convinced
that all players interpret everything just as he (i) does by assuming that in all worlds ω ′ that
player i considers possible, πi (ω ′ ) = πj (ω ′) for all players j.

Ambiguity about information partitions Up to now, we have assumed that players “understand” each other’s probability spaces. This may not be so reasonable in the presence of
8

ambiguity and prior-generated beliefs. We want to model the following type of situation. Players receive information, or signals, about the true state of the world, in the form of strings
(formulas). Each player understands what signals he and other players receive in different
states of the world, but players may interpret signals differently. For instance, player i may
understand that j sees a red car if ω is the true state of the world, but i may or may not be aware
that j has a different interpretation of “red” than i does. In the latter case, i does not have a full
understanding of j’s information structure.
We would like to think of a player’s information as being characterized by a formula (intuitively, the formula that describes the signals received). Even if the formulas that describe each
information set are commonly known, in the presence of ambiguity, they might be interpreted
differently.
To make this precise, let Φ∗ be the set of formulas that is obtained from Φ by closing off
under negation and conjunction. That is, Φ∗ consists of all propositional formulas that can
be formed from the primitive propositions in Φ. Since the formulas in Φ∗ are not composed
of probability formulas, and thus do not involve any reasoning about interpretations, we can
extend the function πi (·) to Φ∗ in a straightforward way, and write [[ϕ]]i for the set of the states
of the world where the formula ϕ ∈ Φ∗ is true according to i.
The key new assumption that we make to model players’ imperfect understanding of the
other players’ probability spaces is that i’s partition cell at ω is described by a formula ϕi,ω ∈
Φ∗ . Roughly speaking, this means that Πi (ω) should consist of all states where the formula
ϕi,ω is true. More precisely, we take Πi (ω) to consist of all states where φi,ω is true according
to i. If player j understands that i may be using a different interpretation than he does (i.e., the
appropriate semantics are the innermost-scope semantics), then j correctly infers that the set of
states that i thinks are possible in ω is Πi (ω) = [[ϕi,ω ]]i . But if j does not understand that i may
interpret formulas in a different way (i.e., under outermost scope), then he thinks that the set of
states that i thinks are possible in ω is given by [[ϕi,ω ]]j . Of course, [[ϕi,ω ]]j does not in general
coincide with Πi (ω). Indeed, [[ϕi,ω ]]j may even be empty. If this happens, j might well wonder
if i is interpreting things the same way that he (j) is. In any case, we require that j understand
that these formulas form a partition and that ω belongs to [[ϕi,ω ]]j . Thus, we consider structures
that satisfy A1–A5, and possibly A6 (when we use outermost scope semantics).
A5. For each i ∈ N and ω ∈ Ω, there is a formula ϕi,ω ∈ Φ∗ such that Πi (ω) = [[ϕi,ω ]]i .
A6. For each i, j ∈ N, the collection {[[ϕi,ω ]]j : ω ∈ Ω} is a partition of Ω and for all ω ∈ Ω,
ω ∈ [[ϕi,ω ]]j .
Assumption A6 ensure that the signals for player i define an information partition according to
every player j when we consider the outermost scope semantics. With innermost scope, this
already follows from A5 and the definition of Πi (ω).
We can now define analogues of outermost scope and innermost scope in the presence of
ambiguous information. Thus, we define two more truth relations, out,ai and in,ai . (The
“ai” here stands for “ambiguity of information”.) The only difference between out,ai and
9

out is in the semantics of probability formulas. In giving the semantics in a structure M,
we assume that M has prior-generated beliefs, generated by (F1 , ν1 ), . . . , (Fn , νn ). As we
observed in Proposition 2.2, this assumption is without loss of generality as long as the structure
is countably partitioned. However, the choice of prior beliefs is relevant, as we shall see, so we
have to be explicit about them. When i evaluates j’s probability at a state ω, instead of using
µj,ω , player i uses νj (· | [[ϕj,ω ]]i ). When i = j, these two approaches agree, but in general they
do not. Thus, assuming that M satisfies A5 and A6 (which are the appropriate assumptions for
the outermost-scope semantics), we have
(M, ω, i) out,ai a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b iff
a1 νj ([[ϕ1 ]]out,ai
| [[ϕj,ω ]]out,ai
) + ...
i
i
out,ai
out,ai
+ak νj ([[ϕk ]]i
| [[ϕj,ω ]]i
) ≥ b,
where [[ψ]]out,ai
= {ω ′ : (M, ω, i) out,ai ψ}.
i
That is, at ω ∈ Ω, player j receives the information (a string) ϕj,ω , which he interprets as
[[ϕj,ω ]]j . Player i understands that j receives the information ϕj,ω in state ω, but interprets this
as [[ϕj,ω ]]i . This models a situation such as the following. In state ω, player j sees a red car, and
thinks possible all states of the world where he sees a car that is red (according to j). Player i
knows that at world ω player j will see a red car (although she may not know that the actual
world is ω, and thus does not know what color of car player j actually sees). However, i has
a somewhat different interpretation of “red car” (or, more precisely, of j seeing a red car) than
j; i’s interpretation corresponds to the event [[ϕj,ω ]]i . Since i understands that j’s beliefs are
determined by conditioning her prior νj on her information, i can compute what she believes
j’s beliefs are.
We can define in,ai in an analogous way. Thus, the semantics for formulas that do not
involve probability formulas are as given by in , while the semantics of probability formulas
is defined as follows (where M is assumed to satisfy A5, which is the appropriate assumption
for the innermost-scope semantics):
(M, ω, i) in,ai a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b iff
a1 νj ([[ϕ1 ]]in,ai
| [[ϕj,ω ]]in,ai
) + ...
j
j
in,ai
+ak νj ([[ϕk ]]j
| [[ϕj,ω ]]in,ai
) ≥ b.
j
Note that although we have written [[ϕj,ω ]]in,ai
, since ϕj,ω is a propositional formula, [[ϕj,ω ]]in,ai
=
i
i
out,ai
out
in
[[ϕj,ω ]]i
= [[ϕj,ω ]]i = [[ϕj,ω ]]i . It is important that ϕj,ω is a propositional formula here;
otherwise, we would have circularities in the definition, and would somehow need to define
[[ϕj,ω ]]in,ai
.
i
Again, here it may be instructive to consider the definition of Bj ϕ in the case that µj,ω is
discrete for all ω. In this case, Bj becomes the set {(ω, ω ′) : νj (ω ′ | [[ϕj,ω ]]in,ai
) > 0. That
j
′
′
is, state ω is considered possible by player j in state ω if agent j gives ω positive probability
after conditioning his prior νj on (his interpretation of) the information ϕj,ω he receives in state
ω. With this definition of Bj , we have, as expected, (M, ω, i) in,ai Bj ϕ iff (M, ω ′ , i) in,ai ϕ
for all ω ′ such that (ω, ω ′) ∈ Bj .
10

The differences in the different semantics arise only when we consider probability formulas. If we go back to our example with the red car, we now have a situation where player j
sees a red car in state ω, and thinks possible all states where he sees a red car. Player i knows
that in state ω, player j sees a car that he (j) interprets to be red, and that this determines his
posterior. Since i understands j’s notion of seeing a red car, she has a correct perception of
j’s posterior in each state of the world. Thus, the semantics for in,ai are identical to those for
in (restricted to the class of structures with prior-generated beliefs that satisfy A5), though the
information partitions are not predefined, but rather generated by the signals.
Note that, given an epistemic structure M satisfying A1–A4, there are many choices for
νi that allow M to be viewed as being generated by prior beliefs. All that is required of
νj is that for all ω ∈ Ω and E ∈ Fj,ω such that E ⊆ [[ϕj,ω ]]jout,ai , it holds that νj (E ∩
[[ϕj,ω ]]out,ai
)/νj ([[ϕj,ω ]]out,ai
) = µj,ω (E). However, because [[ϕj,ω ]]iout,ai may not be a subj
j
= Πj (ω), we can have two prior probabilities νj and νj′ that generate the
set of [[ϕj,ω ]]out,ai
j
same posterior beliefs for j, and still have νj ([[ϕk ]]out,ai
| [[ϕj,ω ]]iout,ai ) 6= νj′ ([[ϕk ]]iout,ai |
i
[[ϕj,ω ]]out,ai
) for some formulas ϕk . Thus, we must be explicit about our choice of priors here.
i

3 Common interpretations suffice
In this section, we show in there is a sense in which we do not need structures with ambiguity.
Specifically, we show that the same formulas are valid in common-interpretation structures as
in structures that do not have a common interpretation, no matter what semantics we use, even
if we have ambiguity about information partitions.
To make this precise, we need some notation. Fix a nonempty, countable set Ψ of primitive
propositions, and let M(Ψ) be the class of all structures that satisfy A1–A4 and that are defined
over some nonempty subset Φ of Ψ such that Ψ \ Φ is countably infinite.3 Given a subset Φ
of Ψ, a formula ϕ ∈ LC
n (Φ), and a structure M ∈ M(Ψ) over Φ, we say that ϕ is valid in
M according to outermost scope, and write M out ϕ, if (M, ω, i) out ϕ for all ω ∈ Ω and
i ∈ N. Given ϕ ∈ Ψ, say that ϕ is valid according to outermost scope in a class N ⊆ M(Ψ)
of structures, and write N out ϕ, if M out ϕ for all M ∈ N defined over a set Φ ⊂ Ψ of
primitive propositions that includes all the primitive propositions that appear in ϕ.
We get analogous definitions by replacing out by in , out,ai and in,ai throughout (in
the latter two cases, we have to restrict N to structures that satisfy A5 and A6 or just A5,
respectively, in addition to A1–A4). Finally, given a class of structures N , let Nc be the
subclass of N in which players have a common interpretation. Thus, Mc (Ψ) denotes the
structures in M(Ψ) with a common interpretation. Let Mai (Ψ) denote all structures in M(Ψ)
3

Most of our results hold if we just consider the set of structures defined over some fixed set Φ of primitive
propositions. However, for one of our results, we need to be able to add fresh primitive propositions to the
language. Thus, we allow the set Φ of primitive propositions to vary over the structures we consider, but require
Ψ \ Φ to be countably infinite so that there are always “fresh” primitive propositions that we can add to the
language.

11

with prior-generated beliefs that satisfy A5 and A6 (where we assume that the prior ν that
describes the initial beliefs is given explicitly).4
Theorem 3.1 For all formulas ϕ ∈ LC
n (Ψ), the following are equivalent:
(a) Mc (Ψ)  ϕ;
(b) M(Ψ) out ϕ;
(c) M(Ψ) in ϕ;
(d) Mai
c (Ψ)  ϕ;
(e) Mai (Ψ) out,ai ϕ;
(f) Mai (Ψ) in,ai ϕ.
Proof. Since the set of structures with a common interpretation is a subset of the set of
structures, it is immediate that (c) and (b) both imply (a). Similarly, (e) and (f) both imply
(d). The fact that (a) implies (b) is also immediate. For suppose that Mc (Ψ)  ϕ and that
M = (Ω, (Πj )j∈N , (Pj )j∈N , (πj )j∈N ) ∈ M(Ψ) is a structure over a set Φ ⊂ Ψ of primitive propositions that contains the primitive propositions that appear in ϕ. We must show that
M out ϕ. Thus, we must show that (M, ω, i) out ϕ for all ω ∈ Ω and i ∈ N. Fix ω ∈ Ω
and i ∈ N, and let Mi′ = (Ω, (Πj )j∈N , (Pj )j∈N , (πj′ )j∈N ), where πj′ = πi for all j. Thus,
Mi′ is a common-interpretation structure over Φ, where the interpretation coincides with i’s
interpretation in M. Clearly Mi′ satisfies A1–A4, so Mi′ ∈ Mc (Ψ). It is easy to check that
(M, ω, i) out ψ if and only if (Mi′ , ω, i)  ψ for all states ω ∈ Ω and all formulas ψ ∈ LC
n (Φ).
′
out
Since Mi  ϕ, we must have that (M, ω, i)  ϕ, as desired.
To see that (a) implies (c), given a structure M = (Ω, (Πj )j∈N , (Pj )j∈N , (πj )j∈N ) ∈ M(Ψ)
over some set Φ ⊂ Ψ of primitive propositions and a player j ∈ N, let Ωj be a disjoint copy of
Ω; that is, for every state ω ∈ Ω, there is a corresponding state ωj ∈ Ωj . Let Ω′ = Ω1 ∪. . .∪Ωn .
Given E ⊆ Ω, let the corresponding subset Ej ⊆ Ωj be the set {ωj : ω ∈ E}, and let E ′ be the
subset of Ω′ corresponding to E, that is, E ′ = {ωj : ω ∈ E, j ∈ N}.
Define M ′ = (Ω′ , (Π′j )j∈N , (Pj′ )j∈N , (πj′ )j∈N ), where Ω′ = Ω1 ∪. . . ∪Ωn and, for all ω ∈ Ω
and i, j ∈ N, we have
• Π′i (ωj ) = (Πi (ω))′;
• πi (ωj )(p) = πj (ω)(p) for a primitive proposition p ∈ Φ;
′
′
• Pi′ (ωj ) = (Ω′i,ωj , Fi,ω
, µ′i,ωj ), where Ω′i,ωj = Ω′i,ω , Fi,ω
= {Eℓ : E ∈ Fi,ω , ℓ ∈ N},
j
j
′
′
µi,ωj (Ei ) = µi,ω (E), µi,ωj (Eℓ ) = 0 if ℓ 6= i.
4

For ease of exposition, we assume A6 even when dealing with innermost scope.

12

Thus, π1 = · · · = πn , so that M ′ is a common-interpretation structure; on a state ωj , these
interpretations are all determined by πj . Also note that the support of the probability measure
µ′i,ωj is contained in Ωi , so for different players i, the probability measures µ′i,ωj have disjoint
supports. Now an easy induction on the structure of formulas shows that(M ′ , ωj )  ψ if and
′
only if (M, ω, j) in ψ for any formula ψ ∈ LC
n (Φ). It easily follows that if M  ϕ, then
in
C
M  ϕ for all ϕ ∈ Ln (Φ).
The argument that (d) implies (e) is essentially identical to the argument that (a) implies (b); similarly, the argument that (d) implies (f) is essentially the same as the argument that (a) implies (c). Since Mai
c (Ψ) ⊆ Mc (Ψ), (a) implies (d). To show that (d)
ai
implies (a), suppose that Mc (Ψ)  ϕ for some formula ϕ ∈ LC
n (Ψ). Given a structure
M = (Ω, (Πj )j∈N , (Pj )j∈N , π) ∈ Mc (Ψ) over a set Φ ⊂ Ψ of primitive propositions that includes the primitive propositions that appear in ϕ, we want to show that (M, ω, i)  ϕ for each
state ω ∈ Ω and player i. Fix ω. Recall that RN (ω) consists of the set of states N-reachable
from ω. Let M ′ = (RN (ω), (Π′j )j∈N , (Pj′ )j∈N , π ′ ), with Π′j and Pj′ the restriction of Πj and
Pj , respectively, to the states in RN (ω), be a structure over a set Φ′ of primitive propositions,
where Φ′ contains Φ and new primitive propositions that we call pi,ω for each player i and state
ω ∈ RN (ω).5 Note that there are only countably many information sets in RN (ω), so Φ′ is
countable. Define π ′ so that it agrees with π (restricted to RN (ω)) on the propositions in Φ,
and so that [[pi,ω ]]i = Πi (ω). Thus, M ′ satisfies A5 and A6. It is easy to check that, for all
′
′
′
ω ′ ∈ RN (ω) and all formulas ψ ∈ LC
n (Φ), we have that (M, ω , i)  ψ iff (M , ω , i)  ψ.
′
Since M  ϕ, it follows that (M, ω, i)  ϕ, as desired.
From Theorem 3.1 it follows that for formulas in LC
n (Ψ), we can get the same axiomatization with respect to structures in M(Ψ) for both the out and in semantics; moreover, this
axiomatization is the same as that for the common-interpretation case. An axiomatization for
this case is already given in [Fagin and Halpern 1994]; there is also a complete characterization
of the complexity of determining whether a formula is valid.
However, the equivalence in Theorem 3.1 does not extend to subclasses of M, Mc , and
ai
M . As shown in our companion paper [Halpern and Kets 2013], the equivalence result does
not hold if we consider the innermost scope semantics and restrict attention to the subclasses
of M and Mc that satisfy the common-prior assumption. We defer a further discussion of the
modeling implications of this result to Section 5.

4 A more general language
Although, when considering innermost scope, we allowed for agents that were sophisticated
enough to realize that different agents might interpret things in different ways, our syntax did
5

This is the one argument that needs the assumption that the set of primitive propositions can be different in
different structures in M(Ψ), and the fact that every Ψ \ Φ is countable. We have assumed for simplicity that
the propositions pi,ω are all in Ψ \ Φ, and that they can be chosen in such a way so that Ψ \ (Φ ∪ {pi,ω : i ∈
{1, . . . , n}, ω ∈ Ω}) is countable.

13

not reflect that sophistication. Specifically, the language does not allow the modeler (or the
agents) to reason about how other agents interpret formulas. Here we consider a language that
is rich enough to allow this. Specifically, we have primitive propositions of the form (p, i), that
can be interpreted as “i’s interpretation of p.” With this extended language, we do not need to
have a different interpretation function πi for each i; it suffices in a precise sense to use a single
(common) interpretation function. We now make this precise, and show that this approach is
general enough to capture both outermost and innermost scope.
More precisely, we consider the same syntax as in Section 2.1, with the requirement that
the set Φ of primitive propositions have the form Φ′ × N, for some set Φ′ ; that is, primitive
propositions have the form (p, i) for some p ∈ Φ′ and some agent i ∈ N. We interpret these
formulas using a standard epistemic probability structure M = (Ω, (Πj )j∈N , (Pj )j∈N , π), with
a common interpretation π, as in [Fagin and Halpern 1994]. Thus, truth is no longer agentdependent, so we have only (M, ω) on the left-hand side of , not (M, ω, i). In particular, if
(p, i) is a primitive proposition,
(M, ω)  (p, i) iff π(ω)((p, i)) = true.
As expected, we have
(M, ω)  a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b iff
a1 µj,ω ([[ϕ1 ]] ∩ Ωj,ω ) + . . . + ak µj,ω ([[ϕk ]] ∩ Ωj,ω ) ≥ b.
in
We no longer need to write [[ϕj ]]ou
i or [[ϕj ]]i , since all agents interpret all formulas the same
way.
We now show how we can capture innermost and outermost scope using this semantics.
Specifically, suppose that we start with an epistemic probability structure M = (Ω, (Πj )j∈N , (Pj )j∈N , (πj )j∈N )
over some set Φ of primitive propositions. Consider the corresponding common-interpretation
structure Mc = (Ω, (Πj )j∈N , (Pj )j∈N , π) over Φ × N, where π(ω)(p, i) = πi (ω)(p). Thus,
M and Mc are identical except in the primitive propositions that they interpret, and how they
interpret them. In Mc , the primitive proposition (p, i) ∈ Φ × N is interpreted the same way
that i interprets p in M.
out
We can now define, for each formula φ, two formulas φin
with the property that
i and φi
in
in
out
out
(M, ω, i)  φ iff (Mc , ω)  φi and (M, ω, i)  φ iff (Mc , ω)  φi . We start with φin
i ,
defining it by induction on structure:

• pin
i = (p, i)
in
′ in
• (ψ ∧ ψ ′ )in
i = ψi ∧ (ψi )
in
in
• (a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b)in
i = a1 pr j ((ϕ1 )j ) + . . . + ak pr j ((ϕk )j ) ≥ b
in
• (CB G ψ)in
i = CB G (∧j∈G Bj ψj ).

14

Note that φin
i is independent of i if φ is a probability formula or of the form CB G ψ. This is
to be expected, since, as we have seen, with innermost scope, the semantics of such formulas
is independent of i. The definition of (CB G ψ)in
i is perhaps the only somewhat surprising
clause here; as we discuss after the proof of Theorem 4.1 below, the more natural definition,
in
(CB G ψ)in
i = CB G (ψi ), does not work.
For outermost scope, the first two clauses of the translation are identical to those above; the
latter two change as required for outermost scope. Thus, we get
• pout
= (p, i)
i
• (ψ ∧ ψ ′ )out
= ψiout ∧ (ψi′ )out
i
out
• (a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b)out
= a1 pr j ((ϕ1 )out
i
i ) + . . . + ak pr j ((ϕk )i ) ≥ b

• (CB G ψ)out
= CB G (ψiout ).
i
Interestingly, here the natural definition of (CB G ψ)out
does work.
i
Theorem 4.1 If M is a probabilistic epistemic structure over Φ and Mc is the corresponding
common-interpretation structure over Φ × N, then
(a) (M, ω, i) in φ iff (Mc , ω)  φin
i ;
(b) (M, ω, i) out φ iff (Mc , ω)  φout
i .
Proof. We prove the result by induction on the structure of φ. The argument for outermost
scope is completely straightforward, and left to the reader. The argument for innermost scope
is also straightforward, except for the case that φ has the form CB G ψ. We now consider this
case carefully.
By definition,
(Mc , ω)  (CB G ψ)in
i
iff (Mc , ω)  CB G (∧j∈G Bj ψjin )
iff (Mc , ω)  (EB G )k (∧j∈G Bj ψjin ) for k = 1, 2, 3, . . ..
in
Note that, by definition, (EB G ψ)in
i = ∧j∈G Bj ψj . Thus, by the induction hypothesis, it follows that
(Mc , ω)  ∧j∈G Bj ψjin iff (M, ω, i) in EB G .

Now by a straightforward induction on k, we can show that
(Mc , ω)  EB k (∧j∈G Bj ψjin ) iff (M, ω, i) in EB k+1
G ψ.
That is,
(Mc , ω)  CB(∧j∈G Bj ψjin ) iff (M, ω, i) in EB kG ψ for k = 2, 3, 4, . . ..
15

(1)

It immediately follows from (1) that if (M, ω, i) in CB G ψ, then (Mc , ω)  CB(∧j∈G Bj ψjin ).
The converse also follows from (1), once we show that (M, ω, i) in EB 2G ψ implies (M, ω, i) in
EB G ψ. But this too follows easily since
implies
implies
iff
iff

(M, ω, i) in
(M, ω, i) in
(M, ω, i) in
(M, ω, i) in
(M, ω, i) in

EB 2G ψ
∧j∈G Bj (∧j∈G Bj ψ)
∧j∈G Bj (Bj ψ)
∧j∈G Bj ψ
EB ψ.

This completes the argument.
To see why we need we need the more complicated definition of (CB G ψ)in
i , it is perhaps
in
best to consider an example. By definition, (CB {1,2} p)1 = CB {1,2} (B1 (p, 1) ∧ B2 (p, 2)). By
way of contrast, CB {1,2} (pin
1 ) = CB {1,2} (p, 1), which (using arguments similar in spirit to
those used above) can be shown to be equivalent to CB {1,2} (B1 (p, 1) ∧ B2 (p, 1)). They key
point here is whether we have B2 (p, 1) or B2 (p, 2). We want the latter, which is what we get
from the more complicated translation that we use; it is easy to show that the former does not
give the desired result. These issues do not arise with outermost scope.
Theorem 4.1 shows that, from a modeler’s point of view, there is no loss in working with
common-interpretations structures. Any structure that uses ambiguous propositions can be
converted to one that uses unambiguous propositions of the form (p, i). In a sense, this can
be viewed as a strengthening of Theorem 3.1. Theorem 3.1 says that any formula that is satisfiable using innermost or outermost semantics in the presence of ambiguity is also satisfiable
in a common-interpretation structure. However, that common-interpretation structure might
be quite different from the original structure. Theorem 4.1 shows that if a formula φ is true
out
according to agent i at a state ω in a structure M, then a variant of φ (namely, φin
i or φi ) is
true at state ω in essentially the same structure.
Moreover, once we add propositions of the form (p, i) to the language, we have a great
deal of additional expressive power. For example, we can say directly that agent i believes
that all agents interpret p the same way that he does by writing Bi (∧j ((p, i) ⇔ (p, j))). We
can also make more complicated statements, such as “agent i believes that agents j and k
interpret p the same way, although they interpret p differently from him: Bi ((p, j) ⇔ (p, k)) ∧
¬Bi ((p, i) ⇔ (p, j)). Clearly, far more subtle relationships among agents’ interpretations of
primitive propositions can be expressed in this language.

5 Discussion
We have defined a logic for reasoning about ambiguity, and then showed that, in two senses,
we really do not need structures with ambiguity: (1) the same axioms hold whether or not we
have ambiguity, and (2) we can use a richer language to talk about the ambiguity, while giving
16

an unambigious interpretation to all formulas. So why do we bother using structures with
ambiguity? Perhaps the main reason is that it allows us to describe the situation from the agent’s
point of view. For example, if we are dealing with outermost scope, an agent does not realize
that there are other interpretations possible other than the one he is using. Thus, the simpler
language more directly captures agents’ assertions. Similarly, a structure with ambiguity may
more accurately describe a situation than a structure with a common interpretation. We thus
believe that structures with ambiguity will prove to be a useful addition to a modeler’s toolkit.
In any case, whatever modeling framework and language is used, it is clear that we need to
take ambiguity into account, and reason explicitly about it.
There are two extensions of our framework that we have not considered. First, we model
ambiguity by allowing a formula to be interpreted differently by different agents, we assume
that each individual agent disambiguates each formula. That is, no agent says “I’m not sure
how to disambiguate φ. It could correspond to the U of worlds, or it could correspond to U ′ ;
I’m not sure which is right.” As we mentioned earlier, this view is closer to that of Lewis
[1982] and Kuijer [2013]. It would involve a nontrivial change to our framework to capture
this. Second, we have allowed only ambiguity about the meaning of primitive propositions
(which then extends to ambiguity about the meaning of arbitrary formulas). But we have not
considered ambiguity about the meaning of belief; for example, i might interpret belief in
φ terms of having a proof of φ in some axiom system, while j might use a possible-worlds
interpretation (as we do in this paper). Capturing this seems interesting, but quite difficult.
Indeed, even without ambiguity, it is not nontrivial to design a logic that captures various
resource-bounded notions of belief. (See [Fagin, Halpern, Moses, and Vardi 1995][Chapters
9–10] for more on this topic.)
Acknowledgments: We thank Moshe Vardi and the anonymous reviewers of this paper for
helpful comments. Halpern’s work was supported in part by NSF grants IIS-0534064, IIS0812045, IIS-0911036, and CCF-1214844, A preliminary version of this work appeared as
“Ambiguous language and differences in beliefs” in the Principles of Knowledge Representation and Reasoning: Proceedings of the Thirteenth International Conference, 2012, pp. 329–
338. by AFOSR grants FA9550-08-1-0438, FA9550-12-1-0040, and FA9550-09-1-0266, and
by ARO grant W911NF-09-1-0281. The work of Kets was supported in part by AFOSR grant
FA9550-08-1-0389.



In previous work
studied the

[BGHK92, BGHK93], we have
random-worlds approach�a particul ar

(and quite powerful) method for generating degrees

of belief (i.e., subjective probabilities) from a knowl­
edge base consisting of objective (first-order, slalisti­

cal, and defaull) infonnation. But all owing a k n ow l
edge base to contain only objective in form ation is

­

sometimes limiting. We occa<;ionally wish to include
infonnation about d egrees of belief in the knowledge
base as well, because there are contex t s in which old

be1iefs represent importan t information that should
influence new beliefs. In this paper, we describe three
quite general techniques for extending a method that
generates degrees of belief from objective informa­
tion to one that can make use of d egrees of belief as
well. All of our techniques

are

ha<;ed on well-known

approaches, such a5 cross-emropy. We di sc uss gen­

eral connections between the techniques and in partic

­

ular show that, although concept u ally and technically
quite different, all of the t echniques give the same
answer when applied to the random-worlds method.

1

halpern@almaden.ihm.com

Daphne Koller

Computer Science Division
University of California, Berkeley
Berkeley, CA 94720
daphne@cs.berkeley.edu

principle determine if the agent's objective information is
cmTect (by ex amin ing what is actually the case in its envi­
ronment), we cannot so ea<;ily say that its subjective beliefs
are '-Xmcct. The truth or falsity of these pieces of informa­
tion is not detc nni n ed by the state of the environment.
Although su b j ecti ve infonnation could take many differ­

ent l(mns. we will concentrate here on

degrees of belief

arc probabilities that are assigned to fonnulas ex­
pressing objective a sse rti ons. For example, the assertion
"the weather is warm in New York" is an objective one: it

These

is either true or false in the agent's environment. But when

we <L..; sig n a degree of belief to this assertion, as above, we

obtain

a

subjective a�sertion: it becomes a statement about

the state of the agent's be l ie fs. In tl1e context of probability

d is tinction between subjective and objective can

theory the

appear somewhat subtle. be cau se some form of objective
information (such <L<; proportions

or frequencies) obey the

laws of probahility,j ust a<; d o degr ees of belief. Yet the dis­

tincrion can be a significant one if we want to use or interpret

a pmbahilistic theory conectly. Camap's work

[Car50]

is

notewort11y for its care ful distinction between, and study
of, both st at i s tical

degree of belief

probabilities, which are objective, and
probabil it ies, which are subjective.

In order to und erstand

Introduction

vide

a

this distinction, it is useful to pro­

formal semantics for degrees of belief that captures

the difference between them and objective information. As

When we examine the knowledge or information possessed
by an agent, it is useful to distinguish between

subjective

objective information. Objective information is infor­
mation about me environment, whcrca� su�iective informa­
and

tion is infonnation about the state of the agen t s hcliefs. For
example, we might c hara ct erize the infonnation of an agent
'

travelling from San Francisco

to New York as con�isting of
the objective infonnation tlmt the weather is warm in San
Francisco, and the subjecti ve infotmation that the proba­

bility rhat the weather is wann in New York is 0.2. The
important thing to notice here i s tlmt altl10ugh we can in
*This research has been supported in p art by the Canadian

Government through their NSERC nnd IRIS programs, by the

Air Force Office of Scientific Research (AFSC) under Contract
F49620-91-C-0080, and by a University of California President's

Postdoctoral Fellowship. The United States Government is au­
thorized to reproduce nnd distribute reprints for govemmentJI
purposes.

,Joseph Y. Halpern
IDM Almaden Research Center
650 Han·y Road
San Jose, CA 95120-6099

dcmonstraled by Halpern [Hal90], a natural, and very gen­

eral . way to give a
d efi nin g
worlds.1

semantics

to degrees of belief is by

probabi li ty distribution over a set of possible
The degree of bel ie f in a fonnul a r.p is then the

a

probab il it y of the set of worlds where r.p is true. In this
framework we can characterize o�j ective infonnation as
consisting of assertions ( ex pressed as fmmulas} that can
he assi gned a truth value by a single world. For example,
in any given world Tweety the bird does or does not fly.
Hence, the fonnula Fly(Tweety) is objective. Statistical
assertions such as IIFly(x)IBird(J:)II'"

i m atel y

80% of

�

0.8, read "approx­

birds fly", are also objective. On the other

hand, Pr( Fly(Tweety))

=

0.8, expressing the assertion that

1Conceptually. this notion of world is just as in classical
··possibl�-worlds semantics": a complete picture or description
of tht! way the world might he. Formally, we take a world to be
an intcrprctntion (model) for first-order logic.

Bacchus, Grove, Halpern, and Koller

38

the agent's degree of belief in Tweety !l yi ng

is 0.8, is n ot

objective, as its truth is determined by whether or not tlle
pr obability of the set of worlds where Tweety flies is

0.8.

Although we cannot easily characterize an agent's degr ees
of beliefs as being correct or incorrect, it is nevertlleless
clear that these beliefs should have some rel ation to objec­
tive reality. One way of guaranteeing this is to actually

generate them from the objective information available to
the agent. Several ways of doing this h ave been consid­
ered in the literature; for example, [BGHK92, PV92] each

discuss several possibilities. The appr oach es in {BGHK92)
are based in a very natural way on tlle semantics des cr ib e d
above. Assume we have a (prior) probability distribution
over some set of worlds.

We cru1 t11e n generate degr ees

of belief from an objective know ledge ba�e KB by u sin g
standard Bayesian conditioning: to t11e formul a 'P we a�­
sign as its degree of belief the conditional prob ab i lity

of

r.p given KB. In [BGHK92] we considered three particu­
lar choices for a prior, and i nve stigat ed the properties of
the resulting inductive inference sy stem s.

In

[BGHK93]

we concentrated on the simp lest of these methods-the
choice of prior is essen­

random-worlds m eth od-whose

tially the uniform prior over the se t of possible worlds.

More precisely, suppose we re strict our attention to worlds
(i.e., interpretations of ru1 appropriate vocabul ary for first­

order logic) with t11e domain

{ I, . . . , N}.

Assumin g we

have a finite vocabulary, there will be only finitely many

such worlds. Random worlds ta kes a'> the set of worlds all

of these worlds, and uses p erh ap s the s i mpl e st probability
distribution over them-the uniform distribution�thus as­
suming that each of t11e worlds is eq u ally likely. This gi ves
a prior distribution on the set of pos sibl e worlds. We can
now induce a degree of belief i n 'P gi ven KB by using the
conditional probability of 'P

given Kll with respect to this
of

uniform distribution. II is ea-;y to see that the degree

belief in r.p given KB is then s i mply tJ1e fraction of possible
worlds satisfying KB that also sati s fy 'P· In general . how­

ever, we do not know the domai n size N; we know only
that it is typically large. We cm1 therefore appro x i mate the

degree

of belief for the true but unknown

N

by computing

the limiting value of this degree of be l ief as N g rows large.
This limiting value (if it exists, which it may not) is denoted

�( r.piKB), and it is what the rando m -w orld method takes

to be the degree of belief in 'P given

KB. In [BGHK93], we

showed that U1is method posse ss es a number of anractive
properties,

such. as a prefe renc e for more

specific infonna­

tion and the ab i lity to ignore inelevant information.
The random-worlds method Clli1 generate degrees of be­
Iief from rich k nowledge b ases that may contain first-order.
statistical , and default information. However, w; with any

conditioning process, is limited to deal ing

with objective

information. When we add subje cti ve formulas to

KB. we

can no longer simply c ond iti o n on KB: the conditioning
process eliminates those worlds inconsistent with our infor­

mation, while U1e truth of a subjective formula cannot be
determined by a single world.2

Hence, we would like to

2
In the context of random worlds (and in other cases where
the degrees of belief are de termined using

a

prior

on

the

set

of

extend the nmdom-worlds method so as to enable it to deal
with both objective and subjective information.
Why do we wrun to take into account subjective beliefs?
There are a number of situations where this seems to make
sense. For example, suppose a birdwa tcher is interested
in a domain of birds, m1d has an obj ective knowledge base
KBbird con si sting of t11e statistical inf ormation

IICardinal(x)i-.Red(.1�)11x � 0.1
IICardina/(;t)IRed(x)llx � 0.7.

1\

Now the b irdw at cher catches a glimpse of a bird

b flying by

that seems to be r ed. The birdwatcher is trying to decide

if b is a cru·dinal.

By the results of [BGHK93], if the

birdwatcher assumes U1at U1e bird is n ot red, random-worlds

gi ves

1\

Prr�(Cardinal(b)IKBbird

-.Red(b))

::::: 0.1. On the

other hand, if she assumes tlmt the bird is red, we get

Red( b))::::: 0.7.

But it does not

to he able to generate a d eg ree of b eli ef

in Cardinal( b)

P•J.:::(Conlinal(b)IKB,;,-rJ

1\

seem ap prop riate for her to do either; rather we would like
that takes into acco u nt the birdwatcher's degree of belief in

Red( b).

For example, if this degree of

be lief is 0.8, then

we would like to use a kn owled ge ba<>e such as KBb;rd 1\
Pr ( Red( b ) ) = O.R. It seems re asonable to expect that the

resul t ing degree of oclief in

Cardinal(b)

somewhere bet wee n the two e xtremes of

would then be

0. 7 and 0.1.

As ano ther example, suppose we have reason to believe

that two sensors are independent. For simplicity, suppose

the sensors measure temperature, and report it to be either

high, h. or low. I. We can imagine t11ree unary predicates:
S/(.r), indica ting that sensor 1 reports the value x; S2(x),

and Actual ( x ) , i ndi cating
That the sensors are inde­
pen d ent (given the actual value) can be represented by the
:11
co nj unct i o n over all choices for .r:, .1J, and J
i n {I, h} of:

a similar predicate for s ensor
that the act u al

Pr(S/(:t')
=

2;

temperature is ;r:.

1\ S2(.r")IActual(;r))

Pr(Sl(:t')IActua/(;r))

x

Pr(S2(x")IActual(x)).

It co u ld he tha t we h ave dctennined that ilie sensors are
independent through the observation
readings.

of a n umber

of test

Such e mpirical evidence c oul d be summarized

by a stati s t ical assertion �md thus added to our knowledge

base without req u i ring a degree of be lie f statement like ilie

ab ove. However. this is not the normal situation. Rather, we

arc more likely to have based our belief in independence

on other i n form ation. such a'>

our

beliefs

about causality.

For example, the sensors may h ave been built by different
manufacturers.

In this ca�e. it seems most reasonable to

represent this kind of informa tion using an assertion about
degrees of belief.

How, t hen , can we incorp or at e information about degrees of
belief into the ran d om -worlds framework? More generally,
3
given any inference proce,u -i.e., a method for generat­
ing degrees of belief from objective infonnat i on-we would
worlds). this problem can be v iewed as an instance of the general
problem of conditioning a distribution on uncertain evidence.
3The term ··inference process" is taken hom Paris and Vencov­

sko I PVX91. Our framework is slightly different from theirs, but
we think t h i s usage of the term is consistent with their intent.

Generating New Beliefs from Old

like to extend it so that it can also deal with subjective infor­
mation. This is an issue that has received some attention re­
cently [PV92, Jae94b, Jae94a]. We discuss lhree techniques

39

by us in g cross-entropy [KL51]. Given two probability dis­
tributions p and p', the cross-entropy of p1 relative to J.l,
denoted C(1/, 1•), is a meao;;ure of how "far" J.t1 is from J.l

here, and consider their application in the specific context
of random worlds. As we shall see, all of our techniques

a p rior

are very closely based on well-known ideas in the litera­
ture. Two make use of cross-entropy, while the t11ird is a

can t11en find the distribution on worlds satisfying the con­
s train ts tllat minimizes cross-entropy relative to the prior,

generalization of a method considered by Paris and Vencov­

ska [PV92]. They are conceptually and

formally d istin ct

,

yet there are some interesting connections between them.
In particular, in the context of random-worlds they gener­
ally yield the same answers (where the compruison makes
sense; the various methods have different nmges of applica­
bility). Many of the results we discuss are, in general terms

if not in specific details, already known. Nevertlleless, th eir
combination is quite interesting.
We now describe the three methods in a lillie more detail.
The first method we exrunin e is

perhaps the simplest to ex­
t1Je context of random worlds.
Fix N. Random worlds considers all of the worlds that have
domain { 1, ... , N}, ru1d assumes they are equal ly likely,
which seems reasonable in the ab sence of information to
the contrary. But now suppose that we have a degree of
belief such as Pr(Red(b)) = 0.8. In thi s case it is no longer
reasonable to assume that all worlds are equally likely; our
knowledge base tells us that the worlds where b is red are
plain. We consider it first in

[SJ80, Sho86]. Given an inference method that generates

and

and a set of constraints determined by the KB, we

then use t11is new distribution to compute degrees of
We call this method CEW (for cross-entropy on

belief.

worlds).
The next method we consider also uses cross-entropy, but
in

a completely different way. Suppose we have the (ob­

KBbird given above, and a separate
(Pr(Red(b)) = 0.8). As we sug­
gested, if the birdwatcher were sure Umt b was red, random
worlds would give a degree of belief of 0.7 in Cardinal( b);
similarly, if she were sure that b was not red, random worlds
would give 0.1. Given that her degree of belief in Red{b)
jective) knowledge base

."

beli e f

ha<;e" BBbird

is O.R. it seems reasonable to a�sign a degree of belief of
O.R x 0.7 + 0.2 x 0.1 to Cardinal(b). In fact, if we consider
any inference process I ( not necessarily one that generates
a prior prnhahility on possihle worlds), it seems reasonable
to ddine
I (Cardinal( b) I KBhird 1\
=

he equally likely, we
divide the worlds into two sets: those which satisfy Red(b)
and those which satisfy -.Red( b). Our beliefs require that
the first set have probability 0.8 �md the second probability
0.2. But otherwise we can make the worlds within each set
equally likely. This is consi ste nt with the random worlds
approach of making all worlds equally likely. Intuitively,
we are considering the probability dis t ibut ion on the worlds
that is as close as possible to our ori g i nal uniform distribu­
tion subject to the constraint that the set of worlds where
Red( b) holds should have probability 0.8.
tion. Rather than taking all worlds to

r

What do we do if we have ru1 inference

process other than

random worlds? As long as it also proceeds by generating
a prior on a set of possible worlds and then conditioning,
we can deal with at least tl1i s examp le.

We simply use the
ao;;sign relative weights to
the worlds in the sets determined hy Red(b) and -.Red( b),
and then scale these weights wi thin each set so that the sets
are assigned probabi l i ty 0.8 ru1d 0.2 respectively (Readers
familiar with Jeffrey s rule [Jer92] will realize that this is es
sentially an application of that rule.) Again, intuitively, we
are considering the distribution closest to Ute original prior
that gives the set of worlds satisfying Red(b) probabi l ity
prior generated by the method to

.

­

'

0.8.
Unfortunately, the knowledge bao;;e is rarely this simple.
Our degrees of belief often place co mple x constraints on
the probability distribution

over possihle worlds. Never­

theless, we would like to maintain the intuition that we are
considering the distribution closest to the original prior
"

"

that satisfies the constnunts imposed by the KB. But how
do we determine the "closest" distribution? One way is

0.8

x

+

more likely than the worlds where b is not red. Nevenheless,

there is a straightforward way of incorporating this informa­

=

BBbird)
I(Cardinal(b)JKBbirdi\Red(b))
0.2 x I(Cardinal(b)JKBbird A -.Red( b)).

More

generally, we might hope that given an inference
and a knowledge bao;;e of t11e form KB A BB,
we can gen erate from i t a collection of objective knowl­
edge hase� KD i, .. , KB,. suc h tlmr I( 'PIKB /\ BB) is a
weighted average of I(;pJKBI), ..., I(lf'IKBm). as in the
example. In general. ho wever achieving this in a reasonable
fas h ion is not so easy. Consider t11e belief base BB�;rd =
(Pr(Red(b)) = 0.8) 1\ (Pr(.'inw/!(b)) = 0.6). In this case,
we would like to d efi ne 1 (Cardinal( b )JKBbird 1\ BB�;rd) us­
ing a weighted average of I(Cardinal(b)JKBbird /\Red(b) 1\
Snwll(b)), I(Cardinal(b)IKBbint 1\ Red(b) 1\ --,Smnll(b)),
etc. As in the simple example, it seems reasonable to take
the we i ght of the tenn l(Cardinal(b)JKBbird 1\ Red(b) 1\
Small( b)) to he the degree of belief in Red( b) 1\ Smnll(b).
Unfortunately, while ss;,;rd tells us t11e degree of belief in
Red( /1) and Small( b) separately, it does not give us a degree
of hel ief for their conjunction. A superficially plausible
heu ri sti c would he to ao;; sume that Red( b) and Smnll(b) are
independent. and thus assi gn degree of belief 0.8 x 0.6 to
their conjunction. While this seems reasonable in this case,
at other times it is completely inapp rop riate For example,
if our knowledge base asser ts that all small things are red,
then Red( b) and Snwll( b) cannot be independent, and we
should dearly take the degree of belief in Red(b) 1\ Smnll( b)
to he the same a� the degree of belief in Small( b), namely,
0.6. In general, our new degre e of belief for the formula
Red(b) 1\ Small(b) may depend not only on the new de­
grees of be lief for the two conjuncts, but also on our old
process I

.

,

.

degree of belief

!(Red( b)

1\

Small(b )JKBb;rd).

One reason­

to computing t11ese degrees of belief is to
make the smaflesr c hange possible to achieve consistency
with the hclief hase. Here, as before, cross-entropy is a
u seful tool. Indeed, a� we shall show, there is a way of
able approach

40

Bacchus, Grove, Halpern, and Koller

applying cross-entropy in this context to gi ve us a general
approach. We call Ibis method CEF, for cross-entropy on
formulas. Altbough both CEW and CEF use cross-entropy,
they use it in conceptually d i fferen t ways. As the names
suggest, CEW uses cross-entropy to compare two probabil­
ity distributions over possible worlds, while CEF uses it to
compare two probability distributions over formulas. On
the other hand, any probability distribution on worlds gen­
erates a probability distribution on formulas in th e obvious
way (tbe probability of a formula is the p rob abil i ty of the set
of worlds where it is true), and so we can use a well-known
property of the cross-entropy function to observe that the
two approaches are in fact equiv ale nt when they can hmh

be applied.

It is worth noting that the two approach es are actua.lly in­
comparable in their scope of application. Because CEF is
not restricted to inference processes tlJat generate a pri or
probability on a set of possible worlds, it can be app lied
to more inference processes than CEW. On the other hand,
CEW is applicable to arbitrary KB's while, as we shall see,
for CEF to apply we need to make mo re restrictions on the
form of the KB.

the metho ds also agree when appli ed to our version of the
ME process and when applied to random worlds. Putting
t11e results together, we can show that all these metbods­
CEW, CEF, and RS-agree when applied to random worlds
and in fact, CEW and CEF agree in general. In addition,
,

t11e r esu lt ing extension of random worlds agrees with the
approach obtained when we apply CEF and RS to the ME

process.

The rest of this paper is organi zed as follows. In the next

sect i on we r evi ew the formal model of [Ha190] for degrees

of belief and statistical information, and some material from
[13GHK93] regarding the random-worlds method. We give
the formal definitions of t11e three metbods we consider in
S ecti on 3, and discuss their equivalence. In passing, we
also d i sc uss the conn ecti on to Jeffrey's rule, which is an­
other very well known met11od of updating by uncertain
infonnation. We conclude in Section 4 witb some discus­
sion or computational issues and possible generalizations
of lhesc approaches.

2

Technical preliminaries

In Ibis paper, we focus on two instantiations of CEF. The
first applies it to the random-worlds method. The seco n d

2.1

used by Paris and Venco vska [PV89] ( an d simililr in spirit to
the metbod used by Jaeger [Jae94b)). which we henceforth
call the ME (inference) process. Using results of [GHK92,
PV89], we prove that t11ese two instm ll ia l ions are equivalent.

and reaso n with hoth statistical information and degrees of
belief. We hriclly review the relevant mmerial here. We
sta rt with a standard first-order la ngu a ge over a finite vo­
cabulary <�, and a u gment it with proportion expressions
and belief expressions. A basic proportion expression has
the form 1!1/•(;r)!O(J:)IIx and denotes the proportion of do­

applies it to a variant of the maximum-entropy approach

The third method we consider also app l i es only to certain
types of in ference processes. In panicu1ar, it take� a.<. its
basic intuition that all degrees of belief must ultimately he
the result of some statistical process. Hence, it re q u ires an
inference process tlmt cm1 gene rate degrees of belief from
statistics, like random-worlds. S uppo se we have the belief
Pr(Red(b)) = 0.8. If we view th is belief as havin g arisen
from some statistical sampling process, then we can regard
it as an abbreviation for statistical information ahout the
class of individuals who are '�just li ke b". For example, say
that we get only a quick glm1ce at b, so we are not certain
it is red. The above asser ti on could be con strued as being
an abbreviated way of saying that 80% of the objects that
give a similar sense perception are red. To capture this
idea formally we can view b as a membe r of a small set
of (possibly fictional) ind ividual s S that are "'just like b" to

the best of our knowledge, and ass u me that our degrees of
belief about b actually represents the statistical information
aboutS: IIRed(x)IS(x)llr � 0.8. Once all degree ofhelief
assertions have been convert e d into statistical assertions.
we can tben apply any method for in fer r in g degn:es of
belief from statistical knowledge bases. We call this the
RS method (for representative set). The general intuition
for this metbod goes back to s tatistical mechanics [Lan80J.
It was also defined (independently it seems) by Paris and
Vencovska [PV 92); we follow their presentation here.
Paris and Vencovska showed t11 a t the RS method and the
CEF metbod agree when app li ed to their version of the ME
process. Using results of[GHK92, PV89], we can show that

A fit·st-ol'<ler

logic of probahility

In [Hal90), a l o g ic is presente d that allows us to represent

main elements s atis fyin g 1j; from among those elements
satisfyin g B. (We take 111/;(.1:)11., to be an abbreviation for
11�1·( ;r) lmw(.r )11., . ) On the other hm1d, a basic beliefexpres­
.

.l.ion has

the form

Pr( 1/J IO) m1d denotes tbe agent's degree
0. The set of p roport ion (resp. belief)

or belief in 11• g i ven

expressions is formed by ad din g the rational numbers to the
set of basic proportion (resp. belief) expressions and then
closing off und er addition m1d mu lt ip lica tio n
.

We compm·e two pro por ti on expression s using the approx­

( ap pro xim at ely less than or equal");
the result is a proportion formu l a. We use { � {' as an
abbreviation for ({ ::5 e) 1\ ({' ::5 0· Thus, for example,
we can express the stateme nt "90% of birds fly" using the
proportion formula IIFly(x)IBird(x)llx � 0.9.4 We com­
pa re two belief expressions using standard �; the result is
a basic belief{ormula. For example, Pr(Re d(b)) � 0.8 is a
ba�ic belief form ula . (Of course Pr(Red(b))
0.8 can be
expressed as the o hv io us c onjunct ion ) In the full language
£ we allow arbitrary llrst-order qu ant i fication and nesting
of belief and proportion formula'>. For example, complex
imate connective ::5

"

,

=

.

fonnulas like

in£.

Pr{V:t(I!Knmvs(:r., y)IIY ::5

0.3 ))

� 0.5

are

4We remark that in [Hal90] there was no use of approximate
equality

(�).

We use it here since, as argued in [BGHK93], its

use is crucial in our intended applications. On the other hand, in

[DGI !K93], we used a whole family of approxima te equality func­

tions of the form�;. i
we use only one here.

=

1, 2, 3, . .

..

To simplify the presentation,

Generating New Beliefs from Old

We will also be interested in various sublanguages of £ . A
fonn ul a in which the "Pr" operator does not appear is m1
objective formula. Such fonnulas are assigned truth values
by single worlds. The sublanguage restricted to objective
fonnulas is den oted by c obj . The standard rand om- worl ds
method is restricted to knowledge bases expressed in cobj .
l
The set of belief formulas, c oe is fanned by starting with
,
basic belief formulas and closing off under conju nction ,
n eg ati on , and first-order quantification. In contra<;t to ob­
j ective fonnulas, the truth value of a be l ief fonnula is com­
pletely i ndepen dent of the world where it is evaluat ed . A
flat formula is a B ool ean combination of belief formulas,
such that in each belief expression Pr( 'P ) , the formula 'P is a
closed (i.e., con taining no free variables) obj ective form u la.
(Hence we have no nesti nr, of "Pr" in nat formulas nor any
"quantifying in".) Let £! at be the language consisting of
the fiat fonnulas.
give semantics to botl1 proportion form u l a-; and belief
formulas, we use a special case of what were called in
[Hal90] type-3 structures. In part i cu l ar, we consider type-:�
structures of the fonn (WN , p.), where WN co n si s t s of al l
worlds (first-order models) with domain { I , . . . , A' } over
the vocabu l ary <ll , and Jl is a probabi l i t y distribution over
WN .5 Given a structure and a world i n that structure. we
eval uate a proportion expression 1 1 1/•( :�: ) IO ( .r ) I I .,· as the frac­
tion of domain el emen ts sat isfying 1,1>( ;I' ) among those sat­
isfying O(x ) . We evaluate a belief form u l a us i n g our proha­
bi1ity distribution over tl1e set of possib l e worlds. More pre­
cisely, given a structure M = (WN , 11), a world w E WN ,
a tolerance r E (0, I] (u sed to i n te rpre t ::::::: and ::S ) , and a
val u ati on V (used to i n terpret the free variables), we asso­
ciate with each fonnula a tru tll value and with each belief
expression or proportion expression ( a numher [(j M , ,v, r .
We give a few representative clauses here:

To

w

•

•

If ( is tlle proporti on expression I I \0( .1:) I �{1:) I I ,. , then
[(]M,w , v , r is the num ber of domai n elements i n w sat i s­
fy i ng \0 1\ 1/; d i v i de d hy the numher sat i s fyi n g ''' · (Note
that these n u m bers may depend on w . ) We take this
fraction to be 1 if n o domain elements sat i s fies ·� · .
I f ( is

the belief expression Pr( 'P I I/' ) , then
_

[(] M,w ,V,r Again,
•

p { tv1 : ( M , w' , V, r ) )= 'P 1\ !}• }
p { w' : ( M, w1 , V, r ) )= 1}1}

·

we take this to he 1 i f the denominator is 0.

( m1d ( ' are two proportion
( M , w , T , V) I= ( ::S (' iff

If

expressions.

[(]M,w , r , V :S [(1] M , w , r ,V

+

th e n

T.

That is, approximate less than or eq ual allows a tolerance
of r .

( is a belief expression, th en i t s value i s
in dependen t of the world w. Moreover, i f it is cl osed then
its value is in dep enden t of the val u a t i on V. Thus, we can
write [(]M,, in tllis ca..o;e. Similarly, if <p E £11'1 is a cl o sed

Notice that if

5

In general, type-3 structures

addition a lly allow for a distribu­
{ l , . . . , N} ) . Here, we always

tion over the domain (in this case,

use the unifonn d isui b u ti on over the doma in.

belief form u l a,

its truth depends only on M and r,
)= cp i n this ca<;e.

can write ( M, r )
2.2

41

so we

The random-worlds method

Given these semantics, tlle random-worlds method is now
easy to describe. S uppose we have a KB of objective for­
mula�. and we wrull to a<;sign a degree of belief to a fonnula
tp . Let p'f.v be the uniform distribution over WN , and let
w
M!V = (WN , Jlf.r ). Let Pr»r (cpi K B) = [Pr(cpjKB)]MN , r ·
Typically, we know on l y tl1at N i s large and that r is small.
Hence. we approximate the value for the true N and r by
ddining

P1-: ( 10lKB)

=

lim lim Pr»rw(cpjKB),
N -+-CXJ

-r - 0

a-;suming the limit exists. P1-: ( cpiKB) is the degree ofbelief
in 'P given KI3 acc ord i ng to the random-worlds method.
2.3

Maxi m u m entl'Opy and cross-entropy

The entropy or a probab i l i t y d is tribu ti on Jl over a finite space

n

Lw E n /l ( w ) ln(,l(w ) ) . It ha� been argued [Jay78]
the amou n t of "information" i n a
probabi l i t y di stri b u t i on, i n t h e sense of information tlleory.
The uniform distri bution has the maximum possible en­
t rop y. I n general, g i ven som e constraints on the probability
is -

that entropy measures

with maximum entropy that
viewed as the one that i ncor­
i nfonnation above and beyond

distributions, the distri b u t i o n

satisfies t h e c onst rai n ts cllil be
porates the least addi t i onal
the constrai nts.

The related cross-entropy function measures the additional
information gained by mov i n g from one di stribu ti on Jl to
another uistri hu t ion Jl1:
( ' ( I' , , Jt )
.. ·

=

'\"" '
tl (w)
)
-.
L 1'· ( w In p (w )

wEn

Various arguments have been pre sent ed showing that cross­
meas u res how close one probability distribution is
to ano t he r [S.TRO, S ho86) . Thus, given a prior distribution
I' and a set S' of add i tio nal constraints, we are typically
i n t ere st c u i n th e unique distribution 1/ tllat satisfies S and
minimizes C(p' , p ) . It is well -known tlmt a su fficient con­
dition for such a u n i q u e distribution to exist is that the set
of di stribu tions sat i sfyi n g S form a convex set, and that
t here be at le<L<;t one d i stribution p" sati sfying S such that
C(IJ'' , I ' ) is fi n i te . These conditions often h old in practice.

entropy

3
3.1

The three methods
CEW

As w e ment ioned in

the introduction, our first metllod,

CEW, assumes as input an inference process I that proceeds
by generati ng a pri or Jl f on a se t of possible worlds W1 and
t h e n conditioning on th e objective i n formati on . Given such
an inference process I , a k n owl ed ge base KB (that can con­
tain subjective infonnation) and an objective formula cp, we
wish t o co m p u te CEIV ( I)(<piKB), where CEW(I) i s a

42

Bacchus, Grove, Halpern, and Koller

new degree of belief generator that can handle knowledge
bases that can include subjective infmmation.

We say that an inference process I is world-based if there is
some structure M1 = (Wr , llt )and a tolerance r such that

l(�PIKB) = [Pr(lf'IKB)]M 1,T . Notice that Pr;_;rw is world­
based for each N (where the structure corresponding to
Pr;_;rw is MjV ). �, on the other hand, is not world-ba<;ed;
we return to this point shortly.

Given a world-based inference process I, we define
as follows: Given a knowledge base KB which
can be an arbitrary formula in the full language .C, let Jt.}8
be the probability distribution o n WI su ch t11at C(Jl}8, JlJ )
is minimized (if a unique such distribution exists) among all
distributions tl such that (WI , p' , T ) I= Pr(KB) = l. Intu­
itively, pf8 is the probability distribution closest to t he prior
Jli that gives KB probability l . Let Mf8 = ( WI , JLf8). We
can then define CEW(I)( c.oiKB ) = [Pr( cp)J.�1Ks ' T .

CEW(I)

I

The first thing to observe is that i f KB is objective, then
standard properties of cross-entropy can he used to show
that pf8 is t11e condilional distri b u t i o n I'· I C l KI3 ) . We thus
immediately get:
Proposition 3.1: lfKB is objective, then CEW( I ) ( <.p j K I3 )
l(lf' I KB ) . Thus, CEW(I) is a true extension of !.

=

Another important property of CEW fo llow s from the wel l ­
known fact that cross-entropy generalizes Jeffrey 's rule
[Jef92]. Standard probability theory tells us that if we
start with a probability function p an d observe that event
E holds, we should update to the conditional probabi lity
function Jl. ( · I E). Jeffrey's rule is mean t to ueal w i th the
possibility that rather than get ting certain i n formation. we
only get partial information, such a<; that E hold s with proh­
ability o:. Jeffrey's rule suggests that in tl1 i s case. we should
update to the probability function p' such that

tt'(A)

=

o:JI.( A I E ) + ( I - rr )p ( A IE) .

where E denotes the complement of E. 111 i s rul e uniformly
rescales the probabilities within E and (separately) those
within E so as to satisfy the constrain t Pr( E) = a. Clearly,
if o: = 1 , then Jl1 is j ust the conditional prohab i l i t y p( · I E ) .
This rule can be generalized i n a strai ght forward fashion .
If we are given a family of mutually ex c l u si ve and ex­
haustive events £1 ,
, Ek with de si red new prohahi l i t ies
0: 1 , . .
, ak (necessarily L:i c.r; = 1 ) , t h en we can define:
.

.

•

.

p'(A)

=

a , lt(A I El )

+

·

·

·

+

ni\ I'·( A I Ek) .

Suppose our knowledge base ha�; t he form ( Pr( 'PI ) = n 1 ) A
· · · A (Pr( lf'k ) = a k ) , where the If'; 's are mu tually exclusive
and exhaustive objective fonnulas and c.r 1 + · · + cr., = I .
The formulas lf' l , . . . , lf't.: conespond to mu tually ex clu sive
and exhaustive events. Tilus, Jeffrey's rule would suggest
that to compute the degree of belief in If' g i ven this knowl­
edge base, we should compute tile degree of belief in 'P
given each of the If'; separately, and then take th e linear
combination. Using t11e fact Umt cross-entropy generalizes
Jeffrey's rule, it is immediate that CEW i n fact docs t h i s .
·

Pr·oposition 3.2: Suppose that I is a world-based inference
process and that KB ' is of the form KB A BB, where KB
is objective and B B has the form (Pr( 1;? 1 ) = o: t ) A · A
(Pr( lf'k ) = a t.: ) , where the lf'i 's are mutually exclusive and
exhaustive objective formulas and cq + · · · + o:k = 1. Then
·

·

k

CEW( I )(c,oi KB ' )

=

L o:; I ( If'I KB 1\ cp; ) .
i=l

As we observed above, CEW as stated does not apply di­
rectly t o U1e random-worlds method Pr:. since it is not
wor l d - based . It is, however, the limit of world-based meth­
ods . (This is also true for the ot11er methods considered in
[BGH K92].) We can ea<;ily extend CEW so that it applies
to limits of world-ba<;ed methods by taking limits in the
obvious way. In particular, we define
CEW( Pr':;;;; H 'f' I K B )

=

l i m lim
r - 0 N - oo

CEW(Pr;,;rw)(�PIKB ) ,

provided t he limit exists. For convenience, we abbreviate
CEW ( Pr:; ) as Pr�w .

note that the distribution defined by
d i s tri bu t i on of maximum entropy that
sati sfies the constraint Pr( KB) = l . 1l1is follows from the
observation that the distri bution that minimizes the cross­
entropy from the u n i form distribution among those distri­
butions sa t i s fy i ng some constraints S', is exactly the distri­
bution or max i m um entropy satisfying 5.6 This maximum­
entropy characterization demonstrates Umt Pr�w extends
ran dom worlds by making the probabilities of the possible
worlds "<t<; equal as possi b l e" given the constraints.
It is i n teresting to

CEW( Prrt\� )

3.2

is the

CEF

Paris and Vencovska [PV89] consider inferences processes
that arc n o t world-based, so CEW cannot be applied to
them. The method CEF we now define applies to arbitrary

but requ i res that the knowledge base
form. For the remainder of this section,
we assume that the knowledge ba..;e has the fonn KB 1\ BB,
where KB i s an objective formula and B B (which we call
the hclicr base) is in £11" ' .
inference processes.
be or a re st ri c t ed

BB is of UJe form Pr( th ) =
U1e t/J; 's were mutually exclu­
sive, then we could define C E F ( I)( lf'IBB) so that Propo­
si t io n 3 . 2 he l d . B u t what i f the �;; 's are not mutually exclu­

First. suppose for simplicity that

d1

!\

·

·

·

A

Pr( V-'d

=

Pk · If

sive?

2 !' atoms over �!J ,

, 1/Jt.:. i .e., those
where each 1/J: is
ei t her ·rJ>; or · ·I/J1 • Atoms are always mutually exclusive
and ex h au s t i ve ; so, if we could find appropriate degrees
of helier for these at om s, we could again define things so
that Propo s i t i o n 3.2 holds. A simple way of doing this
Consider the r\·

conj u nct ions o r

=

the form '1/•;

1\

. . .

. . . A l/JI, ,

6We remark that in [GHK92, PV89] a connection was es­
lahlished between random worlds and maximum entropy. Here
maximum entropy is playing a different role. It is being used here
to extend rnndom worlds rather than to characterize properties of
random worlds as in [GHK92, PV89).

Generating New Beliefs from Old

would be to assume that, after conditioning, the a<>sertions
'if;; are independent. But, as we observed in the i ntroduction,
assuming independence is inappropriate in general .
Our solution is to first employ cross-en trop y to find appro­
priate probabi l i ti es for these atoms. We proceed as follows.
Suppose I is an arbitrary i n feren c e process, 1313 E C f1a t ,
and 1/J1 , . . . , 1/Jk are the formulas that appear in subexpres­
sions of the form Pr( 'if;) in B B . We form the K = 2k at oms
generated by the 'if;; , de no ting them by A 1 , . . . , A K . Con ­
sider the probab i l ity Jl defin ed on the space of atoms via
t-t(Aj ) = I(Aj [ KB ) .7 There is an obvious way of defining
whether the formula B B is satisfied by a probability distri­
bution on the atoms A 1 , . . . , Ak (we defer t11e formal details
to the full paper), but i n gen eral B B will not be sat isfied by
the distribu tion 11. For a si mpl e example, if we take the
in ference procedure to be random worlds and consider the
knowledge base KBb;nJA(Pr(Red(b)) = 0 . 8 ) from the intro­
duction, it turns out that P1-: ( Red ( b ) [ K B ,;,-d ) is around 0 . 5 7 .
Clearly, the di stri bu tio n 11 such that p.(Red( b ) ) i s around
0.57 does not satisfy t11e constraint Pr(Red( b ) ) = 0 . 8 . Let
Jl' be the probability d i stribution over the atoms that mini­
mizes cross-entropy relative to p among those t hat satisfy
BB, provided tl1ere is a unique such distribution. We then
define

CEF(I) ( <p[KB A B B )

=

1-" ( A l )I( 'P [ KB A A ! )

+ · · ·+

JJ.' ( A K ) ! ( <p [ KB A A g )

It is immediate from the definition that CEF( I )
Formally, w e h ave
Proposition 3.3:

I('P [ KB).

lf KB, 'P

E

_

extends

/.

c�<j , then CEF( / ) ( ;p [ K B )

=

Both CEW and CEF use cross-entropy. However, t h e two
applicatio ns are quite different. In the cm;e of CEW, we
apply cro ss- en trop y witl1 respect to probabi lity di s t ri bu ­
tions over pos si ble worlds, whereas with CEE we apply i t
to probabi lity distributions over formulas. Nevert hel e ss , as
we mentioned in the i n trod u c ti o n , there is a tight connection

between the approaches, since any probahi l i t y distri bution
over worlds defines a proba bi l i t y distribution over fonn u ­
las. In fact the following eq u i val e n c e can be proved, usi n g
simple properties of tJJe cross-emropy fu nct i on.
Theorem 3.4 :
Suppose I i s a world-based i�[erence
process, KB , <p E co"i , and B B E [fl•< � .
Then
CEW(I)(�P I KB A B B ) = CEF( / ) ( 'P [ KB A B B ).

Thus, CEF and CEW agree
defined.

i n contexts w h ere both are

By analogy to tl1e defin iti on for CEW, we define
Pr��' (cp!KB A BB)

=

lim

lim

r-O N - ro

CEF( Pr�rw ) ( <p [ KB A BB ) .

It immediately follows from Theorem 3 .4 that
7

S ince

BB

Pr cunnot be nested in
1/J'S are necessa1ily objective, an d �o are the

E c,Jlat by assumption, an d

a flat b e lief base, the

atoms they generate. Thus,

l( A1 [KB)

is well defined.

43

Corollary 3.5: {[ KB , IP E Co"i , and BB E [flat, then

Pr�w ( 'P [ K B A B B )

=

Pr;;r(�t?[KB A BB).

As the no t at i on suggests, we view
of rr: obt ai n ed by applying CEF.

Pr�F as the extension

Why did we not define
Pr�F as CEF(Pr: )? Cl earl y CEF(rr,:; ) and Pr�" are
closely related. Indeed, if both are defined, then they are
equal .

ff both CEF(Pr: )(<piKB A BB) and
Pr�r ( 'P I KB A B B ) are defined then they are equal.

Theon:m 3.6 :

It is q u i te p o ssi ble , in ge n e ral , that eitller one of Pr�" and
defmed while the other is not. The fo l l owin g

CEF( Pr� ) is

example demonstrates one type of situation where Pr�"
is dell ned and CEF( PrC: ) is no t. The converse situation
typical ly ru·ises only in pathological examples. In fact, as
we show in Theorem 3 .8, there is an i mportan t class of cases
where the existence of CEF(Pr:; ) g u arant ees that of Pr�F.
Suppose KB is [[Fly( x ) [Bird(x)[[ r �
I A Rird ( Tweety) and B B is Pr(Fly(Tweety) = 0) A
Pr(Red( Tweety) = 1 ) . Then , just as we would expect,
Pr�,"'(Red( lll'eet v) [ KB 1\ BB) = 1 . On the other hand,
CEF( P1�.,: ) (Red( 1\veety ) [ KB A BB) is undefined. To see
why, let ;t be the probabi lity distribution on the four atoms
ucfineu by Fly( 7\veery) and Red ( Tweety) determined by
Pts-:;:: ( · I K B ) . S i nce Pts� ( Fly( Tweety)IKB) = I , it must
he t he case that 11( Fly( Tweety) ) = 1 (or, more accu­
rately, ; t ( Fiy(7iveety) A Red( Tweety ) ) + p(Fly(Tweety) A
-.Red( liveetv) ) = l ) . On th e other hand, any distri­
but ion 1/ over the four atoms defined by Fly(Tweety)
and Red( 7iveety) that sati s fi es BB mu st be s uch that
it' ( Fiy( 7iveety ) ) = 0. It ea<;ily fo l low s that if p' sat­
isfies B B , then C ( p ' , I') = oo .
Thus, there is not a
unique u i st ri h u t i o n over the atom s that satisfies BB and
mi n i m i zes cross-entropy relative to p . This means that
CEI,.( Prr;: ) ( Red( 7\veety ) I KB 1\ B B ) i s undefined. I
Example 3.7 :

We nex t consider what happen s

when we instantiate CEF
process considered by Paris and
VL:ncovska that u ses max i mum e nt rop y [PV89]. Paris and
Vcncovsk a restrict at t e n t i on to rather simple lang u ag es , cor­
respond ing to the notion of " ess e n tial l y propositional" for­
m u l as de f i n ed below. When co ns i dering (our variant) of
the i r method we shal l make the same restriction.
w i th a p art i c ul a r inference

We say that 1/• ( :r ) is an essentially propositional formula
if it i s a quantifier-free fi rst - ord e r formula that mentions
only u nary predicates (and no con stan t or function sym­
bols), whose onl y free variable is x . A simple kno wl ­
edge base K B abollt c ha<> t he form ll'Pt ( x ) IO, ( x ) ll r ::S
o ,

1\

.,_., , , .

. .

. .

- A [ [ <p k ( :r ) [ fh ( x ) [ [ x ::S O:k 1\ 1/;(c), where
. . . , Ok , .,_;, are all essen t ial ly propositional.8

, 'P k , O , ,

The M E i n fe re nce process

is only defined for

a

simple

M Nolice that II'P( x ) fB ( 1: )f[.
t o- is expressible as
11-.'P{-" ) [ B ( :r: ) f l x :::: I - a ; th is means we can also express :::::: .
I Iowcvcr. because of the fact that we disallow negations in a sim­
ple KB . we cannot ex press s trict i ne qu al i ty. This is an important

rcstric lion.

44

Bacchus, Grove, Halpern, and Koller

knowledge base about c and an essen t i al l y propositional
query <p( c) about c. Let KB :::: KB' 1\ ¢ ( c ) be �m essen t ial l y
propositional knowledge base abou t c (where KB' is the
part of the knowledge base that does not mention c). I f the
unary predicates that appear in KB are P = { Pt , . . . , Pk } ,
then KB' can be viewed as pu tti ng constraints on the 21:
atoms over P .9 The fonn of KB' en s ures that U1ere will
be a unique distribution llme over Ulese atoms Ulat maxi­
mizes entropy and sati sfies the constraints. We then de­
fine ME( <p (c) I KB' 1\ Jj;(c)) to be P me ( lf' l 1/! ) . I n t u i t ively, we
are choosing the d istribution of maximum entropy over the
atoms that satisfies KB' , and treating c a<; a "nmdom" ele ­
ment of Ule domain, assuming i t satisfies each at om over P
wiili Ule probabil i ty dictated by Jlme ·
To apply CEF to ME, we also need to put restrictions
on tlle belief base. We say that D B E .c Jra t is an es·

sentially propositional belief base about c i f every basic
proportion expressi on has the fo nn Pr( y( c ) I O ( c ) ) , w here
<p and B are essen t ial ly p roposi t ional (In p art icu lar, this
.

disallows statistical fotmul a<;
simp le belief base about c is

Pr(<pt (c)IBt (c)) ::;

O'J

in the

a

scope
conjunction

1\ · · · Pr( <pl: ( c) I Ok { c) )

of

Pr.)

o f the

::;

ok.

A

form

w h ere

all of tlle fonnulas th at appear are essential l y proposi tional.
We can only apply CEF to ME if the knowledge ba�e has
the fonn KB 1\ BB, where KB is a simple knowledge hase
about c and B B i s a simp l e belief ba�e abou t c. It fo l lows
from results of [GHK92, PV89] that random worlds and
ME give the same results on th ei r common domain. Hence,
they are also equal after we appl y tl1e CEF transformation.
Moreover, on Ulis domain, if CEF(Pr� ) is de fi ned , then
so is Pr�F. (The con verse docs not hold, as shown hy
Example 3 .7.) Thus, we get
Theorem 3.8: If KB is a simple knowledge base about (
BB is a simple belief base about c, and '-P is on essenTially
propositionalformu!a, then
",

CEF(ME)(<p(c) I KB A B B ) :::: CEF( Prr,.: ) ( '-P ( c ) I K D A I3 I3 ) .
Moreover, ifCEF(ME)( <p ( c ) I KB
CEF(ME) (<p(c) IKB A BB)

=

A

B B ) is defined, then

Pr�F('-P( c ) I KB A B B ) .

3.3 RS
The last method we consider, RS, i s based on t he intu­
ition that degree of belief a�sertions must u l timately ari se
from statistical statements. This general idea goes hack to
work in Ule field of statistical mechanics [LanRO), whe re
it has been applied to the problem of reasoning abou t the
total energy of physical systems. If the system consi sts of
many particles then what is, in essence, a random-worlds
analysis can be appropriate. If the energy of the system is
known exactly no conceptual problem arises: some possibl e
configurations have the specified energy, while others are
impossible because they do not. H owever it turns out that i t
is frequently more appropriate to assume that all we know i s
the expected energy. Unfortunately, i t questionable whether
,

9 An

mulas

atom over P is an atom (as de fi ned ahove ) over the for­

P1 ( x ) . . . . , Pk ( x ) .

this i s real l y an "objective" a-;sertion about th e system in
que.� tion , 1 0 and in fact the physicist<; encounter a problem

analogous to that which motivated our paper. Like us, one
response they have con si dered is to modify the assump­
t i o n of unifonn probability and move to maximum entropy
(thus u si n g essent i al l y, an in st ance of our CEW applied to
a uniform prior). But another response is the fol lowing.
Ph ysi cal l y, expected energy is appropriate for systems in
th ermal equilibrium (i e , at a con stant temperature). But
in practice this means Ulat Ule s yste m is in Ulennal contact
with a (general l y much larger) system, sometimes called a
heat bath. So another approach is to model the system of
i n teres t a> being part of a much larger system, including
the h e a t bath, whose total energy is truly fixed. On Ulis
l arger scale, random-worlds is on ce again applicable. B y
ch oosi ng the energy for the total system appropriately, Ule
ex pec t ed energy of the sm all subsystem will be as speci­
fied . Hence. we have converted s u bjective statements into
objective ones, so that we are able to u se our standard tech­
niques. In this domain, there is a clear physical intuition
for the con nec t i on between t11e objective infonnation (Ule
,

.

energy of the

exp ec t ed

.

heat bat h ) and the subjective infonnation (the

energy of the

small system).

A more recent, and q u i t e

di fferent, appearance of Ulis intu­

ition is in t he work of Paris and Vencovska [PV92) . They
dell ned t h ei r method so that i t ha<> the same restricted scope

ME method. We presen t a more general version
here, that can handle a somewhat richer set of knowledge

as th e

bases. al though i t s scope

is still more restricted than CEF. It

can deal with arbitrary inference proces ses,

but Ule knowl­
BE, where KB is
o bj ec ti ve and D B is an essentially p ro po s i ti onal belief base
ah o u t some co n s tant c . The first step in the method is to
transform 13 I3 in t o an objective formula. Let S be a new
unary predicate, representing the set of individuals ·�ust
l i ke c" . We transform BD to KBnn by replacing all tenus
of the form Pr( 1/J( c)ID(c)) by 1 1 1/! ( x ) IB(x) 1\ S(x)ll:t-. and
rep l aci n g all occmTences of ::; by � . We then add Ule
cnnj u ncts I I S( :r l l l, ::::: 0 <md S(c), s ince S is assumed
For example,
to he a small set and c must be in S.
if D B is Pr(Red(c)) ::; 0.8 A Pr(Small(c)) :::: 0.6, then
the nmcsponding KBnn is ( I I Red(x) I S(x)llx � 0.8) 1\
( /ISmu//( :r J IS(:r ) ll, � 0.6) 1\ ( / I S(.z: ) l /�· ::::: 0) I\ S(c). We
then define R S ( ! ) ( <p( c ) IKB A B B ) = I( IP ( c) I KB A KB aa ) .
I t is al most imme d ia t e from the d efin i ti ons that if B B is
a si mple belief base ahout c , then RS(Pr,: )(<p(c) I KB A
D B ) = li mr o limN - = RS(Pr;r)(<piKB). We abbrevi­
a t e R S ( Prr;;: ) as Pr� .
e d g e hase must have the form

KB

1\

-

RS and

CEF are distinct.

This observa­
of [PV92) concerning an infer­
ence process CM, show i ng that RS(CM) cannot be equal
to CEF( CM ) . On th e other hand, Uley show Ulat, in
the restricted setting in which ME applies, RS(ME)
CEF( ME ) . S i nce ME = Pt� in this se t t ing we have:
In

general,

tion fol l ows from results

,

if'lf it is objective. it is most plausibly a statement about the
average energy over time. While this is a reasonable viewpoint, it

does not rea l l y escape from philosophical or technical problems

either.

Generating New Beliefs from Old

Theorem 3.9: If KB

is a simple knowledge base about

c,

tiona! Conference on Artificial Intelligence
(AAA! '92), pages 602-608, 1 992.

B B is an essentially propositional knowledge base about c,
and tf; is an essentially propositional formula, then

CEF(rc:)(<p(c) IKB /\ B B ) "" CEF(ME)(<p(c) ! KB II B B )
= RS(ME)(<p(c) ! KB II B B ) = Pr� (<p(c) I KB II B B ) .
4

Discussion

A . .T. Grove, J . Y. Halpern, and
D. Koller. Statistical foundations for default
rea<;oning. In Proc. Thirteenth International
Joint Conference on Artificial Intelligence (!J­
CA! '93), 1 993.

[BGHK93] F. B acchus ,

[Car50]

that they can deal with degrees of belief. We

view the fact that the three methods essentially agree when

[GHK92]

�Uld RS a;;su m e cert ain res t ri c ­
tions on the form of the knowl e d ge base, which are not
assumed in CEW. Is it possible to ex t en d these methods
so that they apply to more gen eral knowledge bas e s ? In

M . J aeger. A

350,

facts to KB

[Jac1J4b1

prevents

appl icatio n of

U1is idea

belief bases about some const an t
lems we have found trying to

that

prob­
do this seem dirtlcult but

be viewed as
U1ru1 seq ue ntial updat ing here. Sup­
pose our knowledge base co ntains two constraints:
Pr(<pt ) = u 1 1\ Pr(<p2 ) = o·2 · Although we cannot u s u ­
a l l y apply Jeffrey's rule to such a conj unction, we can a p ­
ply the rule seq uentially, first u pdating by Pr( <p1 )
and then by

Pr( <p2)

=

et 2 • We have

= n1 ,

described o u r meth­

ods in ilie context of u p d a t i ng by any set of constrai n t s
at once, but they can also b e defined

to update b y con­
straints one at a time. TI1e two possibilitie.� usual ly give
different results. Sequential updating may not p reserve
any but the last constraint used, and in general is order
dependent. Whether this should be seen

a-; a problem

doing

is wh y this issue can

be

ig nored when

B ayesian condi tionin g in general, and in ordinary

[ KL5 1 l

S . K u l lback and R . A. Leibler. On informa·
t i o n and su fficiency. Annals of Mathematical

bridge, 1992.

Swtistics, 22: 76-86, 195 1 .

[LanXO]

[BGHK92]

F. B acchus,

Physics, volume 1.

.1 . B . Paris and A. Yencovska. On the appli­
ca bi l i t y of max i m u m entropy to inexact rea­
son i n g . International Jo urn a l of Approximate

[PY02]

J.

[ S ho86 ]

J.

Reasoning, 3 : 1 -34, 1 989.

B. Paris and A . Vencovska. A method for up­
dat ing j u st i fy ing minimum cross entropy. In­
ternational Journal qf Approxi mate Reason·
ing, 1 -2: 1 - 1 8, 1 99 2.

E. Shore. R el ative entropy, probabilistic in­
ference, and AI. In L. N . Kanal and J . F. Lem·

mer, edi tors, Uncertainty in Artificial intelli­
gence. North -Holland, Amsterdam, 1 986.

[S.I80]



We consider a setting where an agent’s uncertainty is represented by
a set of probability measures, rather than a single measure. Measureby-measure updating of such a set of measures upon acquiring new information is well-known to suffer from problems; agents are not always
able to learn appropriately. To deal with these problems, we propose using weighted sets of probabilities: a representation where each measure is
associated with a weight, which denotes its significance. We describe a
natural approach to updating in such a situation and a natural approach
to determining the weights. We then show how this representation can
be used in decision-making, by modifying a standard approach to decision making—minimizing expected regret—to obtain minimax weighted
expected regret (MWER). We provide an axiomatization that characterizes preferences induced by MWER both in the static and dynamic case.

1

Introduction

Agents must constantly make decisions; these decisions are typically made in a
setting with uncertainty. For decisions based on the outcome of the toss of a fair
coin, the uncertainty can be well characterized by probability. However, what
is the probability of you getting cancer if you eat fries at every meal? What if
you have salads instead? Even experts would not agree on a single probability.
Representing uncertainty by a single probability measure and making decisions by maximizing expected utility leads to further problems. Consider the
following stylized problem, which serves as a running example in this paper.
∗ The authors thank Joerg Stoye for useful comments. Work supported in part by NSF
grants IIS-0534064, IIS-0812045, and IIS-0911036, by AFOSR grants FA9550-08-1-0438 and
FA9550-09-1-0266, and by ARO grant W911NF-09-1-0281.

1

cont
back
check

1 broken
10,000
0
5,001

10 broken
-10,000
0
-4,999

Table 1: Payoffs for the robot delivery problem. Acts are in the leftmost column.
The remaining two columns describe the outcome for the two sets of states that
matter.
The baker’s delivery robot, T-800, is delivering 1, 000 cupcakes from the bakery
to a banquet. Along the way, T-800 takes a tumble down a flight of stairs and
breaks some of the cupcakes. The robot’s map indicates that this flight of stairs
must be either ten feet or fifteen feet high. For simplicity, assume that a fall of
ten feet results in one broken cupcake, while a fall of fifteen feet results in ten
broken cupcakes.
T-800’s choices and their consequences are summarized in Table 1. Decision
theorists typically model decision problems with states, acts, and outcomes: the
world is in one of many possible states, and the decision maker chooses an act,
a function mapping states to outcomes. A natural state space in this problem is
{good,broken}1000 , where each state is a possible state of the cupcakes. However,
all that matters about the state is the number of broken cakes, so we can further
restrict to states with either one or ten broken cakes.
T-800 can choose among three acts: cont: continue the delivery attempt;
back : go back for new cupcakes; or check : open the container and count the
number of broken cupcakes, and then decide to continue or go back, depending
on the number of broken cakes. The client will tolerate one broken cupcake, but
not ten broken cupcakes. Therefore, if T-800 chooses cont, it obtains a utility of
10, 000 if there is only one broken cake, but a utility of −10, 000 if there are ten
broken cakes. If T-800 chooses to go back , then it gets a utility of 0. Finally,
checking the cupcakes costs 4, 999 units of utility but is reliable, so if T-800
chooses check , it ends up with a utility of 5, 001 if there is one broken cake, and
a utility of −4, 999 if there are ten broken cakes.
If we try to maximize expected utility, we must assume some probability
over states. What measure should be used? There are two hypotheses that T800 entertains: (1) the stairs are ten feet high and (2) the stairs are fifteen feet
high. Each of these places a different probability on states. If the stairs are ten
feet high, we can take all of the 1, 000 states where there is exactly one broken
cake to be equally probable, and take the remaining states to have probability
0; if the stairs are fifteen feet high, we can take all of the C(1000, 10) states
where there are exactly ten broken cakes to be equally probable, and take the
remaining states to have probability 0. One way to model T-800’s uncertainty
about the height of the stairs is to take each hypothesis to be equally likely.
However, not having any idea about which hypothesis holds is very different
from believing that all hypotheses are equally likely. It is easy to check that
taking each hypothesis to be equally likely makes check the act that maximizes

2

utility, but taking the probability that the stairs are fifteen feet high to be .51
makes back the act that maximizes expected utility, and taking the probability
that the stairs are ten feet high to be .51 makes cont the act that maximizes
expected utility. What makes any of these choices the “right” choice?
It is easy to construct many other examples where a single probability measure does not capture uncertainty, and does not result in what seem to be
reasonable decisions, when combined with expected utility maximization. A
natural alternative, which has often been considered in the literature, is to represent the agent’s uncertainty by a set of probability measures. For example, in
the delivery problem, the agent’s beliefs could be represented by two probability measures, Pr1 and Pr10 , one for each hypothesis. Thus, Pr1 assigns uniform
probability to all states with exactly one broken cake, and Pr10 assigns uniform
probability to all states with exactly ten broken cakes.
But this representation also has problems. Consider the delivery example
again. Why should T-800 be sure that there is exactly either one broken cake or
ten broken cakes? Of course, we can replace these two hypotheses by hypotheses
that say that the probability of a cake being broken is either .001 or .01, but this
doesn’t solve the problem. Why should the agent be sure that the probability
is either exactly .001 or exactly .01? Couldn’t it also be .0999? Representing
uncertainty by a set of measures still places a sharp boundary on what measures
are considered possible and impossible.
A second problem involves updating beliefs. How should beliefs be updated if
they are represented by a set of probability measures? The standard approach
for updating a single measure is by conditioning. The natural extension of
conditioning to sets of measure is measure-by-measure updating: conditioning
each measure on the information (and also removing measures that give the
information probability 0).
However, measure-by-measure updating can produce some rather counterintuitive outcomes. In the delivery example, suppose that a passer-by tells T-800
the information E: the first 100 cupcakes are good. Assuming that the passerby told the truth, intuition tells us that there is now more reason to believe that
there is only one broken cupcake.
However, Pr1 | E places uniform probability on all states where the first
100 cakes are good, and there is exactly one broken cake among the last 900.
Similarly, Pr10 | E places uniform probability on all states where the first 100
cakes are good, and there are exactly ten broken cakes among the last 900.
Pr1 | E still places probability 1 on there being one broken cake, just like Pr1 ,
Pr10 | E still places probability 1 on there being ten broken cakes. There is no
way to capture the fact that T-800 now views the hypothesis Pr10 as less likely,
even if the passer-by had said instead that the first 990 cakes are all good!
Of course, both of these problems would be alleviated if we placed a probability on hypotheses, but, as we have already observed, this leads to other
problems. In this paper, we propose an intermediate approach: representing
uncertainty using weighted sets of probabilities. That is, each probability measure is associated with a weight. These weights can be viewed as probabilities;
indeed, if the set of probabilities is finite, we can normalize them so that they
3

are effectively probabilities. Moreover, in one important setting, we update
them in the same way that we would update probabilities, using likelihood (see
below). On the other hand, these weights do not act like probabilities if the set
of probabilities is infinite. For example, if we had a countable set of hypotheses,
we could assign them all weight 1 (so that, intuitively, they are all viewed as
equally likely), but there is no uniform measure on a countable set.
More importantly, when it comes to decision making, we use the weights
quite differently from how we would use second-order probabilities on probabilities. Second-order probabilities would let us define a probability on events (by
taking expectation) and maximize expected utility, in the usual way. Using the
weights, we instead define a novel decision rule, minimax weighted expected regret (MWER), that has some rather nice properties, which we believe will make
it widely applicable in practice. If all the weights are 1, then MWER is just
the standard minimax expected regret (MER) rule (described below). If the set
of probabilities is a singleton, then MWER agrees with (subjective) expected
utility maximization (SEU). More interestingly perhaps, if the weighted set of
measures converges to a single measure (which will happen in one important
special case, discussed below), MWER converges to SEU. Thus, the weights
give us a smooth, natural way of interpolating between MER and SEU.
In summary, weighted sets of probabilities allow us to represent ambiguity
(uncertainty about the correct probability distribution). Real individuals are
sensitive to this ambiguity when making decisions, and the MWER decision
rule takes this into account. Updating the weighted sets of probabilities using
likelihood allows the initial ambiguity to be resolved as more information about
the true distribution is obtained.
We now briefly explain MWER, by first discussing MER. MER is a probabilistic variant of the minimax regret decision rule proposed by Niehans [13]
and Savage [17]. Most likely, at some point, we’ve second-guessed ourselves
and thought “had I known this, I would have done that instead”. That is, in
hindsight, we regret not choosing the act that turned out to be optimal for
the realized state, called the ex post optimal act. The regret of an act a in a
state s is the difference (in utility) between the ex post optimal act in s and a.
Of course, typically one does not know the true state at the time of decision.
Therefore the regret of an act is the worst-case regret, taken over all states. The
minimax regret rule orders acts by their regret.
The definition of regret applies if there is no probability on states. If an
agent’s uncertainty is represented by a single probability measure, then we can
compute the expected regret of an act a: just multiply the regret of an act a
at a state s by the probability of s, and then sum. It is well known that the
order on acts induced by minimizing expected regret is identical to that induced
by maximizing expected utility (see [8] for a proof). If an agent’s uncertainty
is represented by a set P of probabilities, then we can compute the expected
regret of an act a with respect to each probability measure Pr ∈ P, and then
take the worst-case expected regret. The MER (Minimax Expected Regret) rule
orders acts according to their worst-case expected regret, preferring the act that
minimizes the worst-case regret. If the set of measures is the set of all probability
4

measures on states, then it is not hard to show that MER induces the same
order on acts as (probability-free) minimax regret. Thus, MER generalizes both
minimax regret (if P consists of all measures) and expected utility maximization
(if P consists of a single measure).
MWER further generalizes MER. If we start with a weighted set of measures,
then we can compute the weighted expected regret for each one (just multiply
the expected regret with respect to Pr by the weight of Pr) and compare acts
by their worst-case weighted expected regret.
Sarver [16] also proves a representation theorem that involves putting a
multiplicative weight on a regret quantity. However, his representation is fundamentally different from MWER. In his representation, regret is a factor only
when comparing two sets of acts; the ranking of individual acts is given by
expected utility maximization. By way of contrast, we do not compare sets of
acts.
It is standard in decision theory to axiomatize a decision rule by means of
a representation theorem. For example, Savage [18] showed that if an agent’s
preferences  satisfied several axioms, such as completeness and transitivity,
then the agent is behaving as if she is maximizing expected utility with respect
to some utility function and probabilistic belief.
If uncertainty is represented by a set of probability measures, then we can
generalize expected utility maximization to maxmin expected utility (MMEU).
MMEU compares acts by their worst-case expected utility, taken over all measures. MMEU has been axiomatized by Gilboa and Schmeidler [7]. MER was
axiomatized by Hayashi [8] and Stoye [20]. We provide an axiomatization of
MWER. We make use of ideas introduced by Stoye [20] in his axiomatization
of MER, but the extension seems quite nontrivial.
We also consider a dynamic setting, where beliefs are updated by new information. If observations are generated according to a probability measure that is
stable over time, then, as we suggested above, there is a natural way of updating
the weights given observations, using ideas of likelihood. The idea is straightforward. After receiving some information E, we update each probability Pr ∈ P
to Pr | E, and take its weight to be αPr = Pr(E)/ supPr′ ∈P Pr′ (E). If more than
one Pr ∈ P gets updated to the same Pr | E, the sup of all such weights is used.
Thus, the weight of Pr after observing E is modified by taking into account the
likelihood of observing E assuming that Pr is the true probability. We refer to
this method of updating weights as likelihood updating.
If observations are generated by a stable measure (e.g., we observe the outcomes of repeated flips of a biased coin) then, as the agent makes more and
more observations, the weighted set of probabilities of the agent will, almost
surely, look more and more like a single measure. The weight of the measures
in P closest to the measure generating the observations converges to 1, and
the weight of all other measures converges to 0. This would not be the case
if uncertainty were represented by a set of probability measures and we did
measure-by-measure updating, as is standard. As we mentioned above, this
means that MWER converges to SEU.
We provide an axiomatization for dynamic MWER with likelihood updat5

ing. We remark that a dynamic version of MMEU with measure-by-measure
updating has been axiomatized by Jaffray [10], Pires [14], and Siniscalchi [19].
Likelihood updating is somewhat similar in spirit to an updating method
implicitly proposed by Epstein and Schneider [5]. They also represented uncertainty by using (unweighted) sets of probability measures. They choose a
threshold α with 0 < α < 1, update by conditioning, and eliminate all measures
whose relative likelihood does not exceed the threshold. This approach also
has the property that, over time, all that is left in P are the measures closest
to the measure generating the observations; all other measures are eliminated.
However, it has the drawback that it introduces a new, somewhat arbitrary,
parameter α.
Chateauneuf and Faro [2] also consider weighted sets of probabilities (they
model the weights using what they call confidence functions), although they
impose more constraints on the weights than we do. They then define and
provide a representation of a generalization of MMEU using weighted sets of
probabilities that parallels our generalization of MER. Chateauneuf and Faro
do not discuss the dynamic situation; specifically, they do not consider how
weights should be updated in the light of new information.
The rest of this paper is organized as follows. Section 2 introduces the
weighted sets of probabilities representation, and Section 3 introduces the MWER
decision rule. Axiomatic characterizations of static and dynamic MWER are
provided in Sections 4 and 5, respectively. We conclude in Section 7.

2

Weighted Sets of Probabilities

A set P + of weighted probability measures on a set S consists of pairs (Pr, αPr ),
where αPr ∈ [0, 1] and Pr is a probability measure on S.1 Let P = {Pr :
∃α(Pr, α) ∈ P + }. We assume that, for each Pr ∈ P, there is exactly one α such
that (Pr, α) ∈ P + . We denote this number by αPr , and view it as the weight
of Pr. We further assume for convenience that weights have been normalized
so that there is at least one measure Pr ∈ P such that αPr = 1.2 We remark
that, just as we do, Chateaunef and Faro [2] take weights to be in the interval
[0, 1]. They impose additional requirements on the weights. For example, they
require that the weight of a convex combination of two probability measures is
at least as high as the weight of each one. This does not seem reasonable in
our applications. For example, an agent may know that one of two measures is
generating his observations, and give them both weight 1, while giving all other
distributions weight 0.
1 In this paper, for ease of exposition, we take the state space S to be finite, and assume
that all sets are measurable. We can easily generalize to arbitrary measure spaces.
2 While we could take weights to be probabilities, and normalize them so that they sum to
1, if P is finite, this runs into difficulties if we have an infinite number of measures in P. For
example, if we are tossing a coin, and P includes all probabilities on heads from 1/3 to 2/3,
using a uniform probability, we would be forced to assign each individual probability measure
a weight of 0, which would not work well in the definition of MWER.

6

As we observed in the introduction, one way of updating weighted sets of
probabilities is by using likelihood updating. We use P + | E to denote the
+
result of applying likelihood updating to P + . Define P (E) = sup{αPr Pr(E) :
′
+
Pr ∈ P}; if P (E) > 0, set αPr,E = sup{Pr′ ∈P:Pr′ |E=Pr|E} αPr′+Pr (E) . Note that
P (E)

given a measure Pr ∈ P, there may be several distinct measures Pr′ in P such
that Pr′ | E = Pr | E. Thus, we take the weight of Pr | E to be the sup of the
+
possible candidate values of αPr,E . By dividing by P (E), we guarantee that
αPr,E ∈ [0, 1], and that there is some measure Pr such that αPr,E = 1, as long as
+
+
there is some pair (αPr , Pr) ∈ P such that αPr Pr(E) = P (E). If P (E) > 0,
we take P + | E to be
{(Pr | E, αPr,E ) : Pr ∈ P}.
+

If P (E) = 0, then P + | E is undefined.
In computing P + | E, we update not just the probability measures in P, but
also their weights. The new weight combines the old weight with the likelihood.
Clearly, if all measures in P assign the same probability to the event E, then
likelihood updating and measure-by-measure updating coincide. This is not
surprising, since such an observation E does not give us information about
the relative likelihood of measures. We stress that using likelihood updating
is appropriate only if the measure generating the observations is assumed to
be stable. For example, if observations of heads and tails are generated by
coin tosses, and a coin of possibly different bias is tossed in each round, then
likelihood updating would not be appropriate.
It is well known that, when conditioning on a single probability measure,
the order that information is acquired is irrelevant; the same observation easily
extends to sets of probability measures. As we now show, it can be further
extended to weighted sets of probability measures.
Proposition 1. Likelihood updating is consistent in the sense that for all E1 , E2 ⊆
S, (P + | E1 ) | E2 = (P + | E2 ) | E1 = P + | (E1 ∩ E2 ), provided that
P + | (E1 ∩ E2 ) is defined.
Proof. By standard results, (Pr | E1 ) | E2 = (Pr | E2 ) | E1 = Pr | (E1 ∩ E2 ).
Since the weight of the measure Pr | E1 is proportional to αPr Pr(E1 ), the weight
of (Pr | E1 ) | E2 is proportional to αPr Pr(E1 ) Pr(E2 | E1 ) = αPr Pr(E1 ∩ E2 ).
Likewise, the weight of (Pr | E2 ) | E1 is proportional to αPr Pr(E2 ) Pr(E1 |
E2 ) = αPr Pr(E1 ∩ E2 ). Since, in all these cases, the sup of the weights is
normalized to 1, the weights of corresonding measures in P + | (E1 ∩ E2 ), (P + |
E1 ) | E2 and (P + | E2 ) | E1 must be equal.

3

MWER

We now define MWER formally. Given a set S of states and a set X of outcomes,
an act f (over S and X) is a function mapping S to X. For simplicity in this
paper, we take S to be finite. Associated with each outcome x ∈ X is a utility:
7

u(x) is the utility of outcome x. We call a tuple (S, X, u) a (non-probabilistic)
decision problem. To define regret, we need to assume that we are also given
a set M ⊆ X S of feasible acts, called the menu. The reason for the menu is
that, as is well known (and we will demonstrate by example shortly), regret can
depend on the menu. Moreover, we assume that every menu M has utilities
bounded from above. That is, we assume that for all menus M , supg∈M u(g(s))
is finite. This ensures that the regret of each act is well defined.3 For a menu
M and act f ∈ M , the regret of f with respect to M and decision problem
(S, X, u) in state s is


reg M (f, s) = sup u(g(s)) − u(f (s)).
g∈M

That is, the regret of f in state s (relative to menu M ) is the difference between
u(f (s)) and the highest utility possible in state s (among all the acts in M ). The
regret of f with respect to M and decision problem (S, X, u) is the worst-case
regret over all states:
max reg M (f, s).
s∈S

(S,X,u)
(f ),
reg M

and usually omit the superscript (S, X, u) if it
We denote this as
is clear from context. If there is a probability measure Pr over the states, then
we can consider the probabilistic decision problem (S, X, u, Pr). The expected
regret of f with respect to M is
X
reg M,Pr (f ) =
Pr(s)reg M (f, s).
s∈S

If there is a set P of probability measures over the states, then we consider the
P-decision problem (S, X, u, P). The maximum expected regret of f ∈ M with
respect to M and (S, X, u, P) is
!
X
Pr(s)reg M (f, s) .
reg M,P (f ) = sup
Pr∈P

s∈S

Finally, if beliefs are modeled by weighted probabilities P + , then we consider
the P + -decision problem (S, X, u, P + ). The maximum weighted expected regret
of f ∈ M with respect to M and (S, X, u, P + ) is
!
X
Pr(s)reg M (f, s) .
reg M,P + (f ) = sup αPr
Pr∈P

s∈S

The MER decision rule is thus defined for all f, g ∈ X S as
(S,X,u)

f S,X,u
M,P g iff reg M,P

(S,X,u)

(f ) ≤ reg M,P

(g).

3 Stoye [21] assumes that, for each menu M , there is a finite set A
M of acts such that M
consists of all the convex combinations of the acts in AM . Our assumption is clearly much
weaker than Stoye’s.

8

cont
back
check

1 broken cake
Payoff Regret
10,000 0
0
10,000
5,001
4,999

10 broken cakes
Payoff
Regret
-10,000 10,000
0
0
-4,999
4,999

Table 2: Payoffs and regrets for delivery example.

cont
back
check
new

1 broken cake
Payoff Regret
10,000 10,000
0
20,000
5,001
14,999
20,000 0

10 broken cakes
Payoff
Regret
-10,000 10,000
0
0
-4,999
4,999
-20,000 20,000

Table 3: Payoffs and regrets for the delivery problem with a new choice added.
That is, f is preferred to g if the maximum expected regret of f is less than that
(S,X,u)
S,X,u
of g. We can similarly define M,reg , S,X,u
M,Pr , and M,P + by replacing reg M,P
(S,X,u)

(S,X,u)

(S,X,u)

by reg M
, reg M,Pr , and reg M,P + , respectively. Again, we usually omit the
superscript (S, X, u) and subscript Pr or P + , and just write M , if it is clear
from context.
To see how these definitions work, consider the delivery example from the
introduction. There are 1, 000 states with one broken cake, and C(1000, 10)
states with ten broken cakes. The regret of each action in a state depends only
on the number of broken cakes, and is given in Table 2. It is easy to see that the
action that minimizes regret is check , with cont and back having equal regret.
If we represent uncertainty using the two probability measures Pr1 and Pr10 ,
the expected regret of each of the acts with respect to Pr1 (resp., Pr10 ) is just
its regret with respect to states with one (resp. ten) broken cakes. Thus, the
action that minimizes maximum expected regret is again check .
As we said above, the ranking of acts based on MER or MWER can change
if the menu of possible choices changes. For example, suppose that we introduce
a new choice in the delivery problem, whose gains and losses are twice those of
cont, resulting in the payoffs and regrets described in Table 3. In this new setting, cont has a lower maximum expected regret (10, 000) than check (14, 999),
so MER prefers cont over check . Thus, the introduction of a new choice can
affect the relative order of acts according to MER (and MWER), even though
other acts are preferred to the new choice. By way of contrast, the decision rules
MMEU and SEU are menu-independent ; the relative order of acts according to
MMEU and SEU is not affected by the addition of new acts.
We next consider a dynamic situation, where the agent acquires information. Specifically, in the context of the delivery problem, suppose that T800 learns E—the first 100 items are good. Initially, suppose that T-800

9

has no reason to believe that one hypothesis is more likely than the other,
so assigns both hypotheses weight 1. Note that P1 (E) = 0.9 and Pr10 (E) =
C(900, 10)/C(1000, 10) ≈ 0.35. Thus,
P + | E = {(Pr1 | E, 1), (Pr10 | E, C(900, 10)/(.9C(1000, 10))}.
We can also see from this example that MWER interpolates between MER
and expected utility maximization. Suppose that a passer-by tells T-800 that
the first N cupcakes are good. If N = 0, MWER with initial weights 1 is the
same as MER. On the other hand, if N ≥ 991, then the likelihood of Pr10 is 0,
and the only measure that has effect is Pr1 , which means minimizing maximum
weighted expected regret is just maximizing expected utility with respect to
Pr1 . If 0 < N < 991, then the likelihoods (hence weights) of Pr1 and Pr10 are
1000
9
1 and C(1000−N,10)
C(1000,10) × 1000−N < ((999 − N )/999) . Thus, as N increases, the
weight of Pr10 goes to 0, while the weight of Pr1 stays at 1.

4

An axiomatic characterization of MWER

We now provide a representation theorem for MWER. That is, we provide a
collection of properties (i.e., axioms) that hold of MWER such that a preference order on acts that satisfies these properties can be viewed as arising from
MWER. To get such an axiomatic characterization, we restrict to what is known
in the literature as the Anscombe-Aumann (AA) framework [1], where outcomes
are restricted to lotteries. This framework is standard in the decision theory
literature; axiomatic characterizations of SEU [1], MMEU [7], and MER [8, 20]
have already been obtained in the AA framework. We draw on these results to
obtain our axiomatization.
Given a set Y (which we view as consisting of prizes), a lottery over Y
is just a probability with finite support on Y . Let ∆(Y ) consist of all finite
probabilities over Y . In the AA framework, the set of outcomes has the form
∆(Y ). So now acts are functions from S to ∆(Y ). (Such acts are sometimes
called Anscombe-Aumann acts.) We can think of a lottery as modeling objective
uncertainty, while a probability on states models subjective uncertainty; thus,
in the AA framework we have both objective and subjective uncertainty. The
technical advantage of considering such a set of outcomes is that we can consider
convex combinations of acts. If f and g are acts, define the act αf + (1 − α)g
to be the act that maps a state s to the lottery αf (s) + (1 − α)g(s).
In this setting, we assume that there is a utility function U on prizes in Y .
The utility of a lottery l is just the expected utility of the prizes obtained, that
is,
X
l(y)U (y).
u(l) =
{y∈Y : l(y)>0}

This makes sense since l(y) is the probability of getting prize y if lottery l is
played. The expected utility of an act f with respect to a probability Pr is then

10

P
just u(f ) = s∈S Pr(s)u(f (s)), as usual. We also assume that there are at least
two prizes y1 and y2 in Y , with different utilities U (y1 ) and U (y2 ).
Given a set Y of prizes, a utility U on prizes, a state space S, and a set P +
S,∆(Y ),u
of preference
of weighted probabilities on S, we can define a family M,P +
orders on Anscombe-Aumann acts determined by weighted regret, one per menu
M , as discussed above, where u is the utility function on lotteries determined
S,∆(Y ),u
by U . For ease of exposition, we usually write S,Y,U
.
M,P + rather than M,P +
We state the axioms in a way that lets us clearly distinguish the axioms for
SEU, MMEU, MER, and MWER. The axioms are universally quantified over
acts f , g, and h, menus M and M ′ , and p ∈ (0, 1). We assume that f, g ∈ M
when we write f M g.4 We use l∗ to denote a constant act that maps all states
to l.
Axiom 1. (Transitivity) f M g M h ⇒ f M h.
Axiom 2. (Completeness) f M g or g M f .
Axiom 3. (Nontriviality) f ≻M g for some acts f and g and menu M .
Axiom 4. (Monotonicity) If (f (s))∗ {(f (s))∗ ,(g(s))∗ } (g(s))∗ for all s ∈ S, then
f M g.
Axiom 5. (Mixture Continuity) If f ≻M g ≻M h, then there exist q, r ∈ (0, 1)
such that
qf + (1 − q)h ≻M∪{qf +(1−q)h} g ≻M∪{rf +(1−r)h} rf + (1 − r)h.
Menu-independent versions of Axioms 1–5 are standard. Clearly (menuindependent versions of) Axioms 1, 2, 4, and 5 hold for MMEU, MER, and
SEU; Axiom 3 is assumed in all the standard axiomatizations, and is used to
get a unique representation.
Axiom 6. (Ambiguity Aversion)
f ∼M g ⇒ pf + (1 − p)g M∪{pf +(1−p)g} g.
Ambiguity Aversion says that the decision maker weakly prefers to hedge her
bets. It also holds for MMEU, MER, and SEU, and is assumed in the axiomatizations for MMEU and MER. It is not assumed for the axiomatization of SEU,
since it follows from the Independence axiom, discussed next. Independence also
holds for MWER, provided that we are careful about the menus involved. Given
a menu M and an act h, let pM + (1 − p)h be the menu {pf + (1 − p)h : p ∈ M }.
4 Stoye [21] assumed that menus were convex, so that if f, g ∈ M , then so is pf + (1 − p)g.
We do not make this assumption, although our results would still hold if we did (with the
axioms slightly modified to ensure that menus are convex). While it may seem reasonable to
think that, if f and g are feasible for an agent, then so is pf + (1 − p)g, this not always the
case. For example, it may be difficult for the agent to randomize, or it may be infeasible for
the agent to randomize with probability p for some choices of p (e.g., for p irrational).

11

Axiom 7. (Independence)
f M g iff pf + (1 − p)h pM+(1−p)h pg + (1 − p)h.
Independence holds in a strong sense for SEU, since we can ignore the menus.
The menu-independent version of Independence is easily seen to imply Ambiguity Aversion. Independence does not hold for MMEU.
Although we have menu independence for SEU and MMEU, we do not have
it for MER or MWER. The following two axioms are weakened versions of menu
independence that do hold for MER and MWER.
Axiom 8. (Menu independence for constant acts) If l∗ and (l′ )∗ are constant
acts, then l∗ M (l′ )∗ iff l∗ M ′ (l′ )∗ .
In light of this axiom, when comparing constant acts, we omit the menu.
An act h is never strictly optimal relative to M if, for all states s ∈ S, there
is some f ∈ M such that (f (s))∗  (h(s))∗ .
Axiom 9. (Independence of Never Strictly Optimal Alternatives (INA)) If every
act in M ′ is never strictly optimal relative to M , then f M g iff f M∪M ′ g.
Axiom 10. (Boundedness of menus) For every menu M , there exists a lottery
∗
l ∈ ∆(Y ) such that for all f ∈ M and s ∈ S, (f (s))∗  l .
The boundedness axiom enforces the assumption that we made earlier that every
menu has utilities that are bounded from above. Recall that this assumption is
necessary for regret to be finite.
We now present our representation theorem for MWER. Roughly, the representation theorem states that a family of preferences satisfies Axioms 1–10 if
and only if it has a MWER representation with respect to some utility function and weighted probabilities. In the representation theorem for SEU [1], not
only is the utility function unique (up to affine transformations, so that we can
replace U by aU + b, where a > 0 and b are constants), but the probability is
unique as well. Similarly, in the MMEU representation theorem of Gilboa and
Schmeidler [7], the utility function is unique, and the set of probabilities is also
unique, as long as one assume that the set is convex and closed.
To get uniqueness in the representation theorem for MWER, we need to consider a different representation of weighted probabilities. Define a sub-probability
measure p on S to be like a probability measure (i.e., a function mapping measurable subsets of S to [0, 1] such that p(T ∪ T ′ ) = p(T ) + p(T ′ ) for disjoint sets
T and T ′ ), without the requirement that p = 1. We can identify a weighted
probability distribution (Pr, α) with the sub-probability measure α Pr. (Note
that given a sub-probability measure p, there is a unique pair (α, Pr) such that
P = α Pr: we simply take α = p(S) and Pr = p/α.) A set C of sub-probability
measures is downward-closed if, whenever p ∈ C and q ≤ p, then q ∈ C. We
get a unique set of sub-probability measures in our representation theorem if
we restrict to sets that are convex, downward-closed, closed, and contain at
least one (proper) probability measure. (The latter requirement corresponds to
12

having αPr = 1 for some Pr ∈ P + .) For convenience, we will call a set regular
if it is convex, downward-closed, and closed.
We identify each set of weighted probabilities P + with the set of subprobability measures
C(P + ) = {α Pr : (Pr, αPr ) ∈ P + , 0 ≤ α ≤ αPr }.
Note that if (α, Pr) ∈ P + , then C(P + ) includes all the sub-probability measures
between the all-zero measure and αPr Pr.
We need to restrict to closed and convex sets of sub-probability measures to
get uniqueness in the representation of MWER for much the same reason that we
need to restrict to closed and convex sets to get uniqueness in the representation
of MMEU. To see why convexity is needed, consider the delivery example and
the expected regrets in Table 2, and the distribution a Pr1 +(1 − a) Pr10 , for
some a ∈ (0, 1). The weighted expected regret of any act with respect to
a Pr1 +(1 − a) Pr10 is bounded above by the maximum weighted expected regret
of that act with respect to Pr1 and Pr10 . Therefore, adding a Pr1 +(1−a) Pr10 to
P + for some weight a ∈ (0, 1) does not change the resulting family of preferences.
Similarly, we need to restrict to closed sets for uniqueness, since if we start with
a set C of sub-probability measures that is not closed, taking the closure of C
would result in the same family of preferences.
While convexity is easy to define for a set of sub-probability measures, there
seems to be no natural notion of convexity for a set P + of weighted probabilities.
Moreover, the requirement that P + is closed is different from the requirement
that C(P + ) is closed. The latter requirement seems more reasonable. For
example, fix a probability measure Pr, and let P + = {(1, Pr)} ∪ {(0, Pr′ ) : Pr′ 6=
Pr}. Thus, P + essentially consists of a single probability measure, namely Pr,
with weight 1; all the weighted probability measures (0, Pr′ ) have no impact.
This represents the uncertainty of an agent who is sure that that Pr is true
probability. Clearly P + is not closed, since we can find a sequence Prn such
that (0, Prn ) → (0, Pr), although (0, Pr) ∈
/ P + . But C(Pr+ ) is closed.
Restricting to closed, convex sets of sub-probability measures does not suffice to get uniqueness; we also need to require downward-closedness. This is so
because if p is in C, then adding any q ≤ p to the set leaves all regrets unchanged. Finally, the presence of a proper probability measure is also required,
since for any a ∈ (0, 1], scaling each element in the set C by a leaves the family
of preferences unchanged.
In summary, if we consider arbitrary sets of sub-probability measures, then
the set of sub-probability measures that represent a given family of MWER
preferences would be unique if we required the set to be regular and contain a
probability measure.
Theorem 1. For all Y , U , S, and P + , the family of preference orders S,Y,U
M,P +
satisfies Axioms 1–10. Conversely, if a family of preference orders M on
the acts in ∆(Y )S satisfies Axioms 1–10, then there exist a a utility U on Y
and a weighted set P + of probabilities on S such that C(P + ) is regular and
+
M =S,Y,U
M,P + . Moreover, U is unique up to affine transformations, and C(P )
13

is unique in the sense that if Q + represents M , and C(Q + ) is regular, then
C(Q + ) = C(P + ).
Showing that S,Y,U
M,P + satisfies Axioms 1–10 is fairly straightforward; we leave
details to the reader. The proof of the converse is quite nontrivial, although it
follows the lines of the proof of other representation theorems. We provide an
outline of the proof here; details can be found in the appendix.
Using standard techniques, we can show that the axioms guarantee the existence of a utility function U on prizes that can be extended to lotteries in the
obvious way, so that l∗  (l′ )∗ iff U (l) ≥ U (l′ ). We then use techniques of Stoye
[21] to show that it suffices to get a representation theorem for a single menu,
rather than all menus: the menu consisting of all acts f such that U (f (s)) ≤ 0
for all states s ∈ S. This allows us to use techniques in the spirit of those used
by by Gilboa and Schmeidler [6] to represent (unweighted) MMEU. However,
there are technical difficulties that arise from the fact that we do not have a
key axiom that is satisfied by MMEU: C-independence (discussed below). The
heart of the proof involves dealing with the lack of C-independence; as we said,
the details can be found in the appendix.
It is instructive to compare Theorem 1 to other representation results in
the literature. Anscombe and Aumann [1] showed that the menu-independent
versions of axioms 1–5 and 7 characterize SEU. The presence of Axiom 7 (menuindependent Independence) greatly simplifies things. Gilboa and Schmeidler [7]
showed that axioms 1–6 together with one more axiom that they call Certaintyindependence characterizes MMEU. Certainty-independence, or C-independence
for short, is a weakening of independence (which, as we observed, does not hold
for MMEU), where the act h is required to be a constant act. Since MMEU is
menu-independent, we state it in a menu-independent way.
Axiom 11. (C-Independence) If h is a constant act, then f  g iff pf + (1 −
p)h  pg + (1 − p)h.
As we observed, in general, we have Ambiguity Aversion (Axiom 6) for
regret. Betweenness [3] is a stronger notion than ambiguity aversion, which
states that if an agent is indifferent between two acts, then he must also be
indifferent among all convex combinations of these acts. While betweenness
does not hold for regret, Stoye [20] gives a weaker version that does hold. A
menu M has state-independent outcome distributions if the set L(s) = {y ∈
∆(Y ) : ∃f ∈ M, f (s) = y} is the same for all states s.
Axiom 12. If h is a constant act, and M has state-independent outcome distributions, then
h ∼M f ⇒ pf + (1 − p)h ∼M∪{pf +(1−p)h} f.
The assumption that the menu has state-independent outcome distributions
is critical in Axiom 12.
Stoye [20] shows that Axioms 1–9 together with Axiom 12 characterize
MER.5 Non-probabilistic regret (which we denote REG) can be viewed as a
5 Stoye

actually worked with choice correspondences; see Section 7.

14

cont
1
1
2 cont + 2 back
back
check

1 broken cake
Payoff Regret
10,000 0
5,000
5,000
0
10,000
5,001
4,999

10 broken cakes
Payoff
Regret
-10,000 10,000
-5,000
5,000
0
0
-4,999
4,999

Table 4: Payoffs and regrets for the delivery problem, with cont mixed with the
constant act back .

cont
1
1
2 cont + 2 back
back
check 1
check 2

1 broken cake
Payoff
Regret
10,000 0
5,000
5,000
0
10,000
-5,000
15,000
-10,000 20,000

10 broken cakes
Payoff
Regret
-10,000 20,000
-5,000
15,000
0
10,000
5,000
5,000
10,000
0

Table 5: Payoffs and regrets for the delivery problem, with state-independent
outcome distributions.
special case of MER, where P consists of all distributions. This means that it
satisfies all the axioms that MER satisfies. As Stoye [21] shows, REG is characterized by Axioms 1–9 and one additional axiom, which he calls Symmetry.
We omit the details here.
The assumption that the menu has state-independent outcome distributions
is critical in Axiom 12. For example, suppose that we change the payoffs in the
delivery problem so that cont has the same maximum expected regret as back
(10, 000). However, as seen in Table 4, 21 cont + 21 back has lower maximum expected regret (5, 000) than cont (10, 000), showing that the variant of Axiom 12
without the state-independent outcome distribution requirement does not hold.
Although Axiom 12 is sound for unweighted minimax expected regret, it is
no longer sound once we add weights. For example, suppose that we modified
the delivery problem so that all states we care about have the same outcome
distributions, as required by Axiom 12. Then the payoffs and regrets will be
those shown in Table 5. Suppose that the weights on Pr1 and Pr10 are 1 and
0.5, respectively. Then cont has the same maximum weighted expected regret as
back (10, 000). However, 21 cont + 21 back has lower maximum weighted expected
regret (7, 500) than cont, showing that Axiom 12 with weighted probabilities
does not hold.
Table 6 describes the relationship between the axioms characterizing the
decision rules.

15

Ax. 1-6,8-10
Ind
C-Ind
Ax. 12
Symmetry

SEU
X
X
X
X
X

REG
X
X

MER
X
X

X
X

X

MWER
X
X

MMEU
X
X

Table 6: Characterizing axioms for several decision rules.

5

Characterizing MWER with Likelihood Updating

We next consider a more dynamic setting, where agents learn information. For
simplicity, we assume that the information is always a subset E of the state
space. If the agent is representing her uncertainty using a set P + of weighted
probability measures, then we would expect her to update P + to some new set
Q + of weighted probability measures, and then apply MWER with uncertainty
represented byQ + . In this section, we characterize what happens in the special
case that the agent uses likelihood updating, so that Q + = (P + | E).
For this characterization, we assume that the agent has a family of preference
orders E,M indexed not just by the menu M , but by the information E. Each
preference order E,M satisfies Axioms 1–10, since the agent makes decisions
after learning E using MWER. Somewhat surprisingly, all we need is one extra
axiom for the characterization; we call this axiom MDC, for ‘menu-dependent
dynamic consistency’.
To explain the axiom, we need some notation. As usual, we take f Eh to be
the act that agrees with f on E and with h off of E; that is

f (s) if s ∈ E
f Eh(s) =
h(s) if s ∈
/ E.
In the delivery example, the act check can be thought of as (cont)E(back ),
where E is the set of states where there is only one broken cake.
Roughly speaking, MDC says that you prefer f to g once you learn E if and
only if, for any act h, you also prefer f Eh to gEh before you learn anything.
This seems reasonable, since learning that the true state was in E is conceptually
similar to knowing that none of your choices matter off of E.
To state MDC formally, we need to be careful about the menus involved.
Let M Eh = {f Eh : f ∈ M }. We can identify unconditional preferences with
preferences conditional on S; that is, we identify M with S,M . We also need
to restrict the sets E to which MDC applies. Recall that conditioning using
+
likelihood updating is undefined for an event such that P (E) = 0. That is,
αPr Pr(E) = 0 for all Pr ∈ P. As is commonly done, we capture the idea that
conditioning on E is possible using the notion of a non-null event.
Definition 1. An event E is null if, for all f, g ∈ ∆(Y )S and menus M with
16

f Eg, g ∈ M , we have f Eg ∼M g.
MDC. For all non-null events E, f E,M g iff f Eh MEh gEh for some
h ∈ M .6
The key feature of MDC is that it allows us to reduce all the conditional preference orders E,M to the unconditional order M , to which we can apply
Theorem 1.
Theorem 2. For all Y , U , S, and P + , the family of preference orders S,Y,U
P + |E,M
+

for events E such that P (E) > 0 satisfies Axioms 1–10 and MDC. Conversely, if a family of preference orders E,M on the acts in ∆(Y )S satisfies
Axioms 1–10 and MDC, then there exists a utility U on Y and a weighted
set P + of probabilities on S such that C(P + ) is regular, and for all non-null
E, E,M =S,Y,U
P + |E,M . Moreover, U is unique up to affine transformations, and
C(P + ) is unique in the sense that if Q + represents E,M , and C(Q + ) is regular, then C(Q + ) = C(P + ).
Proof. Since M =S,M satisfies Axioms 1–10, there must exist a weighted set
P + of probabilities on S and a utility function U such that f M g iff f S,Y,U
M,P +
+

g. We now show that if E is non-null, then P (E) > 0, and f E,M g iff
(S,X,u)
f M,P + |E g.
+

For the first part, it clearly is equivalent to show that if P (E) = 0, then E
+
is null. So suppose that P (E) = 0. Then αPr Pr(E) = 0 for all Pr ∈ P. This
means that αPr Pr(s) = 0 for all Pr ∈ P and s ∈ E. Thus, for all acts f and g,
reg M,P + (f Eg)P

= supPr∈P αPr P
s∈S Pr(s)reg M (f Eg, s)

αPr
= supP
M (f, s)
Pr∈P
s∈E Pr(s)reg

+ s∈E c Pr(s)reg
(g,
s)

P M
= supPr∈P αPr s∈S Pr(s)reg M (g, s)
= reg M,P + (g).
Thus, f Eg ∼M g for all acts f, g and menus M containing f Eg and g, which
means that E is null.
+
For the second part, we first show that if P (E) > 0, then for all f, h ∈ M ,
we have that
+
reg MEh,P + (f Eh) = P (E)reg M,P + |E (f ).
6 Although we do not need this fact, it is worth noting that the MWER decision rule has
the property that f Eh M Eh gEh for some act h iff f Eh M Eh gEh for all acts h. Thus,
this property follows from Axioms 1–10.

17

We proceed as follows:
=
=
=
=

reg MEh,P + (f Eh)

P
supPr∈P αPr s∈S Pr(s)reg
MEh (f EH, s)
P
supPr∈P αPr Pr(E) s∈E Pr(s | E)reg M (f, s)
P
+αPr s∈E c Pr(s)reg {h} (h, s)

P
supPr∈P αPr Pr(E) s∈E Pr(s|E)reg M (s, f ) 
P
+
supPr∈P P (E)αPr,E s∈E Pr(s|E)reg M (f, s)
[since αPr,E = sup{Pr′ ∈P:Pr′ |E=Pr|E}

=

+

αPr′ Pr′ (E)
]
+
P (E)

P (E) · reg M,P + |E (f ).

Thus, for all h ∈ M ,
reg MEh,P + (f Eh) ≤ reg MEh,P + (gEh)
+

+

iff P (E) · reg M,P + |E (f ) ≤ P (E) · reg M,P + |E (g)
iff reg M,P + |E (f ) ≤ reg M,P + |E (g).
It follows that the order induced by P + satisfies MDC.
Moreover, if 1–10 and MDC hold, then for a weighted set P + that represents
M , we have
f E,M g
iff
for some h ∈ M, f Eh MEh gEh
iff reg M,P + |E (f ) ≤ reg M,P + |E (g),
as desired.
Finally, the uniqueness of C(P + ) follows from Theorem 1, which says that
the family S,M of preferences is already sufficient to guarantee the uniqueness
of C(P + ).
Analogues of MDC have appeared in the literature before in the context of
updating preference orders. In particular, Epstein and Schneider [4] discuss a
menu-independent version of MDC, although they do not characterize updating in their framework. Sinischalchi [19] also uses an analogue of MDC in his
axiomatization of measure-by-measure updating of MMEU. Like us, he starts
with an axiomatization for unconditional preferences, and adds an axiom called
constant-act dynamic consistency (CDC), somewhat analogous to MDC, to extend the axiomatization of MMEU to deal with conditional preferences.

6

Dynamic Inconsistency

There is an important issue when one attempts to apply MWER with likelihood
updating to dynamic decision problems. If you want to execute a plan, at every
step you’ll need to stick with that plan and execute the corresponding part of
the plan. However, after following the initial steps of an ex-ante optimal plan,
18

a MWER agent may no longer wish to adhere to the plan. In such a situation, the agent is said to have dynamically inconsistent preferences. Dyanmic
inconsistency is well known to hold for regret. Indeed, as Epstein and Le Breton [4] show, dynamic inconsistency arises for any non-Bayesian approach to
decision making (i.e., any approach that does not involve maximizing expected
utility) that satisfies certain minimal assumptions. Not surprisingly, it arises
for MWER as well. In the rest of this section, we discuss the problem and some
standard approaches to dealing with it, and illustrate some subtleties that arise
in dealing with it in the context of MWER.
To understand the problem in the context of regret, consider the two-stage
decision problem of having dinner, represented as a decision tree in Figure 1.
Solid circles denote decision points, and empty circles denote points where nature reveals information to the agent. The decision tree also includes information about what states are considered possible at each node. The set of states
considered possible at the root is always the entire state space, and nature’s
actions at each nature decision point partitions the set of possible states.
{m, b}

Italian restaurant

Chinese restaurant

stirfry

plain rice

{m}
0

{b}

{m}

{b}

0

-2

3

{m}

{b}

5

-3

Figure 1: Dynamic inconsistency example.
First, you have to choose between a Chinese restaurant and an Italian restaurant. Once you have arrived at a particular restaurant, you cannot change your
mind and go to another; so in the second stage you must order something from
the menu at the chosen restaurant. Your utility is a combination of how much
you enjoy the food, and whether you get an allergic reaction. Initially, you know
that there are two possible states: you must be either allergic to MSG (state
m) or to basil (state b), but not both. Assume that all Italian foods will have
traces of basil, and Chinese stir-fry has MSG but plain rice does not. However,
you do not enjoy eating plain rice, so the utility of ordering rice is 0.
Suppose that you make decisions using the minimax regret decision rule,
viewing a plan (i.e., a strategy) as an act. A straightforward computation
19

shows that, ex ante, going to the Chinese restaurant and ordering plain rice has
the lowest regret (5). However, if you go to the Chinese restaurant, the choice of
going to the Italian restaurant is now irrelevant. If we now compute regret with
respect to the menu of the two remaining choices, then the regret of ordering
stir-fry is lower (2) than that of ordering rice (3). You thus end up ordering
the stir-fry. The plan of going to the Chinese restaurant and ordering plain rice
cannot be carried out.
More generally, dynamic consistency requires that the plan considered optimal ex ante continues to be considered optimal at any later time. As we said
earlier, Epstein and Le Breton [4] show that dynamic inconsistency will arise for
essentially all non-Bayesidan decision rules. A standard approach for dealing
with this lack of dynamic consistency in the literature is to consider ‘sophsticated’ agents, who are aware of the potential for dynamic inconsistency, and
thus use backward induction to determine the feasible plans. In the restaurant
example, a sophisticated agent believes correctly that she will prefer stir-fry over
rice, once she is at the Chinese restaurant. Therefore, she no longer considers
going to the Chinese restaurant and ordering plain rice a viable plan. The only
feasible options are going to the Italian restaurant, or having stir-fry at the
Chinese restaurant.
A subtlety arises when trying to apply backward induction to menu-dependent
decision rules: which menu do we use when comparing the viable plans? For
example, in the restaurant example, do we use the menu consisting of all three
plans, or the menu consisting of just the viable plans. It turns out not to matter
in this example—with respect to both menus, going to the Italian restaurant
minimizes regret. However, in general, the choice of menu can matter. Hayashi
[9] uses the menu of viable plans in computing for minimax expected regret
agents in optimal stopping problems, but it seems to us that both choices (and
perhaps others) can be justified.
A second subtlety that arises when considering sophisticated agents: What
choice do they make when they are indifferent between two plans? Sinischalchi
[19] axiomatizes consistent planning (with menu-independent preferences over
plans), which augments backward induction with a tie-breaking assumption.
This tie-breaking assumption in consistent planning allows an agent to commit
to a plan as long as each stage of the plan is considered to be one of the best at
each local decision node.
In order to axiomatize consistent planning, Siniscalchi must assume that the
agent has preferences that are more general than preferences over plans. Rather,
the agent must be assumed to have preferences over decision trees (such as that
in Figure 1). Plans are the special case of decision trees with no branching at
decision nodes; we can identify a decision tree with a set of plans (essentially, the
branches in the decision tree). Sophistication is captured by an axiom that says,
roughly, that the agent is indifferent between a decision tree and the same tree
with a non-optimal (based on backward-induction) plan removed. Preferences
over decision trees are similar in spirit to preferences over menus [11].
If we try to apply Siniscalchi’s approach to regret, we encounter further
difficulties. In a menu-independent setting, we can compare two decision trees
20

by comparing the best plans in each decision tree (if we identify a decision
tree with a set of plans). But once menus become relevant, we must decide
what menu to use when making this comparison. It is not clear which menu to
choose. What we really have here are menus over menus; it is not even clear how
to apply regret in this setting. Defining and axiomatizing consistent planning
in a regret-based setting remains an open problem.

7

Conclusion

We proposed an alternative belief representation using weighted sets of probabilities, and described a natural approach to updating in such a situation and a
natural approach to determining the weights. We also showed how weighted sets
of probabilities can be combined with regret to obtain a decision rule, MWER,
and provided an axiomatization that characterizes static and dynamic preferences induced by MWER.
We have considered preferences indexed by menus here. Stoye [21] used a
different framework: choice functions. A choice function maps every finite set
M of acts to a subset M ′ of M . Intuitively, the set M ′ consists of the ‘best’
acts in M . Thus, a choice function gives less information than a preference
order; it gives only the top elements of the preference order. The motivation
for working with choice functions is that an agent can reveal his most preferred
acts by choosing them when the menu is offered. In a menu-independent setting,
the agent can reveal his whole preference order; to decide if f ≻ g, it suffices
to present the agent with a choice among {f, g}. However, with regret-based
choices, the menu matters; the agent’s most preferred choice(s) when presented
with {f, g} might no longer be the most preferred choice(s) when presented with
a larger menu. Thus, a whole preference order is arguably not meaningful with
regret-based choices. Stoye [21] provides a representation theorem for MER
where the axioms are described in terms of choice functions. The axioms that
we have attributed to Stoye are actually the menu-based analogue of his axioms.
We believe that it should be possible to provide a characterization of MWER
using choice functions, although we have not yet proved this.
Finally, we briefly considered the issue of dynamic consistency and consistent
planning. As we showed, making this precise in the context of regret involves a
number of subtleties. We hope to return to this issue in future work.

A

Proof of Theorem 1

We show here that if a family of menu-dependent preferences M satisfies axioms 1-10, then M can be represented as minimizing expected regret with
respect to a set of weighted probabilities and a utility function. Since the proof
is somewhat lengthy and complicated, we split it into several steps, each in a
separate subsection.

21

A.1

Simplifying the Problem

Our proof starts in much the same way as the proof by Stoye [21] of a representation theorem for regret. Lemma 1 guarantees the existence of a utility
function U on prizes that can be extended to lotteries in the obvious way, so
that l∗  (l′ )∗ iff U (l) ≥ U (l′ ). In other words, preferences over all constant acts
are represented by the maximization of U on the corresponding lotteries that
the constant acts map to. Lemma 1 is a consequence of standard results. Our
menus are arbitrary sets of acts, as opposed to convex hulls of a finite number
of acts in [21]; Lemma 3 shows that Stoye’s technique can be adapted to work
when menus are arbitrary sets of acts. Finally, following Stoye [21], we reduce
the proof of existence of a minimax weighted regret representation for the family
M to the proof of existence of a minimax weighted regret representation for a
single menu-independent preference ordering  (Lemma 4).
Lemma 1. If Axioms 1-3, 5, 7, and 8 hold, then there exists a nonconstant
function U : X → R, unique up to positive affine transformations, such that for
all constant acts l∗ and (l′ )∗ and menus M ,
X
X
l′ (y)U (y).
l(y)U (y) ≥
l∗ M (l′ )∗ ⇔
{y: l′ (y)>0}

{y: l∗ (y)>0}

Proof. By menu independence for constant acts, the family of preferences M all
agree when restricted to constant acts. The lemma then follows from standard
results (see, e.g., [12]), since menu-independence for constant acts, combined
with independence, gives the standard independence (substitution) axiom from
expected utility theory.
P
As is commonly done, given U , we define u(l) = {y: l(y)>0} l(y)U (y). Thus,
u(l) is the expected utility of lottery l. We extend u to contsant acts by taking
u(l∗ ) = u(l). Thus, Lemma 1 says that, for all menus M , l∗  (l′ )∗ iff u(l∗ ) ≥
u(l′ ). If c is the utility of some lottery, let lc∗ be a constant lottery that u(lc∗ ) = c.
The following is now immediate. We state it as a lemma so that we can refer
to it later.
Lemma 2. u(lc∗ ) ≥ u(lc∗′ ) iff lc∗  lc∗′ ; similarly, u(lc∗ ) = u(lc∗′ ) iff lc∗ ∼ lc∗′ , and
u(lc∗ ) > u(lc∗′ ) iff lc∗ ≻ lc∗′ .
The key step in showing that we can reduce to a single menu is to show that,
roughly speaking, for each menu, there exists a menu-dependent function gM
such that u(gM (s)) = − supf ∈M u(f (s)). Stoye [21] proved a similar result, but
he assumed that all menus were obtained by taking the convex hull of a finite
set of acts. Because we allow arbitrary bounded menus, this result is not quite
true for us. For example, suppose that the range of u is (−1, ∞]. Then there
may be a menu M such that supf ∈M u(f (s)) = 5, so − supf ∈M u(f (s)) = −5.
But there is no act g such that u(g(s)) = −5, since u is bounded below by −1.
The following weakening of this result suffices for our purpose.

22

Lemma 3. There exists a utility function U such that for every menu M ,
there exists ǫ ∈ (0, 1] and constant act l∗ such that for all f, g ∈ M , f M
g ⇔ t(f ) t(M) t(g), where t has the form t(f ) = ǫf + (1 − ǫ)l∗ and t(M ) =
{t(f ) : f ∈ M }. Moreover, there exists an act gt(M) such that u(gt(M) (s)) =
− supf ∈t(M) u(f (s)) for all s ∈ S.
Proof. The nontriviality and monotonicity axioms imply there must exist prizes
x and y such that U (x) > U (y). We consider four cases.
Case 1: The range of U is bounded above and below. Then we can rescale
so that the range of U is [−1, 1]. Thus, there must be prizes x and y such that
U (x) = 1 and U (y) = −1. For all c ∈ [−1, 1], there must be a prize x′ that is
a convex combination of x and y such that u(x′ ) = c, so we can clearly define
a function gM such that, for all s ∈ S, we have u(gM (s)) = − supf ∈M u(f (s)).
Furthermore, we know that such a gM exists because it can be formed as an act
which maps each state to an appropriate lottery over the prizes x and y. More
generally, we know that an act with a certain utility profile exists if its utility
for each state is within the range of U . This fact will be used in the other cases
as well.
Thus, in this case we can take t to be the identity (i.e., ǫ = 1).
Case 2: The range of U is (−∞, ∞). Again, for all c ∈ (∞, ∞), there must
exist a prize x such that u(x) = c. Since menus are assumed to be bounded
above, we can again define the required function g and take ǫ = 1.
Case 3: The range of U is bounded above and unbounded below. Then we
can assume without loss of generality that the range is (−∞, 1], and for all c in
the range, there is a prize x such that u(x) = c. For all menus M , ǫ > 0, and
acts f, g ∈ M , by Independence, we have that
f M g ⇔ ǫf + (1 − ǫ)l1∗ ǫM+(1−ǫ)l∗1 ǫg + (1 − ǫ)l1∗ .
There exists an ǫ > 0 such that for all s ∈ S,
1 ≥ sup ǫu(f (s)) + (1 − ǫ) ≥ −1.
f ∈M

Let t(f ) = ǫf +(1−ǫ)l1∗. Clearly there exists an act gt(M) such that u(gt(M) (s)) =
− supf ∈t(M) u(f (s)) for all s ∈ S.
Case 4: The range of U is bounded below and unbounded above. By the
upper-boundedness axiom, every menu has an upper bound on its utility range.
Therefore, for every menu M , ǫ > 0, and all acts f and g in M , by Independence,
∗
∗
f M g ⇔ ǫf + (1 − ǫ)l−1
ǫM+(1−ǫ)l∗−1 ǫg + (1 − ǫ)l−1
.

There exists ǫ > 0 such that for all s ∈ S,
∗
sup ǫu(f (s)) + (1 − ǫ)u(l−1
(s)) ≤ 1.

f ∈M

∗
Let t(f ) = ǫf + (1 − ǫ)l−1
. Again, it is easy to see that gt(M) exists.

23

In light of Lemma 3, we henceforth assume that the utility function u derived
from U is such that its range is either (−∞, ∞), [−, 1, 1], (−∞, 1], or [−1, ∞).
In any case, its range always includes [−1, 1].
Before proving the key lemma, we establish some useful notation for acts
and utility acts. Given a utility act b, let fb , the act corresponding to b, be the
act such that fb (s) = lb(s) , if such an act exists. Conversely, let bf , the utility
act corresponding to the act f , be defined by taking bf (s) = u(f (s)). Note that
monotonicity implies that if fb = gb , then f ∼M g for all menus M . That is,
only utility acts matter. If c is a real, we take c∗ to be the constant utility act
such that c∗ (s) = c for all s ∈ S.
Lemma 4. Let M ∗ be the menu consisting of all acts f such that (−1)∗ ≤ bf ≤
+
0∗ . Then (U, P + ) represents M ∗ (i.e., M ∗ =S,X,U
M ∗ ,P + ) iff (U, P ) represents
M for all menus M .
Proof. Our arguments are similar in spirit to those of Stoye [21].
By Lemma 3, there exists t such that t(f ) = ǫf + (1 − ǫ)h for a constant
function h such that
f M g iff t(f ) t(M) t(g);
moreover, for this choice of t, the act gt(M) defined in Lemma 3 exists.
By Independence,
1
1
1
1
t(f ) + gt(M)  21 t(M)+ 12 gt(M ) t(g) + gt(M) .
2
2
2
2
Let M ∗ be the menu that contains all acts with utilities in [−1, 0]. By INA,
we know that for all acts f and g, and menus M for which gM is defined, we
have
1
1
1
1
f M g iff f + gM M ∗ g + gM .
2
2
2
2
t(f ) t(M) t(g) iff

This is because acts of the form 21 f + 12 gM are never strictly optimal with respect
to the menu 21 M + 12 gM . At every state there must be some act in 21 M + 21 gM
that has utility 0 (namely, the mixture that involves the act argmaxf ∈M u(f (s)).
Thus,
1
1
1
1
f M g iff t(f ) + gt(M) M ∗ t(g) + gt(M) .
2
2
2
2
Since the MWER representation also satisfies Independence and INA, we
know that for all menus M , and acts f and g in M ,
1
1
1
1
t(f ) + gt(M) S,X,U
t(g) + gt(M) .
∗ ,P +
M
2
2
2
2
Therefore, to show that M has a MWER representation with respect to
(U, P + ), it suffices to show that M ∗ has a MWER representation with respect
to (U, P + ).
S,X,U
f S,X,U
M,P + g ⇔ t(f ) t(M),P + t(g) ⇔

In the sequel, we drop the menu subscript when we refer to the family of
preferences, and just write  (to denote M ∗ ); by Lemma 4, it suffices to
consider M ∗ .
24

A.2

Defining a functional on utility acts

As we said, Stoye [20] also started his proof of a representation theorem for
MER by reducing to a single preference order M ∗ . He then noted that, the
expected regret of an act f with respect to a probability Pr and menu M ∗ is just
the negative of the expected utility of f . Thus, the worst-case expected regret
of f with respect to a set P of probability measures is the negative of the worstcase expected utility of f with respect to P. Thus, it sufficed for Stoye to show
that M ∗ had an MMEU representation, which he did by showing that M ∗
satisfied Gilboa and Schmeidler’s [6] axioms for MMEU, and then appealing to
their representation theorem.
This argument does not quite work for us, because now M ∗ does not satisfy
the C-independence axiom. (This is because our preference order M ∗ is based
on weighted regret, not regret.) However, we can get a representation theorem for weighted regret by using some of the techniques used by Gilboa and
Schmeidler to get a representation theorem for MMEU, appropriately modified
to deal with lack of C-independence. Specifically, like Gilboa and Schmeidler,
we define a functional I on utility acts such that the preference order on utility
acts is determined by their value according to I (see Lemma 6). Using I, we can
then determine the weight of each probability in ∆(S), and prove the desired
representation theorem.
Recall that u represents  on constant acts, and that only utility acts matter
to . The space of all utility acts is the Banach space B of real-valued functions
on S. Let B − be the set of nonpositive functions in B, where the function b is
nonpositive if b(s) ≤ 0 for all s ∈ S.
We now define a functional I on utility acts in B − such that for all f, g with
bf , bg ∈ B − , we have I(bf ) ≥ I(bg ) iff f  g. Let
Rf = {α′ : lα∗ ′  f }.
If 0∗ ≥ b ≥ (−1)∗ , then fb exists, and we define
I(b) = inf(Rfb ).
For the remaining b ∈ B − , we extend I by homogeneity. Let ||b|| = | mins∈S b(s)|.
Note that if b ∈ B − , then 0∗ ≥ b/||b|| ≥ (−1)∗ , so we define
I(b) = ||b||I(b/||b||).
∗
.
Lemma 5. If bf ∈ B − , then f ∼ lI(b
f)
∗
Proof. Suppose that bf ∈ B − and, by way of contradiction, that lI(b
≺ f . If
f)
∗
f ∼ l0 , then it must be the case that I(bf ) = 0, since I(bf ) ≤ 0 by definition
of inf, and f ∼ l0∗ ≻ lǫ∗ for all ǫ < 0 by Lemma 2, so I(bf ) > ǫ for all ǫ < 0.
∗
Therefore, f ∼ lI(b
. Otherwise, since bf ∈ B − , by monotonicity, we must have
f)
∗
∗
∗
l0 ≻ f , and thus l0 ≻ f ≻ lI(b
. By mixture continuity, there is some q ∈ (0, 1)
f)
∗
such that q · l0∗ + (1 − q) · lI(b
∼
l(1−q)I(bf ) ≺ f , contradicting the fact that I(b)
f)
is the greatest lower bound of Rf .

25

∗
∗
If, on the other hand, lI(b
≻ f , then lI(b
≻ f  lc∗ for some c ∈ R. If
f)
f)
f ∼ lc∗ then it must be the case that I(bf ) = c. I(bf ) ≤ c since lc∗  lc∗ , and
I(bf ) ≥ c since for all c′ < c, lc∗′ ≺ f ∼ lc∗ .
∗
Otherwise, lI(b
≻ f ≻ lc∗ , and by mixture continuity, there is some q ∈ (0, 1)
f)
∗
such that q ·lI(bf ) +(1−q)lc∗ ≻ f . Since qI(bf )+(1−q)c < I(bf ), this contradicts
the fact that I(bf ) is a lower bound of Rf . Therefore, it must be the case that
∗
lI(b
∼ f.
f)

We can now show that I has the required property.
Lemma 6. For all acts f, g such that bf , bg ∈ B − , f  g iff I(bf ) ≥ I(bg ).
∗
∗
Proof. Suppose that bf , bg ∈ B − . By Lemma 5, lI(b
∼ f and g ∼ lI(b
. Thus,
g)
f)
∗
∗
∗
∗
f  g iff lI(bf )  lI(bg ) , and by Lemma 2, lI(bf )  lI(bg ) iff I(bf ) ≥ I(bg ).

In order to invoke a standard separation result for Banach spaces, we extend
the definition of I to the Banach space B. We extend I to B by taking I(b) =
I(b− ) for b ∈ B − B − , where for all b ∈ B, b− is defined as
(
b(s), if b(s) ≤ 0,
−
b (s) =
0, if b(s) > 0.
Clearly b− ∈ B − and b = b− if b ∈ B − .
We show that the axioms guarantee that I has a number of standard properties. Since we have artificially extended I to B, our arguments require more
cases than those in [6]. (We remark that such an “artificial” extension seem unavoidable in our setting.) Moreover, we must work harder to get the result that
we want. We need different arguments from that for MMEU [6], since the preference order induced by MMEU satisfies C-independence, while our preference
order does not.
Lemma 7.

(a) If c ≤ 0, then I(c∗ ) = c.

(b) I satisfies positive homogeneity: if b ∈ B and c > 0, then I(cb) = cI(b).
(c) I is monotonic: if b, b′ ∈ B and b ≥ b′ , then I(b) ≥ I(b′ ).
(d) I is continuous: if b, b1 , b2 , . . . ∈ B, and bn → b, then I(bn ) → I(b).
(e) I is superadditive: if b, b′ ∈ B, then I(b + b′ ) ≥ I(b) + I(b′ ).
Proof. For part (a), If c is in the range of u, then it is immediate from the
defintion of I and Lemma 2 that I(c∗ ) = c. If c is not in the range of u, then
since [−1, 0] is a subset of the range of u, we must have c < −1, and by definition
of I, we have I(c∗ ) = |c|I(c∗ /|c|) = c.
For part (b), first suppose that ||b|| ≤ 1 and b ∈ B − (i.e., 0∗ ≥ b ≥ (−1)∗ ).
∗
Then there exists an act f such that bf = b. By Lemma 5, f ∼ lI(b)
. We
now need to consider the case that c ≤ 1 and c > 1 separately. If c ≤ 1, by
26

∗
Independence, cfb + (1 − c)l0∗ ∼ clI(b)
+ (1 − c)l0∗ . By Lemma 6, I(bcfb +(1−c)l∗0 ) =
I(bcl∗I(b) +(1−c)l∗0 ). It is easy to check that bcfb +(1−c)l∗0 = cb, and bcl∗I(b) + (1 −
c)l0∗ = cI(b)∗ . Thus, I(cb) = I(cI(b)∗ ). By part (a), I(cI(b)∗ ) = cI(b). Thus,
I(cb) = cI(b), as desired.
If c > 1, there are two subcases. If ||cb|| ≤ 1, since 1/c < 1, by what we have
just shown I(b) = I( 1c (cb)) = 1c I(cb). Crossmultiplying, we have that I(cb) =
cI(b), as desired. And if ||cb|| > 1, by definition, I(cb) = ||cb||I(bc/||cb||) =
c||b||I(b/||b||) (since bc/||cb|| = b/||b||). Since ||b|| ≤ 1, by what we have shows
1
I(b). Again, it follows
I(b) = I(||b||(b/||b||) = ||b||I(b/||b||), so I(b/||b||) = ||b||
that I(cb) = cI(b).
Now suppose that ||b|| > 1. Then I(b) = ||b||I(b/||b||). Again, we have two
subcases. If ||cb|| > 1, then

I(cb) = ||cb||I(cb/||cb||) = c||b||I(b/||b||) = cI(b).
And if ||cb|| ≤ 1, by what we have shown for the case ||b|| ≤ 1,
1
1
I(b) = I( (cb)) = I(cb),
c
c
so again I(cb) = cI(b).
For part (c), first note that if b, b′ ∈ B − . If ||b|| ≤ 1 and |b′ || ≤ −1, then the
acts fb and fb′ exist. Moreover, since b ≥ b′ , we must have (fb (s))∗  (fb′ )∗ (s)
for all states s ∈ S. Thus, by Monotocity, fb  fb′ . If either ||b|| > 1 or
||b′ || > 1, let n = max(||b||, ||b′ ||). Then ||b/n|| ≤ 1 and ||b′ /n|| ≤ 1. Thus,
I(b/n) ≥ I(b′ /n), by what we have just shown. By part (b), I(b) ≥ I(b′ ).
Finally, if either b ∈ B − B − or b′ ∈ B − B − , note that if b ≥ b′ , then b− ≥ (b′ )− .
By definition, I(b) = I(b− ) and I(b′ ) = I(b′ )− ; moreover, b− , (b′ )− ∈ B − . Thus,
by the argument above, I(b) ≥ I(b− ).
For part (d), note that if bn → b, then for all k, there exists nk such that
bn − (1/k)∗ ≤ bn ≤ bn + (1/k)∗ for all n ≥ nk . Moreover, by the monotonicity
of I (part (c)), we have that I(b − (1/k)∗ ) ≤ I(bn ) ≤ I(b + (1/k)∗ ). Thus, it
suffices to show that I(b − (1/k)∗ ) → I(b) and that I(b + (1/k)∗ ) → I(b).
To show that I(b − (1/k)∗ ) → I(b), we must show that for all ǫ > 0, there
exists k such that I(b − (1/k)∗ ) ≥ I(b) − ǫ. By positive homogeneity (part
(b)), we can assume without loss of generality that ||b − (1/2)∗ || ≤ 1 and that
||b|| ≤ 1. Fix ǫ > 0. If I(b − (1/2)∗ ) ≥ I(b) − ǫ, then we are done. If not, then
I(b) > I(b) − ǫ > I(b − (1/2)∗ ). Since ||b|| ≤ 1 and ||b − (1/2)∗ || ≤ 1, fb and
fb−(1/2)∗ exist. Moreover, by Lemma 6, fb ≻ f(I(b)−ǫ)∗ ≻ fb−(1/2)∗ . By mixture
continuity, for some p ∈ (0, 1), we have pfb + (1 − p)f(b−(1/2)∗ ≻ f(I(b)−ǫ)∗ . It is
easy to check that bpfb +(1−p)fb−(1/2)∗ = b − (1 − p)(1/2)∗ . Thus, by Lemma 6,
fb−(1−p)(1/2)∗  f(I(b)−ǫ)∗ , and I(b − (1 − p)1/2)∗ ) > I(b) − ǫ. Choose k such
that 1/k < (1 − p)(1/2). Then I(b − (1/k)∗ ) ≥ I(b − (1 − p)1/2)∗ ) > I(b) − ǫ,
as desired.
The argument that I(b + (1/k)∗ ) → I(b) is similar and left to the reader.
For part (e), first suppose that b, b′ ∈ B − . If ||b||, ||b− || ≤ 1, and I(b), I(b′ ) 6=
b
b′
b
b′
0, consider I(b)
and I(b
′ ) . Since I( I(b) ) = I( I(b′ ) ) = 1, it follows from Lemma 5
27

that f
p)f

b
I(b)

b′
I(b′ )

∼ f

f

b′
I(b′ )

. By Ambiguity Aversion, for all p ∈ (0, 1], pf
′

b
I(b)

+ (1 −

b
I(b)

I(b )
I(b)
b
b
b
b
. Thus, I( I(b)+I(b
′ ) I(b) + I(b)+I(b′ ) I(b′ ) ) ≥ I( I(b) ) = I( I(b′ ) ) = 1.
′

′

Hence, I(b + b′ ) ≥ I(b) + I(b′ ).
If b, b− ∈ B − and either ||b|| > 1 or ||b′ || > 1, and both I(b) 6= 0 and
′
I(b ) 6= 0, then the result easily follows by positive homogeneity (property (b)).
∗
∗
If b, b− ∈ B− and either I(b) = 0 or I(b′ ) = 0, let bn = b− n1 and b′n = b′ − n1 .
′
′
′
Clearly ||bn || > 0, ||bn || > 0, bn → b, and bn → bn . By our argument above,
I(bn + b′n ) ≥ I(bn ) + I(b′n ) for all n ≥ 1. The result now follows from continuity.
Finally, if either b ∈ B − B − or b′ ∈ B − B − , observe that
 −
= b (s) + b′− (s), if b(s) ≤ 0, b′ (s) ≤ 0



= b− (s) + b′− (s), if b(s) ≥ 0, b′ (s) ≥ 0
(b + b′ )− (s)
≥ b− (s) + b′− (s), if b(s) > 0, b′ (s) ≤ 0


 −
≥ b (s) + b′− (s), if b(s) ≤ 0, b′ (s) > 0.

Therefore, (b + b′ )− ≥ b− + b′− . Thus, I(b + b′ ) = I((b + b′ )− ) ≥ I(b− + b′− ) by
the monotonicity of I, and I(b− + b′− ) ≥ I(b− ) + I(b′− ) by superadditivity of I
on B − . Therefore, I(b + b′ ) ≥ I(b) + I(b′ ).

A.3

Defining the weights

In this section, we use I to define a weight αPr for each probability Pr ∈ ∆(S).
The heart of the proof involves showing that the resulting set P + so determined
gives us the desired representation.
Given a set P + of weighted probability measures, for b ∈ B − , define
X
NWREG(b) = inf αPr (
b(s) Pr(s)).
Pr∈P

s∈S

Note that NWREG is the negative of the weighted regret when the menu is B − .
Define
X
b(s) Pr(s).
NREG(b) = inf
Pr∈P

and

NREG Pr (b) =

X

s∈S

b(s) Pr(s) = EPr b.

s∈S

For each probability Pr ∈ ∆(S), define
αPr = sup{α ∈ R : αNREG Pr (b) ≥ I(b) for all b ∈ B − }.

(1)

Note that αPr ≥ 0 for all distributions Pr ∈ ∆(S), since 0 ≥ I(b) for b ∈ B −
(by monotonicity); and αPr ≤ 1, since NREG Pr ((−1)∗ ) = I((−1)∗ ) = −1 for
all distributions Pr. Thus, αPr ∈ [0, 1]. Moreover, it is immediate from the
definition of αPr that αPr NREG Pr (b) ≥ I(b) for all b ∈ B − . The next lemma
shows that there exists a probability Pr where we have equality.
28

Lemma 8.

(a) For some distribution Pr, we have αPr = 1.

(b) For all b ∈ B − , there exists Pr such that αPr NREG Pr (b) = I(b).
Proof. The proofs of both part (a) and (b) use a standard separation result: If
U is an open convex subset of B, and b ∈
/ U , then there is a linear functional
λ that separates U from b, that is, λ(b′ ) > λ(b) for all b′ ∈ U . We proceed as
follows
For part (a), we must show that for some Pr, for all b ∈ B − , NREG Pr (b) ≥
I(b). Since NREG Pr (b) = EPr b, it suffices to show that EPr (b) ≥ I(b) for all
b ∈ B−.
Let U = {b′ ∈ B : I(b′ ) > −1}. U is open (by continuity of I), and convex
(by positive homogeneity and superadditivity of I), and (−1)∗ ∈
/ U . Thus, there
exists a linear functional λ such that λ(b′ ) > λ((−1)∗ ) for b′ ∈ U .
We want to show that λ is a positive linear functional, that is, that λ(b) ≥ 0
if b ≥ 0∗ . Since 0∗ ∈ U , and λ(0∗ ) = 0, it follows that λ((−1)∗ ) < 0. Since λ
is linear, we can assume without loss of generality that λ((−1)∗ ) = −1. Thus,
for all b′ ∈ B − , I(b′ ) > −1 implies λ(b′ ) > −1. (The fact that I(cb′ ) = I(0∗ )
follows from the definition of I on elements in B − B −.) Suppose that c > 0 and
b′ ≥ 0∗ . From the definition of I, it follows that I(cb′ ) = I(0∗ ) = 0 > −1. So
cλ(b′ ) = λ(cb′ ) > −1, so λ(b′ ) > −1/c. Since this is true for all c > 0, it must
be the case that λ(b′ ) ≥ 0. Thus, λ is a positive functional.
Define the probability distribution Pr on S by taking Pr(s) = λ(1s ). To
see that Pr is indeed a probability distribution, note
P that since 1s ≥ 0 and λ
is positive, we must have λ(1s ) ≥ 0. Moreover, s∈S Pr(s) = λ(1∗ ) = 1. In
addition, for all b′ ∈ B, we have
X
X
λ(b′ ) =
λ(1s )b′ (s) =
Pr(s)b′ (s) = EPr (b′ ).
s∈S

s∈S

Next note that, for b ∈ B − ,
for all c < 0, if I(b) > c, then λ(b) > c.

(2)

For if I(b) > c, then I(b/|c|) > −1 by positive homogeneity, so λ(b/|c|) > −1
and λ(b) > c. The result now follows. For if b ∈ B − , then I(b) ≤ I(0∗ ) = 0 by
monotonicity. Thus, if c < I(b), then c < 0, so, by (2), λ(b) > c. Since λ(b) > c
whenever I(b) > c, it follows that EPr (b) = λ(b) ≥ I(b), as desired.
The proof of part (b) is similar to that of part (a). We want to show that,
given b ∈ B − , there exists Pr such that αPr NREG Pr (b) = I(b). First supose
that ||b|| ≤ 1. If I(b) = 0, then there must exist some s such that b(s) = 0, for
otherwise there exists c < 0 such that b ≤ c∗ , so I(b) ≤ c. If b(s) = 0, let Prs
be such that Prs (s) = 1. Then NREG Prs (b) = 0, so (b) holds in this case.
If ||b|| ≤ 1 and I(b) < 0, let U = {b′ : I(b′ ) > I(b)}. Again, U is open and
convex, and b ∈
/ U , so there exists a linear functional λ such that λ(b′ ) > λ(b) for
′
b ∈ U . Since 0∗ ∈ U and λ(0∗ ) = 0, we must have λ(b) < 0. Since (−1)∗ ≤ b,
(−1)∗ is not in U , and therefore we also have λ((−1)∗ ) < 0. Thus, we can
29

assume without loss of generality that λ((−1)∗ ) = −1, and hence λ((1)∗ ) = 1.
The same argument as above shows that λ is positive: for all c > 0 and b′ ≥ 0∗ ,
I(cb′ ) = 0 as before. Since I(b) < 0, it follows that I(cb′ ) > I(b), so cb′ ∈ U
and λ(cb′ ) > λ(b) ≥ λ((−1)∗ ) = −1. Thus, as before, for all c > 0, b′ ≥ 0∗ ,
λ(b′ ) > −1
c , so λ is a positive functional.
Therefore, λ determines a probability distribution Pr such that, for all b′ ∈
B − , we have λ(b′ ) = EPr (b′ ). This, of course, will turn out to be the desired distribution. To show this, we need to show that αPr = I(b)/NREG Pr (b). Clearly
αPr ≤ I(b)/NREG Pr (b), since if α > I(b)/NREG Pr (b), then αNREG Pr (b) <
I(b) (since NREG Pr (b) = λ(b) < 0). To show that αPr ≥ I(b)/NREG Pr b, we
must show that (I(b)/NREG Pr (b))NREG Pr (b′ ) ≥ I(b′ ) for all b′ ∈ B − . Equivalently, we must show that I(b)λ(b′ )/λ(b) ≥ I(b′ ) for all b′ ∈ B − .
Essentially the same argument used to prove (2) also shows
for all c > 0, if I(b′ ) > cI(b), then λ(b′ ) > cλ(b).
′

In particular, if I(b′ ) > cI(b), then by positive homogeneity, I(bc ) > I(b), so
b′
b′
′
c ∈ U , and λ( c ) > λ(b) and hence λ(b ) > cλ(b).
′
Thus, if I(b )/(−I(b)) > c and c < 0, then I(b′ ) > −cI(b), and hence
′
λ(b )/(−λ(b)) > c. It follows that λ(b′ )/(−λ(b)) ≥ I(b′ )/(−I(b)) for all b′ ∈ B − .
Thus, I(b)λ(b′ )/λ(b) ≥ I(b′ ) for all b′ ∈ B − , as required.
Finally, if ||b|| > 1, let b′ = b/||b||. By the argument above, there exists
a probability measure Pr such that αPr NREG Pr (b/||b||) = I(b/||b||). Since
NREG Pr (b/||b||) = NREG Pr (b)/||b||, and I(b/||b||) = I(b)/||b||, we must have
that αPr NREG Pr (b) = I(b).
We can now complete the proof of Theorem 1. By Lemma 8 and the definition of αPr , for all b ∈ B − ,
I(b) =

inf

Pr∈∆(S)

=

inf

αPr NREG(b)

Pr∈∆(S)

= sup
Pr∈P

αPr

X

−αPr

!

b(s) Pr(s)

s∈S

X

(3)

!

b(s) Pr(s) .

s∈S

Recall that, by Lemma 6, for all acts f, g such that bf , bg ∈ B − , f  g iff
I(bf ) ≥ I(bg ). Thus, f  g iff
!
!
X
X
−αPr
−αPr
sup
u(f (s)) Pr(s) ≤ sup
u(g(s)) Pr(s) .
Pr∈∆(S)

s∈S

Pr∈∆(S)

s∈S

Note that, for f ∈ M ∗ = B − , we have reg M ∗ ,Pr (f ) = sup(−u(f (s)) Pr(s), since
+
0∗ dominates all acts in M ∗ . Thus, =S,Y,U
= {(Pr, αPr : Pr ∈
M ∗ ,P + , where P

30

∆(S)}. By Lemma 4, this means (U, P + ) represents M for all menus M , as
required.
We have already observed that U is unique up to affine transformations,
so it remains to show that P + is maximal. This follows from the defini′
′ +
tion of αPr . If M =S,Y,U
M,(P ′ )+ , and (α , Pr) ∈ (P ) , then we claim that
α′ ∈ {α ∈ R : αNREG Pr (b) ≥ I(b) for all b ∈ B − }. If not, there would be
some b ∈ B − with ||b|| ≤ 12 , such that α′ NREG Pr (b) < I(b), which, by the
S,Y,U
S,Y,U
∗
∗
definition of ≺S,Y,U
M ∗ ,(P ′ )∗ , means that l−1 ≺M ∗ ,(P ′ )+ fb ≺M ∗ ,(P ′ )+ lI(b) . Recall that I(bf ) = inf{γ : lγ∗ M ∗ f }. Moreover, since ≺S,Y,U
M ∗ ,(P ′ )+ satisfies
the Mixture Continuity, there exists some p ∈ (0, 1) such that fb ≺S,Y,U
M ∗ ,(P ′ )+
S,Y,U
∗
∗
∗
pl−1
+ (1 − p)lI(b)
≺S,Y,U
M ∗ ,(P ′ )+ ≺M ∗ ,(P ′ )+ lI(b) . This contradicts the definition of
I(b). Therefore, α′ ∈ {α ∈ R : αNREG Pr (b) ≥ I(b) for all b ∈ B − }, and hence
α′ ≤ αPr .

A.4

Uniqueness of Representation

In the preceding sections, we have shown that if a family of menu-dependent
preferences M satisfies axioms 1 − 10, then M can be represented as minimizing weighted expected regret with respect to a canonical set P + of weighted
probabilities and a utility function. We now want to show uniqueness.
In this section, we show that the canonical set of weighted probabilities we
constructed, when viewed as a set of subnormal probability measures, is regular and includes at least one proper probability measure. Moreover, this set of
sub-probability measures is the only regular set that induces a family of preferences M that satisfies axioms 1 − 10. Our uniqueness result is analogous
to the uniqueness results of Gilboa and Schmeidler [7], who show that the convex, closed, and non-empty set of probability measures in their representation
theorem for MMEU is unique.
By Lemma 4, it suffices to consider the preference relation M ∗ . The argument is based on two lemmas: the first lemma says that the canonical set of
sub-probability measures is regular; and the second lemma says that a set of subprobability measures representing M ∗ that is regular and contains at least one
proper probability measure is unique. The proof of this second lemma, like the
proof of uniqueness in Gilboa and Schmeidler [7], uses a separating hyperplane
theorem to show the existence of acts on which two different representations
must ‘disagree’. However, a slightly different argument is required in our case,
since our acts in M ∗ must have utilities corresponding to nonpositive vectors in
R|S| .
Lemma 9. Let P + be the canonical set of weighted probability measures representing M ∗ . The set C(P + ) of sub-probability measures is regular.
Proof. It is useful to note that, by definition, p ∈ C(P + ) if and only if
Ep (b) ≥ I(b) for all b ∈ B −

31

(where expectation with respect to a subnormal probability measure is defined
in the obvious way).
Recall that a set is regular if it is convex, closed, and downward-closed.
We first show that C(P + ) is downward-closed. Suppose that p ∈ C(P + ) and
q ≤ p (i.e., q(s) ≤ α Pr(s) for all s ∈ S. Since p ∈ C(P + ), Ep (b) ≥ I(b)
for all b ∈ B − . Since q ≤ p and, if b ∈ cB − , we have b ≤ 0∗ , it follows that
Eq (b) ≥ Ep (b) ≥ I(b) for all b ∈ B − , and thus q ∈ C(P + ).
To see that C(P + ) is closed, let p = limn→∞ pn , where each pn ∈ C(P + ).
Since pn ∈ C(P + ) it must be the case that Epn (b) ≥ I(b) for all b ∈ B − . By
the continuity of expectation, it follows that Ep (b) ≥ I(b) for all b ∈ B − . Thus,
p ∈ C(P + ).
To show that C(P + ) is convex, suppose that p, q ∈ C(P + ). Then Ep (b) ≥
I(b) and Eq (b) ≥ I(b) for all b ∈ B − . It easily follows that for all a ∈ (0, 1),
Eap+(1−a)q (b) ≥ I(b) for all b ∈ B − . Thus, ap + (1 − a)q ∈ C(P + ).
Lemma 10. A set of sub-probability measures representing M ∗ that is regular,
and has at least one proper probability measure is unique.
Proof. Suppose for contradiction that there exists two regular sets of subnormal
probability distributions, C1 and C2 , that represent M ∗ and have at least one
proper probability measure.
First, without loss of generality, let q ∈ C2 \C1 . We actually look at an
extension of C1 that is downward-closed in each component to −∞. Let C 1 =
{p ∈ R|S| : p ≤ p′ }. Note an element p of C 1 may not be subnormal probability
measures; we do not require that p(s) ≥ 0 for all s ∈ S. Since C 1 and {q} are
closed, convex, and disjoint, and {q} is compact, the separating hyperplane
theorem [15] says that there exists θ ∈ R|S| and c ∈ R such that
θ · p > c for all p ∈ C 1 , and θ · q < c.

(4)

By scaling c appropriately, we can assume that |θ(s)| ≤ 1 for all s ∈ S. Now we
argue that it must be the case that θ(s) ≤ 0 for all s ∈ S (so that θ corresponds
to the utility profile of some act in M ∗ ). Suppose that θ(s′ ) > 0 for some s′ ∈ S.
By (4), θ · p > c for all p ∈ C 1 . However, consider p∗ ∈ C 1 defined by
(
0, if s 6= s′
p∗ (s) = −|c|
′
θ(s) , if s = s .
Clearly, θ · p∗ ≤ c, contradicting (4). Thus it must be the case that θ(s) ≤ 0 for
all s ∈ S.
Consider the θ given by the separating hyperplane theorem, and let f be
an act such that u ◦ f = θ. By continuity, f ∼M ∗ ld∗ for some constant act ld∗ .
Since C1 and C2 both represent M ∗ , and C1 and C2 both contain a proper
probability measure,
min p · (u ◦ f ) = min p · (u ◦ ld∗ ) = d = min p · (u ◦ f ).

p∈C1

p∈C1

p∈C2

32

However, by (4),
min p · (u ◦ f ) > c > min p · (u ◦ f ),
p∈C1

p∈C2

which is a contradiction.




Conditioning is the generally agreed-upon
method for updating probability distribu­
tions when one learns that an event is cer­
tainly true. But it has been argued that
we need other rules, in particular the rule
of cross-entropy minimization, to handle up­
dates that involve uncertain information. In
this paper we re-examine such a case: van
Fraassen's Judy Benjamin problem [1987],
which in essence asks how one might update
given the value of a conditional probability.
We argue that-contrary to the suggestions
in the literature-it is possible to use simple
conditionalization in this case, and thereby
obtain answers that agree fully with intu­
ition. This contrasts with proposals such as
cross-entropy, which are easier to apply but
can give unsatisfactory answers. Based on
the lessons from this example, we speculate
on some general philosophical issues concern­
ing probability update.
1

INTRODUCTION

How should one update one's beliefs, represented as a
probability distribution Pr over some space S, when
new evidence is received? The standard Bayesian an­
swer is applicable whenever the new evidence asserts
that some event T � S is true (and furthermore, this
is all that the evidence tells us). In this case we simply
condition on T, leading to the distribution Pr{·IT).
For successful "real-world" applications of probability
theory so far, conditioning has been a mostly sufficient
answer to the problem of update. But many people
have argued that conditioning is not a philosophically
adequate answer (in particular, [Jeffrey 1983]). Once
we try to build a truly intelligent agent interacting in
complex ways with a rich world, conditioning may end
up being practically inadequate. as well.
The problem is that some of the information that we
receive is not of the form "Tis (definitely) true" for

any T. What would one do with a constraint such as
" Pr ( T ) = 2/3" or "the expected value of some ran­
dom variable on S is 2/3". We cannot condition on
this information, since it is not an event in S. Yet
it is clearly useful information. So how should we in­
corporate it? There is in fact a rich literature on the
subject (e.g., see [Bacchus, Grove, Halpern, and Koller
1994; Diaconis and Zabell 1982; Jeffrey 1983; Jaynes
1983; Paris and Vencovska 1992; Uffink 1995]). Most
proposals attempt to find the probability distribution
that satisfies the new information and is in some sense
the "closest" to the original distribution Pr. Certainly
the best known and most studied of these proposals is
to use the rule of minimizing cross-entropy [Kullback
and Leibler 1951] as a way of updating with general
probabilistic information. This rule can also be shown
to generalize Jeffrey's rule [Jeffrey 1983], which in turn
generalizes conditioning.
But is cross-entropy ( CE) really such a good rule? The
traditional justifications of CE are that it satisfies vari­
ous sets of criteria (such as those of [Shore and Johnson
1980]) which, while plausible, are certainly not com­
pelling [Uffink 1995]. Van Fraassen, in a paper entitled
"A problem for relative information [CE] minimizers
in probability kinematics" [1981] instead approached
the question in a different way: he looked at how CE
behaves on a simple specific example. He calls his ex­
ample the Judy Benjamin (JB) problem; in essence
it is just the question of how one should update by a
conditional probability assertion, i.e., "Pr(AIB) = c"
for some events A, B and c E [0, 1].
As we now explain, van Fraassen uncovers what seems
(to us) to be an unintuitive feature of cross-entropy,
although in later papers on the same issue he endorses
CE and a family of other similar rules. Furthermore,
none of his rules agree with most people's strong in­
tuition about the solution to his problem. The pur­
pose of this paper is to give a new analysis, which is
based on simple conditionalization, and is (we argue)
in good agreement with people's expectations. Our
hope is that the example and our analysis will be an
instructive study of the subtleties involved in proba­
bility update, and in particular the dangers involved
in indiscriminately applying supposedly "simple" and

Probability Update: Conditioning vs. Cross-Entropy

"general" rules like CE.
Van Fraassen explains the JB problem as follows
[1987]:
[The story] derives from the movie Private
in which Goldie Hawn, playing
the title character, joins the Army. She and
her platoon, participating in war games on
the side of the "Blue Army", are dropped in
the wilderness, to scout the opposition ("Red
Army"). They are soon lost. Leaving the
movie script now, suppose the area is di­
vided into two halves, Blue and Red territory,
which each territory is divided into Head­
quarters Company area and Second Com­
pany area. They were dropped more or less
at the center, and therefore feel it is equally
likely that they are now located in one area
as in another. This gives us the following
muddy Venn diagram, drawn as a map of the
area:
Benjamin,

1/4

Red2nd

1/4

RedHQ

12
B !Ue
They have some difficulty contacting their
own HQ by radio, but finally succeed and de­
scribe what they can see around them. After
a while, the office at HQ radios: "I can't be
sure where you are. If you are in Red ter­
ritory, the odds are 3:1 that you are in HQ
Company area ..." At this point the radio
gives out.
We must now consider how Judy Benjamin
should adjust her opinions, if she accepts this
radio message as the sole and correct con­
straint to impose. The question on which we
should focus is: what does it do to the proba­
bility that they are in friendly Blue territory?
Does it increase, or decrease, or stay at its
present level of l /2?
The intuitive response is that the message should not
change the a priori probability of 1/2 of being in Blue
territory. More precisely, according to this response,
Judy's posterior probability distribution should place
probability 1/4 on being at each of the two quad­
rants in the Blue territory, probability 3/8 on being
in the Headquarters Company area of Red territory,
and probability 1/8 on being in the Second Company
area of Red territory. Van Fraassen [1987] notes that
his many informal surveys of seminars and conference
audiences find that people overwhelmingly give this
answer.

209

However, this intuitive answer is inconsistent with
cross-entropy. In fact, it can be shown that cross­
entropy has the rather peculiar property that if HQ
had said "If you are in Red territory, the odds are
o: : 1 that you are in HQ company area ... ", then for
all o: f: 1, the posterior probability of being in Blue
territory (according to the distribution that minimizes
cross-entropy and satisfies this constraint) would be
greater than 1/2; it would stay at 1/2 only if o: = 1.
This seems (to us, at least) highly counterintuitive.
Why should Judy come to believe she is more likely to
be in Blue territory, almost no matter what the mes­
sage says about o:? For example, what if Judy knew
in advance that she would receive such a message for
some a f: 1, and simply did not know the value of
o:. Should she then increase the probability of being
in Blue territory even before hearing the message? As
van Fraassen [1981] says, as part of an extended dis­
cussion of this phenomenon:
It is hard not to speculate that the dangerous
implications of being in the enemy's head­
quarters area are causing Judy Benjamin to
indulge in wishful thinking... 1
However, as van Fraassen points out (crediting Peter
Williams for the observation), there also seems to be a
problem with the intuitive response. Presumably there
is nothing special about hearing the odds of being in
Red territory are 3:1. The posterior probability of be­
ing in Blue territory should be 1/2 no matter what
the odds are, if we really believe that this information
is irrelevant to the probability of being in Blue terri­
tory. But if o: = 0 then, to quote van Fraassen [1987],
"he would have told her, in effect, 'You are not in Red
Second Company area'". Assuming that this is indeed
equivalent, it seems that Judy could have used simple
conditionalization, with the result that her posterior
probability of being in Blue territory would be 2/3,
not 1/2.

In [van Fraassen 1987; van Fraassen, Hughes, and
Harman 1986], van Fraassen and his colleagues for­
mulate various principles that they argue an update
rule should satisfy. Their first principle is motivated
by the observation above and simply says that, when
conditioning seems applicable, the answer should be
that obtained by conditioning. To state this more pre­
cisely, let q = o:/(l+o:) be the probability (rather than
the odds) of being in red HQ company area given that
Judy is in Red territory. In the case of the JB problem,
the first principle becomes:
If q = 1 the prior is transformed by simple
conditionalization on the event "Red HQ area
or Blue territory"; if q = 0 by simple condi­
tionalization on "Red 2nd company area or
Blue territory".

1We remark that this behavior of CE has also been
cussed and criticized
1987].

in

a

dis­

more general setting [Seidenfeld

210

Grove and Halpern

This first principle already eliminates the intuitive
rule, i.e., the rule that the posterior probability should
stay at 1/2 no matter what q is. (Note also that we
cannot make the rule consistent with this principle by
trea.ting q = 0 and q = 1 as special cases, unless we are
prepared to accept a rule that is discontinuous in q.)
For van Fraassen, this is apparently a decisive refuta­
tion of the intuitive rule, which he thus says is flawed
[van Fraassen 1987].

This does not deny the usefulness of rules like CE.
There will be some (perhaps large) family of situations
in which CE is indeed appropriate, and we would like
to better understand what this family is and how to
recognize it. But if CE (or any other rule) is blindly
applied whenever the information is of the appropriate
syntactic form, we should not be surprised if the results
are often unexpected and unhelpful.

However, in this paper we give a new2 and simple anal­
ysis of the JB problem. We believe that our solution is
well-motivated, and it agrees completely with the in­
tuitive answer. It thus also does not exhibit the coun­
terintuitive behavior of CE.

2

Our basic idea is simply to use conditioning, but to do
so in a larger space where it makes sense (i.e., where
the information we receive is an event). Of course,
people have always realized that this option is avail­
able. It is perhaps not popular because it appears to
pose certain serious philosophical and practical prob­
lems as a general approach. In particular, which lar ger
space do we use? There may be many equally natural
possibilities, leading to different answers, so the rule
will be indeterminate. Also possible is that all ex­
tended spaces we can think of seem equally unnatural
and contrived; again, we will be stuck. In addition,
there is the practical concern that a rich enough space
might be vastly larger and more complicated to work
in than the original.
Against this, a rule like cross-entropy seems extremely
attractive. It provides a single general recipe which
can be mechanically applied to a huge space of up­
dates. Even families of rules, such as van Fraassen
proposes, are not so bad: aft er one has chosen a rule
(usually by selecting a single real-valued parameter
[Uffink 1995]), the rest is again mechanical, general,
and determinate. Furthermore, all these rules work in
the original spaceS, without requiring expansion, and
so may be more practical in a computational sense.
all we do in this paper is analyze one particular
problem, we must be careful in making general state­
ments on the basis of our results. Nevertheless, they
do seem to support the claim that sometimes, the only
"right" way to update, especially in an incompletely
specified situation, is to think very carefully about the
real nature and origin of the information we receive,
and then (try to) do whatever is necessary to find a
suitable larger space in which we can condition. If
this doesn't lead to conclusive results, perhaps this is
because we do not understand the information well
enough to update with it. However much we might
wish for one, a genemlly satisfactory mechanical rule
such as cross-entropy, which saves us all this question­
ing and work, probably does not exist.
Since

2 Although

we are aware of no published analysis simi­

lar to our own, we have learned that Seidenfeld has earlier
presented a closely related analysis in several lectures [Sei­
denfeld 1997].

CONDITIONING

In this section, we present our alternative analysis of
the JB problem. In the following, let B1, B2, R�, R2
denote the events that Judy is in, respectively, Blue
HQ, Blue Second Company, Red HQ, and Red Second
Company areas. Let B = B1 V B2 and R = R1 V R2.
The message HQ sent, "If you are in Red territory,
the odds are 3:1 that you are in HQ Company area",
is equivalent to asserting that the conditional proba­
bility of R1 given R is true is 3/4. In general, let M(q)
be the similar message asserting that this probability
is q E [0, lj for some q not necessarily = 3/4 (i.e., the
announced odds are q/(1 - q) : 1 instead of 3 : 1).
We use Prjrior to denote Judy's prior beliefs (i.e., be­
fore the message is received) and Prj to denote her
posterior distribution after receiving M(q).
The key step is to re-examine the problem from the be­
ginning, and ask ourselves how Judy should treat HQ's
message. Note t hat Van Ftaassen explicitly assumes,
in his statement of the problem, that HQ's statement
should be treated as a constraint on Judy's beliefs.
Thus, he interprets it as an imperative: "Make your
beliefs be such that this is true!" This interpretation
of probabilistic informat ion as constraints is a common
one (especially in the context of CE), but is difficult
to justify [Uffi.nk 1996]. Van Fraassen is, of course,
quite aware of the philosophical issues raised by his
interpretation; see [van Fraassen 1980].

But is the interpretation that HQ's stat ement should
be regarded as a constraint on Judy's beliefs the only
possible one? Note that, as the story is presented, it
certainly sounds as though HQ was trying to give Judy
some true and useful information. But, at the time
M is sent, the statement that Prjriar(RtJR) 3/4 is
clearly not true of Judy's beliefs. Thus, if we wish to
interpret M as referring directly to Judy's beliefs, we
will be unable to regard it as a factual assertion in any
straightforward sense.
=

But suppose we instead view M(q) as being a cor­
rect statement regarding HQ 's beliefs; i.e., as asserting
PrnQ(RtJR) = q where PrnQ denotes HQ's distribu­
tion over where Judy might be. This certainly seems
to be a reasonable interpretation in light of the story.
How should Judy react to it? Unfortunately, the story
does not give us enough information to be able to pro­
vide a definite answer to this question. Judy's correct
reaction to the message depends on aspects of the situ­
ation that were not included in the problem statem ent.

Probability Update: Conditioning vs. Cross-Entropy

The followi n g is a partial list of things that could be
relevant: W hat does Judy know about HQ's beliefs
and knowledge? How did HQ expect Judy to react to
the message, and what did Judy know about these ex­
pectations? What messages could HQ have sent? For
instance , might HQ have sent M(q) for some other
value of q if it were appropriate, or would it have said
something else entirely if q # 3/4? Does Judy believe
that HQ even has th e option of sen ding messages that
are not of the form M(q); if so, what mess ag es?3 And
so o n .
We now give one particular analysis,
filling in such missing details in what

which follows by
we feel is a plau­
sible way. As we said, we assume that M(3/4) is a fac­
tual statement about HQ's beliefs. We further assume
that, no matter what HQ actually knows, its message

would always have been simply M(q) for the appropri­
a te value of q. Thus, Judy can read nothing more into
M(q) other than to regard it as a true statement about
HQ's beliefs. As noted in the previous paragraph , even
to get this far relies on several strong assumptions .
Once Judy he ars M(q), it seems n at ural to want to
The problem is t hat , as yet, we
condition on it.
have not introduced a space in which M(q) is an
ev ent. Of course, there are many possible such spaces.
To construct an appropriate one, we must consider
how Jud y models HQ ' s beli efs , and what she believes
about these beliefs. Again, here we make perhaps
the simplest possible choice. We suppose that Judy
models HQ's beliefs as a distribution ove r the four

R1, R2, B1, B2.

Pr��c be the distribution on {R11 R2, B11 B2} such that Pr��c (R1) = a,
p a, ,c
P rHQa ,b , c (R2 ) = b, pr a,b,c
HQ (B1 ) = c, r HbQ (B2 ) =
1 - a - b - c. Thus the set of all possible distri­

quadrants

butions

HQ

Let

might have, given our assumptions, is

PHQ = {Pr��cla,b,c2:0, a+b+c�l}. In the
following , we view Pr nQ (R 1 I R), Pr HQ( RI) , Pr HQ(B ) ,
and so on, as random variables on the space PHQ·
Thus, for example, "Pr nQ(R1IR) < q" denotes the
event {Pr��c I Pr';;� c (R1IR) < q}.
Again, we stress that we are not forced to use P HQ·
Judy might actually have a richer model of HQ's be­
liefs (e.g., she might think that HQ makes finer geo­
graphical distinctions than simply the four quadrants)
or a coarser model (e.g., J udy might take as the space
of possibilities the possible values of PrnQ(RliR), and

ab out HQ' s

211

bel iefs .

Since Judy d oes not know what HQ actually believes,
her beliefs will be a distribution over distributions,
i.e., a dis tribu tion over PnQ·
Which distribution?
Again, we have many choices, but a na tural one is
to suppose that before Judy hears the message, she
considers a uniform distribution over ( a, b, c) t uples .
Formally, we consider the distribution function defined

by Prj7�q{Pr�8c I a::; A,b::; B,c::; C}) =ABC,
so that the density function is j ust 1. We also use the
notation Prj HQ to denote Judy's beliefs about HQ's
/
beliefs after receiving M(q).
It might be thought that, having decided to take PH Q
as the set of possible beliefs that HQ could have and
given the (implicit) assumption that Ju d y is initially
completely ignorant of HQ's beliefs, the prior density
on P HQ is determined complet ely. Unfortunately, this
is not the case. Ther e is no unique "uniform distribu­
tion" on PnQ· Uniformity depends on how we choose
to parameterize the space. We have chosen to param­
eterize the elements of this space by a triple (a, b, c)
denoting the probabilities of R1, R2, and Bt, respec­

tively. However, we could have chosen to characterize
an element of the space by a triple (a', b', c') denot­
ing the square of the probabilities of R1, R2, and B1,
re spec tivel y. Or perhaps more reasonably, we could
have chosen to characterize an element by a triple
( a11, b11, c") denoting the probability of R, the probabil­
ity of R1 given R, and the probability of B1 given B.
A uniform distribution with respect to either of these
parameterizations would be far from uniform with re­
spect to the parameterization we have chosen, and vice
versa.4 This is, of course, just an instan ce of the well­
known impossibility of defining a unique notion of 'l.mi­
form in a continuous sp ace [H ow son and U rbach 1989].
Since M(q) is an event in the new space 'PHQ, Judy
should be able to condition on it. One might object
that, since M(q) is an event of measure 0, condition­
ing is not well defined. This is t rue, but there are
t w o (closely related) ways of dealing with this prob­
lem. The more elementary and intuitive approach is
based on the observation that, in practice, HQ w ill
n ot (in general) be able to announce its value for
Pr nQ(RI IR) exactly, since this could require HQ to
announce an infinite-precision real number. It seems
more reasonable to regard the announced value of q
as being rounded or approximated in some fashion. In

not reason about the rest of HQ's distribution). How­
ever, given the description of the story, PHQ seems to
be the most natural space for Judy to model her beliefs

particular, we might suppose that M(q) really means
PtHQ(RIIR) E [q - E, q + e) for some small val ue
E > 0. This event has non-zero probability according

first

4We note, however, that the uniform densities with
respect to all 4 possible parameterizations that involv­

3To see the possible relevance of this, note that if there
are other possible messages, then the very fact that HQ's
message was not one of these others could be impor­

tant information in and of itself: Judy might reason that

M(3/4)

must have been the most important fact HQ pos­

sessed. On the other hand, since the radio died before the
message was completed, such inferences depend heavily on
the protocol Judy expects HQ to follow .

to Prj7�'Q,

and

so

conditioning is unproblematic.

ing choosing 3 out of the 4 probabilities from PrnQ(Rt),
Pr nQ(R2 ) , PrnQ(Bt), PrnQ(Bz ) do lead to the same dis­
tribution over PnQ. and so o ur decision to use the first
three of these probabilities does not affect

our

analysis.

Grove and Halpern

212

The second approach directly invokes the standard def­
inition of conditioning on (the value of) a random
variable. We briefly review the details here. Sup­
pose we have two random variables X and Y.
If

Pr(X

=

a)

>

Pr(Y

0, then

defined as Pr(Y = b (l X =
would expect. If Pr(X = a )

bJX

) is just
a) as we
0, then we take the

=

a)/ Pr(X

=

=

a

=

straightforward analogue of this definition using den­

sity functions. If fxy(x, y) is the joint density function
for X and Y, and fx ( x ) is the density for X alone,
then the conditional density of Y given X is given
by Jy,x(yJx) = fxy(x,y)ffx(x). Using the density
function we can then compute the probability by inte­
grating as usual. Further details can be found in any
standard text on probability (for instance [Papoulis

1984]).

To use this approach, we need to identify a random
variable X and value b such that M(q) corresponds
to the event X = b. The choice of random variable

is crucial; we can easily have two random variables X
and X' such that X == b and X' = b are the same
event, yet conditioning on X =band X'= b leads to
different results, since X and X' have different density
functions. In our case there is an obvious choice of
random variable, given our description of the situation:

PrHq(B) < p and Prn q ( R1 J R ) < q are independent!
This is of course extremely intuitive: It seems reason"
able that HQ's beliefs about the probability of Judy
being in Blue Territory should be independent of HQ's

beliefs of her being in Red HQ area, given that she is
in Red territory.

Using this, it is trivial to prove the following proposi"
tion, which holds whether we choose to use any par"
ticular t: > 0, or if we use the standard definition of
conditioning on the value of a random variable (which,
as we have said, essentially corresponds to considering
the limit as t-> 0). In this proposition, pr8(p) denotes
the density function for the random variable P r m�(B);
i.e., pr B(P) = dPrJ/HQ(PraQ{B) < p)jdp. Similarly,
pr B (P I M(q))

pr8(p)

=

6p- 6p2,

(1),

for the next result.)

Proposition 2.1

PrnQ(B) = p

< PI M( q))/dp .
it immediately follows that
although we do not need this fact

= dPrJ/HQ(PrnQ( B)

(Note that from

and

:

In (P

M(q)

prB(P)

HQ, Prjj�rQ),

the events

are independent. Formally,

=

pr8(pJM(q)).

is easy to see that the two approaches give us the same

With this result, we are very close to showing that
Judy's beliefs regarding the probability of her be­
ing in Blue territory don't change as a result of the

and then considering the limit as the interval width

regarding where Judy is . The posterior distribution

PrnQ(RtiR).

With this choice of random variable, it

answer; the use of the density function corresponds to
considering a small interval around PrnQ(RtiR} = q,
tends to 0.

message.

PnQ,

Pr';;�c(RtiR) = af( a +b,)
Prjj�'Q(PrnQ(RtiR)

<

we have

q)

1-a.-b 1 dcdbda
r t-a.
1c=O
1qa=O h=a(l-q)
11 rl a 1-a-b1dcdhda
a.=O J b=O 1c=O
1-a-b dcdhd
6 r
a
1
Ja=O b=� c=O
q

=

11-a 1
q

=

==

=

1:0 1:�;1_•1 (1- a- b)dbda
6 lq (q- a)2 da

6

q

q

a=O

< p)

=

(3- 2p)p2,

Prjj�Q(Prnq(R1IR) < qAPr nQ(B)

< p)

and

is a distribution on

j/HQ(·) Prj/;Q(·I M(q)) is still a distribution on
=

Pnq.

As a result of conditioning, Judy revises her
beliefs about HQ's beliefs. But to determine Judy's
beliefs about where she is we need a distribution on
{Rt, R2, Bt, B2 }. The question is how Judy's beliefs
about HQ's beliefs about where she is are related to
her beliefs about where she is. Notice that there is
no necessary relation. After all, for all we know, Judy
might think that HQ has no reliable information, and
thus decide to ignore HQ's statement when forming her
opinion regarding where she is. But, in keeping with
the spirit of the story, we assume that Judy trusts HQ,
and thus forms her beliefs by taking expectations over
her beliefs about HQ's beliefs. For example, if Judy
was certain that HQ was certain that she was in Blue
territory, then she would ascribe probability 1 to being
in Blue territory. More generally, Judy weights HQ's
beliefs according to her beliefs about how likely it is
that HQ holds those beliefs. We formalize this trust
assumption as follows:

22
q

Two other results, which are derived in a similar fash­
ion, also tmn out to be useful:

Prj/;Q(PrnQ(B)

Prj�Q

that is, on Judy's beliefs about HQ's beliefs

Pr

Before computing the result of conditioning on M(q)
(under either approach), it t urns out to be use­
ful to do some more general computations.
Since

Notice that

(1)

== q( 3-2p)p2 .

The point here is not just the values themselves, but,
more importantly, that the final distribution function
is the product of the first two. That is, the events

(Trust)

At any point in time, Judy's beliefs about
event in the space {R1, R2, B1. B2} are
formed by taking expectations of HQ's probability
of the same event, according to Judy's distribu­
any

tion over HQ's possible beliefs. Formally, we have:

Pr �(E)
=

f
Pr�; HQ(Pr';J8c) Pr';J8c(E) dabc
lPr�·�cEPHQ

213

Pr obability Update: Conditioning vs. Cross-Entropy

=
(

:0 prk(e) e de)
1{prior}
probability

U [0, 1].(In the second line, which
for t E
theory, pr}.;(e)
follows from standard
is the density function of the random variable

Pr�8c(E);
e)jde.)

1;

i.e., pr (e)

= dPr�/HQ(Pr�8c(E)

<

Note that when we apply this rule before Judy receives
the message, so that t

prior, we have

=

Pr�;'-ior(B1) =

Pr�rior(B2) = Prrio r(Rl) = Prrior(R2) =

1/4, which

is consistent with our earlier assumption that Judy
started with a uniform prior on { Bt, B2, Rt, R2}.

M(q).
q

The desired result now follows quite readily using the
The re­
trust principle after Judy has received
sult is that, no matter what the value of is, her beliefs
regarding being in Blue territory remain unchanged,
exactly in accord with most people's strong intuitions.

Theorem 2.2:

Proof:

Pr'j(B)

=

1/2

for all

q

E

[0, 1].

{
Prj HQ(Pr';[8c) Pr';j�c(B) dabc
}p,�·Q•EPHQ /

Prj(B)
=

i:/rB(PIM(q))pdp
1:0 pr8(p) pdp
1:0 (6p- 6p2)pdp

=

1/2.

(1))

I

Note that this theorem applies even if q = 1. Van
Fraassen would interpret the message M(l) as mean­
ing that Judy is definitely not in R2. We interpret
this it as PrnQ(R1IR) E [1 - t:, 1] for some (arbitrar­
ily) small and unspecified € > 0. Although the two
interpretations seem close (after all, they differ by at
most t: in the probability that they assign to R1 and
R2), they are not equivalent. As Theorem 2.2 shows,
this is a significant difference. It is the claimed equiv­
alence of the two interpretations that was behind van
Fraassen's first principle, and hence his rejection of the
intuitive answer that P r (B ) = 1/2. This equivalence
may be correct under van Fraassen's constraint based
interpretation of M, but it is not inevitable under our
is indeed a factual
alternative reading, in which
announcement (but about HQ's beliefs, not Judy's).

j

M

3

DIS CUSSION

belief as
characterized by

{Rt, R2,B1, B2}.

to
a

M(q),

4.

q

M(q) is interpreted as meaning that Pr nQ( ) E
[q �, + €], so that we can condition on this

- q

event. (However, our result holds no matter what
t: is. Thu s we can regard t: as an arbitrarily small
and unknown nonzero quantity.)
5. Judy's distribution on {Bt, B2, Rt, R2} was de­
rived by taking the expectation of her beliefs re­
garding HQ's beliefs.
Although these assumptions seem to us quite reason­
able, they are clearly not the only assumptions that
could have been made. It is certainly worth trying to
understand to what extent our results depend on these
assumptions, and in particular whether they can be ex­
tended to provide more general statements of how to
update by probabilistic information.
considered by van Fraassen and his colleagues? As we
said in the introduction, such rules may be useful in
certain cases, but we believe it is an important research
question to understand and explain precisely when.
We do not, in particular, find CE to give particularly
plausible results in the JB problem. But how could
this have been predicted in advance?
The JB problem shows that we need more than just an
axiomatic justification for the use of CE (or any other
method of update). An alternative to the use of a rule
is to do what we have done for the JB problem in this
paper: that is, to try to "complete the picture" in as
much detail as possible, so that ultimately all we need
to do is condition. In practice, this may be unneces­
sarily complex and shortcuts (such as CE) might exist.
However, it would be useful to understand better the
assumptions that are necessary for their use to cor­
respond to conditioning. In any case we believe that
some of the issues we addressed cannot be avoided: it
will never be sensible to blindly apply a rule, like CE,
to all information that merely "seems" probabilistic or
can be reformulated as such. Rather, one must always
think carefully about precisely what the information
means.

Acknowledgments

It is worth reviewing the assumptions that were nec­
essary for us to prove Theorem 2.2. We assumed:
1. HQ's

3. The only messages that HQ could send were those
and the one that HQ did send
of the form
was the one that correctly reflected HQ's beliefs.

W here does this leave CE and all the other methods

(by Theorem 2 .2)

(from

2. Judy's beliefs regarding HQ's beliefs were
characterized by the uniform distribution on
HQ's beliefs, when parameterized by the tuple
(PrsQ(RI), PrsQ(R2), PrnQ(Bt ) ) .

where Judy is can be
distribution on the space

We thank Teddy Seidenfeld and Bas van Fraa.ssen for
useful comments. The second author's work was sup­
ported in part by the NSF, under grant IRl-96-25901,
and the Air Force Office of Scientific Research ( AFSC),
under grant F94620-96-1-0323.

214

Grove and Halpern




in domains with uncertainty. There have been numerous

As probabilistic systems gain popularity and are

proposals, both proba bilistic and qualitative , for defining
explanation in such domains. ((GID'denfors 1988; Hempel

that explains the system's findings and recom­

1 %5; Salmon 1984) describe the work done by the philoso­
phers and give numerous references; the more recent work

will also need a mechanism for ordering compet­
ing explanations. We examine two representa­

Henrion and Druzdzel 1990; Pearl 1988; Shimony 1991;
Suermondt 1992).) Since we are interested in explanation

coming into wider use, the need for a mechanism

mendations becomes more critical. The system

tive approaches to explanation

in the literature­

one due to Gardenfors and one due to Pearl-and
show that both suffer from significant prob lems.
We propose an approach to defining a notion of

"better

explanation" that combines some of the

features of both together with more recent work

by Pearl and others on causality.

in AI includes, for example, (Boutilier and Becher 1995;

in probabilistic systems, our f ocus is on proposals that seek
a probabilistic connection between the explanation and the
explanandum. In the philosophical literature, the focus has
been on the probability of the explanandum given the ex­
planation. The requirements range from just requiring that
this conditional probability change, to requiring that it be

very high, to requiring that it be greater than the uncon­
ditional

probability of the explanandum (so that learning

the explanation increases the probability of the explanan­

1

INTRODUCTION

Probabilistic inference is often hard for humans to under­

stand Even a simple inference in a small domain may
seem counterintuitive and surprising; the situation only
.

gets worse for large and complex domains.
tem doing probabilistic inference

Thus, a sys­
must be able to explain

its findings and recommendations to evoke confidence on

the part of the u ser Indeed, in experiments with medical
diagnosi s systems, medical students not only trusted the
.

system more when presented with an explanation of the
diagnosis, but also were more confident about disagreeing

with it when the explanations did not account adequately
for all of the aspects of the case (Suennondt and Cooper

dum); see (Giirdenfors 1988; Salmon 1984) for discussion
and further references. In contrast, the research on expla­
nation in Bayesian networks (Henrion and Druzdzel 1990;
Pearl 1988; Shimony 1991) has concentrated on comput­
ing the conditional probability of the explanation given the

explanandum, adding in some cases the additional require­
ment that the explanation be a complete world description.

Clearly the appropriateness of a notion of explanation will

depend in large part on the intended app l ication. A scientific

explanation might well have different properties from an

explanation provided by an intelligent tutoring system. In

our intended application, the system will typically have
some uncertainty regarding the true state of the world (and

partially correct explanation should be the best indication

domain's causal structure), represented
a probability distribution. Note that this is different
from, say an intelligent tutoring system, where we assume
the system to have the full knowledge of the domain. For

Our goal is to find a notion of explanation in a probabilistic
setting that can be usefully applied by a reasoning system

knowledge.1 Because we expect that there will typically be
a number of competing explanations that can be provided

1992). Explanation can also play an important role in refin­
ing and debugging probabilistic systems. An i ncorrect or

to an expert of a potential problem.

to explain its findings to a human. Of course, we are not
the first to examine explanation. It has been has analyzed

possibly even the
as

,

simplicity, we make the (admittedly unrealistic) assumption
that the user's knowledge can be identified with the system's

to the user, we are interested not just in finding an absolute
notion of explanation, but a comparative notion. We want

by philosophers for many years. Traditionally, it has been

to be able to
another.

to scientific enquiry, this approach is not easily applicable

to fit it is

modeled by introducing a deductive relation between the
explanation and the fact to be exp lained (explanandum)
(Hempel and Oppenheim 1948). W hile perhaps applicable

1

judge when one explanation is better than

Modeling the user's knowledge and adjusting the explanation
one of the planned extensions of this work.

Defining Explanation in Probabilistic Systems

In this paper, we concentrate on two definitions of expla­
nation, one due to Gardenfors (I 988) and the other to Pearl

(1988), as represen tative s of the two approaches mentioned
above . While, as we point out, there are signifi cant prob­
lems with these definitions, we consider them because they
have some important features that we feel should constitute
part of an ap p roach to defining explanation. We suggest an
approach that combines what we feel are the best features
of these two definitions with s ome ideas from the more re­
cent work on causality (Balke and Pearll994; Druzdzel and
Simon 1993; Beckerman and Shachter 1995; Pearl 1 995).

One of the observations that falls naturally out of our ap­
proach is that we should expect different answers depending
on whether we are asking for an explanation of beliefs or
facts . For example, if the agent believes that it rained last
night and we ask for an explanation for this belief, then a
perfe ctly reasonable explanation i s that he or she notic:OO
the wet grass in the morning, which is correlated with rain.
However, if the agent observes that it is raining and we ask
for the best explanation of this observation, then it would
certainly not be sati sfactory to be told that the grass is wet.
We do not accept the wet grass as an expl anation in the
second case because the wet grass is not a cause of rain.
However, we would accept it in the first case because the
agen t believing that the grass is wet is a cause of the agent
believing that it rained. The critical difference between ex­
planations of beliefs and explanations of observations does
not seem to have been discussed before in the literature.
The rest of the p aper is organized as follows. In Sections

2 a nd 3 we present and analyze Gardenfors' and Pearl's

definitions. In Section 4 we present a new ap proach which

generalizes elements of both. We conclude with some open
pr oblems in Section 5.

2
2.1

GARDENFORS' APPROACH
THE DEFINITION

As we sug gested earlier r oughly speaking for Gardenfors,
X is an explanation of E if Pr(EIX) > Pr(E). That is, X
is an explanati on of E if learning X raises the pr obability
of E. In order to flesh out this intuition, we need to make
precise what probability distribution we are using.
,

,

According to Gardenfors, what requires explanation i s

something that is already known, but was unexpected : A
person asking fo r an explanation expresses a "cognitive dis­
sonance" between the explanandum and the rest of his or
her beliefs. We don t typically require an explanation for
something we expected all along. The amount of disso­
nance is mea sured by the surprise value of the explanan­
dum in the belief state in whi ch we reject our belief in
the explanandum while holding as many as possible of our
other beliefs intact (this operation is called contraction and
c ome s from the belief revision framework (Alchourr6n,
Gardenfors, and Makinson 1985)). An explanation pro­
vides "cognitive relief"; the degree of cognitive relief" is
measured by the degree to which the explanation decreases
the surprise value.
'

"

63

For example if we ask for an explanation of why David
has the flu, then we already know that David has the flu.
Thus, if E is the statement "David has the flu", then in
the current situation, we already ascribe probability 1 to E.
Nothing that we could learn could increase that probability.
On th e other hand, we pr esumably asked for an explanation
,

because before David got sick, we did not expect him to
get sick. That is, if Pr:E describes the agent s probability
distribution in the contracted belief state, before David got
flu, we expect Pr:E (E) not to be too high. An explanation X
'

(like "David was playing with S ara who also has the flu")
would raise the probability of E in the contracted belief
state, that is, we have
,

PrE;(EIX)

>

PrE;(E).

As Giirdenfors' definition stresses, what counts as an e xp la­
nation de pends on the agent s epistemic state. An explana­
tion for one agent may not be an explanation for another, as
the following example, essentially taken from (Giirdenfors
1988), shows.
'

Example 2.1 If we ask why Mr. Johansson has been taken
ill with lung cancer, the information that he worked in as­

bestos manufacturing for many years is not going to be a
satisfactory explanation if we don't know anything about
the effects of asbestos on people's health. Adding the state­
ment "70% of those who work with asbestos develop lung
cancer" makes the explanation complete. The explana­
tion must consist of both statements. However, if we try
to explain Mr. Johansson's illness to his close friend, who
is likely to know his profession, we would supply only the
second piece of information. Similarly, to someone who
knows more about asbestos but less about Mr Johansson,
we would only present the information about his profession.

To formalize these intuitions, Glirdenfors characterizes a
(p robab ili stic) epistemic state using the possible worlds
model. At any given time, an agent is assumed to con­
sider a number of worlds (or states of the world) possible.
For example, if the agent looks out the window and notices
that it is raining, his set of possible worlds would include
only worlds where it is raining. Learning new facts about
the world furthe r restricts the set of the worlds we con�
sider p ossible Among the possible worlds, some may be
more like l y than the others. To describe this likelihood the
.

,

agent is assumed to have a probability distribution over the
possible worlds.
Thus, an epistemic state is taken to be a pair K (W, Pr},
where W is a set of possible worlds (or possible states of the
world) and Pr is a probability distribution on W. A sentence
A is said to be accepted as knowledge in an epistemic state
K if Pr( A) = 1. We sometimes abuse notation and write
=

A

E

K if A is accepted in epistemic state K.

Given an epistemic state K = (W, Pr} of an agent, let
::::: {WE', Pr:E) denote the contraction of K with re­
spect to E, i.e., the epistemic state characterizing the
agent's beliefs that is as close to K as possible such that
E � KF,. Gardenfors describes a number o f postulates that
K£ should satisfy, such as K"E = Kif Eft K. It is be­
yond the scope of this paper to discuss these postulates (see

Ki:

64

Chajewska and Halpern

(Alchourr6n, Giirdenfors, and Makinson 1985)). However,
these postulates do not serve to specify Ki uniquely; that
is, given K and E, there may be several epistemic states
K' that satisfy the postulates. On the other band, there are
some situations where it is straightforward to specify Ki.
For example, if Pr is determined by a Bayesian network
together with some observations, including E , then Pr£j is
just the distribution that results from the Bayesian network
and all the observations but E.
We can now present Giirdenfors' definition of explanation.
Definition 2.2 (from (Gll.rdenfors

1988)) X is an explana­
tion of E relative to a state of belief K = (W, Pr) (where
E

E

K)if

1. Pr:E(EIX)

>

Pr:g(E), and

2. Pr(X) < 1 (that is, X ¢. K).
We have already seen the first clause of this definition.
Note that, in this clause (and throughout this paper), we
identify the formulas E and X with sets of possible worlds,
namely, the sets of worlds {in W.B') in which E and X,
respectively, are true. The second clause helps enforce
the intuition that the explanation depends on the agent's
epistemic state. The explanation cannot be something the
agent already knows. For example, fire will not be an
explanation of smoke if the agent already knows that there
is a fire. Notice that the second clause also prevents E
from being an explanation for itself. (Clearly E satisfies
the first clause, since Pr:E( EIE) > Pr:E( E); since we have
assumed E E K, E does not satisfy the second clause.)
Unfortunately, as we shall see, while the second clause does
exclude E as an explanation, it does not exclude enough.
Given this notion of explanation, we can define an ordering
on explanations that takes into account the degree to which
an explanation raises the probability of the explanandum.
Gardenfors in fact defined explanatory power as the dif­
ference between the posterior and prior probability of the
explanandum. Thus, a better explanation is one with better
explanatory power.
The difference is not always a good measure of distance
between probabilities. An explanation which raises the
probability of a statement of interest from 0.50000 l to 0.51
is not so powerful . On the other hand, an explanation raising
the probability from 0.000001 to 0.01 would be received
quite differently, although the difference in probabilities is
the same. A more natural way to define explanatory power
is by using the ratio of the two probabilities.
Definition 2.3

The explanatory power (EP) of X with re­

spect to E is
EP(X' E)

=

Pr:E(EIX)
PrE( E)

.

According to this definition, the two explanations above
have dramatically different explanatory power. For this
paper, we take the latter definition as our formal definition
of explanatory power.

Before we get to our critique of Gardenfors' definition,
there is one other issue we need to discuss: the language
in which explanations are given. Definition 2.3 makes per"
feet sense if, for example, explanations are propositional
formulas over a finite set of primitive propositions. In tha�
case, a world w could be taken to be a truth assignment to
a finite family of these primitive propositions. We could
also take explanations to be first-order formulas, in which
case a world could be taken to be a first-order interpretation.
Gllrdenfors in fact allows even richer explanations, involv­
ing statistical statements. As we saw, in Example 2.1, a
possible explanation of Mr. Johansson's illness for someone
who already knew that he worked in asbestos manufactur­
ing is to say "70% of those who work with asbestos develop
lung cancer". To make sense of this, Glirdenfors associates
with a world not only a first-order interpretation, but a distri­
bution over individuals in the domain. (This type of model
is also considered in (Halpern 1990), wbere a structure con­
sists of possible worlds, with a distribution over the worlds,
and, in each world there is a distribution on the individuals
in that world; a formal language is provided for reasoning
about such models. If the domain is finite, we could sim­
plify things and assume that the distribution is the uniform
distribution, as is done in (Bacchus, Grove, Halpern, and
Koller 1996).) While it is not necessary to consider such
a rich language to make sense of Glirdenfors' definition,
one of his key insights is that statistical assertions are an
important component of explanations. Indeed, he explicitly
describes an explanation as a conjunction xl 1\ x2. where
X1 is a conjunction of statistical assertions and X2 is what
Glirdenfors calls a singular sentence, by which he means
a Boolean combination of atomic sentences in a first-order
language with only unary predicates. (Either conjunct may
be omitted.) As we shall argue, we need to generalize this
somewhat to allow causal assertions as well as statistical
assertions.

2.2

A

CRITIQUE

While Giirdenfors' definition has some compelling features
(see (Glirdenfors 1988) for further discussion), it also has
some serious problems, both practical and philosophical.
We describe some of them in this section.

1. While the second clause prevents E from being an ex­

planation of itself, there are many other explanations
that it does not block. Let F be any formula such
that Pr(F) < 1 and Pr:g( E 1\ F) > 0. Then E 1\ F
will be an explanation for E. Moreover, it will be
the explanation with the highest possible explanatory
power (both according to G�denfors' original defi­
nition and our modification). This is obvious, since
Pr:g( EI E 1\ F) = 1. Note that F can be practically
any formula here. We surely wouldn't want to accept
" E and the coin lands heads" as an explanation for E.
One possible solution to this problem is to restrict ex­
planations to only involving certain propositions. For
example, if we are looking for an explanation for some
symptoms, we might require that the explanation be a
disease. There are many cases where such restrictions
make sense, but if we are to do this, then we mus�

Defining Explanation in Probabilistic Systems

bilities in that contracted belief state). If an approach
like this is to be used in a system, we need techniques
for computing the contraction. More accurately, since
the contraction is not unique, we need to focus on ap­
plications where there is a relatively straightforward
notion of contraction.

explain where the restrictions are coming from.

2. Even if we restrict attention to a particular vocabulary

for explanations, there is nothing preventing us from
adding irrelevant conjuncts to an explanation. More
precisely, note that if X is an explanation of E, and
C is conditionally independent of E given X, then
PrE:(EJX) = Pri(EJX A C). Thus, X and X A C
are viewed as equally good explanations.

3

4. The fact that learning X raises the probability of E

does not by itself qualify X to be an explanation of
E. For example, suppose s is a symptom of disease
d and Bob knows this. If Bob learns from a doc­
tor that David has disease d and asks the doctor for
an explanation, he certainly would not accept as an
explanation that David has symptoms, even though
Pr;I(djs) > Pr;f(d). Giirdenfors is aware of this is­
sue, and discusses it in some detail {1988, p. 205). He
would call s an explanation of d, but not a causal ex­
planation. Gi!rdenfors provides a definition of causal
explanation. Unfortuna tely, while it deals with this
problem, it does not deal with the other problems we
have raised, so we do not discuss it here.2 We dis­
agree with Gardefnors that there are explanations that
are not causal; we view all explanations as causal. In
particular, we do not think that Bob would accept s
as an explanation of d at all. Note, however, that if
Bob had asked the doctor why he ( the doctor) believed
that David had disease d, an acceptable explanation
would have been that the doctor believed (or knew)
that David had symptoms. There is a big difference
between what Bob would accept as an explanation for
d and what he would accept as an explanation of the
doctor's belief that d. We return to this issue below.

5. As a practical matter, Gardenfors' definition requires

the computation of the contraction of a belief state
(besides the computation of many conditional proba-

2For the interested reader, C is said to be a causal explanation
of E wi th respect to belief state K such that
E Kif ( 1) Pr( C) <

Pr:g(E\C)

>

(Pr;j)(7

E

PrE,

Pr;j

=
where
is
the belief state that arises when we add C to the stock of beliefs
in K. This is the notion called belief expansion (Alchourr6n,

1, (2)

Pr:g(E), (3)

Giirdenfors, and Makinson 1985). Thus, we add clause (3) to
the definition of explanation. Note, however, if F is independent
of E, then E 1\ F would be a causal explanation of
Similar

E.

arguments show that Giirdenfors' definition of causal explanation
still suffers from all the other problems we have raised.

THE MAXIMUM A POSTERIORI

MODEL APPROACH

3. The definition does not take into account the likeli­

hood of the explanation. For example, suppose there
are two explanations for a symptoms, disease d1 and
d isea se d2, with the same explanatory power, but d1
is a relatively common disease, while d2 is quite rare.
If the explanation is given by an expert that is trusted
by the user (as in the case of an intelligent tutoring
system), then once we are told that, say d2 is the ex­
planation. we would presumably accept it as true. In
this case, the prior probability (i.e., the fact that d2 is
rare) is irrelevant. However, in our context, even if
Pr; (sldt) = Pr; (s\d2), it seems clear that we should
prefer the explanation dt to d2.

65

3.1

THE DEFINITION

Most of the work done on explanation in belief networks
was based on the intuition that the best explanation for an
observation is the state of the world that is most proba­
ble given the evidence (Henrion and Druzdzel 1990; Pearl
1988; Shimony 1991). There is no notion of "cognitive
dissonance" or surprise. The explanation is an (informed)
guess about the possible world we are currently in, based
on the evidence (which includes the explanandum). In
some cases (e.g., (Pearl l988)), the guess must specify the
world completely-formulas describing sets of worlds are
not allowed as explanations. This approach, which we
call Maximum A Posteriori model {MAP) after (Shimony
1991 ), has been also known under other names: Most Prob­
able Explanation (MPE) (Pearl1988) and Scenario-Based
Explanation (Henrion and Druzdzell990).
Formally, according to Pearl, given an epistemic state
K = (W, Pr}, an explanation for E is simply a world
w in which E is true.
This notion of explanation in­
duces an obvious ordering on explanations. World w1 is
a better explanation of E than w2 if E is true in both w1
and w2 and Pr(wiJE) > Pr(w2jE). Finally, the best or
most probable explanation (MPE) is the world w* such that
Pr(w*jE) = maxwEW Pr(wJE).3
We remark that although we have spoken here of an ex­
planation as being a world, we could equally well take an
explanation to be the formula that characterizes the world
if we assume (as Pearl does) that each world is uniquely
characterized by a formula. If our vocabulary consists of a
finite number of propositions P1, ... , Pit, and each world is
a truth ass1gnment to these primitive propositions, then an
explanation would have the form QII\
A Qk, where each Qi
is either Pi or •Pi· Of course, if we have richer languages,
finding formulas that characterize worlds becomes more of
an issue.
• • •

Two other variants of the MAP approach have been pro­
posed, by Henrion and Druzdzel (1990) and Shimony
(1991, 1993). They share with Pearl's definition two impor­
tant features: Ftrst, the explanation is a truth assigrunent to
3 Actually, Pearl did not define the notion of explanation, just
that of most probable explanation. However, our definitions are
certainly in the spirit of his. Also, he did not talk explicitly of

worlds and epistemic states, but these are implicit in his defini­
tions. Pearl assumes that there is a Bayesian network that de­
scribes a number of variables of interest. The set W then consists
of all possible assignments to the variables, and the probability
distribution Pr on W is determined by the Bayesian network.

66

Chajewska and Halpern

a subset of propositions, including the explanandum. Sec­
ond, the ordering of explanations is based on their posterior
probability given the explanandum.
Henrion and Drozdzel actually discuss a number of ap­
proaches to explanation. Of most relevance here are what
they call scenario-based explanations. They assume a tree
of propositions (a scenario tree), where a path from the
root to a leaf represents a scenario, or a sequence of events.
They are looking for the scenario with the highest probabil­
4
ity given the explanandum. Thus, their approach differs
from Pearl's in that the system has additional knowledge
(the scenarios). They also allow explanations to be partial.
The truth values of all propositions do not have to be spec­
ified. However, explanations are restricted to coming from
a set of prespecified scenarios.
Shimony (1991, 1993) also allows partial explanations.
He works in the framework of Bayesian networks (as
does Pearl, in fact, although his definition makes sense
even if probabilities are not represented using Bayesian
networks).5 In his framework, the explanandum is an in­
stantiation of (truth assignment to) some nodes in the net­
work; these are called the evidence nodes. An explanation
is a truth assignment to the "relevant" nodes in the net­

work. The relevant nodes include the evidence nodes and

only ancestors of evidence nodes can be relevant. Roughly

speaking, an ancestor of a given node is irrelevant if it has
the property that it is independent of that node given the
values of the other ancestors. In (Shimony 1991), the best

explanation is taken to be the one with the highest poste­
rior probability. In (Shimony 1993), this is extended to
allow explanations to be sets of partial truth assignments,

subject to certain constraints

Section 4.3.)
3.2

A

(discussed in more detail in

CRITIQUE

The MAP approach has an advantage over Gardenfors':
it doesn't require contraction. However, it has its own
problems. Some of the problems are particularly acute in
Pearl's approach, with its requirement that the explanation

be complete; i.e., a world; they are alleviated somewhat if

we allow partial explanations (sets of worlds). However,
some of the problems arise in all variants of the approach,
and are a consequence of ordering according to the posterior
probability distribution.
1. By making the explanation a complete world, the no­

tion becomes very sensitive to the choice of language,
as Pearl himself observes. 6 For example, if our lan­
guage consists of {s, d1, d2}, then the best explanation

4 Actually, they suggest presenting all scenarios that have suf­
ficiently high probability, and pointing out how the most probable
one differs from the other likely scenarios.

5Recall that a Bayesian network is an acyclic directed graph
whose nodes represent primitive propositions {or random vari­
ables), together with conditional probability tables describing the
probability of a node given instantiations of its parents (Pearl

1988).
6Shimony (1991) calls this the

overspecification problem.

for symptoms might be d1, or, more precisely, the
world characterized by s A d 1 A ...,d2. For the purposes
of this example, suppose that diseases are mutually
exclusive, so all worlds where the agent has more than
one disease have probability 0. Now suppose we sub­
divide d1 into two diseases d� and d�', again mutually
exclusive (as, for example, hepatitis can be subdivided
into hepatitis A and hepatitis B). Then we might find
d2 to be a better explanation than either d� or d� (that
is, Pr( ·d� A ·d� A d2l s) may be greater than either
Pr(d� A -.d'( A •d2ls) or Pr( ·d� Ad� A •d2ls ))
.

Pearl gives an even sharper example of this phe­

nomenon. Suppose that d1 is a a diagnosis of per­
fect health, d2 is a diagnosis of a fatal disease,
Pr(dtls) = 0.8, and Pr(d2ls) = 0.2. Now suppose we
expand the vocabulary to include h1, . , hs, where the
hi's are possible holidays that the agent will take next
year (provided he or she is indeed healthy), and the
agent considers each of these vacation plans equally
0.1, and the
likely. Then we have that Pr( h; A dtl s)
most likely explanation of the symptom has changed
.

.

=

from d 1 to d2! So just by considering possible holidays
he might take given that he is healthy, the agent finds
that the best explanation for his symptoms becomes a
fatal disease.

2. A

related problem is the fact that if we have a large

number of primitive propositions, most will probably

be irrelevant or only marginally relevant to explaining
a particular proposition. Yet, Pearl's definition forces
us to consider worlds, thus forcing us to worry about
the truth value of all propositions. This can cause
computational problems. In addition, conciseness is a

desirable feature in an explanation, particularly in an
interactive system. The user usually wants to know
only the most influential elements of the complete
explanation, and does not want to be burdened with
unnecessary detail. This problem is particularly se­
vere if we insist on complete explanations. However,
Shimony's partial explanations are not necessarily as
concise as one would hope either. It is not hard to show
that for each evidence node X, the explanation must
include an assignment to all the nodes in at least one
path from X to the root, since for each relevant node,
at least one of its parents must be relevant Moreover,
the irrelevance condition is quite strong and only in
limited contexts is it likely to achieve significant prun­
ing. Shimony attempts to overcome this problem by
relaxing the irrelevance assumption to what he calls
approximate or 8-irrelevance. While helpful in some
domains, the extent to which it will result in concise
explanations in general is not clear. We discuss this
point in more detail in the full paper.

3. The ordering on explanations used in the MAP ap­
proach is supposed to maximize the probability of
the explanation given the explanandum. However,

if we consider only explanations which include the
explanandum (as all MAP explanations do), this re­
duces to maximizing the prior of the explanation. The
ordering is then based only on the likelihood of the ex­
planation and not in any way on the degree to which the

Defining Explanation in Probabil istic Systems

explanation raises the probability of the explanandum.

4. All the MAP approaches discussed above consider es­
sentially propositional languages. Once we move to
richer languages (like first-order, or languages that al­
low statistical information), then each world may end
up having very low probability. Indeed, if we have a
continuous number of worlds, each world may have
probability 0. In this case, the definition which re­
quires explanations to be complete worlds is not even
useful.
Given the difficulties with complete explanations, why do
Shimony and Henrion and Druzdzel put such restrictions
on the allowable partial explanations? What is the prob­
lem with partial explanations? Suppose our language con­
sists of the propositions {Pl, ... , pk}. Why not just allow
Pl as an explanation, instead of requiring something like
Pl A ...,P2 1\

... A

-.pit? Gardenfors certainly allows such
explanations. It is not bard to see why Pearl does not allow
partial explanations. Notice that a partial explanation is
really a set of worlds (or equivalently, the disjunction of the
formulas representing the worlds). But a disjunction will
always have higher conditional probability than any of its
disjuncts (except in the degenerate case where all but one
of the disjuncts has probability 0), and thus will be viewed
as a more probable explanation than any of its disjuncts.
It is because of this that Shimony puts restrictions on the
allowable partial explanations as well. As we shall see,
we can deal with this problem, at least to some extent, by
modifying the ordering of explanations.

4

SYNTHESIS

As we have seen, both Gardenfors' definition and the MAP
definition have problems. We believe that in order to deal
with these problems, we need to deal with two relatively
orthogonal issues: (1) we must decide what counts as an
explanation, and (2) we must decide how to compare two
explanations.

4.1

WHAT COUNTS AS AN EXPLANATION?

The MAP approach seems somewhat too restrictive in what
counts as an explanation: An explanation must be a com­
plete description of a world (or a restricted form of partial
explanation). Gardenfors, on the other hand, is not restric­
tive enough. He allows E A C to be an explanation of
E, for example, and this seems to us unreasonable. In ad­
dition, he would allow a falling barometer reading to be
an explanation for a storm, thus missing out on the causal
structure.
As we mentioned above, we view all explanations as causal.
We distinguish between explaining facts and explaining be­
liefs, but in both cases we look for the same thing in an
explanation: a causal mechanism which (possibly together
with some facts) is responsible for the fact observed or
the beliefs adopted. By enforcing causality, AGMwe be­
lieve that we can avoid the problems in G!rdenfors' defini­
tion, while still allowing more general explanations than the

67

MAP approach would allow. We remark that we are not the
first to stress the role of causality in explanation. Salmon
(I 984) discusses the issue at length, although the technical
details of his proposal are quite different from ours.
The literature on causality is at l east as large as the literature
on explanation; it is well beyond the scope of this paper to
develop a new theory of causality. For the purposes of the
rest of this paper, we work at the propositional level (since
that is essentially what the recent approaches to causality
do) and assume that the causal mechanism is described by
a causal structure, which we take to be a Bayesian network
interpreted causally.

We believe that much of what we do is independent of the
particular way we choose to model causality. In particular,
we can replace the causal network by structural equations,
as described in (Druzdzel and Simon 1993; Pearl1995). We
have chosen to use Bayesian networks as our representation
for causality simply to make it easier to relate our approach
to Pearl's approach.
In this setting, part of the agent's uncertainty concerns what
the right causal mechanism is. For example, an agent may
be uncertain whether smoking causes cancer or whether
there is a gene that causes both a susceptibility to cancer
and a susceptibility to smoking. Thus, we assume that a

world is a pair ( w, C) consisting of a truth assignment w
and a causal structure C. As before, an epistemic state
K is a pair (W, Pr), where W is a set of worlds of this
form, and Pr is a probability distribution on W. However,
we assume that this epistemic state arises from a simpler

description: We assume that the agent has a probability
distribution Pr' o n causal structures and has made some
observations. Notice that a causal structure

C

also places

a probability distribution Pre on worlds. We require that

the distribution Pr be consistent with the causal mechanisms

considered possible and the observations 0 in the following

sense: There must be a probability distribution IY on causal
mechanisms such that Pr(w, C) = Pr'(C) Prc(wiO): that
is, the probability of (w, C) is the probability of the causal
mechanism C times the probability that C induces on w,
given the observation. In particular, this means that if the
agent considers only one causal mechanism possible, we
can identify Pr with a probability on truth assignments, just
as Pearl does.

We assume that the explanandum E is one of the ob­
This means that Pr:E has a simple form:
servations.

Pr;;;( w,C)

=

Pr'(C) Prc(wiO-

{E}). It is easy to see

that this definition satisfies the postulates for contraction.
An explanation of

E in epistemic

state

K

is a conjunction

X = X 1 1\ X2 consisting of a partial causal mechanism
X1 (that is, a description of a causal structure; see below)
and an instantiation of nodes X2 that causally precede E
in Xt such that Pr(X) < l. (Yt/e defer for now the is­
sue of whether the explanation raises the probability of the
explanandum.)
We are deliberately being vague about the language used to
describe the causal mechanism, since we believe that this is

an area for further research. For the purposes of this paper,

68

Chajewska and Halpern

we can take Xt to be simply a description of a subgraph of
the causal graph (intuitively, that part of the causal graph
that is relevant to explaining E, i.e., a subset of the set of
paths from nodes in X2 to E).
We allow the conjunct describing the causal mechanism to
be missing from the explanation if it is known. (In practice,
this might mean that the system providing the explanation
believes that the agent to whom the explanation is being pro­
vided knows the causal mechanism.) Notice that if the agent
knows the causal mechanism, and thus considers only one
causal mechanism possible (as is implicitly the case when a
situation is described by a Bayesian network which is given
a causal interpretation), then a world can be identified with
a truth assignment. In this case (ignoring the requirement
that all the conjuncts in a basic explanation of E must pre­
cede E causally), what Pearl called an explanation would
be a special case of what we are calling an explanation.
However, we allow more general explanations, in that we
do not require an explanation to be a truth assignment. In
this sense, our framework can be viewed as generalizing
Pearl's and S bimony's.

Our definition also borrows heavily from Glirdenfors' defi­
nition. We take from him the requirement that Pr( X) < 1.

His other requirement, that Pre(E I X ) > Pre (E), will also
play a role in our ordering of explanations. The form of
the explanation-a conjunction of a (partial) causal mech­
anism and an instantiations of nodes-is also taken from
G!irdenfors.7 Since we are working with propositional
Bayesian networks, the instantiation of nodes clearly corre­
sponds to taking the conjunction of atomic sentences in first­
order logic. Giirdenfors allows disjunctions as well (since
he allows singular sentences, which are Boolean combina­
tions of atomic sentences). Allowing disjunctions seems to
cause problems for us; we return to this issue in Section 4.3.
The (partial) causal mechanism can be viewed as a gener­
alization of statistical assertions. We view the requirement
of the causal mechanism as a key difference between our
definition and Giirdenfors'. For one thing, the causality
requirement prevents E 1\ C from being an explanation of
E, since E cannot precede E in the causal ordering. It also
prevents a symptom from being an explanation of a disease.
We would argue that causality is what makes most of
Gardenfors examples involving statistics so compelling.
For example, consider the case of Mr. Johansson. We be­
lieve that the explanation "70% of those who work with
asbestos develop lung cancer" involves more than just the
statistical assertion. It is accepted as an explanation because
we implicitly accept that there is a causal structure with an
edge from a node labeled asbestos to a node labeled lung
cancer (with a conditional probability table saying that the
probability of lung cancer given asbestos is 0.7). And it is
the lack of causality that causes us (if the reader will pardon
the pun) not to accept "70% of the time that the barometer
reading goes down there is a storm" as an explanation of a
storm (unless we happen to believe that barometer readings
have a causal influence on storms).
7 Originally,

the idea carne from Hempel's work on explanation

(Hempel and Oppenheim 1948).

However, the situation is different if we try to explain our
beliefs to someone else. In this case, the causal structure is
symmetric. The fact that I believe that there is a storm does
explain my belief that the barometer reading has gone down;
my belief that the barometer reading has gone down is an
explanation for my belief that there is a storm. Ultimately,
these beliefs should be rooted in an observation (either of
the storm or the barometer).
We can readily convert a causal network describing a sit­
uation to a network describing an agent's beliefs. We just
reinterpret all the nodes so that a node labeled X talks about
the agent's belief in X ,moralize the graph and change all the
directed edges to undirected edges. The resulting Markov
network (Pearl 1 988) captures the causal as well as proba­
bilistic dependencies between the agent's beliefs. Note that
the resulting network is no longer asymmetric. While we
do not view a symptom as a cause for a disease, believing
that a patient has a certain symptom might well cause us to
believe that he has a disease. However, an explanation for
the agent's beliefs would then be an acyclic subnetwork of
this network, together with some new nodes representing
the external causes of some of the beliefs. For example, an
external cause for the belief that the patient has symptom d
is the observation of the symptom; an external cause for the
belief that David has an ear infection might be receiving
that information from a doctor. We discuss this in more
detail in the full paper.

4.2

ORDERING EXPLANATIONS

As we have seen in the few examples presented so far, and
as is indeed the case in many applications, there are typi­
cally several competing explanations. We need to be able to
compare them and choose the best. The two proposals pre­
sented above for ordering explanations-Giirdenfors' no­
tion of explanatory power and Pearl's notion of considering
the probability of the explanation given the explanandum­
both have their merits, but neither seems quite right to us.
The following example might help clarify the differences
between them.

Example 4.1 Assume that we have a bag of 100 coins, 99
of which are strongly biased (9:1) towards heads and one
that is just as strongly biased towards tails. We pick a coin
at random and toss it. The coin lands tails.

We can nwdel this situation by using two random variables:
C (the type ofcoin) with values bh and bt (biased towards
heads and biased towards tails) and R (the result of the
toss), with values h and t. A priori, the probability that we
picked a coin that is biased towards heads is very high; in
fact P(C == bh) 0.99. After receiving the evidence of the
coin landing tails, wefind out that P(C == bhiR=t) is close
to 0.92-less that the prior on C = bh but still very high.
What explanation would we acceptfor thefact that the coin
landed tails? Clearly, the causal structure in this situation
is known: there is a causal relation between C and R, with
the obvious conditional probability table described by the
story. Since the causal structure is known, the allowable
bh and C
bt.
explanations can be identified with C
==

=

=

Defining Explanation in Probabilistic Systems

What is the relative merit of these explanations?

According to Gardenfors' definition, C = bt is a much
better explanation than C = bh, since Pr(R = tiC = bt) is
much greater than Pr(R

69

respect to E and the prior of X . We can then place a partial
order t E on explanations of E by taking X 1 t E X2 iff
EP(X1 , E) � EP(X2, E) and Pr:E(XJ ) � Pr:E( X2 ) ·
Notice that with thi s ordering, the two explanations in the

= ti C = bh) , where Pr is the prior
probability distribution, before the outcome R = t is known.
Intuitively, C = bt has far better explanatory power because
it accounts for the observation far better than C = bh does.

forces the user to decide whether the explanatory power or

According to Pearl's ordering, the best explanation of the
coin landing tails is C = bh, since Pr( C = bh!R=t) is much
greater than Pr( C = bti R = t).8 This explanation, although

Although the ordering is partial in general, it can be viewed
as a natural generalization of Pearl's ordering. Suppose the

On the other hand, the explanation seems unsatisfactory,
since it does not take into account the low probability that
the coin biased towards tails will be picked.

very likely itself, doesn't seem to relieve the "cognitive

dissonance" between the explanandum and the rest of our
beliefs. While it may be the correct diagnosis of the situa­
tion, it doesn't seem right to call it an explanation. The fact
that the potential explanation is less probable a posteriori
than a priori should at least cause some suspicion.

Pr(C

= bh\ R

The term

=

t)

=

R���: �

bh )

X

Pr(C =

bh) .

Pr(���:Jbh) is what we called the explanatory

power of C = bh with respect to R = t. Thus, the degree
to which C
bh is an explanation of R = t according to
Pearl is precisely the product of EP( C = bh, R= t) and the
prior probability of C = bh. Thus, we can see the precise
sense in which Pearl's definition takes into account the prior
whereas Gardenfors' does not.
=

Although the two definitions disagree in this example, there
are many situations of interest in which they agree (which is,
perhaps, why both have seemed to be acceptable definitions
of the notion of explanation) . In particular, they agree in
situations where the prior probability of all explanations
is the same (or almost the same). Thus, if the user has
no particular predisposition to accept one explanation over
another, both approaches will view the same explanation as
most favorable.9"
Since we cannot always count on the prior of all explana­
tions being equal, we would like an ordering on explana­
tions that takes into account both the explanatory power
and the prior. One obvious way of taking both into ac­
count is to multiply them, which is essentially what Pearl
does, but multiplication loses significant information and
sometimes gets counterintuitive results. (More examples
of this appear below.) A straightforward alternative is to
associate with each explanation X of E the pair of numbers

(EP (X E) , Pr:E (X) ) :
,

=

bh

and

C = bt, are incomparable.

This

the prior is the more significant feature here. In a case like
this, such a wide divergence between the explanatory power
and the prior of two explanations might signal a problem
with the causal model. Perhaps the agent's prior on C = bh
vs. C = bt is incorrect in this case.

causal mechanism is known, as is implicitly assumed by

Pearl. If we allow explanations that are complete descrip­

tions of worlds, then all complete descriptions that include
E have exactly the same explanatory power: 1 / Pr:E ( E ) .
Thus, our ordering would order them by their prior, just a s
Pearl's and Shimony's does.

Our ordering also avoids the problem in Gardenfors' or­

Notice that by Bayes' rule,

Pr(

coin example, C

the explanatory power of

X

with

8

Note that, according to Pearl's definition, C = bh would not
be an explanation of R = t. The two possible explanations would

be C = bh i\ R == t and C = bt l\ R = t. What we are analyzing here
is the ordering produced by Pearl's definition of better explanation
on the notion of explanation defined according to our approach.
9
Here we are also implicitly assuming that there is a prior
agreement on what counts as an explanation. As we have ob­
served, the two approaches differ in this respect too.

dering that adding irrelevant conjuncts results in an equally
preferred explanation. For example, if X is an explanation
of E then X 1\ Y (for all Y conditionally independent of
E given the epistemic state) would be considered a worse
explanation than X in our ordering since their explanatory
powers are the same and X 's prior is higher.

If we add a conjunct that is not completely irrelevant, then
our approach forces the agent to decide between more spe­
cific explanations that have higher explanatory power, and
less specific explanations, that have a higher prior. For
example, suppose we want to understand why a somewhat
sheltered part of the lawn is wet. One possible explanation
is that it rained last night, but rain does not always cause
that part of the lawn to get wet. A better explanation might
be that it was raining and very windy. The combination of
rain and wind has better explanatory power than rain alone,
but a lower prior. According to our ordering, this makes
the two explanations incomparable. This does not seem
so unreasonable in this case. We would expect a useful
explanatory system to point out both possible explanations,
and let the user decide if the gain from the extra explanatory
power of wind is sufficiently high to merit the lower prior.

Note that if we multiply the explanatory power of the ex­
planation by its prior, we will always prefer the expla­
nation "rain". To see this, note that for any explanation

X, the product of the explanatory power and the prior is
Pr:E (X JE) . Since clearly Pr:E ( X J E) � Pr:E(X A Y I E),
the simpler explanation i s preferred. This is a case where
multiplication causes a loss of useful information.

4.3

DEALING WlTH DISJUNCTIONS

As we have defined it, an explanation is a conjunction of
a partial causal mechanism together with an instantiation
of nodes. We have not allowed disjunctions. Disallow­
ing disjunctions of causal mechanisms seems reasonable.
It is consistent with the intuition that "you have cancer
either because you smoke or because you have a genetic

70

Chajewska and Halpern

predisposition to cancer" is viewed as a disjunction of two
explanations, not one explanation which has the form of a
disjunction. We suspect that it is for similar reasons that
Gardenfors disallowed the disjunction of statistical asser­
tions in his definition.

On the swface, it may seem less reasonable to disallow

the disjunction of instantiations of nodes. Certainly it is
straightforward to modify our definition so as to allow them,
and doing so would be more in keeping with Glirdenfors'

allowing singular sentences. However, notice that allowing
the disjunction of instantiations bas the effect of allowing
disjunctions of causal mechanisms.

Consider a case in which we ask for an explanation of huge
forest fires recently occurring in California. One possible
explanation is that the fire prevention caused the brush to
overgrow, another that the tourists often leave campfires

unattended. Both these explanations are very plausible (and
so is their conjunction). Suppose the agent considers only
one causal network possible, and it contains both of these

mechanisms. Thus, by allowing the explanation "either
some tourists left their campfire unattended or the brush
was overgrown", we are effectively allowing a disjunction
of causal mechanisms. This example suggests that we may
want to make a distinction between what appear to be two
different causal mechanisms co-existing within the same
causal structure (perhaps using the techniques discussed by
Druzdzel and Simon (1 993)). This is an area for future
research.

On the other hand, there

are cases where allowing disjunc­

tions seems useful. For example, consider a situation in
which we have four coins, Ct , C2, C3 , and C4 , where Ct

and c2 . are biased towards heads and c3 and c4 are bi­
ased toward tails. We pick one coin at random and toss

it three times. The coin lands heads every time. The ob­

vious explanation for this fact is that we picked one of
the coins biased towards heads, that is, either cl or c2 .

And, indeed, our ordering would prefer the explanation
X1 =det ( C = CJ ) v ( C = C2) to either of the explanations
C = C1 or C
same bias.

=

C2, assuming that both

C1

and C2 had the

way for deciding when to add such variables.
Shimony's work can be viewed as an attempt to provide

principles as to when to consider disjunctive explanations.
The partial explanations of (Shimony 1 991) are sets of
worlds where the truth values of some primitive propo­

sitions are fixed, whi le the rest can be arbitrary. The sets of

partial explanations of (Shimony 1 993) correspond to more
general sets of worlds, but there are still significant restric­
tions. For example, the disjunctive explanation must corre­
spond to a node already in the network and the probability
of the explanandum must be the same for every disjunct in
the disjunctive explanation. The latter restriction is quite
severe. In our coin example, if the coins biased towards
heads have different biases, Shimony's approach would not
allow us to consider the explanation xl ' "we picked a coin
biased towards heads". Of course, we can easily loosen
this restriction to allow disjunctions where the conditional
probabilities are almost the same. However, it seems to us
that we want more than just similar conditional probabili­
ties here. We only want to allow disjunctions if the causal
mechanism for each disjunct is the same.

To be fair, Shimony uses his restrictions to allow him to find
good explanations algorithmically. It is not clear whether
there are also philosophical reasons to restrict them in this
way. We hope to explore both the algorithmic and founda­
tional issues in future work.

5

CONCLUSIONS

We feel that the contribution of this paper is twofold: First,
we present a critique of two important approaches to ex­
planation; second, we outline a sketch of a novel approach
that tries to take into account the best features of both, and
combine them with a notion of causality.

Our approach clearly needs to be fieshed out. S ome areas
for future research include:
•

By way of contrast, the explanation X2 =dcf (C = Ct ) V
( C = C2 ) v ( C = C3 ) does not seem at all reasonable
although, according to our ordering, it is incomparable to
XJ . While XI has higher explanatory power, x2 has a
higher prior. While most people would clearly reject X2 ,
it would be useful to have to have some automatic way of
rejecting it.10

on their application to causal reasoning. Along simi­

This example suggests that rather than allowing disjunc­
tions, a better strategy might be to add an additional variable
representing the type of the coin (with possible values bh
and bt, as before). However, we have as yet no principled

lar lines, it would be useful to have a good language
for reasoning about causality, that allowed first-order
reasoning and temporal constructs.
•

1 0orh
is is another case where multiplyin g the components gives a
misleading answer: Xz has a higher product than X1 . In general,
if we compare the explanation X to a disjunctive explanation
X V Y by multiplying the explanatory power times the prior, then
we will always prefer X V Y to X , for the same reasons as given
earlier for preferring X to X 1\ Y.

Obviously, much of the effort will involve research in
causality. Most of the work in causality has allowed
only what amounts to propositional reasoning. (The
nodes represent random variables that take on a small
finite number of values.) Can we extend it to allow
causal explanations that involve first-order constructs
and temporal constructs? There has been some work
on adding these constructs to Bayesian networks (see,
for example, (Dean and Kanazawa 1 989; Glesner and
Koller 1995; Haddawy 1 994)). but no work focusing

As we have observed, our approach, which provides
only a partial ordering on explanations, seems too

weak. While it is not clear that we want to have a
total order, it does seem that we want to allow more
explanations to be comparable than is the case accord­
ing to our ordering. This is particularly the case if we
allow disjunctive explanations.

Defining Explanation in Probabilistic Systems

•

•

A natural extension would be to apply our definition
to countetfactuals.
After all, humans seem to have no problem with ex­
plaining hypothetical facts. We believe that our basic
framework should be able to handle this, although per­
haps we may need to use structural equations and the
interpretation of countetfactuals given by Balke and
Pearl ( 1 994).
As we said earlier, given that our goal is to have the
system provide an explanation that is useful to a user,
it would be important to model the user's knowledge
state and adjust explanations accordingly. The work of
Suermondt (1 992) is relevant in this regard. He also
puts the emphasis on explaining beliefs (or, specifi­
cally, probability distribution over the node of inter­
est) adopted by the system as a result of receiving some
observation. His goal is to find a small subset of ev­
idence responsible for this change and the links most
influential in transmitting it. In our context, we can
understand Suermondt as considering a system which
has full knowledge of the domain (characterized by a
Bayes Net together with all the conditional probability
tables) and knows the values of some variables, trying
to explain its beliefs to a user with no (or minimal)
knowledge. Thus, for him, an explanation amounts to
finding a "small" set of instantiations of variables (i.e.,
a partial truth assignment) and a ' small" partial causal
mechanism that will raise the posterior probability of
the observations.

Given the importance of explanation, we believe that these
questions represent fruitful lines for further research.

Acknowledgments
We thank anonymous reviewers for pointing out several im­
portant references and Adam Grove for useful discussions.
Part of this work was carried out while the second author
was at the ffiM Almaden Research Center. IBM's support
is gratefully acknowledged. The work was also supported
in part by the NSF, under grants IRI-95-03 1 09 and IRI96-25901 , and the Air Force Office of Scientific Research
(AFSC}, under grant f/94620-96-1 -0323.


:

We define a new notion of conditional
which plays the same role for Dempster­
Shafer belief functions as conditional probability
does for probability functions. Our definition is dif­
ferent from the standard definition given by Demp­
ster, and avoids many of the well-known problems
of that definition. Just as the conditional prob­
ability Pr(·IB) is a probability function which is
the result of conditioning on B being true, so too
our conditional belief function Bel( ·IB) is a belief
function which is the result of conditioning on B
being true. We define the conditional belief as the
lower en'llelope (that is, the infimum) of a family
of conditional probability functions, and provide a
closed-form expression for it. We show by example
the intuitive appeal of our definition, and compare
it in detail to the more standard definition, showing
why and how it differs.
belief,

1

Introduction

How should one update one's belief given new ev­
idence? If beliefs are expressed in terms of proba­
bility, then the standard approach is to use condi­
tioning. If an agent's original estimate of the prob­
ability of A is given by Pr(A), and then some new
evidence, say B, is acquired, then the new estimate
is given by the conditional probability Pr(AIB),
defined as Pr(A n B)/Pr(B).1
The Dempster-Shafer approach to reasoning
about uncertainty [Sha76] has recently become
quite popular in expert systems applications (see,
for example, [Abe88, Fal88, LU88, LG83]). This
approach uses belief functions, a class of functions
that satisfy three axioms, somewhat related to the
axioms of probability. In this paper, we consider
how to define a. notion of conditional belief, which
generalizes conditional probability.
1 Thie definition ie not completely uncontrovereial ( aee,
e.g., [DZ82) for a diecu11ion and further references).

One definition for conditional belief was already
suggested by Dempster [Dem67J, and is derived us­
ing the rule of combination; hereafter we refer to
Dempster's definition a.s the DS definition of con­
ditional belief. Although the DS definition also gen­
eralizes conditional probability, it is well known to
give counterintuitive results in a. number of situa­
tions (see, e.g., [Ait68, Bla87, Dia78, DZ82, Hun87,
Lem86, Pea.88, Pea.89, Za.d84]) We provide here
a. new definition of conditional belief, which also
generalizes conditional probability, but is different
from the DS definition in general. We can show
that our definition avoids many of the problems as­
sociated with the DS definition.
The motivation for our definition of conditional
belief comes from probability theory. It is well
known that a. belief function Bel is the lower en­
'l!elope of the family of a.ll probability functions Pr
consistent with Bel. That is, Bel(A) is the infimum
of Pr(A), where the infimum is taken over a.ll pro.b­
a.bility functions Pr such that Bel(A') � Pr(A') for
a.ll A'.2 We define Bel(AIB) to be the lower enve­
lope of the family of a.ll functions Pr(·IB) where
Pr is consistent with Bel (similarly to the situa­
tion with conditional probability, we assume that
Bel(B) > 0, so that everything is well defined).
Although we define Bel( ·IB) in terms ofa.lower en­
velope, we show that there is an elegant closed form
expression for it. Moreover, we can show that just
a.s the conditional probability function is in fact a
probability function, our conditional belief function
is a belief function.
The rest of this paper is organized as follows. In
the next section, we review belief functions and de­
fine our notion conditional belief. We show how it
compares to the DS notion by applying both defiSome author& (e.g., [DP88]) have used the term lower
to denote what we are calling lower envelopes.
We have ueed the term lower envelope here to avoid con·
fuaion with Dempeter'& technical uaage of the phraae lower
probability in [Dem67, Dem68J, which, although related, is
not equivalent to what we are calling a lower envelope.
2

probability

318

nitions to the well-known three pri1oner1 problem
[Gar61, Dia78]. We then conduct a more thor­
ough investigation of the differences between the
two notions, and their relationship to conditional
probability, showing why the DS notion occasion­
ally provides counterintuitive answers. In Section 4
we discuss the relationship between belief functions
and sets of probability functions. We conclude in
Section 5 with some discussion on the implications
of our results to the use of belief functions.
2

Updating belief functions

Recall that a probability space is a tuple (S, X, Pr),
where S is the sample space, X is a collection of sub­
sets of S containing S and closed under complemen­
tation and countable union, and Pr is a probability
function defined on X. Note that Pr is defined not
on all subsets of S, but only on the sets in X, tra­
ditionally called the mea1urable sets. Subsets of S
not in X are called nonmea1urable.
The Dempster-Shafer theory of evidence [Sha76]
provides an approach to attaching likelihoods to
events that is different from probability theory. The
theory starts out with a belief function. For every
event (i.e., set) A, the belief in A, denoted Bel(A),
is a number in the interval [0, 1] that places a lower
bound on likelihood of A. We have a corresponding
number Pl(A) = 1- Bel(A), called the plausibility
of A, which places an upper bound on the likelihood
of A. Thus, to every event A we can attach the in­
terval [Bel(A), Pl(A)]. Like a probability measure,
a belief function assigns a "weight" to subsets of a
set S, but unlike a probability measure, the domain
of a belief function is always taken to be all subsets
of S. Formally, a belief function Bel on a set S is
a function Bel: 2 5 -+ [0, 1] satisfying:
BO. Bel(0)

=

0

Bl. •Bel(A)� 0
B2. Bel(S)
B3. Bel(A1

=

1

A.�:)�
LI!;{l,...,A:},I:f:0 (-1)111+1 Bel(niEI Ai).
u ... u

We remark that a probability function defined on
all of 2 5 is easily seen to be a belief function.
In a companion paper [HF90], we argue that
there are two quite distinct ways of relating be­
lief functions to probability theory. One approach
views belief as a generalized probability; the sec­
ond views it as a way of representing evidence. H
we would like to update beliefs, then it seems most
appropriate to view beliefs as generalized probabil­
ities. There are a number of ways of doing this

[Dem67, Dem68, FH89b, Kyb87, Sha79]. We fo­
cus on one here, since it is perhaps the most well
known: that is the approach of viewing a belief
function as an infimum of a family of probability
functions. Given a set 1' of probability functions
all defined on a sample space S, define the lower
en11elope of 1' to be the function J such that for
each A� S, we have f(A) = inf {Pr(A) : Pr E 1'}.
We have the corresponding definition of the upper
en'!Ielope of 1'. It was already known to Dempster
[Dem67) that a belief function can be viewed as a
lower envelope. More formally, let Bel be a belief
function defined on S, and let (S, X, Pr) be a prob­
ability space with sample space S. We say that Pr
is consistent with Bel if Bel(A) � Pr(A) � Pl(A)
for each A E X. Intuitively, Pr is consistent
with Bel if the probabilities assigned by Pr are
consistent with the intervals [Bel(A), Pl(A)] given
by the belief function Bel. It is easy to see that
Pr is consistent with Bel if Bel(A) � Pr(A) for
each A E X (that is, it follows automatically that
Pr(A) � PI(A) fo� each A E Xl:_ This is because
Pl(A) = 1 - Bel(A) �· 1 - Pr(A) = Pr(A). Let
1'Bel be the set of all probability functions defined
on 2 5 consistent with Bel. The next theorem tells
us that the belief function Bel is the lower envelope
of 1'Bel1 and Pl is the upper envelope.
Theorem 2.1: Let Bel be a belief function on S.
Then for all

A� S,

Bel(A) = infp,e-p8,1 Pr(A)
Pl(A) = supp, E 'Ps ., Pr(A).
We remark that the converse to Theorem 2.1
does not hold: not every lower envelope is a be­
lief function. Counterexamples are well known
[Bla87, Dem67, Kyb87]. We return to this issue
in Section 3.
Theorem 2.1 suggests how we might update a
belief function to a conditional belief function (and
a plausibility function to a conditional plau1ibility
function). We define:
Bel(AIB)

=

Pl(AIB) =

inf

Pr(AiB)

sup

Pr(AiB).

P•E'Psel
P•E'Psel

It is not hard to see that the infimum and supre­
mum above are not well-defined unless Bel(B) > 0;
therefore, we define Bel(AIB) and Pl(AIB) only if
Bei(B) > 0. It is straightforward to check that if
Bel is actually the probability function Pr, then
Bel(AlB) = Pr(AIB). Thus, our definition of con­
ditional belief generalizes that of conditional prob­
ability. Note that taking B = true in the preceding

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

319

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

definition, we get as a special case that Bei(A) =

infp,e1'8,1 Pr(A) and Pl(A) = supp,E1'
s.t Pr(A),

which is Theorem 2.1 above.
Our definition of conditional belief and plausibil­
ity does not give us much help in computing these
expressions. We would like to have a closed-form
expression for them. We can in fact provide an
elegant closed-form expression, as shown in the fol­
lowing theorem.
Theorem 2.2: If Bel is a belief function on S such

that Bei(B) >
Bei(AIB)

O,

=

Pl(AIB) =

then
Bel(A n B)
Bel(An B)+ Pl(A n B)
PI(An B)

Pl(An B)+ Bel(A n B)

The expressions given above for conditional belief
and plausibility are quite natural. Not sm.11risingly,
it turns out that other authors have discovered
them as well. In particular, essentially these ex­
pressions appear in [Wal81], [SK89], and [dCLM90].
Indeed, it even appears (lost in a. welter of notation)
as Equation 4.8 in [Dem67]! (Interestingly, none of
these papers references any other work as the source
of the formula.)
It is well known that the conditional probability
function is a probability function. That is, if we
start with a probability function Pr on S, and B is
a subset of S such that Pr(B) > 0, then the func­
tion Pr(·IB) is a probability function. We might
hope that the same situation holds with belief func­
tions, so that the conditional belief and plausibility
functions are indeed belief and plausibility func­
tions. Given our definitions of conditional belief
and plausibility as lower and upper envelopes, it is
not clear that this should he so, since lower and up­
per envelopes of arbitrary sets of probability func­
tions do not in general result in belief and plau­
sibility functions. Fortunately, as the next result
shows, in this case they do. Thus, we have a way
of updating belief and plausibility functions to give
us new belief and plausibility functions in the light
of new information.
Theorem 2.3: Let Bel be a belief function defined

on S, and PI the corresponding plausibility func­
tion. Let B � S be such that Bel(B) > 0� Then
Bel(·IB) is a belief junction and Pl(·IB) is the cor­
responding plausibility function.

The proof of Theorem 2.3 is somewhat difficult.
We outline the proof in the appendix; full details
can be found in the full paper [FH89a]. We remark

that this result-which we view as the main tech­
nical result of the paper-appears in none of the
papers cited above that contain the expression for
conditional belief from Tlteorem 2.2. In [dCLM90]
the question of whether Bel( ·IB) is a belief function
is discussed, but left unanswered.
As we mentioned in the introduction, our defi­
nition is quite different from that given by Demp­
ster. Given a belief function Bel , Dempster defines
a conditional belief function Bel(·IIB) as follows
[Sha76, p. 97]:3
Bel(AIIB) =

Bel(Au B)- Bel(B)
.
1- Bel(B)

The corresponding plausibility function is shown to
satisfy:
PI( An B)
Pl(AIIB) =
Pl(B) .
A brief glance at the DS definition compared with
the formula in Theorem 2.2 should convince the
reader that in general these two definitions of con­
ditional belief will not agree. It is easy to show
that both definitions of conditional belief general­
ize the standard definition of conditional probabil­
ity as long as all sets are measurable, that is, have
a probability assigned to them. The key difference
turns out to be in the way they treat nonmeasurable
sets. (See [FH89b] for a discussion of nonmeasur­
able sets and their relationship to belief functions.)
We return to this issue below; we first consider an
example that highlights the differences between the
two approaches.
Example: In order to compare our updating tech­
nique with that of Dempster, we consider the well­
known three prisoner11 problem.4

Of three prisoners a, b, and c, two are
to be executed but a does not know which.
He therefore says to the jailer, "Since ei­
ther b or c is certainly going to be ex­
ecuted, you will give me no information
about my own chances if you give me the
name of one man, either b or c, who is go­
ing to be executed." Accepting this argu­
ment, the jailer truthfully replies, "b will
be executed." Thereupon a feels happier
1Dempacer't definition is uaually

given

aa a special ap­

plication of a more {eneral 1'Uie of combination for belief
functions. It would take us too far afield here co discuaa the

rule of combination; aee the companion paper

(HF90]

for a

ditcuaaion of the role of the rule of combination.
4 For

an excellent introduction to the problem as well a• a

Bayeaian aolution, see

(Gar61].

Our description of the story

ia taken from (Dia78] and much of our discuuion is baaed on
that of Diaconis and Zabell [Dia78, DZ82] .

320

because before the jailer replied, his own

Thus, in this case, the jailer's answer does not affect

chance of execution was two-thirds, but

a's probability.
Suppose more generally that

afterwards there are only two people, him­
self and

c,

= a:,

who could be the one not ex­

one-half.

a

replied, he seems to be implicitly assuming that

I

Pr(aaya-b) = Pr( {(a, b)}) +Pr( {(c, b)})= (a:+1)/3

I

the one to get pardoned is chosen at random from
and

c.

Then straightforward compu­

Pr(livea-a)
= a:/3

to believe that his own

chance of execution was two-thirds before the jailer

a, b,

1.

:5

Pr(livea-ajaaya-b) =

We make this assumption ex­

plicit in the remainder of our discussion.

Is a justified in believing that his chances of es­
caping have improved? It seems that the jailer did
not give him any relevant extra information. Yet
how could a's subjective probabilities change if he

does not acquire any relevant extra information?

X

a:/3
= a:/(a: + 1).
(a: + 1)/3

This says that if a:#;

1/2 ( i.e., if the jailer had a par­
ticular preference for answering either b or c when
a was the one pardoned) , then a would learn some­
thing from the answer, in that he would change his
estimate of the probability that he will be executed.

For example, if

a: =

0, then if a is pardoned, the
Thus, if the jailer actu­

[DZ82], we model a possible situation
by an ordered pair (:c, y), where :c, y E {a, b, c}. In­
tuitively, a pair (:c, y) represents a situation where
:c is pardoned and the jailer says that y will be ex­

jailer will definitely say

ecuted in response to a's question. Since the jailer

c is pardoned, then the jailer will say b, while
if b is pardoned the jailer will say c. Given that
the jailer says b, then from a's point of view the
one pardoned is equally likely to be him or c; thus,
Pr(live•-al•ays-b) = 1/2. As a: ranges from 0 to
1, it is easy to check that Pr(livea-aiaaya-b) ranges
from 0 to 1/2.

Following

answers truthfully, we cannot have

:c

= y;

since

the jailer will never tell a directly that a will be

y = a. Thus, the set
of possible outcomes is {( a, b), (a, c) , (b, c) , ( c, b)}.
The event that a lives, which we denote livea-a,
corresponds to the set {(a, b), (a, c)}. Similarly, we
define the events livea-b and livea-c, which corre­
spond to the sets {(b, c)} and {(c, b)}, respectively.
executed, we cannot have

By assumption, each of these three events has prob­
ability

1/3.

aaya-b,

ally says

b,

pardoned, i.e., that

ilarly, if

c.

then a knows that he is definitely not

a: = 1,

Pr(livea-aiaaya-b) =

0. Sim­

then a knows that if either he

or

How can we capture this situation using a belief

function? It seems reasonable that if
lief function and

PI

Bel

is the be­

the corresponding plausibility

function used to capture the situation, then

Bel

b,

which we de­

should agree with probability function where the

corresponds to the set

{(a, b), (c, b)};

probability is known, so that,

The event that the jailer says
note

a priori, both the
li'lles-a, li'lles-b,

the story does not give us a probability for this

belief and plausibility of the events

event.

li'IJt:s-c should be 1/3. All we know about the
a priori probability of says-b is that it lies between
1/3 and 2/3: it is at least the probability that c
is chosen ( since in that case the jailer must say b),

In order to do a Bayesian analysis of the

situation, we will need this probability. Note that
we do know that the probability of

{(c, b)} is 1/3;
{(a, b)}.

we just need to know the probability of

This depends on the jailer's strategy in the one case
that he has free choice, namely when
gets to choose between saying

b

and

c

a

lives. He

in that case.

We need to know the probability that he says
i.e.,

If

b,

Pr(aaya-bjlivea-a).
we assume that the jailer chooses at ran­

b and c if a is pardoned, so
that Pr(aaya-bjlivea-a) = 1/2, then Pr({(a, b)}) =
Pr( {(a, c)})= 1/6, and Pr(aaya-b) = 1/2. We can
dom between saying

now easily compute that

Pr(livea-ajaaya-b)
= Pr(livea-a n aaya-b)/Pr(aaya-b)
(1/6)/(1/2) = 1/3.

I

Pr(aaya-bjlivea-a)

Pr( {( a, b)})

Note that in order for

a:

tations show that

ecuted, and so his chance of execution is

among

for 0 :5

Pr(aaya-bllives-a)

I

and

and it cannot be more than the probability that

b

Bel satis­
Pl(aay-b) = 2/3. Simi­
we can argue that Bel(livea-a n aaya-b) = 0
Pl(livea-a n aaya-b) = 1/3. Plugging these

is not chosen. Thus, we assume that

fies

Bel(aay-b) = 1/3

larly,
while

and

numbers into our formulas, it is easy to compute
that

1/2.

Bel(livea-aii•aya-b) = Pl(livea-aiiaaya-b) =
Thus,

for the DS notion of conditional

probability, the range reduces to the single point

1/2. By way of contrast, it is easy to check that
Bel(livea-ajaaya-b) = 0 while Pl(livea-ajaaya-b) =
1/2. I
This example shows that the two notions of con­
ditioning can give quite different answers.

The

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

321

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

range [0, 1/2] computed by our notion of condition­
ing is easy to explain: it is precisely the range de­
termined by letting the probability that the jailer
will say b in the one situation that he has a choice
between saying b and c, namely, when a is the one
pardoned, range from 0 to 1. The fact that our
definition gives this range is not an accident! It is
a direct consequence of our definitions and Theo­
rem 2.2.
The range [1/2, 1/2] determined by the DS no­
tion of conditioning seems much more mysterious.
The answer 1/2 corresponds to the situation where
the jailer says b whenever he can (i.e., whenever a is
pardoned or c is pardoned). Why is this a reason­
able answer? More importantly, why does it arise?
Is there a natural probabilistic interpretation for it?
In the full paper, we consider this issue in detail.
The following construction, which is a generaliza­
tion of the "beehive" example in [SK89] (as well
as being a formalization of some comments made
in [dCLM90]), may help provide a partial explana­
tion.
Suppose a set S is partitioned into (nonempty)
disjoints sets X1,
, Xk. An agent chooses X; with
probability a; (where a1 +
+ ak = 1) and then
chooses z: E X; with some unknown probability.
Given subsets A and B of S, we want to know what
the probability is that the element z: chosen is in A,
and the probability that x is in A given that it is in
B. H A = X;, then it is clear that the probability
that x E A is a;. However, if A is not one of the
X;'s, then all we can compute are upper and lower
bounds on the probability.
Let 'P be the set of probability functions on S
consistent with this situation; namely, Pr E 'P iff
Pr(X;) = a;, fori= 1, . .., k. Let Bel be the lower
envelope of 'P; it is not hard to show that Bel is a
belief function (we do so in the full paper). It seems
reasonable to argue that the best lower and upper
bounds we can give on the probability that z: E A
are Bel(A) and Pl(A). Similarly, the best lower
and upper bounds we can give on the probability
that z: E A given that x E B are given by the
infimum and supremum of {Pr(AIB) : Pr E 'P}.
These are precisely Bel(AIB) and Pl(AIB).
Now suppose we slightly change the rules of the
game. We are told that the probabilistic process
that chooses an element in X; will definitely choose
an element in B if possible. This does not affect
anything if X; � B or if X; � B. However, if
X; n B #- 0 and X; n B #- 0, then, rather than
choosing X; with probability a;, the probability is
now redistributed so that X; n B is chosen with
probability a;, while X; n B is chosen with proba•

•

•

·

·

·

bility 0. The probability that used to be spread over
all of X; is now concentrated on X; n B. W hat is
the probability that an element of A is chosen given
that the element chosen is definitely in B with re­
spect to this new process, where an element of B
is chosen whenever possible? We now have to con­
sider the family 'P' of probability functions consis­
tent with this new process, and take the infimum
and supremum of {Pr'(AIB) : Pr E 'P'}. As we
show in the full paper, these bounds are given by
Bel(AIJB) and Pl(AIIB).
Suppose we now reconsider the three prisoners
problem from this point of view. We can now
see that Bel(lives-aiJsays-b) gives the probability
that a lives given the extra hypothesis that the
jailer says b whenever possible. In particular, this
means that the jailer definitely says b if a is the
one that is pardoned; i.e., Pr(says-bJlives-a) = 1.
Under this revised situation, the probability that
a lives given that the jailer says b is indeed ex­
actly 1/2. W ith this understanding of the DS no­
tion of updating, the result Bel(lives-aJJsays-b) =
Pl(lives-aJJsays-b) = 1/2 should come as no sur­
prise.
To sununarize, this discussion has shown that
Bel(AIIB) corresponds to a somewhat unnatural
updating process, where before we condition with
respect to B, we first try to choose an element
in B whenever possible. In terms of the process
discussed above, it is easy to see that this extra
step before updating makes no differen�e if B is
the union of some of the X;'s. This amounts to B
being a measurable set. It will make a difference if
B is not measurable. This is the case in the three
prisoner problem, where says-b is not a measurable
set, and is the cause of the answer 1/2 that we get
when we try to apply DS conditioning in this case.
We remark that this analysis can also be
used to explain the well-known observation that
Bel(AIB) � Bel(AIIB) � Pl(AIIB) � Pl(AIB)
([Dem67, Dem68]; see also [Kyb87]).
Not
only does it show why the interval defined
by [Bel(AlB), Pl(AIB)] contains that defined by
[Bel(AIIB), Pl(AIIB)], it explains when and why
we get equality. See the full paper for details.
3

Belief functions
velopes

and

lower

en­

Theorem 2.1 says that each belief function is the
lower envelope of a set of probability functions, and
each plausibility function an upper envelope. Un­
fortunately, the lower envelope of an arbitrary set
of probability functions is not in general a belief
function, nor is the upper envelope of an arbitrary

322

set of probability functions in general a plausibil­
ity function. Nevertheless, results such as Theo­
rem 2.3 show that there are natural sets of proba­
bility functions that do induce belief and plausibil­
ity functions. Although a general characterization
is lacking, further examples in [F H89b, HF90] sug­
gest that this is not an isolated example.
However, even if a set P of probability functions
does induce a belief and plausibility function, say
Bel and PI, it is reasonable to ask whether we
should represent P by Bel and Pl. Clearly the
answer depends very much on the intended appli­
cation. However, it is worth noting that this rep­
resentation of P might result in a loss of valuable
information. For example, consider a sample space
consisting of three points, say {a, b, c}. Let P con­
sist of all probability functions on S with the follow­
ing three properties: (1) 1/4 � Pr({a}) � 1/2, (2)
1/4 � Pr({b}) � 1/2, and (3) Pr({a}) = Pr({b}).
It is not hard to show that the lower envelope of P
is a belief function. If we call this belief function
Bel and take PI to be the corresponding plausi­
bility function, we get Bel({a}) = Bel({b}) = 1/4
and PI( {a}) = PI( {b}) = 1/2. Thus, we retain
the information that the probability of a and b
both range between 1/4 and 1/2. However, we
have lost the information that the probabilities of
a and b are the same in all the probability func­
tions in P. This loss of information has some se­
rious repercussions. As we show in the full paper
(by extending the example above), one consequence
is that updates do not commute. More precisely,
suppose we start with a belief function Bel on a
set S, observe B � S and then observe C � S.
The result is the belief function BeiB(·IC), where
Be18(A) = Bei(AIB). Similarly, if we observe C
and then B, we get the belief function Belc(·IB).
We might hope that for all sets A, we would have
Be18(AIC) = Belc(AIB) = Bei(AIBAC). That is,
observing B then C should be the same as observ­
ing C then B, which in turn should be the same
as observing B A C. This is certainly the case if
Be l is a probability function, but not in general.
It turns out that the problem here is that informa­
tion is lost as we update the belief function. (See
the full paper for further details of this issue.) By
way of contrast, the DS rule of conditioning is com­
mutative. Conditioning with respect to C and then
with respect to B is equivalent to conditioning with
respect to B A C. However, as we have pointed out,
it has other problems when viewed as a technique
for updating beliefs.
These observations suggest to us that the ques­
tion of the "best" representation of evidence does

not have a unique answer. It may be easier to com­
pute with a pair of belief and plausibility functions
than to have to carry around a whole set of prob­
ability functions. Nevertheless, since information
may be lost in this process, this ease of computation
comes at a cost. (See [Pea89] for further examples
of this phenomenon.)
4

Conclusions

We have defined a new notion of conditional belief,
distinct from the DS notion, that seems to lead to
more intuitive results. Our notion also allows us to
avoid some paradoxes associated with the DS no­
tion. For example, we would expect that if both
an agent's belief in a proposition p given q and his
belief in p given -.q are at least a:, then his belief
in p should be at least a:, whether or not he learns
anything about q. This is essentially what Sav­
age [Sav54] has called the sure thing principle. It
is easy to see that conditional probability satisfies
the sure thing principle, but the DS conditioning
rule does not (see [Pea89] for an example). On
the other hand, it is easy to see that our notion of
conditioning does satisfy the sure thing principle.
For suppose we have an arbitrary belief function
Bel such that Bel(plq) � a: and Bel(pl-.q) � a:.
Choose an arbitrary probability function Pr com­
patible with Bel. By our definition of conditional
belief as an infimum, we see that Pr(plq) � a: and
Pr(pl-.q) � a:. So Pr(p) � a:. Thus, Pr(p) � a: for
all probability functions Pr compatible with Bel.
So, from Theorem 2.1, it follows that Bel(p) � a:.
Although our results show that belief functions
can play a useful role even when one wants to think
probabilistically, the observations of the previous
section do show that information can be lost if we
pass to belief functions. This suggests they should
be used with care.
One thing we have not really discussed in this
paper is what is considered perhaps the key com­
ponent of the Dempster-Shafer approach, namely,
the rule of combination. This rule is a way of com­
bining two belief functions to obtain a third one.
The reason we have not discussed it is that we feel
that the rule of combination does not fit in well
with the viewpoint of belief functions as a general­
ization of probability functions that is discussed in
this paper. However, there is another way of view­
ing belief functions, which is as representations of
evidence. This is in fact the view taken in [Sha76].
When belief is viewed as a representation of evi­
dence, then the rule of combination becomes more
appropriate. These issues are discussed in more de­
tail in a companion paper [HF90].

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

323

I
I
I
I
I
I
I

Appendix: Proof of Theorem 2.3
In order to carry out this proof, it will be tech­
nically useful to think in terms of mass function
rather than belief functions. A mass function on S
is simply a function m: 25 -+ [0,1] such that

m( 0) = 0

M2.

:EA�5 m(A)

Proposition 4.1:

I
I
I
I
I
I
I
I
I
I
I

2.

I

( [Sha76, p. 39])

If Bel is a belief function on 25 and S is fi­
nite, then there is a unique mass function m
on 25 such thatBel( A) = :EncA m(B) for every sub6et A of S.
-

Recall that we want to show Bel(·IB) is a belief
function, and PI(·IB) is the corresponding plausi­
bility function, provided that Bel(B) > 0. For sim­
plicity in this proof, we work under the assumption
that S is finite, so that there is a mass function m
corresponding to Bel. We remark that using tech­
niques of [F H89b] we could drop this assumption.
It is easy to see, using the formulas in Theo­
rem 2.2, that
=

=

=

. .. u A�o)

( -1)1II+lBel'( n A;).
iEI
I� { l,...,k) J;t:0

1.

If m is a mass function on S, then the func­
tion Bel: 25 -+ [0,1] defined by Bel( A) =
:En�A m(B) is a belief function.

Pl(AIB)

u

L:

>
=

Intuitively, m(A) is the weight of evidence for A
that has not already been assigned to some proper
subset of A. W ith this interpretation of mass, we
would expect that an agent's belief in A is the sum
of the masses he has assigned to all the subsets of A;
i.e., Bel(A) = :EncA m(B). Indeed, this intuition
is correct.

I

It is clear that Bel' satisfies BO-B2. All that
remains is to show that Bel' satisfies B3. Thus we
must show that the following inequality holds:

Bel'(A1

Ml.

1.

satisfies axioms BO-B3, it immediately follows that

Bel(·IB) does.

Pl(AnB)
PI( AnB) + Bel( AnB)
Bel( An_B
--' )'--­
-=- _ _,__
1
Pl(AnB) + Bel( AnB)
1 - Bel(AIB).
_

Thus, once we show that Bel(·IB) is a belief func­
tion, it will immediately follow that Pl(·IB) is the
corresponding plausibility function.
Let Bel' be the function defined on 28 such that
for each subset A of B,

Let B1 ,
,B1 be the distinct sets with positive
mass contained in B. Let A�, ..., A� be the distinct
sets with positive mass that intersectB but are not
subsets of B, and let Ai = A; n B, for 1 ::::; i ::::; n.
Since Bel(B) > 0, we know that there is some Bi
( but there may be no A;). Let a; = m(A;), and
{jf = m(B;), for each i. Let N = 2:::7=1 a:+ :E:=l f3f .
Note that N > 0, since there is some Bi. Let ai =
a:fN, and !3i = (3:/N, for each i. Thus, the ai's
and f3i's are normalized versions of the a; 's and f3; 's.
We want to define a mass function m' corre­
sponding to Bel'. We first need to do a small
detour. If s1
s�o is a string, and if 1 ::::; i1 <
< ip ::::; k, then we call B i
B i, a substring of
1
s1
Bin which we write as B i1
B i, j B1
Bk.
For example, s1s3s4 is a substring of BJB2B3B4B5.
The substring is proper if it does not equal the full
string ..,1
s,�:; we then write Bi1
Bi, � BJ . .. s�o.
We now define a function m", whose domain is
{A11 ..., A,.,B1, ... ,Bt}•, the set of finite strings
over the alphabet consisting of the names of the
sets with positive mass that intersect B. ( We shall
usually not bother to distinguish between a set and
the name of a set, but, as we shall see, it is con­
venient to consider explicitly strings of names of
sets. ) First, we let m"(B;) = !3i, for 1 ::::; i ::::; t. As­
sume now that we have defined m"(BiA h .. Ai, )
whenever s < r and j1 <
< j,. Assume that
i1 < . . < j•. Let
•

It clearly suffices to show that Bel' is a belief
function, since for all subsets C of S, we have
Bel(CIB) = Bel'(C nB). Once we show that Bel'

•

•

·

·

•

•

·

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

·

·

·

·

·

{3j

...A·)
m" (B·A·
I
}I
}.

(1)
m"(BiY).

L
Y -<Ai J ···Aj,

If A is not of the formBiAh .. Aj. with i1 < . . <
j., then m"(A) = 0.
We are now ready to define the alleged mass func­
tion m'. If X is the string Bi A h
Aj,, where
i1 < · < j., then we say that X represents the
set given by Bi U Ah U
U Aj,. We would like
to let m' be simply m" ( that is, by letting m' ap­
plied to a set be equal to m" applied to a string
that represents the set, and let m'(A) = 0 for sets
not of the form Bi Ah Aj. ). The problem is that
·

·

·

·

Bel'(A) = Bel(A)/(Bel(A) + Pl(AnB)).

•

·

•

·

·

·

·

·

·

·

324

several distinct strings may represent the same set;
for example, it is quite possible that, say, the sets
B1 U A 1 and B2 U A4 U As are the same. We define
m'(A) to be Lx represents A m"( X). For example,
if the set A equals both B1 U A 1 and B2 U A4 U As,
but if A is not of the form B; UAil ··· U Aj. for any
other choices of B;, Ail, ... , Aj. with il < ·· · < j
,,
then m'(A) = m"(B1A!) + m"(B2A4As). We
shall prove that m' is a mass function, and that
Bel '(A) = L ccA m'(C). This will show that Bel'
is a belief funct"'ion.
Thus, we must show that
A.

m'(0) = 0.

B.

m'(A) 2:: 0, for each A�B.

C. LA�B
D.

m'(A) = 1.

Bel'(A) = Lc�A m'(C).

By definition of m" and m', we know that (A)
holds. We now prove (D). Let AA:p• .. , A.�:, (where
k1 < ··· < kq) be the A; 's contained in A, and let
B;1, ••• , B;, be the B;'s contained in A. W hat is
Bel'(A)? As before, let N = 2:�1 a:: + L�=l f3I .
It is easy to see that Bel(A) = /3I1 + ··· + !3L, and
Pl(A n B) = N - ( a:A,1 + ···+ a:A,, + f3t1 + ···+ /3:, ).
Hence,

=

Bel'(A)
Bel(A)/(Bel(A) + Pl(A n B))
(f3tl + ··· + f3t,)/( N - a:A,l - ··· - a:A,,).

W hen we divide numerator and denominator by N,
we see that

To prove (D), we must show that LccA m'(C)
equals the right-hand side of (2). Let us call an
expression m"(B;Ah ···Aj,), where i is a mem­
ber of {i1, ... ,i,}, and where i1 < ... < j
,
are members of {k11 ••• , kq}, a good tenn. Note
that if m"(B;Ah ···Aj.) is a good term, then
B; U Ail U ··• U Aj, �A. Now LccA m'(C) equals
the sum of all good terms. This is because (a)
each good term is a part of the sum defining m'( C)
for exactly one C � A, and (b) if C � A, then
m'(C) is defined as the sum of certain good terms.
So we must show that the sum of all of the good
terms equals the right-hand side of (2). Now let
i be a fixed member of {i1,...,i,}. The sum of
all good terms of the form m"(B;Ah · ·Aj.) ex­
cept for the good term m"(B;A,�:1 ···A.�:,) is simply
·.

I
I

L

Y -<A,1 ··· A ,,

m"(B;Y),

it follows that the sum of all good terms of the form
m"(B;Ah ·· Aj.) equals /3;/(1- 0:,1:1 -···-a:,�:,).
So the sum of all good terms is ( /3;1 + ···+ /3;,)/(1a:,�:1 - ·· - a:.�=,), as desired. This proves (D).
Now (C) follows from (D), since it is easy to see
that Bel'(B) = 1. So we need only prove (B). The
proof of (B) involves some nontrivial combinatorial
arguments; the details can be found in the full pa­
per. I
·

·

We remark that in response to an early draft
of this paper, Zhang [Zha89] constructed a proof
along very different lines (although also quite com­
plicated).
Acknowledgments: The second author would
like to thank Judea Pearl for a series of net conver­
sations that inspired the development of the def­
inition of conditional belief. We also gratefully
acknowledge Nati Linial for his help in the proof
of Theorem 2.3. Comments by Hector Levesque,
Philippe Smets, and Moshe Vardi inspired a num­
ber of useful changes. Finally, we thank Judea
Pearl, Tom Strat, and Sue Andrews, for sending us,
respectively, [dCLM90], [SK89], and [Wal81], and
Enrique Ruspini for pointing out that our expres­
sions for conditional belief and plausibility actually
appear in [Dem67].



Standard models of multi-agent modal logic do not capture the fact that
information is often ambiguous, and may be interpreted in different ways by
different agents. We propose a framework that can model this, and consider
different semantics that capture different assumptions about the agents’ beliefs regarding whether or not there is ambiguity. We consider the impact
of ambiguity on a seminal result in economics: Aumann’s result saying that
agents with a common prior cannot agree to disagree. This result is known
not to hold if agents do not have a common prior; we show that it also does
not hold in the presence of ambiguity. We then consider the tradeoff between
assuming a common interpretation (i.e., no ambiguity) and a common prior
(i.e., shared initial beliefs).

1

Introduction

In the study of multi-agent modal logics, it is always implicitly assumed that all
agents interpret all formulas the same way. While they may have different beliefs
∗

This paper wil appear in the Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning (KR 2012).

1

regarding whether a formula ϕ is true, they agree on what ϕ means. Formally, this
is captured by the fact that the truth of ϕ does not depend on the agent.
Of course, in the real world, there is ambiguity; different agents may interpret
the same utterance in different ways. For example, consider a public announcement
p. Each player i may interpret p as corresponding to some event Ei , where Ei may
be different from Ej if i 6= j. This seems natural: even if people have a common
background, they may still disagree on how to interpret certain phenomena or new
information. Someone may interpret a smile as just a sign of friendliness; someone
else may interpret it as a “false” smile, concealing contempt; yet another person
may interpret it as a sign of sexual interest.
To model this formally, we can use a straightforward approach already used
in [Halpern 2009; Grove and Halpern 1993]: formulas are interpreted relative to a
player. But once we allow such ambiguity, further subtleties arise. Returning to
the announcement p, not only can it be interpreted differently by different players,
it may not even occur to the players that others may interpret the announcement in
a different way. Thus, for example, i may believe that Ei is common knowledge.
The assumption that each player believes that her interpretation is how everyone interprets the announcement is but one assumption we can make about ambiguity. It
is also possible that player i may be aware that there is more than one interpretation
of p, but believes that player j is aware of only one interpretation. For example,
think of a politician making an ambiguous statement which he realizes that different constituencies will interpret differently, but will not realize that there are other
possible interpretations. In this paper, we investigate a number of different semantics of ambiguity that correspond to some standard assumptions that people make
with regard to ambiguous statements, and investigate their relationship.
Our interest in ambiguity is motivated by a seminal result in game theory: Aumann’s [1976] theorem showing that players cannot “agree to disagree”. More
precisely, this theorem says that agents with a common prior on a state space cannot have common knowledge that they have different posteriors.1 This result has
been viewed as paradoxical in the economics literature. Trade in a stock market
seems to require common knowledge of disagreement (about the value of the stock
being traded), yet we clearly observe a great deal of trading.
One well known explanation for the disagreement is that we do not in fact
have common priors: agents start out with different beliefs. We provide a different
explanation here, in terms of ambiguity. It is easy to show that we can agree to disagree when there is ambiguity, even if there is a common prior. We then show that
these two explanations of the possibility of agreeing to disagree are closely related,
but not identical. We can convert an explanation in terms of ambiguity to an ex1

We explain this result in more detail in Section 3.

2

planation in terms of lack of common priors.2 Importantly, however, the converse
does not hold; there are models in which players have a common interpretation
that cannot in general be converted into an equivalent model with ambiguity and a
common prior. In other words, using heterogeneous priors may be too permissive
if we are interested in modeling a situation where differences in beliefs are due to
differences in interpretation.
Although our work is motivated by applications in economics, ambiguity has
long been a concern in linguistics and natural language processing. For example,
there has been a great deal of work on word-sense disambiguation (i.e., trying to
decide from context which of the multiple meanings of a word are intended); see
Hirst [1988] for a seminal contribution, and Navigli [2009] for a recent survey.
However, there does not seem to be much work on incorporating ambiguity into a
logic. Apart from the literature on the logic of context and on underspecification
(see Van Deemter and Peters [1996]), the only paper that we are aware of that does
this is one by Monz [1999]. Monz allows for statements that have multiple interpretations, just as we do. But rather than incorporating the ambiguity directly into
the logic, he considers updates by ambiguous statements. There are also connections between ambiguity and vagueness. Although the two notions are different—a
term is vague if it is not clear what its meaning is, and is ambiguous if it can have
multiple meanings, Halpern [2009] also used agent-dependent interpretations in
his model of vagueness, although the issues that arose were quite different from
those that concern us here.
The rest of this paper is organized as follows. Section 2 introduces the logic that
we consider. Section 3 investigates the implications of the common-prior assumption when there is ambiguity. Section 4 studies the tradeoff between heterogeneous
priors and ambiguity, and Section 5 concludes.

2

Syntax and Semantics

2.1 Syntax
We want a logic where players use a fixed common language, but each player may
interpret formulas in the language differently. We also want to allow the players to
be able to reason about (probabilistic) beliefs.
The syntax of the logic is straightforward (and is, indeed, essentially the syntax
already used in papers going back to Fagin and Halpern [1994]). There is a finite,
2

More precisely, we can convert a model with ambiguity and a common prior to an equivalent
model—equivalent in the sense that the same formulas are true—where there is no ambiguity but no
common prior.

3

nonempty set N = {1, . . . , n} of players, and a countable, nonempty set Φ of
primitive propositions. Let LC
n (Φ) be the set of formulas that can be constructed
starting from Φ, and closing off under conjunction, negation, the modal operators
{CBG }G⊆N,G6=∅ , and the formation of probability formulas. (We omit the Φ if it is
irrelevant or clear from context.) Probability formulas are constructed as follows.
If ϕ1 , . . . , ϕk are formulas, and a1 , . . . , ak , b ∈ Q, then for i ∈ N ,
a1 pr i (ϕ1 ) + . . . + ak pr i (ϕk ) ≥ b
is a probability formula, where pr i (ϕ) denotes the probability that player i assigns
to a formula ϕ. Note that this syntax allows for nested probability formulas. We
m+1
1 ϕ for ∧
ϕ for
use the abbreviation Bi ϕ for pr i (ϕ) = 1, EBG
i∈G Bi ϕ, and EBG
m EB 1 ϕ for m = 1, 2 . . .. Finally, we take true to be the abbreviation for a
EBG
G
fixed tautology such as p ∨ ¬p.

2.2 Epistemic probability structures
There are standard approaches for interpreting this language [Fagin and Halpern 1994],
but they all assume that there is no ambiguity, that is, that all players interpret the
primitive propositions the same way. To allow for different interpretations, we use
an approach used earlier [Halpern 2009; Grove and Halpern 1993]: formulas are
interpreted relative to a player.
An (epistemic probability) structure (over Φ) has the form
M = (Ω, (Πj )j∈N , (Pj )j∈N , (πj )j∈N ),
where Ω is the state space, and for each i ∈ N , Πi is a partition of Ω, Pi is a
function that assigns to each ω ∈ Ω a probability space Pi (ω) = (Ωi,ω , Fi,ω , µi,ω ),
and πi is an interpretation that associates with each state a truth assignment to
the primitive propositions in Φ. That is, πi (ω)(p) ∈ {true, false} for all ω and
each primitive proposition p. Intuitively, πi describes player i’s interpretation of
the primitive propositions. Standard models use only a single interpretation π;
this is equivalent in our framework to assuming that π1 = · · · = πn . We call a
structure where π1 = · · · = πn a common-interpretation structure. Denote by
[[p]]i the set of states where i assigns the value true to p. The partitions Πi are
called information partitions. While it is more standard in the philosophy and
computer science literature to use models where there is a binary relation Ki on
Ω for each agent i that describes i’s accessibility relation on states, we follow the
common approach in economics of working with information partitions here, as
that makes it particularly easy to define a player’s probabilistic beliefs. Assuming
information partitions corresponds to the case that Ki is an equivalence relation
4

(and thus defines a partition). The intuition is that a cell in the partition Πi is
defined by some information that i received, such as signals or observations of the
world. Intuitively, agent i receives the same information at each state in a cell of Πi .
Let Πi (ω) denote the cell of the partition Πi containing ω. Finally, the probability
space Pi (ω) = (Ωi,ω , Fi,ω , µi,ω ) describes the beliefs of player i at state ω, with
µi,ω a probability measure defined on the subspace Ωi,ω of the state space Ω. The
σ-algebra Fi,ω consists of the subsets of Ωi,ω to which µi,ω can assign a probability.
(If Ωi,ω is finite, we typically take Fi,ω = 2Ωi,ω , the set of all subsets of Ωi,ω .) The
interpretation is that µi,ω (E) is the probability that i assigns to event E ∈ Fi,ω in
state ω.
Throughout this paper, we make the following assumptions regarding the probability assignments Pi , i ∈ N :
A1. For all ω ∈ Ω, Ωi,ω = Πi (ω).
A2. For all ω ∈ Ω, if ω ′ ∈ Πi (ω), then Pi (ω ′ ) = Pi (ω).
A3. For all j ∈ N, ω, ω ′ ∈ Ω, Πi (ω) ∩ Πj (ω ′ ) ∈ Fi,ω .
Furthermore, we make the following joint assumption on players’ interpretations
and information partitions:
A4. For all ω ∈ Ω, i ∈ N , and primitive proposition p ∈ Φ, Πi (ω) ∩ [[p]]i ∈ Fi,ω .
These are all standard assumptions. A1 says that the set of states to which player i
assigns probability at state ω is just the set Πi (ω) of worlds that i considers possible
at state ω. A2 says that the probability space used is the same at all the worlds in a
cell of player i’s partition. Intuitively, this says that player i knows his probability
space. Informally, A3 says that player i can assign a probability to each of j’s cells,
given his information. A4 says that primitive propositions (as interpreted by player
i) are measurable according to player i.

2.3 Prior-generated beliefs and the common-prior assumption
One assumption that we do not necessarily make, but want to examine in this
framework, is the common-prior assumption. The common-prior assumption is
an instance of a more general assumption, that beliefs are generated from a prior,
which we now define. The intuition is that players start with a prior probability;
they then update the prior in light of their information. Player i’s information is
captured by her partition Πi . Thus, if i’s prior is νi , then we would expect µi,ω to
be νi (· | Πi (ω)).

5

Definition 2.1 An epistemic probability structure M = (Ω, (Πj )j∈N , (Pj )j∈N , (πj )j∈N )
has prior-generated beliefs (generated by (F1 , ν1 ), . . . , (Fn , νn )) if, for each player
i, there exist probability spaces (Ω, Fi , νi ) such that
• for all i, j ∈ N and ω ∈ Ω, Πj (ω) ∈ Fi ;
• for all i ∈ N and ω ∈ Ω, Pi (ω) = (Πi (ω), Fi | Πi (ω), µi,ω ), where Fi |
Πi (ω) is the restriction of Fi to Πi (ω),3 and µi,ω (E) = νi (E | Πi (ω)) for
all E ∈ Fi | Πi (ω) if νi (Πi (ω)) > 0. (There are no constraints on νi,ω if
νi (Πi (ω)) = 0.)

It is easy to check that if M has prior-generated beliefs, then M satisfies A1, A2,
and A3. More interestingly for our purposes, the converse also holds for a large
class of structures. Say that a structure is countably partitioned if for each player
i, the information partition Πi has countably many elements, i.e., Πi is a finite or
countably infinite collection of subsets of Ω.
Proposition 2.2 If a structure M has prior-generated beliefs, then M satisfies
A1, A2, and A3. Moreover, every countably partitioned structure that satisfies
A1, A2, and A3 is one with prior-generated beliefs, with the priors νi satisfying
νi (Πi (ω)) > 0 for each player i ∈ N and state ω ∈ Ω.
Proof. The first part is immediate. To prove the second claim, suppose that
M is a structure satisfying A1–A3. Let Fi be the unique algebra generated by
∪ω∈Ω Fi,ω . To define νi , if there are Ni < ∞ cells in the partition Πi , define
νi (ω) = N1i µi,ω (ω). Otherwise, if the collection Πi is countably infinite, order
the elements of Πi as p1i , p2i , . . .. Choose some state ωk ∈ pki for each k, with
associated probability space Pi (ωk ) = (Ωi,ωk , Fi,ωk , µi,ωk ). By A2, P
each choice
of ωk in pki gives the same probability measure µi,ωk . Define νi = k 21k µi,ωk .
It is easy to see that νi is a probability measure on Ω, and that M is generated by
(F1 , ν1 ), . . . , (Fn , νn ).
Note that the requirement that that M is countably partitioned is necessary to
ensure that we can have νi (Πi (ω)) > 0 for each player i and state ω.
In light of Proposition 2.2, when it is convenient, we will talk of a structure
satisfying A1–A3 as being generated by (F1 , ν1 ), . . . , (Fn , νn ).
The common-prior assumption is essentially just the special case of priorgenerated beliefs where all the priors are identical. We make one additional technical assumption. To state this assumption, we need one more definition. A state
3

Recall that the restriction of Fi to Πi (ω) is the σ-algebra {B ∩ Πi (ω) : B ∈ Fi }.

6

ω ′ ∈ Ω is G-reachable from ω ∈ Ω, for G ⊆ N , if there exists a sequence
ω0 , . . . , ωm in Ω with ω0 = ω and ωm = ω ′ , and i1 , . . . , im ∈ G such that
ωℓ ∈ Πiℓ (ωℓ−1 ). Denote by RG (ω) ⊆ Ω the set of states G-reachable from ω.
Definition 2.3 An epistemic probability structure M = (Ω, (Πj )j∈N , (Pj )j∈N , (πj )j∈N )
satisfies the common-prior assumption (CPA) if there exists a probability space
(Ω, F, ν) such that M has prior-generated beliefs generated by ((F, ν), . . . , (F, ν)),
and ν(RN (ω)) > 0 for all ω ∈ Ω.
As shown by Halpern [2002], the assumption that ν(RN (ω)) > 0 for each ω ∈ Ω
is needed for Aumann’s [1976] impossibility result.

2.4 Capturing ambiguity
We use epistemic probability structures to give meaning to formulas. Since primitive propositions are interpreted relative to players, we must allow the interpretation
of arbitrary formulas to depend on the player as well. Exactly how we do this depends on what further assumptions we make about what players know about each
other’s interpretations. There are many assumptions that could be made. We focus
on two of them here, ones that we believe arise in applications of interest, and then
reconsider them under the assumption that there may be some ambiguity about the
partitions.
Believing there is no ambiguity The first approach is appropriate for situations
where players may interpret statements differently, but it does not occur to them
that there is another way of interpreting the statement. Thus, in this model, if
there is a public announcement, all players will think that their interpretation of the
announcement is common knowledge. We write (M, ω, i) out ϕ to denote that ϕ
is true at state ω according to player i (that is, according to i’s interpretation of the
primitive propositions in ϕ). The superscript out denotes outermost scope, since
the formulas are interpreted relative to the “outermost” player, namely the player i
on the left-hand side of out . We define out , as usual, by induction.
If p is a primitive proposition,
(M, ω, i) out p iff πi (ω)(p) = true.
This just says that player i interprets a primitive proposition p according to his
interpretation function πi . This clause is common to all our approaches for dealing
with ambiguity.

7

For conjunction and negation, as is standard,
(M, ω, i) out ¬ϕ iff (M, ω, i) 6 out ϕ,
(M, ω, i) out ϕ ∧ ψ iff (M, ω, i) out ϕ and (M, ω, i) out ψ.
Now consider a probability formula of the form a1 pr j (ϕ1 )+. . .+ak pr j (ϕk ) ≥
b. The key feature that distinguishes this semantics is how i interprets j’s beliefs.
This is where we capture the intuition that it does not occur to i that there is another
way of interpreting the formulas other than the way she does. Let
[[ϕ]]out
= {ω : (M, ω, i) out ϕ}.
i
Thus, [[ϕ]]out
is the event consisting of the set of states where ϕ is true, according
i
to i. Note that A1 and A3 guarantee that the restriction of Ωj,ω to Πi (ω) belongs
to Fi,ω . Assume inductively that [[ϕ1 ]]out
∩ Ωj,ω , . . . , [[ϕk ]]out
∩ Ωj,ω ∈ Fj,ω .
i
i
The base case of this induction, where ϕ is a primitive proposition, is immediate
from A3 and A4, and the induction assumption clearly extends to negations and
conjunctions. We now define
(M, ω, i) out a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b iff
a1 µj,ω ([[ϕ1 ]]out
∩ Ωj,ω ) + . . . + ak µj,ω ([[ϕk ]]out
∩ Ωj,ω ) ≥ b.
i
i
Note that it easily follows from A2 that (M, ω, i) out a1 pr j (ϕ1 )+. . .+ak pr j (ϕk ) ≥
b if and only if (M, ω ′ , i) out a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b for all ω ′ ∈
Πj (ω). Thus, [[a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b]]i is a union of cells of Πj , and
hence [[a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b]]i ∩ Ωj,ω ∈ Fj,ω .
With this semantics, according to player i, player j assigns ϕ probability b
if and only if the set of worlds where ϕ holds according to i has probability b
according to j. Intuitively, although i “understands” j’s probability space, player
i is not aware that j may interpret ϕ differently from the way she (i) does. That i
understands j’s probability space is plausible if we assume that there is a common
prior and that i knows j’s partition (this knowledge is embodied in the assumption
that i intersects [[ϕk ]]out
with Ωj,ω when assessing what probability j assigns to
i
ϕk ).4
4

Note that at state ω, player i will not in general know that it is state ω. In particular, even if
we assume that i knows which element of j’s partition contains ω, i will not in general know which
of j’s cells describes j’s current information. But we assume that i does know that if the state is
ω, then j information is described by Ωj,ω . Thus, as usual, “(M, i, ω) out ϕ” should perhaps be
understood as “according to i, ϕ is true if the actual world is ω”. This interpretational issue arises
even without ambiguity in the picture.

8

Given our interpretation of probability formulas, the interpretation of Bj ϕ and
EB k ϕ follows. For example,
(M, ω, i) out Bj ϕ iff µj,ω ([[ϕ]]out
i ) = 1.
For readers more used to belief defined in terms of a possibility relation, note that
if the probability
j,ω is discrete (i.e., all sets are µj,ω -measurable, and
P measure µ
′ ) for all subsets E ⊂ Π (ω)), we can define B =
µj,ω (E) =
µ
(ω
′
j
j
ω ∈E j,ω
{(ω, ω ′ ) : µj,ω (ω ′ ) > 0}; that is, (ω, ω ′ ) ∈ Bj if, in state ω, agent j gives state ω ′
positive probability. In that case, (M, ω, i) out Bj ϕ iff (M, ω ′ , i) out ϕ for all
ω ′ such that (ω, ω ′ ) ∈ Bj . That is, (M, ω, i) out Bj ϕ iff ϕ is true according to i
in all the worlds to which j assigns positive probability at ω.
It is important to note that (M, ω, i)  ϕ does not imply (M, ω, i)  Bi ϕ:
while (M, ω, i) out ϕ means “ϕ is true at ω according to i’s interpretation,” this
does not mean that i believes ϕ at state ω. The reason is that i can be uncertain as
to which state is the actual state. For i to believe ϕ at ω, ϕ would have to be true
(according to i’s interpretation) at all states to which i assigns positive probability.
Finally, we define
k
(M, ω, i) out CBG ϕ iff (M, ω, i) out EBG
ϕ for k = 1, 2, . . .

for any nonempty subset G ⊆ N of players.
Awareness of possible ambiguity We now consider the second way of interpreting formulas. This is appropriate for players who realize that other players may
interpret formulas differently. We write (M, ω, i) in ϕ to denote that ϕ is true
at state ω according to player i using this interpretation, which is called innermost
scope. The definition of in is identical to that of out except for the interpretation
of probability formulas. In this case, we have
(M, ω, i) in a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b iff
in
a1 µj,ω ([[ϕ1 ]]in
j ∩ Ωj,ω ) + . . . + ak µj,ω ([[ϕk ]]j ∩ Ωj,ω ) ≥ b,
in ϕ. Hence, according
where [[ϕ]]in
j is the set of states ω such that (M, ω, j) 
to player i, player j assigns ϕ probability b if and only if the set of worlds where
ϕ holds according to j has probability b according to j. Intuitively, now i realizes
that j may interpret ϕ differently from the way that she (i) does, and thus assumes
that j uses his (j’s) interpretation to evaluate the probability of ϕ. Again, in the
case that µj,ω is discrete, this means that (M, ω, i) in Bj ϕ iff (M, ω ′ , j) in ϕ
for all ω ′ such that (ω, ω ′ ) ∈ Bj .

9

Note for future reference that if ϕ is a probability formula or a formula of the
form CBG ϕ′ , then it is easy to see that (M, ω, i) in ϕ if and only if (M, ω, j) in
ϕ; we sometimes write (M, ω) in ϕ in this case. Clearly, out and in agree in
the common-interpretation case, and we can write .
Ambiguity about information partitions Up to now, we have assumed that
players “understand” each other’s probability spaces. This may not be so reasonable in the presence of ambiguity and prior-generated beliefs. We want to model
the following type of situation. Players receive information, or signals, about the
true state of the world, in the form of strings (formulas). Each player understands
what signals he and other players receive in different states of the world, but players may interpret signals differently. For instance, player i may understand that j
sees a red car if ω is the true state of the world, but i may or may not be aware that
j has a different interpretation of “red” than i does. In the latter case, i does not
have a full understanding of j’s information structure.
We would like to think of a player’s information as being characterized by a
formula (intuitively, the formula that describes the signals received). Even if the
formulas that describe each information set are commonly known, in the presence
of ambiguity, they might be interpreted differently.
To make this precise, let Φ∗ be the set of formulas that is obtained from Φ by
closing off under negation and conjunction. That is, Φ∗ consists of all propositional formulas that can be formed from the primitive propositions in Φ. Since the
formulas in Φ∗ are not composed of probability formulas, and thus do not involve
any reasoning about interpretations, we can extend the function πi (·) to Φ∗ in a
straightforward way, and write [[ϕ]]i for the set of the states of the world where the
formula ϕ ∈ Φ∗ is true according to i.
The key new assumption we make to model players’ imperfect understanding
of the other players’ probability spaces is that i’s partition cell at ω is described by
a formula ϕi,ω ∈ Φ∗ . But, of course, this formula may be interpreted differently by
each player. We want Πi (ω) to coincide with i’s interpretation of the formula ϕi,ω .
If player j understands that i may be using a different interpretation than he does
(i.e., the appropriate semantics are the innermost-scope semantics), then j correctly
infers that the set of states that i thinks are possible in ω is Πi (ω) = [[ϕi,ω ]]i . But
if j does not understand that i may interpret formulas in a different way (i.e., under
outermost scope), then he thinks that the set of states that i thinks are possible in
ω is given by [[ϕi,ω ]]j , and, of course, [[ϕi,ω ]]j may not coincide with Πi (ω). In
any case, we require that j understand that these formulas form a partition and that
ω belongs to [[ϕi,ω ]]j . Thus, we consider structures that satisfy A5 and A6 (for
outermost scope) or A5 and A6’ (for innermost scope), in addition to A1–A4.

10

A5. For each i ∈ N and ω ∈ Ω, there is a formula ϕi,ω ∈ Φ∗ such that Πi (ω) =
[[ϕi,ω ]]i .
A6. For each i, j ∈ N , the collection {[[ϕi,ω ]]j : ω ∈ Ω} is a partition of Ω and
for all ω ∈ Ω, ω ∈ [[ϕi,ω ]]j .
A6′ . For each i ∈ N , the collection {[[ϕi,ω ]]i : ω ∈ Ω} is a partition of Ω and for
all ω ∈ Ω, ω ∈ [[ϕi,ω ]]i .
Assumption A6 is appropriate for outermost scope: it presumes that player j uses
his own interpretation of ϕi,ω in deducing the beliefs for i in ω. Assumption A6′
is appropriate for innermost scope. Note that A6′ is a weakening of A6. While
A6 requires the signals for player i to induce an information partition according
to every player j, the weaker version A6′ requires this to hold only for player i
himself.
We can now define analogues of outermost scope and innermost scope in the
presence of ambiguous information. Thus, we define two more truth relations,
out,ai and in,ai . (The “ai” here stands for “ambiguity of information”.) The only
difference between out,ai and out is in the semantics of probability formulas.
In giving the semantics in a structure M , we assume that M has prior-generated
beliefs, generated by (F1 , ν1 ), . . . , (Fn , νn ). As we observed in Proposition 2.2,
this assumption is without loss of generality as long as the structure is countably
partitioned. However, the choice of prior beliefs is relevant, as we shall see, so
we have to be explicit about them. When i evaluates j’s probability at a state
ω, instead of using νj,ω , player i uses νj (· | [[ϕj,ω ]]i ). When i = j, these two
approaches agree, but in general they do not. Thus, assuming that M satisfies A5
and A6 (which are the appropriate assumptions for the outermost-scope semantics),
we have
(M, ω, i) out,ai a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b iff
) + ...
| [[ϕj,ω ]]out,ai
a1 νj ([[ϕ1 ]]out,ai
i
i
out,ai
+ak νj ([[ϕk ]]out,ai
|
[[ϕ
]]
) ≥ b,
j,ω i
i
= {ω ′ : (M, ω, i) out,ai ψ}.
where [[ψ]]out,ai
i
That is, at ω ∈ Ω, player j receives the information (a string) ϕj,ω , which he
interprets as [[ϕj,ω ]]j . Player i understands that j receives the information ϕj,ω in
state ω, but interprets this as [[ϕj,ω ]]i . This models a situation such as the following.
In state ω, player j sees a red car, and thinks possible all states of the world where
he sees a car that is red (according to j). Player i knows that at world ω player
j will see a red car (although she may not know that the actual world is ω, and
thus does not know what color of car player j actually sees). However, i has a
11

somewhat different interpretation of “red car” (or, more precisely, of j seeing a red
car) than j; i’s interpretation corresponds to the event [[ϕj,ω ]]i . Since i understands
that j’s beliefs are determined by conditioning her prior νj on her information, i
can compute what she believes j’s beliefs are.
We can define in,ai in an analogous way. Thus, the semantics for formulas
that do not involve probability formulas are as given by in , while the semantics of
probability formulas is defined as follows (where M is assumed to satisfy A5 and
A6′ , which are the appropriate assumptions for the innermost-scope semantics):
(M, ω, i) in,ai a1 pr j (ϕ1 ) + . . . + ak pr j (ϕk ) ≥ b iff
a1 νj ([[ϕ1 ]]in,ai
| [[ϕj,ω ]]in,ai
) + ...
j
j
in,ai
in,ai
+ak νj ([[ϕk ]]j
| [[ϕj,ω ]]j ) ≥ b.
Note that although we have written [[ϕj,ω ]]in,ai
, since ϕj,ω is a propositional fori
out,ai
out = [[ϕ ]]in . It is important that
mula, [[ϕj,ω ]]in,ai
=
[[ϕ
]]
=
[[ϕ
]]
j,ω i
j,ω i
j,ω i
i
ϕj,ω is a propositional formula here; otherwise, we would have circularities in the
.
definition, and would somehow need to define [[ϕj,ω ]]in,ai
i
Again, here it may be instructive to consider the definition of Bj ϕ in the case
that µj,ω is discrete for all ω. In this case, Bj becomes the set {(ω, ω ′ ) : νj (ω ′ |
[[ϕj,ω ]]in,ai
) > 0. That is, state ω ′ is considered possible by player j in state
j
ω if agent j gives ω ′ positive probability after conditioning his prior νj on (his
interpretation of) the information ϕj,ω he receives in state ω. With this definition
of Bj , we have, as expected, (M, ω, i) in,ai Bj ϕ iff (M, ω ′ , i) in,ai ϕ for all ω ′
such that (ω, ω ′ ) ∈ Bj .
The differences in the different semantics arise only when we consider probability formulas. If we go back to our example with the red car, we now have a
situation where player j sees a red car in state ω, and thinks possible all states
where he sees a red car. Player i knows that in state ω, player j sees a car that he
(j) interprets to be red, and that this determines his posterior. Since i understands
j’s notion of seeing a red car, she has a correct perception of j’s posterior in each
state of the world. Thus, the semantics for in,ai are identical to those for in
(restricted to the class of structures with prior-generated beliefs that satisfy A5 and
A6′ ), though the information partitions are not predefined, but rather generated by
the signals.
Note that, given an epistemic structure M satisfying A1–A4, there are many
choices for νi that allow M to be viewed as being generated by prior beliefs. All
that is required of νj is that for all ω ∈ Ω and E ∈ Fj,ω such that E ⊆ [[ϕj,ω ]]jout,ai ,
it holds that νj (E ∩ [[ϕj,ω ]]out,ai
)/νj ([[ϕj,ω ]]out,ai
) = µj,ω (E). However, because
j
j
out,ai
out,ai
[[ϕj,ω ]]i
may not be a subset of [[ϕj,ω ]]j
= Πj (ω), we can have two prior
12

probabilities νj and νj′ that generate the same posterior beliefs for j, and still have
) for some formulas
| [[ϕj,ω ]]out,ai
) 6= νj′ ([[ϕk ]]out,ai
| [[ϕj,ω ]]out,ai
νj ([[ϕk ]]out,ai
i
i
i
i
ϕk . Thus, we must be explicit about our choice of priors here.

3

The common-prior assumption revisited

This section applies the framework developed in the previous sections to understand the implications of assuming a common prior when there is ambiguity. The
application in Section 3.1 makes use of the outermost- and innermost-scope semantics, while Section 3.2 considers a setting with ambiguity about information
partitions.

3.1 Agreeing to Disagree
The first application we consider concerns the result of Aumann [1976] that players
cannot “agree to disagree” if they have a common prior. As we show now, this is no
longer true if players can have different interpretations. But exactly what “agreeing
to disagree” means depends on which semantics we use.
Example 3.1 [Agreeing to Disagree] Consider a structure M with a single state
ω, such that π1 (ω)(p) = true and π2 (ω)(p) = false. Clearly M satisfies the
CPA. The fact that there is only a single state in M means that, although the players
interpret p differently, there is perfect understanding of how p is interpreted by each
player. Specifically, taking G = {1, 2}, we have that (M, ω) in CBG (B1 p ∧
B2 ¬p). Thus, with innermost scope, according to each player, there is common
belief that they have different beliefs at state ω; that is, they agree to disagree.
With outermost scope, we do not have an agreement to disagree in the standard
sense, but the players do disagree on what they have common belief about. Specifically, (M, ω, 1) out CBG p and (M, ω, 2) out CBG ¬p. That is, according to
player 1, there is common belief of p; and according to player 2, there is common
belief of ¬p. To us, it seems that we have modeled a rather common situation here!

Note that in the model of Example 3.1, there is maximal ambiguity: the players
disagree with probability 1. We also have complete disagreement. As the following
result shows, the less disagreement there is in the interpretation of events, the closer
the players come to not being able to agree to disagree. Suppose that M satisfies
the CPA, where ν is the common prior, and that ϕ ∈ Φ∗ (so that ϕ is a propositional
formula). Say that ϕ is only ǫ-ambiguous in M if the set of states where the players

13

disagree on the interpretation of ϕ has ν-measure at most ǫ; that is,
ν({ω : ∃i, j((M, ω, i)  ϕ and (M, ω, j)  ¬ϕ})}) ≤ ǫ.
We write  here because, as we observed before, all the semantic approaches agree
on propositional formulas, so this definition makes sense independent of the semantic approach used. Note that if players have a common interpretation, then all
formulas are 0-ambiguous.
Proposition 3.2 If M satisfies the CPA and ϕ is only ǫ-ambiguous in M , then
there cannot exist players i and j, numbers b and b′ with b′ > b + ǫ, and a state ω
such that all states are G-reachable from ω and
(M, ω) in CBG ((pr i (ϕ) < b) ∧ (pr j (ϕ) > b′ )).
Proof. Essentially the same arguments as those used by Aumann [1976] can be
used to show that if all states are reachable from ω and (M, ω) in CBG (pr i (ϕ) <
b), then it must be the case that ν([[ϕ]]i ) < b, where ν is the common prior. Similarly, ν([[ϕ]]j ) > b′ . This contradicts the assumption that ϕ is only ǫ-ambiguous
in M .

3.2 Understanding differences in beliefs
Since our framework separates meaning from message, it is worth asking what
happens if players receive the same message, but interpret it differently. Aumann
[1987] has argued that “people with different information may legitimately entertain different probabilities, but there is no rational basis for people who have always
been fed precisely the same information to do so.” Here we show that this is no
longer true when information is ambiguous, even if players have a common prior
and fully understand the ambiguity that they face, except under strong assumptions
on players’ beliefs about the information that others receive. This could happen
if players with exactly the same background and information can interpret things
differently, and thus have different beliefs.
We assume that information partitions are generated by signals, which may be
ambiguous. That is, in each state of the world ω, each player i receives some signal σi,ω that determines the states of the world he thinks possible; that is, Πi (ω) =
[[rec i (σi,ω )]]i , where rec i (σi,ω ) ∈ Φ∗ is “i received σi,ω .” As usual, we restrict attention to structures with prior-generated beliefs that satisfy A5 and A6′ when considering innermost-scope semantics and A5 and A6 when considering outermostscope semantics.
14

In any given state, the signals that determine the states that players think are
possible may be the same or may differ across players. Following Aumann [1987],
we are particularly interested in the former case. Formally, we say that σω is a
common signal in ω if σi,ω = σω for all i ∈ N . For example, if players have a
common interpretation, and all players observe a red car in state ω, then σω is “red
car”, while rec i (σω ) is “i observes a car that is red.” The fact that “red car” is
a common signal in ω means that all players in fact observe a red car in state ω.
But assuming that players have received a common signal does not imply that they
have the same posteriors, as the next example shows:
Example 3.3 There are two players, 1 and 2, and three states, labeled ω1 , ω2 , ω3 .
The common prior gives each state equal probability, and players have the same
interpretation. In ω1 , both players receive signal σ; in ω2 , only 1 does; in ω3 , only
2 receives σ. The primitive proposition p is true in ω1 and ω2 , and the primitive
proposition q is true in ω1 and ω3 . In state ω1 , both players receive signal σ, but
player 1 assigns probability 1 to p and probability 12 to q, while 2 gives probability
1
2 to p and probability 1 to q.
Thus, players who receive a common signal can end up having a different posterior
over formulas, even if they have a common prior and the same interpretation. The
problem is that even though players have received the same signal, they do not
know that the other has received it, and they do not know that the other knows they
have received it, and so on. That is, the fact that players have received a common
signal in ω does not imply that the signal is common knowledge in ω. We say
that signal σ is a public signal at state ω if (M, ω)  CBN (∧i∈N rec i (σ)): it is
commonly believed at ω that all players received σ.
For the remainder of this section, we will be considering structures with a common prior ν. To avoid dealing with topological issues, we assume that ν is a discrete measure. Of course, if the common prior ν is discrete, then so are all the
measures µi,ω . Let Supp(µ) denote the support of the probability measure µ. If µ
is discrete, then Supp(µ) = {ω : µ(ω) 6= 0}.
Even though common signals are not sufficient for players to have the same
beliefs, as Example 3.3 demonstrates, Aumann’s claim does hold for commoninterpretation structures if players receive a public signal (provided that they started
with a common prior):
Proposition 3.4 If M is a common-interpretation structure with a common prior,
and σ is a public signal at ω, then players’ posteriors are identical at ω: for all
i, j ∈ N and E ∈ F,
µi,ω (E ∩ Πi (ω)) = µj,ω (E ∩ Πj (ω)).
15

In particular, for any formula ϕ,
µi,ω ([[ϕ]] ∩ Πi (ω)) = µj,ω ([[ϕ]] ∩ Πj (ω)).
Proof. Let ν be the common prior in M . By assumption, Πi (ω) = [[rec i (σ)]] for
all players i ∈ N . Since σ is public, we have that (M, ω)  Bi (rec j (σ)). Thus,
(M, ω ′ )  rec j (σ) for all ω ′ ∈ Supp(µi,ω ). It follows that Supp(µi,ω ) ⊆ Πj (ω)
for all players i and j. Since µi,ω (Supp(µi,ω )) = 1, we have that ν(Πj (ω) |
Πi (ω)) = ν(Πi (ω) | Πj (ω)) = 1. Thus, for all E ∈ F, we must have ν(E |
Πi (ω)) = ν(E | Πj (ω). The result now follows immediately.
There is another way of formalizing the assumption that (it is commonly believed that) players are “ fed the same information”; namely, we say that if one
player i receives a signal σ then so do all others. Formally, a signal σ is a shared
signal at state ω if (M, ω)  ∧i,j∈N CBN (rec i (σ) ⇔ rec j (σ)). If there is no ambiguity, a signal is shared iff it is public; we leave the straightforward proof (which
uses ideas from the proof of Proposition 3.4) to the reader.
Proposition 3.5 If M is a common-interpretation structure, and σω is received at
state ω by all players, then σω is a public signal at ω iff σ is a shared signal at ω.
The assumption that signals are public or shared is quite strong: one requires common belief that a particular signal is received (and so precludes any uncertainties
about what one player believes that other players believe that others have received),
while the other requires common belief that different players always receive the
same signal (and, similarly, precludes uncertainties about what is received).
What happens if we introduce ambiguity? If the signal itself is a propositional
formula (which is the case in many cases of interest), then players may interpret
the signal differently; that is, we may have [[σω ]]i 6= [[σω ]]j for i 6= j. Moreover,
players may have a different interpretation of observing a given signal, i.e., it is
possible that [[rec i (σω )]]i 6= [[rec i (σω )]]j . Going back to our example of the red
car, different players may interpret “red car” differently, and they may interpret the
notion of observing a red car differently. In addition, it is now possible that players
have the same posteriors over events, but not over formulas, or vice versa, given
that they may interpret formulas differently.
If we assume that players are not aware that there is ambiguity, then we retain
the equivalence between shared signals and common signals, and players’ posteriors over formulas coincide after receiving a public signal. However, they may have
different beliefs over events:
Proposition 3.6 If M is a structure satisfying A5 and A6, and σ is received at state
ω by all players, then σ is a public signal at ω iff σ is a shared signal at ω under
16

outermost-scope semantics. Moreover, if M has a common prior, and σ is a public
signal at ω, then players’ posteriors on formulas are identical at ω; that is, for all
formulas ψ, we have
µi,ω ([[ψ]]out,ai
∩ Πi (ω)) = µj,ω ([[ψ]]out,ai
∩ Πj (ω)).
i
j
However, players’ posteriors on events may differ; that is, there may exist some E
such that
µi,ω (E ∩ Πi (ω)) 6= µj,ω (E ∩ Πj (ω)).
We leave the straightforward arguments to the reader.
The situation for innermost scope presents an interesting contrast. A first observation is that public signals and shared signals are no longer equivalent:
Example 3.7 Consider a structure M with two players, where Ω = {ω11 , ω12 , ω21 , ω22 }.
Suppose that [[rec 1 (σ)]]1 = [[rec 2 (σ)]]1 = {ω11 , ω12 }, and [[rec 1 (σ)]]2 = [[rec 2 (σ)]]2 =
{ω11 , ω21 }. Assume that the beliefs in M are generated by a common prior that
gives each state probability 1/4. Clearly (M, ω11 ) in,ai CBN (rec 1 (σ) ⇔ rec 2 (σ))∧
¬CBN (rec 1 (σ)).
The problem in Example 3.7 is that, although the signal is shared, the players don’t
interpret receiving the signal the same way. It is not necessarily the case that player
1 received σ from player 1’s point of view iff player 2 received σ from player 2’s
point of view. The assumption that players receive shared signals is not strong
enough to ensure that they have identical posteriors, either over formulas or over
events. In Example 3.7, for example, players clearly have different posteriors on
the event {ω11 , ω12 } in state ω11 ; similarly, it is not hard to show that players can
have different posteriors over formulas. Say that σ is strongly shared at state ω if
• (M, ω) in,ai ∧i,j CBN (rec i (σ) ⇔ rec j (σ)); and
• (M, ω) in,ai ∧i,j CBN (Bi (rec i (σ)) ⇔ Bj (rec j (σ))).
The second clause says that it is commonly believed at ω that each player believes
that he has received σ iff each of the other players believes that he has received
σ. This clause is implied by the first in common-interpretation structures and with
outermost scope, but not with innermost scope.
Proposition 3.8 If M is a structure satisfying A5 and A6′ , and σ is received at ω
by all players, then σ is a public signal at ω iff σ is a strongly shared signal at ω
under the innermost-scope semantics. If M is a structure with a common prior and

17

σ is a public signal at ω, then players’ posteriors over events are identical at ω:
for all i, j ∈ N and all E ∈ F,
µi,ω (E ∩ Πi (ω)) = µj,ω (E ∩ Πj (ω)).
However, players’ posteriors on formulas may differ; that is, for some formula ψ,
we could have that
µi,ω ([[ψ]]in,ai
∩ Πi (ω)) 6= µj,ω ([[ψ]]in,ai
∩ Πj (ω)).
i
j
These results emphasize the effect of ambiguity on shared and public signals.

4

Common priors or common interpretations?

As Example 3.1 shows, we can have agreement to disagree with ambiguity under the in semantics (and thus, also the in,ai semantics). We also know that
we can do this by having heterogeneous priors. As we now show, structures
with ambiguity that satisfy the CPA have the same expressive power as commoninterpretation structures that do not necessarily satisfy the CPA (and commoninterpretation structures, by definition, have no ambiguity). On the other hand,
common-interpretation structures with heterogeneous priors are more general than
structures with ambiguity and common priors.
To make this precise, we consider what formulas are valid in structures with
or without ambiguity or a common prior. To define what it means for a formula
to be valid, we need some more notation. Fix a nonempty, countable set Ψ of
primitive propositions, and let M(Ψ) be the class of all structures that satisfy A1–
A4 and that are defined over some nonempty subset Φ of Ψ such that Ψ \ Φ is
countably infinite.5 Given a subset Φ of Ψ, a formula ϕ ∈ LC
n (Φ), and a structure
M ∈ M(Ψ) over Φ, we say that ϕ is valid in M according to outermost scope, and
write M out ϕ, if (M, ω, i) out ϕ for all ω ∈ Ω and i ∈ N . Given ϕ ∈ Ψ, say
that ϕ is valid according to outermost scope in a class N ⊆ M(Ψ) of structures,
and write N out ϕ, if M out ϕ for all M ∈ N defined over a set Φ ⊂ Ψ of
primitive propositions that includes all the primitive propositions that appear in ϕ.
We get analogous definitions by replacing out by in , out,ai and in,ai
throughout (in the latter two cases, we have to restrict N to structures that satisfy A5 and A6 or A6′ , respectively, in addition to A1–A4). Finally, given a class
5

Most of our results hold if we just consider the set of structures defined over some fixed set Φ
of primitive propositions. However, for one of our results, we need to be able to add fresh primitive
propositions to the language. Thus, we allow the set Φ of primitive propositions to vary over the
structures we consider, but require Ψ \ Φ to be countably infinite so that there are always “fresh”
primitive propositions that we can add to the language.

18

of structures N , let Nc be the subclass of N in which players have a common interpretation. Thus, Mc (Ψ) denotes the structures in M(Ψ) with a common interpretation. Let Mai (Ψ) denote all structures in M(Ψ) with prior-generated beliefs
that satisfy A5 and A6 (where we assume that the prior ν that describes the initial
beliefs is given explicitly).6 Finally, let Mcpa (Ψ) (resp., Mcpa,ai (Ψ)) consist of
the structures in M(Ψ) (resp., Mai (Ψ)) satisfying the CPA.
Proposition 4.1 For all formulas ϕ ∈ LC
n (Ψ), the following are equivalent:
(a) Mc (Ψ)  ϕ;
(b) M(Ψ) out ϕ;
(c) M(Ψ) in ϕ;
(d) Mai
c (Ψ)  ϕ;
(e) Mai (Ψ) out,ai ϕ;
(f) Mai (Ψ) in,ai ϕ.
Proof. Since the set of structures with a common interpretation is a subset of the
set of structures, it is immediate that (c) and (b) both imply (a). Similarly, (e) and
(f) both imply (d). The fact that (a) implies (b) is also immediate. For suppose
that Mc (Ψ)  ϕ and that M = (Ω, (Πj )j∈N , (Pj )j∈N , (πj )j∈N ) ∈ M(Ψ) is a
structure over a set Φ ⊂ Ψ of primitive propositions that contains the primitive
propositions that appear in ϕ. We must show that M out ϕ. Thus, we must show
that (M, ω, i) out ϕ for all ω ∈ Ω and i ∈ N . Fix ω ∈ Ω and i ∈ N , and
let Mi′ = (Ω, (Πj )j∈N , (Pj )j∈N , (πj′ )j∈N ), where πj′ = πi for all j. Thus, Mi′ is
a common-interpretation structure over Φ, where the interpretation coincides with
i’s interpretation in M . Clearly Mi′ satisfies A1–A4, so Mi′ ∈ Mc (Ψ). It is easy
to check that (M, ω, i) out ψ if and only if (Mi′ , ω, i)  ψ for all states ω ∈ Ω
′
out ϕ,
and all formulas ψ ∈ LC
n (Φ). Since Mi  ϕ, we must have that (M, ω, i) 
as desired.
To see that (a) implies (c), given a structure M = (Ω, (Πj )j∈N , (Pj )j∈N , (πj )j∈N ) ∈
M(Ψ) over some set Φ ⊂ Ψ of primitive propositions and a player j ∈ N , let Ωj
be a disjoint copy of Ω; that is, for every state ω ∈ Ω, there is a corresponding state
ωj ∈ Ωj . Let Ω′ = Ω1 ∪ . . . ∪ Ωn . Given E ⊆ Ω, let the corresponding subset
Ej ⊆ Ωj be the set {ωj : ω ∈ E}, and let E ′ be the subset of Ω′ corresponding to
E, that is, E ′ = {ωj : ω ∈ E, j ∈ N }.
6

For ease of exposition, we assume A6 even when dealing with innermost scope. Recall that A6
implies A6′ , which is actually the appropriate assumption for innermost scope.

19

Define M ′ = (Ω′ , (Π′j )j∈N , (Pj′ )j∈N , (πj′ )j∈N ), where Ω′ = Ω1 ∪ . . . ∪ Ωn
and, for all ω ∈ Ω and i, j ∈ N , we have
• Π′i (ωj ) = (Πi (ω))′ ;
• πi (ωj )(p) = πj (ω)(p) for a primitive proposition p ∈ Φ;
′ , µ′
′
′
′
• Pi′ (ωj ) = (Ω′i,ωj , Fi,ω
i,ωj ), where Ωi,ωj = Ωi,ω , Fi,ωj = {Eℓ : E ∈
j
Fi,ω , ℓ ∈ N }, µ′i,ωj (Ei ) = µi,ω (E), µ′i,ωj (Eℓ ) = 0 if ℓ 6= i.

Thus, π1 = · · · = πn , so that M ′ is a common-interpretation structure; on a
state ωj , these interpretations are all determined by πj . Also note that the support
of the probability measure µ′i,ωj is contained in Ωi , so for different players i, the
probability measures µ′i,ωj have disjoint supports. Now an easy induction on the
structure of formulas shows that(M ′ , ωj )  ψ if and only if (M, ω, j) in ψ for
′
in ϕ for all
any formula ψ ∈ LC
n (Φ). It easily follows that if M  ϕ, then M 
ϕ ∈ LC
n (Φ).
The argument that (d) implies (e) is essentially identical to the argument that (a)
implies (b); similarly, the argument that (d) implies (f) is essentially the same as the
argument that (a) implies (c). Since Mai
c (Ψ) ⊆ Mc (Ψ), (a) implies (d). To show
C
that (d) implies (a), suppose that Mai
c (Ψ)  ϕ for some formula ϕ ∈ Ln (Ψ).
Given a structure M = (Ω, (Πj )j∈N , (Pj )j∈N , π) ∈ Mc (Ψ) over a set Φ ⊂ Ψ
of primitive propositions that includes the primitive propositions that appear in
ϕ, we want to show that (M, ω, i)  ϕ for each state ω ∈ Ω and player i.
Fix ω. Recall that RN (ω) consists of the set of states N -reachable from ω. Let
M ′ = (RN (ω), (Π′j )j∈N , (Pj′ )j∈N , π ′ ), with Π′j and Pj′ the restriction of Πj and
Pj , respectively, to the states in RN (ω), be a structure over a set Φ′ of primitive
propositions, where Φ′ contains Φ and new primitive propositions that we call pi,ω
for each player i and state ω ∈ RN (ω).7 Note that there are only countably many
information sets in RN (ω), so Φ′ is countable. Define π ′ so that it agrees with
π (restricted to RN (ω)) on the propositions in Φ, and so that [[pi,ω ]]i = Πi (ω).
Thus, M ′ satisfies A5 and A6. It is easy to check that, for all ω ′ ∈ RN (ω) and
′
′
′
all formulas ψ ∈ LC
n (Φ), we have that (M, ω , i)  ψ iff (M , ω , i)  ψ. Since
′
M  ϕ, it follows that (M, ω, i)  ϕ, as desired.
The proof that (a) implies (c) shows that, starting from an arbitrary structure
M , we can construct a common-interpretation structure M ′ that is equivalent to
7
This is the one argument that needs the assumption that the set of primitive propositions can
be different in different structures in M(Ψ), and the fact that every Ψ \ Φ is countable. We have
assumed for simplicity that the propositions pi,ω are all in Ψ \ Φ, and that they can be chosen in such
a way so that Ψ \ (Φ ∪ {pi,ω : i ∈ {1, . . . , n}, ω ∈ Ω}) is countable.

20

M in the sense that the same formulas hold in both models. Note that because the
probability measures in the structure M ′ constructed in the proof of Proposition 4.1
have disjoint support, M ′ does not satisfy the CPA, even if the original structure
M does. As the next result shows, this is not an accident.
cpa (Ψ) out ϕ, Mcpa (Ψ) in
Proposition 4.2 For all formulas ϕ ∈ LC
n (Ψ), if either M
ϕ, Mcpa,ai (Ψ) out,ai ϕ, or Mcpa,ai (Ψ) in,ai ϕ, then Mcpa
c (Ψ)  ϕ. Moreover,
cpa (Ψ) out ϕ and Mcpa,ai (Ψ) out,ai ϕ. However, in
if Mcpa
(Ψ)

ϕ,
then
M
c
cpa (Ψ) in ϕ.
general, if Mcpa
c (Ψ)  ϕ, then it may not be the case that M

Proof. All the implications are straightforward, with proofs along the same lines
as that of Proposition 4.1. To prove the last claim, let p ∈ Ψ be a primitive proposition. Aumann’s agreeing to disagree result shows that Mcpa
c (Ψ)  ¬CBG (B1 p ∧
B2 ¬p), while Example 3.1 shows that Mcpa (Ψ) 6 in ¬CBG (B1 p ∧ B2 ¬p).
Proposition 4.2 depends on the fact that we are considering belief and common
belief rather than knowledge and common knowledge, where knowledge is defined
in the usual way, as truth in all possible worlds:
(M, ω, i) in Ki ϕ iff (M, ω ′ , i) in ϕ for all ω ′ ∈ Πi (ω),
with Ki the knowledge operator for player i, and where we have assumed that
ω ∈ Πi (ω) for all i ∈ N and ω ∈ Ω. Aumann’s result holds if we consider
common belief (as long as what we are agreeing about are judgments of probability and expectation). With knowledge, there are formulas that are valid with a
common interpretation that are not valid under innermost-scope semantics when
there is ambiguity.For example, Mc (Ψ)  Ki ϕ ⇒ ϕ, while it is easy to construct
a structure M with ambiguity such that (M, ω, 1) in K2 p ∧ ¬p. What is true
is that M(Ψ) in (K1 ϕ ⇒ ϕ) ∨ . . . ∨ (Kn ϕ ⇒ ϕ). This is because we have
(M, ω, i) in Ki ϕ ⇒ ϕ, so one of K1 ϕ ⇒ ϕ, . . . , Kn ϕ ⇒ ϕ must hold. As
shown in [Halpern 2009], this axiom essentially characterizes knowledge if there
is ambiguity.
As noted above, the proof of Proposition 4.1 demonstrates that, given a structure M with ambiguity and a common prior, we can construct an equivalent commoninterpretation structure M ′ with heterogeneous priors, where M and M ′ are said to
be equivalent (under innermost scope) if for every formula ψ, M in ψ if and only
if M ′ in ψ. The converse does not hold, as the next example illustrates: when
formulas are interpreted using innermost scope, there is a common-interpretation
structure with heterogeneous priors that cannot be converted into an equivalent
structure with ambiguity that satisfies the CPA.

21

Example 4.3 We construct a structure M with heterogeneous priors for which
there is no equivalent ambiguous structure that satisfies the CPA. The structure
M has three players, one primitive proposition p, and two states, ω1 and ω2 . In ω1 ,
p is true according to all players; in ω2 , the proposition is false. Player 1 knows
the state: his information partition is Π1 = {{ω1 }, {ω2 }}. The other players have
no information on the state, that is, Πi = {{ω1 , ω2 }} for i = 2, 3. Player 2 assigns probability 23 to ω1 , and player 3 assigns probability 43 to ω1 . Hence, M is a
common-interpretation structure with heterogeneous priors. We claim that there is
no equivalent structure M ′ that satisfies the CPA.
To see this, suppose that M ′ is an equivalent structure that satisfies the CPA,
with a common prior ν and a state space Ω′ . As M ′ in pr 2 (p) = 23 and M ′ in
pr 3 (p) = 34 , we must have
ν({ω ′ ∈ Ω′ : (M ′ , ω ′ , 2) in p}) = 23 ,
ν({ω ′ ∈ Ω′ : (M ′ , ω ′ , 3) in p}) = 34 .
Observe that M in B2 (p ⇔ B1 p) ∧ B3 (p ⇔ B1 p). Thus, we must have M ′ in
B2 (p ⇔ B1 p) ∧ B3 (p ⇔ B1 p). Let E = {ω ′ ∈ Ω′ : (M ′ , ω ′ , 1) in B1 p}. It
follows that we must have ν(E) = 2/3 and ν(E) = 3/4, a contradiction.
Example 4.3 demonstrates that there is no structure M ′ that is equivalent to the
structure M (defined in Example 4.3) that satisfies the CPA. In fact, as we show
now, an even stronger result holds: In any structure M ′ that is equivalent to M ,
whether it satisfies the CPA or not, players have a common interpretation.
Proposition 4.4 If a structure M ′ ∈ M(Ψ) is equivalent under innermost scope
to the structure M defined in Example 4.3, then M ′ ∈ Mc .
Proof. Note that M  p ⇔ (pr 1 (p) = 1). Hence, if a structure M ′ is equivalent to
M , we must have that M ′ in p ⇔ (pr 1 (p) = 1), that is, for all ω ∈ Ω and i ∈ N ,
(M ′ , ω, i) in p ⇔ (pr 1 (p) = 1). By a similar argument, we obtain that for every
ω ∈ Ω and i ∈ N , it must be the case that (M ′ , ω, i) in ¬p ⇔ (pr 1 (¬p) = 1).
Since the truth of a probability formula does not depend on the player under the
innermost-scope semantics, it follows that for each i, j ∈ N , the interpretations πi′
and πj′ in M ′ coincide. In other words, M ′ is a common-interpretation structure.

5

Discussion

We have defined a logic for reasoning about ambiguity, and considered the tradeoff
between having a common prior (so that everyone starts out with the same belief)
22

and having a common interpretation (so that everyone interprets all formulas the
same way). We showed that, in a precise sense, allowing different beliefs is more
general than allowing multiple interpretations. But we view that as a feature, not
a weakness, of considering ambiguity. Ambiguity can be viewed as a reason for
differences of beliefs; as such, it provides some structure to these differences.
We have not discussed axiomatizations of the logic. From Proposition 4.1 it
follows that for formulas in LC
n (Ψ), we can get the same axiomatization with respect to structures in M(Ψ) for both the out and in semantics; moreover, this
axiomatization is the same as that for the common-interpretation case. An axiomatization for this case is already given in [Fagin and Halpern 1994]. Things
get more interesting if we consider Mcpa (Ψ), the structures that satisfy the CPA.
Halpern [2002] provides an axiom that says that it cannot be common knowledge
that players disagree in expectation, and shows that it can be used to obtain a sound
and complete characterization of common-interpretation structures with a common
prior. (The axiomatization is actually given for common knowledge rather than
common belief, but a similar result holds with common belief.) By Proposition 4.2,
the axiomatization remains sound for outermost-scope semantics if we assume the
CPA. However, using Example 4.3, we can show that this is no longer the case
for the innermost-scope semantics. The set of formulas valid for innermost-scope
semantics in the class of structures satisfying the CPA is strictly between the set of
formulas valid in all structures and the set of formulas valid for outermost-scope
semantics in the class of structures satisfying the CPA. Finding an elegant complete
axiomatization remains an open problem.
Acknowledgments: Halpern’s work was supported in part by NSF grants IIS0534064, IIS-0812045, and IIS-0911036, by AFOSR grants FA9550-08-1-0438
and FA9550-09-1-0266, and by ARO grant W911NF-09-1-0281. The work of Kets
was supported in part by AFOSR grant FA9550-08-1-0389.




We propose a new definition of actual causes,
model counterfac­
tuals. We show that the definition yields a plausi­
ble and elegant account of causation that handles
well examples which have caused problems for
other definitions and resolves major difficulties in
the traditional account.
using structural equations to

1

INTRODUCTION

What does it mean that an event A actually causes event
B? This is a question that goes beyond mere philosophical
speculation. As Good [1993] argues persuasively, in many
legal settings, what needs to be established (for determining
responsibility) is not a counterfactual kind of causation, but
"cause in fact." A typical example considers two fires ad­
vancing toward a house. If fire A burned the house before
fire B, we (and many juries nationwide) would consider
fire A "the actual cause" for the damage, even supposing
the house would have definitely burned down by fire B, if
it were not for A. Actual causation is also important in arti­
ficial intelligence applications. W henever we undertake to
explain a set of events that unfold in a specific scenario, the
explanation produced must acknowledge the actual cause of
those events. The automatic generation of adequate expla­
nations, a task essential in planning, diagnosis and natural
language processing, therefore requires a formal analysis
of the concept of actual cause.
Giving a precise and useful definition of actual causality is
notoriously difficult. The philosophical literature has been
struggling with this notion since the days of Hume [ 1739].
(See [Sosa and Tooley 1993], [Hall 2002], and [Pearl2000]
for some recent discussions.) To borrow just one example
from Hall [2002], suppose a bolt lightning hits a tree and
starts a forest fire. It seems reasonable to say that the
lightning bolt is a cause of the fire. (Indeed, the description
"the lightning bolt ... starts a forest fire" can be viewed as
saying this.) But what about the oxygen in the air and the
fact that the wood was dry? Presumably, if there has not
been oxygen or the wood was wet there would not have
been a fire. Carrying this perhaps to the point of absurdity,

what about the Big Bang? This problem is relatively easy
to deal with, but there are a host of other, far more subtle,
difficulties that have been raised over the years.
Here we give a definition of actual causality based on the
language of structural equations; in a companion paper
([Halpern and Pearl 2001]; see also the full paper [Halpern
and Pearl 2000]), we give a definition of (causal) explana­
tion using the definition of causality. The use of structural
equations as a model for causa l relationships is standard
in the social sciences, and seems to go back to the work
of Sewall W right in the 1920s (see [Goldberger 1972] for
a discussion); the framework we use here is due to Pearl
[1995], and is further developed in [Galles and Pearl 1997;
Halpern 2000; Pearl 2000]. While it is hard to argue that
our definition (or any other definition, for that matter) is
the "right definition", we show that it deals well with the
difficulties that have plagued other approaches in the past,
especially those exemplified by the rather extensive com­
pendium of Hall [2002].
There has been extensive discussion about causality in the
literature, particularly in the philosophy literature. To keep
this paper to manageable length, we spend only minimal
time describing other approaches and comparing ours to
them. We refer the reader to [Hall 2002; Pearl 2000; Sosa
and Tooley 1993; Spirtes, Glymour, and Scheines 1993]
for details and criticism of the probabilistic and logical
approaches to causality in the philosophy literature. (We do
try to point out where our definition does better than perhaps
the best known approach, due to Lewis [1986, 2000] in the
course of discussing the examples.)
There has also been work in the AI literature on causal­
ity. Perhaps the closest to this are papers by Pearl and his
colleagues that use the structural-model approach. The def­
inition of causality in this paper was i n spired by an earlier
paper of Pearl's [1998] that defined actual causality in terms
of a construction called a causal beam. The causal beam
definition was later modified somewhat (see [Pearl 2000,
Chapter 10]), largely due to the considerations addressed
in this paper. The definition given here is more transparent
and handles a number of cases better (see Example 4.4).
Tian and Pearl [2000] give results on calculating the prob­
ability that A is a necessary cause of B-that is, the proba­
bility that B would not have occurred if A had not occurred.

HALPERN & PEARL

UAI 2001

Necessary causality is related to but different from actual
causality, as the definitions should make clear. Other work
(for example, [Heckerman and Shachter 1995]) focuses on
when a random variable X is the cause of a random vari­
able Y; by way of contrast, we focus on when an event
y. As we
such as X = x causes an event such as Y
shall see, many of the subtleties that arise when dealing
with events simply disappear if we look at causality at the
level of random variables. Finally, there is also a great
deal of work in AI on formal action theory (see, for exam­
ple, [Lin 1995; Sandewall1994]), which is concerned with
the proper way of incorporating causal relationships into a
knowledge base so as to guide actions. The focus of our
work is quite different; we are concerned with extracting
the actual causality relation from such a knowledge base,
coupled with a specific scenario.
=

195

The set U ofexogenous variables includes things we need to
assume so as to render all relationships deterministic (such
as whether the wood is dry, there is enough oxygen in the
air, etc.). If i1 is a setting of the exogenous variables that
makes a forest fire possible (i.e., the wood is sufficiently
dry, there is oxygen in the air, and so on) then, for example,
FF(il, L,ML) is such that F = I if L 1 or ML 1.1
=

=

Given a causal model M

= (S, :F), a (possibly empty)
vector X of variables in V, and vectors x and i1 of values
for the variables in X and U, we can define a new causal
model denoted Mx ._fi over the signature Sx
(U, V X, 'R.Iv-x ). Intuitively, this is the causal model that results
=

when the variables in X are set to x by some external
action that affects only the variables in X; we do not model
the action or its causes explicitly. Formally, Mx .... :x
=

CAUSAL MODELS: A REVIEW

2

We briefly review the basic definitions of causal models,
as defined in terms of structural equations, and the syntax
and semantics of a language for reasoning about causality.
See [Galles and Pearl 1997; Halpern 2000; Pearl 2000] for
more details, motivation, and intuition.
Causal Models: The basic picture here is that the world
is described by random variables, some of which may have
a causal influence on others. This influence is modeled by a
set of structural equations, where each equation represents
a distinct mechanism (or law) in the world, one that may be
modified (by external actions) without altering the others.
In practice, it seems useful to split the random variables into
two sets, the exogenous variables, whose values are deter­
mined by factors outside the model, and the endogenous
variables. It is these endogenous variables whose values
are described by the structural equations.

More formally, a signatureS is a tuple (U, V, 'R}, where
U is a finite set of exogenous variables, V is a finite set of
endogenous variables, and n associates with every variable
Y E U U V a nonempty set 'R(Y) of possible values for Y
(that is, the set of values over which Y ranges). A causal
model (or structural model) over signature S is a tuple
M = (S,:F), where:Fassociates with each variableX E V
a ftmction denoted Fx such that Fx : (xuEuR(U)) x
(xYEV-{x}R(Y))-+ R(X). Fx tells us the value ofX
given the values of all the other variables in U U V.
Example 2.1: Suppose that we want to reason about a forest
fire that could be caused by either lightning or a match lit
by an arsonist. Then the causal model would have the
following endogenous variables (and perhaps others):
•

F

•

L

•

for fire (F

=

1 if there is one, F

for lightning (L
otherwise)

=

ML for match lit (ML
ML 0 otherwise).
=

=

0 otherwise)

1 if lightning occurred, L

=

=

0

I if the match was lit and

(Sx, ;:x.-:x), where F� ,_fi is obtained from Fy by setting
the values of the variables in X to x.

It may seem strange that we are trying to understand causal­
ity using causal models, which clearly already encode
causal relationships. Our reasoning is not circular. Our
aim is not to reduce causation to noncausal concepts, but to
interpret questions about causes of specific events in fully
specified scenarios in terms of generic causal knowledge
such as what we obtain from the equations of physics. The
causal models encode background knowledge about the ten­
dency of certain event types to cause other event types (such
as the fact that lightning can cause forest fires). We use the
models to determine the causes and explanations of single
events, such as whether it was arson that caused the fire of
June 10, 2000, given what is known or assumed about that
particular fire.
We can describe (some salient features of) a causal model
M using a causal network. This is a graph with nodes
corresponding to the random variables in V and an edge
from a node labeled X to one labeled Y ifFy depends
on the value of X. Intuitively, variables can have a causal
effect only on their descendants in the causal network; if Y
is not a descendant of X, then a change in the value of X
has no affect on the value of Y. In this paper, we restrict
attention to what are called recursive (or acyclic) equations;
these are ones that can be described with a causal network
that is a dag. It should be clear that if M is a recursive
causal model, then there is always a unique solution to the
equations in Mx .... :r• given a setting i1 for the variables in
U (we call such a setting i1 a context).
As we shall see, there are many nontrivial decisions to be
made when choosing the structural model. The exogenous
variables to some extent encode the background situation,
that which we wish to take for granted. Other implicit back­
ground assumptions are encoded in the structural equations
themselves. Suppose that we are trying to decide whether a
lightning bolt or a match was the cause of the forest fire, and
we want to take for granted that there is sufficient oxygen
in the air and the wood is dry. We could model the dryness
of the wood by an exogenous variable D with values 0 (the
wood is wet) and I (the wood is dry).
By making D

HALPERN & PEARL

196

exogenous, its value is assumed to be given and out of the
control of the modeler. We could also take the amount of
oxygen as an exogenous variable (for example, there could
be a variable 0 with two values-0, for insufficient oxygen,
and 1, for sufficient oxygen); alternatively, we could choose
not to model oxygen explicitly at all. For example, suppose
we have, as before, a random variable ML for match lit,
and another variable WB for wood burning, with values 0
(it's not) and l (it is). The structural equation Fw 8 would
describe the dependence of WB on D and ML. By setting
Fw 8 ( 1, 1) = 1, we are saying that the wood will bum if
the match is lit and the wood is dry. Thus, the equation is
implicitly modeling our assumption that there is sufficient
oxygen for the wood to burn. If we were to explicitly model
the amount of oxygen in the air (which certainly might be
relevant if we were analyzing fires on Mount Everest), then
Fw 8 would also take values of 0 as an argument.
Besides encoding some of our implicit assumptions, the
structural equations can be viewed as encoding the causal
mechanisms at work. Changing the underlying causal
mechanism can affect what counts as a cause. Section 4
provides several examples of the importance of the choice
of random variables and the choice of causal mechanism.
It is not always straightforward to decide what the "right"
causal model is in a given situation, nor is it always obvi­
ous which of two causal models is "better" in some sense.
These may be difficult decisions and often lie at the heart
of determining actual causality in the real world. Neverthe­
less, we believe that the tools we provide here should help
in making principled decisions about those choices.
To make the definition of actual
causality precise, it is helpful to have a logic with a formal
syntax. Given a signature S = (U, V, R), a formula of
the form X = x, for X E V and x E R(X), is called a
primitive event. A basic causa/formula (overS) is one of
the form [Y, o- y1, ... , Yk o- Yk]'P where cp is a Boolean
combination of primitive events, Y,, ... , Yk,X are vari­
ables in V, with Yi, .. . , Yk are distinct, x E R(X), and
y; E R(}i). Such a formula is abbreviated as [Y o- Y1'P·
The special case where k
0 is abbreviated as cp. In­
tuitively, [Y, o- y,, . . . , Yk o- Yk]cp says that cp holds in
the counterfactual world that would arise if Y; is set toy;,
i = 1, . . . , k. A causal formula is a Boolean combination
of basic causal formulas.
Syntax and Semantics:

=

A causal formula 'lj; is true or false in a causal model, given
a context. We write (M, u) f= '1/J if '1/J is true in causal
model M given context u. (M, u) f= [Y o- Y1 (X
x)
if the variable X has value x in the (unique, since we are
dealing with recursive models) solution to the equations in
M.y_ii in context u (that is, the unique vector of values
for the exogenous variables that simultaneously satisfies
all equations Fi-ii, Z E V- Y, with the variables in U
set to ii). (M, u) F [Y ....... YJcp for an arbitrary Boolean
combination <p of formulas of the form X = x is defined
similarly. We extend the definition to arbitrary causal for­
mulas, i.e., Boolean combinations of basic causal formulas,
in the standard way.
=

UAI2001

Note that the structural equations are deterministic. We can
make sense out of probabilistic counterfactual statements,
even conditional ones (the probability that X would be 3
if Y1 were 2, given that Y is in fact 1) in this framework
(see [Balke and Pearl 1994]), by putting a probability on
the set of possible contexts. This is not necessary for our
discussion of causality, although it plays a more significant
role in the discussion of explanation.
3

THE DEFINITION OF CAUSE

With all this notation in hand, we can now give our defini­
tion of actual cause ("cause" for short). We want to make
sense out of statements of the form "event A is an actual
cause of event <p (in context ii)". As we said earlier, the
context is the background information. While this has been
left implicit in some treatments of causality, we find it use­
ful to make it explicit. The picture here is that the context
(and the structural equations) are given. Intuitively, they
encode the background knowledge. All the relevant events
are known. The only question is picking out which of them
are the causes of <p or, alternatively, testing whether a given
set of events can be considered the cause of <p.
The types of events that we allow as actual causes are
ones of the form X1
X! /1. . . . /1. Xk = Xk-that is,
conjunctions of primitive events; we typically abbreviate
this as X = x. The events that can be caused are arbitrary
Boolean combinations of primitive events.
=

Definition 3.1: (Actual cause) X =xis an actual cause of
<pin (M, it) if the following three conditions hold:

ACl. (M, it) F (X = x) /1. <p. (That is, both X= X and
<pare true in the actual world.)
AC2. There exists a partition (Z, W) of V with X � i
and some setting (.i',w') of the variables in (X, W)
such that if (M, u) F z = z* for z E i, then
(a) (M,il) 1= [ X --- .i',W--- w'j-,cp. In wards,
changing (X, w) from (x, w) to (x', w') changes
<p from true to false,
(b) (M, u) f= [X ....... x, w ....... w', i' ,_. Z*]cp for
all subsets Z' of i. In words, setting w to W1
should have no effect on <p as long as X is kept
at its current value x, even if all the variables in
an arbitrary subset of Z are set to their original
values in the context a.
AC3. X is minimal; no subset of X satisfies conditions
AC1 and AC2. Minimality ensures that only those
elements of the conjunction X = x that are essential
for changing cp cin AC2(a) are considered part of a
cause; inessential elements are pruned. I
Note that we allow X = x to be a cause of itself. While we
do not find such trivial causality terribly bothersome, it can

UAI 2001

be avoided by requiring that X
x xto be a cause ofcp.

HALPERN

=

x1\ ...,IP be consistent for

=

The core of this definition lies in AC2. Informally, the
variables in Z should be thought of as describing the "ac­
tive causal process" from X to cp. (also called "intrinsic
process" by Lewis [1986]).1 These are the variables that
mediate between X and cp. Indeed, we can define an ac­
tive causal process from X = x to 1.p as a minimal set
Z that satisfies AC2. AC2(a) is reminiscent of the tradi­
tional counterfactual criterion of Lewis [ 1986], according
to which cp should be false if it were not for X being x.
However, AC2(a) is more permissive than the traditional
criterion; it allows the dependence of cp on X to be tested
under special circumstances in which the variables W are
held constant at some setting w'. This modification of the
traditional criterion was proposed by Pearl [ 1998, 2000]
and was named structural contingency-an alteration of the
model M that involves the breakdown of some mechanisms
(possibly emerging from external action) but no change in
the context il. The need to invoke such contingencies will
be made clear in Example 3.2.
AC2(b), which has no obvious analogue in the literature,
is an attempt to counteract the "permissiveness" of AC2(a)
with regard to structural contingencies. Essentially, it en­
sures that X alone suffices to bring about the change from
1.p to -.cp; setting W to �� merely eliminates spurious side
effects that tend to mask the action of X. It captures the fact
that setting w to w' does not affect the causal process by
requiring that changingW from w to ;' has no effect on the
value of 1.p. Moreover, although the values in the variables
Z involved in the causal process may be perturbed by the
change, the perturbation has no impact on the value of 1.p.
The upshot of this requirement is that we are not at liberty to
conduct the counterfactual test of AC2(a) under an arbitrary
alteration of the model. The alteration considered must not
affect the causal process. Clearly, if the contingencies con­
sidered are limited to "freezing" variables at their actual
value, so that ( M, il) F w = W1, then AC2(b) is satisfied
automatically. However, as the examples below show, gen­
uine causation may sometimes be revealed only through a
broader class of counterfactual tests in which variables in
W are set to values that differ from their actual values. In
[Pearl 2000], a notion of contributory cause is defined as
well as actual cause. Roughly speaking, if AC2(a) holds
only with W = w' f. w, the A is a contributory cause of
B; actual causality holds only ifW
w.
=

We remark that, like the definition here, the causal beam def­
inition [Pearl 2000] tests for the existence of counterfactual
dependency in an auxiliary model of the world, modified by
a select set of structural contingencies. However, whereas
1

Recently, Lewis [2000] has abandoned attempts to define

"intrinsic process" formally. Pearl's "causal beam" [Pearl 2000,
p. 3 18] is a special kind of active causal process, in which AC2(b)

is expected to hold (with Z """ z) for all settings
necessarily the one used in (a).

w'

of W, not

&

PEARL

197

the beam criterion selects the choice of contingencies de­
pends only on the relationship a variable and its parents in
the causal diagram, the current definition selects the mod­
ifying contingencies based on the specific cause and effect
pair that is being tested. This refinement permits our def­
inition to avoid certain pitfalls (see Example 4.4) that are
associated with graphical criteria for actual causation.
AC3 is a minimality condition. Heckerman and Shachter
[ 1995] have a similar minimality requirement; Lewis [2000]
mentions the need for minimality as well. Interestingly, in
all the examples we have considered, AC3 forces the cause
to be a single conjunct of the form X
x.
Eiter and
Lukasiewicz [200 1] and, independently, Hopkins [200 1],
have recently proved that this is in fact a consequence of
our definition.
=

How reasonable are these requirements? In particular, is
it appropriate to invoke structural changes in the defini­
tion of actual causation? The following example may help
illustrate why we believe it is.
Example 3.2: Suppose that two arsonists drop lit matches
in different parts of a dry forest, and both cause trees to
start burning. Consider two scenarios. In the first, called
"disjunctive," either match by itself suffices to burn down
the whole forest. That is, even if only one match were lit,
the forest would bum down. In the second scenario, called
"conjunctive," both matches are necessary to burn down
the forest; if only one match were lit, the fire would die
down. We can describe the essential structure of these two
scenarios using a causal model with four variables:
•

an exogenous variable U which determines, among
other things, the motivation and state of mind of
the arsonists. For simplicity, assume that R(U)
{uoo, u10, uo1, u 1 I}; if U = uii, then the first arsonist
intends to start a fire iff i = 1 and the second arson­
ist intends to start a fire iff j = I. In both scenarios
U =ull·
=

•

•

endogenous variables ML1 and ML2, each either 0 or
I, where MLi = 0 if arsonist i doesn't drop the match
and ML; = I if he does, fori = 1, 2.
an endogenous variable FB for forest bums down, with
values 0 (it doesn't) and I (it does).

Both scenarios have the same causal network (see
Figure l ); they differ only in the equation for FB.
For the disjunctive scenario we have Fp 8 (u, 1, 1)
Fpa(u,O, 1) = Fpa(u, 1 , 0) = land Fpa(u,O,O) = 0
(where u E n(U)); for the conjunctive scenario we have
Fpa(u, I, l)
l and Fp8(u, 0, 0) = Fp8(u, 1, 0) =
Fp8 (u , 0, l)
0. In general, we expect that the causal
model for reasoning about forest fires would involve many
other variables; in particular, variables for other potential
causes of forest fires such lightning and unattended camp­
fires; here we focus on that part of the causal model that
involves forest fires started by arsonists. Since for causal­
ity we assume that all the relevant facts are given, we can
assume here that it is known that there were no unattended
campfires and there was no lightning, which makes it safe to
=

=

=

198

HALPERN & PEARL

ignore that portion of the causal model. Denote by M 1 and
causal models associated with the disjunctive and
conjunctive scenarios, respectively. The causal network for
the relevant portion of M 1 and M2 is described in Figure 1.

M2 the

0

ML2

FB
Figure 1: The causal network for

M1 and Mz.

Despite the differences in the underlying models, it is not
hard to show that each of ML 1 = 1 and MLz = I is a cause
of FB = 1 in both scenarios. We present the argument for

ML1
1 is a cause in M1
let Z = {ML1,FB}, so W = {ML2}. It is easy to see
that the contingency ML2 = 0 satisfies the two conditions
ML1

=

1 here. To show that

that there was a heavy rain in April and electrical storms in
the following two months; and in June the lightning took
hold. If it hadn't been for the heavy rain in April, the forest
would have caught fire in May. The question is whether
the April rains caused the forest fire. According to a naive

counterfactual analysis, they do, since if it hadn't rained,
there wouldn't have been a forest fire in June. Bennett says
"That is unacceptable. A good enough story of events and
of causation might give us reason to accept some things that
seem intuitively to be false, but no theory should persuade
us that delaying a forest's burning for a month (or indeed a
minute) is causing a forest fire."

u

ML1

=

in AC2. AC2(a) is satisfied because, in the absence of the
second arsonist (ML2 = 0), the first arsonist is necessary
and sufficient for the fire to occur (FB = 1). AC2(b)
is satisfied because, if the first match is lit (ML1 = 1) the

In our framework, as we now show, it is indeed false to say
that the April rains caused the fire, but they were a cause of
there being a fire in June, as opposed to May. This seems to
us intuitively right. To capture the situation, it suffices to use
a simple model with three endogenous random variables:
•

•

ML 1

=

of FB
1 in Mz. (Again, taking Z
W :::::: {ML2} works.)
=

I is also a cause

=

{ML,,FB}

and

This example also illustrates the need for the minimality
condition AC3. !f lighting a match qualifies as the cause of
firethen lighting a match and sneezing would also pass the

tests of AC1 and AC2 and awkwardly qualify as the cause
of fire. Minimality serves here to strip "sneezing" and other
irrelevant, over-specific details from the cause. I
This is a good place to illustrate the need for structural con­

tingencies in the analysis of actual causation. The reason
we consider ML1 = 1 to be a cause of FB = 1 in M1 is
that if ML2 had been 0, rather than 1, FB would depend on
MLt. In words, we imagine a situation in which the second

match is not lit, and we then reason counterfactually that
the forest would not have burned down if it were not for the
first match.

4

EXAMPLES

In this section we show how our definition of actual causal­
ity handles some examples that have caused problems for
other definitions. The full paper has further examples.

Example 4.1: The first example is due to Bennett (and
appears in [Sosa and Tooley 1993, pp. 222-223]). Suppose

"April showers", with two values-{) standing
for did not rain heavily in April and I standing for

ES

for "electric storms", with four possible values:

(0, 0) (no electric storms in either May or June), (1,0)
(electric storms in May but not June), (0,1) (storms
in June but not May), and (1,1) (storms in April and

May);

=

order to reveal the latent dependence of FB on ML 1· Such a
setting constitutes a structural change in the original model,
since it involves the removal of some structural equations.)

AS for

rained heavily in April;

contingency ML2 = 0 does not prevent the fire from burning
1 in Mt.
the forest. Thus, ML1 = 1 is a cause of FB
(Note that we needed to set ML2 to 0, contrary to facts, in

A similar argument shows that

UAI2001

•

andF for "fire", with three possible values: 0 (no fire
at all), 1 (fire

in May), or 2 (fire in June).

We do not describe the context explicitly, either here or
in the other examples. Assume its value ii is such that it
ensures that there is a shower in April, there are electric
storms in both May and June, there is sufficient oxygen,
there are no other potential causes of fire (like dropped
matches), no other inhibitors of fire (alert campers setting
up a bucket brigade), and so on. That is, we choose i1 so as
to allow us to focus on the issue at hand and to ensure that
the right things happened (there was both fire and rain).
We will not bother writing out the details of the structural
equations-they should be obvious, given the story (at least,
for the context il); this is also the case for all the other
examples in this section. The causal network is simple:
there are edges from AS to F and from ES to F. It is easy
to check that each of the following hold.
•

AS
W

= 1 is a cause ofthe June fire (F = 2) (taking
=

{ES}

2VF=1).

•

and

Z

=

{AS, F})

but not of fire (F

=

(1, I) is a cause of both F = 2 and (F
1 VF = 2). Having electric storms in both May and

ES

=

June caused there to be a fire.
•

I 1\ ES = ( l, 1) is not a cause of F = 2, because
it violates the minimality requirement of AC3; each
conjunct alone is a cause ofF = 2. Similarly, AS =
I 1\ ES = ( 1, 1) is not a cause of ( F = 1 VF
2).

AS =

=

UA12001

HALPERN

The distinction between April showers being a cause of the
fire (which they are not, according to our analysis) and April
showers being a cause of a fire in June (which they are) is
one that seems not to have been made in the discussion of
this problem (cf. [Lewis 2000]); nevertheless, it seems to
us an important distinction. I
Although we did not describe the context explicitly in Ex­
ample 4.1, it still played a crucial role. If the presence of
oxygen is relevant then we must take this factor out of the
context and introduce it as an explicit endogenous variables.
Doing so can affect the causality picture. The next example
already shows the importance of choosing an appropriate
granularity in modeling the causal process and its structure.
4.2: The following story from [Hall 2002] is
an example of preemption, where there are two potential
Example

causes of an event, one of which preempts the other. An
adequate definition of causality must deal with preemption
in all of its guises.
Suzy and Billy both pick up rocks and throw them
at a bottle. Suzy's rock gets there first, shattering
the bottle. Since both throws are perfectly accu­
rate, Billy's would have shattered the bottle had
it not been preempted by Suzy's throw.

Common sense suggests that Suzy's throw is the cause of the
shattering, but Billy's is not. This holds in our framework
too, but only if we model the story appropriately. Consider
first a coarse causal model, with three endogenous variables:
•

ST for "Suzy throws", with values 0 (Suzy does not

throw) and 1 (she does);

&

PEARL

199

cheating; the answer is almost programmed into the model
by invoking the relation "as a result of', which requires the
identification of the actual cause.
A more useful choice is to add two new random variables
to the model:
•

•

BHfor "Billy's rock hits the (intact) bottle", with val­
ues 0 (it doesn't) and 1 (it does); and
SHfor "Suzy's rock hits the bottle", again with values
0 and 1.

With this addition, we can go back to BS being two-valued.
In this model, we have the causal network shown in Figure
2, with the arrow SH -+ BH being inhibitory; BH BT 1\
·SH (that is, BH 1 iff BT 1 andSH 0). Note that, to
simplify the presentation, we have omitted the exogenous
variables from the causal network in Figure 2. In addition,
we have only given the arrows for the particular context of
interest, where Suzy throws first. In a context where Billy
throws first, the arrow would go from BH to SH rather than
going fromSH to BH, as it does in the figure.
=

=

=

=

Figure 2: The rock-throwing example.
Now it is the case that ST = 1 is a cause of BS == 1. To
satisfy AC2, we choose W = { BT} and w' = 0 and note
that, because BT is set to 0, BS will track the setting of
ST. Also note that BT 1 is not a cause of BS = 1; there
is no partition Z U W that satisfies AC2. Attempting the
symmetric choice W = {BT} and w' = 0 would violate
AC2(b) (with Z'
{ BH} ), because cp becomes false when
we set ST 0 and restore BH to its current value ofO.
=

•

BT for "Billy throws", with values 0 (he doesn't) and

1 (he does);

•

BS for "bottle shatters', with values 0 (it doesn't shat­
ter) and 1 (it does).

Again, we have a simple causal network, with edges from
both ST and BT to BS. In this simple causal network,
BT and ST play absolutely symmetric roles, with BS
ST V BT, and there is nothing to distinguish one from the
other. Not surprisingly, both Billy's throw and Suzy's throw
are classified as causes of the bottle shattering.
=

The trouble with this model is that it cannot distinguish
the case where both rocks hit the bottle simultaneously (in
which case it would be reasonable to say that both ST = 1
and BT 1 are causes of BS = 1) from the case where
Suzy's rock hits first. The model has to be refined to express
this distinction. One way is to invoke a dynamic model
[Pearl 2000, p. 326). Another way to gain expressiveness
is to allow BS to be three valued, with values 0 (the bottle
doesn't shatter), 1 (it shatters as a result of being hit by
Suzy's rock), and 2 (it shatters as a result of being hit by
Billy's rock). We leave it to the reader to check thatST = 1
is a cause of BS = 1, but BT = 1 is not (if Suzy hadn't
thrown but Billy had, then we would have BS 2). Thus,
to some extent, this solves our problem. But it borders on
=

=

=

==

This example illustrates the need for invoking subsets of Z
in AC2(b). I
Example 4.3: Is causality transitive? Consider the fol­
lowing story, again taken from (an early version of) [Hall

2002]:

Billy contracts a serious but nonfatal disease. He
is hospitalized and treated on Monday, so is fine
Tuesdsay morning. Had Monday's doctor for­
gotten to treat Billy, Tuesday's doctor would have
treated him, and he would have been fine Tues­
day afternoon. But there is a twist: one dose of
medication is harmless, but two doses are lethal.
The causal model for this story is straightforward. There
are three random variables: MT for Monday's treatment (1
if Billy was treated Monday; 0 otherwise), TT for Tuesday's
treatment ( 1 if Billy was treated Tuesday; 0 otherwise), and
BMC for Billy's medical condition (0 if Billy is fine both
Tuesday morning and Tuesday afternoon; 1 if Billy is sick

HALPERN

200

Tuesday morning, fine Tuesday afternoon; 2 if Billy is sick
both Tuesday morning and afternoon; 3 if Bill is fine Tues­
day morning and dead Tuesday afternoon). In the causal
network corresponding to this causal model, shown in Fig­
ure 3, there is an edge from MT to TT, since whether the
Tuesday treatment occurs depends on whether the Monday
treatment occurs, and edges from both MT and 1T to BMC,
since Billy's medical condition depends on both treatments.

&

PEARL

UAI2001

then in the context where V1 = V2 = I, it is easy to see that
each of Vi = I and V2 = I is a cause of P = l . However,
suppose we now assume that there is a voting machine that
tabulates the votes. Let M represent the total number of
votes recorded by the machine. Clearly M
V1 + V2 and
P
1 iff M � 1. The following causal network repre­
sents this more refined version of the story. In this more
=

=

MT-TT

\I
BMC

Figure 3: Billy's medical condition.
In this causal model, it is true that MT = 1 is a cause of

BMC = 0, as we would expect-because Billy is treated
Monday, he is not treated on Tuesday morning, and thus
recovers Tuesday afternoon. 2 MT = I is also a cause of
1T = 0, as we would expect, and 1T = 0 is a cause of
Billy's being alive (BMC = 0 v BMC = I v BMC = 2).
However, MT = I is not a cause of Billy's being alive. It
fails condition AC2(a): setting MT = 0 still leads to Billy's
being alive (with W = 0). Note that it would not help to
takeW = { IT} For ifTT= O,thenBilly is alive no matter
what MT is, while if 1T = 1, then Billy is dead when MT
has its original value, so AC2(b) is violated (with Z' = 0).
.

This shows that causality is not transitive, according to our
definitions. Although MT = I is a cause of 1T = 0 and
1T = 0 is a cause of BMC = 0 v BMC
1 V BMC
2,
MT= I is not a cause ofBMC = OVBMC = 1VBMC = 2.
Nor is causality closed under right weakening: MT = 1 is
a cause of BMC = 0, which logically implies BMC =
0 V BMC = 1 V BMC = 2, which is not caused by MT = 1.
=

=

Lewis [I986, 2000] insists that causality is transitive, partly
to be able to deal with preemption [Lewis 1986].
Our
approach handles preemption without needing to invoke
transitivity, which, as Lewis's own examples show, leads to
counterintuitive conclusions. I
Clause AC2(b) in the definition is complicated by the need
to check that no change in the value of the variables in Z
can affect the value of <.p. In all the previous examples,
Z = z* for each variable Z E i. Could we not just require
this? The following example shows that we cannot.
Example 4.4: Imagine that a vote takes place. For sim­
plicity, two people vote. The measure is passed if at least
one of them votes in favor. In fact, both of them vote in
favor, and the measure passes. This version of the story is
almost identical to Example 3.2. If we use V1 and Vz to
denote how the voters vote (V; = 0 if voter i votes against
and Vi = I if she votes in favor) and P to denote whether
the measure passes (P
I if it passes, P = 0 if it doesn't),
=

2
Lewis's [1986]
revised · criterion of counterfactual­
dependence-chain fails in this example; BMC does not depend
on either MT or TT in the context given.

Figure 4: An example showing the need for AC2(b).
refined scenario, vl = 1 and v2 = 1 are still both causes
of P = l . Consider V1 = I. Take Z = {Vi, M, P} and
W = V:t. Much like the simpler version of the story, if
we choose the contingency V2
0, then Pis counterfac­
tually dependent on V1, so AC2(a) holds. To check if this
contingency satisfies AC2(b), we set Vi to 1 (their original
value) and check that setting V2 to 0 does not change the
value of P. This is indeed the case. Although M becomes
1, not 2 as it is when V1 = Vz
1, nevertheless, P = 1
continues to hold, so AC2(b) is satisfied and V1 = 1 is a
cause of P = 1. However, if we had insisted in AC2(b)
that (M, u) f= [X of- x,W of- w']Z = z* for all variables
Z E Z (which in this case means that M would have to
retain its original value of2 when V1 = 1 and V2 = 0), then
neither V1 = 1 nor V2 = 1 would be a cause of P = l. I
=

=

We remark that this example is not handled correctly by
Pearl's causal beam definition. According to the causal
beam definition, there is no cause for P = 1! It can be
shown if X = x is an actual (or contributory) cause of
Y = y according to the causal beam definition given in
[Pearl 2000], then it is an actual cause according to the
definition here. As Example 4.4 shows, the converse is not
necessarily true.
Example 4.5: This example concerns what Hall calls the
distinction between causation and determination. Again,
we quote Hall [2002]:

You are standing at a switch in the railroad tracks.
Here comes the train: If you flip the switch, you'll
send the train down the left-hand track; if you
leave it where it is, the train will follow the right­
hand track. Either way, the train will arrive at
the same point, since the tracks reconverge up
ahead. Your action is not among the causes of
this arrival; it merely helps to determine how the
arrival is brought about (namely, via the left-hand
track, or via the right-hand track).

Again, our causal model gets this right. Suppose we have
three random variables:

UAI 2001

•

•

•

HALPERN

F for "flip", with values 0 (you don't flip the switch)
and 1 (you do);
T for "track", with values 0 (the train goes on the
left-hand track) and I (it goes on the right-hand track);

A for "arrival", with values 0 (the train does not arrive
at the point of reconvergence) and I (it does).

Now it is easy to see that flipping the switch (F = I) does
cause the train to go down the left-hand track (T = 0), but
does not cause it to arrive (A = I), thanks to AC2(a}-­
whether or not the switch is flipped, the train arrives.
However, our proposal goes one step beyond this simple
picture. Suppose that we model the tracks using two vari­
ables:
•

•

LT for "left-track", with values 1 (the train goes on the
left-hand track) and 0 (it does not); and
RT for "right-track", with values 1 (the train goes on
the right-hand track) and 0 (it does not).

The resulting causal diagram is shown in Figure 5 ; it is iso­
morphic to a class of problems Pearl [2000] calls "switch­
ing causation" . Lo and behold, this representation classifies
F = I as a cause of A, which, at first sight, may seem coun­
terintuitive: Can a change in representation turn a non-cause
into a cause?

& PEARL

201

this possibility that should enter our mind whenever we de­
cide to designate each track as a separate mechanism (i.e.,
equation) in the model and, keeping this contingency in
mind, it should not be too odd to name the switch position
a cause of the train arrival (or non-arrival). I
We conclude this section with an example that shows a
potential problem for our definition, and suggest a solution.
Example 4.6: Fred has his finger severed by a machine at
the factory (FS = I). Fortunately, Fred is covered by a
health plan. He is rushed to the hospital, where his finger is
sewn back on. A month later, the finger is fully functional
(FF
1). In this story, we would not want to say that
FS 1 is a cause ofFF 1 and, indeed, according to our
definition, it is not, since FF = 1 whether or not FS = 1
(in all contingencies satisfying AC2(b)).
=

=

=

However, suppose we introduce a new element to the story,
representing a nonactual structural contingency: Larry the
Loanshark may be waiting outside the factory with the in­
tention of cutting off Fred's finger, as a warning to him to
repay his loan quickly. Let LL represent whether or not
Larry is waiting and let LC represent whether Larry cuts
of the Fred's finger. If Larry cuts off Fred's finger, he will
throw it away, so Fred will not be able to get it sewn back
on. In the actual situation, LL = LC = 0; Larry is not
waiting and Larry does not cut off Fred's finger. So, intu­
itively, there seems to be no harm in adding this fanciful
element to the story. Or is there? Suppose that, if Fred's
finger is cut off in the factory, then Larry will not be able
to cut off the finger himself (since Fred will be rushed off
1 becomes a cause of FF 1.
to the hospital). Now FS
1, if FS 0
For in the structural contingency where LL
then FF
0 (Larry will cut off Fred's finger and throw it
away, so it will not become functional again). Moreover,
if FS = I, then LC
0 and FF I, just as in the actual
situation.4 I
=

=

=

=

=

A

=

Figure 5: Flipping the switch.
It can and it should! The change to a two-variable model
is not merely syntactic, but represents a profound change
in the story. The two-variable model depicts the tracks as
two independent mechanisms, thus allowing one track to be
set (by action or mishap) to false (or true) without affecting
the other. Specifically, this permits the disastrous mishap
of flipping the switch while the left track is malfunctioning.
Such abnormal eventualities are imaginable and expressible
in the two-variable model, but not in the one-variable model.
The potential for such eventualities is precisely what renders
F = I a cause ofthe A in the model of Figure 5.3
Is flipping the switch a legitimate cause of the train's ar­
rival? Not in ideal situations, where all mechanisms work
as specified. But this is not what causality (and causal
modeling) are all about. Causal models earn their value in
abnormal circumstances, created by structural contingen­
cies, such as the possibi lity of a malfunctioning track. It is
3

This can be seen by noting that condition AC2 is satisfied by

the partition

Z

=

{ F, LT, A } ,

W

=

{RT}, and choosing

the setting R T = 0. The event RT = 0 conflicts with F
normal situations.

=

w'

as

0 under

=

This example seems somewhat disconcerting. Why should
the Loanshark to the
story affect (indeed, result in) the accident being a cause of
the finger being functional one month later? While it is true
that the accident would be judged a cause of Fred's good
fortune by anyone who knew of Larry's vicious plan (many
underworld figures owe their lives to "accidents" of this
sort), the question remains how to distinguish genuine plans
that just happened not to materialize from sheer fanciful
scenarios that have no basis in reality. To some extent,
the answer here is the same as the answer to essentially all
the other concerns we have raised: it is a modeling issue.
If we know of Larry's plan, or it seems like a reasonable
possibility, we should add it to the model (in which case the
accident is a cause ofthe finger being functional); otherwise
we shouldn't.

adding a fanciful scenario like Larry

But this answer makes the question of how reasonable a
possibility Larry's plan are into an aU-or-nothing decision.
One solution to this problem is to extend our notion of
4We thank Eric Hiddleston for bringing this issue and this

example to our attention.

202

HALPERN & PEARL

causal model somewhat, so as to be able to capture more
directly the intuition that the Larry the Loanshark scenario
is indeed rather fanciful. There a number of ways of doing
this; we choose one based on Spohn's notion of a ranking
function (or ordinal conditionalfunction) [Spohn 1 988]. A
ranking K on a space W is a function mapping subsets of W
to IN* = IN U { oo} such that K(W)
0, K(0) = oo, and
K(A) = minw E A (K( { w} )) . Intuitively, an ordinal ranking
assigns a degree of surprise to each subset of worlds in W,
where 0 means unsurprising and higher numbers denote
greater surprise. Let a world be a complete setting of the
exogenous variables. Suppose that, for each context il, we
have a ranking /"£;; on the set of worlds. The unique setting
of the exogenous variables that holds in context il is given
rank 0 by /"£;;; other worlds are assigned ranks according to
how "fanciful" they are, given context il. Presumably, in
Example 4.6, an appropriate ranking K would give a world
where Larry is waiting to cut off Fred's finger (i.e., where
LL = 1 ) a rather high "' ranking, to indicate that it is rather
fanciful. We can then modify the definition of causality so
that we can talk about X = x being an actual cause of r.p in
(M, u) at rank k. The definition is a slight modification of
condition AC2 in Definition 3 . 1 so the contingency ( :2 , ,l;')
must hold in a world of rank at most k; we omit the formal
details here. We then can restrict actual causality so that
the structural contingencies involved have at most a certain
rank. This is one way of ignoring fanciful scenarios.
=

5

DISCUSSION

We have presented a principled way of determining actual
causes from causal knowledge. The essential principles
of our account include using structural equations to model
causal mechanisms; using uniform counterfactual notation
to encode and distinguish facts, actions, outcomes, and
contingencies; using structural contingencies to uncover
causal dependencies; and careful screening of these con­
tingencies to avoid tampering with the causal processes to
be uncovered. While our definitions has some unsatisfy­
ing features (see Example 4.6), we hope that the examples
presented here illustrate how well it deals with many of the
problematic cases found in the literature. As the examples
have shown, much depends on choosing the "right" set of
variables with which to model a situation, which ones to
make exogenous, and which to make endogenous. While
the examples have suggested some heuristics for making
appropriate choices, we do not have a general theory for
how to make these choices. We view this as an important
direction for future research.
Acknowledgments
We thank Christopher Hitchcock for many useful comments
on earlier versions of the paper, Zoltan Szabo for stim­
ulating discussions, and Eric Hiddleston for pointing out
Example 4.6. Halpern's work supported in part by NSF un­
der grants IRI-96-25901 and IIS-0090 145 and ONR under
grant N0001400 1 034 l . Pearl's work supported in part by
grants from NSF, ONR, AFOSR, and MICRO.

UA I 2001


