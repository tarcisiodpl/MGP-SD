
We study contextual bandits with ancillary constraints on resources, which are common in realworld applications such as choosing ads or dynamic pricing of items. We design the first algorithm
for solving these problems that handles constrained resources other than time, and improves over a
trivial reduction to the non-contextual case. We consider very general settings for both contextual
bandits (arbitrary policy sets, Dudik et al. (2011)) and bandits with resource constraints (bandits
with knapsacks, Badanidiyuru et al. (2013a)), and prove a regret guarantee with near-optimal statistical properties.

1. Introduction
Contextual bandits is a machine learning framework in which an algorithm makes sequential decisions according to the following protocol: in each round, a context arrives, then the algorithm
chooses an action from the fixed and known set of possible actions, and then the reward for this action is revealed; the reward may depend on the context, and can vary over time. Contextual bandits
is one of the prominent directions in the literature on online learning with exploration-exploitation
tradeoff; many problems in this space are studied under the name multi-armed bandits.
A canonical example of contextual bandit learning is choosing ads for a search engine. Here,
the goal is to choose the most profitable ad to display to a given user based on a search query and
the available information about this user, and optimize the ad selection over time based on user
feedback such as clicks. This description leaves out many important details, one of which is that
every ad is associated with a budget which constrains the maximum amount of revenue which that
ad can generate. In fact, this issue is so important that in some formulations it is the primary problem
(e.g., Devanur and Vazirani, 2004).
The optimal solution with budget constraints fundamentally differs from the optimal solution
without constraints. As an example, suppose that one ad has a high expected revenue but a small
budget such that it can only be clicked on once. Should this ad be used immediately? From all
∗

This is the full version of a paper in the 26th Conf. on Learning Theory (COLT), 2014. The present version includes
a correction for Theorem 3, a corollary for contextual dynamic pricing with discretization, and an updated discussion
of related work.
The main results have been obtained while A. Badanidiyuru was a research intern at Microsoft Research New York
City. A. Badanidiyuru was also partially supported by NSF grant AF-0910940 of Robert Kleinberg.

c A. Badanidiyuru, J. Langford & A. Slivkins.

BADANIDIYURU L ANGFORD S LIVKINS

reasonable perspectives, the answer is “no”. From the user’s or advertiser’s perspective, we prefer
that this ad be displayed for the user with the strongest interest rather than for a user who simply
has more interest than in other options. From a platform’s viewpoint, it is better to have more ads
in the system, since they effectively increases the price paid in a second price auction. And from
everyone’s viewpoint, it is simply odd to burn out the budget of an ad as soon as it is available.
Instead, a small budget should be parceled out over time.
To address these issues, we consider a generalization of contextual bandits in which there are
one or several resources that are consumed by the algorithm. This formulation has many natural applications. Dynamic ad allocation follows the ad example described above: here, resources
correspond to advertisers’ budgets. In dynamic pricing, a store with a limited supply of items to
sell can make customized offers to customers. In dynamic procurement, a contractor with a batch
of jobs and a limited budget can experiment with prices offered to the workers, e.g. workers in a
crowdsourcing market. The above applications have been studied on its own, but never in models
that combine contexts and limited resources.
We obtain the first known algorithm for contextual bandits with resource constraints (other than
time) that improves over a trivial reduction to the non-contextual version of the problem. As such,
we merge two lines of work on multi-armed bandits: contextual bandits and bandits with resource
constraints. While significant progress has been achieved in each of the two lines of work (in
particular, optimal solutions have been worked out for very general models), the specific approaches
break down when applied to our model.
Our model. We define resourceful contextual bandits (in short: RCB), a common generalization
of two general models for contextual bandits and bandits with resource constraints: respectively,
contextual bandits with arbitrary policy sets (e.g., Langford and Zhang, 2007; Dudik et al., 2011)
and bandits with knapsacks (Badanidiyuru et al., 2013a).
There are several resources that are consumed by the algorithm, with a separate budget constraint on each. (Time is one of these resources, with deterministic consumption of 1 for every
action.) In each round, the algorithm receives a reward and consumes some amount of each resource, in a manner that depends on the context and the chosen action, and may be randomized.
We consider a stationary environment: in each round, the context and the mapping from actions to
rewards and resource consumption is sampled independently from a fixed joint distribution, called
the outcome distribution. Rewards and consumption of various resources can be correlated in an
arbitrary way. The algorithm stops as soon as any constraint is violated. Initially the algorithm is
given no information about the outcome distribution (except the distribution of context arrivals). In
particular, expected rewards and resource consumptions are not known.
An algorithm is given a finite set Π of policies: mappings from contexts to actions. We compete
against algorithms that must commit to some policy in Π before each round. Our benchmark is a
hypothetical algorithm that knows the outcome distribution and makes optimal decisions given this
knowledge and the restriction to policies in Π. The benchmark’s expected total reward is denoted
OPT(Π). Regret of an algorithm is defined as OPT(Π) minus the algorithm’s expected total reward.
For normalization, per-round rewards and resource consumptions lie in [0, 1]. We assume that
the distribution of context arrivals is known to the algorithm.
Discussion of the model. Allowing stochastic resource consumptions and arbitrary correlations
between per-round rewards and per-round resource consumptions is essential: this is why our model

2

R ESOURCEFUL C ONTEXTUAL BANDITS

subsumes diverse applications such as the ones discussed above,1 and many extensions thereof.
Further discussion of the application domains can be found in Appendix A.
Intuitively, the policy set Π consists of all policies that can possibly be learned by a given
learning method, such as linear estimation or decision trees. Restricting to Π allows meaningful
performance guarantees even if competing against all possible policies is intractable. The latter is
common in real-life applications, as the set of possible contexts can be very large.
Our benchmark can change policies from one round to another without restriction. As we prove,
this is essentially equivalent in power to the best fixed distribution over policies. However, the best
fixed policy may perform substantially worse.2
Our stopping condition corresponds to hard constraints: an advertiser cannot exceed his budget,
a store cannot sell more items than it has in stock, etc. An alternative stopping condition is to restrict
the algorithm to actions that cannot possibly violate any constraint if chosen in the current round,
and stop if there is no such action. This alternative is essentially equivalent to the original version.3
Moreover, we can w.l.o.g. allow our benchmark to use this alternative.
Our contributions: main algorithm. We design an algorithm, called MixtureElimination, and
prove the following guarantee on its regret.
Theorem 1 For all RCB problems with K actions, d resources, time horizon T , and for all policy
sets Π. Algorithm MixtureElimination achieves expected total reward
p
REW ≥ OPT(Π) − O 1 + B1 OPT(Π)
dKT log (dKT |Π|),
(1)
where B = mini Bi is the smallest of the resource constraints B1 , . . . , Bd .

This regret guarantee is optimal in several regimes. First, we achieve an optimal square-root
“scaling” of regret: if all constraints are scaled by the same parameter α > 0, then√regret scales
√
= T (i.e., there are no constraints), we recover the optimal Õ( KT ) regret.
as α. Second, if B √
Third, we achieve Õ( KT ) regret for the important regime when OPT(Π) and B are at
√ least a constant fraction of T . In fact, Badanidiyuru et al. (2013a) provide a complimentary Ω( KT ) lower
bound forpthis regime, which holds in a very strong sense: for any given tuple (K, B, OPT(Π), T ).
The log |Π| term in Theorem 1 is unavoidable (Dudik et al., 2011). The dependence on the
minimum of the constraints (rather than, say, the maximum or some weighted combination thereof)
is also unavoidable (Badanidiyuru et al., 2013a). For strongest results, one can rescale per-round
rewards and per-round consumption of each resource so that they can be as high as 1.4
Note that the regret bound in Theorem 1 does not depend on the number of contexts, only on
the number of policies in Π. In particular, it tolerates infinitely many contexts. On the other hand,
if the set X of contexts is not too large, we can also obtain a regret bound with respect to the best
policy among all possible policies. Formally, take Π = {all policies} and observe that |Π| ≤ K |X| .
Further, Theorem 1 extends to policy sets Π that consist of randomized policies: mappings from
contexts to distributions over actions. This may significantly reduce |Π|, as a given randomized
1. For example, in dynamic pricing the algorithm receives a reward and loses an item only if the item is sold.
2. The expected total reward of the best fixed policy can be half as large as that of the best distribution. This holds
for several different domains including dynamic pricing / procurement, even without contexts (Badanidiyuru et al.,
2013a). Note that without resource constraints, the two benchmarks are equivalent.
3. Each budget constraint changes by at most one, which does not affect our regret bounds in any significant way.
1
, then multiplying it by 10 would
4. E.g., if per-round consumption of some resource i is deterministically at most 10
effectively increase the corresponding budget Bi by a factor of 10, and hence can only improve the regret bound.

3

BADANIDIYURU L ANGFORD S LIVKINS

policy might not be representable as a distribution over a small number of deterministic policies.5
We assume deterministic policies in the rest of the paper.
Computational issues. This paper is focused on proving the existence of solutions to this problem, and the mathematical properties of such a solution. The algorithm is specified as a mathematically well-defined mapping from histories to actions; we do not provide a computationally efficient
implementation. Such “information-theoretical” results are common for the first solutions to new,
broad problem formulations (e.g. Kleinberg et al., 2008; Kleinberg and Slivkins, 2010; Dudik et al.,
2011). In√particular, in the prior work for RCB without resource constraints there exists an algorithm
with Õ( KT ) regret (Auer et al., 2002; Dudik et al., 2011), but for all known computationally
efficient algorithms regret scales with T as T 2/3 (Langford and Zhang, 2007).
Our contributions: partial lower bound. We derive a√partial lower bound: we prove that RCB
is essentially hopeless for the regime OPT(Π) ≤ B ≤ KT /2. The condition OPT(Π) ≤ B is
satisfied, for example, in dynamic pricing with limited supply.
Theorem 2 Any algorithm for RCB√incurs regret Ω(OPT(Π)) in the worst case over all problem
instances such that OPT(Π) ≤ B ≤ KT /2 (using the notation from Theorem 1).
The above lower bound is specific to the general (“contextual”) case of RCB. In fact, it points
to a stark difference between RCB and the non-contextual version: in the latter, o(OPT) regret is
achievable as long as (for example) B ≥ log T (Badanidiyuru et al., 2013a).
While Theorem 2 is concerned
√ with the regime of small B, note that in the “opposite” regime
in Theorem 1 is quite low: it can be
of very large B,√namely B ≫ KT , the regret achieved
√
expressed as Õ( KT + ǫ · OPT(Π)), where B = 1ǫ KT .
Our contributions: discretization. In some applications of RCB, such as dynamic pricing and
dynamic procurement, the action space is a continuous interval of prices. Theorem 1 usefully applies
whenever the policy set Π is chosen so that the number of distinct actions used by policies in Π is
finite and small compared to T . (Because one can w.l.o.g. remove all other actions.) However, one
also needs to handle problem instances in which the policies in Π use prohibitively large or infinite
number of actions.
We consider a paradigmatic example of RCB with an infinite action space: contextual dynamic
pricing with a single product and prices in the [0, 1] interval. We derive a corollary of Theorem 1
that applies to an arbitrary finite policy set Π. To the best of our knowledge, this is the first result
on contextual dynamic pricing with infinite price set.
We use discretization: we reduce the original problem to one in which actions (i.e., prices) are
multiples of some carefully chosen ǫ > 0. Our approach proceeds as follows. For each ǫ > 0 and
each policy π let πǫ be a policy that takes the price computed by π and rounds it down to the nearest
multiple of ǫ. We define the “discretized” policy set Πǫ = {πǫ : π ∈ Π}. We use Theorem 1 to
obtain a regret bound relative to Πǫ . Here the ǫ controls the tradeoff between the number of actions
in that regret bound and the “discretization error” of Πǫ . Then we optimize the choice of ǫ to obtain
5. We can reduce RCB with randomized policies to RCB with deterministic policies simply by replacing each context x
with a vector (a(x, π) : π ∈ Π) such that a(x, π) = π(x), and encoding the randomization in policies through the
randomization in the context arrivals. While this blows up the context space, it does not affect our regret bound.

4

R ESOURCEFUL C ONTEXTUAL BANDITS

the regret bound relative to Π. The technical difficulty here is to bound the discretization error in
terms of ǫ; for this purpose we assume Lipschitz demands.6
Theorem 3 Consider contextual dynamic pricing with a single product and prices in [0, 1]. Use
standard notation: supply B, policy set Π and time horizon T . Assume Lipschitz demands with
Lipschitz constant L. Then algorithm MixtureElimination with discretized policy set Πǫ (defined
as above) and ǫ suitably chosen as a function of (B, T, L, |Π|) achieves expected total reward
REW ≥ OPT(Π) − O(T 3/5 B 1/5 ) · (L log (T |Π|))1/5

(2)

This regret bound is most interesting for the important regime B ≥ Ω(T ) (studied, for example,
in Besbes and Zeevi (2009, 2011); Wang et al. (2014)). Then regret is O(T 4/5 ) (L log (T |Π|))1/5 .
It is unclear whether this regret bound is optimal. When specialized to the non-contextual
case, it is not optimal. The optimal regret is then O(B 2/3 ), even for an arbitrary budget B and
even without the Lipscitz assumption (Babaioff et al., 2015). Extending the discretization approach
beyond dynamic pricing with a single product is problematic even without contexts, see Section 10
for further discussion.
Discussion: main challenges in RCB. The central issue in bandit problems is the tradeoff between
exploration: acquiring new information, and exploitation: making seemingly optimal decisions
based on this information. In this paper, we resolve the explore-exploit tradeoff in the presence of
contexts and resource constraints. Each of the three components (explore-exploit tradeoff, contexts,
and resource constraints) presents its own challenges, and we need to deal with all these challenges
simultaneously. Below we describe these individual challenges one by one.
A well-known naive solution for explore-exploit tradeoff, which we call pre-determined exploration, decides in advance to allocate some rounds to exploration, and the remaining rounds to
exploitation. The decisions in the exploration rounds do not depend on the observations, whereas
the observations from the exploitation rounds do not impact future decisions. While this approach is
simple and broadly applicable, it is typically inferior to more advanced solutions based on adaptive
exploration – adapting the exploration schedule to the observations, so that many or all rounds serve
both exploration and exploitation.7 Thus, the general challenge in most explore-exploit settings is
to design an appropriate adaptive exploration algorithm.
Resource constraints are difficult to handle for the following three reasons. First, an algorithm’s
ability to exploit is constrained by resource consumption for the purpose of exploration; the latter
is stochastic and therefore difficult to predict in advance. Second, the expected per-round reward is
no longer the right objective to optimize, as the action with the highest expected per-round reward
could consume too much resources. Instead, one needs to take into account the expected reward
over the entire time horizon. Third, with more than one constrained resource (incl. time) the best
fixed policy is no longer the right benchmark; instead, the algorithm should search over distributions
over policies, which is a much larger search space.
In contextual bandit problems, an algorithm effectively chooses a policy π ∈ Π in each round.
Naively, this can be reduced to a non-contextual bandit problem in which “actions” correspond to
6. Lipschitz demands is a common assumption in some of the prior work on (non-contextual) dynamic pricing, even with
a single product (Besbes and Zeevi, 2009; Wang et al., 2014). However, the optimal algorithm for the single-product
case (Babaioff et al., 2015) does not need this assumption.
√
7. For example, the difference in regret between pre-determined and adaptive exploration is Õ( KT ) vs. O(K log T )
3/4
2/3
for stochastic K-armed bandits, and Õ(T ) vs. Õ(B ) for dynamic pricing with limited supply.

5

BADANIDIYURU L ANGFORD S LIVKINS

policies. In particular, the main results in Badanidiyuru et al. (2013a) directly apply to this reduced
problem.
However, the action space in the reduced problem has size |Π|; accordingly, regret scales
p
as |Π| in the worst case. The
pchallenge in contextual bandits is to reduce this dependence. In
particular, note that we replace |Π| with log |Π|, an exponential improvement.

Organization of the paper. We start with a survey of related work and preliminaries (Sections 2-3).
We define the main algorithm, prove its correctness, and describe the key steps of regret analysis
in Sections 4-6. The remaining details of the regret analysis are in Section 7. We prove the lower
bound in Section 8. We conclude with an extensive discussion of the state-of-art for RCB and the
directions for further work (Sections 10). Appendix A contains a discussion of the main application
domains for RCB.

2. Related work
Multi-armed bandits have been studied since Thompson (1933) in Operations Research, Economics,
and several branches of Computer Science, see (Gittins et al., 2011; Bubeck and Cesa-Bianchi,
2012) for background. This paper unifies two active lines of work on bandits: contextual bandits
and bandits with resource constraints.
Contextual Bandits (Auer, 2002; Langford and Zhang, 2007) add contextual side information
which can be used in prediction. This is a necessary complexity for virtually all applications of
bandits since it is far more common to have relevant contextual side information than no such information. Several versions have been studied in the literature, see (Bubeck and Cesa-Bianchi, 2012;
Dudik et al., 2011; Slivkins, 2014) for a discussion. For contextual bandits with policy sets, there
exist two broad families of solutions, based on multiplicative weight algorithms (Auer et al., 2002;
McMahan and Streeter, 2009; Beygelzimer et al., 2011) or confidence intervals (Dudik et al., 2011;
Agarwal et al., 2012). We rework the confidence interval approach, incorporating and extending the
ideas from the work on resource-constrained bandits (Badanidiyuru et al., 2013a).
Prior work on resource-constrained bandits includes dynamic pricing with limited supply (Babaioff et al.,
2015; Besbes and Zeevi, 2009, 2012), dynamic procurement on a budget (Badanidiyuru et al., 2012;
Singla and Krause, 2013; Slivkins and Vaughan, 2013), dynamic ad allocation with advertisers’
budgets (Slivkins, 2013), and bandits with a single deterministic resource (Guha and Munagala,
2007; Gupta et al., 2011; Tran-Thanh et al., 2010, 2012). Badanidiyuru et al. (2013a) define and
optimally solve a common generalization of all these settings: the non-contextual version of RCB.
An extensive discussion of these and other applications, including applications to repeated auctions
and network routing, can be found in (Badanidiyuru et al., 2013a).
To the best of our knowledge, the only prior work that explicitly considered contextual bandits
with resource constraints is (György et al., 2007). This paper considers a somewhat incomparable
setting with arbitrary policy sets and a single constrained resource: time, whose consumption is
stochastic and depends on the context and the chosen action. György et al. (2007) design an algorithm whose regret scales O(f (t) log t) for any time t, where f is any positive diverging function
and the constant in O() depends on the problem instance and on f .
Our setting can be seen as a contextual bandit version of stochastic packing (e.g. Devanur and Hayes,
2009; Devanur et al., 2011). The difference is in the feedback structure: in stochastic packing, full
information about each round is revealed before that round.

6

R ESOURCEFUL C ONTEXTUAL BANDITS

While we approximate our benchmark OPT(Π) with a linear program optimum, our algorithm
and analysis are conceptually very different from the vast literature on approximately solving linear
programs, and in particular from LP-based work on bandit problems such as Guha et al. (2010).
Concurrent and independent work. Agrawal and Devanur (2014) study a model for contextual
bandits with resource constraints that is incomparable with ours. The model for contexts is more
restrictive: contexts do not change over time,8 and expected outcome of each round is linear in
the context. Whereas the model for rewards and resource constraints is more general: the total
reward can be an arbitrary concave function of the time-averaged outcome vector v̄, and the resource
constraint states that v̄ must belong to a given convex set (which can be arbitrary).

3. Problem formulation and preliminaries
We consider an online setting where in each round an algorithm observes a context x from a possibly
infinite known set of possible contexts X and chooses an action a from a finite known set A. The
world then specifies a reward r ∈ [0, 1] and the resource consumption. There are d resources that can
be consumed, and the resource consumption is specified by numbers ci ∈ [0, 1] for each resource i.
Thus, the world specifies the vector (r; c1 , . . . , cd ), which we call the outcome vector; this vector
can depend on the the chosen action a and the round. There is a known hard constraint Bi ∈ R+
on the consumption of each resource i; we call it a budget for resource i. The algorithm stops at the
earliest time τ when any budget constraint is violated; its total reward is the sum of the rewards in
all rounds strictly preceding τ . The goal of the algorithm is to maximize the expected total reward.
We are only interested in regret at a specific time T (time horizon) which is known to the
algorithm. Formally, we model time as a specific resource with budget T and a deterministic consumption of 1 for every action. So d ≥ 2 is the number of all resources, including time. W.l.o.g.,
Bi ≤ T for every resource i.
We assume that an algorithm can choose to skip a round without doing anything. Formally, we
posit a null action: an action with 0 reward and 0 consumption of all resources except the time. This
is for technical convenience, so as to enable Lemma 5.
Stochastic assumptions. We assume that there exists an unknown distribution D(x, r, ci ), called
the outcome distribution, from which each round’s observations are created independently and identically, where the vectors are indexed by individual actions. In particular, context x is drawn from
the marginal distribution DX (·), and the observed reward and resource consumptions for each action
a are drawn from the conditional distribution D(r a , cia |x). We assume that the marginal distribution over contexts D(x) is known.
Policy sets and the benchmark. An algorithm is given a finite set Π of policies – mappings from
contexts to actions. Our benchmark is a hypothetical algorithm that knows the outcome distribution
D, and makes optimal decisions given this knowledge. The benchmark is restricted to policies in Π:
before each round, it must commit to some policy π ∈ Π, and then choose action π(x) upon arrival
of any given context x. The expected total reward of the benchmark is denoted OPT(Π). Regret of
an algorithm is OPT(Π) minus the algorithm’s expected total reward.
8. Agrawal and Devanur (2014) also claimed an extension to contexts that change over time, which has subsequently
been retracted (see Footnote 1 in Agrawal and Devanur (2015)). This extension constitutes the main result in
Agrawal and Devanur (2015) (which is subsequent work relative to the present paper).

7

BADANIDIYURU L ANGFORD S LIVKINS

Uniform budgets. We say that the budgets are uniform if Bi = B for each resource i. Any problem
instance can be reduced to one with uniform budgets by dividing all consumption values for every
resource i by Bi /B, where B = mini Bi . (That is tantamount to changing the units in which we
measure consumption of resource i.) We assume uniform budgets B from here on.
Notation. Let r(π) = E(x,r)∼D [r π(x) ] and ci (π) = E(x,ci )∼D [ci π(x) ] be the expected per-round
reward and the expected per-round consumption of resource i for policy π. Similary, define r(P ) =
Eπ∼P [r(π)] and ci (P ) = Eπ∼P [ci (π)] as the natural extension to a distribution P over policies.
The tuple µ = ( (r(π); c1 (π) , . . . , cd (π)) : π ∈ Π ) is called the expected-outcomes tuple.
For a distribution P over policies, let
PP (π) is the probability that P places over policy π. By a
slight abuse of notation, let P (a|x) = π(x)=a P (π) be the probability that P places on action a
given context x. Thus, each context x induces a distribution P (·|x) over actions.
3.1. Linear approximation and the benchmark
We set up a linear relaxation that will be crucial throughout the paper. As a by-product, we (effectively) reduce our benchmark OPT(Π) to the best fixed distribution over policies.
A given distribution P over policies defines an algorithm ALGP : in each round a policy π is
sampled independently from P , and the action a = π(x) is chosen. The value of P is the total
reward of this algorithm, in expectation over the outcome distribution.
As the value of P is difficult to characterize exactly, we approximate it (generalizing the approach from (Babaioff et al., 2015; Badanidiyuru et al., 2013a) for the non-contextual version). We
use a linear approximation where all rewards and consumptions are deterministic and the time is
continuous. Let r(P, µ) and ci (P, µ) be the expected per-round reward and the expected per-round
consumption of resource i for policy π ∼ P , given expected-outcomes tuple µ. Then the linear
approximation corresponds to the solution of a simple linear program:
Maximise t r(P, µ)
in t ∈ R
subject to t ci (P, µ) ≤ B for each i
t ≥ 0.

(3)

The solution to this LP, which we call the LP-value of P , is
LP(P, µ) = r(P, µ) mini B/ci (P, µ).

(4)

Denote OPTLP = supP LP(P, µ), where the supremum is over all distributions P over Π.
Lemma 4 OPTLP ≥ OPT(Π).
Therefore, it suffices to compete against the best fixed distribution over Π, as approximated
by OPTLP , even though our benchmark OPT(Π) allows unrestricted changes over time. Note that
proving regret bounds relative to OPTLP rather than to OPT(Π) only makes our results stronger.
A distribution P over Π that attains the supremum value OPTLP is called LP-optimal. Such P is
called LP-perfect if furthermore |support(P )| ≤ d and ci (P, µ) ≤ B/T for each resource i. We
find it useful to consider LP-perfect distributions throughout the paper.
Lemma 5 An LP-perfect distribution exists for any instance of RCB.

8

R ESOURCEFUL C ONTEXTUAL BANDITS

Lemma 4 and Lemma 5 are proved for the non-contextual version of RCB in Badanidiyuru et al.
(2013a). The general case can be reduced to the non-contextual version via a standard reduction
where actions in the new problem correspond to policies in Π in the original problem. For Lemma 5,
Badanidiyuru et al. (2013a) obtain an LP-perfect distribution by mixing an LP-optimal distribution
with the “null action”; this is why we allow the null action in the setting.

4. The algorithm: MixtureElimination
The algorithm’s goal is to converge on a LP-perfect distribution over policies. The general design
principle is to explore as much as possible while avoiding obviously suboptimal decisions.
Overview of the algorithm. In each round t, the following happens.
1.
Compute estimates. We compute high-confidence estimates for the per-round reward r(π)
and per-round consumption ci (π), for each policy π ∈ Π and each resource i. The collection I of
all expected-outcomes tuple that are consistent with these high-confidence estimates is called the
confidence region.
2. Avoid obviously suboptimal decisions. We prune away all distributions P over policies in Π
that are not LP-perfect with high confidence. More precisely, we prune all P that are not LP-perfect
for any expected-outcomes tuple in the confidence region I; the remaining distributions are called
potentially LP-perfect. Let F be the convex hull of the set of all potentially LP-perfect distributions.
3. Explore as much as possible. We choose a distribution P ∈ F which is balanced, in the sense
that no action is starved; see Equation (5) for the precise definition. Note that balanced distributions
are typically not LP-perfect.
4. Select an action. We choose policy π ∈ Π independently from P . Given context x, the action
a is chosen as a = π(x). The algorithm adds some random noise: with probability q0 , the action a
is instead chosen uniformly at random, for some parameter q0 .
The algorithm halts as soon as the time horizon is met, or one of the resources is exhausted.
The pseudocode can be found in Algorithm 1.
Some details. After each round t, we estimate the per-round consumption ci (π) and the per-round
reward r(π), for each policy π ∈ Π and each resource i, using the following unbiased estimators:
e
ci (π) =

r 1{a=π(x)}
ci 1{a=π(x)}
and re(π) =
.
P [a = π(x) | x]
P [a = π(x) | x]

The corresponding time-averages up to round t are denoted
ĉt,i (π) =

1
t−1

t−1
X
s=1

e
cs,i (π) and r̂t (π) =

1
t−1

t−1
X
s=1

res (π).

We show that with high probability these time-averages are close to their respective expectations. To express the confidence
p term in a more lucid way, we use the following shorthand, called
confidence radius: radt (ν) = Crad ν/t, where Crad = Θ(log(d T |Π|)) is a parameter which we
will fix later. We show that w.h.p. the following holds:
|r(π) − r̂t (π)| ≤ radt (K/απ,t ) ,

|ci (π) − ĉt,i (π)| ≤ radt ( K/απ,t )
9

(6)
for all i.

(7)

BADANIDIYURU L ANGFORD S LIVKINS

Algorithm 1 MixtureElimination
1: Parameters: #actions K, time horizon T , budget B, benchmark set Π, context distribution DX .
2: Data structure: “confidence region” I ← {all feasible expected-outcomes tuples}.
3:
4:
5:
6:
7:

8:
9:
10:
11:
12:

For each round t = 1 . . . T do
∆t = {distributions P over Π: P is LP-perfect for some µ ∈ I}.
Let Ft be the convex hull of ∆t .
Let απ,t = maxP ∈Ft P (π), ∀π ∈ Π.
Choose a “balanced” distribution Pt ∈ Ft : any P ∈ Ft such that ∀π ∈ Π


 q

2K
1
K
1
≤
E
, where q0 = min 2 , T log(K T |Π|) .
q
x∼DX (1 − q0 ) P (π(x)|x) + K0
απ,t

(5)

Observe context xt ; choose action at to ”play”:
with probability q0 , draw at u.a.r. in A; else, draw π ∼ Pt and let at = π(xt ).
Observe outcome vector (r, c1 , . . . , cd ).
Halt if one of the resources is exhausted.
Eliminate expected-outcomes tuples from I that violate equations (6-7)

(Here απ,t = maxP ∈Ft P (π), as in Algorithm 1.)

5. Correctness of the algorithm
We need to prove that in each round t, some P ∈ Ft satisfies (5), and Equations (6-7) hold for all
policies π ∈ Π with high probability.
Notation. Recall that Pt is the distribution over Π chosen in round t of the algorithm, and q0 is the
noise probability. The “noisy version” of Pt is defined as
Pt′ (a|x) = (1 − q0 ) Pt (a|x) + q0 /K

(∀x ∈ X, a ∈ A).

Then action at in round t is drawn from distribution Pt′ (·|xt ).
Lemma 6 In each round t, some P ∈ Ft satisfies (5).
Proof First we prove that Ft is compact; here each distribution over Π is interpreted as a |Π|dimensional vector, and compactness is w.r.t. the Borel topology on R|Π| . This can be proved via
standard real analysis arguments; we provide a self-contained proof in Appendix B.
In what follows we extend the minimax argument from Dudik et al. (2011). Our proof works
for any q0 ∈ [0, 21 ] and any compact and convex set F ⊂ FΠ .
Denote απ = maxP ∈F P (π), for each π ∈ Π. Let FΠ be the set of all distributions over Π.
Equation (5) holds for a given P ∈ F if and only if for every distribution Z ∈ FΠ we have that


απ
E
f (P, Z) , E
≤ 2K,
x∼DX π∼Z P ′ (π(x)|x)
where P ′ is the noisy version of P . It suffices to show that
min max f (P, Z) ≤ 2K.

P ∈F Z∈FΠ

10

(8)

R ESOURCEFUL C ONTEXTUAL BANDITS

We use a min-max argument: noting that f is a convex function of P and a concave function of Z,
by the Sion’s minimax theorem (Sion, 1958) we have that
min max f (P, Z) = max min f (P, Z).

P ∈F Z∈FΠ

Z∈FΠ P ∈F

(9)

For each policy π ∈ Π, let βπ ∈ argmaxβ∈F β(π) be a distribution which maximizes the probability
of selecting π. Such distribution exists because β 7→ β(π) is a continuous function on a compact
set F. Recall that απ = βπ (π).
P
Given any Z ∈ FΠ , define distribution PZ ∈ FΠ by PZ (π) = φ∈Π Z(φ) βφ (π). Note that
PZ is a convex combination
of distributions in F. Since F is convex, it follows that PZ ∈ F. Also,
P
note that PZ (a|x) ≥ π∈Π: π(x)=a Z(π) απ . Letting PZ′ be the noisy version of PZ , we have:
"
#
X Z(π) απ
min f (P, Z) ≤ f (PZ , Z) = E
P ∈F
x∼DX
PZ′ (π(x)|x)
π


#
"
P
X
X
X
Z(π)
α
π
Z(π)
α
π∈Π:
π(x)=a
π
= E
= E 
x∼DX
x∼DX
PZ′ (a|x)
(1 − q0 )PZ (a|x) + q0 /K
a∈X
a∈A π∈Π: π(x)=a
#
"
X 1
K
≤ 2K.
=
≤ E
x∼DX
1 − q0
1 − q0
a∈X



Thus, by Equation (9) we obtain Equation (8).

To analyze Equations (6-7), we will use Bernstein’s inequality for martingales (Freedman,
1975), via the following formulation from Bubeck and Slivkins (2012):
Lemma 7 Let G0 ⊆ G1 ⊆ . . . ⊆ Gn be a filtration, and X1 , . . . , Xn be real random variables P
such that Xt is Gt -measurable, E(Xt |Gt−1 ) = 0 and |Xt | ≤ b for some b > 0. Let
Vn = nt=1 E(Xt2 |Gt−1 ). Then with probability at least 1 − δ it holds that
q
Pn
X
≤
4Vn log(nδ−1 ) + 5b2 log2 (nδ−1 ).
t=1 t

Lemma 8 With probability at least 1− T1 , Equations (6-7) hold for all rounds t and policies π ∈ Π.

Proof Let us prove Equation (6). (The proof of (7) is similar.) Fix round t and policy π ∈ Π. We
bound the conditional variance of the estimators ret (π). Specifically, let Gt be the σ-algebra induced
by all events up to (but not including) round t. Then
#
"


2 1


r
1
2K
{π(x)=a}
t
2
E ret (π) | Gt =
.
E
≤ E
≤
′
′
2
′
x∼DX Pt (π(x)|x)
Pt (a|x)
απ,t
x∼DX , a∼Pt

The last inequality holds by the algorithm’s choice of distribution Pt . Since the confidence region
I in our algorithm is non-increasing over time, it follows that απ,t is non-increasing in t, too. We
conclude that Var [e
rs (π) | Gs ] ≤ 2K/απ,t for each round s ≤ t. Therefore, noting that ret (π) ≤
1/P ′ (π(xt )|xt ) ≤ K/q0 , we obtain Equation (6) by applying Lemma 7 with Xt = ret (π) − r(π). 
11

BADANIDIYURU L ANGFORD S LIVKINS

6. Regret analysis: proof of Theorem 1
We provide the key steps of the proof; the details can be found in Section 7.
Let It and ∆t be, resp., the confidence region I and the set ∆ of potentially LP-perfect distributions computed in round t. Let Conv(∆t ) be the convex hull of ∆t .
First we bound the deviations within the confidence region.
Lemma 9 For any two expected-outcomes tuples µ′ , µ′′ ∈ It and a distribution P ∈ Conv(∆t ):
|ci (P, µ′ ) − ci (P, µ′′ )| ≤ radt (dK)
′

Proof

for each resource i

′′

|r(P, µ ) − r(P, µ )| ≤ radt (dK)

(10)
(11)

Let us prove Equation (11). (Equation (10) is proved similarly.) By definition of It :
P
|r(P, µ′ ) − r(P, µ′′ )| ≤ π∈Π P (π) |r(π, µ′ ) − r(π, µ′′ )|
P
≤ π∈Π P (π) radt (K/απ,t ) .

It remains to prove that the right-hand side is at most radt (dK). By linearity, it suffices to prove
this for P ∈ ∆t . So let us assume P ∈ ∆t from here on. Recall that |support(P )| ≤ d since P is
LP-perfect, and P (π) ≤ απ,t for any policy π ∈ Π. Therefore:
P
P
π∈Π P (π) radt (K/απ,t ) ≤
π∈Π radt (KP (π))

P

≤ radt dK π∈Π P (π) = radt (dK) .
Using Lemma 9 and a long computation (fleshed out in Section 7), we prove the following.

Lemma 10 For any two expected-outcomes tuples µ′ , µ′′ ∈ It and a distribution P ∈ Conv(∆t ):
LP(P, µ′ ) − LP(P, µ′′ ) ≤ ( B1 LP(P, µ′ ) + 2) · T · radt (dK).
Let REWt and Ct,i be, respectively, the (realized) total reward and average consumption of resource i up to and including round t. Recall that Pt′ is the noisy version of distribution Pt chosen by
the algorithm in round t. Given Pt , the expected revenue
in round t is,
P and resource-i consumption
P
respectively, r(Pt′ , µ) and ci (Pt′ , µ). Denote r t = 1t ti=1 r(Pt′ , µ) and ci,t = 1t ti=1 ci (Pt′ , µ).

Analysis of a clean execution. Henceforth, without further notice, we assume a clean execution
where several high-probability conditions are satisfied. Formally, the algorithm’s execution is clean

if in each round t Equations (6-7) are satisfied, and moreover min | 1t REWt − rt |, |Ct,i − ct,i | ≤
radt (1).
In particular, the set ∆t of potentially LP-perfect distributions indeed contains a LP-perfect distribution. By Lemma 8 and Azuma-Hoeffding Inequality, clean execution happens with probability
at least 1 − T1 . Thus, it suffices to lower-bound the total reward REWT for a clean execution.
Lemma 11 For any distribution P ′ ∈ Conv(∆t ) and any expected-outcomes tuple µ ∈ It ,
min LP(P, µ) ≤ LP(P ′ , µ) ≤ max LP(P, µ).
P ∈∆t

P ∈∆t

12

(12)

R ESOURCEFUL C ONTEXTUAL BANDITS

Proof The proof consists of two parts. The second inequality in Equation (12) follows easily
because the distribution which maximizes LP(P, µ) by definition belongs to ∆t , and so
LP(P ′ , µ) ≤

max

P ∈Conv(∆t )

LP(P, µ) = max LP(P, µ).
P ∈∆t

To prove the first inequality in Equation (12), we first argue that LP(P, µ) is a quasi-concave
function of P . Denote ηi (P, µ) = B · r(P, µ)/ci (P, µ) for each resource i. Then ηi is a quasiconcave function of P since each level set (the set of distributions P that satisfy ηi (P, µ) ≥ α for
some α ∈ R) is a convex set. Therefore LP(P, µ) = mini ηi (P, µ) is a quasi-concave function of P
as a minimum of quasi-concave functions.
P
P
Since P ′ ∈ Conv(∆t ), it is a convex combination P ′ = Q∈∆t αQ Q with Q∈∆t αQ = 1.
Therefore:


X
αQ Q, µ
LP(P ′ , µ) = LP 
Q∈∆t

≥

min

Q∈∆t ,αQ >0

LP(Q, µ)

By definition of quasi-concave functions



≥ min LP(Q, µ).
Q∈∆t

The following lemma captures a crucial argument. Denote



1
Φt = 2 + B
max LP(P, µ) · T · radt (dK)
P ∈FΠ , µ∈It

Lemma 12 For any expected-outcomes tuple µ∗ , µ∗∗ ∈ It and distributions P ′ , P ′′ ∈ Conv(∆t ):

|LP(P ′ , µ∗ ) − LP(P ′′ , µ∗∗ )| ≤ 3Φt .
(13)
′
′′
′
′′
Proof Assume P , P ∈ ∆t . In particular, P , P are LP-perfect for some expected-outcomes
tuples µ′ , µ′′ ∈ It , resp. Also, some distribution P ∗ ∈ ∆t is LP-perfect for µ∗ (by Lemma 5).
Therefore:
LP(P ′ , µ∗ ) ≥ LP(P ′ , µ′ ) − Φt

(by Lemma 10: P = P ′ )

≥ LP(P ∗ , µ′ ) − Φt

≥ LP(P ∗ , µ∗ ) − 2Φt

(by Lemma 10: P = P ∗ )

≥ LP(P ′′ , µ∗ ) − 2Φt .
We proved Equation (13) for P ′ , P ′′ ∈ ∆t . Thus:

max LP(P, µ∗ ) − min LP(P, µ∗ ) ≤ 2Φt .

P ∈∆t

P ∈∆t

(14)

Next we generalize to P ′ , P ′′ ∈ Conv(∆t ).

LP(P ′ , µ∗ ) ≥ min LP(P, µ∗ )
P ∈∆t

(by Lemma 11)

≥ max LP(P, µ∗ ) − 2Φt
P ∈∆t

≥ LP(P ′′ , µ∗ ) − 2Φt

(by Equation (14))

(by Lemma 11).

We proved Equation (13) for µ∗ = µ∗∗ . We obtain the general case by plugging in Lemma 10.
13



BADANIDIYURU L ANGFORD S LIVKINS

Next, we upper-bound Φt in terms of
Ψt = (2 +

1
B

OPTLP ) · T · radt (dK).

Corollary 13 Φt ≤ 2Ψt , assuming that B ≥ 6 · T · radt (dK).
Proof Follows from Lemma 12 via a simple computation, see Section 7.



Corollary 14 LP(Pt , µ) ≥ OPTLP − 12 Ψt , where µ is the actual expected-outcomes tuple.
Proof Follows from Lemma 12 and Corollary 13, observing that Pt ∈ Conv(∆t ) and OPTLP =
LP(P ∗ , µ) for some P ∗ ∈ ∆t .

In the remainder of the proof (which is fleshed out in Section 7) we build on the above lemmas
and corollaries to prove the following sequence of claims:
t
(OPTLP − O(Ψt ))
T
≤ B/T + O(radt (dK))

REWt ≥
Ct,i

(15)

REWT ≥ OPTLP − O(ΨT ).

To complete the proof of Theorem 1, we re-write the last equation as REWT ≥ f (OPTLP ) for an
appropriate function f (), and observe that f (OPTLP ) ≥ f (OPT) because function f () is increasing.

7. Regret analysis: remaining details for the proof of Theorem 1
7.1. Proof of Lemma 10
We restate the lemma for convenience.
Lemma For any two expected-outcomes tuples µ′ , µ′′ ∈ It and a distribution P ∈ Conv(∆t ):
Proof

LP(P, µ′ ) − LP(P, µ′′ ) ≤ ( B1 LP(P, µ′ ) + 2) · T · radt (dK).
For brevity, we will denote:
LP′ = LP(P, µ′ ) and

LP′′ = LP(P, µ′′ )

r ′ = r(P, µ′ ) and

r ′′ = r(P, µ′′ )

c′i = ci (P, µ′ ) and

c′′i = ci (P, µ′′ ).

By symmetry, it suffices to prove the upper bound for LP′ − LP′′ . Henceforth, assume LP′ > LP′′ .
We consider two cases, depending on whether
T ≤ B/c′′i

for all resources i.

(16)

Case 1. Assume Equation (16) holds. Then LP′′ = T r ′′ . Therefore by Lemma 9
LP′ − LP′′ ≤ T r ′ − T r ′′ ≤ T radt (dK).
Case 2. Assume Equation (16) fails. Then LP′′ = B r ′′ /c′′i for some resource i. We consider
two subcases, depending on whether
T ≤ B/c′j

for all resources j.
14

(17)

R ESOURCEFUL C ONTEXTUAL BANDITS

Subcase 1. Assume Equation (17) holds. Then:
LP′ = T r ′

(18)

′′

′

′′

′

LP ≤ T · min(r , r ) ≤ LP

(19)

Equation (19) follows from (18) and LP′ > LP′′ .
For δ ∈ [0, c′′i ), define
r(δ) = r ′′ + δ
ci (δ) = c′′i − δ

f (δ) = B r(δ)/ci (δ).

Then f () is monotonically and continuously increasing function, with f (δ) → ∞ as δ → c′′i . For
convenience, define f (c′′i ) = ∞.
Let δ0 = min(c′′i , radt (dK)). By Lemma 9, we have f (δ0 ) ≥ Br ′ /c′i . Therefore:
f (0) = LP′′ < LP′ ≤ Br ′ /c′i ≤ f (δ0 ).
Thus, by Equation (19), we can fix δ ∈ [0, δ0 ) such that f (δ) = T · min(r ′ , r ′′ ).
r ′′
r(δ) − δ
=B
c′′i
ci (δ) + δ


r(δ) − δ
δ
≥B
1−
.
ci (δ)
ci (δ)
r(δ)
B
δ+B
δ
f (δ) − LP′′ ≤
ci (δ)
ci (δ)2


r(δ) B δ
= 1+
ci (δ) ci (δ)


f (δ) f (δ) δ
= 1+
B
r(δ)


T r ′ T r ′′ δ
≤ 1+
B
r(δ)


′
LP
≤ 1+
Tδ
B

≤ LP′ /B + 1 · T · radt (dK).
LP′′ = B

LP′ − f (δ) = T r ′ − T min(r, r ′ )

≤ T · radt (dK)


LP − LP = LP′ − f (δ) + f (δ) − LP′′

≤ LP′ /B + 2 · T · radt (dK).
′

′′

Subcase 2. Assume Equation (17) fails. Then LP′ = B r ′ /c′j for some resource j. Note that
c′i ≤ c′j and c′′j ≤ c′′i by the choice of i and j.
15

BADANIDIYURU L ANGFORD S LIVKINS

From these inequalities and Lemma 9 we obtain c′′i ≤ c′j + radt (dK). Therefore,
B

r ′ − radt (dK)
r ′′
≥
B
c′′i
c′j + radt (dK)
r ′ − radt (dK)
≥B
c′j

LP′ − LP′′ = B

(by Lemma 9)
radt (dK)
1−
c′j

!

.

r ′′
r′
−
B
c′j
c′′i

!
r′
B
+ B ′ 2 radt (dK)
≤
c′j
(cj )


′T
radt (dK)
≤ T + LP
B

≤ LP′ /B + 1 · T · radt (dK).



7.2. Remainder of the proof after Lemma 12
We start with Corollary 13, which we restate here for convenience.
Corollary Φt ≤ 2Ψt , assuming that B ≥ 6 · T · radt (dK).
Proof Let γ = maxP ∈FΠ ,µ∈It LP(P, µ). Note that γ ≤ T . Then from Lemma 12 we obtain:
γ − OPTLP ≤ 3( Bγ + 2) · T · radt (dK) ≤

γ
2

+ 6 · T · radt (dK).

Using (20) and Lemma 12 we get the desired bound:
Φt ≤ ( Bγ + 2) · T · radt (dK)


2 OPTLP + 12 · T · radt (dK)
+ 2 · T · radt (dK)
≤
B


2 OPTLP
+ 4 · T · radt (dK) = 2Ψt .

≤
B
In the remainder of this appendix, we prove the claims in Equation (15) one by one.
Corollary 15 REWt ≥ Tt (OPTLP − O(Ψt )) for each round t ≤ τ .
Proof From Lemma 14 we obtain
T r(Pt′ , µ) ≥ (1 − q0 ) LP(Pt , µ)

≥ (1 − q0 ) (OPTLP − 12Ψt )

≥ OPTLP − 13Ψt .

Summing up and taking average over rounds, we obtain:
Pt
T r t ≥ OPTLP − 13
s=1 Ψs ≥ OPTLP − O(Ψt ).
t
By definition of clean execution, we obtain:

REWt ≥ t(r t − radt (rt )) ≥

t
T (OPTLP

16

− O(Ψt )).



(20)

R ESOURCEFUL C ONTEXTUAL BANDITS

Corollary 16 Ct,i ≤ B/T + O(radt (dK)) for each round t ≤ τ .

Proof Let µ be the (actual) expected-outcomes tuple, and recall that Pt is LP-optimal for some
expected-outcomes tuple µ′ ∈ ∆t . Then, by Lemma 9, it follows that ci (Pt , µ) ≤ ci (Pt , µ′ ) +
radt (dK). Furthermore since Pt is LP-optimal for µ′ we have ci (Pt , µ′ ) ≤ B
T . Therefore:
ci (Pt , µ) ≤

B
T

≤

B
T

ci (Pt′ , µ)

+ radt (dK)

≤ (1 − q0 ) ci (Pt , µ) + q0
+ O(radt (dK)).
B
T

Now summing and taking average we obtain ct,i ≤
clean execution, it follows that
Ct,i ≤ ct,i + radt (ct,i ) ≤

B
T

+ O(radt (dK)). Using the definition of


+ O(radt (dK)).

Lemma 17 REWT ≥ OPTLP − O(ΨT ).

Proof

Either τ = T or some resource i gets exhausted, in which case (using Corollary 16)
τ=

B
≥
Cτ,i

B
T

B
+ radτ (dK)

⇒τB
T + τ radτ (dK) ≥ B

⇒τB
T + T radT (dK) ≥ B

⇒τ ≥T 1−

T
B


radT (dK) .

(21)

Using this lower bound and Corollary 15, we obtain the desired bound on the total revenue REWT .
REWT = REWτ ≥

τ
( OPTLP − O(Ψτ ) )
T

≥ OPTLP (1 −

T
B

radT (dK)) −

≥ OPTLP − ΨT −

O(τ Ψτ )
T

O(τ Ψτ )
.
T

In the above, the first inequality holds by Corollary 15, the second by Equation (21), and the third
by definition of ΨT .
Finally, we note that τ Ψτ is an increasing function of τ , and substitute τ Ψτ ≤ T ΨT .


We complete the proof of Theorem 1 as follows. Re-writing Lemma 17 as REWT ≥ f (OPTLP ),
for an appropriate function f (), note that REWT ≥ f (OPT) because function f () is increasing.

8. Lower bound: proof of Theorem 2
In fact, we prove a stronger theorem that implies Theorem 2.
√
Theorem 18 Fix any tuple (K, T, B) such that K ∈ [2, T ] and B ≤ KT /2. Any algorithm
for RCB incurs regret Ω(OPT(Π)) in the worst case over all problem instances with K actions, time
horizon T , smallest budget B, and policy sets Π such that OPT(Π) ≤ B.
17

BADANIDIYURU L ANGFORD S LIVKINS

We will use the following lemma (which follows from simple probability arguments).
Lemma 19 Consider two collections of n balls I1 and I2 , each numbered from 1 to n. Let I1
consists of all red balls, while I2 consist of n − 1 red balls and 1 green ball (with labels chosen
uniformly at random). In this setting, let an algorithm is given access to random samples from one
of Ii with replacement. The algorithm is allowed to first look at the ball’s number and then decide
whether to inspect it’s color. Then any algorithm A which with probability at least 12 can distinguish
between I1 and I2 must inspect color of at least n/2 balls in expectation.
In the remainder of this section we prove Theorem 18.
Let us define a family of problem instances as follows. Let the set of arms be {a1 , a2 , . . . , aK }.
There are T /B different contexts labelled {x1 , ..., xT /B } and there is a uniform distribution over
contexts. The policy set Π consists of T (K − 1)/B policies πi,j , where 2 ≤ i ≤ K and 1 ≤ j ≤
T /B. Define them as follows: πi,j (xl ) = ai for l = j, and πi,j (xl ) = a1 for l 6= j.
There is just one resource constraint B (apart from time). Pulling arm a1 always costs 0 and
arm ai , i 6= 1 always costs 1. Now consider the following problem instances:
•

Let F0 be the instance in which every arm always gives a reward 0. Note that OPT(F0 ) = 0.

• Let Fi,j be the instance in which arm ai on context xj gives reward 1, otherwise every arm on
every context gives reward 0. Note that in this case the optimal distribution over policies is just to
follow πi,j and gets reward ≈ B.
Now consider any algorithm A and let the expected number of times it pulls arm ai be pi
on input F0 . Let i′ , i′ 6= 1 be the arm for which this is minimum. Then by simple linearity of
expectation we get that B ≥ (K − 1)pi′ . It is also simple to see that for the algorithm to get a regret
better than Ω(OPT) it should be able to distinguish between F0 and Fi′ ,. at least with probability
1
′
/(2B). Combining the two equations we get
2 . From lemma 19 this can be done iff pi ≥ T√
B ≥ (K − 1)T /(2B). Solving for B we get B ≥ KT /2.

9. Discretization for contextual dynamic pricing (proof of Theorem 3)
We consider contextual dynamic pricing with B copies of a single product. The action space consists
of all prices p ∈ [0, 1]. We obtain regret bounds relative to an arbitrary policy set Π.
Preliminaries. Let S(p|x) be the contextual sales rate: the probability of a sale for price p and
context x. Note that S(p|x) is non-increasing in p, for any given x.
The assumption of Lipschitz demands is stated as follows:
|S(p|x) − S(p′ |x)| ≤ L · |p − p′ | for all contexts x,

(22)

for some constant L called the Lipschitz constant. For simplicity, assume L ≥ 1.
For a (possibly randomized) policy π, define the contextual sales rate S(π|x) = Ep∼π(x) [ S(p|x) ]
and the absolute sales rate S(π) = Ex [ S(π|x) ]. The latter is exactly the expected per-round resource consumption for π. Let r(π) be the expected per-round reward for π.
As discussed in the Introduction, we define the discretization with step ǫ as follows. For each
price p, let fǫ (p) be p rounded down to the nearest multiple of ǫ, i.e. the largest price p′ ≤ p such
18

R ESOURCEFUL C ONTEXTUAL BANDITS

that p′ ∈ ǫN. For each policy π we define a discretized policy πǫ = fǫ (π). The discretized policy
set is then Πǫ = {πǫ : π ∈ Π}. Note that for all policies π and all contexts x we have
π(x) ≥ πǫ (x) ≥ π(x) − ǫ.
By monotonicity of the sales rate and the Lipschitz assumption, resp., it follows that
S(π|x) ≤ S(πǫ |x) ≤ S(π|x) + ǫL.
Consequently, S(π) ≤ S(πǫ ) ≤ S(π) + ǫL.
Discretization error. The key technical step is to bound the discretization error of the discretized
policy set Πǫ compared to the original policy set Π, as quantified by the difference in OPTLP (·).
Our proof will use an intermediate policy class Φδ = {S(π) ≥ δ}, where δ > 0. First we bound
the discretization error relative to Φδ .
Lemma 20 OPTLP (Φδ ) − OPTLP (Πǫ ) ≤ 2 · ǫ(1 + Lδ−2 ) · B, for each ǫ, δ > 0.
Proof Using a trivial reduction to the non-contextual case (when a policy corresponds to an
action in the bandits-with-knapsacks problem), one can use a generic discretization result from
Badanidiyuru et al. (2013a). According to this result (specialized to contextual dynamic pricing), it
suffices to prove that for each policy π ∈ Φδ the following two properties hold:
(P1) S(πǫ ) ≥ S(π),
(P2) r(πǫ )/S(πǫ ) ≥ r(π)/S(π) − ǫ(1 + Lδ−2 ), as long as S(πǫ ) > 0.
In words: the sales rate of the discretized policy πǫ is at least the same, and the reward-to-consumption
ratio is not much worse.
Property (P1) holds trivially because πǫ ≤ π (deterministically and for every context), and the
contextual sales rate S(p|x) is decreasing in p for any fixed context x.
r(πǫ ) = E [ fǫ (π(x)) · S(πǫ |x) ]
x,π

≥ E [ (π(x) − ǫ) · S(πǫ |x) ]
x,π

≥ E [ π(x) · S(π|x) ] − ǫ E [ S(πǫ |x) ]
x,π

x,π

= r(π) − ǫ S(πǫ ).

r(πǫ )/S(πǫ ) ≥ r(π)/S(πǫ ) − ǫ.
Now, by the Lipschitz assumption, S(πǫ ) ≤ S(π) + ǫL, so to complete the proof
r(π)
r(π)
ǫL
r(π)
ǫL
r(πǫ )
≥
−ǫ≥
−
−ǫ≥
− 2 − ǫ.
2
S(πǫ )
S(π) + ǫL
S(π) (S(π))
S(π)
δ



Now we bound the loss in OPTLP between Π and Φδ .
Lemma 21 OPTLP (Π) − OPTLP (Φδ ) ≤ δT , for each δ > 0.
Proof If δ ≥ B/T , the statement is trivial because OPTLP (Π) ≤ B. So w.l.o.g. assume δ < B/T .
By Lemma 5, there exists an LP-perfect distribution P over policies in Π. Recall that P is a
mixture of (at most) two policies, say π and π ′ , and c(P ) ≤ B/T . W.l.o.g. assume S(π) ≤ S(π ′ ).
If S(π) ≥ δ then π, π ′ ∈ Φδ , so OPTLP (Π) = OPTLP (Φδ ).
19

BADANIDIYURU L ANGFORD S LIVKINS

The remaining case is S(π) < δ. Then S(π ′ ) ≥ B/T > δ, so π ′ ∈ Φδ . Therefore:
OPTLP (Π) = LP(P ) ≤ LP(π) + LP(π ′ ) ≤ LP(π) + OPTLP (Φδ ).
It remains to prove that LP(π) ≤ δT . Indeed,
r(π) = E [ π(x) · S(π|x) ] ≤ E [ S(π|x) ] = S(π) ≤ δ.
x,π

x,π

LP(π) = r(π) min(T, B/S(π)) ≤ r(π) T ≤ δT.



Putting Lemma 20 and Lemma 20 together and optimizing δ, we obtain:
Lemma 22 For each ǫ > 0, letting δ = (2ǫBL/T )1/3 , we have
OPTLP (Π) − OPTLP (Πǫ ) ≤ 2δT + 2ǫB.
Plugging in the general result. Let REW(Π′ ) be the expected total reward when MixtureElimination
is run with policy set Π′ which uses only K distinct actions. Recall that we actually prove a somewhat stronger version of Theorem 1: the same regret bound (1), but with respect to OPTLP (Π′ ) rather
than OPT(Π′ ). In our setting we have d = 2 resource constraints (incl. time) and OPTLP (Π′ ) ≤ B.
Therefore:

p
KT log (KT |Π′ |) .
REW(Π′ ) ≥ OPTLP (Π′ ) − O
Plugging in Π′ = Πǫ and K = 1ǫ , and using Lemma 22, we obtain


q

T
T
REW(Πǫ ) ≥ OPTLP (Π) − O ǫB + δT + ǫ log ǫ |Πǫ | ,

(23)

for each ǫ > 0 and δ = (2ǫBL/T )1/3 .
We obtain Theorem 3 choosing ǫ = (BL)−2/5 T −1/5 (log(T |Πǫ |))3/5 and noting |Πǫ | ≤ |Π|.

10. Conclusions and open questions
We define a very general setting for contextual bandits with resource constraints (denoted RCB). We
design an algorithm for this problem, and derive a regretpbound which achieves the optimal root-T
scaling in terms of the time horizon T , and the optimal log |Π| scaling in terms of the policy set
Π. Further, we consider discretization issues, and derive a specific corollary for contextual dynamic
pricing with a single product; we obtain a regret bound that applies to an arbitrary policy set Π.
Finally, we derive a partial lower bound which establishes a stark difference from the non-contextual
version. These results set the stage for further study of RCB, as discussed below.
The main question left open by this work is to combine provable regret bounds and a computationally efficient (CE) implementation. While we focused on the statistical properties, we believe
our techniques are unlikely to lead to CE implementations. Achieving near-optimal regret bounds
in a CE way has been a major open question for contextual bandits with policy sets (without resource constraints). This question has been resolved in the positive in a simultaneous and independent work (Agarwal et al., 2014). Very recently, a follow-up paper (Agrawal et al., 2015) has
20

R ESOURCEFUL C ONTEXTUAL BANDITS

achieved the corresponding advance on RCB, by combing the techniques from Agarwal et al. (2014)
and Agrawal and Devanur (2014) (which, in turn, builds on Badanidiyuru et al. (2013a)).
Computational issues aside, several open questions concern our regret bounds.
First, it is desirable to achieve the same regret bounds without assuming a known time horizon
T (as it is in most bandit problems in the literature). This may be difficult because time is one of
the resource constraints in our problem, and our techniques rely on knowing all resource constraints
in advance. More generally, one can consider a version of RCB in which some of the resource
constraints are not fully revealed to an algorithm; instead, the algorithm receives updated estimates
of these constrains over time.
Second, while our main regret bound in Theorem 1 is optimal in the important regime when
OPT(Π) and B are at least a constant fraction of T , it is not tight for some other regimes. For a concrete comparison, consider problem instances with a constant number of resources (d), a constant
number √
of actions (K), and OPT(Π) ≥ Ω(B). Then, ignoring logarithmic factors, we√obtain regret
OPT(Π) T /B, whereas the lower bound in Badanidiyuru et al. (2013a) is OPT(Π)/ B. So there
is a gap when B ≪ T . Likewise, for contextual dynamic pricing with a single product, there is a
gap between our algorithmic result (Theorem 3) and the B 2/3 lower bound for the non-contextual
case from Babaioff et al. (2015). In both cases, both upper and lower bounds can potentially be
improved.
Third, for special cases when actions correspond to prices one would like to extend the discretization approach beyond contextual dynamic pricing with a single product. However, this is
problematic even without contexts: essentially, nothing is known whenever one has multiple resource constraints, and even with a single resource constraint (besides time) the solutions are very
non-trivial; see Badanidiyuru et al. (2013a) for more discussion.
Fourth, if there are no contexts or resource constraints then one can achieve O(log T ) regret
with an instance dependent constant; it is not clear whether one can meaningfully extend this result
to contextual bandits with resource constraints.
The model of RCB can be extended in several directions, two of which we outline below. The
most immediate extension is to an unknown distribution of context arrivals. This extension has been
addressed, among other results, in the follow-up paper (Agrawal et al., 2015). The most important
extension, in our opinion, would be from a stationary environment to one controlled by an adversary
(perhaps restricted in some natural way). We are not aware of any prior work in this direction, even
for the non-contextual version.



We present a new algorithm for the contextual bandit learning problem, where the learner repeatedly takes one of K actions in response to the observed context, and observes the reward only for that
chosen action. Our method assumes access to an oracle for solving fully supervised cost-sensitive
clasp
sification problems and achieves the statistically optimal regret guarantee with only Õ( KT / log N )
oracle calls across all T rounds, where N is the number of policies in the policy class we compete
against. By doing so, we obtain the most practical contextual bandit learning algorithm amongst
approaches that work for general policy classes. We further conduct a proof-of-concept experiment
which demonstrates the excellent computational and prediction performance of (an online variant of)
our algorithm relative to several baselines.

1

Introduction

In the contextual bandit problem, an agent collects rewards for actions taken over a sequence of rounds;
in each round, the agent chooses an action to take on the basis of (i) context (or features) for the current
round, as well as (ii) feedback, in the form of rewards, obtained in previous rounds. The feedback is
incomplete: in any given round, the agent observes the reward only for the chosen action; the agent
does not observe the reward for other actions. Contextual bandit problems are found in many important
applications such as online recommendation and clinical trials, and represent a natural half-way point
between supervised learning and reinforcement learning. The use of features to encode context is inherited
from supervised machine learning, while exploration is necessary for good performance as in reinforcement
learning.
The choice of exploration distribution on actions is important. The strongest known results (Auer et al.,
2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011) provide algorithms that carefully control
the exploration distribution to achieve an optimal regret after T rounds of

p
KT log(|Π|/δ) ,
O

with probability at least 1 − δ, relative to a set of policies Π ⊆ AX mapping contexts x ∈ X to actions
a ∈ A (where K is the number of actions). The regret is the difference between the cumulative reward of
the best policy in Π and the cumulative reward collected by the algorithm. Because the bound has a mild
logarithmic dependence on |Π|, the algorithm can compete with very large policy classes that are likely
1

to yield high rewards, in which case the algorithm also earns high rewards. However, the computational
complexity of the above algorithms is linear in |Π|, making them tractable for only simple policy classes.
A sub-linear in |Π| running time is possible for policy classes that can be efficiently searched. In
this work, we use the abstraction of an optimization oracle to capture this property: given a set of context/reward vector pairs, the oracle returns a policy in Π with maximum total reward. Using such an oracle in an i.i.d. setting (formally defined in Section 2.1), it is possible to create ǫ-greedy (Sutton and Barto,
1998) or epoch-greedy (Langford and Zhang, 2007) algorithms that run in time O(log |Π|) with only
a single call to the oracle per round. However, these algorithms have suboptimal regret bounds of
O((K log |Π|)1/3 T 2/3 ) because the algorithms randomize uniformly over actions when they choose to
explore.
The Randomized UCB algorithm of Dudı́k et al. (2011a) achieves the optimal regret bound (up to logarithmic factors) in the i.i.d. setting, and runs in time poly(T, log |Π|) with Õ(T 5 ) calls to the optimization
oracle per round. Naively this would amount to Õ(T 6 ) calls to the oracle over T rounds, although a doubling trick from our analysis can be adapted to ensure only Õ(T 5 ) calls to the oracle are needed over all
T rounds in the Randomized UCB algorithm. This is a fascinating result because it shows that the oracle
can provide an exponential speed-up over previous algorithms with optimal regret bounds. However, the
running time of this algorithm is still prohibitive for most natural problems owing to the Õ(T 5 ) scaling.
In this work, we prove the following1 :
Theorem 1.
an algorithm for the i.i.d. contextual bandit problem with an optimal regret bound
qThere is 
KT
requiring Õ
calls to the optimization oracle over T rounds, with probability at least 1 − δ.
ln(|Π|/δ)

p
p
Concretely, we make Õ( KT / ln(|Π|/δ)) calls to the oracle with a net running time of Õ(T 1.5 K log |Π|),
vastly improving over the complexity of Randomized UCB. The major components of the new algorithm
are (i) a new coordinate descent procedure for computing a very sparse distribution over policies which
can be efficiently sampled from, and (ii) a new epoch structure which allows the distribution over policies to be updated very infrequently. We consider variants of the epoch structure that make different
computational trade-offs;
on one extreme we concentrate the entire computational burden on O(log T )
p
KT
/
ln(|Π|/δ))
oracle calls each time, while on the other we spread our computation
rounds
with
Õ(
p
√
over T rounds with Õ( K/ ln(|Π|/δ)) oracle calls for each of these rounds. We stress that in either
case, the total number of calls to the oracle is only sublinear in T . Finally, we develop a more efficient
online variant, and conduct a proof-of-concept experiment showing low computational complexity and
high reward relative to several natural baselines.
Motivation and related work. The EXP4-family of algorithms (Auer et al., 2002; McMahan and Streeter,
2009; Beygelzimer et al., 2011) solve the contextual bandit problem with optimal regret by updating
weights (multiplicatively) over all policies in every round. Except for a few special cases (Helmbold and Schapire,
1997; Beygelzimer et al., 2011), the running time of such measure-based algorithms is generally linear in
the number of policies.
In contrast, the Randomized UCB algorithm of Dudı́k et al. (2011a) is based on a natural abstraction
from supervised learning—the ability to efficiently find a function in a rich function class that minimizes
the loss on a training set. This abstraction is encapsulated in the notion of an optimization oracle,
which is also useful for ǫ-greedy (Sutton and Barto, 1998) and epoch-greedy (Langford and Zhang, 2007)
algorithms. However, these latter algorithms have only suboptimal regret bounds.
Another class of approaches based on Bayesian updating is Thompson sampling (Thompson, 1933; Li,
2013), which often enjoys strong theoretical guarantees in expectation over the prior and good empirical
performance (Chapelle and Li, 2011). Such algorithms, as well as the closely related upper-confidence
bound algorithms (Auer, 2002; Chu et al., 2011), are computationally tractable in cases where the posterior distribution over policies can be efficiently maintained or approximated. In our experiments, we
compare to a strong baseline algorithm that uses this approach (Chu et al., 2011).
1 Throughout this paper, we use the Õ notation to suppress dependence on logarithmic factors in T and K, as well as
log(|Π|/δ) (i.e. terms which are O(log log(|Π|/δ)).

2

To circumvent the Ω(|Π|) running time barrier, we restrict attention to algorithms that only access the
policy class via the optimization oracle. Specifically, we use a cost-sensitive classification oracle, and a key
challenge is to design good supervised learning problems for querying this oracle. The Randomized UCB
algorithm of Dudı́k et al. (2011a) uses a similar oracle to construct a distribution over policies that solves
a certain convex program. However, the number of oracle calls in their work is prohibitively large, and
the statistical analysis is also rather complex.2
Main contributions. In this work, we present a new and simple algorithm for solving a similar convex
program as that used by Randomized UCB. The new algorithm is based on coordinate descent: in each
iteration, the algorithm calls the optimization oracle to obtain a policy; the output is a sparse distribution
over
p these policies. The number of iterations required to compute the distribution is small—at most
Õ( Kt/ ln(|Π|/δ)) in any round t. In fact, we present a more general scheme based on p
epochs and warm
start in which the total number of calls to the oracle is, with high probability, just Õ( KT / ln(|Π|/δ))
over all T rounds; we prove that this is nearly optimal for a certain class of optimization-based algorithms.
The algorithm is natural and simple to implement, and we provide an arguably simpler analysis than that
for Randomized UCB. Finally, we report proof-of-concept experimental results using a variant algorithm
showing strong empirical performance.

2

Preliminaries

In this section, we recall the i.i.d. contextual bandit setting and some basic techniques used in previous
works (Auer et al., 2002; Beygelzimer et al., 2011; Dudı́k et al., 2011a).

2.1

Learning Setting

Let A be a finite set of K actions, X be a space of possible contexts (e.g., a feature space), and Π ⊆ AX
be a finite P
set of policies that map contexts x ∈ X to actions a ∈ A.3 Let ∆Π := {Q ∈ RΠ : Q(π) ≥
0 ∀π ∈ Π, π∈Π Q(π) ≤ 1} be the set of non-negative weights over policies with total weight at most
A
one, and let RA
+ := {r ∈ R : r(a) ≥ 0 ∀a ∈ A} be the set of non-negative reward vectors.
Let D be a probability distribution over X × [0, 1]A , the joint space of contexts and reward vectors; we
assume actions’ rewards from D are always in the interval [0, 1]. Let DX denote the marginal distribution
of D over X.
In the i.i.d. contextual bandit setting, the context/reward vector pairs (xt , rt ) ∈ X × [0, 1]A over all
rounds t = 1, 2, . . . are randomly drawn independently from D. In round t, the agent first observes the
context xt , then (randomly) chooses an action at ∈ A, and finally receives the reward rt (at ) ∈ [0, 1]
for the chosen action. The (observable) record of interaction resulting from round t is the quadruple
(xt , at , rt (at ), pt (at )) ∈ X × A × [0, 1] × [0, 1]; here, pt (at ) ∈ [0, 1] is the probability that the agent chose
action at ∈ A. We let Ht ⊆ X × A × [0, 1] × [0, 1] denote the history (set) of interaction records in the
b x∼H [·] to denote expectation when a context x is chosen
first t rounds. We use the shorthand notation E
t
from the t contexts in Ht uniformly at random.
Let R(π) := E(x,r)∼D [r(π(x))] denote the expected (instantaneous) reward of a policy π ∈ Π, and
let π⋆ := arg maxπ∈Π R(π) be a policy that maximizes the expected reward (the optimal policy). Let
Reg(π) := R(π⋆ ) − R(π) denote the expected (instantaneous) regret of a policy π ∈ Π relative to the
optimal policy. Finally, the (empirical cumulative) regret of the agent after T rounds4 is defined as
T
X
t=1

2 The


rt (π⋆ (xt )) − rt (at ) .

paper of Dudı́k et al. (2011a) is colloquially referred to, by its authors, as the “monster paper” (Langford, 2014).
to VC classes is simple using standard arguments.
4 We have defined empirical cumulative regret as being relative to π , rather than to the empirical reward maximizer
⋆
p
PT
arg maxπ∈Π t=1 rt (π(xt )). However, in the i.i.d. setting, the two do not differ by more than O( T ln(|Π|/δ)) with
probability at least 1 − δ.
3 Extension

3

2.2

Inverse Propensity Scoring

An unbiased estimate of a policy’s reward may be obtained from a history of interaction records Ht using
inverse propensity scoring (IPS; also called inverse probability weighting): the expected reward of policy
π ∈ Π is estimated as
t
X
ri (ai ) · 1{π(xi ) = ai }
b t (π) := 1
R
.
(1)
t i=1
pi (ai )

This technique can be viewed as mapping Ht 7→ IPS(Ht ) of interaction records (x, a, r(a), p(a)) to context/reward vector pairs (x, r̂), where r̂ ∈ RA
+ is a fictitious reward vector that assigns to the chosen
action a a scaled reward r(a)/p(a) (possibly greater than one), and assigns to all other actions zero
rewards. This transformation IPS(Ht ) is detailed in Algorithm 3 (in Appendix A); we may equivalently
b t by R
b t (π) := t−1 P
define R
(x,r̂)∈IPS(Ht ) r̂(π(x)). It is easy to verify that E[r̂(π(x))|(x, r)] = r(π(x)), as
b t (π) is
p(a) is indeed the agent’s probability (conditioned on (x, r)) of picking action a. This implies R
an unbiased estimator for any history Ht .
b t (π) denote a policy that maximizes the expected reward estimate based
Let πt := arg maxπ∈Π R
d t (π) := R
b t (πt ) − R
b t (π)
on inverse propensity scoring with history Ht (π0 can be arbitrary), and let Reg
d
denote estimated regret relative to πt . Note that Regt (π) is generally not an unbiased estimate of Reg(π),
because πt is not always π⋆ .

2.3

Optimization Oracle

One natural mode for accessing the set of policies Π is enumeration, but this is impractical in general.
In this work, we instead only access Π via an optimization oracle which corresponds to a cost-sensitive
learner. Following Dudı́k et al. (2011a), we call this oracle AMO5 .
Definition 1. For a set of policies Π, the arg max oracle (AMO) is an algorithm, which for any sequence
of context and reward vectors, (x1 , r1 ), (x2 , r2 ), . . . , (xt , rt ) ∈ X × RA
+ , returns
arg max
π∈Π

2.4

t
X

rτ (π(xτ )).

τ =1

Projections and Smoothing

In each round, our algorithm chooses an action by randomly drawing a policy π from a distribution over
Π, and then picking the action π(x) recommended
by π on the current context x. This is equivalent
P
to drawing an action according to Q(a|x) := π∈Π:π(x)=a Q(π), ∀a ∈ A. For keeping the variance of
reward estimates from IPS in check, it is desirable to prevent the probability of any action from being
µ
too small. Thus, as in
P previous work, we also use a smoothed projection Q (·|x) for µ ∈ [0, 1/K],
µ
Q (a|x) := (1 − Kµ) π∈Π:π(x)=a Q(π) + µ, ∀a ∈ A. Every action has probability at least µ under
Qµ (·|x).
For technical reasons, our algorithm maintains non-negative weights Q ∈ ∆Π over policies that sum
to at most one, but not necessarily equal to one; hence, we put any remaining mass
 policy
Pon a default
π̄ ∈ Π to obtain a legitimate probability distribution over policies Q̃ = Q + 1 − π∈Π Q(π) 1π̄ . We
then pick an action from the smoothed projection Q̃µ (·|x) of Q̃ as above. This sampling procedure
Sample(x, Q, π̄, µ) is detailed in Algorithm 4 (in Appendix A).

3

Algorithm and Main Results

Our algorithm (ILOVETOCONBANDITS) is an epoch-based variant of the Randomized UCB algorithm of
Dudı́k et al. (2011a) and is given in Algorithm 1. Like Randomized UCB, ILOVETOCONBANDITS solves
5 Cost-sensitive

learners often need a cost instead of reward, in which case we use ct = 1 − rt .

4

an optimization problem (OP) to obtain a distribution over policies to sample from (Step 7), but does
so on an epoch schedule, i.e., only on certain pre-specified rounds τ1 , τ2 , . . .. The only requirement of
the epoch schedule is that the length of epoch m is bounded as τm+1 − τm = O(τm ). For simplicity, we
assume τm+1 ≤ 2τm for m ≥ 1, and τ1 = O(1).
The crucial step here is solving (OP). Before stating the main result, let us get some intuition about
this problem. The first constraint, Eq. (2), requires the average estimated regret of the distribution Q
over policies to be small, since bπ is a rescaled version of the estimated regret of policy π. This constraint
skews our distribution to put more mass on “good policies” (as judged by our current information), and
can be seen as the exploitation component of our algorithm. The second set of constraints, Eq. (3),
requires the distribution Q to place sufficient mass on the actions chosen by each policy π, in expectation
over contexts. This can be thought of as the exploration constraint, since it requires the distribution to
be sufficiently diverse for most contexts. As we will see later, the left hand side of the constraint is a
bound on the variance of our reward estimates for policy π, and the constraint requires the variance to
be controlled at the level of the estimated regret of π. That is, we require the reward estimates to be
more accurate for good policies than we do for bad ones, allowing for much more adaptive exploration
than the uniform exploration of ǫ-greedy style algorithms.
This problem is very similar to the one in Dudı́k et al. (2011a), and our coordinate descent algorithm
in Section 3.1 gives a constructive proof that the problem is feasible. As in Dudı́k et al. (2011a), we have
the following regret bound:
Theorem 2. Assume the optimization problem ( OP) can be solved whenever required in Algorithm 1.
With probability at least 1 − δ, the regret of Algorithm 1 (ILOVETOCONBANDITS) after T rounds is

p
KT ln(T |Π|/δ) + K ln(T |Π|/δ) .
O
Algorithm 1 Importance-weighted LOw-Variance Epoch-Timed Oracleized CONtextual BANDITS algorithm (ILOVETOCONBANDITS)
input Epoch schedule 0 = τ0 < τ1 < τ2 < · · · , allowed failure probability δ ∈ (0, 1).
1: Initial weights Q0 := 0 ∈p∆Π , initial epoch m := 1.
2 |Π|/δ)/(Kτ )} for all m ≥ 0.
Define µm := min{1/2K , ln(16τm
m
2: for round t = 1, 2, . . . do
3:
Observe context xt ∈ X.
4:
(at , pt (at )) := Sample(xt , Qm−1 , πτm −1 , µm−1 ).
5:
Select action at and observe reward rt (at ) ∈ [0, 1].
6:
if t = τm then
7:
Let Qm be a solution to (OP) with history Ht and minimum probability µm .
8:
m := m + 1.
9:
end if
10: end for

Optimization Problem (OP)
d

Π
t (π)
Given a history Ht and minimum probability µm , define bπ := Reg
ψµm for ψ := 100, and find Q ∈ ∆
such that
X
Q(π)bπ ≤ 2K
(2)

b x∼H
∀π ∈ Π : E
t



π∈Π

1
µ
m
Q (π(x)|x)

5



≤ 2K + bπ .

(3)

3.1

Solving (OP) via Coordinate Descent

We now present a coordinate descent algorithm to solve (OP). The pseudocode is given in Algorithm 2.
Our analysis, as well as the algorithm itself, are based on a potential function which we use to measure
progress. The algorithm can be viewed as a form of coordinate descent applied to this same potential
function. The main idea of our analysis is to show that this function decreases substantially on every
iteration of this algorithm; since the function is nonnegative, this gives an upper bound on the total
number of iterations as expressed in the following theorem.
Theorem 3. Algorithm 2 (with Qinit := 0) halts in at most
solution Q to ( OP).

4 ln(1/(Kµm ))
µm

iterations, and outputs a

Algorithm 2 Coordinate Descent Algorithm
Require: History Ht , minimum probability µ, initial weights Qinit ∈ ∆Π .
1: Set Q := Qinit .
2: loop
3:
Define, for all π ∈ Π,
Vπ (Q)
Sπ (Q)
Dπ (Q)
if

4:
5:

6:
7:
8:

9:
10:
11:
12:

P

b x∼H [1/Qµ (π(x)|x)]
= E
t


b x∼Ht 1/(Qµ (π(x)|x))2
= E
= Vπ (Q) − (2K + bπ ).

π Q(π)(2K + bπ ) > 2K then
Replace Q by cQ, where

c := P

2K
< 1.
Q(π)(2K
+ bπ )
π

(4)

end if
if there is a policy π for which Dπ (Q) > 0 then
Add the (positive) quantity
Vπ (Q) + Dπ (Q)
απ (Q) =
2(1 − Kµ)Sπ (Q)

to Q(π) and leave all other weights unchanged.
else
Halt and output the current set of weights Q.
end if
end loop

3.2

Using an Optimization Oracle

We now show how to implement Algorithm 2 via AMO (c.f. Section 2.3).
Lemma 1. Algorithm 2 can be implemented using one call to AMO before the loop is started, and one
call for each iteration of the loop thereafter.
Proof. At the very beginning, before the loop is started, we compute the best empirical policy so far, πt ,
by calling AMO on the sequence of historical contexts and estimated reward vectors; i.e., on (xτ , r̂τ ), for
τ = 1, 2, . . . , t.
Next, we show that each iteration in the loop of Algorithm 2 can be implemented via one call to AMO.
Going over the pseudocode, first note that operations involving Q in Step 4 can be performed efficiently
since Q has sparse support. Note that the definitions in Step 3 don’t actually need to be computed for
all policies π ∈ Π, as long as we can identify a policy π for which Dπ (Q) > 0. We can identify such a
policy using one call to AMO as follows.
6

First, note that for any policy π, we have
b x∼Ht
Vπ (Q) = E

and

bπ =




t
1
1
1X
=
,
Qµ (π(x)|x)
t τ =1 Qµ (π(xτ )|xτ )

t
d t (π)
b t (πt )
Reg
R
1 X
=
−
r̂τ (π(xτ )).
ψµ
ψµ
ψµt τ =1

Now consider the sequence of historical contexts and reward vectors, (xτ , r̃τ ) for τ = 1, 2, . . . , t, where
for any action a we define


ψµ
1
+ r̂τ (a) .
(5)
r̃τ (a) :=
t Qµ (a|xτ )
It is easy to check that
t

Since 2K +

b t (πt )
R
ψµ

1 X
Dπ (Q) =
r̃τ (π(xτ )) −
ψµ τ =1

b t (πt )
R
2K +
ψµ

!

.

is a constant independent of π, we have
arg max Dπ (Q) = arg max
π∈Π

π∈Π

t
X

r̃τ (π(xτ )),

τ =1

and hence, calling AMO once on the sequence (xτ , r̃τ ) for τ = 1, 2, . . . , t, we obtain a policy that maximizes
Dπ (Q), and thereby identify a policy for which Dπ (Q) > 0 whenever one exists.

3.3

Epoch Schedule

Recalling
the setting of µm in Algorithm 1, Theorem 3 shows that Algorithm 2 solves (OP) with
p
if we use the epoch schedule τm = m (i.e., run
Õ( Kt/ ln(|Π|/δ)) calls to AMO in round t. Thus, p
Algorithm 2 in every round), then we get a total of Õ( KT 3/ ln(|Π|/δ)) calls to AMO over all T rounds.
This number can be dramatically reduced using a more carefully chosen epoch schedule.
p
Lemma 2. For the epoch schedule τm := 2m−1 , the total number of calls to AMO is Õ( KT / ln(|Π|/δ)).
Proof. The epoch schedule satisfies the requirement τm+1 p
≤ 2τm . With this epoch schedule, Algorithm 2
is run only O(log T ) times over T rounds, leading to Õ( KT / ln(|Π|/δ)) total calls to AMO over the
entire period.

3.4

Warm Start

We now present a different technique to reduce the number of calls to AMO. This is based on the
observation that practically speaking, it seems terribly wasteful, at the start of a new epoch, to throw out
the results of all of the preceding computations and to begin yet again from nothing. Instead, intuitively,
we expect computations to be more moderate if we begin again where we left off last, i.e., a “warm-start”
approach. Here, when Algorithm 2 is called at the end of epoch m, we use Qinit := Qm−1 (the previously
computed weights) rather than 0.
p
We can combine warm-start√with a different epoch schedule to guarantee Õ( KT / ln(|Π|/δ)) total
calls to AMO, spread across O( T ) calls to Algorithm 2.
Lemma 3. Define the epoch schedule (τ1 , τ2 ) := (3, 5) and τm := m2 for m ≥p3 (this satisfies τm+1 ≤
2τm ). With high probability,√the warm-start variant of Algorithm 1 makes Õ( KT / ln(|Π|/δ)) calls to
AMO over T rounds and O( T ) calls to Algorithm 2.

7

3.5

Computational Complexity

So far, we have only considered computational complexity in terms of the number of oracle calls. However,
the reduction also involves the creation of cost-sensitive classification examples, which must be accounted
for in the net computational cost. As observed in the proof of Lemma 1 (specifically Eq. (5)), this requires
the computation of the probabilities Qµ (a|xτ ) for τ = 1, 2, . . . , t when the oracle has to be invoked at
round
p t. According to Lemma 3, the support of the distribution Q at time t can be over at most
Õ( Kt/ ln(|Π|/δ))
p policies (same as the number of calls to AMO). This would suggest a computational
complexity of Õ( Kt3 / ln(|Π|/δ)) for querying the oracle at time t, resulting in an overall computation
cost scaling with T 2 .
We can, however, do better with some natural bookkeeping. Observe that at the start of round t, the
conditional distributions Q(a|xi ) for i = 1, 2, . . . , t − 1 can be represented as a table of size K × (t − 1),
where rows and columns correspond to actions and contexts. Upon receiving the new example
in round
p
t, the corresponding t-th column can be added to this table in time K · |supp(Q)| = Õ(K Kt/ ln(|Π|/δ))
(where supp(Q) ⊆ Π denotes the support of Q), using the projection operation described in Section 2.4.
Hence the net cost of these updates, as a function of K and T , scales with as (KT )3/2 . Furthermore,
the cost-sensitive examples needed for the AMO can be obtained by a simple table lookup now, since the
action probabilities are directly available. This involves O(Kt) table lookups when the oracle is invoked
at time t, and again results in an overall cost scaling as (KT )3/2 . Finally, we have to update the table
when the distribution Q is updated in Algorithm 2. If we find ourselves in the rescaling step 4, we can
simply store the constant c. When we enter step 8 of the algorithm, we can do a linear scan over the table,
rescaling and incrementing the entries. This also resutls in a cost of O(Kt) when the update happens at
time t, resulting in a net scaling as (KT )3/2 . Overall,
p we find that the computational complexity of our
algorithm, modulo the oracle running time, is Õ( (KT )3 / ln(|Π|/δ)).

3.6

A Lower Bound on the Support Size

An attractive feature of the coordinate descent algorithm, Algorithm 2, is that the number of oracle calls
is directly related to the number of policies in the support of Qm . Specifically, for the doubling schedule
m ))
of Section 3.3, Theorem 3 implies that we never have non-zero weights for more than 4 ln(1/(Kµ
policies
µm
in epoch m. Similarly, the total number of oracle calls for the warm-start approach in Section 3.4 bounds
the total number of policies which ever have non-zero weight over all T rounds. The support size of the
distributions Qm in Algorithm 1 is crucial to the computational complexity of sampling an action (Step 4
of Algorithm 1).
In this section, we demonstrate a lower bound showing that it is not possible to construct substantially
sparser distributions that also satisfy the low-variance constraint (3) in the optimization problem (OP).
To formally define the lower bound, fix an epoch schedule 0 = τ0 < τ1 < τ2 < · · · and consider the
following set of non-negative vectors over policies:
Qm :={Q ∈ ∆Π : Q satisfies Eq. (3) in round τm }.
(The distribution Qm computed by Algorithm 1 is in Qm .) Recall that supp(Q) denotes the support of
Q (the set of policies where Q puts non-zero entries). We have the following lower bound on |supp(Q)|.
Theorem 4. For any epoch schedule 0 = τ0 < τ1 < τ2 < · · · and any M ∈ N sufficiently large, there
exists a distribution D over X × [0, 1]A and a policy class Π such that, with probability at least 1 − δ,
s
!
KτM
inf
inf |supp(Q)| = Ω
.
m∈N: Q∈Qm
ln(|Π|τM /δ)
τm ≥τM /2

The proof of the theorem is deferred to Appendix E. In the context of our problem, this lower bound
shows that the bounds in Lemma 2 and Lemma 3 are unimprovable, since the number of calls to AMO
is at least the size of the support, given our mode of access to Π.
8

4

Regret Analysis

In this section, we outline the regret analysis for our algorithm ILOVETOCONBANDITS, with details
deferred to Appendix B and Appendix C.
b t (π) are controlled by (a bound on) the variance of
The deviations of the policy reward estimates R
b x∼H [·] replaced by
each term in Eq. (1): essentially the left-hand side of Eq. (3) from (OP), except with E
t
Ex∼DX [·]. Resolving this discrepancy is handled using deviation bounds, so Eq. (3) holds with Ex∼DX [·],
with worse right-hand side constants.
The rest of the analysis, which deviates from that of Randomized UCB, compares the expected regret
d t (π) using the variance constraints Eq. (3):
Reg(π) of any policy π with the estimated regret Reg
Lemma 4 (Informally). With high probability, for each m such that τm ≥ Õ(K log |Π|), each round t in
d t (π) + O(Kµm ).
epoch m, and each π ∈ Π, Reg(π) ≤ 2Reg

This lemma can easily be combined with the constraint Eq. (2) from (OP): since the weights Qm−1
P
d
used in any round t in epoch m satisfy π∈Π Qm−1 (π)Reg
τm −1 (π) ≤ ψ · 2Kµτm −1 , we obtain a bound
on the (conditionally) expected regret in round t using the above lemma: with high probability,
X
e m−1 Reg(π) ≤ O(Kµm−1 ).
Q
π∈Π

Summing these terms up over all T rounds and applying martingale concentration gives the final regret
bound in Theorem 2.

5

Analysis of the Optimization Algorithm

In this section, we give a sketch of the analysis of our main optimization algorithm for computing weights
Qm on each epoch as in Algorithm 2. As mentioned in Section 3.1, this analysis is based on a potential
function.
Since our attention for now is on a single epoch m, here and in what follows, when clear from context,
we drop m from our notation and write simply τ = τm , µ = µm , etc. Let UA be the uniform distribution
over the action set A. We define the following potential function for use on epoch m:
!
P
b x [RE (UA kQµ (· | x))]
E
π∈Π Q(π)bπ
Φm (Q) = τ µ
.
(6)
+
1 − Kµ
2K
The function in Eq. (6) is defined for all vectors Q ∈ ∆Π . Also, RE (pkq) denotes the unnormalized
relative entropy between two nonnegative vectors p and q over the action space (or any set) A:
X
RE (pkq) =
(pa ln(pa /qa ) + qa − pa ).
a∈A

This number is always nonnegative. Here, Qµ (·|x) denotes the “distribution” (which might not sum to
1) over A induced by Qµ for context x as given in Section 2.4. Thus, ignoring constants, this potential
function is a combination of two terms: The first measures how far from uniform are the distributions
induced by Qµ , and the second is an estimate of expected regret under Q since bπ is proportional to the
empirical regret of π. Making Φm small thus encourages Q to choose actions as uniformly as possible
while also incurring low regret — exactly the aims of our algorithm. The constants that appear in this
definition are for later mathematical convenience.
For further intuition, note that, by straightforward calculus, the partial derivative ∂Φm /∂Q(π) is
roughly proportional to the variance constraint for π given in Eq. (3) (up to a slight mismatch of constants). This shows that if this constraint is not satisfied, then ∂Φm /∂Q(π) is likely to be negative,
meaning that Φm can be decreased by increasing Q(π). Thus, the weight vector Q that minimizes Φm
satisfies the variance constraint for every policy π. It turns out that this minimizing Q also satisfies the
9

low regret constraint in Eq. (2), and also must sum to at most 1; in other words, it provides a complete
solution to our optimization problem. Algorithm 2 does not fully minimize Φm , but it is based roughly
on coordinate descent. This is because in each iteration one of the weights (coordinate directions) Q(π)
is increased. This weight is one whose corresponding partial derivative is large and negative.
To analyze the algorithm, we first argue that it is correct in the sense of satisfying the required
constraints, provided that it halts.
Lemma 5. If Algorithm 2 halts and outputs a weight vector Q, then the constraints Eq. (3) and Eq. (2)
must hold, and furthermore the sum of the weights Q(π) is at most 1.
The proof is rather straightforward: Following Step 4, Eq. (2) must hold, and also the weights must
sum to 1. And if the algorithm halts, then Dπ (Q) ≤ 0 for all π, which is equivalent to Eq. (3).
What remains is the more challenging task of bounding the number of iterations until the algorithm
does halt. We do this by showing that significant progress is made in reducing Φm on every iteration. To
begin, we show that scaling Q as in Step 4 cannot cause Φm to increase.
P
Lemma 6. Let Q be a weight vector such that π Q(π)(2K + bπ ) > 2K, and let c be as in Eq. (4). Then
Φm (cQ) ≤ Φm (Q).
Proof sketch. We consider Φm (cQ) as a function of c, and argue that its derivative (with respect to
c) at the value of c given in the lemma statement is always nonnegative. Therefore, by convexity, it is
nondecreasing for all values exceeding c. Since c < 1, this proves the lemma.
Next, we show that substantial progress will be made in reducing Φm each time that Step 8 is executed.
Lemma 7. Let Q denote a set of weights and suppose, for some policy π, that Dπ (Q) > 0. Let Q′ be
a new set of weights which is an exact copy of Q except that Q′ (π) = Q(π) + α where α = απ (Q) > 0.
Then
τ µ2
.
(7)
Φm (Q) − Φm (Q′ ) ≥
4(1 − Kµ)
Proof sketch. We first compute exactly the change in potential for general α. Next, we apply a secondorder Taylor approximation, which is maximized by the α used in the algorithm. The Taylor approximation, for this α, yields a lower bound which can be further simplified using the fact that Qµ (a|x) ≥ µ
always, and our assumption that Dπ (Q) > 0. This gives the bound stated in the lemma.
So Step 4 does not cause Φm to increase, and Step 8 causes Φm to decrease by at least the amount
given in Lemma 7. This immediately implies Theorem 3: for Qinit = 0, the initial potential is bounded by
τ µ ln(1/(Kµ))/(1 − Kµ), and it is never negative, so the number of times Step 8 is executed is bounded
by 4 ln(1/(Kµ))/µ as required.

5.1

Epoching and Warm Start

As shown in Section 2.3, the bound on the number of iterations of the algorithm from Theorem 3 also
gives a bound on the number of times the oracle is called. To reduce the number of oracle calls, one
approach is the “doubling trick” of Section 3.3, which enables
p us to bound the total combined number of
iterations of Algorithm 2 in the first T rounds is p
only Õ( KT / ln(|Π|/δ)). This means that the average
number of calls to the arg-max oracle is only Õ( K/(T ln(|Π|/δ))) per round, meaning that the oracle
is called far less than once per round, and in fact, at a vanishingly low rate.
We now turn to warm-start approach of Section 3.4, where in each epoch m + 1 we initialize the
coordinate descent algorithm with Qinit = Qm , i.e. the weights computed in the previous epoch m.
To analyze this, we bound how much the potential changes from Φm (Qm ) at the end of epoch m to
Φm+1 (Qm ) at the very start of epoch m + 1. This, combined with our earlier results regarding how
quickly Algorithm 2 drives down the potential, we are able to get an overall bound on the total number
of updates across T rounds.

10

Table 1: Progressive validation loss, best hyperparameter values, and running times of various algorithm
on RCV1.
Algorithm
P.V. Loss
Searched
Seconds

ǫ-greedy
0.148
0.1 = ǫ
17

Explore-first
0.081
2 × 105 first
2.6

Bagging
0.059
16 bags
275

LinUCB
0.128
103 dim, minibatch-10
212 × 103

Online Cover
0.053
cover n = 1
12

Supervised
0.051
nothing
5.3

Lemma 8. Let M be the largest integer for which τM+1 ≤ T . With probability at least 1 − 2δ, for all T ,
the total epoch-to-epoch increase in potential is
!
r
M
X
T ln(|Π|/δ)
(Φm+1 (Qm ) − Φm (Qm )) ≤ Õ
,
K
m=1
where M is the largest integer for which τM+1 ≤ T .
Proof sketch. The potential function, as written in Eq. (6), naturally breaks into two pieces whose
epoch-to-epoch changes can be bounded separately. Changes affecting the relative entropy term on the
left can be bounded, regardless of Qm , by taking advantage of the manner in which these distributions
are smoothed. For the other term on the right, it turns out that these epoch-to-epoch changes are related
to statistical quantities which can be bounded with high probability. Specifically, the total change in this
term is related first to how the estimated reward of the empirically best policy compares to the expected
reward of the optimal policy; and second, to how the reward received by our algorithm compares to that
of the optimal reward. From our regret analysis, we are able to show that both of these quantities will
be small with high probability.
This lemma, along with Lemma 7 can be used to further establish Lemma 3. We only provide an
intuitive sketch here, with the details deferred to the appendix. As we
p observe in Lemma 8, the total
amount that the potential increases across T rounds is at most Õ( T ln(|Π|/δ)/K). On the other
hand, Lemma 7 shows that each time Q is updated by Algorithm 2 the potential decreases by at least
Ω̃(ln(|Π|/δ)/K) (using our choice
p of µ). Therefore, the total number of updates of the algorithm totaled2
over all T rounds is at most Õ( KT / ln(|Π|/δ)). For instance,
√ if we use (τ1 , τ2 ) := (3, 5) and τm := m
T times in T rounds, and on each of those
for m ≥ 3, then the weight vector
Q
is
only
updated
about
p
rounds, Algorithm 2 requires Õ( K/ ln(|Π|/δ)) iterations, on average, giving the claim in Lemma 3.

6

Experimental Evaluation

In this section we evaluate a variant of Algorithm 1 against several baselines. While Algorithm 1 is
significantly more efficient than many previous approaches, the overall computational complexity is still
at least Õ((KT )1.5 ) plus the total cost of the oracle calls, as discussed in Section 3.5. This is markedly
larger than the complexity of an ordinary supervised learning problem where it is typically possible to
perform an O(1)-complexity update upon receiving a fresh example using online algorithms.
A natural solution is to use an online oracle that is stateful and accepts examples one by one. An
online cost-sensitive classification (CSC) oracle takes as input a weighted example and returns a predicted
class (corresponding to one of K actions in our setting). Since the oracle is stateful, it remembers and
uses examples from all previous calls in answering questions, thereby reducing the complexity of each
oracle invocation to O(1) as in supervised learning. Using several such oracles, we can efficiently track a
distribution over good policies and sample from it. We detail this approach (which we call Online Cover)
in the full version of the paper. The algorithm maintains a uniform distribution over a fixed number
n of policies where n is a parameter of the algorithm. Upon receiving a fresh example, it updates all
n policies with the suitable CSC examples (Eq. (5)). The specific CSC oracle we use is a reduction to
11

squared-loss regression (Algorithms 4 and 5 of Beygelzimer and Langford (2009)) which is amenable to
online updates. Our implementation is included in Vowpal Wabbit.6
Due to lack of public datasets for contextual bandit problems, we use a simple supervised-to-contextualbandit transformation (Dudı́k et al., 2011b) on the CCAT document classification problem in RCV1 (Lewis et al.,
2004). This dataset has 781265 examples and 47152 TF-IDF features. We treated the class labels as actions, and one minus 0/1-loss as the reward. Our evaluation criteria is progressive validation (Blum et al.,
1999) on 0/1 loss. We compare several baseline algorithms to Online Cover; all algorithms take advantage
of linear representations which are known to work well on this dataset. For each algorithm, we report
the result for the best parameter settings (shown in Table 6).
1. ǫ-greedy (Sutton and Barto, 1998) explores randomly with probability ǫ and otherwise exploits.
2. Explore-first is a variant that begins with uniform exploration, then switches to an exploit-only
phase.
3. A less common but powerful baseline is based on bagging: multiple predictors (policies) are trained
with examples sampled with replacement. Given a context, these predictors yield a distribution
over actions from which we can sample.
4. LinUCB (Auer, 2002; Chu et al., 2011) has been quite effective in past evaluations (Li et al., 2010;
Chapelle and Li, 2011). It is impractical to run “as is” due to high-dimensional matrix inversions,
so we report results for this algorithm after reducing to 1000 dimensions via random projections.
Still, the algorithm required 59 hours7 . An alternative is to use diagonal approximation to the
covariance, which runs substantially faster (≈1 hour), but gives a worse error of 0.137.
5. Finally, our algorithm achieves the best loss of 0.0530. Somewhat surprisingly, the minimum occurs
for us with a cover set of size 1—apparently for this problem the small decaying amount of uniform
random sampling imposed is adequate exploration. Prediction performance is similar with a larger
cover set.
All baselines except for LinUCB are implemented as a simple modification of Vowpal Wabbit. All
reported results use default parameters where not otherwise specified. The contextual bandit learning
algorithms all use a doubly robust reward estimator instead of the importance weighted estimators used
in our analysis Dudı́k et al. (2011b).
Because RCV1 is actually a fully supervised dataset, we can apply a fully supervised online multiclass
algorithm to solve it. We use a simple one-against-all implementation to reduce this to binary classification, yielding an error rate of 0.051 which is competitive with the best previously reported results.
This is effectively a lower bound on the loss we can hope to achieve with algorithms using only partial
information. Our algorithm is less than 2.3 times slower and nearly achieves the bound. Hence on this
dataset, very little further algorithmic improvement is possible.

7

Conclusions

In this paper we have presented the first practical algorithm to our knowledge that attains the statistically
optimal regret guarantee and is computationally efficient in the setting of general policy classes. A
remarkable feature of the algorithm is that the total number of oracle calls over all T rounds is sublinear—
a remarkable improvement over previous works in this setting. We believe that the online variant of the
approach which we implemented in our experiments has the right practical flavor for a scalable solution
to the contextual bandit problem. In future work, it would be interesting to directly analyze the Online
Cover algorithm.
6 http://hunch.net/
7 The

~ vw. The implementation is in the file cbify.cc and is enabled using --cover.
linear algebra routines are based on Intel MKL package.

12

Acknowledgements
We thank Dean Foster and Matus Telgarsky for helpful discussions. Part of this work was completed
while DH and RES were visiting Microsoft Research.



We study the problem of multiclass classification with an extremely large number
of classes (k), with the goal of obtaining train and test time complexity logarithmic in the number of classes. We develop top-down tree construction approaches
for constructing logarithmic depth trees. On the theoretical front, we formulate a
new objective function, which is optimized at each node of the tree and creates
dynamic partitions of the data which are both pure (in terms of class labels) and
balanced. We demonstrate that under favorable conditions, we can construct logarithmic depth trees that have leaves with low label entropy. However, the objective
function at the nodes is challenging to optimize computationally. We address the
empirical problem with a new online decision tree construction procedure. Experiments demonstrate that this online algorithm quickly achieves improvement in
test error compared to more common logarithmic training time approaches, which
makes it a plausible method in computationally constrained large-k applications.

1

Introduction

The central problem of this paper is computational complexity in a setting where the number of
classes k for multiclass prediction is very large. Such problems occur in natural language (Which
translation is best?), search (What result is best?), and detection (Who is that?) tasks. Almost all
machine learning algorithms (with the exception of decision trees) have running times for multiclass
classification which are O(k) with a canonical example being one-against-all classifiers [1].
In this setting, the most efficient possible accurate approach is given by information theory [2].
In essence, any multiclass classification algorithm must uniquely specify the bits of all labels that
it predicts correctly on. Consequently, Kraft’s inequality ([2] equation 5.6) implies that the expected computational complexity of predicting correctly is Ω(H(Y )) per example where H(Y ) is
the Shannon entropy of the label. For the worst case distribution on k classes, this implies Ω(log(k))
computation is required.
Hence, our goal is achieving O(log(k)) computational time per example1 for both training and
testing, while effectively using online learning algorithms to minimize passes over the data.
The goal of logarithmic (in k) complexity naturally motivates approaches that construct a logarithmic depth hierarchy over the labels, with one label per leaf. While this hierarchy is sometimes
available through prior knowledge, in many scenarios it needs to be learned as well. This naturally
leads to a partition problem which arises at each node in the hierarchy. The partition problem is
finding a classifier: c : X → {−1, 1} which divides examples into two subsets with a purer set of
labels than the original set. Definitions of purity vary, but canonical examples are the number of
labels remaining in each subset, or softer notions such as the average Shannon entropy of the class
labels. Despite resulting in a classifier, this problem is fundamentally different from standard binary
classification. To see this, note that replacing c(x) with −c(x) is very bad for binary classification,
but has no impact on the quality of a partition2 . The partition problem is fundamentally non-convex
1
2

Throughout the paper by logarithmic time we mean logarithmic time per example.
The problem bears parallels to clustering in this regard.

1

for symmetric classes since the average c(x)−c(x)
of c(x) and −c(x) is a poor partition (the always-0
2
function places all points on the same side).
The choice of partition matters in problem dependent ways. For example, consider examples on a
line with label i at position i and threshold classifiers. In this case, trying to partition class labels
{1, 3} from class label 2 results in poor performance.

accuracy

The partition problem is typically solved for decision tree learning via an enumerate-and-test approach amongst a small set of possible classifiers (see e.g. [3]). In the multiclass setting, it is
desirable to achieve substantial error reduction for each node in the tree which motivates using a richer set of classifiers in the nodes to minimize the number of nodes, and thereby decrease the computational complexity. The main theoretical contribution of this work is to establish a boosting algorithm for learning trees with O(k) nodes and O(log k) depth, thereby addressing the goal of logarithmic time train and test complexity. Our main theoretical result,
presented in Section 2.3, generalizes a binary boosting-by-decision-tree theorem [4] to multiclass boosting. As in all boosting results, performance is critically dependent on the quality
of the weak learner, supporting intuition that we need sufficiently rich partitioners at nodes.
The approach uses a new objective for decision tree learning, which we optimize at each
node of the tree. The objective and its theoretical properties are presented in Section 2.
A complete system with multiple partitions
LOMtree vs one−against−all
could be constructed top down (as the boost1
OAA
ing theorem) or bottom up (as Filter tree [5]).
LOMtree
A bottom up partition process appears impossi0.8
ble with representational constraints as shown
in Section 6 in the Supplementary material so
we focus on top-down tree creation.
0.6
Whenever there are representational constraints
on partitions (such as linear classifiers), finding a strong partition function requires an efficient search over this set of classifiers. Ef0.2
ficient searches over large function classes are
routinely performed via gradient descent tech0
niques for supervised learning, so they seem
26
105
1000
21841 105033
number of classes
like a natural candidate. In existing literature,
Figure 1: A comparison of One-Against- examples for doing this exist when the problem
All (OAA) and the Logarithmic Online Multi- is indeed binary, or when there is a prespeciclass Tree (LOMtree) with One-Against-All con- fied hierarchy over the labels and we just need
strained to use the same training time as the to find partitioners aligned with that hierarchy.
LOMtree by dataset truncation and LOMtree con- Neither of these cases applies—we have multistrained to use the same representation complex- ple labels and want to dynamically create the
ity as One-Against-All. As the number of class choice of partition, rather than assuming that
labels grows, the problem becomes harder and the one was handed to us. Does there exist a purity criterion amenable to a gradient descent apLOMtree becomes more dominant.
proach? The precise objective studied in theory
fails this test due to its discrete nature, and even natural approximations are challenging to tractably
optimize under computational constraints. As a result, we use the theoretical objective as a motivation and construct a new Logarithmic Online Multiclass Tree (LOMtree) algorithm for empirical
evaluation.
0.4

Creating a tree in an online fashion creates a new class of problems. What if some node is initially
created but eventually proves useless because no examples go to it? At best this results in a wasteful
solution, while in practice it starves other parts of the tree which need representational complexity.
To deal with this, we design an efficient process for recycling orphan nodes into locations where
they are needed, and prove that the number of times a node is recycled is at most logarithmic in the
number of examples. The algorithm is described in Section 3 and analyzed in Section 3.1.
And is it effective? Given the inherent non-convexity of the partition problem this is unavoidably
an empirical question which we answer on a range of datasets varying from 26 to 105K classes in
Section 4. We find that under constrained training times, this approach is quite effective compared
to all baselines while dominating other O(log k) train time approaches.
What’s new? To the best of our knowledge, the splitting criterion, the boosting statement, the
LOMtree algorithm, the swapping guarantee, and the experimental results are all new here.

2

1.1

Prior Work

Only a few authors address logarithmic time training. The Filter tree [5] addresses consistent (and
robust) multiclass classification, showing that it is possible in the statistical limit. The Filter tree
does not address the partition problem as we do here which as shown in our experimental section is
often helpful. The partition finding problem is addressed in the conditional probability tree [6], but
that paper addresses conditional probability estimation. Conditional probability estimation can be
converted into multiclass prediction [7], but doing so is not a logarithmic time operation.
Quite a few authors have addressed logarithmic testing time while allowing training time to be O(k)
or worse. While these approaches are intractable on our larger scale problems, we describe them
here for context. The partition problem can be addressed by recursively applying spectral clustering
on a confusion graph [8] (other clustering approaches include [9]). Empirically, this approach has
been found to sometimes lead to badly imbalanced splits [10]. In the context of ranking, another
approach uses k-means hierarchical clustering to recover the label sets for a given partition [11].
The more recent work [12] on the multiclass classification problem addresses it via sparse output
coding by tuning high-cardinality multiclass categorization into a bit-by-bit decoding problem. The
authors decouple the learning processes of coding matrix and bit predictors and use probabilistic
decoding to decode the optimal class label. The authors however specify a class similarity which is
O(k 2 ) to compute (see Section 2.1.1 in [12]), and hence this approach is in a different complexity
class than ours (this is also born out experimentally). The variant of the popular error correcting
output code scheme for solving multi-label prediction problems with large output spaces under the
assumption of output sparsity was also considered in [13]. Their approach in general requires O(k)
running time to decode since, in essence, the fit of each label to the predictions must be checked
and there are O(k) labels. Another approach [14] proposes iterative least-squares-style algorithms
for multi-class (and multi-label) prediction with relatively large number of examples and data dimensions, and the work of [15] focusing in particular on the cost-sensitive multiclass classification.
Both approaches however have O(k) training time.
Decision trees are naturally structured to allow logarithmic time prediction. Traditional decision
trees often have difficulties with a large number of classes because their splitting criteria are not
well-suited to the large class setting. However, newer approaches [16, 17] have addressed this effectively at significant scales in the context of multilabel classification (multilabel learning, with
missing labels, is also addressed in [18]). More specifically, the first work [16] performs brute force
optimization of a multilabel variant of the Gini index defined over the set of positive labels in the
node and assumes label independence during random forest construction. Their method makes fast
predictions, however has high training costs [17]. The second work [17] optimizes a rank sensitive
loss function (Discounted Cumulative Gain). Additionally, a well-known problem with hierarchical
classification is that the performance significantly deteriorates lower in the hierarchy [19] which
some authors solve by biasing the training distribution to reduce error propagation while simultaneously combining bottom-up and top-down approaches during training [20].
The reduction approach we use for optimizing partitions implicitly optimizes a differential objective.
A non-reductive approach to this has been tried previously [21] on other objectives yielding good
results in a different context.

2

Framework and theoretical analysis

In this section we describe the essential elements of the approach, and outline the theoretical properties of the resulting framework. We begin with high-level ideas.
2.1

Setting

We employ a hierarchical approach for learning a multiclass decision tree structure, training this
structure in a top-down fashion. We assume that we receive examples x ∈ X ⊆ Rd , with labels
y ∈ {1, 2, . . . , k}. We also assume access to a hypothesis class H where each h ∈ H is a binary
classifier, h : X 7→ {−1, 1}. The overall objective is to learn a tree of depth O(log k), where
each node in the tree consists of a classifier from H. The classifiers are trained in such a way that
hn (x) = 1 (hn denotes the classifier in node n of the tree3 ) means that the example x is sent to the
right subtree of node n, while hn (x) = −1 sends x to the left subtree. When we reach a leaf, we
predict according to the label with the highest frequency amongst the examples reaching that leaf.
3
Further in the paper we skip index n whenever it is clear from the context that we consider a fixed tree
node.

3

In the interest of computational complexity, we want to encourage the number of examples going
to the left and right to be fairly balanced. For good statistical accuracy, we want to send examples
of class i almost exclusively to either the left or the right subtree, thereby refining the purity of the
class distributions at subsequent levels in the tree. The purity of a tree node is therefore a measure
of whether the examples of each class reaching the node are then mostly sent to its one child node
(pure split) or otherwise to both children (impure split). The formal definitions of balancedness and
purity are introduced in Section 2.2. An objective expressing both criteria4 and resulting theoretical
properties are illustrated in the following sections. A key consideration in picking this objective is
that we want to effectively optimize it over hypotheses h ∈ H, while streaming over examples in
an online fashion5 . This seems unsuitable with some of the more standard decision tree objectives
such as Shannon or Gini entropy, which leads us to design a new objective. At the same time, we
show in Section 2.3 that under suitable assumptions, optimizing the objective also leads to effective
reduction of the average Shannon entropy over the entire tree.
2.2

An objective and analysis of resulting partitions

We now define a criterion to measure the quality of a hypothesis h ∈ H in creating partitions at a
fixed node n in the tree. Let πi denotes the proportion of label i amongst the examples reaching this
node. Let P (h(x) > 0) and P (h(x) > 0|i) denote the fraction of examples reaching n for which
h(x) > 0, marginally and conditional on class i respectively. Then we define the objective6 :
k
X
J(h) = 2
πi |P (h(x) > 0) − P (h(x) > 0|i)| .
(1)
i=1

We aim to maximize the objective J(h) to obtain high quality partitions. Intuitively, the objective
encourages the fraction of examples going to the right from class i to be substantially different from
the background fraction for each class i. As a concrete simple scenario, if P (h(x) > 0) = 0.5 for
some hypothesis h, then the objective prefers P (h(x) > 0|i) to be as close to 0 or 1 as possible for
each class i, leading to pure partitions. We now make these intuitions more formal.
Definition 1 (Purity). The hypothesis h ∈ H induces a pure split if
k
X
α :=
πi min(P (h(x) > 0|i), P (h(x) < 0|i)) ≤ δ,
i=1

where δ ∈ [0, 0.5), and α is called the purity factor.
In particular, a partition is called maximally pure if α = 0, meaning that each class is sent exclusively
to the left or the right. We now define a similar definition for the balancedness of a split.
Definition 2 (Balancedness). The hypothesis h ∈ H induces a balanced split if
c ≤ P (h(x) > 0) ≤ 1 − c,
{z
}
|
=β

where c ∈ (0, 0.5], and β is called the balancing factor.
A partition is called maximally balanced if β = 0.5, meaning that an equal number of examples
are sent to the left and right children of the partition. The balancing factor and the purity factor
are related as shown in Lemma 1 (the proofs of Lemma 1 and the following lemma (Lemma 2) are
deferred to the Supplementary material).
Lemma 1. For any hypothesis h, and any distribution over examples (x, y), the purity factor α and
the balancing factor β satisfy α ≤ min{(2 − J(h))/(4β) − β, 0.5}.
A partition is called maximally pure and balanced if it satisfies both α = 0 and β = 0.5. We see
that J(h) = 1 for a hypothesis h inducing a maximally pure and balanced partition as captured in
the next lemma. Of course we do not expect to have hypotheses producing maximally pure and
balanced splits in practice.
Lemma 2. For any hypothesis h : X 7→ {−1, 1}, the objective J(h) satisfies J(h) ∈ [0, 1].
Furthermore, if h induces a maximally pure and balanced partition then J(h) = 1.
4

We want an objective to achieve its optimum for simultaneously pure and balanced split. The standard
entropy-based criteria, such as Shannon or Gini entropy, as well as the criterion we will propose, posed in
Equation 1, satisfy this requirement (for the entropy-based criteria see [4], for our criterion see Lemma 2).
5
Our algorithm could also be implemented as batch or streaming, where in case of the latter one can for
example make one pass through the data per every tree level, however for massive datasets making multiple
passes through the data is computationally costly, further justifying the need for an online approach.
6
The proposed objective function exhibits some similarities with the so-called Carnap’s measure [22, 23]
used in probability and inductive logic.

4

2.3

Quality of the entire tree

The above section helps us understand the quality of an individual split produced by effectively
maximizing J(h). We next reason about the quality of the entire tree as we add more and more
nodes. We measure the quality of trees using the average entropy over all the leaves in the tree, and
track the decrease of this entropy as a function of the number of nodes. Our analysis extends the
theoretical analysis in [4], originally developed to show the boosting properties of the decision trees
for binary classification problems, to the multiclass classification setting.
Given a tree T , we consider the entropy function Gt as the measure of the quality of tree:


k
X X
1
Gt =
wl
πl,i ln
πl,i
i=1
l∈L

where πl,i ’s are the probabilities that a randomly chosen data point x drawn from P, where P is
a fixed target distribution over X , has label i given that x reaches node l, L denotes the set of all
tree leaves, t denotes the number of internal tree nodes, and wl is the weight
P of leaf l defined as the
probability a randomly chosen x drawn from P reaches leaf l (note that l∈L wl = 1).
We next state the main theoretical result of this paper (it is captured in Theorem 1). We adopt
the weak learning framework. The weak hypothesis assumption, captured in Definition 3, posits that
each node of the tree T has a hypothesis h in its hypothesis class H which guarantees simultaneously
a ”weak” purity and a ”weak” balancedness of the split on any distribution P over X . Under this
assumption, one can use the new decision tree approach to drive the error below any threshold.
Definition 3 (Weak Hypothesis Assumption). Let m denote any node of the tree T , and let βm =
P (hm (x) > 0) and Pm,i = P (hm (x) > 0|i). Furthermore, let γ ∈ R+ be such that for all m,
γ ∈ (0, min(βm , 1 − βm )]. We say that the weak hypothesis assumption is satisfied when for any
distribution P over X at each node m of the tree T there exists a hypothesis hm ∈ H such that
Pk
J(hm )/2 = i=1 πm,i |Pm,i − βm | ≥ γ.
Theorem 1. Under the Weak Hypothesis Assumption, for any α ∈ [0, 1], to obtain Gt ≤ α it suffices
to make t ≥ (1/α)

4(1−γ)2 ln k
γ2

splits.

We defer the proof of Theorem 1 to the Supplementary material and provide its sketch now. The
analysis studies a tree construction algorithm where we recursively find the leaf node with the highest
weight, and choose to split it into two children. Let n be the heaviest leaf at time t. Consider splitting
it to two children. The contribution of node n to the tree entropy changes after it splits. This change
(entropy reduction) corresponds to a gap in the Jensen’s inequality applied to the concave function,
and thus can further be lower-bounded (we use the fact that Shannon entropy is strongly concave
with respect to `1 -norm (see e.g., Example 2.5 in Shalev-Shwartz [24])). The obtained lower-bound
turns out to depend proportionally on J(hn )2 . This implies that the larger the objective J(hn )
is at time t, the larger the entropy reduction ends up being, which further reinforces intuitions to
maximize J. In general, it might not be possible to find any hypothesis with a large enough objective
J(hn ) to guarantee sufficient progress at this point so we appeal to a weak learning assumption. This
assumption can be used to further lower-bound the entropy reduction and prove Theorem 1.

3

The LOMtree Algorithm

The objective function of Section 2 has another convenient form which yields a simple online algorithm for tree construction and training. Note that Equation 1 can be written (details are shown in
Section 12 in the Supplementary material) as
J(h) = 2Ei [|Ex [1(h(x) > 0)] − Ex [1(h(x) > 0|i)]|].
Maximizing this objective is a discrete optimization problem that can be relaxed as follows
J(h) = 2Ei [|Ex [h(x)] − Ex [h(x)|i]|],
where Ex [h(x)|i] is the expected score of class i.
We next explain our empirical approach for maximizing the relaxed objective. The empirical estimates of the expectations can be easily stored and updated online in every tree node. The decision
whether to send an example reaching a node to its left or right child node is based on the sign of the
difference between the two expectations: Ex [h(x)] and Ex [h(x)|y], where y is a label of the data
point, i.e. when Ex [h(x)]−Ex [h(x)|y] > 0 the data point is sent to the left, else it is sent to the right.
This procedure is conveniently demonstrated on a toy example in Section 13 in the Supplement.
During training, the algorithm assigns a unique label to each node of the tree which is currently a
leaf. This is the label with the highest frequency amongst the examples reaching that leaf. While
5

Algorithm 1 LOMtree algorithm (online tree training)
Input: regression algorithm R, max number of tree non-leaf nodes T , swap resistance RS
Subroutine SetNode (v)
mv = ∅ (mv (y) - sum of the scores for class y)
lv = ∅ (lv (y) - number of points of class y reaching v)
nv = ∅ (nv (y) - number of points of class y which are used to train regressor in v)
ev = ∅ (ev (y) - expected score for class y)
Ev = 0 (expected total score)
Cv = 0 (the size of the smallest leaf7 in the subtree with root v)
Subroutine UpdateC (v)
While (v 6= r AND CPARENT(v) 6= Cv )
v = PARENT(v); Cv = min(CLEFT(v) , CRIGHT(v) )8
Subroutine Swap (v)
Find a leaf s for which (Cs = Cr )
sPA=PARENT(s); sGPA= GRANDPA(s); sSIB=SIBLING(s)9
If (sPA = LEFT(sGPA )) LEFT(sGPA ) = sSIB Else RIGHT(sGPA ) = sSIB
UpdateC (sSIB ); SetNode (s); LEFT(v) = s; SetNode (sPA ); RIGHT(v) = sPA
Create root r = 0: SetNode (r); t = 1
For each example (x, y) do
Set j = r
Do
If (lj (y) = ∅)
mj (y) = 0; lj (y) = 0; nj (y) = 0; ej (y) = 0
lj (y)++
If(j is a leaf)
If(lj has at least 2 non-zero entries)
If(t<T OR Cj−maxi lj (i)>RS (Cr+1))
If (t<T )
SetNode (LEFT(j)); SetNode (RIGHT(j)); t++
Else Swap(j)
CLEFT(j)=bCj /2c; CRIGHT(j)=Cj−CLEFT(j) ; UpdateC (LEFT(j))
If(j is not a leaf)
If (Ej > ej (y)) c = −1 Else c = 1
Train hj with example (x, c): R(x, c)
Pk
mj (i) 10
nj (y) ++; mj (y) += hj (x); ej (y) = mj (y)/nj (y); Ej = Pi=1
k
i=1 nj (i)
Set j to the child of j corresponding to hj
Else
Cj ++
break

testing, a test example is pushed down the tree along the path from the root to the leaf, where in each
non-leaf node of the path its regressor directs the example either to the left or right child node. The
test example is then labeled with the label assigned to the leaf that this example descended to.
The training algorithm is detailed in Algorithm 1 where each tree node contains a classifier (we use
linear classifiers), i.e. hj is the regressor stored in node j and hj (x) is the value of the prediction
of hj on example x11 . The stopping criterion for expanding the tree is when the number of non-leaf
nodes reaches a threshold T .
3.1 Swapping
Consider a scenario where the current training example descends to leaf j. The leaf can split (create
two children) if the examples that reached it in the past were coming from at least two different
7

The smallest leaf is the one with the smallest total number of data points reaching it in the past.
PARENT (v), LEFT (v) and RIGHT (v) denote resp. the parent, and the left and right child of node v.
9
GRANDPA (v) and SIBLING (v) denote respectively the grandparent of node v and the sibling of node v, i.e.
the node which has the same parent as v.
10
In the implementation both sums are stored as variables thus updating Ev takes O(1) computations.
11
We also refer to this prediction value as the ’score’ in this section.
8

6

r
...
j

r

...
...

...

...

sGPA
...
s

...

j

sPA

s

sSIB

...
...

sPA

sGPA
...
sSIB
...
...

...
...
Figure 2: Illustration of the swapping procedure. Left: before the swap, right: after the swap.
classes. However, if the number of non-leaf nodes of the tree reaches threshold T , no more nodes
can be expanded and thus j cannot create children. Since the tree construction is done online, some
nodes created at early stages of training may end up useless because no examples reach them later
on. This prevents potentially useful splits such as at leaf j. This problem can be solved by recycling
orphan nodes (subroutine Swap in Algorithm 1). The general idea behind node recycling is to allow
nodes to split if a certain condition is met. In particular, node j splits if the following holds:
Cj −

max
i∈{1,2,...,k}

lj (i) > RS (Cr + 1),

(2)

where r denotes the root of the entire tree, Cj is the size of the smallest leaf in the subtree with root
j, where the smallest leaf is the one with the smallest total number of data points reaching it in the
past, lj is a k-dimensional vector of non-negative integers where the ith element is the count of the
number of data points with label i reaching leaf j in the past, and finally RS is a “swap resistance”.
The subtraction of maxi∈{1,2,...,k} lj (i) in Equation 2 ensures that a pure node will not be recycled.
If the condition in Inequality 2 is satisfied, the swap of the nodes is performed where an orphan
leaf s, which was reached by the smallest number of examples in the past, and its parent sPA are
detached from the tree and become children of node j whereas the old sibling sSIB of an orphan node
s becomes a direct child of the old grandparent sGPA . The swapping procedure is shown in Figure 2.
The condition captured in the Inequality 2 allows us to prove that the number of times any given
node is recycled is upper-bounded by the logarithm of the number of examples whenever the swap
resistance is 4 or more (Lemma 3).
Lemma 3. Let the swap resistance RS be greater or equal to 4. Then for all sequences of examples,
the number of times Algorithm 1 recycles any given node is upper-bounded by the logarithm (with
base 2) of the sequence length.

4

Experiments

We address several hypotheses experimentally.
1. The LOMtree algorithm achieves true logarithmic time computation in practice.
2. The LOMtree algorithm is competitive with or better than all other logarithmic train/test
time algorithms for multiclass classification.
3. The LOMtree algorithm has statistical performance close to more common O(k) approaches.
To address these hypotheses, we conTable 1: Dataset sizes.
ducted experiments on a variety of
Isolet Sector Aloi ImNet ODP
benchmark multiclass datasets: Isosize
52.3MB 19MB 17.7MB104GB12 3GB
let, Sector, Aloi, ImageNet (Im# features 617 54K 128
6144 0.5M
Net) and ODP13 . The details of the
# examples 7797 9619 108K 14.2M 1577418
datasets are provided in Table 1. The
datasets were divided into training
# classes
26
105 1000 ∼22K ∼105K
(90%) and testing (10%). Furthermore, 10% of the training dataset was
used as a validation set.
The baselines we compared LOMtree with are a balanced random tree of logarithmic depth (Rtree)
and the Filter tree [5]. Where computationally feasible, we also compared with a one-against-all
classifier (OAA) as a representative O(k) approach. All methods were implemented in the Vowpal
Wabbit [25] learning system and have similar levels of optimization. The regressors in the tree nodes
for LOMtree, Rtree, and Filter tree as well as the OAA regressors were trained by online gradient
descent for which we explored step sizes chosen from the set {0.25, 0.5, 0.75, 1, 2, 4, 8}. We used
12
13

compressed
The details of the source of each dataset are provided in the Supplementary material.

7

linear regressors. For each method we investigated training with up to 20 passes through the data and
we selected the best setting of the parameters (step size and number of passes) as the one minimizing
the validation error. Additionally, for the LOMtree we investigated different settings of the stopping
criterion for the tree expansion: T = {k − 1, 2k − 1, 4k − 1, 8k − 1, 16k − 1, 32k − 1, 64k − 1},
and swap resistance RS = {4, 8, 16, 32, 64, 128, 256}.
In Table 2 and 3 we report respectively train time and per-example test time (the best performer is
indicated in bold). Training time (and later reported test error) is not provided for OAA on ImageNet
and ODP due to intractability14 -both are petabyte scale computations15 .
Table 2: Training time on selected problems. Table 3: Per-example test time on all problems.
Isolet Sector
Aloi
Isolet Sector Aloi ImNet ODP
LOMtree 16.27s 12.77s 51.86s
LOMtree 0.14ms 0.13ms 0.06ms 0.52ms 0.26ms
OAA
19.58s 18.37s 11m2.43s
OAA 0.16 ms 0.24ms 0.33ms 0.21s 1.05s

log2(time ratio)

The first hypothesis is consistent with the experimental results. Time-wise LOMtree significantly
outperforms OAA due to building only close-to logarithmic depth trees. The improvement in the
training time increases with the number of classes in the classification problem. For instance on Aloi
training with LOMtree is 12.8 times faster than with OAA. The same can be said about the test time,
where the per-example test time for Aloi, ImageNet and ODP are respectively 5.5, 403.8 and 4038.5
times faster than OAA. The significant advantage of LOMtree over OAA is also captured in Figure 3.
Next, in Table 4 (the best logarithmic time perLOMtree vs one−against−all
former is indicated in bold) we report test error
12
of logarithmic train/test time algorithms. We
10
also show the binomial symmetrical 95% confidence intervals for our results. Clearly the sec8
ond hypothesis is also consistent with the experimental results. Since the Rtree imposes a
6
random label partition, the resulting error it ob4
tains is generally worse than the error obtained
by the competitor methods including LOMtree
2
which learns the label partitioning directly from
the data. At the same time LOMtree beats Fil6
8
10
12
14
16
ter tree on every dataset, though for ImageNet
log2(number of classes)
Figure 3: Logarithm of the ratio of per-example and ODP (both have a high level of noise) the
advantage of LOMtree is not as significant.
test times of OAA and LOMtree on all problems.
Table 4: Test error (%) and confidence interval on all problems.
LOMtree
Rtree
Filter tree
OAA
Isolet 6.36±1.71 16.92±2.63 15.10±2.51 3.56±1.30%
Sector 16.19±2.33 15.77±2.30 17.70±2.41 9.17±1.82%
Aloi 16.50±0.70 83.74±0.70 80.50±0.75 13.78±0.65%
ImNet 90.17±0.05 96.99±0.03 92.12±0.04
NA
ODP 93.46±0.12 93.85±0.12 93.76±0.12
NA
The third hypothesis is weakly consistent with the empirical results. The time advantage of LOMtree
comes with some loss of statistical accuracy with respect to OAA where OAA is tractable. We
conclude that LOMtree significantly closes the gap between other logarithmic time methods and
OAA, making it a plausible approach in computationally constrained large-k applications.

5

Conclusion

The LOMtree algorithm reduces the multiclass problem to a set of binary problems organized in a
tree structure where the partition in every tree node is done by optimizing a new partition criterion
online. The criterion guarantees pure and balanced splits leading to logarithmic training and testing
time for the tree classifier. We provide theoretical justification for our approach via a boosting
statement and empirically evaluate it on multiple multiclass datasets. Empirically, we find that this
is the best available logarithmic time approach for multiclass classification problems.
14
Note however that the mechanics of testing datastes are much easier - one can simply test with effectively
untrained parameters on a few examples to measure the test speed thus the per-example test time for OAA on
ImageNet and ODP is provided.
15
Also to the best of our knowledge there exist no state-of-the-art results of the OAA performance on these
datasets published in the literature.

8

Acknowledgments
We would like to thank Alekh Agarwal, Dean Foster, Robert Schapire and Matus Telgarsky for
valuable discussions.




We show how to reduce the process of predicting conditional quantiles (and the median in particular) to solving classification. The accompanying theoretical statement shows that the regret
of the classifier bounds the regret of the quantile regression under a quantile loss. We also
test this reduction empirically against existing
quantile regression methods on large real-world
datasets and discover that it provides state-of-theart performance.

1 Introduction
Regression is the problem of estimating a mapping from
some feature space X to a real-valued output Y , given a
finite sample of the form (x, y) drawn from a distribution
D over X × Y . Typically, the goal of regression is to minimize the squared-error loss over the distribution D, that
is, E(x,y)∼D (y − f (x))2 . One standard justification for
this form of regression is that the minimizer is the mean:
f ∗ (x) = Ey∼D|x [y].

Bianca Zadrozny
Universidade Federal Fluminense
Rua Passo da Pátria 156, Bl. E, 3 andar
Niterói, RJ 24210-240, Brazil
bianca@ic.uff.br

sons for doing quantile regression as opposed or in addition
to typical regression include:
1. Quantiles tend to behave well under noise. For instance, the median (i.e. the 1/2-quantile) equals the
mean under Gaussian noise, but the median is often inherently more robust to heavy tailed and non-Gaussian
noise;
2. As mentioned above, many important practical problems are naturally expressed in terms of quantiles. For
instance, wallet estimation – e.g. estimating the potential amount of money a customer can spend on computer hardware, as opposed to the expected amount –
can be done by looking at the conditional upper quantiles of expenditures [10, 9]. Quantile regression also
been applied to many other problems in Econometrics,
Sociology and Ecology, among other fields [5, 6];
3. The actual distribution of conditional noise can be estimated as required for some applications [8] using
quantile regression.

However, there are many important applications for which
mean estimates are either irrelevant or insufficient, and
quantiles (also known as general order statistics) are the
main quantities of interest. For instance, consider trying
to assess the risk of a business proposal. Estimates of the
lower quantiles of the conditional return distribution would
give a better indication of how worthwhile the proposal is
than a simple estimate of the mean return (which could be
too high because of very unlikely high profits).

This paper shows that the quantile regression problem can
be reduced to classification. The Quanting algorithm that
we introduce takes as input an instance of quantile regression and outputs a family of classification problems such
that solving the latter problems with small average error
leads to a provably accurate estimate of the conditional
quantile. Reducing quantile to classification automatically
gives us access to a large array of quantile regression methods, since the reduction applies to any existing or future
classification method.

The process of estimating the quantiles of a conditional distribution is known as quantile regression. More specifically, the goal of quantile regression is to obtain estimates
on the q-quantiles of the conditional distribution D|x. Intuitively, q-quantiles for different q describe different segments of the conditional distribution D|x and thus offer
more refined information about the data at hand. Other rea-

We compare empirically the Quanting algorithm with other
methods for quantile regression in the literature. Koenker
[5] has developed a linear quantile regression method,
while Takeuchi et al.[11] have recently devised a kernelbased quantile estimation method. Our approach, which is
intrinsically non-linear and conceptually simpler, compares
favorably with the existing alternatives in our experiments.

4

Algorithm 1 Quanting-train (importance-weighted classifier learning algorithm A, training set S, quantile q)
1. For t in [0, 1]

q=0.5
q=0.75
q=0.25

2

quantile loss

3

(a) St = {};
(b) For each (x, y) in S:
St = St ∪ {(x, I(y ≥ t), qI(y ≥ t) +
(1 − q)I(y < t))}
(c) ct = A(St )

1

2. Return the set of classifiers {ct }

0

ball loss” [11]. Pictorially, this is a tilted absolute loss as in
figure 1. Mathematically, this is Ex,y∼D lq (y, f (x)), where
−4

−2

0

2

4

lq (y, f (x))

= q[y − f (x)]I(y ≥ f (x))
+(1 − q)[f (x) − y]I(y < f (x)), (1)

error = prediction − actual

and I(·) = 1 if its argument is true and 0 otherwise.
Figure 1: Loss functions which induce quantile regression.

2 Basic Details
The quantile regression problem is defined in a setting
where we have a measure D over a set of features X and
real-valued outputs Y .
Definition 1 (Conditional q-quantile) Let 0 ≤ q ≤ 1
f = f (x) is a conditional q-quantile (or conditional qorder statistic) for D if for (D-almost every) x ∈ X
D(y ≤ f (x)|x) ≥ q and D(y ≥ f (x)|x) ≥ 1 − q. The
1/2-quantile is also known as the median.
Note that the q-quantile may not be unique when the conditional distribution has regions with zero mass.
2.1 Optimization
It is well-known that the optimal estimator for the absoluteerror loss is a median [6]. In other words, we have that for
every regression problem D,
arg min Ex,y∼D |y − f (x)| is a (conditional) median.
f

This can be verified by considering two equal point masses
at locations y1 and y2 . The absolute value loss for a point
y ∈ [y1 , y2 ] is (y − y1 ) + (y2 − y) = (y2 − y1 ) which is
constant independent of y; whereas any y 6∈ [y1 , y2 ] yields
a larger value. Since we can take any distribution over y
and break into equal mass pairs with y2 above and y1 below
the median, the absolute-error loss is minimized when f (x)
is a median.
The generalization of absolute-error loss for arbitrary order
statistics is the quantile loss function, also known as “pin-

The correctness of this can be (again) seen by considering
two points y1 and y2 with probability ratio 1−q
q ; or by noting that the result is implied by equation (3) in the proof of
Lemma 1 (the integral is positive unless Q(x) = q(x) or
D(y < t|x) = q for all t between Q(x) and q(x)).

3 An Algorithm
In this section and the next, we assume that we are given
samples (x, y) from a distribution D, where 0 ≤ y ≤ 1
(if this is not the case, we can re-normalize the data).
Given these samples, our proposed algorithm, which we
call Quanting, estimates the qth quantile of the conditional
distribution D|x using any importance weighted classification algorithm A. In fact, using an extra reduction discussed in Corollary 1 below, one can also do Quanting via
an unweighted binary classification algorithm, but we defer
any further discussion of this to the next section.
The Quanting algorithm has two parts. Algorithm 1 receives a set of training examples S of the form (x, y) and
a quantile q as input and uses algorithm A to compute a
family of binary classifiers ct . We assume that algorithm
A receives as input a set of training examples of the form
(x, y, w) where w is a weight and attempts to minimize the
weighted error. In the algorithm, positive examples receive
weight q while negative examples receive weight (1 − q).
Using the classifiers ct , Algorithm 2 produces a prediction
of the q-quantile for each x in a test set S 0 , in precisely
the same manner as the Probing algorithm for estimating
conditional class probabilities using 0/1 classifiers [7].
The essential idea of Quanting is also similar to Probing.
Each ct attempts to answer the question “is the q-quantile
above or below t?” In the (idealized) scenario where A
is perfect, one would have ct (x) = 1 if and only if t ≤

Algorithm 2 Quanting-test (set of classifiers {ct }, test set
S0)
1. For each x in S 0 :
Q(x) = Et∼U (0,1) [ct (x)]

Applying this formula to f (x) = Q(x) and f (x) = q(x)
and taking the difference yields
Ex,y∼D [lq (y, Q(x)) − lq (y, q(x))]
·
¸
R q(x) qD(y ≥ u|x)
Ex Q(x)
du
−(1 − q)D(y < u|x)
·
¸
R q(x) q − qD(y < u|x)
du
Ex Q(x)
−(1 − q)D(y < u|x)
R q(x)
Ex Q(x) [q − D(y < u|x)] du.

=
q(x) for a q-quantile q(x), hence Algorithm 2 would output
R q(x)
dt = q(x) exactly. Our analysis shows that if the
0
error of A is small on average over t, the quantile estimate
is accurate.
We note in passing that in reality, one cannot find a different
classifier for each t ∈ [0, 1]. Constructing classifiers ct for
t in a discrete mesh {0, 1/n, 2/n, . . . , (n − 1)/n, 1} will
add a 1/n term to the error bound.

4 Quanting Reductions Analysis
The Lemma we prove next relates the average regret of the
classifiers ct (how well the classifiers do in comparison to
how well they could do) to the regret of the quantile loss
incurred by the Quanting algorithm. For each x, the output
produced by the quanting algorithm is denoted by Q(x),
whereas q(x) is a correct q-quantile. In this analysis, we
use the standard one-classifier trick [7]: instead of learning
different classifiers, we learn one classifier c = {ct } with
an extra feature t used to index classifier ct .
Lemma 1 (Quanting Regret Transform) For all D, c,

=

(3)

We will show that e(D, c) − minc0 e(D, c0 ) is at least this
last expression. The expected importance-weighted error
incurred by the classifiers {ct } is
·
¸
R1
qI(y ≥ t)(1 − ct (x))
e(D, c) = Ex,y∼D 0
dt
+(1 − q)I(y < t)ct (x)
·
¸
R1
qD(y ≥ t|x)
=
Ex 0
dt
+(D(y < t|x) − q)ct (x)
R1
= qEx [y] + Ex 0 [D(y < t|x) − q]ct (x) dt (4)
R Q(x)
≥
qEx [y] + Ex 0
[D(y < t|x) − q]dt. (5)
Here only the last line is non-trivial, and it follows from
the fact that D(y < t|x) − q is increasing in t. Thus the
smallest possible value for the integral in (4) is achieved by
placing as much “weight” ct (x) as possible on the smallR1
est t while respecting the constraints 0 ct (x) dt = Q(x)
and 0 ≤ ct (x) ≤ 1. This corresponds precisely to setting
ct (x) = I(t ≤ Q(x)), from which (5) follows.
On can show that the inequality (5) is in fact an equality
when instead of {ct } we use the (optimal) classifiers

Ex,y∼D [lq (y, Q(x))] − Ex,y∼D [lq (y, q(x))]

{c∗t (x) = I(D(y ≤ t|x) ≤ q)}

≤ e(D, c) − min
e(D, c0 )
0

and substitute q(x) for Q(x). Therefore,

c

where e(D, c) is the expected importance weighted binary
loss of c over D.
Proof: For any function f = f (x), Ex,y∼D lq (y, f (x)) is
given by eqn. (1):
qEx,y∼D (y − f (x))I(y − f (x) > 0)
+(1 − q)Ex,y∼D (f (x) − y)I(f (x) − y > 0).
R +∞
It is known that E[XI(X > 0)] = 0 Pr(X ≥ t)dt =
R +∞
Pr(X > t)dt for any random variable X, so we
0
rewrite:
Ex,y∼D lq (y, f (x))
R∞
=
qEx 0 D(y − f (x) ≥ t1 |x)dt1
R∞
+(1 − q)Ex 0 D(f (x) − y > t2 |x)dt2
R1
=
qEx f (x) D(y ≥ u|x) du
R f (x)
+(1 − q)Ex 0
D(y < u|x) du.

=

≥

e(D, c) − e(D, c∗ )
R Q(x)
Ex q(x) [D(y < t|x) − q]dt

=

Ex,y∼D [lq (y, Q(x)) − lq (y, q(x))],

using (3). This finishes the proof.
We now show how to reduce q-quantile estimation to unweighted binary classification using the results of previous
work [2]. We apply rejection sampling: we feed the unweighted classifier samples of the form ((x, t), I(y ≥ t)),
each of the samples being independently discarded with
probability 1 − w(I(y ≥ t)), where w(b) = qb + (1 −
q)(1 − b) is the example’s weight. Notice that by [2, Theorem 2.3], sample complexity is not significantly affected
by this.
Corollary 1 (Quanting to Binary Regret) For D as above
and unweighted binary classifier c = {ct }, let D̃ be the
distribution produced by rejection sampling. Then

(2)

Ex,y∼D [lq (y, Q(x))] − Ex,y∼D [lq (y, q(x))]

≤ e(D̃, c) − min
e(D̃, c0 ).
0
c

Proof: Let ĉ = {ĉt }t be the importance-weighted classifiers induced by the rejection sampling procedure. A folk
theorem [2] implies that
e(D, ĉ) − minĉ0 e(D, ĉ0 ) = e(D̃, c) − minc0 e(D̃, c0 )
and the result follows from Lemma 1.

5 Related Work
A standard technique for quantile regression that has been
developed and extensively applied in the Econometrics
community [6] is linear quantile regression. In linear quantile regression, we assume that the conditional quantile
function is a linear function of the features of the form βx
and we estimate the parameters β̂ that minimize the quantile loss function (Equation 1). It can be shown that this
minimization is a linear programming problem and that it
can be efficiently solved using interior point techniques [5].
Implementations of linear quantile regression are available
in standard statistical analysis packages such as R and SAS.
The obvious limitation of linear quantile regression is that
the assumption of a linear relationship between the explanatory variables and the conditional quantile function
may not be true. Recently, Takeuchi et al. have recently
proposed a technique for nonparametric quantile estimation [11] that applies the two standard features of kernel
methods to conditional quantile estimation: regularization
and the kernel trick. They show that a regularized version
of the quantile loss function can be directly minimized using standard quadratic programming techniques. By choosing an appropriate kernel, such as a radial basis function
kernel, one can obtain nonlinear conditional quantile estimates. They compare their method experimentally to linear
quantile regression and to a nonlinear spline approach suggested by Koenker [5] on many small datasets for different
quantiles and find that it performs the best in most cases.

6 Experiments
Here we compare experimentally the Quanting algorithm to
the two existing methods for quantile regression described
in section 5: linear quantile regression and kernel quantile
regression.
We compare the methods on three different performance
metrics:
1. The quantile loss (Equation 1).
2. The percentage of examples for which the prediction
f (x) exceeds the actual value y, which should be close
to the quantile q for which we are optimizing.

3. The running time (Pentium 1.86GHz, 1.00GB RAM).
As base classifier learners for Quanting, we use two algorithms available in the WEKA machine learning software [13]: the J48 decision tree learner and logistic regression. For both methods, we use the default parameters provided by WEKA. We use rejection sampling to
perform importance-weighted classification with standard
unweighted classifiers. We use an adaptive discretization
scheme to choose the thresholds t in algorithm 1, the same
scheme that we use in the Probing reduction [7]. We fix the
number of classifiers at 100 for all the datasets.
For the kernel quantile regression, we have followed the
same experimental setup as described by Takeuchi et al.
[11]. We use a radial basis function kernel and choose
its radius and the regularization parameter using crossvalidation on the training data. We also scale the features
and the label to have zero mean and a standard deviation
of 1, as required for kernel methods. When predicting, we
convert the label back to the original scale to compute the
quantile loss.
As our benchmarks, we use four large, publicly available
datasets, from real-world domains where quantile regression is clearly applicable:
1. Adult: available from the UCI Machine Learning
Repository [4] as a classification dataset. The data was
originally extracted from the Census Bureau Database
and describes individual demographic characteristics
of such as age, education, sex and occupation. For
the original dataset, the objective is to predict a label
that indicates whether or not the individual’s income is
above $50K. We have retrieved the original numerical
income values from the Census Bureau Database and
used the income as the dependent variable in the quantile regression. Our objective is to predict the quantiles
of the conditional income distribution, which is useful
if we want to determine what is a low or a high income
for a given individual.
2. KDD-Cup 1998: available from the UCI KDD
Archive [3]. This dataset consists of records of individuals who have made a donation in the past to
a particular charity. Each example consists of attributes describing each individual’s donation history
over a series of donation campaigns, as well as demographic information, such as income and age. The dependent variable is the individual’s donation amount
in the most recent donation campaign. The original
dataset contains 95412 training records and 96367 test
records, but only 5% of the individuals donated in
the current campaign. Our objective is to predict the
quantiles of the conditional donation amount for individuals who donate. For this reason, we only use
4843 donor examples in the training set and the 4876

Dataset
Adult
KDD-Cup 1998
California Housing
Boston Housing

Features
14
10
8
14

Training
32560
4840
13760
450

Test
16280
4870
6880
56

Table 1: Number of features and examples (training and
test) of each of the datasets.
donor examples in the test set. Predicting quantiles
of the conditional donation distribution is important
for “anchoring”, i.e., deciding how much to suggest
as possible donation values when soliciting donations.
Anchoring is a well-established concept in marketing,
see, for example, [12].
3. California Housing: available from the StatLib repository [1]. It contains data on California housing characteristics aggregated at the block level (a sample block
group on average includes 1425.5 individuals living
in a geographically compact area). The independent
variables are median income, housing median age,
total rooms, total bedrooms, population, households,
latitude, and longitude. The dependent variable is
the median house value. Our objective is to predict
the quantiles of the conditional house value distribution. This information is very valuable for house
sellers and buyers, since it indicates what would be
a ‘lower bound’ and an ‘upper bound’ on the house
value, given its characteristics.
4. Boston Housing: available from the StatLib repository
[1]. It contains data on Boston housing characteristics
and values. The prediction task is analogous to the
one described for the California Housing dataset.
Table 1 shows the number of features and the number of
examples for each dataset. We use the standard train/test
splits for training and testing. Because the kernel quantile regression method has very high memory and computational time requirements, we could not run it using all the
examples in the training set for the Adult, KDD-Cup 1998
and California Housing. We have run it for the maximum
number of examples possible, which in this case was 3000
for the three datasets (after trying with 1000, 2000, 3000,
4000, etc.). The 3000 examples were chosen at random
from the training data.
We have run the methods for 3 different quantile values for
each dataset: 0.1, 0.5 and 0.9. The results are shown in
tables 2, 3, 4 and 5.
In terms of running time, it is clear that the linear method is
the most efficient and that the kernel method is inefficient.
Even with the number of examples limited at 3000, the kernel method takes more than one hour to run on the larger
datasets. Quanting is relatively efficient: with our choice of

classifier learners it does not take more than 8 minutes to
run with 100 classifiers even on the larger datasets. In terms
of the quantile loss, Quanting-J48 is clearly the best performer for the Adult and California Housing datasets. The
kernel method and Quanting-LogReg are performing about
the same for these two datasets, while the linear method is
inferior. For the KDD-Cup 1998 dataset, the linear method
is the best for q=0.5 and q=0.9, while Quanting-J48 is the
best for q=0.1. This can be explained by the fact that there
is a strong linear correlation between the label and one
of the features in this dataset, which is well-captured by
the linear quantile regression but not so easily captured by
Quanting and by the kernel method. Finally, for the Boston
Housing dataset, the best method depends on the particular
value of q but the linear method is performing consistently
worse than the others.
In terms of the percentage of examples for which the prediction exceeds the actual value, all the methods come close
to desired value (the same as q) in most of the cases. But
we can observe that the linear method is consistently close,
while the kernel method shows the largest deviations.
To give an idea of how the Quanting algorithm progresses
as we add more classifiers, in figure 2 we plot the quantile
loss as a function of the number of classifiers for the Adult
and Boston Housing datasets (q = 0.9). For comparison,
we also plot the values of the quantile loss for the linear
and kernel methods as horizontal lines in the picture. It is
clear that Quanting converges very fast. In both cases, the
convergence occurs with about 50 classifiers.

7 Conclusion
In this paper, we present a reduction from quantile regression to classification. Theoretically, we are now able to
quantify the regret of quantile regression under a quantile
loss in terms of the error rate of a base classifier. In practice,
this means that we can apply classifier learning methods to
solve quantile regression problems, which appear very often in real-world applications. Our experiments show that
the Quanting reduction is efficient in terms of computational time and performs well compared to existing quantile
regression methods.
Acknowledgements
We thank Saharon Rosset and Claudia Perlich for useful
discussions on the topic of this paper.




Keywords

In evaluating prediction markets (and other crowd-prediction
mechanisms), investigators have repeatedly observed a socalled wisdom of crowds effect, which can be roughly summarized as follows: the average of participants performs
much better than the average participant. The market price—
an average or at least aggregate of traders’ beliefs—offers a
better estimate than most any individual trader’s opinion.
In this paper, we ask a stronger question: how does the
market price compare to the best trader’s belief, not just
the average trader. We measure the market’s worst-case log
regret, a notion common in machine learning theory. To arrive at a meaningful answer, we need to assume something
about how traders behave. We suppose that every trader
optimizes according to the Kelly criteria, a strategy that
provably maximizes the compound growth of wealth over
an (infinite) sequence of market interactions. We show several consequences. First, the market prediction is a wealthweighted average of the individual participants’ beliefs. Second, the market learns at the optimal rate, the market price
reacts exactly as if updating according to Bayes’ Law, and
the market prediction has low worst-case log regret to the
best individual participant. We simulate a sequence of markets where an underlying true probability exists, showing
that the market converges to the true objective frequency
as if updating a Beta distribution, as the theory predicts. If
agents adopt a fractional Kelly criteria, a common practical
variant, we show that agents behave like full-Kelly agents
with beliefs weighted between their own and the market’s,
and that the market price converges to a time-discounted
frequency. Our analysis provides a new justification for fractional Kelly betting, a strategy widely used in practice for
ad-hoc reasons. Finally, we propose a method for an agent
to learn her own optimal Kelly fraction.

Auction and mechanism design, electronic markets, economically motivated agents, multiagent learning

Categories and Subject Descriptors
I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence—Intelligent agents, Multiagent systems

General Terms
Economics

Short Version Appears in: Proceedings of the 11th International Conference on Autonomous Agents and Multiagent
Systems (AAMAS 2012), Conitzer, Winikoff, Padgham, and van der
Hoek (eds.), June, 4–8, 2012, Valencia, Spain.

1.

INTRODUCTION

Consider a gamble on a binary event, say, that Obama
will win the 2012 US Presidential election, where every x
dollars risked earns xb dollars in net profit if the gamble
pays off. How many dollars x of your wealth should you risk
if you believe the probability is p? The gamble is favorable
if bp−(1−p) > 0, in which case betting your entire wealth w
will maximize your expected profit. However, that’s extraordinarily risky: a single stroke of bad luck loses everything.
Over the course of many such gambles, the probability of
bankruptcy approaches 1. On the other hand, betting a
small fixed amount avoids bankruptcy but cannot take advantage of compounding growth.
The Kelly criteria prescribes choosing x to maximize the
expected compounding growth rate of wealth, or equivalently to maximize the expected logarithm of wealth. Kelly
betting is asymptotically optimal, meaning that in the limit
over many gambles, a Kelly bettor will grow wealthier than
an otherwise identical non-Kelly bettor with probability 1
[1, 3, 7, 16, 17].
Assume all agents in a market optimize according to the
Kelly principle, where b is selected to clear the market. We
consider the implications for the market as a whole and
properties of the market odds b or, equivalently, the market probability pm = 1/(1 + b). We show that the market
prediction pm is a wealth-weighted average of the agents’
predictions pi . Over time, the market itself—by reallocating wealth among participants—adapts at the optimal rate
with bounded log regret to the best individual agent. When
a true objective probability exists, the market converges to
it as if properly updating a Beta distribution according to
Bayes’ rule. These results illustrate that there is no “price
of anarchy” associated with well-run prediction markets.
We also consider fractional Kelly betting, a lower-risk variant of Kelly betting that is popular in practice but has less
theoretical grounding. We provide a new justification for
fractional Kelly based on agent’s confidence. In this case,
the market prediction is a confidence-and-wealth-weighted
average that empirically converges to a time-discounted version of objective frequency. Finally, we propose a method
for agents to learn their optimal fraction over time.

2.

KELLY BETTING

When offered b-to-1 odds on an event with probability p,
the Kelly-optimal amount to bet is f ∗ w, where
bp − (1 − p)
b
is the optimal fixed fraction of total wealth w to commit to
the gamble.
If f ∗ is negative, Kelly says to avoid betting: expected
profit is negative. If f ∗ is positive, you have an information
edge; Kelly says to invest a fraction of your wealth proportional to how advantageous the bet is. In addition to maximizing the growth rate of wealth, Kelly betting maximizes
the geometric mean of wealth and asymptotically minimizes
the mean time to reach a given aspiration level of wealth
[17].
Suppose fair odds of 1/b are simultaneously offered on the
opposite outcome (e.g., Obama will not win the election). If
bp − (1 − p) < 0, then betting on this opposite outcome is
favorable; substituting 1/b for b and 1 − p for p, the optimal
fraction of wealth to bet becomes 1 − p − bp.
An equivalent way to think of a gamble with odds b is as
a prediction market with price pm = 1/(1 + b). The volume
of bet is specified by choosing a quantity q of shares, where
each share is worth $1 if the outcome occurs and nothing
otherwise. The price represents the cost of one share: the
amount needed to pay for a chance to win back $1. In this
interpretation, the Kelly formula becomes
f∗ =

f∗ =

p − pm
.
1 − pm

The optimal action for the agent is to trade q ∗ = f ∗ w/pm
shares, where q ∗ > 0 is a buy order and q ∗ < 0 is a sell
order, or a bet against the outcome.
Note that q ∗ is the optimum of expected log utility
p ln((1 − pm )q + w) + (1 − p) ln(−pm q + w).

odds reached when all agents are optimizing, and supply
and demand are precisely balanced. Recall that the market’s
probability implied by
Pthe odds of b is pm = 1/(1 + b). We
will show that pm is i wi pi .

4.1

Payout balance

The first approach we’ll use is payout balance: the amount
of money at risk must be the same as the amount paid out.
Theorem 1. (Market Pricing) For all normalized agent
wealths wi and agent beliefs pi ,
X
pi wi
pm =
i

Proof. To see this, recall that fi∗ = (pi − pm )/(1 − pm ) for
pi > pm . For pi < pm , Kelly betting prescribes taking the
other side of the bet, with fraction
(1 − pi ) − (1 − pm )
pm − pi
=
.
1 − (1 − pm )
pm
So the market equilibrium occurs at the point pm where the
payout is equal to the payin. If the event occurs, the payin
is
X pi − pm
X pi − pm
1
wi =
wi .
(1 + b)
1 − pm
pm i:p >p 1 − pm
i:p >p
Thus we want
X
1
pm i:p >p
i

1 − pm
pm

X pi − pm
pi − pm
wi =
wi +
1 − pm
1 − pm
i:pi >pm
X pm − pi
wi ,
pm
i:p <p

i:pi <pm

X

pi w i =

i

X

pm wi .

i

Using

P

4.2

Log utility maximization

wi = 1, we get the theorem.

An alternate derivation of the market prediction utilizes
the fact that Kelly betting is equivalent to maximizing expected log utility. Let q = x(b + 1) be the gross profit of an
agent who risks x dollars, or in prediction market language
the number of shares purchased. Then expected log utility
is
E[U (q)] = p ln((1 − pm )q + w) + (1 − p) ln(−pm q + w).
The optimal q that maximizes E[U (q)] is
q(pm ) =

w p − pm
·
.
pm 1 − pm

(1)

Proposition 2. In a market of agents each with log utility
and initial wealth w, the competitive equilibrium price is
X
pm =
wi pi
(2)

MARKET PREDICTION

In order to define the prediction market’s performance,
we must define its prediction b, or the equilibrium payoff

or

m

X pm − pi
pi − pm
wi =
wi , or
1 − pm
pm
i:pi >pm
i:pi <pm
X
X
(pi − pm )wi =
(pm − pi )wi , or

i:pi >pm

i

m

X

MARKET MODEL

Suppose that we have a prediction market,
P where participant i has a starting wealth wi with
i wi = 1. Each
participant i uses Kelly betting to determine the fraction fi∗
of their wealth bet, depending on their predicted probability
pi .
We model the market as an auctioneer matching supply and demand, taking no profit and absorbing no loss.
We adopt a competitive equilibrium concept, meaning that
agents are ”price takers”, or do not consider their own effect
on prices if any. Agents optimize according to the current
price and do not reason further about what the price might
reveal about the other agents’ information. An exception
of sorts is the fractional Kelly setting, where agents do consider the market price as information and weigh it along
with their own.
A market is in competitive
at price pm if all
P equilibrium
∗
agents are optimizing and
q
=
0,
or
every buy order
i
i
and sell order are matched. We discuss next what the value
of pm is.

4.

m

i

i

This is not a coincidence: Kelly betting is identical to maximizing expected log utility.

3.

m

i

i

where we assume
absolute wealth.

P

i wi = 1, or w is normalized wealth not

P
Proof. These prices satisfy i qi = 0, the condition for
competitive equilibrium (supply equals demand), by substitution. 2
This result can be seen as a simplified derivation of that
by Rubinstein [13, 14, 15] and is also discussed by Pennock
and Wellman [11, 10] and Wolfers and Zitzewitz [18].

5.

as
L≡

5.1

Wealth redistributed according to Bayes’
Law

In an individual round, if an agent’s belief is pi > pm ,
i −pm
wi and have a total wealth afterward
then they bet p1−p
m
dependent on y according to:

I(yt = 1) log

t=1

1
1
+ I(yt = 0) log
.
pt
1 − pt

Similarly, we measure the quality of market participant making prediction pit as
Li ≡

LEARNING PREDICTION MARKETS

Individual participants may have varying prediction qualities and individual markets may have varying odds of payoff. What happens to the wealth distribution and hence the
quality of the market prediction over time? We show next
that the market learns optimally for two well understood
senses of optimal.

T
X

T
X

I(yt = 1) log

t=1

1
1
+ I(yt = 0) log
.
pit
1 − pit

So after T rounds, the total wealth of player i is
y 
1−yt
T 
Y
pit t 1 − pit
wi
,
pt
1 − pt
t=1
where wi is the starting wealth. We next prove a well-known
theorem for learning in the present context (see for example [4]).
Theorem 3. For all sequences of participant predictions pit
and all sequences of revealed outcomes yt ,
L ≤ min Li + ln
i



If

y = 1,

If

y = 0,


1
pi − pm
pi
−1
wi + wi =
wi
pm
1 − pm
pm
pi − pm
1 − pi
(−1)
wi + wi =
wi
1 − pm
1 − pm

Similarly if pi < pm , we get:
If

y = 1,

If

y = 0,

pm − pi
pi
wi + wi =
wi
pm
pm

1
pm − pi
1 − pi
−1
wi + wi =
wi ,
1 − pm
pm
1 − pm

(−1)


This theorem is extraordinarily general, as it applies to all
market participants and all outcome sequences, even when
these are chosen adversarially. It states that even in this
worst-case situation, the market performs only ln 1/wi worse
than the best market participant i.
P
Proof. Initially, we have that i wi = 1. After T rounds,
the total wealth of any participant i is given by
y 
1−yt
T 
Y
pit t 1 − pit
wi
= wi eL−Li ≤ 1,
p
1
−
p
t
t
t=1
where the last inequality follows from wealth being conserved. Thus ln wi + L − Li ≤ 0, yielding

which is identical.
If we treat the prior probability that agent i is correct
as wi , Bayes’ law states that the posterior probability of
choosing agent i is
P (i | y = 1) =

P (y = 1 | i)P (i)
p i wi
pi w i
,
=
= P
P (y = 1)
pm
i pi wi

which is precisely the wealth computed above for the y = 1
outcome. The same holds when y = 0, and so Kelly bettors
redistribute wealth according to Bayes’ law.

5.2

Market Sequences

It is well known that Bayes’ law is the correct approach for
integrating evidence into a belief distribution, which shows
that Kelly betting agents optimally summarize all past information if the true behavior of the world was drawn from
the prior distribution of wealth.
Often these assumptions are too strong—the world does
not behave according to the prior on wealth, and it may act
in a manner completely different from any one single expert.
In that case, a standard analysis from learning theory shows
that the market has low regret, performing almost as well as
the best market participant.
For any particular sequence of markets we have a sequence
pt of market predictions and yt ∈ {0, 1} of market outcomes.
We measure the accuracy of a market according to log loss

1
.
wi

L ≤ Li + ln

6.

1
.
wi

FRACTIONAL KELLY BETTING

Fractional Kelly betting says to invest a smaller fraction
λf ∗ of wealth for λ < 1. Fractional Kelly is usually justified
on an ad-hoc basis as either (1) a risk-reduction strategy,
since practitioners often view full Kelly as too volatile, or
(2) a way to protect against an inaccurate belief p, or both
[17]. Here we derive an alternate interpretation of fractional
Kelly. In prediction market terms, the fractional Kelly formula is
p − pm
.
λ
1 − pm
With some algebra, fractional Kelly can be rewritten as
p0 − pm
1 − pm
where
p0 = λp + (1 − λ)pm .

(3)

In other words, λ-fractional Kelly is precisely equivalent to
full Kelly with revised belief λp+(1−λ)pm , or a weighted average of the agent’s original belief and the market’s belief. In

this light, fractional Kelly is a form of confidence weighting
where the agent mixes between remaining steadfast with its
own belief (λ = 1) and acceding to the crowd and taking the
market price as the true probability (λ = 0). The weighted
average form has a Bayesian justification if the agent has a
Beta prior over p and has seen t independent Bernoulli trials
to arrive at its current belief. If the agent envisions that the
market has seen t0 trials, then she will update her belief to
λp + (1 − λ)pm , where λ = t/(t + t0 ) [9, 10, 12]. The agent’s
posterior probability given the price is a weighted average of
its prior and the price, where the weighting term captures
her perception of her own confidence, expressed in terms of
the independent observation count seen as compared to the
market.

1.0

7.

MARKET PREDICTION WITH FRACTIONAL
0.9
KELLY

When agents play fractional Kelly, the competitive equilibrium price naturally changes. The resulting market price
is easily compute, as for fully Kelly agents.
Theorem 4. (Fractional Kelly Market Pricing) For all agent
beliefs pi , normalized wealths wi and fractions λi
P
λi wi pi
.
(4)
pm = Pi
l λl wl
Prices retain the form of a weighted average, but with
weights proportional to the product of wealth and self-assessed
confidence.
Proof. The proof is a straightforward corollary of Theorem 1.
In particular, we note that a λ-fractional Kelly agent of
wealth w bets precisely as a full-Kelly agent of wealth λw.
Consequently, we can apply theorem 1 with wi0 = Pλiλwiiwi
i
and p0i = pi unchanged.

8.

MARKET DYNAMICS WITH STATIONARY OBJECTIVE FREQUENCY

The worst-case bounds above hold even if event outcomes
are chosen by a malicious adversary. In this section, we
examine how the market performs when the objective frequency of outcomes is unknown though stationary.
The market consists of a single bet repeated over the
course of T periods. Unbeknown to the agents, each event
unfolds as an independent Bernoulli trial with probability of
success π. At the beginning of time period t, the realization
of event Et is unknown and agents trade until equilibrium.
Then the outcome is revealed, and the agents’ holdings pay
off accordingly. As time period t + 1 begins, the outcome of
Et+1 is uncertain. Agents bet on the t + 1 period event until
equilibrium, the outcome is revealed, payoffs are collected,
and the process repeats.
In an economy of Kelly bettors, the equilibrium price is
a wealth-weighted average (2). Thus, as an agent accrues
relatively more earnings than the others, its influence on
price increases. In the next two subsections, we examine how
this adaptive process unfolds; first, with full-Kelly agents
and second, with fractional Kelly agents. In the former case,
prices react exactly as if the market were a single agent
updating a Beta distribution according to Bayes’ rule.

0.8
0.7
0.6
20

(a)

0.035
0.030
0.025
0.020
0.015
0.010
0.005
0.000
(b)

40

0.2

60

0.4

80

100

0.6

120

0.8

140

1.0

Figure 1: (a) Price (black line) versus the observed
frequency (gray line) of the event over 150 time periods. The market consists of 100 full-Kelly agents
with initial wealth wi = 1/100. (b) Wealth after
15 time periods versus belief for 100 Kelly agents.
The event has occurred in 10 of the 15 trials. The
solid line is the posterior Beta distribution consistent with observing 10 successes in 15 independent
Bernoulli trials.

8.1

Market dynamics with full-Kelly agents

Figure 1.a plots the price over 150 time periods, in a
market composed of 100 Kelly agents with initial wealth
wi = 1/100, and pi generated randomly and uniformly on
(0, 1). In this simulation the true probability of success π
is 0.5. For comparison, the figure also shows the observed
frequency, or the number of times that E has occurred divided by the number of periods. The market price tracks
the observed frequency extremely closely. Note that price
changes are due entirely to a transfer of wealth from inaccurate agents to accurate agents, who then wield more power
in the market; individual beliefs remain fixed.
Figure 1.b illustrates the nature of this wealth transfer.
The graph provides a snapshot of agents’ wealth versus their
belief pi after period 15. In this run, E has occurred in 10
out of the 15 trials. The maximum in wealth is near 10/15 or
2/3. The solid line in the figure is a Beta distribution with
parameters 10 + 1 and 5 + 1. This distribution is precisely
the posterior probability of success that results from the
observation of 10 successes out of 15 independent Bernoulli
trials, when the prior probability of success is uniform on
(0,1). The fit is essentially perfect, and can be proved in the
limit since the Beta distribution is conjugate to the Binomial
distribution under Bayes’ Law.
Although individual agents are not adaptive, the market’s composite agent computes a proper Bayesian update.
Specifically, wealth is reallocated proportionally to a Beta
distribution corresponding to the observed number of successes and trials, and price is approximately the expected
value of this Beta distribution.1 Moreover, this correspondence holds regardless of the number of successes or failures,
or the temporal order of their occurrence. A kind of collective Bayesianity emerges from the interactions of the group.
We also find empirically that, even if not all agents are
Kelly bettors, among those that are, wealth is still redistributed according to Bayes’ rule.

8.2

0.5
0.4
0.3
0.2
0.1

where 1E(t) is the indicator function for the event at period
t, and γ is the discount factor. Note that γ = 1 recovers the
standard observed frequency.
1
As t grows, this expected value rapidly approaches the observed frequency plotted in Figure 1.

40

60

80

100

120

140

0.5
0.4
0.3
0.2
0.1

Market dynamics with fractional Kelly agents

In this section, we consider fractional Kelly agents who,
as we saw in Section 2, behave like full Kelly agents with
belief λp + (1 − λ)pm . Figure 2.a graphs the dynamics of
price in an economy of 100 such agents, along with the observed frequency. Over time, the price remains significantly
more volatile than the frequency, which converges toward
π = 0.5. Below, we characterize the transfer of wealth that
precipitates this added volatility; for now concentrate on the
price signal itself. Inspecting Figure 2.a, price changes still
exhibit a marked dependence on event outcomes, though at
any given period the effect of recent history appears magnified, and the past discounted, as compared with the observed
frequency. Working from this intuition, we attempt to fit
the data to an appropriately modified measure of frequency.
Define the discounted frequency at period n as
Pn
n−t
(1
)
t=1 γ
P E(t) n−t
,
(5)
dn = Pn
n−t
(1E(t) ) + n
(1E(t) )
t=1 γ
t=1 γ

20

(a)

(b)

20

40

60

80

100

120

140

Figure 2: (a) Price (black line) versus observed frequency (gray line) over 150 time periods for 100
agents with Kelly fraction λ = 0.2. As the frequency
converges to π = 0.5, the price remains volatile. (b)
Price (black line) versus discounted frequency (gray
line), with discount factor γ = 0.96, for the same
experiment as (a).

For example, if you allocate an initial weight of 0.5 to your
predictions and 0.5 to the market’s prediction, then the regret guarantee of section 5.2 implies that at most half of all
wealth is lost.

0.020
0.015
0.010

10.

0.005
0.000

0.2

0.4

0.6

0.8

1.0

Figure 3: (a) Wealth wi versus belief pi at period 150
of the same experiment as Figure 2 with 100 agents
with Kelly fraction λ = 0.2. The observed frequency
is 69/150 and the solid line is Beta(69 + 1, 81 + 1).
The wealth distribution is significantly more evenly
dispersed than the corresponding Beta distribution.

Figure 2.b illustrates a very close correlation between discounted frequency, with γ = 0.96 (hand tuned), and the
same price curve of Figure 2.a. While standard frequency
provides a provably good model of price dynamics in an
economy of full-Kelly agents, discounted frequency (5) appears a better model for fractional Kelly agents.
To explain the close fit to discounted frequency, one might
expect that wealth remains dispersed—as if the market’s
composite agent witnesses fewer trials than actually occur.
That’s true to an extent. Figure 3 shows the distribution of
wealth after 69 successes have occurred in 150 trials. Wealth
is significantly more evenly distributed than a Beta distribution with parameters 69+1 and 81+1, also shown. However,
the stretched distribution can’t be modeled precisely as another, less-informed Beta distribution.

9.

LEARNING THE KELLY FRACTION

In theory, a rational agent playing against rational opponents should set their Kelly fraction to λ = 0, since, in a
rational expectations equilibrium [6], the market price is by
definition at least as informative as any agent’s belief. This
is the crux of the no-trade theorems [8]. Despite the theory
[5], people do agree to disagree in practice and, simply put,
trade happens. Still, placing substantial weight on the market price is often prudent. For example, in an online prediction contest called ProbabilitySports, 99.7% of participants
were outperformed by the unweighted average predictor, a
typical result.2
In this light, fractional Kelly can be seen as an experts
algorithm [2] with two experts: yourself and the market.
We propose dynamically updating λ according to standard
experts algorithm logic: When you’re right, you increase
λ appropriately; when you’re wrong, you decrease λ. This
gives a long-term procedure for updating λ that guarantees:
• You won’t do too much worse than the market (which
by definition earns 0)
• You won’t do too much worse than Kelly betting using
your original prior p
2

http://www.overcomingbias.com/2007/02/how_and_when_to.html

DISCUSSION

We’ve shown something intuitively appealing here: selfinterested agents with log wealth utility create markets which
learn to have small regret according to log loss. There are
two distinct “log”s in this statement, and it’s appealing to
consider what happens when we vary these. When agents
have some utility other than log wealth utility, can we alter
the structure of a market so that the market dynamics make
the market price have low log loss regret? And similarly if
we care about some other loss—such as squared loss, 0/1
loss, or a quantile loss, can we craft a marketplace such that
log wealth utility agents achieve small regret with respect to
these other losses?
What happens in a market without Kelly bettors? This
can’t be described in general, although a couple special cases
are relevant. When all agents have constant absolute risk
aversion, the market computes a weighted geometric average of beliefs [10, 11, 13]. When one of the bettors acts
according to Kelly and the others in some more irrational
fashion. In this case, the basic Kelly guarantee implies that
the Kelly bettor will come to dominate non-Kelly bettors
with equivalent or worse log loss. If non-Kelly agents have
a better log loss, the behavior can vary, possibly imposing
greater regret on the marketplace if the Kelly bettor accrues
the wealth despite a worse prediction record. For this reason, it may be desirable to make Kelly betting an explicit
option in prediction markets.

11.



We improve “learning to search” approaches to structured prediction in two ways.
First, we show that the search space can be defined by an arbitrary imperative
program, reducing the number of lines of code required to develop new structured
prediction tasks by orders of magnitude. Second, we make structured prediction
orders of magnitude faster through various algorithmic improvements.

1

Introduction

In structured prediction problems, the goal is creating a good set of joint predictions. As an example, consider recognizing a handwritten word where each character might be recognized in turn to
understand the word. Here, it is commonly observed that exposing information from related predictions (i.e. adjacent letters) aids individual predictions. Furthermore, optimizing a joint loss function
can improve the gracefulness of error recovery. Despite this, it is empirically common to build
independent predictors in settings where structured prediction naturally applies. Why? Because
independent predictors are much simpler, easier and faster to train. Our primary goal is to make
structured prediction algorithms as easy and fast as possible to both program and compute.
A new programming abstraction, together with several algorithmic pearls, radically reduce the complexity of programming and the running time of our solution.
1. We enable structured prediction as a library which has a function PREDICT(...) returning
predictions. The PREDICT(...) interface is the minimal complexity approach to producing a
structured prediction. Surprisingly, this single library interface is sufficient for both testing
and training, when augmented to include label “advice” from a training set. This means
that a developer need only code desired test time behavior and gets training “for free.”
2. Although the PREDICT(...) interface is the same as the interface for an online learning
algorithm, the structured prediction setting commonly differs in two critical ways. First,
the loss may not be simple 0/1 loss over subproblems. For optimization of a joint loss, we
add a LOSS(...) function which allows the declaration of an arbitrary loss for the joint set
of predictions. The second difference is that predictions are commonly used as features for
other predictions. This can be handled either implicitly or explicitly, but the algorithm is
guaranteed to work either way.
Here PREDICT(...) and LOSS(...) enable a concise specification of structured prediction problems.
Basic sequence labeling as shown in algorithm 1 is the easiest possible structured prediction problem, so it forms a good use case. The algorithm takes as input a sequence of examples (consider
features of handwritten digits in words), and predicts the meaning of each element in turn. This
is a specific case of sequential decision making, in which the ith prediction may depend on previous predictions. In this example, we make use of the library’s support for implicit feature-based
dependence on previous predictions.
1

Algorithm 1 S EQUENTIAL _RUN(examples)
1: for i = 1 to LEN (examples) do
2:
prediction ← PREDICT(examples[i], examples[i].label)
// make a prediction on the ith example
3:
if output.good then
4:
output « ’ ’ « prediction
// if we should generate output, append our prediction
5:
end if
6: end for

The use of this function for decoding is clear, but how can the PREDICT(...) interface be effective?
There are two challenges to overcome in creating a viable system.
1. Given the available information, are there well-founded structured prediction algorithms?
For Conditional Random Fields [Lafferty et al., 2001] and structured SVMs [Taskar et al.,
2003, Tsochantaridis et al., 2004], the answer is “no”, because we have not specified the
conditional independence structure of the system of predicted variables. Instead, we use a
system that implements search-based structured prediction methods such as Searn [Daumé
III et al., 2009] or DAgger [Ross et al., 2011]. These have formal correctness guarantees
which differ qualitatively from the conditional log loss guarantees of CRFs. For example,
given a low regret cost-sensitive classification algorithm, Searn guarantees competition
according to LOSS(...) with an oracle policy and local optimality w.r.t. one-step deviations
from the learned policy. We discuss how these work below.
2. A sequential program has only one execution stack, which is used by the decoding algorithm above. This conflicts because the learning algorithm would naturally also use the
stack. We refactor the learning algorithm into a state machine which runs before the RUN
function is called and after the various library calls are made. In essence, RUN is invoked
many times with different example sequences and different versions of PREDICT(...) so as
to find a version of PREDICT(...) with a small LOSS(...).
Given this high level design, the remaining challenge is computational. How do we efficiently and
effectively find a PREDICT(...) which achieves a small LOSS(...)?

2

Learning to Search

A discrete search space is defined by states s ∈ S and a mapping m : S → 2S defining the set of
valid next states. One of the states is a unique start state a while some of the others are end states
s ∈ E. A loss function l(s) is defined for any end state s ∈ E on the training set. We are interested
in algorithms which learn the transition function f : Xs → S which uses the features of an input
state (Xs ) to choose a next state so as to minimize the loss l on a heldout test set. Two canonical
algorithms to solve this problem are Searn [Daumé III et al., 2009] and DAgger [Ross et al., 2011]
which we review next.
Searn uses some oracle transition function f ∗ which is defined on the training set, but not on the
heldout test set. As searn operates it learns a sequence of transition functions f0 , f1 , ...., fn where
f0 = f ∗ and fn is entirely learned. At each iteration i ∈ {1, ..., n}, Searn uses fi−1 to generate
a set of cost-sensitive examples. A cost-sensitive example is defined using local features, features
which express previous predictions, and a set of costs defined for each possible next state. The costs
are derived by rollouts: for each s0 ∈ m(s), the transition function is applied until an end state
s ∈ E is observed and a loss l(s) is computed. This vector of losses, one for each s0 ∈ m(s), forms
the vector of costs. Together with local features it is fed to the cost sensitive learning algorithm.
The cost-sensitive learning algorithm generates a classifier ci : Xs → S, then a new policy fi =
(1 − α)fi−1 + αci is defined using stochastic interpolation. In essence, with probability α ci is
used to define the transition while with probability 1 − α fi−1 is used to define the transition matrix.
Since the probability of calling f ∗ decreases exponentially, a fully learned policy is quickly found.
DAgger differs from Searn in two computationally helpful ways: it mixes datasets rather than policies and uses a loss function l0 defined on all states, so rollouts are not required. When a loss is only
defined for end states, a DAgger style algorithm can operate with rollouts.
2

Algorithm 2 TDOLR(X)
1: s ← a
2: while s 6∈ E do
3:
Compute Xs from X and s
4:
s ← O(Xs )
5: end while
6: return L OSS (s)

The rollout versions of the previous algorithms require O(t2 knp) where t is the average end state
depth (i.e. sequence length), k = |m(s)| is the number of next states (i.e. branching factor), n is the
number of distinct searches (i.e. sequences), and p is the number of data passes. Three computational
tricks: online learning [Collins, 2002, Bottou, 2011], memoization, and rollout collapse allow the
computational complexity to be reduced to O(tkn), similar to independent prediction. For example,
we can train a part-of-speech tagger 100 times faster than CRF++ [Kudo, 2005] which is unsurprising
since Viterbi decoding in a CRF is O(tk 2 ). Surpringly we can do it with 6 lines of “user code,”
versus almost 1000.
We show that learning to search can be implemented with the library interface in section 3. This
provides a radical reduction in the coding complexity of solving new structured prediction problems
as discussed. We also radically reduce the computational complexity as discussed next in section 4,
then conduct experiments in section 5.

3

System Equivalences

Here we show the equivalence of a class of programs and search spaces. The practical implication
of this equivalence is that instead of specifying a search space, we can specify a program, which can
radically reduce the programming complexity of structured prediction.
Search spaces are defined in the introduction, so we must first define the set of programs that we
consider. Terminal Discrete Oracle Loss Reporting (TDOLR) programs:
1. Always terminate.
2. Takes as input any relevant feature information X.
3. Make zero or more calls to an oracle O : X 0 → Y which provides a discrete outcome.
4. Report a loss L on termination.
To show equivalence, we prove a theorem. This theorem holds for the case where the number of
choices is fixed in a search space (and, hence, m(s) is implicitly defined).
Theorem 1. For every TDOLR program there exist an equivalent search space and for every search
space there exists an equivalent TDOLR program.
The practical implication of this theorem is that instead of speicfying search spaces, we can specify
a TDOLR program (such as algorithm 1), and apply any learning to search algorithm such as Searn,
DAgger, or variants thereof.
Proof. A search space is defined by (a, E, S, l). We show there is a TDOLR program which can
simulate the search space in algorithm 3. This algorithm does a straightforward execution of the
search space, followed by reporting of the loss on termination. This completes the second claim.
For the first claim, we need to define, (a, E, S, l) given a TDOLR program such that the search
space can simulate the TDOLR program. At any point in the execution of TDOLR, we define an
equivalent state s = (O(X1 ), ..., O(Xn )) where n is the number of calls to the oracle. We define a
as the sequence of zero length, and we define E as the set of states after which TDOLR terminates.
For each s ∈ E we define l(s) as the loss reported on termination. This search space manifestly
outputs the same loss as the TDOLR program.

3

Algorithm 3 L EARN(X)
1: T ← 0
2: ex ← []
3: Define P REDICT (x, y) := { ex[++T] ← x; return fi (x, y) }
4: Define S NAPSHOT (...) := R ECORD S NAPSHOT (...)
5: RUN (X)
6: for t0 = 1 to T do
7:
losses ← []
8:
for a0 = 1 to M(ex[t0 ]) do
return a0
if t = t0
9:
Define P REDICT(...) :=
return fi (...) if t 6= t0
(
J UMP T O(t0 )
if t < t0
T RY FAST F ORWARD(...) if t > t0
10:
Define S NAPSHOT(...) :=
no op
if t = t0
11:
Define L OSS(val) := { losses[a0 ] += val }
12:
RUN(X)
13:
end for
14:
Online update with cost-sensitive example (ex[t0 ], losses)
15: end for

4

Imperative Structured Prediction

The full learning algorithm (for a single structured input, X) is depicted in Algorithm 4. In lines 1–5,
an “initialization” pass of RUN is executed. RUN can generally be any TDOLR program as discussed
in appendix 3, with a specific example being algorithm 1. In this pass, predictions are made according to the current policy, fi , and every time S NAPSHOT is called, the results are memoized for future
use (on the current example). Furthermore, the examples (feature vectors) encountered during prediction are stored in ex, indexed by their position in the sequence (T).
The algorithm then initiates one-step deviations from this initial trajectory. For every time step,
(line 6), we generate a single cost-sensitive classification example; its features are ex[t0 ], and there
are M(ex[t0 ]) possible labels (=actions). For each action (line 8), we compute the cost of that action.
To do so, we execute RUN again (line 12) with a “tweaked” P REDICT that at a particular time-step
(t0 ) simply returns the perturbed action a0 . Finally, the L OSS function simply accumulates the loss
for the query action. Finally, a cost-sensitive classification is generated (line 14) and fed into an
online learning algorithm.
When the learning-to-search algorithm is Searn, this implies a straightforward update of the next
policy. The situation with online DAgger is more subtle—in essence a dependence on the reference
policy must be preserved for many updates to achieve good performance. We do this using policy
interpolation (as in Searn) between the reference policy and learned policy.
Without any speed enhancements, each execution of RUN takes O(t) time, and we execute it tk + 1
times, yielding an overall complexity of O(kt2 ) per structured example. For comparison, structured
SVMs or CRFs with first order Markov dependencies run in O(k 2 t) time.
To improve this running time, we make two optimizations using the idea of S NAPSHOTs. Together,
they reduce the overall runtime to O(kt), when paths collapse frequently (this is tested empirically
in Section 6.1). These optimizations take advantage of the fact that most predictions only depend
on a small subset of previous predictions, not all of them. In particular, if the ith prediction only
depends on the i − 1st prediction, then there are at most tk unique predictions ever made.1 This
is what enables dynamic programming for sequences (the Viterbi algorithm). We capitalize on this
observation in a more generic way: memoization. A program is allowed to S NAPSHOT its state
before making a prediction. Because the S NAPSHOT encapsulates its entire state, we can efficiently
store that state together with relevant statistics in a hash table.

1

We use tied randomness [Ng and Jordan, 2000] to ensure that for any time step, the same policy is called.

4

4.1

Optimization 1: JumpTo

In Algorithm 4, suppose that when we execute RUN on line 12, we have t0 = T − 1. Naïvely,
one must execute T − 1 P REDICTs in order to reach the desired state at which we vary a0 . This is
inefficient. Instead, assuming that RUN recorded a snapshot at time T − 1 during the initialization
(line 5), we simply restore that stored state the first time S NAPSHOT is called: the t < t0 condition
in line 10. Even when we cannot restore the state precisely to T − 1 (for instance, perhaps the most
recent snapshot was at T − 2), we can additionally memoize the previous results of P REDICT and
regurgitate those predictions. This alone saves O(td) time, where d is the time to make a prediction.
Correctness. In line 14, the learned policy changes. For policy mixing algorithms (like Searn),
this is fine and correctness is guaranteed. However, for data mixing algorithms (like DAgger),
this potentially changes fi , implying the memoized predictions may no longer be up-to-date so the
recorded snapshots may no longer be accurate. Thus, for DAgger-like algorithms, this optimization
is okay if the policy does not change much. We evaluate this empirically in Section 6.1. The next
section has the same correctness properties.
4.2

Optimization 2: TryFastForward

The second optimization is fast forwarding to the end of the sequence using T RY FAST F ORWARD.
For example, suppose t0 = 2. After perturbing the action at time point 2 we have t > 2. Every
time S NAPSHOT is called, the snapshotted data might exactly match a previous snapshot. Suppose
at t = 3 it does not, because the perturbation at t = 2 cascaded and changed the prediction at t = 3.
But perhaps at t = 4 there is a perfect match (paths have collapsed). We remember that a match has
occurred and then at t = 5, we can “fast forward” to t = T because all subsequent predictions are
identical.
This intuitive explanation is correct, except for accumulating LOSS(...). If LOSS(...) is only declared
at the end of RUN, then we must execute T − t0 time steps making (possibly memoized) predictions.
However, for many problems, it is possible to declare loss early as with Hamming loss (= number
of incorrect predictions). There is no need to wait until the end of the sequence to declare a persequence loss: one can declare it after every prediction, and have the total loss accumulate (hence
the “+=” on line 11). We generalize this notion slightly to that of a history-independent loss:
Definition 1 (History-independent loss). A loss function is history-independent at state s0 if, for any
final state e reachable from s0 , and for any sequence s0 s1 s2 . . . si = e: it holds that L OSS(e) =
A(s0 ) + B(s1 s2 . . . si ), where B does not depend on any state before s1 .
For example, Hamming loss is history-independent: A(s0 ) corresponds to Hamming loss up to and
including s0 and B(s1 . . . si ) is the Hamming loss after s0 .2
When the loss function being optimized is history-independent, we allow LOSS(...) to be declared
early, allowing an additional S NAPSHOT optimization. In the previous example, at time t = 4 the
snapshot matched. Suppose that at this time, a total loss of 2 had been accumulated (this corresponds
to A(. . .)). Then at time t = 5 we can immediately jump to the end of the sequence t ← T , provided
that we’ve memoized the total loss incurred from t = 5 to t = T on this trajectory (this corresponds
to B(. . .)), which may be 0. The total cost for this a0 perturbation is then 2 + 0 = 2.
4.3

Overall Complexity

Suppose that the cost of calling the policy is d.3 Then the complexity of the unoptimized learning
function is O(t2 kd). By adding the memoization optimizations only, and assuming paths collapse
after a constant number of steps, this drops to O(t2 k+tkd). (The first term is from retrieving memoized predictions, the second from executing the policy a constant number of times for each perturbed
sequence.) Adding the S NAPSHOT restoration in addition to the memoization, the complexity drops
2
Any loss function that decomposes over structure, as required by structured SVMs, is guaranteed to also
be history-independent; the reverse is not true. Furthermore, when structured SVMs are run with a nondecomposible loss function, their runtime becomes exponential in t. When our approach is used with a loss
function that’s not history-independent, our runtime increases by a factor of t.
3
Because the policy is a multiclass classifier, d might hide a factor of k or log k.

5

POS
NER

NNP

NNP

, CD NNS

JJ , MD

VB

DT

NN

IN DT

JJ

NN

Pierre Vinken , 61 years old , will join the board as a nonexecutive director . . .
LOC
ORG
PER
z }| {
}|
{
}|
{
z
z
Germany ’s rep to the European Union ’s committee Werner Zwingmann said . . .

Figure 1: Example inputs (below, black) and desired outputs (above, blue) for part of speech tagging
task and named entity recognition task.
further to O(tkd). In comparison, a first order CRF or structured SVM for sequence labeling has a
complexity of O(tk 2 f ), where f is the number of features and d ≈ f k or ≈ f log k depending on
the underlying classifier used.

5

Experimental Results

We conduct two experiments based on variants of the sequence labeling problem (Algorithm 1). The
first is a pure sequence labeling problem: Part of Speech tagging based on data form the Wall Street
Journal portion of the Penn Treebank. The second is a sequence chunking problem: named entity
recognition using data from the CoNLL 2003 dataset. See Figure 1 for example inputs and outputs
for these two tasks.
We use the following freely available systems/algorithms as points of comparison:
CRF++ The popular CRF++ toolkit [Kudo, 2005] for conditional random fields [Lafferty et al.,

2001], which implements both L-BFGS optimization for CRFs [Nash and Nocedal, 1991,
Malouf, 2002] as well as “structured MIRA” [Crammer and Singer, 2003, McDonald et al.,
2004].
CRF SGD A stochastic gradient descent conditional random field package [Bottou, 2011].
Structured Perceptron An implementation of the structured perceptron [Collins, 2002] due to
[Chang et al., 2013].
Structured SVM The cutting-plane implementation [Joachims et al., 2009] of the structured SVMs
[Tsochantaridis et al., 2004] for “HMM” problems.
Structured SVM (DEMI-DCD) A multicore algorithm for optimizing structured SVMs called DEcoupled Model-update and Inference with Dual Coordinate Descent.
VW Search Our approach is implemented in the Vowpal Wabbit [Langford et al., 2007] toolkit on
top of a cost-sensitive classifier [Beygelzimer et al., 2005] that reduces to regression trained
with an online rule incorporating AdaGrad [Duchi et al., 2011], per-feature normalized updates [Ross et al., 2013], and importance invariant updates [Karampatziakis and Langford,
2011].
VW Classification An unstructured baseline that predicts each label independently, using oneagainst-all multiclass classification [Beygelzimer et al., 2005].
These approaches vary both objective function (CRF, MIRA, structured SVM, learning to search)
and optimization approach (L-BFGS, cutting plane, stochastic gradient descent, AdaGrad). All
implementations are in C/C++, except for the structured perceptron and DEMI-DCD (Java).
5.1

Methodology

Comparing different systems is challenging because one wishes to hold constant as many variables
as possible. In particular, we want to control for both features and hyperparameters. In general, if
a methodological decision cannot be made “fairly,” we made it in favor of competing approaches.
To control for features, we use the built-in feature template approach of CRF++ (duplicated in CRF
SGD) to generate features. The other approaches (Structured SVM, VW Search and VW Classification) all use the features generated (offline) by CRF++. For each task, we tested six feature templates
and picked the one with best development performance using CRF++. The templates included neighboring words and, in the case of NER, neighboring POS tags. However, because VW Search is also
6

POS
NER

Sents
38k
15k

Toks
912k
205k

Training
Labels Features
45 13,685k
7
8,592k

Unique Fts
629k
347k

Heldout
Sents Toks
5.5k 132k
3.5k
52k

Test
Sents Toks
5.5k 130k
3.6k
47k

Table 1: Basic statistics about the data sets used for part of speech (POS) tagging and named entity
recognition (NER).

able to generate features from its own templates, we also provide results for VW Search (own fts)
in which it uses its own, internal, feature template generation, which were tuned to maximize it’s
heldout performance on the most time-consuming run (4 passes) and include neighboring words
(and POS tags, for NER) and word prefixes/suffixes.4 In all cases we use first order Markov dependencies, which lessens the speed advantage of search based structured prediction.
To control for hyperparameters, we first separated each system’s hyperparameters into two sets: (1)
those that affect termination condition and (2) those that otherwise affect model performance. When
available, we tune hyperparameters for (a) learning rate and (b) regularization strength5 . Additionally, we vary the termination conditions to sweep across different amounts of time spent training.
For each termination condition, we can compute results using either the default hyperparameters
or the tuned hyperparameters that achieved best performance on heldout data. We report both
conditions to give a sense of how sensitive each approach is to the setting of hyperparameters (the
amount of hyperparameter tuning directly affects effective training time).
One final confounding issue is that of parallelization. Of the baseline approaches, only CRF++
supports parallelization via multiple threads at training time. In our reported results, CRF++’s time
is the total CPU time (i.e., effectively using only one thread). Experimentally, we found that wall
clock time could be decreased by a factor of 1.8 by using 2 threads, a factor of 3 using 4 threads,
and a (plateaued) factor of 4 using 8 threads. This should be kept in mind when interpreting results.
DEMI-DCD (for structured SVMs) also must use multiple threads. To be as fair as possible, we
used 2 threads. Likewise, it can be sped up more using more threads [Chang et al., 2013]. VW
(Search and Classification) can also easily be parallelized using AllReduce [Agarwal et al., 2011].
We do not conduct experiments with this option here because none of our training times warranted
parallelization (a few minutes to train, max).

5.2

Task Specifics

Part of speech tagging for English is based on the Penn Treebank tagset that includes 45 discrete
labels for different parts of speech. The overall accuracy reported is Hamming accuracy (number
of tokens tagged correctly). This is a pure sequence labeling task. We use 912k tokens (words) of
training data and approximately 50k tokens of heldout data and test data. The CRF++ templates
generate 630k unique features for the training data; additional statistics are in Table 1.
Named entity recognition for English is based on the CoNLL 2003 dataset that includes four entity
types: Person, Organization, Location and Miscellaneous. We report accuracy as macro-averaged
F-measure over the correct identification and labeling of these entity spans (the standard evaluation
metric). In order to cast this chunking task as a sequence labeling task, we use the standard BeginIn-Out (BIO) encoding, though some results suggest other encodings may be preferable [Ratinov
and Roth, 2009]. The example sentence from Figure 1 in this encoding is:
LOC
ORG
PER
z }| {
z
}|
{
z
}|
{
Germany ’s rep to the European Union ’s committee Werner Zwingmann said . . .
B-LOC

O O

O

O

B-ORG

I-ORG O

O

B-PER

I-PER

O

In our system, the only change made to the sequence labeling algorithm (Algorithm 1) is that I-x
may only follow B-x or I-x. We still optimize Hamming loss because macro-averaged F measure
does not decompose over individual sentences.
7

Part of speech tagging (tuned hps)

0.96

96.6
96.1 95.896.1
95.7

Named entity recognition (tuned hps)

0.94

0.90
0.88

1m

100

0.75

95.7

95.0

0.92

79.280.0
76.5

0.80

VW Search
VW Search (own fts)
90.7
VW Classification
CRF SGD
CRF++
Str. Perceptron
SVM
10m
30m 1hStructured
Str.SVM (DEMI-DCD)
101
102
103
Training Time (minutes)

F-score (per entity)

Accuracy (per tag)

0.98

73.3

76.5

78.3
75.9
74.6

0.70
0.65
0.60
0.55
0.50
0.45

10s

10-1

1m

100

Training Time (minutes)

10m

101

Figure 2: Training time versus evaluation accuracy for part of speech tagging (left) and named
entity recognition (right). X-axis is in log scale. Different points correspond to different termination
criteria for training. Both figures use hyperparameters that were tuned (for accuracy) on the heldout
data. (Note: lines are curved due to log scale x-axis.)

5.3

Efficiency versus Accuracy

In Figure 2, we show trade-offs between training time (x-axis, log scaled) and prediction accuracy
(y-axis) for the six systems described previously. The left figure is for part of speech tagging (912k
training tokens) and the right figure is for named entity recognition (205k training tokens).
For POS tagging, the independent classifier is by far the fastest (trains in less than one minute) but its
performance peaks at 95% accuracy. Three other approaches are in roughly the same time/accuracy
tradeoff: VW Search, VW Search (own fts) and Structured Perceptron. All three can achieve very
good prediction accuracies in just a few minutes of training. CRF SGD takes about twice as long.
DEMI-DCD eventually achieves the same accuracy, but it takes a half hour. CRF++ is not competitive
(taking over five hours to even do as well as VW Classification). Structured SVM (cutting plane implementation) looks promising, but runs out of memory before achieving competitive performance
(likely due to too many constraints).
For NER the story is a bit different. The independent classifiers are quite fast (a few seconds to train)
but are far from being competitive6 . Here, the two variants of VW Search totally dominate.7 . In this
case, Structured Perceptron, which did quite well on POS tagging, is no longer competitive and is
essentially dominated by CRF SGD. The only system coming close to VW Search’s performance is
DEMI-DCD, although it’s performance flattens out after a few minutes.8
To achieve the results in Figure 2 required fairly extensive hyperparameter tuning (on the order of 50
to 100 different runs for each system). To see the effects of hyperparameter tuning, we also ran each
system with the built-in hyperparameter options.9 The trends in the runs with default hyperparame4

The exact templates used are provided in the supplementary materials.
Precise details of hyperparameters tuned and their ranges is in the supplementary materials.
6
When evaluating F measure for a system that may produce incoherent tag sequences, like “O I-LOC” we
replace any malpositioned I-x with B-x; of all heuristics we tried, this worked best.
7
We verified the prediction performance with great care here—it is the first time we have observed learning
to search approaches significantly exceeding the prediction performance of other structured prediction techniques when the feature information available is precisely the same.
8
We also tried giving CRF SGD the features computed by VW Search (own fts) on both POS and NER.
On POS, its accuracy improved to 96.5—on par with VW Search (own fts)—with essentially the same speed.
On NER it’s performance decreased. For both tasks, clearly features matter. But which features matter is a
function of the approach being taken.
9
The only exceptions is Structured SVMs, which do not have a default C value (we used C = 128 because
that setting won most often across all experiments).
5

8

Part of speech tagging (default hps)

0.98

96.5
96.1
95.7
95.5

79.279.9
76.5

95.0

0.75

96.0
95.1

0.94
0.92
90.7

0.90

F-score (per entity)

0.96

Accuracy (per tag)

Named entity recognition (default hps)
0.80

0.88

10m

30m 1h

101

100

0.65
0.60
0.55

Training Time (minutes)

0.45

102

74.4

0.70

0.50
1m

73.3

77.9
74.5

VW Search
VW Search (own fts)
VW Classification
CRF SGD
CRF++
Str. Perceptron
48.6
Structured SVM
10s
1m
Str.SVM (DEMI-DCD)
10-1
100
Training Time (minutes)

10m

101

Figure 3: Training time versus evaluation accuracy for POS tagging (left) and NER (right). X-axis
is in log scale. Different points correspond to different termination criteria for training. Both figures
use default hyperparameters.

Prediction (test-time) Speed
POS

13
5.7

129
133

5.3
14
5.6

NER

98

24

0

VW Search
VW Search (own fts)
CRF SGD
CRF++
Str. Perceptron
Structured SVM
Str. SVM (DEMI-DCD)
218

50

100

150

200

285

250

300

Thousands of Tokens per Second
Figure 4: Comparison of test-time efficiency of the different approaches in thousands of tokens per
second. For NER, this ranges from 5k tokens/sec (DEMI-DCD) to over a quarter million tokens/sec.
These numbers include feature computation time only for the two CRF approaches.
ters (Figure 3) show similar behavior to those with tuned, though some of the competing approaches
suffer significantly in prediction performance. Structured Perceptron has no hyperparameters.

6

Test-time Prediction Performance

In addition to training time, one might care about test time behavior. On NER, prediction times
varied from 5.3k tokens/second (DEMI-DCD and Structured Perceptron to around 20k (CRF SGD
and Structured SVM) to 100k (CRF++) to 220k (VW (own fts)) and 285k (VW). Although CRF SGD
and Structured Perceptron fared well in terms of training time, their test-time behavior is suboptimal.
When looking at POS tagging, the effect of the O(k 2 ) dependence on the size of the label set further
increased the (relative) advantage of VW Search over the alternatives.
Figure 4 shows the speed at which the different systems can make predictions on raw text. Structured
SVMs and friends (DEMI-DCD and Structured Perceptron) are by far the slowest (NER: 14k tokens
per second), followed closely by CRF SGD (NER: 24k t/s). This is disappointing because CRF SGD
performed very well in terms of training efficiency. CRF++ achieves respectable test-time efficiency
9

(NER: almost 100k t/s).10 VW Search using CRF++’s features is the fastest (NER: 285k t/s) but,
like Structured SVM, this is a bit misleading because it requires feature computation from CRF++ to
be run as a preprocessor. A fairer comparison is VW Search (own fts), which runs on (nearly) raw
text and achieves a speed of 218k t/s for NER.
One thing that is particularly obvious comparing the prediction speed for VW Search against the
other three approaches is the effect of the size of the label space. When the number of labels
increases from 9 (NER) to 45 (POS), the speed of VW Search is about halved. For the others, it is
cut down by as much as a factor of 8. This is a direct complexity argument. Prediction time for VW
Search is O(tkf ) versus O(tk 2 f ) for all other approaches.
Overall, we found our approach to achieve comparable or higher accuracy in as little or less time,
both with tuned hyperparameters and default hyperparameters. The closest competitor for POS
tagging was the Structured Perceptron (but that did poorly on NER); the closest competitor for NER
was CRF SGD (but that was several times slower on POS tagging).
6.1

Empirical evaluation of path-collapse

In Section 4, we discussed two approaches for computational improvements. First, memoization:
avoid re-predicting on the same input multiple times (which is fully general). Second, snapshot
restoration: to jump to an arbitrary desired position in the search space (which requires a historyindependent loss function). Both are effectively only when paths collapse frequently.
The effect of different optimizations (none, memoization alone, or memoization combined with
snapshot restoration) is shown below. Columns are internal loss, F measure on heldout data, number
of predictions made, time to train and one standard deviation of training time over 5 runs.
Optimization
All
Memoization
None

heldout
loss
0.426
0.432
0.431

heldout
F-score
85.6%
85.6%
85.4%

# training
predictions
1,722,173
1,724,957
18,620,344

training
time
1.24m ±0.16
1.76m ±0.09
2.23m ±0.04

The above results show the effect of these optimizations on the best NER system we trained, which
achieved a test F score of 79.9% in 1.24m. In this table, we can see that memoization alone reduces
the number of predictions made by over 90%, with only a very small increase of 0.001 in loss on
the heldout data (the loss reported here in the internal average per-sequence Hamming loss, rather
than F measure). Recall that this optimization is only provably correct in Searn mode, not DAgger
mode as run here. The memoization reduces overall runtime by about 21% because not all time
is being spent making predictions. When the second optimization (snapshot jumps) is enabled, the
total number of predictions drops imperceptibly, but the runtime improves by another 30%, yielding
a total improvement of about 45% over the baseline. Note that the loss here actually goes down
slightly, perhaps due to a slightly less noisy cost function.

7

Relation to Probabilistic Programming

Probabilistic programming [Gordon et al., 2014] has been an active area of research for the past
decade or more. While our approach bears a family resemblace to the idea of probabilistic programming, it differs in two key ways. First, we have not designed a new programming language. Instead
we have a three-function library. This is advantageous because it makes adoption easier. Moreover,
our library is in C/C++, which makes integration into existing code bases (relatively) easy. Second,
the abstract we focus on is that of prediction. In contrast, the typical abstraction for probabilistic
programming is distributions. We believe that prediction is a more natural abstraction for a lay
programmer to think about than probability distributions.
The closest work to ours is Factorie [McCallum et al., 2009]. Factorie is a domain specific language
embedded in Scala, and is essentially an embedded language for writing factor graphs. It compiles
10
This suggests gains are possible by combining
mizer.

CRF++’s

10

“decoder” and I/O system with

CRF SGD’s

opti-

them into Scala, which in turn produces JVM code that can be run reasonably efficiently. Nonetheless, as far as we are aware, Factorie-based implementations of simple tasks like sequence labeling
are still less efficient than systems like CRF SGD. Factorie, more than other probabilistic programming languages we are aware of, acts more like a library than a language; though it’s abstraction
is still distributions (more precisely: factor graphs). Another approach which takes the approach of
formulating a library is Infer.NET, from Minka et al. [2010]. Infer.NET is a library for constructing probabilistic graphical models in a .NET programming framework. It supports approximate
inference methods for things like variational inference and message passing.
In the same spirit as Factorie of having a concise programmic method of specifying factors in a
Markov network are: Markov Logic Networks (MNLs) due to Richardson and Domingos [2006]
and Probabilistic Soft Logic (PSL) due to Kimmig et al. [2012]. Although neither of these was
derived specifically from the perspective of formulating factors in a conditional random field (or
hinge-loss Markov network), that is the net result. Neither of these is an embedded language: one
must write declarative code and provide data in an appropriate format, which makes it somewhat
difficult to use in complex systems. BLOG [Milch et al., 2007] falls in the same category, though
with a very different focus. Similarly, Dyna [Eisner et al., 2005] is a related declarative language
for specifying probabilistic dynamic programs which can be compiled into C++ (and then used as
library code inside another C++ program). All of these example have picked particular aspects of
the probabilistic modeling framework to focus on.
Beyond these examples, there are several approaches that essentially “reinvent” an existing programming language to support probabilistic reasoning at the first order level. IBAL [Pfeffer, 2001]
derives from O’Caml; Church [Goodman et al., 2008] derives from LISP. IBAL uses a (highly optimized) form of variable elimination for inference that takes strong advantage of the structure of the
program; Church uses MCMC techniques, coupled with a different type of structural reasoning to
improve efficiency.
It is worth noting that most of these approaches have a different goal than we have. Our goal is to
build a framework that allows a developer to solve a quite general, but still specific type of problem:
learning to solve sequencial decision-making problems (“learning to search”). The goal of (most)
probabilistic programming languages is to provide a flexible framework for specifying graphical
models and performing inference in those models. While these two goals are similar, they are
different enough that the minimalistic library approach we have provided is likely to be insufficient
for general graphical model inference.

8

Discussion

We have shown a new abstraction for a structured prediction library that yields state-of-the-art or
better prediction accuracies on two tasks, with runtimes up to two orders of magnitude faster than
competing approaches. Moreover, we achieve this with minimal programming effort on the part
of the developer who must implement RUN. Our sequence labeling implementation is 6 lines of
code; compared to: CRF SGD at 1068 LOC, CRF++ at 777 LOC and Structured SVM at 876 LOC.11
Somewhat surprisingly, this is all possible through a very simple (three function) library interface
which does not require the development of an entirely new programming language. This is highly
advantageous as it allows very easy adoption. Moreover, since our library functions in a reduction
stack, as base classifiers and reductions for cost-sensitive classification improve, so does structured
prediction performance.



Contextual bandit learning is a reinforcement
learning problem where the learner repeatedly receives a set of features (context), takes an action
and receives a reward based on the action and
context. We consider this problem under a realizability assumption: there exists a function in
a (known) function class, always capable of predicting the expected reward, given the action and
context. Under this assumption, we show three
things. We present a new algorithm—Regressor
Elimination— with a regret similar to the agnostic setting (i.e. in the absence of realizability assumption). We prove a new lower bound
showing no algorithm can achieve superior performance in the worst case even with the realizability assumption. However, we do show that
for any set of policies (mapping contexts to actions), there is a distribution over rewards (given
context) such that our new algorithm has constant regret unlike the previous approaches.

1 Introduction
We are interested in the online contextual bandit setting,
where on each round we first see a context x ∈ X , based
on which we choose an action a ∈ A, and then observe
a reward r. This formalizes several natural scenarios. For
example, a common task at major internet engines is to display the best ad from a pool of options given some context such as information about the user, the page visited,
the search query issued etc. The action set consists of the
candidate ads and the reward is typically binary based on
whether the user clicked the displayed ad or not. Another
Appearing in Proceedings of the 15th International Conference on
Artificial Intelligence and Statistics (AISTATS) 2012, La Palma,
Canary Islands. Volume XX of JMLR: W&CP XX. Copyright
2012 by the authors.

natural application is the design of clinical trials in the medical domain. In this case, the actions are the treatment options being compared, the context is the patient’s medical
record and reward is based on whether the recommended
treatment is a success or not.
Our goal in this setting is to compete with a particular set
of policies, which are deterministic rules specifying which
action to choose in each context. We note that this setting includes as special cases the classical K-armed bandit problem (Lai and Robbins, 1985) and associative reinforcement learning with linear reward functions (Auer,
2003; Chu et al., 2011).
The performance of algorithms in this setting is typically
measured by the regret, which is the difference between the
cumulative reward of the best policy and the algorithm. For
the setting with an arbitrary
set of policies, the achieved rep
gret guarantee is O( KT ln(N/δ)) where K is the number of actions, T is the number of rounds, N is the number of policies and δ is the probability of failing to achieve
the regret (Beygelzimer et al., 2010; Dudı́k et al., 2011).
While this bound has a desirably small dependence on the
parameters T, N , the scaling with respect to K is often too
big to be meaningful. For instance, the number of ads under consideration can be huge, and a rapid scaling with the
number of alternatives in a clinical trial is clearly undesirable. Unfortunately, the dependence on K is unavoidable
as proved by existing lower bounds (Auer et al., 2003).
Large literature on “linear bandits” manages to avoid this
dependence on K by making additional assumptions. For
example, Auer (2003) and Chu et al. (2011) consider the
setting where the context x consists of feature vectors xa ∈
Rd describing each action, and the expected reward function (given a context x and action a) has the form wT xa
for some fixed vector w ∈ Rd . Dani et al. (2008) consider
a continuous action space with a ∈ Rd , without contexts,
with a linear expected reward wT a, which is generalized
by Filippi et al. (2010) to σ(wT a) with a known Lipschitzcontinuous link function σ. A striking aspect of the linear and generalized linear setting is that while the regret

Contextual Bandit Learning with Predictable Rewards

grows rapidly with the dimension d, it grows either only
gently with the number of actions K (poly-logarithmic
for Auer, 2003), or is independent of K (Dani et al., 2008;
Filippi et al., 2010). In this paper, we investigate whether
a weaker dependence on the number of actions is possible
in more general settings. Specifically, we omit the linearity
assumption while keeping the “realizability”—i.e., we still
assume that the expected reward can be perfectly modeled,
but do not require this to be a linear or a generalized linear
model.
We consider an arbitrary class F of functions f : (X , A) →
[0, 1] that map a context and an action to a real number. We
interpret f (x, a) as a predicted expected reward of the action a on context x and refer to functions in F as regressors. For example, in display advertising, the context is
a vector of features derived from the text and metadata of
the webpage and information about the user. The action
corresponds to the ad, also described by a set of features.
Additional features might be used to model interaction between the ad and the context. A typical regressor for this
problem is a generalized linear model with a logistic link,
modeling the probability of a click.
The set of regressors F induces a natural set of policies
ΠF containing maps πf : X → A defined as πf (x) =
argmaxa f (x, a). We make the assumption that the expected reward for a context x and action a equals f ∗ (x, a)
for some unknown function f ∗ ∈ F . The question we address in this paper is: Does this realizability assumption
allow us to learn faster?
We show that for an arbitrary function
√ class, the answer
to the above question is “no”. The K dependence in regret is in general unavoidable even with the realizability
assumption. Thus, the structure of linearity or controlled
non-linearity was quite important in the past works.
Given this answer, a natural question is whether it is at least
possible to do better in various special cases. To answer
this, we create a new natural algorithm, Regressor Elimination (RE), which takes advantage of realizability. Structurally, the algorithm is similar to Policy Elimination (PE)
of Dudı́k et al. (2011), designed for the agnostic case (i.e,
the general case without realizability assumption). While
PE proceeds by eliminating poorly performing policies,
RE proceeds by eliminating poorly predicting regressors.
However, realizability assumption allows much more aggressive elimination strategy, different from the strategy
used in PE. The analysis of this elimination strategy is the
key technical contribution of this paper.
Thepgeneral regret guarantee for Regressor Elimination is
O( KT ln(N T /δ)), similar to the agnostic case. However, we also show that for all sets of policies Π there exists
a set of regressors F such that Π = ΠF and the regret of
Regressor Elimination is O(ln(N/δ)), i.e., independent of
the number of rounds and actions. At the first sight, this

seems to contradict our worst-case lower bound. This apparent paradox is due to the fact that the same set of policies
can be generated by two very different sets of regressors.
Some regressor sets allow better discrimination of the true
reward function, whereas some regressor sets will lead to
the worst-case guarantee.
The remainder of the paper is organized as follows. In
the next section we formalize our setting and assumptions.
Section 3 provides our algorithm which is analyzed in Section 4. In Section 5 we present the worst-case lower bound,
and in Section 6, we show an improved dependence on
K in favorable cases. Our algorithm assumes the exact
knowledge of the distribution over contexts (but not over
rewards). In Section 7 we sketch how this assumption can
be removed. Another major assumption is the finiteness of
the set of regressors F . This assumption is more difficult
to remove, as we discuss in Section 8.

2 Problem Setup
We assume that the interaction between the learner and nature happens over T rounds. At each round t, nature picks
a context xt ∈ X and a reward function rt : A → [0, 1]
sampled i.i.d. in each round, according to a fixed distribution D(x, r). We assume that D(x) is known (this assumption is removed in Section 7), but D(r|x) is unknown. The
learner observes xt , picks an action at ∈ A, and observes
the reward for the action rt (at ). We are given a function
class F : X × A → [0, 1] with |F | = N , where |F | is
the cardinality of F . We assume that F contains a perfect
predictor of the expected reward:
Assumption 1 (Realizability). There exists a function f ∗ ∈
F such that Er|x [r(a)] = f ∗ (x, a) for all x ∈ X , a ∈ A.
We recall as before that the regressor class F induces the
policy class ΠF containing maps πf : X → A defined by
f ∈ F as πf (x) = argmaxa f (x, a). The performance of
an algorithm is measured by its expected regret relative to
the best fixed policy:
regretT = sup

T h
i
X

f ∗ xt , πf (xt ) − f ∗ (xt , at ) .

πf ∈ΠF t=1

By definition of πf , this is equivalent to
regretT =

T h
i
X

f ∗ xt , πf ∗ (xt ) − f ∗ (xt , at ) .
t=1

3 Algorithm
Our algorithm, Regressor Elimination, maintains a set of
regressors that accurately predict the observed rewards. In
each round, it chooses an action that sufficiently explores

Agarwal, Dudı́k, Kale, Langford and Schapire

among the actions represented in the current set of regressors (Steps 1–2). After observing the reward (Step 3), the
inaccurate regressors are eliminated (Step 4).
Sufficient exploration is achieved by solving the convex optimization problem in Step 1. We construct a distribution
Pt over current regressors, and then act by first sampling
a regressor f ∼ Pt and then choosing an action according to πf . Similarly to the Policy Elimination algorithm
of Dudı́k et al. (2011), we seek a distribution Pt such that
the inverse probability of choosing an action that agrees
with any policy in the current set is in expectation bounded
from above. Informally, this guarantees that actions of any
of the current policies are chosen with sufficient probabilities. Using this construction we relate the accuracy of regressors to the regret of the algorithm (Lemma 4.3).
A priori, it is not clear whether the constraint (3.1) is even
feasible. We prove feasibility by a similar argument as
in Dudı́k et al. (2011) (see Lemma A.1 in Appendix A).
Compared with Dudı́k et al. (2011) we are able to obtain
tighter constraints by doing a more careful analysis.
Our elimination step (Step 4) is significantly tighter than a
similar step in Dudı́k et al. (2011): we eliminate regressors
according to a very strict O(1/t) bound on the suboptimality of the least squares error. Under the realizability assumption, this stringent constraint will not discard the optimal regressor accidentally, as we show in the next section.
This is the key novel technical contribution of this work.
Replacing D(x) in the Regressor Elimination algorithm
with the empirical distribution over observed contexts is
straightforward, as was done in Dudı́k et al. (2011), and is
discussed further in Section 7.

Algorithm 1 Regressor Elimination
Input:
a set of reward predictors F = {f : (X , A) → [0, 1]}
distribution D over contexts, confidence parameter δ.
Notation:
πf (x) := argmaxa′ f (x, a′ ).
Pt
R̂t (f ) := 1t t′ =1 (f (xt′ , at′ ) − rt′ (at′ ))2 .
For F ′ ⊆ F , define
A(F ′ , x) := {a ∈ A : πf (x) = a for some f ∈ F ′ }
√
µ := min{1/2K, 1/ T }.
For a distribution P on F ′ ⊆ F , define conditional distribution P ′ (·|x) on A as:
w.p. (1 − µ), sample f ∼ P and return πf (x), and
w.p. µ, return a uniform random a ∈ A(F ′ , x).
δt = δ/2N t3 log2 (t), for t = 1, 2, . . . , T .
Algorithm:
F0 ← F
For t = 1, 2, . . . , T :
1. Find distribution Pt on Ft−1 such that




1
∀f ∈ Ft−1 : E ′
≤ E |A(Ft−1 , x)| (3.1)
x Pt (πf (x)|x)
x
2. Observe xt and sample action at from Pt′ (·|xt ).
3. Observe rt (at ).
4. Set


18 ln(1/δt )
Ft = f ∈ Ft−1 : R̂t (f ) < ′min R̂t (f ′ ) +
f ∈Ft−1
t

4 Regret Analysis
Here we prove an upper bound on the regret of Regressor
Elimination. The proved bound is no better than the one for
existing agnostic algorithms. This is necessary, as we will
see in Section 5, where we prove a matching lower bound.
Theorem 4.1. For all sets of regressors F with |F | = N
and all distributions D(x, r), with probability
1 − δ, the
p
regret of Regressor Elimination is O( KT ln(N T /δ)).

Proof. By Lemma 4.1 (proved below), in round t if we
sample an action by sampling f frompPt and choosing
πf (xt ), then the expected regret is O( K ln(N T /δ)/t)
with probability at least 1 − δ/2t2 . The excess regret for sampling a uniform random action is at most
Summing up over all the
µ ≤ √1T per round.
T rounds and taking
a
union
bound,
the total exp

pected regret is O KT ln(N T /δ) with probability
at least 1 − δ. Further, the net regret is a martingale; hence the Azuma-Hoeffding inequality with range
[0, 1] applies. So with probability at least 1 − δ we

p
p

have a regret of O KT ln(N T /δ) + T ln(1/δ) =
p

O KT ln(N T /δ) .

Lemma 4.1. With probability at least 1 − δt N t log2 (t) ≥
1 − δ/2t2 , we have:
1. f ∗ ∈ Ft .
2. For any f ∈ Ft ,
E [r(πf (x)) − r(πf ∗ (x))] ≤

x,r

r

200K ln(1/δt )
.
t

Proof. Fix an arbitrary function f ∈ F . For every round t,
define the random variable
Yt = (f (xt , at ) − rt (at ))2 − (f ∗ (xt , at ) − rt (at ))2 .
Here, xt is drawn from the unknown data distribution D,
rt is drawn from the reward distribution conditioned on xt ,
and at is drawn from Pt′ (which is defined conditioned on

Contextual Bandit Learning with Predictable Rewards

the choice of xt and is independent of rt ). Note that this
random variable is well-defined for all functions f ∈ F ,
not just the ones in Ft .
Let Et [·] and Vart [·] denote the expectation and variance
conditioned on all the randomness up to round t. Using
a form of Freedman’s inequality from Bartlett et al. (2008)
(see Lemma B.1) and noting that Yt ≤ 1, we get that with
probability at least 1 − δt log2 (t), we have
t
X

t′ =1

Et′ [Yt′ ] −

t
X

Summing up over all t′ ≤ t, and using (4.1) along with
Jensen’s inequality we get that
r
200K ln(1/δt )
.
E [r(πf (x)) − r(πf ∗ (x))] ≤
t
x,r
Lemma 4.2. Fix a function f ∈ F . Suppose we sample
x, r from the data distribution D, and an action a from an
arbitrary distribution such that r and a are conditionally
independent given x. Define the random variable
Y = (f (x, a) − r(a))2 − (f ∗ (x, a) − r(a))2 .

Yt′

t′ =1

Then we have

v
u t
uX
≤ 4t
Vart′ [Yt′ ] ln(1/δt ) + 2 ln(1/δt ).



∗
2
E [Y ] = E (f (x, a) − f (x, a))

From Lemma 4.2, we see that Vart′ [Yt′ ] ≤ 4 Et′ [Yt′ ] so
t
X

t′ =1

Et′ [Yt′ ] −

t
X

Yt′

Var[Y ] ≤ 4 E [Y ].
x,r,a

∗
∗
− 2ra ) .
Y = (fxa − fxa
)(fxa + fxa

∗
∗
E [Y ] = E [(fxa − fxa )(fxa + fxa − 2ra )]

x,r,a

qP
t

x,a

X 2 − Z ≤ 8CX + 2C 2 ⇔ (X − 4C)2 − Z ≤ 18C 2 .
This gives −Z ≤ 18C 2 . Since Z = t(R̂t (f ) − R̂t (f ∗ )),
we get that

Furthermore, suppose f is also not eliminated and survives
in Ft . Then we must have R̂t (f ) − R̂t (f ∗ ) ≤ 18C 2 /t, or
in other words, Z ≤ 18C 2 . Thus, (X − 4C)2 ≤ 36C 2 ,
which implies that X 2 ≤ 100C 2 , and hence:
(4.1)

By Lemma 4.3 and since Pt is measurable with respect to
the past sigma field up to time t − 1, for all t′ ≤ t we have
2
E [r(πf (x)) − r(πf ∗ (x))] ≤ 2K



,

proving the first part of the lemma. From (4.2), noting that
∗
fxa , fxa
, ra are between 0 and 1, we obtain
∗ 2
≤ 4(fxa − fxa
) ,

yielding the second part of the lemma:


∗ 2
Var[Y ] ≤ E [Y 2 ] ≤ 4 E (fxa − fxa
)
x,r,a

and so f ∗ is not eliminated in any elimination step and remains in Ft .

x,r

x,a

∗ 2
fxa
)

∗ 2
∗
Y 2 ≤ (fxa − fxa
) (fxa + fxa
− 2ra )2

18 ln(1/δt )
t′

Et′ [Yt′ ] ≤ 100 ln(1/δt ).

r|x



= E (fxa −

18C 2
.
t

By a union bound, with probability at least 1 −
δt N t log2 (t), for all f ∈ F and all rounds t′ ≤ t, we have

t′ =1

x,r,a

∗
∗
)(fxa + fxa
− 2ra )]
= E E [(fxa − fxa
x,a r|x



∗
∗
= E (fxa − fxa ) fxa + fxa − 2 E [ra ]

For notational convenience, define X =
t′ =1 Et′ [Yt′ ],
p
Pt
Z = t′ =1 Yt′ , and C = ln(1/δt ). The above inequality is equivalent to:

t
X

(4.2)

Hence, we have

t′ =1

R̂t′ (f ∗ ) ≤ R̂t′ (f ) +

x,r,a

Proof. Using shorthands fxa for f (x, a) and ra for r(a),
we can rearrange the definition of Y as

t′ =1

v
u t
uX
Et′ [Yt′ ] ln(1/δt ) + 2 ln(1/δt ).
≤ 8t

R̂t (f ∗ ) ≤ R̂t (f ) +

x,a

x,r,a

t′ =1

Et′ [Yt′ ].
xt′ ,rt′ ,at′

x,r,a

x,r,a

= 4 E [Y ] .
x,r,a

Next we show how the random variable Y defined in
Lemma 4.2 relates to the regret in a single round:
Lemma 4.3. In the setup of Lemma 4.2, assume further
that the action a is sampled from a conditional distribution
p(·|x) which satisfies the following constraint, for f ′ = f
and f ′ = f ∗ :


1
≤ K.
(4.3)
E
x p(πf ′ (x)|x)
Then we have
h

i2
≤ 2K E [Y ].
E r πf ∗ (x) − r πf (x)
x,r

x,r,a

Agarwal, Dudı́k, Kale, Langford and Schapire

This lemma is essentially a refined form of theorem 6.1
in Beygelzimer and Langford (2009) which analyzes the
regression approach to learning in contextual bandit settings.
Proof. Throughout, we continue using the shorthand fxa
for f (x, a). Given a context x, let ã = πf (x) and a∗ =
πf ∗ (x). Define the random variable
h

i
∗
∗
∆x = E r πf ∗ (x) − r πf (x) = fxa
∗ − fxã .
r|x

Note that ∆x ≥ 0 because f ∗ prefers a∗ over ã for context x. Also we have fxã ≥ fxa∗ since f prefers ã over a∗
for context x. Thus,
∗
∗
fxã − fxã
+ fxa
∗ − fxa∗ ≥ ∆x .

(4.4)

As in proof of Lemma 4.2,


∗ 2
E [Y ] = E (fxa − fxa )
a|x

r,a|x

∗ 2
∗
2
≥ p(ã|x)(fxã −fxã
) +p(a∗ |x)(fxa
∗ −fxa∗ )

≥

p(ã|x)p(a∗ |x)
∆2 .
p(ã|x) + p(a∗ |x) x

(4.5)

The last inequality follows by first applying the chain
ax2 + by 2 =

ab(x + y)2 + (ax − by)2
ab
≥
(x + y)2
a+b
a+b

(valid for a, b > 0), and then applying inequality (4.4).
For convenience, define
Qx =

p(ã|x)p(a∗ |x)
1
1
1
=
, i.e.,
+
.
p(ã|x) + p(a∗ |x)
Qx
p(ã|x) p(a∗ |x)

Now, since p satisfies the constraint (4.3) for f ′ = f and
f ′ = f ∗ , we conclude that





1
1
1
+
≤ 2K . (4.6)
=
E
E
E
x p(a∗ |x)
x p(ã|x)
x Qx


We now have


p
1
· Qx ∆x
E[∆x ] = E √
x
x
Qx




1
2
≤E
E Qx ∆x
x Qx
x
2

2

≤ 2K E [Y ] ,
x,r,a

where the first inequality follows from the CauchySchwarz inequality and the second from the inequalities
(4.5) and (4.6).

5 Lower bound
Here we prove a lower bound showing that the realizability
assumption is not enough in general to eliminate a dependence on the number of actions K. The structure of this
proof is similar to an earlier lower bound (Auer et al., 2003)
differing in two ways: it applies to regressors of the sort we
consider, and we work N , the number of regressors, into
the lower bound. Since for every policy there exists a regressor with argmax on that regressor realizing the policy,
this lower bound also applies to policy based algorithms.
Theorem 5.1. For every N and K such that ln N/ ln K ≤
T , and every algorithm A, there exists a function class F of
cardinality at most N and a distribution D(x, r) for which
the realizability
assumption holds, but the expected regret
p
of A is Ω( KT ln N/ ln K).
Proof. Instead of directly selecting
F and D for which the
p
expected regret of A is Ω( KT ln N/ ln K), we create a
distribution over instances
p (F, D) and show that the expected regret of A is Ω( KT ln N/ ln K) when the expectation is taken also over our choice of the instance. This
will immediately yield a statement of the theorem, since
the algorithm must suffer at least this amount of regret on
one of the instances.

The proof proceeds via a reduction to the construction
used in the lower bound of Theorem 5.1 of Auer et al.
(2003). We will use M different contexts for a suitable
number M . To define the regressor class F , we begin with
the policy class G consisting of all the K M mappings of
the form g : X → A, where X = {1, 2, . . . , M } and
A = {1, 2, . . . , K}. We require M to be the largest integer such that K M ≤ N , i.e., M = ⌊ln N/ ln K⌋. Each
mapping g ∈ G defines a regressor fg ∈ F as follows:
(
1/2 + ǫ if a = g(x)
fg (x, a) =
1/2
otherwise.
The rewards are generated by picking a function f ∈ F
uniformly at random at the beginning. Equivalently, we
choose a mapping g that independently maps each context
x ∈ X to a random action a ∈ A, and set f = fg . In each
round t, a context xt is picked uniformly from X . For any
action a, a reward rt (a) is generated as a {0, 1} Bernoulli
trial with probability of 1 being equal to f (x, a).
Now fix a context x ∈ X . We condition on all of the randomness of the algorithm A, the choices of the contexts xt
for t = 1, 2, . . . , T , and the values of g(x′ ) for x′ 6= x.
Thus the only randomness left is in the choice of g(x) and
the realization of the rewards in each round. Let P′ denote
the reward distribution where the rewards of any action a
for context x are chosen to be {0, 1} uniformly at random
(the rewards for other contexts x′ 6= x are still chosen according to f (x′ , a), however), and let E′ denote the expectation under P′ .

Contextual Bandit Learning with Predictable Rewards

Let Tx be the rounds t where the context xt is x. Now fix
an action a ∈ A and let Sa be a random variable denoting
the number of rounds t ∈ Tx when A chooses at = a.
Note that conditioned on g(x) = a, the random variable
Sa counts the number of rounds in Tx that A chooses the
optimal action a.

for t = 1, 2, . . . , T by taking an expectation, we get the
following lower bound on the expected regret of A:
!
X
ǫ2
3/2
Ω
ǫ E[|Tx |] − √ E[|Tx | ]
.
K
x∈X

We use a corollary of Lemma A.1 in Auer et al. (2003):

Note
 that
 |Tx | is distributed as Binomial(T, 1/M ). Thus,
E |Tx | = T /M . Furthermore, by Jensen’s inequality

Corollary 5.1 (Auer et al., 2003). Conditioned on the
choices of the contexts xt for t = 1, 2, . . . , T , and the values of g(x′ ) for x′ 6= x, we have
p
′
E[Sa |g(x) = a] ≤ E [Sa ] + |Tx | 2ǫ2 E′ [Sa ].

The proof uses the fact that when g(x) = a, rewards chosen
using P′ are identical to those from the true distribution
except for the rounds when A chooses the action a.
Thus, if Nx is a random variable that counts the number
the rounds in Tx that A chooses the optimal action for x
(without conditioning on g(x)), we have
E[Nx ] = E [E[Sg(x) ]]
g(x)
q
h
i
≤ E E′ [Sg(x) ] + |Tx | 2ǫ2 E′ [Sg(x) ]
g(x)

r
i
h


≤ E E′ [Sg(x) ] + |Tx | 2ǫ2 E E′ [Sg(x) ] ,
g(x)

g(x)

by Jensen’s inequality. Now note that
" "
##
h
i
X
′
′
1{at = g(x)}
E E [Sg(x) ] = E E
g(x)

g(x)

=

t∈Tx

X

E [ E [1{at = g(x)}]]

X

E

′

g(x)

t∈Tx

=

t∈Tx

′



1
K



=

|Tx |
.
K

The third equality follows because g(x) is independent of
the choices of the contexts xt for t = 1, 2, . . . , T , and g(x′ )
for x′ 6= x, and its distribution is uniform on A. Thus
r
|Tx |
|Tx |
+ |Tx | 2ǫ2
.
E[Nx ] ≤
K
K
Since in the rounds in Tx \ Nx , the algorithm A suffers an
expected regret of ǫ, the expected
regret of A over all the

2
rounds in Tx is at least Ω ǫ|Tx | − √ǫK |Tx |3/2 . Note that

this lower bound is independent of the choice of g(x′ ) for
x′ 6= x. Thus, we can remove the conditioning on g(x′ ) for
x′ 6= x and conclude that only conditioned on the choices
of the contexts xt for t = 1, 2, . . . , T , the expected regret
over
 of the algorithm
 all the rounds in Tx is at least
2
Ω ǫ|Tx | − √ǫK |Tx |3/2 . Summing up over all x, and removing the conditioning on the choices of the contexts xt

q 



3/2
≤
E |Tx |
E |Tx |3
1/2

3T (T − 1) T (T − 1)(T − 2)
T
+
+
=
M
M2
M3
√ 3/2
5T
,
≤
M 3/2

as long as M ≤ T . Plugging these bounds in, the lower
bound on the expected regret becomes


ǫ2
3/2
√
.
Ω ǫT −
T
KM
p

Choosing ǫ = Θ KM/T , we get that the expected regret of A is lower bounded by
p
√
Ω( KM T ) = Ω( KT ln N/ ln K) .

6 Analysis of nontriviality
Since the worst-case regret bound of our new algorithm is
the same as for agnostic algorithms, a skeptic could conclude that there is no power in the realizability assumption.
Here, we show that in some cases, realizability assumption
can be very powerful in reducing regret.
Theorem 6.1. For any algorithm A working with a set of
policies (rather than regressors), there exists a set of regressors F and a distribution D satisfying the realizability
assumption
such that the regret of A using the set ΠF is
√
Ω̃( T K ln N ), but the expected regret of Regressor Elimination using F is at most O ln(N/δ) .

Proof. Let F ′ be the set of functions and D the data distribution that achieve the lower bound of Theorem 5.1 for
the algorithm A. Using Lemma 6.1 (see below), there exists a set of functions F such that ΠF = ΠF ′ and the expected regret
 of Regressor Elimination using F is at most
O ln(N/δ) . This set of functions F and distribution D
satisfy the requirements of the theorem.

Lemma 6.1. For any distribution D and a set of policies Π
containing the optimal policy, there exists a set of functions
F satisfying the realizability assumption, such that Π =
ΠF and the regret
 of regressor elimination using F is at
most O ln(N/δ) .

Agarwal, Dudı́k, Kale, Langford and Schapire

Proof. The idea is to build a set of functions F such that
Π = ΠF , and for the optimal policy π ∗ the corresponding function f ∗ exactly gives the expected rewards for each
context x and a, but for any other policy π the corresponding function f gives a terrible estimate, allowing regressor
elimination to eliminate them quickly.
The construction is as follows. For π ∗ , we define the function f ∗ as f ∗ (x, a) = Ex,r [r(a)]. By optimality of π ∗ ,
πf ∗ = π ∗ . For every other policy π we construct an f
such that π = πf but for which f (x, a) is a very bad
estimate of Ex,r [r(a)] for all actions a. Fix x and consider two cases: the first is that Er|x [r(π(x))] > 0.75
and the other is that Er|x [r(π(x))] ≤ 0.75. In the first
case, we let f (x, π(x)) = 0.51. In the second case we let
f (x, π(x)) = 1.0. Now consider each other action a′ in
turn. If Er|x [r(a′ )] > 0.25 then we let f (x, a′ ) = 0, and if
Er|x [r(a′ )] ≤ 0.25 we let f (x, a′ ) = 0.5.
The regressor elimination algorithm eliminates regressor
with a too-large squared loss regret. Now fix any policy
π 6= π ∗ , and the corresponding f , define, as in the proof of
Lemma 4.1, the random variable
Yt = (f (xt , at ) − rt (at ))2 − (f ∗ (xt , at ) − rt (at ))2 .
Note that
1
,
Et [Yt ] = E [(f (xt , at ) − f (xt , at )) ] ≥
20
xt ,at
∗

2

(6.1)

1
since for all (x, a), (f (x, a)−f ∗ (x, a))2 ≥ 20
by construction. This shows that the expected regret is significant.

Now suppose f is not eliminated and remains in Ft . Then
by equation 4.1 we get:
t
X
t
≤
Et′ [Yt′ ] ≤ 100 ln(1/δt ).
20
′
t =1

The above bound holds with probability 1 − δt N t log2 (t)
uniformly for all f ∈ Ft . Using the choice of δt =
δ/2N t3 log2 (t), we note that the bound fails to hold when
t > 106 ln(N/δ). Thus, within 106 ln(N/δ) rounds all
suboptimal regressors are eliminated, and the algorithm
suffers no regret thereafter. Since the rewards are bounded
in [0, 1], the total regret in the first 106 ln(N/δ) rounds can
be at most 106 ln(N/δ), giving us the desired bound.

7 Removing the dependence on D
While Algorithm 1 is conceptually simple and enjoys nice
theoretical guarantees, it has a serious drawback that it depends on the distribution D from which the contexts xt ’s
are drawn in order to specify the constraint (3.1). A similar
issue was faced in the earlier work of Dudı́k et al. (2011),
where they replace the expectation under D with a sample

average over the contexts observed. We now discuss a similar modification for Algorithm 1 and give a sketch of the
regret analysis.
The key change in Algorithm 1 is to replace the constraint (3.1) with the sample version.
Let Ht =
{x1 , x2 , . . . , xt−1 }, and denote by x ∼ Ht the act of selecting a context x from Ht uniformly at random. Now we
pick a distribution Pt on Ft−1 such that
∀f ∈ Ft−1 : E

x∼Ht






1
≤ E |A(Ft−1 , x)|
′
Pt (πf (x)|x)
x∼Ht
(7.1)

Since Lemma A.1 applies to any distribution on the contexts, in particular, the uniform distribution on Ht , this
constraint is still feasible. To justify this sample based
approximation, we appeal to Theorem 6 of Dudı́k et al.
(2011) which shows that for any ǫ ∈ (0, 1) and t ≥
16K ln(8KN/δ), with probability at least 1 − δ


1
E
′
x∼D Pt (πf (x)|x)


7500
1
+ 3 K.
≤ (1 + ǫ) E
′
ǫ
x∼Ht Pt (πf (x)|x)
Using Equation (7.1), since |A(Ft−1 , xt′ )| ≤ K, we get


1
≤ 7525K,
E
′
x∼D Pt (πf (x)|x)
using ǫ = 0.999. The remaining analysis of the algorithm remains the same as before, except we now apply
Lemma 4.3 with a worse constant in the condition (4.3).

8 Conclusion
The included results gives us a basic understanding of the
realizable assumption setting: it can, but does not necessarily, improve our ability to learn.
We did not address computational complexity in this paper. There are some reasons to be hopeful however. Due
to the structure of the realizability assumption, an eliminated regressor continues to have an increasingly poor regret over time, implying that it may be possible to avoid the
elimination step and simply restrict the set of regressors we
care about when constructing a distribution. A basic question then is: can we make the formation of this distribution
computationally tractable?
Another question for future research is the extension to infinite function classes. One would expect that this just involves replacing the log cardinality with something like a
metric entropy or Rademacher complexity of F . This is not
completely immediate since we are dealing with martingales, and direct application of covering arguments seems

Contextual Bandit Learning with Predictable Rewards

√
to yield a suboptimal O(1/ t) rate in Lemma 4.1. Extending the variance based bound coming from Freedman’s
inequality from a single martingale to a supremum over
function classes would need a Talagrand-style concentration inequality for martingales which is not available in the
literature to the best of our knowledge. Understanding this
issue better is an interesting topic for future work.

Proof. Let ∆t−1 refer to the space of all distributions on
Ft−1 . We observe that ∆t−1 is a convex, compact set. For
a distribution Q ∈ ∆t−1 , define the conditional distribution
Q̃(·|x) on A as sample f ∼ Q, and return πf (x). Note
that Q′ (a|x) = (1 − µ)Q̃(a|x) + µ/Kx, where Kx :=
|A(Ft−1 , x)| for notational convenience.

Acknowledgements This research was done while AA,
SK and RES were visiting Yahoo!.

The feasibility of constraint (3.1) can be written as


1
min max E
≤ E [|A(Ft−1 , x)|] .
Pt ∈∆t−1 f ∈Ft−1 x Pt′ (πf (x)|x)
x


