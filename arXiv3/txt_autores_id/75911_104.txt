

Probably the most naive strategy is to perform simple gra­
dient ascent in the likelihood. This method has two prob­

Many applications require that we learn the pa­
rameters of a model from data. EM (E xpectation­
Maximization) is a method for learning the pa­
rameters of probabilistic models with missing or
hidden data. There are instances in which this
method is slow to converge.

Therefore, sev­

eral accelerations have been proposed to improve
the method. None of the proposed acceleration
methods are theoretically dominant and experi­
mental comparisons are lacking. In this paper,
we present the different proposed accelerations
and compare them experimentally. From the re­
sults of the experiments, we argue that some
acceleration of EM is always possible, but that
which acceleration is superior depends on prop­
erties of the problem.

lems. First, it is known to be relatively inefficient among
the class of local search methods. Second, it is often the

case that the model class A constrains the choice of param­
eters (}. For example, some of the components of

(}

may

be constrained to describe a probability distribution, and so
must be between 0 and 1 and must sum to

I. Naive gra­

dient methods must be specially modified to respect such
constraints.
The EM algorithm was described by Dempster et al. [1977]
as a generalization of the Baum-Welsh algorithm for learn­

ing hidden Markov models [Rabiner, 1989]. Typically, it
is monotonically convergent to a local optimum in likeli­
hood space for probabilistic models, while directly satisfy­
ing possible constraints on the parameters.
There are two main classes of strategies for finding
maximum-likelihood models with hidden parameters: ac­
celerated gradient methods with constraint handling and
EM. It has been observed empirically that, in some set­

INTRODUCTION

1

tings one algorithm appears to work better, and in other

There are many applications in artificial intelligence and
statistics that require the fitting of a parametric model to
data.

It is often desired to find the maximum-likelihood

(ML) or maximum-a-posteriori-probability (MAP) model

of the data. W hen all of the variables of the model are di­

settings, other algorithms appear to work better. Further­
more, a number of researchers, both in the statistics and
AI literatures, have proposed extensions, accelerations, and
combinations of these methods.
In this paper, we seek to understand the relative merits

rectly observable in the data, then this is relatively straight­

of these optimization strategies and their extensions. Al­

forward. When some variables are hidden, as is common

though there have been some attempts at theoretical com­
parisons of convergence rate [Xu and Jordan, 1996], the re­

in popular model classes such as Bayesian networks with
hidden variables and hidden Markov models, maximum­

sults are never clear-cut because they depend on properties

likelihood parameter estimation is much more complicated.

of the individual problems to which they are applied. We

The problem can be cast directly as an optimization prob­

have undertaken an empirical study in one of the simplest

D and a model of the form A(O), find

hidden-variable models: density estimation with a mixture

hood,p(DIA(O)) (orp(A(O)ID) in theMAPcase). Unfor­
tunately, the obj ective function does not have a form that

ations we can encounter, our only hope is to present em­

lem: given a data set

the setting of the parameters

(}

that maximizes the likeli­

can be easily optimized globally, so we are generally re­
duced to local search methods.
• This material
is based upon work supported in part un­
der a National Science Foundation Graduate Fellowship and by

DARPA/Rome Labs Planning Initiative grant F30602-95-l-0020.

1 This work was supported in part by DARPA/Rome Labs

Planning Initiative grant F30602-95-l-0020.

of Gaussians. Given the infinite number of possible situ­
pirical results suggesting the merits and disadvantages of
each method in some situations. We believe that this study
yields some insight into general properties of the methods,
though the results cannot be guaranteed to transfer.
In the following sections, we describe the problem, some of
the most common optimization methods available to solve
it, the experiments we ran and the results we obtained.
Please refer to the technical report [Ortiz and Kaelbling,

Accelerating EM: An Empirical Study

1999] for additional details.

2

This method does not respect any constraints on the param­
eters. To satisfy the constraints on aj, we project the part of
the gradient relevant to these parameters into the constraint
space and take a step size that does not take us out of this
space [Bertsekas, 1995, Binder et a!., 1997]. The projected
gradient is

DENSITY ESTIMATION WITH A
MIXTURE OF GAUSSIA NS

One of the simplest ML estimation problems with hidden
variables is to model the probability density of a data set
with a mixture of Gaussians. The model assumes that
there is some number, M, of underlying "centers" in the d­
dimensional space. Each data point is independently gen­
erated by first choosing a center with probability aj, and
then drawing from a Gaussian distribution, with mean /1-j
(the center) and covariance matrix �j.
AutoClass [Cheeseman et a!., 1988] casts the problem of
how many centers to use in a Bayesian perspective by stat­
ing we should use the most probable number of centers
given the data. In this paper, we address a subproblem
of AutoClass: given a desired number of centers, find the
model that maximizes the posterior probability.
In this problem, the parameter vector (} is made up of the
aj, 11-j and �j for each j. From the independence of the
data points, we can write the logarithm of the likelihood of
the parameters with respect to the data, p(DIA(O)) as the
sum of the logarithm of the likelihood with respect to the
individual points,
N

L(O) = lnp(DIA(O)) = �)np(x;IA(O)),
i=l

(I)

where

where ( v) j denotes the l h component of vector v.
Another strategy is to parameterize a in terms of another
parameter w such that w is unconstrained and any assign­
ment to w satisfies the constraints on a. One way of doing
this is as follows:
(5)

To satisfy the constraint on �j, we verify that the step size
does not take us out of the constraint space and decrease
the step size if it does until we get a step size that does not
take us out of the constraint space.
Taking a fixed or predetermined step size at each step can
slow down convergence. Instead, we can optimize the step
size at every step by trying to find the largest value of the
function in the direction of the gradient at every step by
means of a line search; that is, this is the same as in gradient
ascent but with
-y(k) +- argmaxL((J(k)

M

p(x;IA(O)) = I>jg(x;IJJ-i• �j),
j= l

"Y

(2)

and g(xiJJ-, �) is the multivariate Gaussian density with pa­
rameters 11- and�- The constraints on(} are: (I) for all j,
aj > 0; (2) I;�1 aj = 1; (3) for all j, �j is a symmetric,
positive-definite matrix. It is sufficient to maximize L( 0)
in order to maximize p(DIA(O)). Unfortunately, there is
no direct method for performing ML estimation of (}.
3

513

GRADIENT METHODS

The simplest gradient method we can use to find the
maximum of the log-likelihood function is gradient as­
cent [Shewchuk, 1994, Bertsekas, 1995, Polak, 1971]. In
this case, starting at some initial values for the parameters,
at each iteration k, we obtain new values as follows:

where 'i1 L((J(k) ) is the gradient of the log-likelihood func­
tion evaluated at the current values of the parameters and
-y(k) is the step size we take uphill along the gradient di­
rection. The value of -y(k) can be fixed or predetermined to
decrease at every iteration.

+ -y'ilL((J(k) ))).

This method seems appealing but it has a drawback in that
if the Hessian of the function at the local optimum is ill­
conditioned (i.e., the function is "elongated"), it exhibits a
zig-zagging behavior that can significantly slow down con­
vergence [Bertsekas, 1995].
The conjugate gradient method tries to eliminate the zig­
zagging behavior of optimized-step-size gradient ascent by
requiring that we optimize along conjugate directions at
every step. Formally, starting with some initial setting of
the parameters (J(O), r(o) +- 'i1L((J(0l), and the first di­
rection d(o) +- r(0), at each iteration k, we do as fol­
lows:
line search
-y(k) ;- argmax.., L((J(k) + "fd(k))

take best step in current direction

(J(k+l) +- (J(k) + "'(k)d (k)
r(k+l) ;- 'i1 L((J(k+ ) )
if((k + 1) mod d)== 0 then
,B(k+l) ;- 0

1

start over
weight to current direction

else

,6

new gradient

(k+l) +-

end if

(rl•+•lf (r(k+I) -rl•l)
(rl•l)tr(k)

d(k+l) +- r(k+l) + ,B(k+l)d(k)

new conjugate direction

514

Ortiz and Kaelbling

If we disregard the line search iterations, the conjugate gra­
dient method has the appealing property that the number
of iterations to convergence when close to a solution 1 is
roughly equal to the number of parameters.
There are other more sophisticated methods, such as New­
ton and Quasi-Newton or variable metric, that try to solve
the deficiencies of fixed- and optimized-step-size gradient
ascent by reshaping the function. These methods use ad­
ditional information about the function, like higher-order
derivatives. However, using higher-order derivatives re­
quires us to use additional storage and perform additional
computations, which typically outweigh the reduction in
the number of iterations.
4

EMBASICS

In the case of the mixture of Gaussians, at each iteration,
we find, for each data point and each center, the condi­
tional probability that the center generated the data point
and then use that probability distribution to assign new val­
ues to the probability, mean and covariance matrix of each
center. The expectation step is providing the maximization
step with the information that will allow it to compute the
expected sufficient statistics for each center under the con­
ditional probability distribution over the center given the
data and the current value of the parameters. EM uses the
expected sufficient statistics in the maximization step as if
they were the true sufficient statistics to obtain new values
for the parameters.
More specifically, starting from some initial setting of the
parameters, at each iteration k, the EM algorithm for mix­
ture of Gaussians is

IJ(k) and D
ajk)g(x;IJ1)k),r;n

E-step: compute distribution induced by

fori+-- 1 to N,j +-- 1 to M do
+-- P(C = ilx;,A(IJ(k) )) ex
end for

hl�)

M-step: update parameters
for j +-- 1 to M do

(k+1) f-- L.. z-1 h(k)
•j
N
Jlj(k+l) f-- I:/-1 hhTk)x;
.

�
�

I: j-1

h�j>x,x;"
h(k)

"N
L..... ;=l

IJ

-

end for

Jlj(k+l) (Jlj(k+l))T

For the mixture-of-Gaussians model, the method typically
converges to a local maximum of the likelihood function,
but it can stop in other stationary points, or it can go to a
singular point where the likelihood function grows with­
out bound [Redner and Walker, 1984]. A singular point
in the mixture model occurs when we use data points as
one of the means and let the variance of that center go to
zero [Duda and Hart, 1973]. In practice, we can avoid sin­
gularities through good initializations. In cases where this
is not enough, we can either ( 1) assign a small value to vari­
ances when they go below some small threshold value, (2)
delete components with too-small variances, (3) use priors
on
and perform MAP estimation, or (4) restart EM with
a different initial value for the parameters.

Ej

EM is a method for optimizing log-likelihood functions
in the case of missing data or hidden variables [Dempster
et al., 1977, McLachlan and Krishnan, 1997]. Starting with
some initial value for the parameters, at each iteration, it
uses the value of the parameters to compute the distribu­
tion or density over the hidden variables conditioned on the
data (the expectation step) and then uses that distribution to
get new values for the parameters (the maximization step).
The likelihood function monotonically increases at each it­
eration, and under some regularity conditions on the like­
lihood function, the improvement is strict except at a sta­
tionary point of the likelihood function [Wu, 1983].

Q

..,(
"-'jk+l)

We can also view EM as a special form of the general gra­
dient method. This allows us to see how EM reshapes the
function it is optimizing to make it better conditioned [Xu
and Jordan, 1996]. It also allows us to analyze its conver­
gence theoretically. However, in the mixture-of-Gaussians
model, the method gives updates that automatically satisfy
the constraints on the parameters (in the case of
this is
true with probability 1 for sufficiently large N [Redner and
Walker, 1984, Xu and Jordan, 1996]). In practice, numer­
ical errors can still take us out of the parameter space. In
those cases, we can either: (l) take a smaller step in the
direction of the EM update so as to guarantee constraint
satisfaction, (2) eliminate components for which ai = 0
and put small thresholds on
or (3) use priors on IJ and
perform MAP estimation.

Ej,

Ej,

5

ACCELERATION METHODS

Although EM has a very appealing monotonicity prop­
erty, its convergence rate is significantly slow in some in­
stances. For mixture of Gaussians, EM slows down when
the centers (i.e., the Gaussian components) are very close
together [Redner and Walker, 1984, Xu and Jordan, 1996].
One alternative is to start the optimization with EM and
move to gradient methods when we are close to a solution.
Another is to accelerate EM directly by using information
about the EM iteration [McLachlan and Krishnan, 1997].
One of the direct accelerations is parameterized EM [Pe­
ters and Walker, 1978, Redner and Walker, 1984, Meilij­
son, 1989, Bauer et al., 1997]. Starting from some initial
setting of the parameters, at each iteration, we get new val­
ues for the parameters as follows:

..,... N

J

"N
.£... ;=1

IJ

1 More formally, close to a solution means the neighborhood
around the local optimum that can be well approximated by a
quadratic function.

1

where e};A'f ) is the EM update with respect to the current
parameters IJ(k). In this method, we use the change in val­
ues in the parameters at an EM iteration and take a step
uphill in this direction from the current position or value of
the parameters. The step size can be fixed as in gradient

Accelerating EM: An Empirical Study

ascent, or optimized at every step as in optimized-step-size
gradient ascent. It is equivalent to EM when l(k) = 1. In
the case of mixture of Gaussians, we can achieve conver­
gence using this method when we are close to a solution
and 0 < l(k) < 2 [Redner and Walker, 1984], and improve
convergence speed when l(k) > 0.5 [Xu, 1997].
Parameterized EM is actually gradient or steepest ascent to
find the zero of a function that is the change in parameters
provided by EM (i.e., finding a fixpoint of the EM update).
Another acceleration method is conjugate gradient accel­
eration of EM [Jamshidian and Jennrich, 1993, Thiesson,
1995]. The idea is to use the change in value of the pa­
rameters of the EM iteration to find better conjugate di­
rections when performing conjugate gradient. The method
uses the information provided by the EM iteration to re­
shape the function and improve convergence speed when
close to a solution. Formally, starting with some initial set­
ting of the parameters 1J(0), r(0) +- V' L(IJ(0)), and the first
direction d(o) = 0�1- IJ(o), at each iteration k, we do as
follows:
line search
l(k) f- argmax.., L(IJ(k) + jd(k))

take best step in current direction

(J(k+!) +-IJ(k) + l(k)d(k)
r(k+I) +- V' L(IJ(k+I))
uik+!) +-IJ(k+I) -IJ(k)
EM
if ( (k + 1) mod d)== 0 then
f3(k+I) +- 0

else

new gradient
EM direction

start over
weight to current direction using EM information

{3(k+!)
end if
d(k+I)

f-

f-

(uiHI) f)T(r(k+l)_r(k) )
( di•l) (rlk+I)-r(k))

u(k+J) + f3(k+!)d(k)

new conjugate direction

This method is actually a special form of a generalized con­
jugate gradient method. The interesting aspect of the con­
jugate gradient acceleration of EM is that, contrary to the
traditional generalized conjugate gradient method, it does
not require the specification of a preconditioning matrix or
the evaluation of second order derivatives or matrix-vector
multiplications, since the change in parameters provided by
the EM update rule approximates the generalized gradient.
We conjecture that the relationship between parameterized
EM and conjugate gradient using EM information is similar
to the relationship between fixed- and optimized-step-size
gradient ascent and regular conjugate gradient: finding a
good step size is problem dependent but optimizing the step
size might not be a good idea (i.e., produces zig-zagging
behavior) and moving in conjugate directions is better.
As pointed out by an anonymous reviewer, there are other
extensions of the EM algorithm that can speed up conver­
gence. Due to lack of space, we refer the reader to McLach­
lan and Krishnan [ 1997] for additional information about
extensions and variations of EM. Two extensions of
the EM algorithm are the ECM (Expectation-Conditional
Maximization) [Meng and Rubin, 1993] and the ECME
(Expectation-Conditional Maximization Either) [Liu and

515

Rubin, 1994] algorithms. Both are useful when the regu­
lar maximization step is complex (i.e., no closed-form op­
timization) yet simpler if conditioned on a function of the
current value of the parameters. Furthermore, ECME dif­
fers from ECM only in that it conditionally maximize the
log-like/ihood function directly in some of the steps. ECM
typically has a slower convergence rate than EM, although
it can be faster in actual total computing time. The con­
vergence rate of ECME is typically faster than that of EM
and ECM (the actual computing time to convergence is also
typically faster). Given that the maximization step is simple
in the context of the mixture-of-Gaussians model, these ex­
tensions do not really apply here except in the case that we
reparameterized a as in equation (5). In this case, a version
of an ECM algorithm can speed up convergence with re­
spect to the typical alternative, which is an algorithm based
on the generalized version of EM. However, it will still be
typically slower than regular EM. We note that a version of
the ECM algorithm can help learning in the context of the
mixture-of-experts architecture [Jordan and Xu, 1993].
6

EXPERIMENTS

The theoretical convergence speed of the different methods
presented is problem dependent. No one is theoretically
dominant. There is empirical evidence that EM is superior
to gradient ascent in the mixture-of-Gaussians case [Xu and
Jordan, 1996]. The conjugate gradient and the accelera­
tion methods work well when they are close to a solution.
We argue that running conjugate gradient by itself is not a
good idea since it requires a very precise line search when
it is far from a solution. In practice, precise line searches
can increase the time to convergence. Hence, the meth­
ods we compare to EM in this paper are all based on an
idea that uses the monotonic convergence properties of EM.
The algorithmic description is as follows [Jamshidian and
Jennrich, 1993, Thiesson, 1995]: starting from some initial
setting of the parameters,
repeat
Run EM to get us close to a solution
Run acceleration
until stopping condition
When needed, we use inexact line searches during the ac­
celerations to save time. This seems to work well when
we are close to a solution. However, a decrease in log­
likelihood can occur due to the inexact line search. If a
decrease in log-likelihood occurs during the acceleration,
we return to EM and repeat the process. We interpret the
condition close to a solution to be true when the change in
log-likelihood is less than 0.5. This means that we continue
to run EM "as long as the x2 statistic for testing the equal­
ity of two successive iterates is more than I" [Jamshidian
and Jennrich, 1993].
W hen needed, the line search we use is an adapted version
of the secant-like line search used by Jamshidian and Jen­
nrich [1993]. Note that even in methods that do not use a
line search, like parameterized EM, we still need to make
sure that the step size does not take us outside the constraint

516

Ortiz and Kaelbling

space. In those cases, we reduce the step size until we find
one that keeps us inside the parameter space.
The basis of our empirical analysis is the idea that the work
needed to compute both the gradient and EM update is ap­
proximately the same. Actually, as long as N is sufficiently
large such that the extra computation is not significant, we
can compute both in about the same time, since they re­
quire about the same information. We can see this from
the expression of the gradient [Xu and Jordan, 1996]. Let
the expected counts for center j be
= I;�
1 and
let o:fM, JJfM and I:fM be the result of applying the EM
update rule to o:J, fJj and L;j, then

Nj

8L(O)
8o:

N

- -3
-- -

j

V:;;;L(O)

=

O:j

-

h;j.

al?M

N -3- '

(7)

O:j

(I:j1 (I:fM- I:J+
(JJfM- fJJ)(JJfM- JJJf) I:j1)

1
2_Nivec

·

(9)

First of all, we note that both require the computation
of the expected sufficient statistics I;�1
I;�1
and I;�1
to obtain the EM update. This takes
O(NMd3) ( O(NMd) when dealing with independent fea­
tures) 2. Using the EM update, computing the EM direction
takes 0(Md2 ) extra work and computing the gradient takes
O(Md3) extra work (in case of independent features, both
bounds are 0( Md)). We also note that the constants in the
bounds for extra work are small. Therefore, for sufficiently
large N, the time to compute the expected sufficient statis­
tics dominates all others. Finally, we say that the computa­
tion of the gradient, the EM iteration, and both at the same
time are all EM-equivalent iterations. This way we do not
need to compare CPU times, which significantly depend
on the implementation details of the different methods. All
we need to do is optimize the methods with respect to EM­
equivalent iterations.

h;jx;xT

h;i,

h;jx;,

There are many different initialization methods. Some of
them have been studied for large-dimensional data in the
context of the naive Bayesian Network model [Meila and
Heckerman, 1998]. For simplicity, we use the following
initialization:
Initialize o: as a uniform random sample from the space
of all distribution over M events.
Initialize fJ for each center by sampling uniformly at ran­
dom from the space defined by the hypercube of mini­
mum volume containing all the data points.
Initialize L; for each center as a diagonal matrix with
variances equal to the square of the distance to the center
closest to it [Bishop, 1996].
2

This is assuming that the complexity of exponentiation is

0(1).

One remaining issue is when to stop. Ideally, an iterative
method should stop when it has reached values for the pa­
rameters such that they provide a "good" model. However,
there is no clear way for an iterative method to determine
this. Therefore, detecting when to stop is a crucial but hard
problem in general. For instance, it is common to encounter
situations where the function we want to optimize has many
areas of large and small changes towards the local opti­
mum. In tum, this causes some of the methods to produce
burst of large and small improvements, which must be han­
dled by stopping rules. Many different stopping rules have
been used in the optimization literature [Bertsekas, 1995].
In this paper, we do not deal with the stopping problem
and use a very simple, typical stopping rule based on the
progress of the method in log-likelihood (or log-posterior)
space 3. We stop when the change in log-likelihood from
one iteration to the next is less than 10-5. We can obtain
the information we need to test this condition easily from
the EM-equivalent iteration. In our experiments on syn­
thetic data, all the methods that we tested converged to the
same point in log-likelihood space (and parameter sy,ace,
sometimes modulo symmetrically equivalent models ).
All the methods we tested, besides regular EM (EM), have
the algorithmic structure presented above and only the ac­
celeration step differed. We tried the following accelera­
tions:
•

regular conjugate gradient (CG),

•

conjugate gradient with EM information (CG+EM),

•

parameterized EM with inexact line search to optimize
the step size (PEM(opt)),

•

parameterized EM with fixed step sizes
(PEM(l.S)) and 1.9 (PEM(1.9)).

•

conjugate gradient with EM information on reparam­
eterized (w) space 5 (CG+EM(rp)).

1.5

In order to examine the properties of the different methods,
we tested them on data we generated from simple models
with varying degree of separation between the Gaussians.
We generated data from 3 models with 2 Gaussians in 2dimensions. All 3 models had the same parameters o: and L;
(o:1 = o:2 = 0.5, I:1 = I:2 = J). The first center f.Jt for one
of the Gaussians in the 3 models was also the same (J.J1 =
(0, 0)). The models differed in the center of the second
Gaussian: (1) J.J2 = (3, 3), (2) J.J2 = (2, 2), and (3) f.J2 =
( 1, 1). We generated one data set of 2000 points from each
of the models (See Figure 1). We then generated 40 initial
sets of parameters using the method presented above and
3

In theory, convergence speed in log-likelihood space is faster

than in parameter space.
4
In almost all cases, the methods converged to the same pa­
rameter values (i.e., equivalent models) and therefore there was
no need to compare them in terms of KL-divergence of the result­
ing model from the true model.
5
The EM algorithm in this case is actually a Generalized EM
algorithm since there is no exact (i.e., closed-form) optimization
in the maximization step.

Accelerating EM: An Empirical Study

Data set 1

517

Data set 3

Data set 2
Figure 1: Data sets 1, 2, and 3.
Data Set

Method
EM
CG
CG+EM
CG+EM(rp)
PEM(opt)
PEM(l.5)
PEM(l.9)

1 : (0,0)- (3,3)

num. iters.
120
163
100
116
120
83
81

3 : (0,0)-(1,1)

2 : (0,0)-(2,2)

speed-up

0.78 ±O.D7
1.18 ±0.19
1.04±0.18
1.01±0.10
1.40±0.04
1.32±0.09

198
225
122
131
202
137
110

2356
585
187
214
1967
1642
1329

1.04±0.07
1.78±0.19
1.70±0.19
1.02±0.05
1.44±0.02
1.79±0.03

3.98±0.38
12.80±1.50
11.92±1.46
1.58±0.18
1.41±0.02
1.74±0.04

Table 1: This table presents the average number of EM-equivalent iterations that each method took to converge on the
different data sets for the first set of experiments. Also in the table are the (approximate) 95% confidence intervals on the
average speed-up of the methods with respect to the number of iterations taken by EM (i.e., speed-up of a run= number of
iterations of EM I number of iterations of acceleration for that run).
ran each algorithm on each data set starting from each one
of those initial parameters. Table 1 presents the results. For
each data set, the results in the first column are the average
number of EM-equivalent iterations for each method. The
results in the second column are the average speed-up. We
define the speed-up achieved by a proposed acceleration
method in a run as the number of iterations of EM divided
by the number of EM-equivalent iterations of the method
for that run. Average speed-up may be a better measure
because it is not so drastically influenced by cases that are
hard for everyone.
We performed a bootstrap version (shift method) of the
one-sided paired-sample test [Cohen, 1995] to compare the
method with the best average number of iterations and/or
speed-up with each of the other methods. For each data set,
results are in bold for the method with the best empirical
mean. The test did not reject the null hypothesis that the
difference in mean was significant (p ::; 0.05, K = 10000)
with respect to the method with the best empirical mean
only for the other methods with the results in bold.
The results from this experiment show that accelerating
with CG+EM and CG+EM(rp) significantly improve con­
vergence speed as the Gaussians get closer together. When
the Gaussians are farther apart, all accelerations except
PEM(l.S) and PEM(1.9) can slow down convergence due
to false starts of the acceleration (false signalings of close­
ness to a solution). However, the improvement in conver-

4000

Scatter Plot CG+EM vs EM (Data

Set3, 40 runs)

3500
3000
2500
2000

'

1500
1000

'

'

'

'

'

500

Figure 2: Scatter plot of the number of EM-equivalent iter­
ations of CG+EM vs. the number iterations of EM on data
set 3.

518

Ortiz and Kaelbling

Data set

4

Data set 5

Figure 4: Data set

6:

Method

(M

Data Set

-

5, d- 5)

2:

(M - 5, d- 2 (hard))

4 : (M - 5, d- 2 (easy))

5:

136

1672

speed-up

num. iters.

Table

4 and 5.

EM

122

CG+EM

140

1.20± 0.32

140

1.05± 0.25

195

7.98± 1.36

PEM(opt)

140

1.01±0.10

1.14± 0.50

PEM(l .5)

91

1.13± 0.04

121
111

PEM(l.9)

89

1.13± 0.06

100

1.20± 0.08

1028
1160
940

2.02 ± 0.28
1.42±0.03
1.73± 0.05

1.12± 0.06

This table presents the average number of EM-equivalent iterations that each method took to converge and the

(approximate) 95% confidence intervals on the average speed-up on the different data sets for the second set of experiments.

gence speed provided by PEM(l.S) and PEM(1.9) is not
as impressive as that ofCG+EM and CG+EM(rp) in hard
instances. Also, the slow-downs produced by the attempted
accelerations tend to occur mainly in easier instances (mod­
Seanar Plot PEM(1.9) vs. EM (Data Set 3, 40 nme)

4000

els with means at

(0,0)-(3,3)

and

(0,0)-(2,2))

hard instances (model with means at
3500
'
'
'

3000

'
'

1500

.

.

'
'

1000

'
'

.

.

.•

2

the confidence intervals for the speed-up for the fixed-step­

consistent.

'
'
'

Figure

step-size parameterized EM methods seems in general very

'
'
'

'

(0,0)-(1,1)).

presents a scatter plot of the behavior of CG+EM in data
set 3. Finally, note from Figure 3 and the small values of

size parameterized EM method that the behavior of fixed­

'

'

of the prob­

lem and are not as severe as the potential improvements in

Although the results are not reported in this paper, we also
tried running EM on the reparameterized space alone and
running conjugate gradient alone in all the experiments but
they had much less success on average.

'
'

500

We also ran experiments with models of more Gaussians

'
,;

and/or higher dimensions with similar results. Models

•• •

��-=soo��1�000��1500��rooo�-2�S00�-30��
t itera EM

4,

5 and 6 are random models with different characteristics.

Models 4 and 5 both have d =

2 and M

=

5.

They dif­

fer in that in the data generated from model 5 it is harder
to distinguish the different clusters than in that generated

Figure

3:

Scatter plot of the number of EM-equivalent it­

erations of PEM(1.9) vs. the number of EM iterations on
data set

3.

This linear behavior is typical of fixed-step-size

parameterized EM on all the data set�.

from model

M

=

model

5.

4 (See Figure 4). Model6is larger with d = 5,

We generated a data set of

5000

points from

4, 10000 points from model 5, and 20000 points

2 presents the
40, 40 and 38 random

from model 6. Table

results. The results

are averages of

initial settings of the

parameters for models

4, 5 and 6 respectively. Again, we

have CG+EM being superior in the hard case, and while

it might slow down in the easy cases, the slow down does
not seem that severe. We note again that PEM(l.S) and

Accelerating EM: An Empirical Study

Method
35

EM

CG+EM
PEM(l.9)

519

IRAS data results
num. iters. speed-up
480
0.99
474
272

1.66

25

Table 3: This table presents the average number of EM­
equivalent iterations and speed-up that each method took to
converge on the IRAS data set starting with initially M =
77.

"
10

0
0

1

.•
500

1000

1500

2000
2500
nlll'lberol iterations

3000

3500

4000

4500

Figure 5: Histogram of the number of iteration of EM on
data set 6.
PEM(1.9) seem almost consistently better than EM. Fi­
nally, we note that there are 2 runs missing from the re­
sults in the table for model 6. For one of those runs,EM
took 4041 iterations on that run compared to 2974 for
PEM(l.S), 2483 for PEM(1.9) and 21456 for PEM(opt).
We stopped CG+EM when it had more iterations than all
the others. CG+EM and PEM(opt) were failing during
the line search because they were running out of time 6.
The value of the log-likelihood for the point where all the
methods converged in that run was smaller than the most
common one. We suspect that this is a saddle point or
some flat region in log-likelihood space. For the other run,
textbfEM took 2976 iterations on that run, compared to
180 for CG+EM, 5836 for PEM(opt), 1999 for PEM(l.S),
and 1998 for PEM(1.9). Inspection of the behavior of
PEM(opt) showed that the method was wasting time be­
cause the line searches were failing almost immediately,
and therefore going back to EM immediately after each at­
tempt. For this run, however, the point where the methods
converged was the most common point in log-likelihood
space. Finally, we note that the behavior of both of these
two runs was uncommon as suggested by the histogram of
the number of iterations of EM for this data set in Figure 5;
the two largest values in the histogram are for the runs men­
tioned above.
As an anonymous reviewer pointed out, the stopping rule
we use does not have a scale. Therefore, it is insensitive to
the range of the log-likelihood function. Restating the issue
of stopping rules, we note that partial preliminary experi­
ments suggest that a scaled stopping rule 7 can indeed help
reduce the number of iterations required by EM and PEM
in hard instances. In such cases, the log-likelihood func6

The inexact line search that we used had

a

optimum num­

ber of trials before failing which was set to 10 [Jamshidian and
Jennrich, 1993).
7
For instance, as suggested by that anonymous reviewer, we
stop when the change in log-likelihood at one iteration relative to

the total change so far is smaller than some threshold (i.e.,

w-•).

tion is ill-conditioned and stopping anywhere in the rela­
tively flat region close to the solution produces very good
estimates with regard to maximizing log-likelihood. It can
also help in preventing over-fitting when the amount of data
is small; a very important issue when we are learning mod­
els from data. However, partial preliminary experiments
also suggest that, unless we use different threshold values
for different instances, a scaled stopping rule increases the
potential for stopping too early in easier instances, thus
producing bad estimates. Other stopping rules have been
used for EM, but we do not know of any study that has
been done to compare them. Finally, we conjecture that the
"right" stopping rule eliminates the need for most of the
type of accelerations proposed since it eliminates the basis
for them. This is because the proposed accelerations work
well when we are close to a solution and the problem is
ill-conditioned, which is exactly what the "right" stopping
rule would detect.
We also ran experiments on the Infrared Astronomical
Satellite (IRAS) data set [Cheeseman et al., 1988, Cheese­
man and Stutz, 1995]. This data set contains N = 5420
data points in d = 93 dimensions. AutoClass found
M = 77 classes using a mixture-of-Gaussians model with
independent features. Given the high dimensionality and
other problems that this data poses, we performed MAP es­
timation over a model with independent features and took
careful steps in the way we computed the expected suffi­
cient statistics and the value of the change in log-posterior.
Therefore, our implementation is similar to that of Auto­
Class.
We assumed a priori that the parameters were independent.
We used a Dirichlet prior on the a parameters with the
same prior counts which we set to ( M + 1) /M. We used
a uniform prior over the hypercube of minimum volume
containing all the data points for the mean parameters of
the Gaussian components. We used a (scaled) inverse-x2
prior for the variances with 1/M degrees of freedom and
scale 1. In some instances, the update rule took us close
to the boundary constraint. Our solution to this problem
was to eliminate components if their probability was less
than 1/N and/or at least one of their variances was less
than 10-100. Once we removed those invalid components,
we restarted the method using the value of the parameters
of the remaining components as the initial value of the pa­
rameters.

520

Ortiz and Kaelbling

We ran experiments assuming a mixture-of-Gaussians
model with initially 77 components. We generated random
initial parameters using an adapted version of the initializa­
tion procedure used by AutoClass C and ran each method
starting from each one of the initial parameters. Table 3
presents the results based on 5 runs. First, we note that
EM is very stable at the beginning and makes most of
its progress in the first 20 - 25 iterations. This behavior
has also been noted by others [Redner and Walker, 1984,
Xu and Jordan, 1996]. Fixed-step-size parameterized EM
tends to perform best in this data set with respect to speed
of convergence. Conjugate gradient acceleration of EM can
slow down convergence. To understand this result we note
that the shape of the log-posterior seems to have many val­
leys, as suggested by the typical behavior of EM on this
data as shown in Figure 6. Therefore, we make many false
signalings of closeness and start the acceleration before it
is really close to a solution. The methods that use line
searches wasted time, since most of the line searches fail
and therefore the attempt to accelerate fails. Again, this
type of behavior suggests that we need better ways of sig­
naling closeness or stopping. Preliminary analysis seems
to indicate that this behavior of EM does not simply result
from performing MAP estimation. Therefore, we wonder
whether the cause of this is related to the fact that the ratio
of the number of parameters to the number of samples is
not small enough, or this is just a consequence of the high
dimensionality, or both 8.
7

(a)

t

"

CONCLUSIONS

First of all, we note, as many others authors have before us,
that the performance of EM away from the solution is im­
pressive, particularly in log-likelihood (and log-posterior)
space. Given the "right" stopping rule for a problem, it
typically produces reasonable estimates relatively fast. In
addition, it typically exhibits global convergence 9 in prac­
tice. These properties, along with its monotonicity prop­
erty and its simplicity, make EM a very powerful method
and a first choice for finding ML (or MAP) estimates in the
context of the mixture-of-Gaussians and other probabilistic
models.
Nevertheless, using the assumption that the amount of
data is sufficiently large such that the extra computation
of the gradient and the EM direction is not significant
and a simple albeit conservative stopping rule, our exper­
imental results on synthetic data suggest that the method
based on conjugate gradient acceleration of EM can be a
good choice for finding ML estimates for the mixture-of­
Gaussians model. This is because it significantly improves
convergence in the hard cases and, while it can slow down
convergence in the easy cases, the slow-down is not as se­
vere, given the relatively low number of iterations required
to converge in those cases. In addition, although it is more
8
Numerical instability can certainly be a reason too.
9
Here, global convergence means the property of an optimiza­
tion method to be able to converge to a stationary point (not nec­
essarily a global optimum) in function space from any initial point
in the parameter space.

(b)

EM behavior altar !liQnallng clo99nBSS

(c)
Figure 6: Plots of (a) the log-posterior (without constants)
divided by N, (b) the change in log-posterior and (c) the
relative change in log-posterior after signaling closeness
for a typical run of EM on the IRAS data set. We plot
the log-posterior/N as opposed to the log-posterior itself
to reduce the scale of the y-axis. The relative change is the
current change divided by the previous change and can be
used as an approximation of the convergence rate.

Accelerating EM: An Empirical Study

complicated to implement than parameterized EM, it elim­
inates the setting of the step size parameter. Furthermore,
although it requires line searches, those searches can be
simple and inexact in a neighborhood close to a solution.
Finally, the behavior of conjugate gradient acceleration of
EM seems best when the function is smooth but very flat
and "elongated" in the neighborhood of the local optimum.
On the other hand, the results from the experiments on the
IRAS data suggest that it is a good idea to attempt a sim­
ple acceleration method, such as fixed-step-size parameter­
ized EM, before trying the more complicated conjugate­
gradient-based accelerations. This is because there are
cases in which the surface of the log-likelihood (or log­
posterior) is relatively flat but not very smooth in the neigh­
borhood of the local optimum. We can attempt those more
complicated methods once we note that the simple acceler­
ation method is still too slow.
We think that it is necessary to perform a similar compar­
ison analysis in the context of learning Bayesian networks
and HMMs to verify that the same characterization of su­
periority of the accelerations based on "easy" and "hard"
instances suggested in this paper carries over.


We apply decision theoretic techniques to construct nonplayer characters that are able to assist a human player in collaborative games. The method is based on solving Markov
decision processes, which can be difficult when the game
state is described by many variables. To scale to more complex games, the method allows decomposition of a game task
into subtasks, each of which can be modelled by a Markov
decision process. Intention recognition is used to infer the
subtask that the human is currently performing, allowing the
helper to assist the human in performing the correct task. Experiments show that the method can be effective, giving nearhuman level performance in helping a human in a collaborative game.

Introduction
Traditionally, the behaviour of Non-Player Characters
(NPCs) in games is hand-crafted by programmers using
techniques such as Hierarchical Finite State Machines (HFSMs) and Behavior Trees (Champandard 2007). These techniques sometimes suffer from poor behavior in scenarios
that have not been anticipated by the programmer during
game construction. In contrast, techniques such as Hierarchical Task Networks (HTNs) or Goal-Oriented Action
Planner (GOAP) (Orkin 2004) specify goals for the NPCs
and use planning techniques to search for appropriate actions, alleviating some of the difficulties of having to anticipate all possible scenarios.
In this paper, we study the problem of creating NPCs that
are able to help players play collaborative games. The main
difficulties in creating NPC helpers are to understand the intention of the human player and to work out how to assist
the player. Given the successes of planning approaches to
simplifying game creation, we examine the application of
planning techniques to the collaborative NPC creation problem. In particular, we extend a decision-theoretic framework
Copyright c 2011, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

for assistance used in (Fern and Tadepalli 2010) to make it
appropriate for game construction.
The framework in (Fern and Tadepalli 2010) assumes that
the computer agent needs to help the human complete an unknown task, where the task is modeled as a Markov decision
process (MDP) (Bellman 1957). The use of MDPs provide
several advantages such as the ability to model noisy human
actions and stochastic environments. Furthermore, it allows
the human player to be modelled as a noisy utility maximization agent where the player is more likely to select actions
that has high utility for successfully completing the task. Finally, the formulation allows the use of Bayesian inference
for intention recognition and expected utility maximization
in order to select the best assistive action.
Unfortunately, direct application of this approach to
games is limited by the size of the MDP model, which grows
exponentially with the number of characters in a game. To
deal with this problem, we extend the framework to allow
decomposition of a task into subtasks, where each subtask
has manageable complexity. Instead of inferring the task
that the human is trying to achieve, we use intention recognition to infer the current subtask and track the player’s intention as the intended subtask changes through time.
For games that can be decomposed into sufficiently small
subtasks, the resulting system can be run very efficiently in
real time. We perform experiments on a simple collaborative
game and demonstrate that the technique gives competitive
performance compared to an expert human playing as the
assistant.

Scalable Decision Theoretic Framework
We will use the following simple game as a running example, as well as for the experiments on the effectiveness of
the framework. In this game, called Collaborative Ghostbuster, the assistant (illustrated as a dog) has to help the
human kill several ghosts in a maze-like environment. A
ghost will run away from the human or assistant when they
are within its vision limit, otherwise it will move randomly.
Since ghosts can only be shot by the human player, the dog’s

role is strictly to round them up. The game is shown in Figure 1. Note that collaboration is often truly required in this
game - without surrounding a ghost with both players in order to cut off its escape paths, ghost capturing can be quite
difficult.

This algorithm is guaranteed to converge to the optimal
value function V ∗ (s), which gives the expected cumulative
reward of running the optimal policy from state s.
The optimal value function V ∗ can be used to construct
the optimal actionsPby taking action a∗ in state s such that
a∗ = argmaxa { s0 Ta (s, s0 )V ∗ (s0 )}. The optimal Qfunction is constructed from V ∗ as follows:
X
Q∗ (s, a) =
Ta (s, s0 )(Ra (s, s0 ) + γV ∗ (s0 )).
s0

The function Q∗ (s, a) denotes the maximum expected longterm reward of an action a when executed in state s instead
of just telling how valuable a state is, as does V ∗ .

Figure 1: A typical level of Collaborative Ghostbuster. The
protagonists, Shepherd and Dog in the bottom right corner,
need to kill all three ghosts to pass the level.

Markov Decision Processes
We first describe a Markov decision process and illustrate
it with a Collaborative Ghostbuster game that has a single
ghost.
A Markov decision process is described by a tuple
(S, A, T, R) in which
• S is a finite set of game states. In single ghost Collaborative Ghostbuster, the state consists of the positions of the
human player, the assistant and the ghost.
• A is a finite set of actions available to the players; each
action a ∈ A could be a compound action of both players.
If each of the human player and the assistant has 4 moves
(north, south, east and west), A would consist of the 16
possible combination of both players’ moves.
• Ta (s, s0 ) = P (st+1 = s0 |st = s, at = a) is the probability that action a in state s at time t will lead to state s0
at time t + 1. The human and assistant move deterministically in Collaborative Ghostbuster but the ghost may
move to a random position if there are no agents near it.
• Ra (s, s0 ) is the immediate reward received after the state
transition from s to s0 triggered by action a. In Collaborative Ghostbuster, a non-zero reward is given only if the
ghost is killed in that move.
The aim of solving an MDP is to obtain a policy
maximizes the expected cumulative reward
P∞π that
t
t=0 γ Rπ(st ) (st , st+1 ) where 0 < γ < 1 is the discount
factor.
Value Iteration. An MDP can be effectively solved using a simple algorithm proposed by Bellman in 1957 (Bellman 1957). The algorithm maintains a value function V (s),
where s is a state, and iteratively updates the value function
using the equation
!
X
0
0
0
Vt+1 (s) = max
Ta (s, s )(Ra (s, s ) + γVt (s )) .
a

s0

Intractability. One key issue that hinders MDPs from being widely used in real-life planning tasks is the large state
space size (usually exponential in the number of state variables) that is often required to model realistic problems.
Typically in game domains, a state needs to capture all essential aspects of the current configuration and may contain
a large number of state variables. For instance, in a Collaborative Ghostbuster game with a maze of size m (number
of valid positions) consisting of a player, an assistant and n
ghosts, the set of states is of size O(mn+2 ), which grows
exponentially with the number of ghosts.

Subtasks
To handle the exponentially large state space, we decompose
a task into smaller subtasks and use intention recognition to
track the current subtask that the player is trying to complete.

Figure 2: Task decomposition in Collaborative Ghostbuster.
In Collaborative Ghostbuster, each subtask is the task of
catching a single ghost, as shown in Figure 2. The MDP for
a subtask consists of only two players and a ghost and hence
has manageable complexity.

Human Model of Action Selection In order to assist effectively, the AI agent must know how the human is going to
act. Without this knowledge, it is almost impossible for the
AI to provide any help. We assume that the human is mostly
rational and use the Q-function to model the likely human
actions.
Specifically, we assume
maxaAI Q∗
i (si ,ahuman ,aAI )

P (ahuman |wi , si ) = α.e
(1)
where α is the normalizing constant, wi represents subtask i
and si is the state in subtask i. Note that we assume that the
human player knows the best response from the AI sidekick
and plays his part in choosing the action that matches the
most valued action pair. However, the human action selection can be noisy, as modelled by Equation (1).

Intention Recognition and Tracking
We use a probabilistic state machine to model the subtasks
for intention recognition and tracking. At each time instance, the player is likely to continue on the subtask that
he or she is currently pursuing. However, there is a small
probability that the player may decide to switch subtasks.
This is illustrated in Figure 3, where we model a human
player who tends to stick to his chosen sub-goal, choosing
to solve the current subtask 80% of the times and switching
to other sub-tasks 20% of the times. The transition probability distributions of the nodes need not be homogeneous, as
the human player could be more interested in solving some
specific subtask right after another subtask. For example, if
the ghosts need to be captured in a particular order, this constraint can be encoded in the state machine. The model also
allows the human to switch back and forth from one subtask
to another during the course of the game, modelling change
of mind.

where T (wj → wi ) is the switching probability from subtask j to subtask i.
Next, we compute the posterior belief distribution using
Bayesian update, after observing the human action a and
subtask state si,t at time t, as follows:
Bt (wi |at = a, st , θt−1 ) = α.Bt (wi |θt−1 ).P (at = a|wi , si,t )
(3)
where α is a normalizing constant. Absorbing current human action a and current state into θt−1 gives us the game
history θt at time t.
Complexity This component is run in real time, and thus
its complexity dictates how responsive our AI is. We are
going to show that it is at most O(k 2 ), with k being the
number of subtasks.
The first update step as depicted in Equation 2 is executed
for all subtasks, thus of complexity O(k 2 ).
The second update step as of Equation 3 requires the computation of P (at = a|wi , si ) (Equation 1), which takes
O(|A|) with A being the set of compound actions. Since
Equation 3 is applied for all subtasks, that sums up to
O(k|A|) for this second step.
In total, the complexity of our real-time Intention Recognition component is O(k 2 + k|A|), which will be dominated
by the first term O(k 2 ) if the action set is fixed.

Decision-theoretic Action Selection
Given a belief distribution on the players targeted subtasks
as well as knowledge to act collaboratively optimally on
each of the subtasks, the agent chooses the action that maximizes its expected reward.
(
)
X
i
∗
Bt (wi |θt )Qi (st , a)
a = argmaxa
i

CAPIR: Collaborative Action Planner with
Intention Recognition
We implement the scalable decision theoretic framework
as a toolkit for implementing collaborative games, called
Collaborative Action Planner with Intention Recognition
(CAPIR).
Figure 3: A probabilistic state machine, modeling the transitions between subtasks.
Belief Representation and Update The belief at time t,
denoted Bt (wi |θt ), where θt is the game history, is the conditional probability of that the human is performing subtask
i. The belief update operator takes Bt−1 (wi |θt−1 ) as input
and carries out two updating steps.
First, we obtain the next subtask belief distribution, taking into account the probabilistic state machine model for
subtask transition T (wk → wi )
X
Bt (wi |θt−1 ) =
T (wj → wi )Bt−1 (wj |θt−1 )
(2)
j

CAPIR’s Architecture
Each game level in CAPIR is represented by a GameWorld
object, which consists of two Players and multiple SubWorld
objects, each of which contains only the elements required
for a subtask (Figure 4). The game objective is typically to
interact with these NPCs in such a way that gives the players the most points in the shortest given time. The players
are given points in major events such as successfully killing
a monster-type NPC or saving a civilian-type NPC – these
typically form the subtasks.
Each character in the game, be it the NPC or the protagonist, is defined in a class of its own, capable of executing
multiple actions and possessing none or many properties.
Besides movable NPCs, immobile items, such as doors or

Figure 4: GameWorld’s components.
shovels, are specified by the class SpecialLocation. GameWorld maintains and updates an internal game state that captures the properties of all objects.
At the planning stage, for each SubWorld, an MDP is generated and a collaboratively optimal action policy is accordingly computed (Figure 5). These policies are used by the AI
assistant at runtime to determine the most appropriate action
to carry out, from a decision-theoretic viewpoint.







   
  

   

   





  
   

 

   

!  "#

$ $     

"#   

(
'


%

&

Figure 5: CAPIR’s action planning process. (a) Offline subtask Planning, (b) in-game action selection using Intention
Recognition.

busters. We chose five levels (see Appendix) with roughly
increasing state space size and game play complexity to assess how the technique can scale with respect to these dimensions.
The participants were requested to play five levels of the
game as Shepherd twice, each time with a helping Dog controlled by either AI or a member of our team, the so-called
human expert in playing the game. The identity of the dog’s
controller was randomized and hidden from the participants.
After each level, the participants were asked to compare
the assistant’s performance between two trials in terms of
usefulness, without knowing who controlled the assistant at
which turn.
In this set of experiments, the player’s aim is to kill three
ghosts in a maze, with the help of the assistant dog. The
ghosts stochastically1 run away from any protagonists if they
are 4 steps away. At any point of time, the protagonists could
move to an adjacent free grid square or shoot; however, the
ghosts only take damage from the ghost-buster if he is 3
steps away. This condition forces the players to collaborate
in order to win the game. In fact, when we try the game with
non-collaborative dog models such as random movement,
the result purely relies on chance and could go on until the
time limit (300 steps) runs out, as the human player hopelessly chases ghosts around obstacles while the dog is doing
some nonsense at a corner. Oftentimes the game ends when
ghosts walk themselves into dead-end corners.
The twenty participants are all graduate students at our
school, seven of whom rarely play games, ten once to twice
a week, and three more often.
When we match the answers back to respective controllers, the comparison results take on one of three possible
values, being AI assistant performing “better”, “worse” or
“indistinguishable” to the human counterpart. The AI assistant is given a score of 1 for a “better”, 0 for an “indistinguishable” and -1 for a “worse” evaluation.
Qualitative evaluation For simpler levels 1, 2 and 3, our
AI was rated to be better or equally good more than 50%
the times. For level 4, our AI rarely got the rating of being
indistinguishable, though still managed to get a fairly competitive performance. Subsequently, we realized that in this
particular level, the map layout is confusing for the dog to
infer the human’s intention; there is a trajectory along which
the human player’s movement could appear to aim at any
one of three ghosts. In that case, the dog’s initial subtask belief plays a crucial role in determining which ghost it thinks
the human is targeting. Since the dog’s belief is always initialized to a uniform distribution, that causes the confusion.
If the human player decides to move on a different path, the
AI dog is able to efficiently assist him, thus getting good ratings instead. In level 5, our AI gets good ratings only for
less than one third of the times, but if we count “indistinguishable” ratings as satisfactory, the overall percentage of
positive ratings exceeds 50%.

Experiment and Analysis
In order to evaluate the performance of our AI system, we
conducted a human experiment using Collaborative Ghost-

1
The ghosts run away 90% of the times and perform some random actions in the remaining 10%.

12	
  
10	
  
8	
  
-­‐1	
  

6	
  

0	
  
1	
  

4	
  
2	
  
0	
  
1	
  

2	
  

3	
  

4	
  

5	
  

Figure 6: Qualitative comparison between CAPIR’s AI assistant and human expert. The y-axis denotes the number of
ratings.
120	
  
100	
  
80	
  

AI	
  

60	
  

Human	
  

40	
  
20	
  
0	
  
1	
  

2	
  

3	
  

4	
  

5	
  

Figure 7: Average time, with standard error of the mean as
error bars, taken to finish each level when the partner is AI
or human. The y-axis denotes the number of game turns.
Quantitative evaluation Besides qualitative evaluation,
we also recorded the time taken for participants to finish
each level (Figure 7). Intuitively, a well-cooperative pair
of players should be able to complete Collaborative Ghostbuster’s levels in shorter time.
Similar to our qualitative result, in levels 1, 2 and 3, the
AI controlled dog is able to perform at near-human levels in
terms of game completion time. Level 4, which takes the
AI dog and human player more time on average and with
higher fluctuation, is known to cause confusion to the AI
assistant’s initial inference of the human’s intention and it
takes a number of game turns before the AI realizes the true
target, whereas our human expert is quicker in closing down
on the intended ghost. Level 5, larger and with more escape
points for the ghosts but less ambiguous, takes the protagonist pair (AI, human) only 4.3% more on average completion
time.

Related Work
Since plan recognition was identified as a problem on its
own right in 1978 (Schmidt, Sridharan, and Goodson 1978),
there have been various efforts to solve its variant in different domains. In the context of modern game AI research,
Bayesian-based plan recognition has been inspected using

different techniques such as Input Output Hidden Markov
Models (Gold 2010), Plan Networks (Orkin and Roy 2007),
text pattern-matching (Mateas and Stern 2007), n-gram and
Bayesian networks (Mott, Lee, and Lester 2006) and dynamic Bayesian networks (Albrecht, Zukerman, and Nicholson 1998). As far as we know, our work is the first to use
a combination of precomputed MDP action policies and online Bayesian belief update to solve the same problem in a
collaborative game setting.
Related to our work in the collaborative setting is the work
reported by Fern and Tadepalli (Fern and Tadepalli 2010)
who proposed a decision-theoretic framework of assistance.
There are however several fundamental differences between
their targeted problem and ours. Firstly, they assume the task
can be finished by the main subject without any help from
the AI assistant. This is not the case in our game, which
presents many scenarios in which the effort from one lone
player would amount to nothing and a good collaboration
is necessary to close down on the enemies. Secondly, they
assume a stationary human intention model, i.e. the human
only has one goal in mind from the start to the end of one
episode, and it is the assistant’s task to identify this sole intention. In contrary, our engine allows for a more dynamic
human intention model and does not impose a restriction on
the freedom of the human player to change his mind mid
way through the game. This helps ensure our AI’s robustness when inferring the human partner’s intention.
In a separate effort that also uses MDP as the game AI
backbone, Tan and Cheng (Tan and Cheng 2010) model the
game experience as an abstracted MDP - POMDP couple.
The MDP models the game world’s dynamics; its solution
establishes the optimal action policy that is used as the AI
agent’s base behaviors. The POMDP models the human play
style; its solution provides the best abstract action policy
given the human play style. The actions resulting from the
two components are then merged; reinforcement learning is
applied to choose an integrated action that has performed
best thus far. This approach attempts to adapt to different
human play styles to improve the AI agent’s performance. In
contrast, our work introduces the multi-subtask model with
intention recognition to directly tackle the intractability issue of the game world’s dynamics.

Conclusions
We describe a scalable decision theoretic approach for constructing collaborative games, using MDPs as subtasks and
intention recognition to infer the subtask that the player is
targeting at any time. Experiments show that the method is
effective, giving near human-level performance.
In the future, we also plan to evaluate the system in more
familiar commercial settings, using state-of-the-art game
platforms such as UDK or Unity. These full-fledged systems offer development of more realistic games but at the
same time introduce game environments that are much more
complex to plan. While experimenting with Collaborative
Ghostbuster, we have observed that even though Value Iteration is a simple naive approach, in most cases, it suffices,
converging in reasonable time. The more serious issue is the

state space size, as tabular representation of the states, reward and transition matrices takes much longer to construct.
We plan to tackle this limitation in future by using function
approximators in place of tabular representation.

Appendix
Game levels used for our experiments.

Acknowledgments
This work was supported in part by MDA GAMBIT grant R252-000-398-490 and AcRF grant T1-251RES0920 in Singapore. The authors would like to thank Qiao Li (NUS),
Shari Haynes and Shawn Conrad (MIT) for their valuable
feedbacks in improving the CAPIR engine, and the reviewers for their constructive criticism on the paper.




The ways in which an agent’s actions affect the
world can often be modeled compactly using a
set of relational probabilistic planning rules. This
paper addresses the problem of learning such rule
sets for multiple related tasks. We take a hierarchical Bayesian approach, in which the system learns a prior distribution over rule sets. We
present a class of prior distributions parameterized by a rule set prototype that is stochastically modified to produce a task-specific rule set.
We also describe a coordinate ascent algorithm
that iteratively optimizes the task-specific rule
sets and the prior distribution. Experiments using this algorithm show that transferring information from related tasks significantly reduces
the amount of training data required to predict
action effects in blocks-world domains.

1

Introduction

One of the most important types of knowledge for an intelligent agent is that which allows it to predict the effects of
its actions. For instance, imagine a robot that performs the
familiar task of retrieving items from cabinets in a kitchen.
This robot needs to know that if it grips the knob on a cabinet door and pulls, the door will swing open; if it releases
its grip when the cabinet is only slightly open, the door will
probably swing shut; and if it releases its grip when the cabinet is open nearly 90 degrees, the door will probably stay
open. Such knowledge can be encoded compactly as a set
of probabilistic planning rules [Kushmerick et al., 1995;
Blum and Langford, 1999]. Each rule specifies a probability distribution over sets of changes that may occur in
the world when an action is executed and certain preconditions hold. To represent domains concisely, the rules must
be relational rather than propositional: for example, they
must make statements about cabinets in general rather than
individual cabinets.

Luke S. Zettlemoyer
MIT CSAIL
Cambridge, MA 02139
lsz@csail.mit.edu

Leslie Pack Kaelbling
MIT CSAIL
Cambridge, MA 02139
lpk@csail.mit.edu

Algorithms have been developed for learning relational
probabilistic planning rules by observing the effects of actions [Pasula et al., 2004; Zettlemoyer et al., 2005]. But
with current algorithms, if a robot learns planning rules for
one kitchen and then moves to a new kitchen where its actions have slightly different effects (because, say, the cabinets are built differently), it must learn a new rule set from
scratch. Current rule learning algorithms fail to capture an
important aspect of human learning: the ability to transfer knowledge from one task to another. We address this
transfer learning problem in this paper.
In statistics, the problem of transferring predictions across
related data sets has been addressed with hierarchical
Bayesian models [Lindley, 1971]. The first use of such
models for the multi-task learning problem appears to
be due to Baxter [1997]; the approach has recently become quite popular [Yu et al., 2005; Marx et al., 2005;
Zhang et al., 2006]. The basic idea of hierarchical Bayesian
learning is to regard the task-specific models R1 , . . . , RK
as samples from a global prior distribution G. This prior
distribution over models is not fixed in advance, but is
learned by the system; thus, the system discovers what the
task-specific models have in common.
However, applying the hierarchical Bayesian approach to
sets of first-order probabilistic planning rules poses both
conceptual and computational challenges. In most existing
applications, the models Rk are represented as real-valued
parameter vectors, and the hypothesis space for G is a class
of priors over real vectors. But a rule set is a discrete structure that may contain any number of rules, and each rule
includes a precondition and a set of outcomes that are represented as arbitrary-length conjunctions of first-order literals. How can we define a class of prior distributions over
such rule sets? Our proposal is to let G be defined by a
rule set prototype that is modified stochastically to create
the task-specific rule sets.
Our goal is to take data from K source tasks, plus a limited set of examples from a target task K + 1, and find the
∗
rule set RK+1
for the target task with the greatest posterior
probability. In principle, this involves integrating out the

84

DESHPANDE ET AL.

pickup(X, Y ) :

→

8
>
>
>
<
>
>
>
:

on(X, Y ), clear(X), inhand-nil,
block(Y ), ¬wet
inhand(X), ¬clear(X), ¬inhand-nil,
.7 :
¬on(X, Y ), clear(Y )
.2 : on(X, TABLE), ¬on(X, Y )
.05 : no change
.05 : noise

pickup(X, Y ) :
8
>
>
.2 :
>
<
.2 :
→
>
>
>
: .3 :
.3 :

s in sentence 1 and a = pickup(B-A, B-B), both of the rules
in Fig. 1 would have the binding θ = {X/B-A, Y /B-B}. The
first rule would apply, since its preconditions are all satisfied, while the second one would not because wet is not
true in s. We disallow rule sets in which two or more rules
apply to the same (s, a) pair (these are called overlapping
rules). In cases where no rules apply, a default rule is used
that has an empty context and two outcomes: no change
and noise, which will be described shortly.

on(X, Y ), clear(X), inhand-nil,
block(Y ), wet
inhand(X), ¬clear(X), ¬inhand-nil,
¬on(X, Y ), clear(Y )
on(X, TABLE), ¬on(X, Y )
no change
noise

Figure 1: Two rules for the pickup action in the “slippery
gripper” blocks world domain.
other rule sets R1 , . . . , RK and the rule set prototype G.
As an approximation, however, we use estimates of G∗ and
∗
found by a greedy local search algorithm. We
R1∗ , . . . , RK
present experiments with this algorithm on blocks world
tasks, showing that transferring data from related tasks significantly reduces the number of training examples required
to achieve high accuracy on a new task.

2

Probabilistic Planning Rules

Probabilistic planning rule sets define a state transition distribution p(st |st−1 , at ). In this section, we present a simplified version of the representation developed by [Zettlemoyer et al., 2005]. A state st is represented by a conjunctive formula with constants denoting objects in the world
and proposition and function symbols representing the objects’ properties and relations. The sentence
inhand-nil ∧ on(B-A, B-B) ∧ on(B-B, TABLE) ∧ clear(B-A)
∧block(B-A) ∧ block(B-B) ∧ table(TABLE)

(1)

represents a blocks world where the gripper holds nothing
and the two blocks are in a single stack on the table. This
is a full description of the world; all of the false literals
are omitted for compactness. Block B-A is on top of the
stack, while B-B is below B-A and on the table TABLE. Actions
at are ground literals where the predicate names the action
to be performed and the arguments are constant terms that
correspond to the objects which will be manipulated. For
example, at = pickup(B-A, B-B) would represent an attempt
to pick block B-A up off of block B-B.
Each rule r has two parts that determine when it is applicable: an action z and a context Ψ that encodes a set
of preconditions. Both of the rules in Fig. 1 model the
pickup(X, Y ) action. Given a particular state st−1 and action a, we can determine whether a rule applies by computing a binding θ that finds objects for all the variables,
by matching against a, and then testing whether the preconditions hold for this binding. For example, for the state

Given the applicable rule r, the discrete distribution p over
outcomes O, described on the right of the →, defines what
changes may happen from st−1 to st . Each non-noise outcome o ∈ O implicitly defines a successor state function fo
with associated probability po , an entry in p. The function
fo builds st from st−1 by copying st−1 and then changing
the values of the relevant literals in st to match the corresponding values in θ(o). In our running example of executing pickup(B-A, B-B) in sentence 1, for the first outcome of
the first rule, where the picking up succeeds, fo would set
five truth values, including setting on(B-A, B-B) to be false.
In the third outcome, which indicates no change, fo is the
identity function. In this paper, we will enforce the restriction that outcomes do not overlap: for each pair of outcomes o1 and o2 in a rule r, there cannot exist a state–action
pair (s, a) such that r is applicable and fo1 (s) = fo2 (s). In
other words, if we observe the state that results from applying a rule, then there is no ambiguity about which outcome
occurred.1 Finally, the noise outcome is treated as a special
case. There is no associated successor function, which allows the rule to define a type of partial model where r does
not describe how to construct the next state with probability pnoise . Noise outcomes allow rule learners to ignore
overly complex, rare action effects and have been shown
to improve learning in noisy domains [Zettlemoyer et al.,
2005]. Since rules with noise outcomes are partial models,
the distribution p(st |st−1 , at ) is replaced with an approximation:

p̂(st |st−1 , at ) =

po
pnoise pmin

if fo (st−1 ) = st
otherwise

(2)

where the set of possible outcomes o ∈ O is determined by the applicable rule. The probabilities po and
pnoise make up the parameter vector p. The constant
pmin can be viewed as an approximation to a distribution
p(st |st−1 , at , onoise ) that would provide a complete model.

3

Hierarchical Bayesian Model

In a hierarchical Bayesian model, as illustrated in Fig. 2,
the data points xkn in task k come from a task-specific dis1

This restriction simplifies parameter estimation (as we will
see in Sec. 4) without limiting the class of transition distributions
that can be defined. Any rule with overlapping outcomes can be
replaced by an equivalent set of rules applying to more specific
contexts, with non-overlapping outcomes.

DESHPANDE ET AL.
G

R1

...

R2

x1n

x2n
N1

RK

xKn
N2

NK

Figure 2: A hierarchical Bayesian model with K tasks,
where the number of examples for task k is Nk .
tribution p(xkn |Rk ), and the task-specific parameters Rk
are in turn modeled by a prior distribution p(Rk |G). The
hyperparameter G has its own prior distribution p(G). By
observing data from the first K tasks, the learner gets information about R1 , . . . , RK and hence about G. For instance, the learner can compute (perhaps approximately)
∗
the values (R1∗ , . . . , RK
, G∗ ) that have maximum a posteriori (MAP) probability given the data on the first K tasks.
Then when it encounters task K +1, the learner’s estimates
of the task-specific model RK+1 are influenced by both the
data observed for task K + 1 and the prior p(RK+1 |G∗ ),
which captures its expectations about the model based on
the preceding tasks.
3.1

Rule Set Prototypes

In the context of learning planning rules, the task-specific
models Rk are rule sets. Our intuition is that if the tasks
are related, then these rule sets have some things in common. Certain rules may appear in the rule sets for many
tasks, perhaps with some modifications to their contexts,
outcomes, and outcome probabilities. To capture these
commonalities, we assume that the rule sets are all generated from an underlying rule set prototype G.
A rule set prototype consists of a set of rule prototypes.
A rule prototype is like an ordinary rule, except that rather
than specifying a probability distribution over its outcomes,
it specifies a vector of Dirichlet parameters that define a
prior over outcome distributions. For a rule prototype with
n explicit outcomes, this is a vector Φ of n+2 non-negative
real numbers: Φn+1 corresponds to a special seed outcome
o∗n+1 that generates new outcomes in local rules, and Φn+2
accounts for the noise outcome. Unlike in local rule sets,
we allow overlapping rules and outcomes in rule set prototype to allow for better generalization.
3.2

85

can be found by identifying the single rule in Rk that applies to (st−1 , at ) (or the default rule, if no explicit rule
applies) and using Eq. 2. Then theQprobability of the entire
Nk
data set for task k is p(xk |Rk ) = n=1
p(xkn |Rk ).
The distribution for G and R1 , . . . , Rk is defined by a
generative process that first creates G, and then creates
R1 , . . . , Rk by modifying G. Note that this generative process is purely a conceptual device for defining our probability model: we never actually draw samples from it. As we
will see in Sec. 4, our learning algorithm uses the generative model solely to define a scoring function for evaluating
rule sets and prototypes.
Two difficulties arise in using our generative process to define a joint distribution. One is that the process can yield
rule sets Ri that are invalid, in the sense of containing overlapping rules or outcomes. It is difficult to design a generative process that avoids creating invalid rule sets, but still
allows the probability of a rule set to be computed efficiently. Intuitively, we want to discard runs of the generative process that yield invalid rule sets. The other difficulty is that there may be many possible runs of a generative process that yield the same rule set. For instance, as
we will see, a rule set prototype is generated by choosing
a number m, generating a sequence of m rule prototypes
independently, and then returning the set of distinct rule
prototypes that were generated. In principle, a set of m∗
distinct rules could be created by generating a list of any
length m ≥ m∗ (with duplicates); we do not want to force
ourselves to sum over all these possibilities to compute the
probability of a given rule set prototype. Again, it is convenient to discard certain non-canonical runs of the generative
process: in this case, runs where the same rule prototype is
generated twice.
Thus, we will define measures PG (G) and Pmod (Rk |G)
that give the probability of generating a rule set prototype
G, or a rule set Rk , through a “valid” sampling run. Because some runs are considered invalid, these measures do
not sum to one. The resulting joint distribution is:
p(G, R1 , . . . , RK , x1 , . . . , xK ) =
K

Y
1
PG (G)
Pmod (Rk |G)p(xk |Rk )
Z

(3)

k=1

The normalization constant Z is the total probability of
valid runs of our generative process. Since we are just interested in the relative probabilities of hypotheses, we never
need to compute this normalization constant.2

Overview of Model

Our hierarchical model defines a joint probability distribution p(G, R1 , . . . , RK , x1 , . . . , xK ). In our setting, each
example xkn is a state st obtained by performing a known
action at in a known initial state st−1 . Thus, p(xkn |Rk )

2

One might be tempted to define a model where the normalization is more local: for instance, to replace the factor Pmod (Rk |G)
in Eq. 3 with a normalized distribution Pmod (Rk |G)/Z(G).
However, the normalization factor Z(G) is not constant, so it
would have to be computed to compare alternative values of G.

86

DESHPANDE ET AL.

3.3

set R of size m from a prototype G of size m∗ is:

Modifying the Rule Set Prototype

Pmod (R|G) =

We begin the discussion of our generative process by describing how a rule set prototype G is modified to create a
rule set R (the process that generates G will be a simplified version of this process). The first step is to choose the
rule set size m from a distribution Pnum (m|m∗ ), where
m∗ is the number of rule prototypes in G. We define
Pnum (m|m∗ ) so that all natural numbers have non-zero
probability, but m is likely to be close to m∗ , and the probability drops off geometrically for greater values of m.

Pnum (m|m∗ ) =

Geom[α](m − m∗ )
(1 − α)Binom[m∗ , β](m)

if m > m∗
otherwise

(4)

Here Geom[α] is a geometric distribution with success
probability α. Thus, if m > m∗ , then Pnum (m|m∗ ) =
∗
(1 − α)α(m−m ) . We set α to a small value to discourage
the rule set R from being much larger than G. The sum of
the Geom[α] distribution over all values greater than zero
is α, leaving a probability mass of 1 − α to be apportioned
over rule set sizes from 0 through m∗ . The binomial distribution Binom[m∗ , β] — which yields the probability of
getting exactly m heads when flipping m∗ coins with heads
probability β — is a convenient distribution over this range
of integers. We set β to a value close to 1 to express a preference for local rule sets that are not much smaller than the
prototype set.
Next, for i = 1 to m, we generate a local rule ri . The
first step in generating ri is to choose which rule prototype
in G it will be derived from. This choice is represented
by an assignment variable Ai , whose value is either a rule
prototype in G, or a special value NIL indicating that this
rule is generated from scratch with no prototype. The distribution PA (ai |G) assigns the probability γrule to NIL and
spreads the remaining mass uniformly over the rule prototypes. Since the Ai are chosen independently, a single
rule in G may serve as the prototype for several rules in R,
or for none. Next, given the rule prototype (or null value)
ai , the local rule ri is generated according to a distribution
Prule (ri |ai ). We discuss this distribution in Section 3.4.
The rule set generated by this process is the set of distinct
rules in the list r1 , . . . , rm . We consider a run of the generative process to be invalid if any of these rules have overlapping contexts; in particular, this constraint rules out cases
where the same rule occurs twice. So the probability of
generating a set {r1 , . . . , rm } on a valid run is the sum of
the probabilities of all permutations of this set. This is m!
times the probability of generating the rules in any particular order. Thus, the probability of getting a valid local rule

Pnum (m|m∗ ) · m! ·

m
Y
i=1

3.4

X

PA (ai |G)Prule (ri |ai )

(5)

ai ∈
G∪{NIL}

Modifying and Creating Rules

We will now define the distribution Prule (r|r∗ ), where r∗
may be either a rule prototype, or the value NIL, indicating
that r is generated from scratch. Suppose r consists of a
context formula Ψ, an action term z, a set of non-noise
outcomes O, and a probability vector p. The corresponding
parts of r∗ will be referred to as Ψ∗ , z ∗ , O∗ , and Φ (recall
that this last component is a vector of Dirichlet parameters).
If r∗ = NIL, then Ψ∗ is an empty formula, z ∗ is NIL, O∗
consists of just the seed outcome, and Φ is a two-element
vector consisting of a 1 for the seed outcome and a 1 for
the noise outcome.
For rules derived from a rule prototype, we assume the action term is unchanged. So if z ∗ is not NIL, we use the distribution Pact (z|z ∗ ) that assigns probability one to z ∗ . If a
rule is generated from scratch, we need to generate its action term. For simplicity, we assume that each action term
consists of an action symbol and a distinct logical variable
for each argument; we do not allow repeated variables or
more complex terms in the argument list. The distribution
Pact (z|z ∗ ) chooses the action term uniformly from the set
of such terms when z ∗ = NIL.
The next step in generating r is to choose its context Ψ. We
define the distribution for Ψ by means of a general formulamodification distribution Pfor (Ψ|Ψ∗ , v̄), where v̄ is the set
of logical variables that occur in z and thus are eligible to
be included in Ψ. This distribution is explained in Sec. 3.5.
To generate the outcome set O from O∗ , we use essentially
the same method we used to generate the rule set R from G.
We begin by choosing the size n of the outcome set from
the distribution Pnum (n|n∗ ), where n∗ = |O∗ |. The distribution Pnum here is the same one used in Sec. 3.3 (one
could use different α and β parameters here). Then, for
i = 1 to n, we choose which prototype outcome serves as
the source for the ith local outcome. This choice is represented by an assignment variable Bi . As in the case of
rules, we allow some local outcomes to be generated from
scratch rather than from a prototype; this choice is represented by the seed outcome. The value of Bi is chosen
from PB (bi |O∗ ), which assigns probability γout to the seed
outcome and is uniform over the rest of the outcomes.
Once the source for each local outcome has been chosen,
the next step is to generate the outcomes themselves. Recall
that an outcome is just a formula. Thus, we define the outcome modification distribution using the general formula-

DESHPANDE ET AL.
modification process Pfor (oi |bi , v̄) that we will discuss in
Sec. 3.5 (again, v̄ is the set of logical variables in z). If bi is
the seed outcome, then Pfor treats it as an empty formula. A
list of outcomes is considered valid if it contains no repeats
and no overlapping outcomes. Since repeats are excluded,
the probability of a set of n outcomes is n! times the probability of any corresponding list. Thus, we get the following
probability of generating a valid outcome set O and an assignment vector b, given that the prototype outcome set is
O∗ and the number of local outcomes is n:
Pout (O, b|O∗ , n) = n!

n
Y

PB (bi |O∗ )Pfor (oi |bi , v̄)

(6)

i=1

The last step is to generate the outcome probabilities p.
These probabilities are sampled from a Dirichlet distribution whose parameter vector depends on the prototype parameters Φ and the assignment vector b ≡ (b1 , . . . , bn ).
Specifically, define the function f (Φ, b) to yield a parameter vector (Φ01 , . . . , Φ0n+1 ) such that:
Φ0i

=

8
<
:

Φb
i
C(b,bi )

Φn+2

if i ≤ n

(7)

if i = n + 1

87

where T is a set of simple terms and I is a function from
elements of T to values. This representation guarantees
that the elements of T are unordered, and each element is
mapped to only one value.
So to define our formula-modification distribution
Pfor (ϕ|ϕ∗ , v̄), we will suppose ϕ = (T, I) and
ϕ∗ = (T ∗ , I ∗ ). Recall that v̄ is the set of logical
variables that may be used in ϕ and ϕ∗ . To generate ϕ,
we first choose a set Tkeep ⊆ T ∗ , where each term in
T ∗ is included in Tkeep independently with probability
βterm . The terms in Tkeep will be included in T . Next,
we generate a set Tnew of new terms to include in T . The
size of Tnew , denoted knew , is chosen from a geometric
distribution with parameter αterm . Then, for i = 1 to
knew , we generate a term ti according to a distribution
Pterm (ti |v̄). This distribution chooses a predicate or
function symbol f uniformly at random, and then chooses
each argument of f uniformly from the set of constant
symbols plus v̄. We consider a run invalid if any element
of Tnew is in T ∗ : this ensures that while computing the
probability of a term set T given a prototype term set T ∗ ,
we can recover Tkeep as T ∩ T ∗ and Tnew as T \ T ∗ .

This definition says that if oi is generated from prototype
outcome bi (including the seed outcome), then Φ0i is obtained by dividing up Φbi over all the local outcomes derived from bi . The number of such outcomes is computed
by the function C(b, bi ), which returns the number of indices j ∈ {1, . . . , n} such that bj = bi . Finally, for the
noise outcome, we have Φ0n+1 = Φn+2 .

Next, we choose the term-to-value function I. For a term
t ∈ T ∩ T ∗ , the value I(t) is equal to I ∗ (t) with probability
ρ, and with probability (1 − ρ) it is sampled according to
a distribution Pvalue (x|v̄). If t ∈
/ T ∗ , then I(t) is always
sampled from Pvalue (x|v̄). This distribution Pvalue (x|v̄) is
uniform over the constant symbols in the language, plus v̄.

To define the overall distribution for a local rule r given a
rule prototype r∗ , we sum out the assignment variables Bi .
For valid rules r, we get:

3.6

Prule (r|r∗ ) = Pact (z|z ∗ ) Pfor (Ψ|Ψ∗ , v̄) Pnum (n|n∗ ) ·
X
Pout (O, b|O∗ , n) Dir[f (Φ, b)](p)

(8)

b∈
(O ∪{NIL})n
∗

Here Dir[f (Φ, b)] is the Dirichlet distribution with parameter vector f (Φ, b).

Generative Model for Rule Set Prototypes

The process that generates rule set prototypes G is similar
to the process that generates local rule sets from G, but
all the rule prototypes are generated from scratch — there
are no higher-level prototypes from which they could be
derived. We assume that the number of rule prototypes in
G has a geometric distribution with parameter αproto . Thus
the probability of a rule set prototype G of size m∗ with
∗
rule prototypes {r1∗ , . . . , rm
∗ } is:
∗

3.5

Modifying Formulas

The formulas that serve as contexts and outcomes are very
simple: they are just conjunctions of literals, where a literal
has the form t = x for some term t and value x. The term
must be simple in the sense that each of its arguments is
either a constant symbol or a logical variable; similarly,
x must be a constant symbol or a logical variable.3 We
do not care about the order of literals in a formula, and
we would also like to rule out self-contradictory formulas
in which multiple values are assigned to the same term.
It is convenient to think of a formula ϕ as a pair (T, I),
3
We are treating true and false as constant symbols, so a literal
such as ¬on(X, Y ) is represented as on(X, Y ) = false.

PG (G) = Geom[αproto ](m∗ ) · m∗ ! ·

m
Y

Pproto (ri∗ )

(9)

i=1

We consider a generative run to be invalid if it generates
the same rule prototype more than once, although we allow
rule prototypes to have overlapping contexts.
The rule prototypes are generated independently from the
distribution Pproto (r∗ ). This is similar to the distribution
for generating a local rule from scratch (as given by Eq. 8).
The action term z ∗ is chosen from the uniform distribution
Pact (z ∗ |NIL); the context formula Ψ∗ is generated by running our formula modification process on the empty formula ∅ given the logical variables v̄ from z ∗ ; the number of outcomes n∗ has a geometric distribution; and each
outcome o∗ in the outcome set O∗ is also generated from

88

DESHPANDE ET AL.

Pfor (o∗ |∅, v̄). The main difference from the case of local
rules is that rather than generating an outcome probability vector p, we generate a vector of Dirichlet weights Φ,
defining a prior over outcome distributions. We use a hyperprior PΦ (Φ|n∗ ) on Φ in which the sum of the Dirichlet
weights has an exponential distribution. Thus, if r∗ consists of an action term z ∗ containing logical variables v̄, a
context Ψ∗ , and an outcome set O∗ of size n∗ , then:
Pproto (r∗ ) = Pact (z ∗ |NIL) Pfor (Ψ∗ |∅, v̄)
Y
· Geom[α](n∗ )PΦ (Φ|n∗ )
Pfor (o|∅, v̄)
o∈O ∗

4

Learning

In our problem formulation, we are given sets of examples x1 , ..., xK from K source tasks, and a set of examples
(xK+1 ) from the target task . In principle, one could maximize the objective in Eq. 3 using the data from the source
and target tasks simultaneously. However, if K is fairly
large, the data from task K + 1 is unlikely to have a large
effect on our beliefs about the rule set prototype G. Thus,
we work in two stages. First, we find the best rule set prototype G∗ given the data for the K source tasks. Then,
∗
given G∗
holding G∗ fixed, we find the best rule set RK+1
and xK+1 . This approach has the benefit of allowing us to
throw away our data from the source tasks, and just transfer
the relatively small G∗ .
Our goal in the first stage, then, is to find the prototype G∗
with the greatest posterior probability given x1 , . . . , xK .
Doing this exactly would involve integrating out the source
rule sets R1 , . . . , RK . It turns out that if we think of each
rule set Rk as consisting of a structure RkS and parameters RkP (namely the outcome probability vectors for all the
rules), then we can integrate out RkP efficiently. However,
summing over all the discrete structures RkS is difficult.
Thus, we apply another MAP approximation, searching for
S
the prototype G and rule set structures R1S , . . . , RK
that together have maximal posterior probability. It is important
that we integrate out the parameters RkP , because the posterior density for RkP is defined over a union of spaces of
different dimensions (corresponding to different numbers
of rules and outcomes in Rk ). The heights of density peaks
in spaces of differing dimension are not necessarily comparable. So it would not be correct to use a MAP estimate
of RkP obtained by maximizing this density.

the outcome probabilities:
S
P (G, R1S , . . . , RK
)∝

PG (G)

K Z
Y
k=1

Scoring Function

S
In our search over G and R1S , . . . , RK
, our goal is to maximize the marginal probability obtained by integrating out

(10)

This equation trades off three factors: the complexity of the
rule set prototype, represented by PG (G); the differences
between the local rule sets and the prototype, Pmod (Rk |G),
and how well the local rule sets fit the data, P (xk |Rk ).
Computing the value of Eq. 10 for a given choice of G
and R1 , . . . , RK is expensive, because it involves summing
over all possible mappings from local rules to global rules
(the a values in Eq. 5) and all mappings from local outcomes to prototype outcomes (the b values in Eq. 8). Integrating out the outcome probabilities p in each rule is not a
computational bottleneck: we can push the integral inside
the sums over a and b, and use a modified version of a standard estimation technique [Minka, 2003] for the Polya (or
Dirichlet-multinomial) parameters.4
Rather than summing over all possible local-to-global
correspondences for rules and outcomes, we approximate by using a single correspondence. Specifically,
for each rule set Rk ≡ {r1 , . . . , rm }, we choose
the rule correspondence vector â that maximizes the
probability of the local rule contexts Ψi given the
global rule
Qmcontexts Ψ(ai ) (ignoring outcomes) â =
argmaxa i=1 PA (ai |G)Pfor (Ψi |Ψ∗(ai ) , v̄i ). Since each
factor contains only one assignment variable ai , we can
find the corresponding rule prototype for each local rule
separately. Given the rule correspondence â, we next construct an outcome correspondence for each rule ri . We use
the outcome correspondence that maximizes the probability of the local outcomes o1 , . . . , on given the outcome set
O∗ of the rule prototype
Qn âi (ignoring the outcome probabilities) b̂ = argmaxb i=1 PB (bi |O∗ )Pfor (oi |bi , v̄). Again,
the maximization decomposes into a separate maximization for each outcome. This greedy matching scheme can
yield a poor result if a local rule ri has a context similar to
a prototype rule, but very different outcomes. So as a final
step, we compute the probability of each ri being generated
from scratch, and set âi to NIL if this is a better correspondence.
These approximations yield the following scoring function
(an approximate version of Eq. 10), which we use to guide
our search.
S
Score(G, R1S , . . . , RK
)=
K Z
Y
PG (G)
k=1

4.1

Pmod (Rk |G)P (xk |Rk )
P
Rk

P
Rk

Pbmod (Rk |G)P (xk |Rk )

(11)

4
We modify the standard technique to take into account our
hyperprior PΦ . Also, we adjust for cases where some global outcomes are not included in a corresponding local rule. For a more
detailed explanation, see the master’s thesis by Deshpande [2007].

DESHPANDE ET AL.

Here Pbmod is a version of the measure Pmod from Eq. 5 in
which we simply use â rather than summing over ai values,
and we replace Prule with a modified version that uses b̂
rather than summing over b vectors.
4.2

Coordinate Ascent

We find a local maximum of Eq. 3 using a coordinate ascent algorithm. We alternate between maximizing over local rule set structures given an estimate of the rule set prototype G, and maximizing over the rule set prototype given
S
estimates of the rule set structures (R1S , ..., RK
):
K Z
Y

argmaxRS ,...,RS
1

K

k=1

argmaxG P (G)

K
Y

P (xk |Rk )P (Rk |G)

P (RkS |G)

We begin with an empty rule set prototype, and use a
greedy local search algorithm (described below) to optimize the local rule sets. Since R1 , . . . , RK are conditionally independent given G, we can do this search for each
task separately. When these searches stabilize — that is,
no search operator improves the objective function — we
run another greedy local search to optimize G. We repeat
this alternation until no more changes occur.
Learning Local Rule Sets

During the coordinate ascent one task is to find the highest
scoring local rule set Rk∗ given the rule set G. The search
is closely related the rule set learning algorithm problem
in Zettlemoyer et al. [2005]. There are three major differences: (1) G provides a prior that did not exist before;
(2) the outcomes O for each rule are constrained to be nonoverlapping; and (3) the rule parameters p are integrated
out instead of being set to maximum likelihood estimates.
4.3.1

uses a subalgorithm to find the best set of outcomes. This
outcome learning is done with a greedy search algorithm,
as described in the next section. The following operators
construct changes to the current rule set.
Add/Remove Rule. Two types of new rules can be added
to the set. Rules can be created by an ExplainExamples
procedure [Zettlemoyer et al., 2005] which uses a heuristic
search to find high quality potential rules in a data driven
manner. In addition, rules can be created by copying the
action and context of one of the prototypes in the global
rule set. This provides a strong search bias towards rules
that have been found to be useful for other tasks. New rule
sets can also be created by removing one of the existing
rules in the current set.

P
Rk

k=1

4.3

89

Rule Set Search

In this section, we briefly outline a local rule learning algorithm that is a direct adaptation of the approach of Zettlemoyer et al. [2005] and highlight the places where the two
algorithms differ. The search starts with a rule set that contains only the noisy default rule. At every step, we take the
current rule set and apply a set of search operators to create new rule sets. Each of these new rule sets is scored, as
described in section 4.1. The highest scoring set is selected
and set as the new Rk , and the search continues until no
new improvements are found.
The operators create new rule sets by directly manipulating
the current set: either adding or removing some number of
the existing rules. Whenever a new rule is created, the relevant operator constructs the rule’s action and context and

Add/Remove Literal. This operator selects a rule in the
current rule set, and replaces it with a new rule that is the
same except that one literal is added or removed from the
context. All possible additions and deletions are proposed.
Split on Literal. This operator chooses an existing rule
and a new term that does not occur in that rule’s context. It
removes the chosen rule and adds multiple new rules, one
for each possible assignment of a value to the chosen term.
Any time a new rule is added to a rule set, there is a check to
make sure that only one rule is applicable for each training
example. Any preexisting rules with overlapping applicability are removed from the rule set.
4.3.2

Outcome Search

Given a rule action z and a context Ψ, the set of outcomes
O is learned with a greedy search that optimizes the score,
computed as described in section 4.1. This algorithm is
a modified version of a previous outcome search procedure
[Pasula et al., 2004], which has been changed to ensure that
the outcomes do not overlap. Initially, O contains only the
noise outcome, which can never be removed. It each step,
a set of search operators is applied to build new outcome
sets, which are scored and the best one is selected. The
search finishes when no improvements can be found. The
operators include:
Add/Remove Outcome. This operator adds or removes
an outcome from the set. Possible additions include any
outcomes from the corresponding prototype rule or an outcome derived from concatenating the changes seen as a result of action effects in a training example (following [Pasula et al., 2004]). Any existing outcome can be removed.
Add/Remove Literal. This operator appends or removes
a literal from a specific outcome in the set. Any literal that
is not present can be added and any currently present literal
can be removed.

90

DESHPANDE ET AL.

Split on Literal. This operator takes an existing outcome
and replaces it with multiple new outcomes, each containing one of the possible value assignments for a new term.
Merge Outcomes. This operator creates a new outcome
computing the union of an existing outcome and one that
could be added by the add operator described above. The
original outcome is removed from the set.
Two of the operators, add outcome and remove function,
have the potential to create overlapping outcomes. To fix
this condition, functions are greedily added to overlapping
outcomes until no pair of outcomes overlap. This new outcome set is scored, and the search continues.
4.4

Estimating the Dirichlet parameters for the Polya distribution does not have a closed form solution, but gradient
ascent techniques have been developed for the maximum
likelihood solution [Minka, 2003]. To estimate the parameters for a rule prototype r∗ , the required occurrence counts
are computed for each prototype outcome and each local
rule that corresponds to r∗ (under the correspondence â
described in Sec. 4.1). If a local rule contains several outcomes corresponding to the same prototype outcome (under b̂), their counts are merged.

Experiments

We evaluate our learning algorithm on synthetic data from
four families of related tasks, all variants of the classic
blocks world. We restrict ourselves to learning the effects
of a single action, pickup(X, Y ). Adding more actions
would not significantly change the problem: since the action is always observed, one can learn a rule set for multiple
actions by learning a rule set for each action separately.
5.1

2. For each source task, generate a set of Nsource state
transitions to serve as a training set. In each state transition, the action is pickup(A, B) and the initial state is
created by assigning random values to all functions on
{A, B}.5 Then the resulting state is sampled according
to the task-specific rule set. Note that the state transitions are sampled independently of each other; they
do not form a trajectory.

Learning the Rule Set Prototype

The second optimization involves finding the highest scor∗
).
ing rule set prototype G given rule sets (R1∗ , ..., RK
Again, we adopt an approach based on greedy search
through the space of possible rule sets. This search has exactly the same initialization and uses all of the same search
operators as the local rule set search. There are three differences: (1) the AddRule operator tries to add rules that are
present in the local rule sets, without directly referencing
the training sets; (2) we relax the restriction that rules and
outcomes can not overlap, simplifying some of the checking that the operators have to perform; and (3) we need to
estimate the Dirichlet parameters for the outcomes for each
new prototype rule considered by the structure search.

5

1. Generate K “source task” rule sets from a prior distribution. This prior distribution is implemented by
a special-purpose program for each family of tasks.
This is slightly more realistic than generating the rule
sets from a rule set prototype expressed in our modeling language.

Methodology

3. Run our full learning algorithm on the K source-task
training sets to find the best rule set prototype G∗ .
4. Generate a “target task” rule set RK+1 from the same
distribution used in Step 1.
5. Generate a training set of Ntarget state transitions as in
Step 2, using RK+1 as the rule set.
bK+1 for the target task using the
6. Learn a rule set R
algorithm from Sec. 4.3, with G∗ as the fixed rule set
prototype.
7. Generate a test set of 1000 initial states using the same
distribution as in Step 2. For each initial state s, compute the variational distance between the next-state
distributions defined by the true rule set RK+1 and
bK+1 . This is defined in our case
the learned rule set R
as follows, with a equal to pickup(A, B) and s0 ranging
over possible next states:
X

Finally, compute the average variational distance over
the test set.
Variational distance is a measure of error, but we would like
the y-axis in our graphs to be a measure of accuracy, so we
use 1 − (variational distance).
The free parameters in our hierarchical Bayesian model
(and hence in our scoring function) are set to the same values in all experiments. While we found that the scoring
function in Eq. 11 leads to good results on large training
sets, we also saw that with small training sets, the very
small probabilities of formulas (in contexts and outcomes)
tend to dominate the score. For the experiments reported
5

Each run of our experiments consists of the following steps:

bK+1 )
p(s0 |s, a, RK+1 ) − p(s0 |s, a, R

s0

The distribution used here is biased so that A is always a
block and the robot’s gripper is usually empty; this focuses our
evaluation on cases where pickup(A, B) has a chance of success.

DESHPANDE ET AL.

91
Slippery Gripper Domain
1

No Transfer
1x5000
2x2500
20

40

1-(Variational Distance)

1-(Variational Distance)

Gripper Size Domain
1
0.98
0.96
0.94
0.92
0.9
0.88
0.86
0.84
0.82
0.8

0.9
0.8
0.7
0.6

No Transfer
1x5000
2x2500

0.5

60 80 100 120 140 160 180 200
Target Task Examples

20

40

(a)

(b)

Slippery Gripper with Size Domain

Random Domain
1

0.95
0.9
0.85
0.8

No Transfer
1x5000
2x2500

0.75
20

40

60 80 100 120 140 160 180 200
Target Task Examples

(c)

1-(Variational Distance)

1
1-(Variational Distance)

60 80 100 120 140 160 180 200
Target Task Examples

0.9
0.8
0.7
No Transfer
1x1000
4x250
10x100

0.6
0.5
20

40

60 80 100 120 140 160 180 200
Target Task Examples

(d)

Figure 3: Accuracy using an empty rule set prototype (labeled “No Transfer”) and transfer learning, labeled KxN where
K represents the number of source tasks and N represents the number of examples per source task.
here, we use a modified scoring function in which each occurrence of the formula distribution Pfor is raised to the
power 0.5. The fact that this ad hoc modification yields
better results suggests that our distribution over formulas
is overly flat, and it would be worthwhile to develop a formula distribution that gives common literals or subformulas higher probability.
5.2

Results

In this section, we present results in the four blocks world
domains. For each domain, we briefly describe the task
generation distribution and then present results.6 For each
experiment, we graph variational distance as a function of
the number of training examples in the target task. Each
experiment was repeated 20 times; our graphs show the average results with 95% confidence bars. The time required
for each run varied from 30 seconds to 10 minutes depending on the complexity of the domain.
Our first experiment investigates transfer learning in a domain where the rule sets are very simple — just single
rules — but the rule contexts vary across tasks. We use
a family of tasks where the robot is equipped with grippers of varying sizes. There are seven different sizes of
6
Deshpande [2007] presents a more detailed description of
these domains.

blocks on the table; the robot can only pick up blocks that
are the same size as its gripper. Thus, each task can be
described by a single rule saying that if block X has the
proper size, then pickup(X, Y ) succeeds with some significant probability (this probability also varies across tasks).
If X has the wrong size, then no rule applies and there is
no change. Since the “proper size” varies from task to task,
the rules for different tasks have different contexts. To increase the learning difficulty, two extra distracter predicates
(color and texture) are randomly set to different values in
each example state.
Fig. 3(a) shows the transfer learning curves for this domain. The transfer learners are consistently able to learn
the dynamics of the domain with fewer examples than the
non-transfer learner. In practice, in each source task, the
algorithm learns the specific pickup rule with the appropriate size literal in the context. The algorithm learns a single
rule prototype whose context also contains some size literal. This rule prototype provides a strong bias for learning
the correct target-task rule set: the learner only has to replace the size literal in the prototype with the correct size
literal for the given task.
To see how transfer learning works for more complex rule
sets, our next experiment uses a “slippery gripper” domain
adapted from [Kushmerick et al., 1995]. The correct model
for this domain has four fairly complex rules, describing

92

DESHPANDE ET AL.

cases where the gripper is wet or not wet (which influences
the success probability for pickup) and the block is being
picked up from the table or from another block (in the latter case, the rule must include an additional outcome for the
block falling on the table). The various tasks are all modeled by rules with the same structure, but include relatively
large variation in outcome probabilities.
Fig. 3(b) shows the transfer learning curves for the slippery gripper domain. Again, transfer significantly reduces
the number of examples required to achieve high accuracy.
We found that the transfer learners create prototype rule
sets that effectively represent the dynamics of the domain.
However, the structure of the prototype rules do not exactly match the structure of the four specific rules that are
present in each source task. Despite this fact, these prototypes still capture common structure that can be specialized
to quickly learn the correct rules in the target task.
Our third domain, the slippery gripper domain with size, is
a cross between the slippery gripper domain and the gripper size domain. In this domain, all four rules of the slippery gripper domain apply with the addition that each rule
can only succeed if the targeted block is of a certain taskspecific size. Thus, the domain exhibits both structural and
parametric variation between tasks.
As can be seen in Fig. 3(c), the transfer learners perform
significantly better than the non-transfer learner. In this
case, the rule set prototype provides both a parametric and
structural bias to better learn the domain.
Our final experiment investigates whether our algorithm
can avoid erroneous transfer when the tasks are actually
unrelated. For this experiment, we generate random source
and target rule sets with 1 to 4 rules. Rule contexts and
outcomes are of random length and contain random sets
of literals. Since rule sets sampled this way may contain
overlapping rules or outcomes, we use rejection sampling
to ensure that a valid rule set is generated for each task.
As can be seen in Fig. 3(d), the transfer and non-transfer
learners’ performances are statistically indistinguishable.
The learning algorithm often builds a rule set prototype
containing a few rules with random structure and high variance outcome distribution priors. These prototype rules do
not provide any specific guidance about the structure or parameters of the specific rules to be learned in the target task.
However, their presence does not lower performance in the
target task.

6

Conclusion

In this paper, we developed a transfer learning approach for
relational probabilistic world dynamics. We presented a hierarchical Bayesian model and an algorithm for learning a
generic rule set prior which, at least in our initial experiments, holds significant promise for generalizing across

different tasks. This learning problem is particularly difficult due to the need to learn relational structure along with
probabilities simultaneously for a large number of tasks.
The current approach addresses many of the fundamental
challenges for this task and provides a strong example that
can be extended to work in more complex domains and
with a wide range of representation languages.


Sampling is an important tool for estimating
large, complex sums and integrals over high­
dimensional spaces. For instance, importance
sampling has been used as an alternative to exact
methods for inference in belief networks. Ideally,
we want to have a sampling distribution that pro­
vides optimal-variance estimators. In this paper,
we present methods that improve the sampling
distribution by systematically adapting it as we
obtain information from the samples. We present
a stochastic-gradient-descent method for sequen­
tially updating the sampling distribution based on
the direct minimization of the variance. We also
present other stochastic-gradient-descent meth­
ods based on the minimization of typical notions
of distance between the current sampling distri­
bution and approximations of the target, optimal
distribution. We finally validate and compare the
different methods empirically by applying them
to the problem of action evaluation in influence
diagrams.

1

INTRODUCTION

Often, we are interested in computing quantities involving
large sums, such as expectations in uncertain, structured
domains. For instance, belief inference in Bayesian net­
works (BNs) requires that we sum or marginalize over the
remaining variables that are not of interest. Similarly, in
order to solve the problem of action selection in influence
diagrams, we sum over the variables that are not observed
at the time of the decision in order to compute the value of
different action choices.
We can represent the uncertainty in structured environ­
ments using a BN. A BN allows us to compactly define
a joint probability distribution over the relevant variables
in a domain. It provides a graphical representation of the

distribution by means of a directed acyclic graph (DAG).
It defines locally a conditional probability distribution for
each relevant variable, represented as a node in the graph,
given the state of its parents in the graph. This decomposi­
tion can help in the evaluation of the sums. However, due
to factors regarding the connectivity of the graph, in gen­
eral this is not sufficient to allow an efficient computation
of the exact value of the sums of interest.
Sampling provides an alternative tool for approximately
computing these sums. Sampling methods have been pro­
posed as an alternative to exact methods for such problems.
In particular, importance sampling (see Geweke [ 1989],
and the references therein) has been applied to the prob­
lem of belief inference in BNs [Fung and Chang, 1989,
Shachter and Peot, 1989] and action selection in IDs (see
Charnes and Shenoy [ 1999] and the references therein,
and Ortiz and Kaelbling [2000]). In its simpler form, the
importance-sampling distribution used is the "prior" dis­
tribution of the BN resulting from setting the value of the
evidence. It has been noted early on that this sampling dis­
tribution is far from optimal in the sense that it provides es­
timates with larger variance than necessary [Shachter and
Peot, 1989]. For instance, the optimal sampling distribu­
tion in the case of belief inference is to sample the unob­
served variables from the posterior distribution over them
given the observed evidence. If we knew this distribution
we would know the answer to the belief inference problem.
Several modifications have been proposed to improve the
estimation of the simple importance sampling distribu­
tion discussed above, based on information obtained from
the samples [Fung and Chang, 1989, Shachter and Peot,
1989, Shwe and Cooper, 199 1]. In this paper, we pro­
pose methods to systematically and sequentially update the
importance-sampling distribution. We view the updating
process as one of learning a separate BN just for sampling.
The learning objective is to minimize some error criterion.
A stochastic-gradient method results from the direct min­
imization of the variance of the estimator with respect to
the importance sampling distribution as an error function.
Other stochastic-gradient methods result from minimizing

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

447

error functions based on typical measures of the notion of
distance between the current sampling distribution and ap­
proximations of the optimal sampling distribution.

2

DEFINITIONS

We begin by introducing some notation used throughout
the paper. We denote one-dimensional random variables
by capital letters and denote multi-dimensional random
variables by bold capital letters. For instance, we de­
note a multi-dimensional random variable by X and de­
note all its components by
where
is the
ith one-dimensional random variable. We use small let­
ters to denote assignments to random variables. For in­
stance, X = x means that for each component
of X,
=
We denote the set of possible values that
can
take by Ox, and the set of possible values that X can take

(X1, ... ,Xn)

Xi

Xi
xi

xi Xi.

by Ox
X :1 nx,. We also denote by capital letters the
nodes in a graph. We denote by
the parents of node
in a directed graph.
=

Pa(Y)

Y

We now introduce notation that will become useful dur­
ing the description of the methods presented in this pa­
per.
We denote by the operator Lz the sum over
the possible values of the individual variables forming
Z, Lz Lz . . Lz . For any function h with vari1

2

·

n1

abies Z and 0, the expression h(Z,O)lo=o stands for
a function f' over variables Z that results from setting
the values of 0 in h with assignment o while letting
the values for Z remain unassigned. In other words,
f'(Z) = h(Z,O)lo=o = h(Z, 0
o) . The notation
X = (Z, 0) means that the variable X is formed by
all the variables that form Z and 0. That is, X =
(Z,O),
= (Z1, ... , Znp01, . . .
(X1, . .
where n = n1 + n2. Note that we are assuming that the set
of variables forming Z and those forming 0are disjoint.
The notation Z "" f means that the random variable Z is
distributed according to probability distribution f.
=

. ,Xn)

,On2)

=

A Bayesian network (BN) is a graphical probabilistic model
used to represent uncertainty in structured domains. It com­
pactly represents the joint probability distribution over the
relevant variables of the system of interest. It uses a di­
rected acyclic graph (DAG) to represent the relationship
between the relevant variables. A node in the graph rep­
resents a variable. The model defines a local conditional
distribution
for each node or variable
I
given its parents
in the graph. The joint distribution
is then

P(Xi Pa(Xi))
Pa(Xi)

Xi

For instance, we can define a BN on the graph given in
Figure l (a).
The inference problem in BNs is that of computing the pos­
terior probability of an assignment to a subset of variables

(b)
Figure 1: Example of (a) Bayesian network and (b) influ­
ence diagram.
given evidence about another subset of variables in the sys­
tem. Assume that the variables are discrete and their sam­
ple spaces or the possible values each variable can take are
finite. In general, let X = (Z, 0) where 0 is the set of
variables of interest, o is an assignment to it and Z are the
remaining variables. For this problem we want to compute
probabilities of the kind

P(O

= o

)

=

LzP(Z,O = o ) .

Often, the local decomposition of the joint distribution still
leads to the evaluation of sums over a large number of
variables. In general, this problem is intractable [Cooper,
1990].
An influence diagram (ID) is a probabilistic model for
decision-making under uncertainty. We can think of an ID
as a BN with decision and utility nodes added. For instance,
we can use our example BN to build an ID as shown in Fig­
ure 1(b). The square is a decision node. The diamond is a
utility node. We now have potentially different joint distri­
butions over the variables, for each action choice available.
Assume for simplicity that there is a single decision node
in the graph. The joint distribution over the variables, given
the action choice aassigned to the decision variable, is

P(X I A= a) =

TI�=l P(Xi I Pa(Xi))IA=a

·

The decision associated with a decision node is a function
of its parent nodes in the graph. We will have access to

448

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

the value of these variables at the time of making the deci­
sion. Similarly, the utility associated with a utility node is
a function of its parent nodes in the graph.
Assume that we have a finite number of discrete action
choices. Then, one problem is to select the best strategy or
function 7r* mapping each possible value of the parents of
the decision node to an action choice. The best strategy is
the strategy with highest expected utility. Let X = (Z, 0)
where the variables in 0are parents of the decision node
and Z are the remaining variables. The problem of ob­
taining an optimal strategy reduces to obtaining, for each
assignment 0 = o, the action that maximizes the value
associated with the action and the assignment:

V0(a) =I::z P(Z , 0= o I A= a) U(Z , 0=o, A= a) .
Note once again that computing this value requires the eval­
uation of a sum. For the same reasons as in the previous
problem of belief inference in BNs, the exact computation
of this value is intractable in general.

3

to it the value given by the evidence assignment o. There­
fore, the resulting samples will be assignments to those
variables that are not in the evidence set according to the
"prior" distribution of the BN. We call the method resulting
from this importance-sampling distribution the traditional
method. In the context of belief inference, this method is
called likelihood-weighting (LW) since the weight function
is a "likelihood" and thus each sample is weighted by its
"likelihood."
We can similarly apply this technique in the context of ac­
tion selection in IDs to evaluate V0 (a) . In general, we let

g(Z)
f(Z)

P(Z ,O = o I A=a) U(Z ,O =o, A= a) ,
TI��l P(ZiI Pa(Zi) ) lo=o,A=a'

w(Z)

IJ?�1 P(OjI Pa(Oj) ) U(Z , 0, A)
O=oA
, =a

=

1'\' N
(l)
GA = N ul
=lw( z ) .

(1)

We can apply this technique to the problem of belief infer­
ence in BNs. Typically, we let

g(Z) =P(Z , 0

=

o

)

I

= TI��1P(ZiI Pa(Zi) ) IJ?�1P(Oj I Pa(Oj) ) = '
O o
f(Z)
TI��1 P(ZiI Pa(Zi) ) lo=o, which implies
w(Z)

I

TI?�1 P(OjI Pa(Oj) ) O=o.

Note that we are defining the importance sampling distri­
bution to be the "prior" distribution of the BN. We obtain
samples from this distribution by sampling the variables
in the (partial) order defined by the DAG and according
to the local conditional distribution of the original BN for
each variable. As we obtain samples from each variable by
traversing the nodes in the graph and sampling the variable
corresponding to it, if we get to a node or variable that is in
the evidence set 0, we do not sample it. Instead, we assign

·

In particular, for our example,

g(Z)

IMPORTANCE SAMPLING

Importance sampling provides an alternative to the exact
methods for evaluating sums. Let the quantity of inter­
est be G = I::z g( Z) for some real function g. We
can turn the sum into an expectation by expressing G =
I::z f(Z) (g(Z)/ f(Z)), where f is a probability distribu­
tion over Z satisfying, for all Z, g(Z) =f 0 =? f(Z) =f 0.
We call f the importance-sampling distribution. We de­
fine the weight function w(Z) = g(Z)/ f(Z) which al­
lows us to express G
I::z f(Z)w(Z). Hence, we can
obtain an unbiased estimate of G by obtaining N samples
z<1l, . .. , z(N) from Z "' f and computing the estimate

I

f(Z)

P(Xl) P(X2 I Xl) P(X3 I Xl) X
P(X6 I x2, A= a) P(X7 I X3, X6) X
P(X4 =X4 I X2) P(Xs =Xs I x2, X3)
U(X1, A = a) ,
P(Xl) P(X2 I Xl) P(X3 I Xl) X
P(X6 I x2, A a) P(X7 I x3, X6) ,
P(X4 X4 I X2) P(Xs =Xs I x2, X3)
U(X1, A=a) .

X

=

w(Z)

=

X

An important property of the estimator G is the variance of
the weights associated with the importance-sampling dis­
tribution. This is

Var[w(Z ) ] = I::z f(Z)w(Z)2- G 2•
Recall that G = I::z g(Z) by definition and assume that
g is a positive function. From this we can derive that the
optimal or minimum-variance importance-sampling distri­
bution is proportional to g(Z):

f*(Z) = g(Z)/ I::z g(Z).

(2)

The weights will have zero variance in that case, since the
weight function will always output our value of interest
G. We also note that we need to avoid letting f(Z) be
too small with respect to g(Z), since this will increase
the variance. As a matter of fact, Var[w(Z ) ] --+ oo as
f(Z) --+ 0 for at least one value of Z. This implies that
we should use importance-sampling distributions with suf­
ficiently "fat tails."

4

ADAPTIVE IMPORTANCE SAMPLING

The traditional method presented above uses as the
importance-sampling distribution the "prior" distribution

449

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

of the BN which can be far from optimal in the sense that
it can have higher variance than necessary. In the case of
evaluating actions in IDs, it also completely ignores poten­
tially useful information about the utility values. Therefore,
we try to learn the optimal importance-sampling distribu­
tion by adapting the current sampling distribution as we
obtain samples from it.
We view the adaptive process as one of learning a distribu­
tion over the variables the sum is over to use specifically as
an importance-sampling distribution. In particular, we can
view this process as learning BNs from the samples just for
sampling. From the expression of the optimal importance­
sampling distribution given in equation 2 (and, in particu­
lar, from the factorization of the function g for the different
estimation problems), we can deduce that in order to be
able to represent this distribution graphically using a BN
we need to add arcs that connect every pair of nodes that
are parents of observations and/or utility nodes, if they are
not already connected. However, doing so can increase the
size of the model, particularly in cases where the local con­
ditional probabilities and the utilities have a smaller, more
compact parametric representation (i.e., noise-or's). In this
paper, we do not deal with this issue and instead concen­
trate on the problem of learning a BN with the same struc­
ture as the original BN (or ID). Hence, we only need to
update the local conditional probability distributions as we
obtain samples.
We can parameterize the importance-sampling distribution
using a set of parameters E>. Let the indicator function
I(Zi k, Pa(Zi) = j I Z) 1 if the condition zi = k
and Pa(Zi) = j agrees with the value assigned to Z; 0
otherwise. Then, we can express the importance-sampling
distribution as
=

=

n

J (Z I

E>) = II

II elCkZ;=k,PaCZ;)=jiZ) '
•J

II

(3)
where for each i,j, k, eijk
P(Zi k I Pa(Zi) = j, E>).
Hence, for all i,j, L:k eijk
1, and for all k, eijk > 0.
Note that this representation uses the assumptions of global
and local parameter independence typically used in BNs.
The weight function is also parameterized and defined as
=

=

=

w(Z

IE>) = g(Z)/ f(Z I E>).

4.1

LEARNING CRI TERIA AND UPDATE RULES

In the following subsections we present different methods
for updating the sampling distribution. The update rules
are all based on gradient-descent. Hence, at each time t,
we update the parameters as follows:

(4)
oCt+1) +-oCt)- a(t)\i'Pe(OCt)).
In the update rule above, a(t) denotes the learning rate or
the step size rule and \i'Pe(E>) denotes the gradient of error

function e, appropriately projected to satisfy the constraints
onE>. The methods differ in how they define \i'Pe(oCtl).

In the discussion below we denote the N(t) i.i.d. samples
as zCt,1), ... , zCt,NCt)) drawn according to Z
f (Z I
oCtJ). If we gather samples to estimate G using many dif­
ferent sampling distributions, how can we combine them
to get an unbiased estimate? It is sufficient to weight them
using any weighting function that is independent of the sub­
estimates obtained by using just the samples for one sam­
pling distribution. For instance, the estimator
""

C;CT) = I::'{=1 W(t)G(oCtl),

(5)

I::'{=1 W(t) = 1 and W(t) ;:::: 0, for all t, and
"NCt) w(zCt,l) I oCt))
G(oCt)) = N_1Ct_) L...
(6)
l=1
is unbiased as long as W(t) and G(oCt)) are independent
for each t. Letting W(t)
1/T will produce an unbi­
where

'

=

ased estimate. This is the weight we use in the experi­
ments. In general, we would like to give more weight to
importance-sampling distributions with smaller variances.
Assuming that the variance decreases with t, we would like
W(t) to be an increasing sequence oft. Note that using
W(t) ex 1/ where is the sample variance at time t,
though appealing, does not necessarily lead to an unbiased
estimator since W(t) and G(oCt)) are not independent.

&f,

&f

We will consider three general strategies: minimizing vari­
ance directly, minimizing distance to global approxima­
tions of the optimal sampling distribution, and minimizing
distance to the empirical distribution of the optimal sam­
pling distribution based on local approximations. For the
first two strategies, we will find that we can express the
partial derivatives that form the gradient as, for all i,j, k,

8eCE>)
8!Jijk

= I: z J (Z

aCZ;)=jiZ)
IE>) [-JCZ;=k,P
(Jijk
<p(Z, E>)]'
X

where <p( Z, E>) is a function that depends on the error func­
tions. Note that this is an expectation. Then, the methods
update the parameters by estimating the value of the partial
derivatives evaluated at the current setting of the parame­
ters oCt) as

ae.co<'))
8!J;jk

4.1.1

=

__
1 "r-:.__C ) [-JCZ;=k,PaCZ;
)=jiZ=z(t,l)) X
(t)
NCt) L...l-1t
(Jijk
<p(zCt,l), oCt))].

Minimizing Variance Directly

As we noted above, the optimal importance-sampling dis­
tribution for estimating G is that which minimizes the
variance of w. Using that as our objective, we derive a
stochastic-gradient update rule for the parameters of the

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

450

importance-sampling distribution. Let the error function
be

evar(8)

Var(w(Z I 8))
Ez f(Z I e)w(Z I

8)2.

eKL2(8) = Ez f(Z I 8) log (f(Z I 8)/ f*(Z)).

(7)

The corresponding function for the gradient is

4?KL2(Z, 8) =log (f*(Z)/ f(Z I 8)) - 1
�log (w(z<t,t) 1 o<tJ)fGCtl\ - 1.

8).

Approximate Global Minimization

Recall the optimal importance-sampling distribution f* for
estimating G given in equation 2. The update rules of the
following subsection are all motivated by the idea of reduc­
ing some notion of distance between the current sampling
distribution and this optimal sampling distribution. Note
that we cannot really compute the values of the optimal dis­
tribution since that requires knowing the normalizing con­
stant Ez g(Z) = G which is exactly the value we want
to estimate. We approximate the optimal distribution using
the current estimate of G as follows
(8)

In the following, we will consider four error functions, one
based on the sum-squared-error and three based on versions
of the Kullback-Leibler divergence.

eKL.(8)= ! eKL1(8) + !eKL2(8).
We can obtain the partial derivatives for this error function
and their approximation accordingly.

4.1.3

Heuristic Local Minimization Based on
Empirical Distribution

The update methods in this subsection are motivated by the
idea of minimizing different notions of distance between
the current sampling distribution and an empirical distribu­
tion of the optimal importance-sampling distribution that
we build from the samples. The hope is that the empirical
distribution is a good approximation of the optimal sam­
pling distribution. We define the empirical distribution, pa­
rameterized by 8 locally as follows: for all i, j, k,

z=[':_<;l I(Z;==k,Pa(Z;}==jiZ==z(t,l))w(z<t,l) l9(t))
(t)
Bijk
z=[:<;) I(Pa(Z;)==jiZ==z(t,l))w(z(t,l) 19('))
_

If we use the 12 norm or sum-squared-error function as a
notion of distance between the distributions, then the error
function is

f*(Z)- f(Z I 8)
f(z(t,l} I o<t)) X
(w(z<t,t> 1 o<t>);c<t> - 1\,

(9)

where the approximation results from using ]t(Z) as de­
fined in equation 8 as an approximation to f*(Z).
An alternative, commonly-used notion of distance between
two probability distributions is given by the Kullback­
Leibler (KL) divergence. This measure is not symmetric.
One version of the KL divergence in this context is given
by the error function

eKL1(8) = Ez f*(Z) log (f*(Z)/f(Z I 8)).

if

L:{�g) I(Pa(Zi) = j

I

z=

'

(l2)

z(t,l))w(z(t,l) I oCtl) of- 0;

e;;� = e�� otherwise. We are essentially defining the em­

pirical distribution using the samples if there are samples
that can be used to define it; otherwise, we revert to the
current distribution. We try to minimize the distance be­
tween the current sampling distribution and the empirical
distribution locally.

The corresponding function for the gradient is

�

( 1 1)

A "symmetrized" version of KL sometimes used is given
by the error function

Minimizing Variance Indirectly via

]t(Z)= g(Z)jc<tl.

( 10)

Another version of the KL divergence is given by the error
function

Note that using this definition of <p yields an unbiased es­
timate of the gradient. This is because the gradient is the
expectation of a particular function and, in this case, we can
always evaluate the function exactly. Hence, we can obtain
an unbiased estimate by sampling from f(Z I

4.1.2

f*(Z)/ f(Z I 8)
w(z(t,l) I (J(tl)jG(tJ .

�

8)2- G 2

The corresponding function for the gradient is

<t'Var(Z, e)= w(Z I

The corresponding function for the gradient is

Similar to the case of the previous strategies, we will find
that we can express the partial derivatives that form the gra­
dient of the error functions discussed in this subsection as,
for all i, j, k,

8e'(E>)
80,3k

'

=

1
-cp (eijk, eijk),

where cp'(Bijk. eijk) is a function that depends on the error
functions. Then, the methods update the parameters by es­
timating the value of the partial derivatives evaluated at the
current setting of the parameters o< t) as
ae' (9 < ') >
ae,j k-=

'C J

' t
<tJ
-cp (eijk' eijk).

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

el2,

The local Lz error function,
leads to an update rule
for which the step size has a very intuitive interpretation
as a weighting between the current importance-sampling
distribution and the empirical distribution. In the case
of
, the update direction is proportional to the ratio
of the empirical distribution with respect to the current
importance-sampling distribution. On the other hand, for
the update direction is proportional to the logarithm
is not defined if at least
of the same ratio. Note

We define the local L2-norm error function as

el2 (e) = ! �i,j,k (eijk- eijk I

2

,

the error function for one version of KL as

ekLl (8) = �i,j,k eijk log ( eijk/Bijk I

ekL1

'

ekL2,

and the other as

ekL2(8) �i,j,k eijk log ( eijk/Bijk I .
From this we obtain the corresponding functions for the
gradient:

v{2(eijk, eijk)
'PkL1(Bijk, Bijk)
'PkL2(Bijk, Bijk)

DISCUSSION OF UPDATE RULES

First, note that of all the update rules, only the one derived
for
clearly uses an unbiased estimate of the gradient. It
is not immediately apparent whether the update rules based
and
on
use unbiased estimates.

evar

eKL2

Note also that the magnitude of the components of the re­
sulting gradients are different, as suggested by their respec­
tive functions. The function
has magnitude propor­
tional to the squares of the weights. The magnitudes of
and
are linear in the weights. However, the magni­
tude of
is potentially smaller since it has the probabil­
ity of the sample as a factor. The magnitude of
is
logarithmic in the weights.

<p

<pvar

<pL2

'PKL1
<pL2

'PKL2

Because we assume that g is positive, the weights are pos­
itive. Hence,
and
are always positive. The
function
is positive if w(Z I
> 1. Similarly,
is positive if log(w(Z I
the function
> 1.
If w(Z I
>
then the sampling distribution under­
estimates the value of g while if w(Z I 0) <
then it
overestimates the value. Therefore, the sign of
and
depends on whether we under- or over-estimated the
value of g. Similarly, the magnitudes of
and
are related to the amount of under- or over­
the magnitude is larger
estimation. For
and
when the sampling distribution underestimates than when it
overestimates. For
the logarithm brings the amount
of over- and underestimation to the same scale. Note that
for the approximations of
and
G can­
not be zero, and in addition for
w(Z I 0) cannot
be zero. These conditions hold from the assumption that g
is positive. Note that unless we constrain the importance­
sampling distribution, all the functions
and
will be unbounded even if g is bounded.

'PL2

'PVar

'PKL2
8) G

'PKL1

8)/G

'PKL2

8)/G)

G
'PL2

'PKL2

'PVar, 'PL2, 'PKL1,

'PVar, <pL2

'PKL1

'PKL2,

'PL2, 'PKL1,
'PKL2,

'PKL2

=

_

This is essentially imposing a Dirichlet prior with parame­
ters equal to the current probability values on the empirical
distribution parameters.

eijk - eijk'
eijk;eijk,
log (eijk/Bijk I - 1.

We can obtain an update rule based on the "symmetrized"
version of KL accordingly.

eL2, eKL1

'PKL2

eiJ� 0. We can fix this by letting, for each i, j, k,
( z:;;;,<:> I(Z;=k,Pa(Z;)=jJZ=z(t,l))w(z<t,l) JO(tl)) +iii;�
t)
B(ijk - ( z:;;;.<;> I(Pa(Z;)=jJZ=z(t,l) )w(z(t,l) IO(t))) +1

one

=

4.2

451

'PKL2,

'PVar, 'PL2, 'PKL1

We can interpret the update rules based on local KL­
divergence as adding weights to the elements of the domain
of the importance-sampling distribution and renormalizing.
For the version of KL-divergence with respect to the em­
pirical distribution, we are always adding weights. We add
values relative to the amount we underestimated or over­
estimated the magnitude of the distribution for a particu­
lar state. If we underestimated, we add weights larger than
one. If we overestimated, we add weights smaller than one.
For the other version of KL-divergence, due to the loga­
rithm function, we add weight if we underestimated while
we subtract weight if we overestimated. Therefore, the log­
arithm brings the amount of underestimation and overesti­
mation to the same scale and adds or subtracts weight ac­
cordingly.

evar, eL2,

Note that when approximating the gradients for
and
we can use as little as one sample to obtain
1) . This is not ad­
an estimate of the gradient (i.e., N(t)
visable for the method based on the local heuristic since the
empirical distribution of the optimal sampling distribution
will be highly inaccurate. Hence, the update rules based on
the empirical distribution will work better when we take a
larger number of samples between updates. Finally, note
that when t
1 and N(t) 1,
= 0, and therefore, the
parameters will not change in the first iteration.

eKL1

eKL2,

=

=

5

=

<pL2

RELATED WORK

Different variations of importance sampling have been used
for the problems discussed in this paper (See Lin and
Druzdzel [1999] and the references therein). Our methods
belong to the class of forward samplers since they sam­
ple from a distribution based on the original structure of
the BN. Of these, self-importance sampling [Shachter and
Peot, 1989, Shwe and Cooper, 1991] is the method closest
to the methods proposed in this paper since it also updates
the sampling distribution as it obtains information from the
samples. This method has an update rule that is very sim­
ilar to the one derived for
It updates the distribution

el2•

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

452

after obtaining the empirical distribution, but the update is
a weighting between the empirical distribution and the first
sampling distribution used [Shwe and Cooper, 199 1]. The
update rule is

eiJ t1

)

,__

e

(1- a(t))
e<tlk-

m + a(t)Bi�Z

'1

eiJ�
iJ�
a(t) e - (1- a(t))
a(t) - Bijk .
a(t)

(

(o))

In our framework, we can think of this update rule as re­
sulting from the error function

esJs(E>, t)
1

"""

(

=

(

'

(0)

)) 2 .

a(t))Bijk + a(t)Bijk
2a(t) �k Bijk- (1'1

Annealed importance sampling [Neal, 1998] is a related
technique in that it tries to obtain samples from the opti­
mal sampling distribution. As we understand it, the user
sets up a sequence of distributions, the last distribution be­
ing the optimal distribution, typically defined by Markov
chains. We move from one distribution to another as we
"anneal" and the sequence converges to the optimal sam­
pling distribution. The hope is that we can get an inde­
pendent sample from that distribution, then we restart the
process to try to obtain another independent sample, and
so on. Finally, it uses those independent samples to obtain
an estimate. Notice that each "traversal" of the sequence
of distributions (or Markov chains) produces a single sam­
ple. The technique is very general and we are unaware of
whether it has been applied to the problems considered in
this paper. We are currently investigating possible connec­
tions between our methods and this technique.

6

EMPIRICAL RESULTS

We implemented all of the adaptive importance-sampling
methods described above. We Jet the learning rate a(t) =
(3jt, where (3 is a value that depends on the updating
method. We need different values of (3 for the different
methods because of the differences in magnitude of their
gradients. We impose an additional constraint on the pa­
rameters which we call the €-boundary. We require that for
all i, j, k, Bijk 2: E(IDx,l)
"!/ IDx,l, where"( is a con­
stant factor. In our experiments, we Jet"(
0.1. We do
this so that our sampling distribution has "fat tails", avoid­
ing extrema in probability and hence the possibility of in­
finite variance. We initialize the parameters o<O) such that
the starting importance-sampling distribution is the "prior"
probability distribution of the original BN. However, if one
of the local conditional probability values does not satisfy
the E-boundary constraint, we change the distribution so
that it does.
=

=

Figure 2: Graphical representation of the ID for the com­
puter mouse problem.
In order to satisfy the constraint that for all i, j, Lk Bijk
1, we project the approximation of the gradients onto the

=

simplex of the local conditional probability distribution.
We do so by Jetting, for all i, j, k,

8Pe(&)
ao,jk

,__

8e(&)
ao,jk

_

1 "'lnx , I ae(&)
Jnx, 1 L....k=l ao,jk

·

( 13)

Note that this is not enough to guarantee that after taking a
step in the projected direction, the parameters will remain
in the constraint space. If, when updating a local condi­
tional probability distribution, its respective parameters do
not satisfy the constraint, we find the minimum step a' that
will allow them to remain inside the constraint space and
take a step of size a' /2 along the gradient direction (i.e.,
half the distance between the current position of the param­
eter we are updating in the simplex and the closest point on
the €-boundary along the gradient direction).
We tested the methods on the computer mouse prob­
lem [Ortiz and Kaelbling, 2000], a simple made-up ID
shown in Figure 2. We added one to all the utility val­
ues presented in Ortiz and Kaelbling [2000] to make g
positive. We will consider the problem of obtaining the
value VMP, (A) for the action A = 2 and the observation

MPt = l.

We evaluated each method by computing the mean­
squared-error (MSE) between the true value of the expec­
tation of interest (VM p, (A)) and the estimate generated us­
ing the adaptive sampling method. The first results show
how the methods achieve better MSEs with fewer samples
for this problem. We only show results for those methods
that were the most competitive. We denote by "Var" the
method based on the minimization of the variance, and by
"L2 " "KL1", and "KLS" the methods based on the global
minimization of 12, KL1 and KLs respectively. For the
1 for all t. We take into
update methods we use N(t)
,

=

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

453

LW--->Var ···)(···
L2 ........
Kl1 ......(;)..M..
KLS -·-•·-·

LW--->Var ···)(···
L2 ........
KL1 ......oEJ......
KLS -·-•·-·

\

\

50

100

150

200

250

50

100

number of samples

Figure 3: Average mean squared error, over 40 runs, as a
function of the number of samples taken. We allow LW
twice as many samples.

account that the update methods have to traverse the graph
once every iteration to update the parameters relevant to the
sample taken. To compensate for this time, we allow the es­
timate based on LW to use twice as many samples. Figure 3
shows the results. The graph shows the average MSE over
40 runs as a function of the total number of samples taken
(times 2 for LW) by the methods. We note that Var and L2
achieve better MSEs than LW and converge to them faster.
With significance level 0.005 we can state (individually)
for each total number of samples N = 50, 150, 250, that
Var and L2 (individually) are better with respect to MSE
than LW. Also, for N = 250, KLS is better than LW.
We also ran the methods with N(t) = 50, including the
local heuristic methods. They were only competitive after
a larger total number of samples (N > 150). Although fur­
ther analysis is necessary, we would like to convey some
general observations. We believe that in general there is a
tradeoff in the setting of N(t) and (3. We note that, of the
updates based on the two KL versions, KLl typically per­
forms better than KL2. We believe this is because the error
function eKL1 is defined with respect to the optimal sam­
pling distribution while eKL2 is with respect to the current
sampling distribution. KLS seems to perform better than
both. L2 is more stable than any of the other methods, sug­
gesting further theoretical analysis which we are currently
undertaking. Several possible reasons for this behavior are
(1) the variance of the gradient might be smaller than in
other cases, (2) the error function is bounded, and/or (3)
the error surface might be smoother than in other cases. We
conjecture that L2 converges to a stationary point of eL2•
The second result shows that the update methods indeed
lead to importance-sampling distributions with smaller
variance relatively quickly for this problem. Figure 4

150

200

250

300

350

400

450

number of samples

Figure 4: Average of the true variance of the weight func­
tion, over 40 runs, as a function of the total number of sam­
ples taken.

shows a graph of the true variance of the sampling distribu­
tion learned using the different update methods as a func­
tion of the total number of samples used. The horizontal
line shows the variance associated with the sampling dis­
tribution used by LW (i.e., the "prior" distribution of the
original BN).
These experiments are all carried out on a single prob­
lem. Although they must clearly be extended to a variety
of larger problems, they indicate that adaptive importance­
sampling methods, particularly those that minimize vari­
ance and the 12 norm, can lead to significant improvements
in the efficiency of sampling as a method for computing
large expectations.
Acknowledgments

The dynamic weighting scheme and the 1/ CJ2 recommen­
dation in Section 4.1 and the E-boundary in Section 6
were independently developed by Jian Cheng and Marek
Druzdzel. Both heuristics are reported in a manuscript that
the first author saw while he was working on this paper.
We would like to thank Milos Hauskrecht, Thomas Hof­
mann, Kee-Eung Kim and Thomas Dean for many discus­
sions and feedback. Also, our implementation uses some of
the functionality of the Bayes Net Toolboxfor Matlab [Mur­
phy, 1999], for which we thank Kevin Murphy. We would
also like to thank the anonymous reviewers for their in­
sightful comments.
Luis E. Ortiz was supported in part by an NSF Gradu­
ate Fellowship and in part by NSF IGERT award SBR
9870676. Leslie Pack Kaelbling was supported in part by
a grant from NTT and in part by DARPA Contract #DABT
63-99-1-0012.

500

454

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000




the expeeted number of actions required to reach the
goal.

We describe a method for time-critical de­
cision making involving sequential tasks and
stochastic processes. The method employs
several iterative refinement routines for solv­
ing different aspects of the decision mak­
ing problem. This paper concentrates on
the meta-level control problem of delibera­
tion scheduling, allocating computational re­
sources to these routines. We provide dif­
ferent models corresponding to optimization
problems that capture the different circum­
stances and computational strategies for de­
cision making under time constraints. We
consider precursor models in which all deci­
sion making is performed prior to execution
and recurrent models in which decision mak­
ing is performed in parallel with execution,
accounting for the states observed during ex­
ecution and anticipating future states. We
describe algorithms for precursor and recur­
rent models and provide the results of our
empirical investigations to date.

We represent goals of aehievement in terms of an opti­
mal sequential decision making problem in which there
is a reward function specially formulated for a partie­
ular goal. For the goal of achieving p as quiekly as
possible, the reward is 0 for all states satisfying p and
-1 otherwise. The optimization problem is to find a
policy (a mapping from states to actions) maximiz­
ing the expected discounted cumulative reward with
respect to the underlying stochastic process and the
specially formulated reward function. In our formula­
tion, a policy is nothing more than a conditional plan
for achieving goals quickly on average.

Introduction

We are interested in solving sequential decision making
problems given a model of the underlying dynamical
system specified as a stochastic automaton (i.e., a set
of states, actions, and a transition matrix which we
assume is sparse ) . In the following, we refer to the
specified automaton as the system automaton. Our
approach builds on the theoretical work in operations
research and the decision sciences for posing and solv­
ing sequential decision making problems, but it draws
its power from the goal-directed perspective of artifi­
cial intelligence. Achieving a goal corresponds to per­
forming a sequence of actions in order to reach a state
satisfying a given proposition. In general, the shorter
the sequence of actions the better. Because the state
transitions are governed by a stochastic proeess, we
cannot guarantee the length of a sequenee achieving a
given goal. Instead, we are interested in minimizing

Instead of generating an optimal policy for the sys­
tem automaton, which would be impractical for an
automaton with a large state space, we formulate a
simpler or restrictert stochastic automaton and then
search for an optimal policy in this restricted automa­
ton. At all times, the system maintains a restricted au­
tomaton. The restricted automaton and correspond­
ing policy are improved as time permits by successive
refinement. This approach was inspired by the work
of Drummond and Bresina [Drummond and Bresina,
1990] on anytime synthetic projeC-tion.
The state space for the restricted automaton corre­
sponds to a subset ·of the states of the system au­
tomaton (this subset is called the envelope of the re­
stricted automaton) and a special state OUT that rep­
resents being in some state outside of the envelope.
For states in the envelope, the transition funetion of
the restricted automaton is the same as in the system
automaton. The pseudo state OUT is a sink (i.e., all
actions result in transitions back to OUT) and, for a
given action and state in the envelope, the probability
of making a transition to OUT is one minus the sum
of the probabilities of making a transition to the same
or some other state in the envelope.
There are two basic types of operations on the re­
stricted automaton. The first is called envelope al­
teration and serves to increase or decrease the num­
ber of states in the restricted automaton. The second
is called policy generation and determines a policy for

310

Dean et al.

b

i.

ii.

Figure 1: Stochastic process and a restricted version

the system automaton using the restricted automaton.
Note that, while the policy is constructed using the re­
stricted automaton, it is a complete policy and applies
to all of the states in the system automaton. For states
outside of the envelope, the policy is defined by a set of
reflexes that implement some default behavior for the
agent. In this paper, deliberation scheduling refers to
the problem of allocating processor ·time to envelope
alteration and policy generation.
There are several different methods for envelope al­
teration. In the first method, we simply search for
a (new) path or trajectory from the initial state to a
state satisfying the goal and add the states traversed in
this path to the state space for the restricted automa­
ton. This method need not make use of the current
restricted automaton. A second class of methods op­
erates by finding the first state outside the envelope
that the agent is most likely to transition to using its
current policy, given that it leaves the set of states
corresponding the current envelope. There are several
variations on this: add the state, add the state and the
n next most likely states, add all of the states in a path
from the state to a state satisfying the goal, add all of
the states in a path from the state to a state back in
the current envelope. Finally, there are methods that
prune states from the current envelope on the grounds
that the agent is unlikely to end up in those states
and therefore need not consider them in formulating a
policy.
Figure 1.i shows an example system automaton con­
sisting of five states. Suppose that the initial state is
1, and state 4 satisfies the goal. The path 1 � 2 � 4
goes from the initial state to a state satisfying the
goal and the corresponding envelope is {1, 2, 4}. Fig­
ure 1.ii shows the restricted automaton for that en­
velope. Let 1r( x) be the action specified by the pol­
icy 1r to be taken in state x; the optimal policy for
the restricted automaton shown in Figure l.ii is de­
fined by 1r ( 1) = 1r (2) = 1r (4) = a on the states of
the envelope and the reflexes by 1r(OUT)
b (i.e.,
'ifX (/_ {1 1 2 1 4} 1 1r( X) = b)·
=

All of our current policy generation techniques are
based on iterative algorithms such as value iteration
[Bellman, 1957] and policy iteration [Howard, 1960].
In this paper, we use the latter. These techniques can
be interrupted at any point to return a policy whose

value improves in expectation on each iteration. Each
iteration of policy iteration takes 0( IE13) where E is
the envelope or set of states for the restricted automa­
ton. The total number of iterations until no further
improvement is possible varies but is guaranteed to be
polynomial in lEI. This paper is primarily concerned
with how to allocate computational resources to enve­
lope alteration and policy generation. In the following,
we consider several different models.
In the simpler models called precursor-deliberation
models, we assume that the agent has one opportu­
nity to generate a policy and that, having generated
a policy, the agent must use that policy thereafter.
Precursor-deliberation models include
1. a deadline is given in advance, specifying when
to stop deliberating and start acting according to
the generated policy
2. the agent is given an unlimited amount of time to
respond, with a _linear cost of delay
There are also more complicated precursor­
deliberation models, which we do not address in this
paper, such as the following two models, in which a
trigger event occurs, indicating that the agent must
begin following its policy immediately with no further
refinement.
3. the trigger event can occur at any time in a fixed
interval with a uniform distribution
4. the trigger event is governed by a more compli­
cated distribution, e.g., a normal distribution cen­
tered on an expected time
In more complicated models, called recurrent­
deliberation models, we assume that the agent period­
ically replans. Recurrent-deliberation models include
1. the agent performs further envelope alteration
and policy generation if and only if it 'falls out'
of the envelope _defined by the current restricted
automaton
2. the agent performs further envelope alteration
and policy generation periodically, tailoring the
restricted automaton and its corresponding pol­
icy to states expected to occur in the near future
The rest of this paper assumes some familiarity
with basic methods for sequential decision making in
stochastic domains. A companion paper [Dean et
al., 1993] provides additional details regarding algo­
rithms for precursor-deliberation models. In this pa­
per, we dispense with the mathematical preliminaries,
and concentrate on conveying basic ideas and empir­
ical results. A complete description of our approach
including relevant background material is available in
a forthcoming technical report.
2

Deliberation Scheduling

In the previous section, we sketched an algorithm that
generates policies. Each policy 1r has some value with

Deliberation Scheduling for Time-Critical Sequential Decision Making

311

respect to an initial state x0; this value is denoted
V,.(x0) and corresponds to the expected cumulative
reward that results from executing the policy starting
in x0• Given a stochastic process and reward function,
V,.(x0) is well defined for any policy 1r and state x0.
We are assuming that, in time critical applications,
it is impractical to compute V,. (x0) for a given policy
and initial state and, more importantly, that it is im­
practical to compute the optimal policy for the entire
system automaton.
In order to control complexity, in generating a pol­
icy, our algorithm considers only a subset of the state
space of the stochastic process. The algorithm starts
with an initial policy and a restricted state space (or
envelope), extends that envelope, and then computes
a new policy. We would like it to be the case that the
new policy 1r1 is an improvement over (or at the very
least no worse than ) the old policy 1r in the sense that
V1r' (xo)- V ,.(xo) 2: 0 .
In general, however, we cannot guarantee that the pol­
icy will improve without extending the state space to
be the entire space of the system automaton, which
results in computational problems. The best that we
can hope for is that the algorithm improves in expecta­
tion. Suppose that the initial envelope is just the ini­
tial state and the initial policy is determined entirely
by the reflexes. The difference Vrr'(xo)- V1r(xo) is a
random variable, where 1r is the reflex policy and 1r' is
the computed policy. We would like it to be the case
that E[V1r'(x0)- V1r(x0)] > 0, where the expectation
is taken over start states and goals drawn from some
fixed distribution. Although it is possible to construct
system automata for which even this improvement in
expectation is impossible, we believe most moderately
benign navigational environments, for instance, are
well-behaved in this respect.
Our algorithm computes its own estimate of the value
of policies by using a smaller and computationally
more tractable stochastic process. Ideally, we wo'uld
like to show that there is a strong correllation be­
tween the estimate that our algorithm uses and the
value of the policy as defined above with respect to
the complete stochastic process, but for the time be­
ing we show empirically that our algorithm provides
policies whose values increase over time.
Our basic algorithm consists of two stages: envelope
alteration (EA) followed by policy generation (PG).
The algorithm takes as input an envelope and a policy
and generates as output a new envelope and policy.
We also assume that the algorithm has access to the
state transition matrix for the stochastic process. In
general, we assume that the algorithm is applied in
the manner of iterative refinement, with more than
one invocation of the algorithm. We will also treat en­
velope alteration and policy generation as separate, so
we east the overall process of poliey formation in terms
of some number of rounds of envelope alteration fol­
lowed by poliey generation, resulting in a sequenee of

Figure 2: Sequenee of restrieted automata and associ­
ated paths through state space
polieies. Figure 2 depicts a sequenee of automata gen­
erated by iterative refinement along with the associ­
ated paths through state spaee traversed in extending
the envelope.
Envelope alteration can be further classified in terms
of three basic operations on the envelope: trajectory
planning, envelope extension, and envelope pruning.
Trajectory planning eonsists of searching for some path
from an initial state to a state satisfying the goal. En­
velope extension consists of adding states to the enve­
lope. Envelope pruning involves removing states from
the envelope and is generally used only in recurrent­
deliberation models.
Let

1r;

represent the policy after the ith round and let

tEA; be the time spent in the ith round of envelope
alteration. We say that poliey generation is inflexi­
ble if the ith round of poliey generation is always run
to completion on IEil . Policy generation is itself an

iterative algorithm that improves an initial policy by
estimating the value of policies with respect to the re­
stricted stochastic. process mentioned earlier. W hen
run to eompletion, policy generation continues to iter­
ate until it finds a policy that it cannot improve with
respect to its estimate of value. The time spent on the
ith round of policy generation tpa, depends on the
size of the state space IEil .
In the following, we present a number of deeision mod­
els. Note that for each instance of the problems that
we eonsider, there is a large number of possible deci­
sion models. Our seleetion of which decision models to
investigate is guided by our interest in providing some
insight into the problems of time-critical deeision mak­
ing and our antieipation of the combinatorial problems
involved in deliberation scheduling.
3

Precursor Deliberation

In this section we consider the first precursor­
deliberation model, in which there is a fixed deadline
known in advance. It is straightforward to extend this
to model 2, where the agent is given an unlimited re­
sponse time with a Linear eost of delay; models :3 and
4 are more eomplicated and and are not eonsidered in
this paper.

312

3.1

Dean et al.

The Model

Let troT be the total amount of time from the current
time until the deadline. If there are k rounds of enve­
lope alteration and policy generation, then we have
tEA1 + tpa, +···+ tEAk+ tpak =trOT·
Case 1: Single round; inflexible policy genera­
tion In the simplest case, policy generation does not

inform envelope alteration and so we might as well do
all of the envelope alteration before policy generation,
and tEA, + tpa, = troT· Here is what we need in
order to schedule time for EA1 and PG1:
1. the expected value, taken over randomly-chosen
pairs of initial states and goals, of the improve­
ment of the value of the initial state, given a fixed
amount of time allocated to envelope alteration,
E[V1r, (xo)- V1ro(xo)itEA.Ji
2. the expected size of the envelope given the time
allocated to the first round of envelope alteration,
E[IE1IItEA,]; and
3. the expected time required for policy generation,
given the size of the envelope after the first round
of envelope alteration, E [tpa,IIE1I].
Note that, because policy generation is itself an
iterative refinement algorithm, we can interrupt
it at any point and obtain a policy, for instance,
when policy generation takes longer than pre­
dicted by the above expectation.
Each of (1), (2) and (3) can be determined empm­
cally, and, at least in principle, the optimal allocation
to envelope alteration and policy generation can be
determined.
Case II: Multiple rounds; inflexible policy gen­
eration Assume that policy generation can prof­

itably inform envelope alteration, i.e., the policy after
round i provides guidance in extending the environ­
ment during round i +1. In this case, we also have k
rounds and tEA, +tpa, +···+tEAk+ tpak =troT·

Informally, let the fringe states for a given envelope
and policy correspond to those states outside the enve­
lope- that can be reached with some probability greater
than zero in a single step by following the policy start­
ing from some state within the envelope. Let the most
likely falling-out state with respect to a given envelope
and policy correspond to that fringe state that is most
likely to be reached by following the policy starting
in the initial state. We might consider a very simple
method of envelope alteration in which we just add the
most likely falling-out state and then the next most
likely and so on. Suppose that adding each additional
state takes a fixed amount of time. Let

E[V1r; (xo)- V1r;_, (xo)IIE;-11 = m, IE;I = m + n]
denote the expected improvement after the ith round
of envelope alteration and policy generation given that

there are n states added to the m states already in the
envelope after round i - 1.
Again, the expectations described above can be ob­
tained empirically. Coupled with the sort of expecta­
tions described for Case I (e.g., E [tPa;IIE;I] ) , one
could (in principle) determine the optimal number
of rounds k and the allocation to tEA; and tpa; for
1 � j � k . In practice, we use slightly different statis­
tics and heuristic methods for deliberation scheduling
to avoid the combinq.torics.
Case III: Single round: flexible policy genera­
tion Actually, this case is simpler in concept than

Case I, assuming that we can compile the following
statistics.
Case IV: Multiple round: flexible policy gener­
ation Again, with ;tdditional statistics, e.g.,

E[V1r;(xo)-V1r;_, (xo)IIE;-11 = m, IE;I = m+n, tpa;_.],
this case is not much more difficult than Case II.
3.2

Algorithms and Experimental Results

Our initial.experiments are based on stochastic au­
tomata with up to several thousand states; automata
were chosen to be small enough that we can still
compute the optimal policy using exact techniques
for comparison, but large enough to exercise our ap­
proach. The domain, mobile-robot path planning, was
chosen so that it would be easy to understand the poli­
cies generated by our algorithms. For the experiments
reported here, there were 166 locations that the robot
might find itself in and four possible orientations re­
sulting in 664 states. These locations are arranged on
a grid representing the layout of the fourth floor of the
Brown University Computer Science department. The
robot is given a tasK to navigate from some starting
location to some target location. The robot has five ac­
tions: stay, go forward, turn right, turn left, and turn
about. The stay action succeeds with probability one,
the other actions succeed with probability 0.8, except
in the case of sinks corresponding to locations that
are difficult or impossible to get out of. In the mobile­
robot domain, a sink might correspond to a stairwell
that the robot could fall into. The reward function
for the sequential des_:ision problem associated with a
given initial and target location assigns 0 to the four
states corresponding to the target location and -1 to
all other states.
We gathered a variety of statistics on how extend­
ing the envelope increases value. The statistics that
proved most useful corresponded to the expected im­
provement in value for different numbers of states
added t"o the envelope. Instead of conditioning just on
the size of the envelope prior to alteration we found it
necessary to condition on both the size of the envelope
and the estimated value of the current policy (i.e., the

Deliberation Scheduling for Time-Critical Sequential Decision Making

value of the optimal policy computed by policy itera­
tion on the restricted automaton). At run time, we use
the size of the automaton and the estimated value of
the current policy to index into a table of performance
profiles giving expected improvement as a function of
number of states added to the envelope. Figure 3 de­
picts some representative functions for different ranges
of the value of the current policy.

313

Value
10.00
s.oo
6.00

-+--r
--·--=s=·-:t""'"'--+-...,---+---r - hCXi6ie-----·
=--··
-+--+---n------t----1---t---1
---t
..

.J
'

-+-1--i+----+----l--t-- -----t--

r'
-+----t----'j-+-,1---++---+1
2.00 -+-1---i-+---+--+----t----'j­
..�
/
0,00 -fl--L--1----+--'--t------t--

4.00

0.00

2.00

4.00

6.00

8.00

Time (secoo.ds}

Figure 4: Comparison of the greedy algorithm with
standard (inflexible) policy iteration and interruptable
(flexible) policy iteration

10.00

20.00

30.00

40.00

Figure 3: Expected improvement as a function of the
number of states n added to initial envelope of size m
In general, computing the optimal deliberation sched­
ule for the multiple-round precursor-deliberation mod­
els described above is computationally complex. We
have experimented with a number of simple, greedy
and myopic scheduling strategies; we report on one
such strategy here.
Using the mobile-robot domain, we generated 380,000
data points to compute statistics of the sort shown in
Figure 3 plus estimates of the time required for one
round of envelope alteration followed by policy gen­
eration given the size of the envelope, the number of
states added, and value of the current policy. We use
the following simple greedy strategy for choosing the
number of states to add to the envelope on each round.
For each round of envelope alteration followed by pol­
icy generation, we use the statistics to determine the
number of states which, added to the envelope, max­
imizes the ratio of performance improvement to the
time required for computation. Figure 4 compares the
greedy algorithm with the standard (inflexible) pol­
icy iteration on the complete automaton and with an
interruptable (flexible) version of policy iteration on
the complete automaton. The data for Figure 4 was
determined from one representative run of the three
algorithms on a particular initial state and goal. In
another paper [ Dean et al., 1993] we present results
for the average improvement of the start state under
the policy available at time t as a function of time.
4
4.1

Recurrent Deliberation
The Model

In recurrent-deliberation models, the agent has to re­
peatedly decide how to allocate time to deliberation,
taking into account new information obtained during
execution. In this section, we consider a particular

model for recurrent deliberation in which the agent al­
locates time to deliberation only at prescribed times.
We assume that the agent has separate deliberation
and execution modules that run in parallel and com­
municate by message passing; the deliberation module
sends policies to the execution module and the execu­
tion module sends observed states to the deliberation
module. We also assume that the agent correctly iden­
tifies its current state; in the extended version of this
paper, we consider the case in which there is uncer­
tainty in observation.
We call the model considered in this section the dis­
crete, weakly-coupled, recurrent deliberation model. It
is discrete because each tick of the clock corresponds to
exactly one state transition; recurrent because the exe­
cution module gets a new policy from the deliberation
module periodically; weakly coupled in that the two
modules communicate by having the execution mod­
ule send the deliberation module the current state and
the deliberation module send the execution module the
latest policy. In this section, we consider the case in
which communication between the two modules occurs
exactly once every n ticks; at times n, 2n, 3n, . . ., the
deliberation module sends off the policy generated in
the last n ticks, recei�es the current state from the ex­
ecution module, and begins deliberating on the next
policy. In the next section, we present an algorithm for
the case where the interval between communications is
allowed to vary.
In the recurrent models, it is often necessary to remove
states from the envelope in order to lower the compu­
tational costs of generating policies from the restricted
automata. For instance, in the mobile-robot domain,
it may be appropriafe to remove states corresponding
to portions of a path the robot has already traversed
if there is little chance of returning to those states. In
general, there are many more possible strategies for
deploying envelope alteration and policy generation in
recurrent models than in the case of precursor mod­
els. Figure 5 shows a typical sequence of changes to
the envelope corresponding to the state space for the
restricted automaton. The current state is indicated

314

Dean et al.

Find path to the goal

�
Extend the envelope

Extend and then prune the envelope

�·
0

Find path back to the

interval, the execution module is given a new policy 11"1,
and the deliberation module is given the current state
x'. It is possible that x' is not included in the enve­
lope for 11"1; if the reflexes do not drive the robot inside
the envelope then the agent's behavior throughout the
next n-tick interval will be determined entirely by the
reflexes. Figure 6 shows a possible run depicting inter­
vals in which the system is executing reflexively and
intervals in which it is using the c.urrent policy; for this
example, we assume_reflexes that enable an agent to
remain in the same state indefinitely.

Let 8n (x,1r, x') be the probability of ending up in x'
starting from x and following 1r for n steps. Suppose
that we are given a set of strategies {F1,F2, }. As
Extend and then prune the envelope
is usual in such combinatorial problems with indefi­
nite horizons, we adopt a myopic decision model. In
particular, we assume that, at the beginning of each
n-tick interval, we are planning to follow the current
policy 1r for n steps, .follow the policy F(1r) generated
Figure 5: Typical sequence of changes to the envelope
by some strategy F attempting to improve on 1r for the
next n steps, and thereafter follow the optimal policy
intervals during which the system is executing reflexively
7r*. If we assume that it is impossible to get to a goal
0
3n
4n
n
2n
state in the next 2n steps, the expected value of using
strategy F is given by
falls out�ofthe envelo
2n-l
n
curre nt state happens tt be contaied in the new envelo
Z:-l+i2 L 8n (x,7r,x1) L8n (x',F (7r),x")V.,.. (x") ,
.

�

I

falls out of the envelope again

+

r

ctuTent state is not in the new envelope

'

current state is in the new envelope

Figure 6: Recurrent-deliberation
by + and the goal state is indicated by D.
To cope with the attendant combinatorics, we raise
the level of abstraction and assume that we are given
a small set of strategies that have been determined
empirically to improve policies significantly in vari­
ous circumstances. Each strategy corresponds to some
fixed schedule for allocating processor time to envelope
alteration and policy generation routines. Strategies
would be tuned to a particular n-tick deliberation cy­
cle. One strategy might be to use a particular pruning
algorithm to remove a specified number of states and
then use whatever remains of the n ticks to generate
a new policy. In this regime, deliberation scheduling
consists of choosing which strategy to use at the begin­
ning of each n-tick interval. In this section, we ignore
the time spent in deliberation scheduling; in the next
section, we will arrange it so that the time spent in
deliberation scheduling is negligible.
Before we get into the details of our decision model,
consider some complications that arise in recurrent­
deliberation problems. At any given moment, the
agent is exec.uting a polic.y, call it 1r, defined on the cur­
rent envelope and augmented with a set of reflexes for
states falling outside the envelope. The agent begins
exec.uting 1r in state x. At the end of the c.urrent n-tick

x'EX

i=O

.

•

[

x"EX

l

where 0 <= 1 < 1 i& a discounting factor, controlling
the degree of influence of future results on the current
decision.
Extending the above model to account for the possi­
bility of getting to the goal state in the next 2n steps
is straightforward; computing a good estimate of v.,..
is not, however. We might use the value of some pol­
icy other than 7r*, but then we risk choosing strategies
that are optimized to support a particular suboptimal
policy when in fact. the agent should be able to do
much better. In general, it is difficult to estimate the
value of prospects beyond any given limited horizon
for sequential decision problems of indefinite duration.
In the next section, we consider one possible practical
expedient that appears to have heuristic merit.
4.2

Algorithms and Experimental Results

In this section, we present a method for solving
recurrent-deliberation problems of indefinite duration
using statistical estimates of the value of a variety of
deliberation strategies. We deviate from the decision
model described in the previous sec.tion in one addi­
tional important way; we allow variable-length inter­
vals for deliberation. Although fixed-length facilitate
exposition, it is much easier to collect useful statistical
estimates of the utility of deliberation strategies if the
deliberation interval is allowed to vary.
For the remainder of-this section, a deliberation strat­
egy is just a particular sequence of invocations of enve­
lope alteration and policy generation routines. Delib-

Deliberation Scheduling for Time-Critical Sequential Decision Making

eration strategies are parameterized according to at­
tributes of the policy such as the estimated value of
policies and the size of the envelopes. The function
EIV (F,V11, IE11l) provides an estimate of the expected
improvement from using the strategy F assuming that
the estimated value of the current policy and the size
of the corresponding envelope fall within the speci­
fied ranges. This function is implemented as a table in
which each entry is indexed by a strategy F and a set of
ranges, e.g., { [minV11,maxV,.], [miniE,.I,maxiE111]}.

315

online deliberation scheduling.

We determine the EIV function off line by gathering
statistics for F running on a wide variety of policies.
The ranges are established so that, for values within
the specified ranges the expected improvements have
low variance. At run time, the deliberation scheduler
computes an estimate of the current policy V,., deter­
mines the size IE,.. I of the corresponding envelope and
chooses the strategy F maximizing EIV (F, V,., IE111).

To avoid complicating the online decision making, we
have adopted the following expedient which allows us
to keep our one-step-lookahead model. We modify the
transition probabilities for the restricted automaton so
that there is always a non-zero probability of getting
back into the envelope having fallen out of it. Exactly
what this probability should be is somewhat eompli­
cated. The particular value chosen will determine just
how concerned the agent will be with the prospect of
falling out of the envelope. In fact, the value is depen­
dent on the actual strategies chosen by deliberation
scheduling which, in our particular case, depends on
EIV and this value of falling back in. We might pos­
sibly resolve the circularity by solving a large and very
complicated set of simultaneous equations; instead, we
have found that in practice it is not difficult to find a
value that works reasonably well.

To build a table of estimates of function EIV off line,
we begin by gathering data on the performance of
strategies ranging over possible initial states, goals,
and policies. For a particular strategy F , initial state
x, and policy 1r, we run F on 1r, determine the elapsed
number of steps k, and compute estimated improve­
ment in value,

The experimental results for the recurrent model were
obtained on the mobile-robot domain with 1422 possi­
ble locations and hence 5688 states. The actions avail­
able to the agent were the same as those used to obtain
the precursor-model-results. The transition probabil­
ities were also the same, except that the domain no
longer contained sinks.

[-�

'Yi

+ 'Yk

x� 8k(x,

71",

]

x')VF(1r)(x') - V,.(x),

where the first term corresponds to the value of using
1r for the first k steps and F (1r) there after and the
second term corresponds to the case in which we do
no deliberation whatsoever and use 1r forever. As in
the model described in the previous section, we assume
that the goal cannot be reached in the next k steps;
again it is simple to extend the analysis to the case in
which the goal may be reached in less than k steps.
Given data of the sort described above, we build the
table for EIV (F,V,., IE,..I ) by appropriately dividing
the data into subsets with low variance.
One unresolved problem with this approach is exactly
how we are to compute V11 ( x). Recall that 1r is only
a partial policy defined on a subset of X augmented
with a set of reflexes to handle states outside the cur­
rent envelope. In estimating the value of a policy, we
are really interested in estimating the value of the aug­
mented partial policy. If the reflexes kept the agent in
the same place indefinitely, then as long as there was
some nonzero probability of falling out of the envelope
with a given policy starting in a given state the actual
value of the policy in that state would be 1/ ( 1 1).
Of course, this is an extremely pessimistic estimate for
the long term value of a particular policy since in the
recurrent model the agent will periodically compute a
new policy based on where it is in the state space. The
problem is that we cannot directly account for these
subsequent policies without extending the horizon of
the myopic decision model and absorbing the associ­
ated computational costs in offline data gathering and
-

-

We used a set of 24 hand-crafted strategies, which were
combinations of envelope optimization (a) and the
following types of envelope alteration;
1. findpath (FP): if the agent's current state X cur
is not in the envelope, find a path from Xcur to a
goal state, and add this path to the envelope
2. robustify (R[N]): we used the following heuris­
tic to extend the envelope: find the N most l�kely
fringe states that the agent would fall out of the
envelope into, and add them to the envelope
3. prune (P[N]): of the states that have a worse
value than the current state, remove the N least
likely to be reached using the current policy.
Each of the strategies used began with findpath and
ended with optimization. Between the first and last
phases, robustification, pruning and optimization were
used in different combinations, with the number of
states to be added or deleted E {10, 20, 50, 100}; exam­
ples of the strategies we used are {FP R[10] a}, {FP
P[20] a}, {FP P[20] R[50] o}, {FP R[100] P[50]
0}, {FP R[50] a P[50] 0}.
We collected statistics over about 4000 runs generat­
ing 100,000 data points for strategy execution. The
start/goal pairs were, chosen uniformly at random and
we ran the simulated robot in parallel with the plan­
ner until the goal was reached. The planner executed
the following loop: choose one of the 24 strategies uni­
formly .at random, execute that strategy, and then pass
the new policy to the simulated robot. We found the
following conditioning variables to be significant: the
envelope size, lEI, the value of the current state V,.,
the "fatness" of the envelope (the ratio of envelope

316

Dean et al.

size to fringe size), and the Manhattan distance, M,
between the start and goal locations. We then build
a lookup table of expected improvement in value over
the time the strategy takes to compute, 8V11' / k, as a
function of E, V11', the fatness, M and the strategy s.
To test our algorithm, we took 25 pairs of start and
goal states, chosen uniformly at random from pairs of
Manhattan distance less than one third of the diameter
of the world. For each pair we ran the simulated robot
in parallel with the following deliberation mechanisms:
• recurrent-deliberation with strategies chosen us­
ing statistical estimates of EIV (LOOKUP)
• dynamic programming policy iteration over the
entire domain, with a new policy given to the
robot after each iteration (ITER) and only after
it has been optimized ( wHOLE )
The average number of steps taken by LOOKUP,
and WHOLE were 71, 87 and 246 respectively

ITER

W hile the improvement obtained using the recurrent­
deliberation algorithm is only small it is statistically
significant. These preliminary results were obtained
when there were still bugs in the implementation, how­
ever, since we have determined that the strategies are
in fact being pessimistic, we expect to obtain further
performance improvement using LOOKUP. Recall also
that we are still working in the comparatively small
domain necessary to be able to compute the optimal
policy over the whole domain; for larger domains, ITER
and WHOLE are computationally infeasible.
5

Related Work and Conclusions

Our primary interest is in applying the sequential de­
cision making techniques of Bellman [Bellman, 1957]
and Howard [Howard, 1960] in time-critical applica­
tions. Our initial motivation for this research arose
in attempting to put the anytime synthetic projec' ­
tion work of Drummond and Bresina [Drummond and
Bresina, 1990] on more secure theoretical foundations.
The approach described in this paper represents a
particular instance of time-dependent planning [Dean
and Boddy, 1988] and borrows from, among others,
Horvitz' [Horvitz, 1988] approach to flexible compu�
tation. Hansson and Mayer's BPS (Bayesian Problem
Solver) [Hansson and Mayer, 1989] supports general
state space search with decision theoretic control of in­
ference; it may be that BPS could be used as the basis
for envelope alteration. Boddy [Boddy, 1991] describes
solutions to related problems involving dynamic pro­
gramming. For an overview of resource-bounded de­
cision making methods, see chapter 8 of the text by
Dean and Wellman [Dean and Wellman, 1991].
We have presented an approach to coping with un­
certainty and time pressure in decision making. The
approach lends itself to a variety of online computa­
tional strategies, a few of which are described in this
paper. Our algorithms exploit both the goal-directed,

state-space search methods of artificial intelligence and
the dynamic programming, stochastic decision making
methods of operations research. Our empirical results
demonstrate that it is possible to obtain high perfor­
mance policies for large stochastic processes in a man­
ner suitable for time critical decision making.
Acknowledgements

Tom Dean's work was supported in part by a Na­
tional Science Foundation Presidential Young Investi­
gator Award IRI-8957601, by the Advanced Research
Projects Agency of the DoD monitored by the Air
Force under Contract No. F30602-91-C-0041, and by
the National Science foundation in conjunction with
the Advanced Research Projects Agency of the DoD
under Contract No. IRI-8905436. Leslie Kaelbling's
work was supported· in part by a National Science
Foundation National Young Investigator Award IRI9257592 and in part by ONR Contract N00014-914052, ARPA Order 8225. Thanks also to Moises Lejter
for his input during the development and implementa­
tion of the recurrent deliberation model.

