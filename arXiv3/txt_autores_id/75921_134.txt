

eas include distributed robot control, networking and
e-commerce.

We present a memory-bounded optimization
approach for solving infinite-horizon decentralized POMDPs. Policies for each agent
are represented by stochastic finite state controllers. We formulate the problem of optimizing these policies as a nonlinear program,
leveraging powerful existing nonlinear optimization techniques for solving the problem.
While existing solvers only guarantee locally
optimal solutions, we show that our formulation produces higher quality controllers than
the state-of-the-art approach. We also incorporate a shared source of randomness in
the form of a correlation device to further
increase solution quality with only a limited
increase in space and time. Our experimental results show that nonlinear optimization
can be used to provide high quality, concise
solutions to decentralized decision problems
under uncertainty.

Although there has been some recent work on exact
and approximate algorithms for DEC-POMDPs (Nair
et al., 2003; Bernstein et al., 2005; Hansen et al., 2004;
Szer et al., 2005; Szer and Charpillet, 2005; Seuken
and Zilberstein, 2007), only two algorithms (Bernstein et al., 2005; Szer and Charpillet, 2005) are able
to find solutions for the infinite-horizon case. Such
domains as networking and robot control problems,
where the agents are in continuous use are more appropriately modeled as infinite-horizon problems. Exact
algorithms require an intractable amount of space for
all but the smallest problems. This may occur even if
an optimal or near-optimal solution is concise. DECPOMDP approximation algorithms can operate with a
limited amount of memory, but as a consequence may
provide poor results.

Introduction

Markov decision processes (MDPs) have been widely
used to study single agent sequential decision making
with full observability. Partially observable Markov
decision processes (POMDPs) have had success modeling the more general situation in which the agent has
only partial information about the state of the system. The decentralized partially observable Markov
decision processes (DEC-POMDP) is an even more
general framework which extends the POMDP model
to mutiagent settings. In a DEC-POMDP each agent
must make decisions based on uncertainty about the
other agents as well as imperfect information of the
system state. The agents seek to maximize a shared
total reward using solely local information in order to
act. Some examples of DEC-POMDP application ar-

In this paper, we propose a new approach that addresses the space requirement of DEC-POMDP algorithms while maintaining a principled method based
on the optimal solution. This approach formulates
the optimal memory-bounded solution for the DECPOMDP as a nonlinear program (NLP), thus allowing a wide range of powerful nonlinear optimization
techniques to be applied. This is done by optimizing
the parameters of fixed-size independent controllers for
each agent, which when combined, produce the policy
for the DEC-POMDP. While no existing NLP solver
guarantees finding an optimal solution, our new formulation facilitates a more efficient search of the solution
space and produces high quality controllers of a given
size.
We also discuss the benefits of adding a shared source
of randomness to increase the solution quality of our
memory-bounded approach. This allows a set of independent controllers to be correlated in order to produce higher values, without sharing any local information. Correlation adds another mechanism in efforts
to gain the most possible value with a fixed amount of

2

AMATO ET AL.

space. This has been shown to be useful in order to
increase value of fixed-size controllers (Bernstein et al.,
2005) and we show that is also useful when combined
with our NLP approach.
The rest of the paper is organized as follows. We first
present some background on the DEC-POMDP model
and the finite state controller representation of their
solution. We then describe the current infinite-horizon
algorithms and describe some of their flaws. As an
alternative, we present a nonlinear program that represents the optimal fixed-size solution. We also incorporate correlation into the NLP method and discuss
its benefits. Lastly, experimental results are provided
comparing the nonlinear optimization methods with
and without correlation and the current state-of-theart DEC-POMDP approximation algorithm. This is
done by using an off-the-shelf, locally optimal nonlinear optimization method to solve the NLPs, but
more sophisticated methods are also possible. For a
range of domains and controller sizes, higher-valued
controllers are found with the NLP and using correlation further increases solution quality. This suggests
that high quality, concise controllers can be found in
many diverse DEC-POMDP domains.

2

DEC-POMDP model and solutions

We first review the decentralized partially observable
Markov decision process (DEC-POMDP) model. For
clarity, we present the model for two agents as it is
straightforward to extend it to n agents.
A two agent DEC-POMDP can be defined with the
tuple: M = hS, A1 , A2 , P, R, Ω1 , Ω2 , O, T i
• S, the finite set of states
• A1 and A2 , the finite sets of actions for each agent
• P , the set of state transition probabilities:
P (s0 |s, a1 , a2 ), the probability of transitioning
from state s to s0 when actions a1 and a2 are
taken by agents 1 and 2 respectively
• R, the reward function: R(s, a1 , a2 ), the immediate reward for being in state s and agent 1 taking
action a1 and agent 2 taking action a2
• Ω1 and Ω2 , the finite sets of observations for each
agent
• O, the set of observation probabilities:
O(o1 , o2 |s0 , a1 , a2 ), the probability of agents
1 and 2 seeing observations o1 and o2 respectively
given agent 1 has taken action a1 and agent 2
has taken action a2 and this results in state s0
Since we are considering the infinite-horizon DECPOMDP, the decision making process unfolds over an
infinite sequence of stages. At each step, every agent

chooses an action based on their local observation histories, resulting in an immediate reward and an observation for each agent. Note that because the state
is not directly observed, it may be beneficial for the
agent to remember the observation history. A local
policy for an agent is a mapping from local observation histories to actions while a joint policy is a set of
policies, one for each agent in the problem. The goal is
to maximize the infinite-horizon total cumulative reward, beginning at some initial distribution over states
called a belief state. In order to maintain a finite sum
over the infinite-horizon, we employ a discount factor,
0 ≤ γ < 1.
As a way to model DEC-POMDP policies with finite
memory, finite state controllers provide an appealing
solution. Each agent’s policy can be represented as a
local controller and the resulting set of controllers supply the joint policy, called the joint controller. Each
finite state controller can formally be defined by the
tuple hQ, ψ, ηi, where Q is the finite set of controller
nodes, ψ : Q → ∆A is the action selection model for
each node, and η : Q × A × O → ∆Q represents the
node transition model for each node given an action
was taken and an observation seen. For n agents, the
value for starting in agent 1’s nodes ~q and at state s
is given by

n
XY
X
P (s0 |~a, s)·
V (~q, s) =
P (ai |qi ) R(s, ~a) + γ
0
i
s
~
a

n
X
XY
0
0
0
~
O(~o|s , ~a)
P (qi |qi , ai , oi )V (q , s )
~
o

q~0

i

This is also referred to as the Bellman equation. Note
that the values can be calculated offline in order to
determine controllers for each agent that can then be
executed online for distributed control.

3

Previous work

As mentioned above, the only other algorithms that
we know of that can solve infinite-horizon DECPOMDPs are those of Bernstein et al. (2005) and
Szer and Charpillet (2005). Bernstein et al.’s approach, called bounded policy iteration for decentralized POMDPs (DEC-BPI), is an approximate algorithm that also uses stochastic finite state controllers.
Szer and Charpillet’s approach is also an approximate
algorithm, but it uses deterministic controllers.
DEC-BPI improves a set of fixed-size controllers by
using linear programming. This is done by iterating
through the nodes of each agent’s controller and attempting to find an improvement. A linear program
searches for a probability distribution over actions and
transitions into the agent’s current controller that increases the value of the controller for any initial state

AMATO ET AL.
For variables: x(~q, ~a), y(~q, ~a, ~o, q~0 ) and z(~q, s)
Maximize
X

3

b0 (s)z(q~0 , s)

s

Given the Bellman constraints:
∀~q, s z(~q, s) =

X



X
X
X
0
0
0
0
0
~
~
y(~q, ~a, ~o, q )z(q , s )
O(~o|s , ~a)
P (s |s, ~a)
x(~q, ~a) R(s, ~a) + γ
s0

~
a

~
o

q~0

For each agent i and set of agents, −i, apart from i,
Independence constraints:
∀ai , ~q

X

X

x(~q, ~a) =

a−i

∀~a, ~q, ~o, qi0

X

y(~q, ~a, ~o, q~0 ) =

0
q−i

f
, ~a)
x(qi , q−i

a−i

X

f
y(qi , q−i
, ai , af−i , oi , of−i , q~0 )

0
q−i

And probability constraints:
X
X
f
f
∀qi
x(qi , q−i
, ~a) = 1 and ∀qi , oi , ai
y(qi , q−i
, ai , ac−i , oi , of−i , q~0 ) = 1
~
a

q~0

∀~q, ~a x(~q, ~a) ≥ 0 and ∀~q, ~o, ~a y(~q, ~a, ~o, q~0 ) ≥ 0
Table 1: The nonlinear program representing the optimal fixed-size controller. Variable x(~q, ~a) represents P (~a|~q),
variable y(~q, ~a, ~o, q~0 ) represents P (q~0 |~q, ~a, ~o), variable z(~q, s) represents V (~q, s), q~0 represents the initial controller
f
node for each agent. Superscripted f ’s such as q−i
represent arbitrary fixed values.
and any initial node of the other agents’ controllers.
If the improvement is discovered, the node is updated
based on the probability distributions found. Each
node for each agent is examined in turn and the algorithm terminates when no agent can make any further
improvements.
This algorithm allows memory to remain fixed, but
provides only a locally optimal solution. This is due to
the linear program considering the old controller values
from the second step on and the fact that improvement
must be over all possible states and initial nodes for
the controllers of the other agents. As the number of
agents or size of controllers grows, this later drawback
is likely to severely hinder improvement.
Szer and Charpillet have developed a best-first search
algorithm that finds deterministic finite state controllers of a fixed size. This is done by calculating a
heuristic for the controller given the known deterministic parameters and filling in the remaining parameters
one at a time in a best-first fashion. They prove that
this technique will find the optimal deterministic finite
state controller of a given size, but its use remains limited. This approach is very time and memory intensive
and is restricted to deterministic controllers.

4

Nonlinear optimization approach

Due to the high space complexity of finding an optimal solution for a DEC-POMDP, fixed-size solutions
are very appealing. Fixing memory balances optimality and computational concerns and should allow high
quality solutions to be found for many problems. Using Bernstein et al.’s DEC-BPI method reduces problem complexity by fixing controller size, but solution
quality is limited by a linear program that requires
improvement across all states and initial nodes of the
other agents. Also, each agent’s controller is improved
separately without consideration for the knowledge of
the initial problem state, thus reducing solution quality. Both of these limitations can be eliminated by
modeling a set of optimal controllers as a nonlinear
program. By setting the value as a variable and using
constraints to maintain validity, the parameters can be
updated in order to represent the globally optimal solution over the infinite-horizon of the problem. Rather
than the the iterative process of DEC-BPI, the NLP
improves and evaluates the controllers of all agents at
once for a given initial state in order to make the best
possible use of the controller size.
Compared with other DEC-POMDP algorithms, the
NLP approach makes more efficient use of memory

4

AMATO ET AL.

than the exact methods, and using locally optimal
NLP algorithms provides an approximation technique
with a search based on the optimal solution of the
problem. Rather than adding nodes and then attempting to remove those that will not improve the controller, as a dynamic programming approach might do,
we search for the best controllers of a fixed size. The
NLP is also able to take advantage of the start distribution, thus making better use of its size.
The NLP approach has already shown promise in
the POMDP case. In a previous paper (Amato et
al., 2007), we have modeled the optimal fixed-size
controller for a given POMDP as an NLP and with
locally optimal solution techniques produced consistently higher quality controllers than a current stateof-the-art method. The success of the NLP in the single agent case suggested that an extension to DECPOMDPs could also be successful. To construct this
NLP, extra constraints are needed to guarantee independent controllers for each agent, while still maximizing the value.
4.1

Nonlinear problem model

The nonlinear program seeks to optimize the value
of fixed-size controllers given a initial state distribution and the DEC-POMDP model. The parameters
of this problem in vector notation are the joint action selection probabilities at each node of the controllers P (~a|~q), the joint node transition probabilities
P (q~0 |~q, ~a, ~o) and the values of each node in each state,
V (~q, s). This approach differs from Bernstein et al.’s
approach in that it explicitly represents the node values as variables. To ensure that the values are correct
given the action and node transition probabilities, nonlinear constraints must be added to the optimization.
These constraints are the Bellman equations given the
policy determined by the action and transition probabilities. Constraints are also added to ensure distributed action selection and node transitions for each
agent. We must also ensure that all probabilities are
valid numbers between 0 and 1.
Table 1 describes the nonlinear program that defines
the optimal controller for an arbitrary number of
agents. The value of designated initial local nodes is
maximized given the initial state distribution and the
necessary constraints. The independence constraints
guarantee that action selection and transition probabilities can be summed out for each agent by ensuring
that they do not depend on any information that is
not local.
Theorem 1 An optimal solution of the NLP results
in optimal stochastic controllers for the given size and
initial state distribution.

Proof sketch. The optimality of the controllers follows from the NLP constraints and maximization of
given initial nodes at the initial state distribution.
The Bellman equation constraints restrict the value
variables to valid amounts based on the chosen probabilities, the independence constraints guarantee distributed control and the maximum value is found for
the initial nodes and state. Hence, this represents optimal controllers.
4.2

Nonlinear solution techniques

There are many efficient algorithms for solving large
NLPs. When the problem is non-convex, as in our
case, there are multiple local maxima and no NLP
solver guarantees finding the optimal solution. Nevertheless, existing techniques proved useful in finding
high-quality results for large problems.
For this paper, we used a freely available nonlinearly
constrained optimization solver called filter (Fletcher
et al., 2002) on the NEOS server (http://wwwneos.mcs.anl.gov/neos/). Filter finds solutions by a
method of successive approximations called sequential
quadratic programming (SQP). SQP uses quadratic
approximations which are then more efficiently solved
with quadratic programming (QP) until a solution to
the more general problem is found. A QP is typically
easier to solve, but must have a quadratic objective
function and linear constraints. Filter adds a “filter”
which tests the current objective and constraint violations against those of previous steps in order to
promote convergence and avoid certain locally optimal
solutions. The DEC-POMDP and nonlinear optimization models were described using a standard optimization language AMPL.

5

Incorporating correlation

Bernstein et al. also allow each agent’s controller to be
correlated by using a shared source of randomness in
the form of a correlation device. As an example of one
such device, imagine that before each action is taken, a
coin is flipped and both agents have access to the outcome. Each agent can then use that new information
to affect their choice of action. Along with stochasticity, correlation is another means of increasing value
when memory is limited.
A correlation device provides extra signals to the
agents and operates independently of the DECPOMDP. That is, the correlation device is a tuple
hC, ψi, where C is a set of states and ψ : C → ∆C
is a stochastic transition function that we will represent as P (c0 |c). At each step of the problem, the device
transitions and each agent can observe its state.

AMATO ET AL.

5

For variables: w(c, c0 ), x(~q, ~a, c), y(~q, ~a, ~o, q~0 , c) and z(~q, s, c)
Maximize
X
b0 (s)z(q~0 , s)
s

Given the Bellman constraints:


X
X
X
X
X
0
0
0
0
0
0
~
~
w(c, c )z(q , s , c)
y(~q, ~a, ~o, q , c)
O(~o|s , ~a)
P (s |s, ~a)
x(~q, ~a, c) R(s, ~a) + γ
∀~q, s z(~q, s, c) =
~
a

s0

~
o

q~0

c0

Table 2: The nonlinear program representing the optimal fixed-size controller including a correlation device.
Variable x(~q, ~a, c) represents P (~a|~q, c), variable y(~q, ~a, ~o, q~0 , c) represents P (q~0 |~q, ~a, ~o, c), variable z(~q, s, c) represents V (~q, s, c), q~0 represents the initial controller node for each agent and w(c, c0 ) represents P (c0 |c). The other
constraints are similar to those above with the addition of a sum to one constraint for the correlation device.
size
DEC-BPI DEC-BPI corr NLO NLO-corr
The independent local controllers defined above can be
modified to make use of the correlation device. This
1
4.687
6.290
9.1
9.1
is done by making the parameters dependent on the
2
4.068
7.749
9.1
9.1
signal from the correlation device. For agent i, ac3
8.637
7.781
9.1
9.1
tion selection is then P (ai |qi , c) and node transition is
4
7.857
8.165
9.1
9.1
P (qi0 |qi , ai , oi , c). For n agents, the value of the correTable 3: Broadcast problem values using NLP methlated joint controller beginning in nodes ~q, state s and
ods and DEC-BPI with and without a 2 node correlacorrelation device state c is defined as V (~q, s, c) =

n
tion device
X
X
XY
P (s0 |~a, s)
O(~o|s0 , ~a)·
P (ai |qi , c) R(s, ~a) + γ
i
s0
~
o
~
a
size
DEC-BPI DEC-BPI corr
NLO
NLO-corr

n
XY
X
0
1
< 1s
< 1s
1s
2s
P (qi |qi , ai , oi , c)
P (c |c)V (q~0 , s0 , c0 )
0
2
<
1s
2s
3s
8s
i
c
0
q~
3
2s
7s
764s
2119s
Our NLP can be extended to include a correlation
4
5s
24s
4061s
10149s
device. This optimization problem, the first part of
which is shown in Table 2, is very similar to the preTable 4: Broadcast problem mean optimization times
vious NLP. A new variable is added for the transiusing NLP methods and DEC-BPI with and without
tion function of the correlation device and the other
a 2 node correlation device
variables now include the signal from the device. The
Bellman equation incorporates the new correlation device signal at each step, but the other constraints reEach NLP and DEC-BPI algorithm was run until conmain the same. A new probability constraint is also
vergence was achieved with ten different random deadded to ensure that the transition probabilities for
terministic initial controllers, and the mean values and
each state of the correlation device sum to one.
times are reported. The times reported for each NLP

6

Experimental results

We tested our nonlinear programming approach in
three DEC-POMDP domains. In each experiment, we
compare Bernstein et al.’s DEC-BPI with NLP solutions using filter for a range of controller sizes. We
also implemented each of these approaches with a correlation device of size two. We do not compare with
Szer and Charpillet’s algorithm because the problems
presented in that work are slightly different than those
used by Bernstein et al. Nevertheless, on the problems
that we tested, our approach can and does achieve
higher values than Szer and Charpillet’s algorithm for
all of the controller sizes for which that the best-first
search is able to find a solution.

method can only be considered estimates due to running each algorithm on external machines with uncontrollable load levels, but we expect that they vary by
only a small constant. Note that our goal in these experiments is to demonstrate the benefits of our formulation when used in conjunction with an “off the shelf”
solver such as filter. The formulation is very general
and many other solvers may be applied. Throughout
this section we will refer to our nonlinear optimization
as NLO and the optimization with the correlation device with two states as NLO-corr.
6.1

Broadcast problem

A DEC-POMDP used by Bernstein et al. was a simplified two agent networking example. This problem

6

AMATO ET AL.

Figure 1: Recycling robots values using NLP methods
and DEC-BPI with and without a 2 node correlation
device
has 4 states, 2 actions and 5 observations. At each
time step, each agent must choose whether or not to
send a message. If both agents send, there is a collision
and neither gets through. A reward of 1 is given for
every step a message is successfully sent over the channel and all other actions receive no reward. Agent 1
has a 0.9 probability of having a message in its queue
on each step and agent 2 has only a 0.1 probability.
The domain is initialized with only agent 1 possessing
a message and a discount factor of 0.9 was used.
Table 3 shows the values produced by DEC-BPI and
our nonlinear programming approach with and without a correlation device for several controller sizes.
Both nonlinear techniques produce the same value, 9.1
for each controller size. In all cases this is a higher
value than that produced by Bernstein et al.’s independent and correlated approaches. As 9.1 is the maximum value that any approach that we tested receives
for the given controller sizes, it is likely that it is optimal for these sizes.
The time used by each algorithm is shown in Table
4. As expected, the nonlinear optimization methods
require more time to find a solution than the DECBPI methods. As noted above, solution quality is also
higher using nonlinear optimization. Either NLP approach can produce a higher valued one node controller
in an amount of time similar to or less than each DECBPI method. Therefore, for this problem, the NLP
methods are able to find higher valued, more concise
solutions given a fixed amount of space or time.
6.2

Recycling robots

As another comparison, we have extended the Recycling Robot problem (Sutton and Barto, 1998) to the
multiagent case. The robots have the task of picking
up cans in an office building. They have sensors to

Figure 2: Recycling robots graphs for value vs time
for the NLP and DEC-BPI methods with and without
the correlation device.
find a can and motors to move around the office in
order to look for cans. The robots are able to control a gripper arm to grasp each can and then place it
in an on-board receptacle. Each robot has three high
level actions: (1) search for a small can, (2) search for
a large can or (3) recharge the battery. In our two
agent version, the larger can is only retrievable if both
robots pick it up at the same time. Each agent can
decide to independently search for a small can or to
attempt to cooperate in order to receive a larger reward. If only one agent chooses to retreive the large
can, no reward is given. For each agent that picks up
a small can, a reward 2 is given and if both agents
cooperate to pick the large can, a reward of 5 is given.
The robots have the same battery states of high and
low, with an increased likelihood of transitioning to a
low state or exhausting the battery after attempting
to pick up the large can. Each robot’s battery power
depends only on its own actions and each agent can
fully observe its own level, but not that of the other
agent. If the robot exhausts the battery, it is picked
up and plugged into the charger and then continues to
act on the next step with a high battery level. The two
robot version used in this paper has 4 states, 3 actions
and 2 observations. A discount factor of 0.9 was used.
We can see in Figure 1 that in this domain higher
quality controllers are produced by using nonlinear optimization. Both NLP methods permit higher mean
values than either DEC-BPI approach for all controller
sizes. Also, correlation is helpful for both the NLP and
DEC-BPI approaches, but becomes less so for larger
controller sizes. For the nonlinear optimization cases,
both approaches converge to within a small amount of
the maximum value that was found for any controller
size tested. As controller size grows, the NLP methods
are able to reliably find this solution and correlation
is no longer useful.

AMATO ET AL.

7

The running times of each algorithm follow the same
trend as above in which the nonlinear optimization
approaches required much more time as controller size
increases. The ability for the NLP techniques to produce smaller, higher valued controllers with similar or
lesser running time also follows the same trend.
Figure 2 shows the values that can be attained for each
method based on the mean time necessary for convergence. Results are included for NLP techniques up to
four nodes with the correlation device and five nodes
without it while DEC-BPI values are given for fourteen nodes with the correlation device and eighteen
without it. This graph demonstrates that even if we
allow controller size to continue to grow and examine
only the amount of time that is necessary to achieve a
solution, the NLP methods continue to provide higher
values. Although the values of the controllers produced by the DEC-BPI methods are somewhat close
to those of the NLP techniques as controller size grows,
our approaches produce that value with a fraction of
the controller size.
6.3

Figure 3: Multiagent Tiger problem values using NLP
methods and DEC-BPI with and without a 2 node
correlation device.

Multiagent tiger problem

Another domain with 2 states, 3 actions and 2 observations called the multiagent tiger problem was introduced by Nair et al. (Nair et al., 2003). In this
problem, there are two doors. Behind one door is a
tiger and behind the other is a large treasure. Each
agent may open one of the doors or listen. If either
agent opens the door with the tiger behind it, a large
penalty is given. If the door with the treasure behind it is opened and the tiger door is not, a reward is
given. If both agents choose the same action (i.e., both
opening the same door) a larger positive reward or a
smaller penalty is given to reward this cooperation. If
an agent listens, a small penalty is given and an observation is seen that is a noisy indication of which door
the tiger is behind. While listening does not change
the location of the tiger, opening a door causes the
tiger to be placed behind one of the door with equal
probability. A discount factor of 0.9 was used.
Figure 3 shows the values attained by each NLP and
DEC-BPI method for the given controller sizes. Figure 4 shows the values of just the two NLP methods.
These graphs show that not only do the NLP methods significantly outperform the DEC-BPI approaches,
but correlation greatly increases the value attained by
the nonlinear optimization. The individual results for
this problem suggest the DEC-BPI approach is more
dependent on the initial controller and the large penalties in this problem result in several results that are
very low. This outweighs the few times that more reasonable value is attained. Nevertheless, the max value
attained by DEC-BPI for all cases is still less than the

Figure 4: Multiagent Tiger problem values using just
the NLP methods with and without a 2 node correlation device.

Figure 5: Multiagent Tiger problem graphs for value
vs. time for the NLP methods with and without the
correlation device.
mean value attained by the NLP methods. Again for
this problem, more time is needed for the NLP approaches, but one node controllers are produced with
higher value than any controller size for the DEC-BPI

8

AMATO ET AL.

methods and require very little time.
The usefulness of the correlation device is illustrated
in Figure 5. For given amounts of time, the nonlinear
optimization that includes the correlation device produces much higher values. The DEC-BPI methods are
not included in this graph as they were unable to produce mean values greater than -50 for any controller
size up to 22 for which mean time to convergence was
over 5000 seconds. This shows the importance of correlation in this problem and the ability of our NLP
technique to take advantage of it.

7

Conclusion

We introduced a novel approach to solving decentralized POMDPs by using a nonlinear program formulation. This memory-bounded stochastic controller formulation allows a wide range of powerful nonlinear
programming algorithms to be applied to solve DECPOMDPs. The approach is easy to implement as it
mostly involves reformulating the problem and feeding it into an NLP solver.
We showed that by using an off-the-shelf locally optimal NLP solver, we were able to produce higher valued
controllers than the current state-of-the-art technique
for an assortment of DEC-POMDP problems. Our experiments also demonstrate that incorporating a correlation device as a shared source of randomness for
the agents can further increase solution quality. While
the time taken to find a solution to the NLP can be
higher, the fact that higher values can be found with
smaller controllers by using the NLP suggests adopting more powerful optimization techniques for smaller
controllers can be more productive in a given amount
of time. The combination of start state knowledge
and more advanced optimization allows us to make efficient use of the limited space of the controllers. These
results show that this method can allow compact optimal or near-optimal controllers to be found for various
DEC-POMDPs.
In the future, we plan to conduct a more exhaustive
analysis of the NLP representation and explore more
specialized algorithms that can be tailored for this optimization problem. While the performance we get
using a standard nonlinear optimization algorithm is
very good, specialized solvers might be able to further
increase solution quality and scalability. We also plan
to characterize the circumstances under which introducing a correlation device is cost effective.
Acknowledgements
An earlier version of this paper without improvements
such as incorporating a correlation device was pre-

sented at the AAMAS-06 Workshop on Multi-Agent
Sequential Decision Making in Uncertain Domains.
This work was supported in part by the Air Force Office of Scientific Research (Grant No. FA9550-05-10254) and by the National Science Foundation (Grant
No. 0535061). Any opinions, findings, conclusions
or recommendations expressed in this manuscript are
those of the authors and do not reflect the views of the
US government.


This article presents the state-of-the-art in optimal solution methods for decentralized
partially observable Markov decision processes (Dec-POMDPs), which are general models for
collaborative multiagent planning under uncertainty. Building off the generalized multiagent A* ( GMAA*) algorithm, which reduces the problem to a tree of one-shot collaborative
Bayesian games (CBGs), we describe several advances that greatly expand the range of DecPOMDPs that can be solved optimally. First, we introduce lossless incremental clustering
of the CBGs solved by GMAA*, which achieves exponential speedups without sacrificing
optimality. Second, we introduce incremental expansion of nodes in the GMAA* search
tree, which avoids the need to expand all children, the number of which is in the worst case
doubly exponential in the node’s depth. This is particularly beneficial when little clustering
is possible. In addition, we introduce new hybrid heuristic representations that are more
compact and thereby enable the solution of larger Dec-POMDPs. We provide theoretical
guarantees that, when a suitable heuristic is used, both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent. Finally,
we present extensive empirical results demonstrating that GMAA*-ICE, an algorithm that
synthesizes these advances, can optimally solve Dec-POMDPs of unprecedented size.

1. Introduction
A key goal of artificial intelligence is the development of intelligent agents that interact with
their environment in order to solve problems, achieve goals, and maximize utility. While such
agents sometimes act alone, researchers are increasingly interested in collaborative multiagent
systems, in which teams of agents work together to perform all manner of tasks. Multiagent
systems are appealing, not only because they can tackle inherently distributed problems, but
because they facilitate the decomposition of problems too complex to be tackled by a single
c 2013 AI Access Foundation. All rights reserved.

Oliehoek, Spaan, Amato, & Whiteson

agent (Huhns, 1987; Sycara, 1998; Panait & Luke, 2005; Vlassis, 2007; Buşoniu, Babuška, &
De Schutter, 2008).
One of the primary challenges of multiagent systems is the presence of uncertainty. Even
in single-agent systems, the outcome of an action may be uncertain, e.g., the action may fail
with some probability. Furthermore, in many problems the state of the environment may be
uncertain due to limited or noisy sensors. However, in multiagent settings these problems
are often greatly exacerbated. Since agents have access only to their own sensors, typically a
small fraction of those of the complete system, their ability to predict how other agents will
act is limited, complicating cooperation. If such uncertainties are not properly addressed,
arbitrarily bad performance may result.
In principle, agents can use communication to synchronize their beliefs and coordinate
their actions. However, due to bandwidth constraints, it is typically infeasible for all agents
to broadcast the necessary information to all other agents. In addition, in many realistic
scenarios, communication may be unreliable, precluding the possibility of eliminating all uncertainty about other agents’ actions.
Especially in recent years, much research has focused on approaches to (collaborative)
multiagent systems that deal with uncertainty in a principled way, yielding a wide variety
of models and solution methods (Pynadath & Tambe, 2002; Goldman & Zilberstein, 2004;
Seuken & Zilberstein, 2008). This article focuses on the decentralized partially observable
Markov decision process (Dec-POMDP), a general model for collaborative multiagent planning under uncertainty. Unfortunately, solving a Dec-POMDP, i.e., computing an optimal
plan, is generally intractable (NEXP-complete) (Bernstein, Givan, Immerman, & Zilberstein,
2002) and in fact even computing solutions with absolutely bounded error (i.e., ǫ-approximate
solutions) is also NEXP-complete (Rabinovich, Goldman, & Rosenschein, 2003). In particular,
the number of joint policies grows exponentially with the number of agents and observations
and doubly exponentially with respect to the horizon of the problem.1 Though these complexity results preclude methods that are efficient on all problems, developing better optimal
solution methods for Dec-POMDPs is nonetheless an important goal, for several reasons.
First, since the complexity results describe only the worst case, there is still great potential
to improve the performance of optimal methods in practice. In fact, there is evidence that
many problems can be solved much faster than the worst-case complexity bound indicates
(Allen & Zilberstein, 2007). In this article, we present experiments that clearly demonstrate
this point: on many problems, the methods we propose scale vastly beyond what would be
expected for a doubly-exponential dependence on the horizon.
Second, as computer speed and memory capacity increase, a growing set of small and
medium-sized problems can be solved optimally. Some of these problems arise naturally while
others result from the decomposition of larger problems. For instance, it may be possible
to extrapolate optimal solutions of problems with shorter planning horizons, using them as
the starting point of policy search for longer-horizon problems as in the work of Eker and
Akın (2013), or to use such shorter-horizon, no-communication solutions inside problems with
communication (Nair, Roth, & Yohoo, 2004; Goldman & Zilberstein, 2008). More generally,
optimal policies of smaller problems can potentially be used to find good solutions for larger
problems. For instance, transfer planning (Oliehoek, 2010; Oliehoek, Whiteson, & Spaan,
1. Surprisingly, the number of states in a Dec-POMDP is less important, e.g., brute-force search depends on
the number of states only via its policy evaluation routine, which scales linearly in the number of states.

450

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

2013) employs optimal solutions to problems with few agents to better solve problems with
many agents. By performing (approximate) influence-based abstraction and influence search
(Witwicki, 2011; Oliehoek, Witwicki, & Kaelbling, 2012), optimal solutions of component
problems can potentially be used to find (near-)optimal solutions of larger problems.
Third, optimal methods offer important insights into the nature of specific Dec-POMDP
problems and their solutions. For instance, the methods introduced in this article enabled the
discovery of certain properties of the BroadcastChannel benchmark problem that make
it much easier to solve.
Fourth, optimal methods provide critical inspiration for principled approximation methods. In fact, almost all successful approximate Dec-POMDP methods are based on optimal
ones (see, e.g., Seuken & Zilberstein, 2007b, 2007a; Dibangoye, Mouaddib, & Chai-draa, 2009;
Amato, Dibangoye, & Zilberstein, 2009; Wu, Zilberstein, & Chen, 2010a; Oliehoek, 2010) or
locally optimal ones (Velagapudi, Varakantham, Scerri, & Sycara, 2011)2 , and the clustering technique presented in this article forms the basis of a recently introduced approximate
clustering technique (Wu, Zilberstein, & Chen, 2011).
Finally, optimal methods are essential for benchmarking approximate methods. In recent
years, there have been huge advances in the approximate solution of Dec-POMDPs, leading
to the development of solution methods that can deal with large horizons, hundreds of agents
and many states (e.g., Seuken & Zilberstein, 2007b; Amato et al., 2009; Wu et al., 2010a;
Oliehoek, 2010; Velagapudi et al., 2011).
However, since computing even ǫ-approximate
solutions is NEXP-complete, any method whose complexity is not doubly exponential cannot
have any guarantees on the absolute error of the solution (assuming EXP6=NEXP). As such,
existing effective approximate methods have no quality guarantees.3
Consequently, it is difficult to meaningfully interpret their empirical performance without
the upper bounds optimal methods supply. While approximate methods can also be benchmarked against lower bounds (e.g., other approximate methods), such comparisons cannot
detect when a method fails to find good solutions. Doing so requires benchmarking against
upper bounds and, unfortunately, upper bounds that are easier to compute, such as QMDP
and QPOMDP, are too loose to be helpful (Oliehoek, Spaan, & Vlassis, 2008). As such,
benchmarking with respect to optimal solutions is an important part of the verification of any
approximate algorithm. Since existing optimal methods can only tackle very small problems,
scaling optimal solutions to larger problems is a critical goal.
1.1 Contributions
This article presents the state-of-the-art in optimal solution methods for Dec-POMDPs. In
particular, it describes several advances that greatly expand the horizon to which many DecPOMDPs can be solved optimally. In addition, it proposes and evaluates a complete algorithm
that synthesizes these advances. Our approach is based on the generalized multiagent A*
(GMAA*) algorithm (Oliehoek, Spaan, & Vlassis, 2008), which makes it possible to reduce
the problem to a tree of one-shot collaborative Bayesian games (CBGs). The appeal of this
2. The method by Velagapudi et al. (2011) repeatedly computes best responses in a way similar to DP-JESP
(Nair, Tambe, Yokoo, Pynadath, & Marsella, 2003). The best response computation, however, exploits
sparsity of interactions.
3. Note that we refer to methods without quality guarantees as approximate rather than heuristic to avoid
confusion with heuristic search, which is used throughout this article and is exact.

451

Oliehoek, Spaan, Amato, & Whiteson

approach is the abstraction layer it introduces, which has led to various insights into DecPOMDPs and, in turn, to the improved solution methods we describe.
The specific contributions of this article are:4
1. We introduce lossless clustering of CBGs, a technique to reduce the size of the CBGs
for which GMAA* enumerates all possible solutions, while preserving optimality. This
can exponentially reduce the number of child nodes in the GMAA* search tree, leading
to huge increases in efficiency. In addition, by applying incremental clustering (IC) to
GMAA*, our GMAA*-IC method can avoid clustering exponentially sized CBGs.
2. We introduce incremental expansion (IE) of nodes in the GMAA* search tree. Although
clustering may reduce the number of children of a search node, this number is in the
worst case still doubly exponential in the node’s depth. GMAA*-ICE, which applies
IE to GMAA*-IC, addresses this problem by creating a next child node only when it
is a candidate for further expansion.
3. We provide theoretical guarantees for both GMAA*-IC and GMAA*-ICE. In particular, we show that, when using a suitable heuristic, both algorithms are both complete
and search equivalent.
4. We introduce an improved heuristic representation. Tight heuristics like those based
on the underlying POMDP solution (QPOMDP ) or the value function resulting from
assuming 1-step-delayed communication (QBG ) are essential for heuristic search methods like GMAA* (Oliehoek, Spaan, & Vlassis, 2008). However, the space needed to
store these heuristics grows exponentially with the problem horizon. We introduce hybrid representations that are more compact and thereby enable the solution of larger
problems.
5. We present extensive empirical results that show substantial improvements over the
current state-of-the-art. Whereas Seuken and Zilberstein (2008) argued that GMAA*
can at best optimally solve Dec-POMDPs only one horizon further than brute-force
search, our results demonstrate that GMAA*-ICE can do much better. In addition, we
provide a comparative overview of the results of competitive optimal solution methods
from the literature.
The primary aim of the techniques introduced in this article is to improve scalability
with respect to the horizon. Our empirical results confirm that these techniques are highly
successful in this regard. As an added bonus, our experiments also demonstrate improvement
in scalability with respect to the number of agents. In particular, we present the first optimal
results on general (non-special case) Dec-POMDPs with more than three agents. Extensions
of our techniques to achieve further improvements with respect to the number of agents,
as well as promising ways to combine the ideas behind our methods with state-of-the-art
approximate approaches, are discussed under future work in Section 7.
4. This article synthesizes and extends research that was already reported in two conference papers (Oliehoek,
Whiteson, & Spaan, 2009; Spaan, Oliehoek, & Amato, 2011).

452

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

1.2 Organization
The article is organized as follows. Section 2 provides background on the Dec-POMDP model,
the GMAA* heuristic search solution method, as well as suitable heuristics. In Section 3, we
introduce lossless clustering of the CBGs and its integration into GMAA*. Section 4 introduces the incremental expansion of search nodes. The empirical evaluation of the proposed
techniques is reported in Section 5. We give a treatment of related work in Section 6. Future
work is discussed in Section 7 and conclusions are drawn in Section 8.

2. Background
In a Dec-POMDP, multiple agents must collaborate to maximize the sum of the common
rewards they receive over multiple timesteps. Their actions affect not only their immediate
rewards but also the state to which they transition. While the current state is not known to
the agents, at each timestep each agent receives a private observation correlated with that
state.
Definition 1. A Dec-POMDP is a tuple D, S, A, T, O, O, R, b0 , h , where
• D = {1, . . . ,n} is the finite set of agents.

• S = s1 , . . . ,s|S| is the finite set of states.

• A = ×i Ai is the set of joint actions a = ha1 , . . . , an i, where Ai is the finite set of actions
available to agent i.
• T is a transition function specifying the state transition probabilities Pr(s′ |s,a).
• O = ×i Oi is the finite set of joint observations. At every stage one joint observation
o = ho1 ,...,on i is received. Each agent i observes only its own component oi .
• O is the observation function, which specifies observation probabilities Pr(o|a,s′ ).
• R(s,a) is the immediate reward function mapping (s,a)-pairs to real numbers.
• b0 ∈ ∆(S) is the initial state distribution at time t = 0, where ∆(S) denotes the infinite
set of probability distributions over the finite set S.
• h is the horizon, i.e., the number of stages. We consider the case where h is finite.
At each stage t = 0 . . . h − 1, each agent takes an individual action and receives an individual
observation.
Example 1 (Recycling Robots). To illustrate the Dec-POMDP model, consider a team of robots tasked
with removing trash from an office building, depicted in Fig. 1. The robots have sensors to find marked
trash cans, motors to move around in order to look for cans, as well as gripper arms to grasp and carry
a can. Small trash cans are light and compact enough for a single robot to carry, but large trash cans
require multiple robots to carry them out together. Because more people use them, the larger trash
cans fill up more quickly. Each robot must also ensure that its battery remains charged by moving
to a charging station before it expires. The battery level for a robot degrades due to the distance the
robot travels and the weight of the item being carried. Each robot knows its own battery level but not
that of the other robots and only the location of other robots within sensor range. The goal of this
problem is to remove as much trash as possible in a given time period.
This problem can be represented as a Dec-POMDP in a natural way. The states, S, consist of
the different locations of each robot, their battery levels and the different amounts of trash in the
cans. The actions, Ai , for each robot consist of movements in different directions as well as decisions
453

Oliehoek, Spaan, Amato, & Whiteson

Figure 1: Illustration of the Recycling Robots example, in which two robots have to remove
trash in an office environment with three small (blue) trash cans and two large (yellow) ones.
In this situation, the left robot might observe that the large trash can next to it is full, and
the other robot that the small trash can is empty. However, none of them is sure of the trash
cans’ state due to limited sensing capabilities, nor do they see the state of trash cans further
away. In particular, one robot has no knowledge regarding the observations of the other robot.
to pick up a trash can or recharge the battery (when in range of a can or a charging station). The
observations, Oi , of each robot consist of its own battery level, its own location, the locations of other
robots in sensor range and the amount of trash in cans within range. The rewards, R, could consist of
a large positive value for a pair of robots emptying a large (full) trash can, a small positive value for
a single robot emptying a small trash can and negative values for a robot depleting its battery or a
trash can overflowing. An optimal solution is a joint policy that leads to the expected behavior (given
that the rewards are properly specified). That is, it ensures that the robots cooperate to empty the
large trash cans when appropriate and the small ones individually while considering battery usage.

For explanatory purposes, we also consider a much simpler problem, the so-called decentralized tiger problem (Nair et al., 2003).
Example 2 (Dec-Tiger). The Dec-Tiger problem concerns two agents that find themselves in a hallway with two doors. Behind one door, there is a treasure and behind the other is a tiger. The state
describes which door the tiger is behind—left (sl ) or right (sr )—each occurring with 0.5 probability
(i.e., the initial state distribution b0 is uniform). Each agent can perform three actions: open the left
door (aOL ), open the right door (aOR ) or listen (aLi ). Clearly, opening the door to the treasure will
yield a reward, but opening the door to the tiger will result in a severe penalty. A greater reward
is given for both agents opening the correct door at the same time. As such, a good strategy will
probably involve listening first. The listen actions, however, also have a minor cost (negative reward).
At every stage the agents get an observation. The agents can either hear the tiger behind the left
(oHL ) or right (oHR ) door, but each agent has a 15% chance of hearing it incorrectly (getting the wrong
observation). Moreover, the observation is informative only if both agents listen; if either agent opens
a door, both agents receive an uninformative (uniformly drawn) observation and the problem resets to
sl or sr with equal probability. At this point the problem just continues, such that the agents may be
able to open the door to the treasure multiple times. Also note that, since the only two observations
the agents can get are oHL , oHR , the agents have no way of detecting that the problem has been reset:
if one agent opens the door while the other listens, the other agent will not be able to tell that the
door was opened. For a complete specification, see the discussion by Nair et al. (2003).

Given a Dec-POMDP, the agents’ common goal is to maximize the expected cumulative
reward or return. The planning task entails finding a joint policy π = hπ1 , . . . ,πn i from the
space of joint policies Π, that specifies an individual policy πi for each agent i. Such an
454

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

individual policy in general specifies an individual action for each action-observation history
(AOH) θ~it = (a0i ,o1i , . . . ,ait−1 ,oti ), e.g., πi (θ~it ) = ati . However, it is possible to restrict our
attention to deterministic or pure policies, in which case πi maps each observation history
~ t to an action, e.g., πi (~o t ) = at . The number of such policies is
(OH) (o1i , . . . ,oti ) = ~oit ∈ O
i
i
i
h −1)/(|O |−1)
(|O
|
i
|Ai | i
and the number of joint policies is therefore

n|O∗ |h −1 
O |A∗ | |O∗ |−1 ,
(2.1)
where A∗ and O∗ denote the largest individual action and observation sets. The quality of a
particular joint policy is expressed by the expected cumulative reward it induces, also referred
to as its value.
Definition 2. The value V (π) of a joint policy π is
V (π) , E

h−1
hX
t=0

i
R(st ,at ) π,b0 ,

(2.2)

where the expectation is over sequences of states, actions and observations.
The planning problem for a Dec-POMDP is to find an optimal joint policy π ∗ , i.e., a joint
policy that maximizes the value: π ∗ = arg maxπ V (π).
Because an individual policy πi depends only on the local information ~oi available to an
agent, the on-line execution phase is truly decentralized: no communication takes place other
than that modeled via actions and observations. The planning itself however, may take place
in an off-line phase and be centralized. This is the scenario that we consider in this article. For
a more detailed introduction to Dec-POMDPs see, e.g., the work of Seuken and Zilberstein
(2008) and Oliehoek (2012).
2.1 Heuristic Search Methods
In recent years, numerous Dec-POMDP solution methods have been proposed. Most of these
methods fall into one of two categories: dynamic programming and heuristic search methods.
Dynamic programming methods take a backwards or ‘bottom-up’ perspective by first considering policies for the last time step t = h − 1 and using them to construct policies for stage
t = h − 2, etc. In contrast, heuristic search methods take a forward or ‘top-down’ perspective
by first constructing plans for t = 0 and extending them to later stages.
In this article, we focus on the heuristic search approach that has shown state-of-the-art
results. As we make clear in this section, this method can be interpreted as searching over a
tree of collaborative Bayesian games (CBGs). These CBGs provide a convenient abstraction
layer that facilitates the explanation of the techniques introduced in this article.
This section provides some concise background on heuristic search methods. For a more
detailed description, see the work of Oliehoek, Spaan, and Vlassis (2008). For a further description of dynamic programming methods and their relationship to heuristic search methods,
see the work of Oliehoek (2012).
2.1.1 Multiagent A*
Szer, Charpillet, and Zilberstein (2005) introduced a heuristically guided policy search method
called multiagent A* (MAA*). It performs an A* search over partially specified joint policies,
455

Oliehoek, Spaan, Amato, & Whiteson

t=0

t=1

t=2

δi0

aLi
γiτ =2

ϕ2i

oHR

oHL
aOL

δi1

aOL

oHL

oHR

oHL

oHR

aLi

aLi

aOL

aLi

δi2

γiτ =1
Figure 2: An arbitrary policy for the Dec-Tiger problem. The figure illustrates the different
types of partial policies used in this paper. The shown past policy ϕ2i consists of two decision
rules δi0 , δi1 . Also shown are two sub-tree policies γiτ =1 , γiτ =2 (introduced in Section 3.1.2).
pruning joint policies that are guaranteed to be worse than the best (fully specified) joint policy
found so far. Oliehoek, Spaan, and Vlassis (2008) generalized the algorithm by making explicit
the expand and selection operators performed in the heuristic search. The resulting algorithm,
generalized MAA* (GMAA*) offers a unified perspective of MAA* and the forward sweep
policy computation method (Emery-Montemerlo, 2005), which differ in how they implement
GMAA*’s expand operator: forward sweep policy computation solves (i.e., finds the best
policy for) collaborative Bayesian games, while MAA* finds all policies for those collaborative
Bayesian games, as we describe in Section 2.1.2.
The GMAA* algorithm considers joint policies that are partially specified with respect
to time. These partially specified policies can be formalized as follows.
Definition 3. A decision rule δit for agent i’s decision for stage t is a mapping from action~ t → Ai .
observation histories for stage t to actions δit : Θ
i
In this article, we consider only deterministic policies. Since such policies need to condition
their actions only on observation histories, they are made up of decision rules that map length~ t → Ai . A joint decision rule δ t = hδ t , . . . ,δnt i specifies
t observation histories to actions: δit : O
1
i
a decision rule for each agent. Fig. 2 illustrates this concept, as well as that of a past policy,
which we introduce shortly. As discussed below, decision rules allow partial policies to be
defined and play a crucial role in GMAA* and the algorithms developed in this article.
Definition 4. A partial or past policy for stage t, ϕti , specifies the part of agent i’s policy
that relates to stages t′ < t. That is, it specifies the decision rules for the first t stages:
ϕti = (δi0 ,δi1 , . . . ,δit−1 ). A past policy for stage h is just a regular, or fully specified, policy
ϕhi = πi . A past joint policy ϕt = (δ 0 ,δ 1 , . . . ,δ t−1 ) specifies joint decision rules for the first t
stages.
GMAA* performs a heuristic search over such partial joint policies ϕt by constructing
a search tree as illustrated in Fig. 3a. Each node q = hϕt , v̂i in the search tree specifies a
past joint policy ϕt and heuristic value v̂. This heuristic value v̂ of the node represents an
optimistic estimate of the past joint policy Vb (ϕt ), which can be computed via
Vb (ϕt ) = V 0...t−1 (ϕt ) + H t...h−1 (ϕt ),
456

(2.3)

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

B(ϕ0 )

ϕ0
δ0

β0

δ 0′′

β 0′′
β 0′

δ 0′
ϕ1′

ϕ1
δ1

ϕ2

...

B(ϕ1 )

ϕ1′′

β1

δ 1′

...

B(ϕ1′ )

B(ϕ2 )

ϕ2′

(a) The MAA* perspective.

...

B(ϕ1′′ )

β 1′

...

B(ϕ2′ )

(b) The CBG perspective.

Figure 3: Generalized MAA*. Associated with every node is a heuristic value. The search
trees for the two perspectives shown are equivalent under certain assumptions on the heuristic,
as explained in Section 2.2.
where H t...h−1 is a heuristic value for the remaining h − t stages and V 0...t−1 (ϕt ) is the actual
expected reward ϕt achieves over the first t stages (for its definition, see Appendix A.3).
Clearly, when H t...h−1 is an admissible heuristic—a guaranteed overestimation—so is Vb (ϕt ).5
Algorithm 1 illustrates GMAA*. It starts by creating a node q 0 for a completely unspecified joint policy ϕ0 and placing it in an open list L. Then, it selects nodes (Algorithm 2) and
expands them (Algorithm 3), repeating this process until it is certain that it has found the
optimal joint policy.
The Select operator returns the highest ranked node, as defined by the following comparison operator.
Definition 5. The node comparison operator < is defined for two nodes q = hϕt ,v̂i, q ′ =
hϕt′ ,v̂ ′ i as follows:

′

, if v̂ 6= v̂ ′
v̂ < v̂
q < q ′ = depth(q) < depth(q ′ ) , otherwise if depth(q) 6= depth(q ′ )
(2.4)

 t
t′
ϕ <ϕ
, otherwise.
That is, the comparison operator first compares the heuristic values. If those are equal,
it compares the depth of the nodes. Finally, if nodes have equal value and equal depth, it
lexically compares the past joint policies. This ranking leads to A* behavior (i.e., selecting the
node from the open list with the highest heuristic value) of GMAA*, as well as guaranteeing
the same selection order in our incremental expansion technique (introduced in Section 4).
Ranking nodes with greater depth higher in case of equal heuristic value helps find tight
lower bounds early by first expanding deeper nodes (Szer et al., 2005) and is also useful in
incremental expansion.
5. More formally, H should not underestimate the value. Note that, unlike classical A* applications such as
path planning–in which an admissible heuristic should not overestimate–in our setting we maximize reward,
rather than minimize cost.

457

Oliehoek, Spaan, Amato, & Whiteson

Algorithm 1 Generalized multiagent A*.
Input: a Dec-POMDP, an admissible heuristic H, an empty open list L
Output: an optimal joint policy π ∗
1: vGM AA ← −∞
2: q 0 ← hϕ0 = (), v̂ = +∞i
3: L.insert(q 0 )
4: repeat
5:
q ← Select(L)
6:
QExpand ← Expand(q, H)
7:
if depth(q) = h − 1 then
8:
{ QExpand contains fully specified joint policies, we only are interested in the best one }
9:
hπ, vi ← BestJointPolicyAndValue(QExpand )
10:
if v > vGM AA then
11:
π∗ ← π
{found a new best joint policy}
12:
vGM AA ← v
13:
L.Prune(vGM AA )
{(optionally) prune the open list}
14:
end if
15:
else

{add expanded children to open list}
16:
L.insert( q ′ ∈ QExpand | q ′ .v̂ > vGM AA )
17:
end if
18:
PostProcessNode(q, L)
19: until L is empty
20: return π ∗

Algorithm 2 Select(L): Return the highest ranked node from the open list.
Input: open list L, total order on nodes <
Output: the highest ranked node q ∗
1: q ∗ ← q ∈ L s.t. ∀q′ ∈L (q ′ 6= q =⇒ q ′ < q)
2: return q ∗

The Expand operator constructs QExpand , the set of all child nodes. That is, given a node
that contains partial joint policy ϕt = (δ 0 ,δ 1 , . . . ,δ t−1 ), it constructs Φt+1 , the set of all
ϕt+1 = (δ 0 ,δ 1 , . . . ,δ t−1 ,δ t ), by appending all possible joint decision rules δ t for the next time
step t. For all these ϕt+1 , a heuristic value is computed and a node is constructed.
After expansion, the algorithm checks (line 7) if the expansion resulted in fully specified
joint policies. If not, all children with sufficient heuristic value are placed in the open list
Algorithm 3 Expand(q, H). The expand operator of plain MAA*.
Input: q = hϕt , v̂i the search node to expand, H the admissible heuristic.
Output: QExpand the set containing all expanded child nodes.
1: QExpand ← {}
2: Φt+1 ← {ϕt+1 | ϕt+1 = (ϕt , δ t )}
3: for ϕt+1 ∈ Φt+1 do
4:
Vb (ϕt+1 ) ← V 0...t (ϕt+1 ) + H(ϕt+1 )
5:
q ′ ← hϕt+1 , Vb (ϕt+1 )i
6:
QExpand .Insert(q ′ )
7: end for
8: return QExpand

458

{create child node}

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

Algorithm 4 PostProcessNode(q,L)
Input: q the expanded parent node, L the open list.
Output: the expanded node is removed.
1: L.Pop(q)

(line 16). If the children are fully specified, BestJointPolicyAndValue returns only the best
joint policy (and its value) from QExpand (see Algorithm 12 in Appendix A.1 for details of
BestJointPolicyAndValue). GMAA* also maintains a lower bound vGM AA which corresponds to the actual value of the best fully-specified joint policy found so far. If the newly
found joint policy has a higher value this lower bound is updated (lines 11 and 12). Also, any
nodes for partial joint policies ϕt+1 with an upper bound that is lower than the best solution
so far, Vb (ϕt+1 ) < vGM AA , can be pruned (line 13). This pruning takes additional time, but
can save memory. Finally, PostProcessNode simply removes the parent node from the open
list (this procedure is augmented for incremental expansion in Section 4). The search ends
when the list becomes empty, at which point an optimal joint policy has been found.
GMAA* is complete, i.e., it will search until it finds a solution. Therefore, in theory,
GMAA* is guaranteed to eventually produce an optimal joint policy (Szer et al., 2005).
However, in practice, this is often infeasible for larger problems. A major source of complexity
is the full expansion of a search node. The number of joint decision rules for stage t that can
form the children of a node at depth t in the search tree6 is


t
O |A∗ |n(|O∗ | ) ,
(2.5)
which is doubly exponential in t. Comparing (2.1) with (2.5), we see that the worst case
complexity of expanding a node for the deepest level in the tree t = h − 1 is comparable to
that of brute force search for the entire Dec-POMDP. Consequently, Seuken and Zilberstein
(2008) conclude that MAA* “can at best solve problems whose horizon is only 1 greater than
those that can already be solved by naı̈ve brute force search.”
2.1.2 The Bayesian Game Perspective
GMAA* makes it possible to interpret MAA* as the solution of a collection of collaborative
Bayesian games (CBGs). We employ this approach throughout this article, as it facilitates
the improvements to GMAA* that we introduce, each of which results in significant advances
in the state-of-the-art in Dec-POMDP solutions.
A Bayesian game (BG) models a one-shot interaction between a number of agents. It is
an extension of the well-known strategic game (also known as a normal form game) in which
each agent holds some private information (Osborne & Rubinstein, 1994). A CBG is a BG
in which the agents receive identical payoffs. In the Bayesian game perspective, each node q
in the GMAA* search tree, along with its corresponding partial joint policy ϕt , defines a
CBG (Oliehoek, Spaan, & Vlassis, 2008). That is, given state distribution b0 , for each ϕt ,
it is possible to construct a CBG B(b0 ,ϕt ) that represents the decision-making problem for
stage t given that ϕt was followed for the first t stages starting from b0 . When it is clear what
b0 is, we simply write B(ϕt ).
6. We follow the convention that the root has depth 0.

459

Oliehoek, Spaan, Amato, & Whiteson

Definition 6. A collaborative Bayesian game (CBG) B(b0 ,ϕt ) = hD, A, Θ, Pr(·), ui modeling
stage t of a Dec-POMDP, given initial state distribution b0 and past joint policy ϕt , consists
of:
• D, the set of agents {1 . . . n},
• A, the set of joint actions,
• Θ, the set of their joint types, each of which specifies a type for each agent θ =
hθ1 , . . . ,θn i,
• Pr(·), a probability distribution over joint types,
• u, a (heuristic) payoff function mapping joint type and action to a real number: u(θ,a).
In any Bayesian game, the type θi of an agent i represents the private information it holds.
For instance, in a Bayesian game modeling of a job recruitment scenario, the type of an agent
may indicate whether that agent is a hard worker. In a CBG for a Dec-POMDP, an agent’s
private information is its individual AOH. Therefore, the type θi of an agent i corresponds to
θ~it , its history of actions and observations: θi ↔ θ~it . Similarly, a joint type corresponds to a
joint AOH: θ ↔ ~θ t .
Consequently, u should provide a (heuristic) estimate for the long-term payoff of each
(~θ t ,a)-pair. In other words, the payoff function corresponds to a heuristic Q-value: u(θ,a) ↔
b ~θ t ,a). We discuss how to compute such heuristics in Section 2.2. Given ϕt , b0 , and the
Q(
correspondence of joint types and AOHs, the probability distribution over joint types is:
Pr(θ) , Pr(~θ t |b0 ,ϕt ),

(2.6)

where the latter probability is the marginal of Pr(s,~θ t |b0 ,ϕt ) as defined by (A.2) used in the
computation of the value of a partial joint policy V 0...t−1 (ϕt ) in Appendix A.3. Note that due
to the correspondence between types and AOHs, the size of a CBG B(b0 ,ϕt ) for a stage t is
exponential in t.
In a CBG, each agent uses a Bayesian game policy βi that maps individual types to actions:
βi (θi ) = ai . Because of the correspondence between types and AOHs, a (joint) policy for the
CBG β corresponds to a (joint) decision rule: β ↔ δ t . In the remainder of this article, we
assume deterministic past joint policies ϕt , which implies that only one ~θ t will have non-zero
probability given the observation history ~o t . Thus, β effectively maps observation histories
to actions. The number of such β for B(b0 ,ϕt ) is given by (2.5). The value of a joint CBG
policy β for a CBG B(b0 ,ϕt ) is:
X
b ~θ t ,β(~θ t )),
Pr(~θ t |b0 ,ϕt )Q(
(2.7)
Vb (β) =
~
θt

where β t (~θ t ) = hβi (θ~it )ii=1...n denotes the joint action that results from application of the
individual CBG-policies to the individual AOH θ~it specified by ~θ t .
Example 3. Consider a CBG for Dec-Tiger given the past joint policy ϕ2 that specifies to listen at the first two stages. At stage t = 2, each agent has four possible observation histories:
~ 2 = {(oHL ,oHL ), (oHL ,oHR ), (oHR ,oHL ), (oHR ,oHR )} that correspond directly to its possible types. The
O
i
probabilities of these joint types given ϕ2 are listed in Fig. 4a. Since the joint OHs together with
ϕ2 determine the joint AOHs, they also correspond to so-called joint beliefs: probability distributions
over states (introduced formally in Section 2.2). Fig. 4b shows these joint beliefs, which can serve as
the basis for the heuristic payoff function (as further discussed in Section 2.2).
460

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

~o12
(oHL ,oHL )
(oHL ,oHR )
(oHR ,oHL )
(oHR ,oHR )

~o22
(oHL ,oHL )
0.261
0.047
0.047
0.016

(oHL ,oHR )
0.047
0.016
0.016
0.047

(oHR ,oHL )
0.047
0.016
0.016
0.047

(oHR ,oHR )
0.016
0.047
0.047
0.261

(a) The joint type probabilities.

~o12
(oHL ,oHL )
(oHL ,oHR )
(oHR ,oHL )
(oHR ,oHR )

~o22
(oHL ,oHL )
0.999
0.970
0.970
0.5

(oHL ,oHR )
0.970
0.5
0.5
0.030

(oHR ,oHL )
0.970
0.5
0.5
0.030

(oHR ,oHR )
0.5
0.030
0.030
0.001

(b) The induced joint beliefs. Listed is the probability Pr(sl |~θ 2 ,b0 ) of
the tiger being behind the left door.

Figure 4: Illustration for the Dec-Tiger problem with a past joint policy ϕ2 that specifies
only listen actions for the first two stages.

Algorithm 5 Expand-CBG(q, H). The expand operator of GMAA* that makes use of CBGs.
Input: q = hϕt , v̂i the search node to expand.
b ~θ,a).
Input: H the admissible heuristic that is of the form Q(
Output: QExpand the set containing all expanded child nodes.
b
1: B(b0 ,ϕt ) ← ConstructBG(b0 ,ϕt , Q)
2: QExpand ← GenerateAllChildrenForCBG(B(b0 ,ϕt ))
3: return QExpand

{as explained in Section 2.1.2}

A solution to the CBG is a β that maximizes (2.7). A CBG is equivalent to a team
decision process and finding a solution is NP-complete (Tsitsiklis & Athans, 1985). However,
in the Bayesian game perspective of GMAA*, illustrated in Fig. 3b, the issue of solving a
CBG (i.e., finding the highest payoff β) is not so relevant because we need to expand all β.
That is, the Expand operator enumerates all β and appends them to ϕt to form the set of
extended joint policies

Φt+1 = (ϕt , β) | β is a joint CBG policy of B(b0 ,ϕt )
and uses this set to construct QExpand , the set of child nodes. The heuristic value of such a
child node q ∈ QExpand that specifies ϕt+1 = (ϕt , β) is given by
Vb (ϕt+1 ) = V 0...t−1 (ϕt ) + Vb (β).

(2.8)

The Expand operator that makes use of CBGs is summarized in Algorithm 5, which uses the
GenerateAllChildrenForCBG subroutine (Algorithm 13 in Appendix A.1). Fig. 3b illustrates
the Bayesian game perspective of GMAA*.
461

Oliehoek, Spaan, Amato, & Whiteson

2.2 Heuristics
To perform heuristic search, GMAA* defines the heuristic value Vb (ϕt ) using (2.3). In contrast, the Bayesian game perspective uses (2.8). These two formulations are equivalent when
b faithfully represents the expected immediate reward (Oliehoek, Spaan, & Vlasthe heuristic Q
sis, 2008). The consequence is that GMAA* via CBGs is complete (and thus finds optimal
solutions) as stated by the following theorem.
Theorem 1. When using a heuristic of the form
b ~θ t ,a) = Est [R(st ,a) | ~θ t ] + E~ t+1 [Vb (~θ t+1 ) | ~θ t , a],
Q(
θ

(2.9)

where Vb (~θ t+1 ) ≥ Qπ∗ (~θ t+1 , π ∗ (~θ t+1 )) is an overestimation of the value of an optimal joint
policy π ∗ , GMAA* via CBGs is complete.
Proof. See appendix.
In this theorem, Qπ (~θ t ,a) is the Q-value, i.e., the expected future cumulative reward of
performing a from ~θ t under joint policy π (Oliehoek, Spaan,P
& Vlassis, 2008). The expectation
t
~
of the immediate reward will also be written as R(θ ,a) = s∈S R(s,a) Pr(s|~θ t ,b0 ). It can be
computed using Pr(s|~θ t ,b0 ), a quantity we refer to as the joint belief resulting from ~θ t and
that we also denote as b. The joint belief itself can be computed via repeated application of
Bayes’ rule (Kaelbling, Littman, & Cassandra, 1998), or as the conditional of (A.2).
The rest of this subsection reviews several heuristics that have been used for GMAA*.
2.2.1 QMDP
b ~θ,a) is to solve the underlying MDP, i.e., to
One way to obtain an admissible heuristic Q(
assume the joint action is chosen by a single ‘puppeteer’ agent that can observe the true
state. This approach, known as QMDP (Littman, Cassandra, & Kaelbling, 1995), uses the
t
MDP value function Qt,∗
M (s ,a), which can be computed using standard dynamic programming
t
b ~t
techniques (Puterman, 1994). In order to transform the Qt,∗
M (s ,a)-values to QM (θ ,a)-values,
we compute:
X t,∗
b M (~θ t ,a) =
(2.10)
Q
Q (s,a) Pr(s|~θ t ,b0 ).
M

s∈S

Solving the underlying MDP has time complexity that is linear in h, which makes it,
especially compared to the Dec-POMDP, easy to compute. In addition, it is only necessary
to store a value for each (s,a)-pair, for each stage t. However, the bound it provides on the
optimal Dec-POMDP Q∗ -value function is loose (Oliehoek & Vlassis, 2007).
2.2.2 QPOMDP
Similar to the underlying MDP, one can define the underlying POMDP of a Dec-POMDP,
i.e., assuming the joint action is chosen by a single agent with access to the joint observation.7
7. Alternatively one can view this POMDP as a multiagent POMDP in which the agents can instantaneously
broadcast their private observations.

462

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

Tree

Vector

t=0
t=1
t=2
t=3
Figure 5: Visual comparison of tree and vector-based Q representations.

The resulting solution can be used as a heuristic, called QPOMDP (Szer et al., 2005; Roth,
Simmons, & Veloso, 2005). The optimal QPOMDP value function satisfies:

Q∗P (bt , a) = R(bt ,a) +

X

P (ot+1 |bt ,a) max Q∗P (bt+1 ,at+1 ),
at+1

ot+1 ∈O

(2.11)

P
where bt is the joint belief, R(bt ,a) = s∈S R(s,a)bt (s) is the immediate reward, and bt+1 is
the joint belief resulting from bt by action a and joint observation ot+1 . To use QPOMDP , for
b P (~θ t ,a) , Qt (b~θt ,a).
each ~θ t , we can directly use the value for the induced joint belief: Q
P

There are two approaches to computing QPOMDP . One is to construct the ‘belief MDP
tree’ of all joint beliefs, illustrated in Fig. 5 (left). Starting with b0 (corresponding to the
empty joint AOH ~θ 0 ), for each a and o we compute the resulting ~θ 1 and corresponding
~1
belief bθ and continue recursively. Given this tree, it is possible to compute values for all the
nodes by standard dynamic programming.
Another possibility is to apply vector-based POMDP techniques (see Fig. 5 (right)). The
Q-value function for a stage QtP (b,a) can be represented using a set of vectors for each joint
t } (Kaelbling et al., 1998). Qt (b,a) is then defined as the maximum
action V t = {V1t , . . . ,V|A|
P
inner product:
QtP (b,a) , max b · vat .
t ∈V t
va
a

Given V h−1 , the vector representation of the last stage, we can compute V h−2 , etc. In order
to limit the growth of the number of vectors, dominated vectors can be pruned.
Since QMDP is an upper bound on the POMDP value function (Hauskrecht, 2000), QPOMDP
provides a tighter upper bound to Q∗ than QMDP . However, it is also more costly to compute
and store: both the tree-based and the vector-based approach may need to store a number of
values exponential in h.
463

Oliehoek, Spaan, Amato, & Whiteson

2.2.3 QBG
A third heuristic, called QBG , assumes that each agent in the team has access only to its
individual observation but it can communicate with a 1-step delay.8 We define QBG as
X
Q∗B (~θ t ,a) = R(~θ t ,a) + max
Pr(ot+1 |~θ t ,a)Q∗B (~θ t+1 ,β(ot+1 )),
(2.12)
β

ot+1 ∈O

t+1
where β = hβ1 (ot+1
1 ),...,βn (on )i is a tuple of individual policies βi : Oi → Ai for the CBG
t
constructed for ~θ ,a. Like QPOMDP , QBG can also be represented using vectors (Varaiya &
Walrand, 1978; Hsu & Marcus, 1982; Oliehoek, Spaan, & Vlassis, 2008) and the same two
manners of computation (tree and vector based) apply. It yields a tighter heuristic than
QPOMDP , but its computation has an additional exponential dependence on the maximum
number of individual observations (Oliehoek, Spaan, & Vlassis, 2008), which is particularly
troubling for the vector-based computation, since it precludes effective application of incremental pruning (A. Cassandra, Littman, & Zhang, 1997). To overcome this problem, Oliehoek
and Spaan (2012) introduce novel tree-based pruning methods.

3. Clustering
GMAA* solves Dec-POMDPs by repeatedly constructing CBGs and expanding all the joint
BG policies β for them. However, the number of such β is equal to the number of regular
MAA* child nodes given by (2.5) and thus grows doubly exponentially with the horizon h.
In this section, we propose a new approach for improving scalability with respect to h by
clustering individual AOHs. This reduces the number of β and therefore the number of
constructed child nodes in the GMAA* search tree.9
Previous research has also investigated such clustering: Emery-Montemerlo, Gordon,
Schneider, and Thrun (2005) propose clustering types based on the profiles of the payoff
functions of the CBGs. However, the resulting method is ad hoc. Even given bounds on the
error of clustering two types in a CBG, no guarantees can be made about the quality of the
Dec-POMDP solution, as the bound is with respect to a heuristic payoff function.
In contrast, we propose to cluster histories based on the probability these histories induce
over histories of the other agents and over states. The critical advantage of this criterion,
which we call probabilistic equivalence (PE), is that the resulting clustering is lossless: the
solution for the clustered CBG can be used to construct the solution for the original CBG and
the values of the two CBGs are identical. Thus, the criterion allows for clustering of AOHs
in CBGs that represent Dec-POMDPs while preserving optimality.10
In Section 3.1, we describe how histories in Dec-POMDPs can be clustered using the
notions of probabilistic and best-response equivalence. This allows histories to be clustered
8. The name QBG stems from the fact that such a 1-step delayed communication scenario can be modeled
as a CBG. Note, however, that the CBGs used to compute QBG are of a different form than the B(b0 ,ϕt )
discussed in Section 2.1.2: in the latter, types correspond to length-t (action-) observation histories; in the
former, types correspond to length-1 observation histories.
9. While CBGs are not essential for clustering, they provide a convenient level of abstraction that simplifies
exposition of our techniques. Moreover, this level of abstraction makes it possible to employ our results
concerning CBGs outside the context of Dec-POMDPs.
10. The probabilistic equivalence criterion and lossless clustering were introduced by Oliehoek et al. (2009).
This article presents a new, simpler proof of the optimality of clustering based on PE.

464

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

when it is rational to always choose the same action. In Section 3.2, we describe the application
of these results to GMAA*. Section 3.3 introduces improved heuristic representations that
allow for the computation over longer horizons.
3.1 Lossless Clustering in Dec-POMDPs
In this section, we discuss lossless clustering based on the notion of probabilistic equivalence.
We show that this clustering is lossless by demonstrating that probabilistic equivalence implies
best response equivalence, which describes the conditions that a rational agent will select the
same action for two of its types. To prove this implication, we show that the best response
depends only on the multiagent belief (i.e., the probability distribution over states and policies
of the other agents), which is the same for two probabilistically equivalent histories. Relations
to other equivalence notions are discussed in Section 6.
3.1.1 Probabilistic Equivalence Criterion
We first introduce the probabilistic equivalence criterion, which can be used to decide whether
two individual histories θ~ia ,θ~ib can be clustered without loss in value.
Criterion 1 (Probabilistic Equivalence). Two AOHs θ~ia ,θ~ib for agent i are probabilistically
equivalent (PE), written P E(θ~ia ,θ~ib ), when the following holds:
∀~θ6=i ∀s

Pr(s,~
θ 6=i |θ~ia ) = Pr(s,~
θ 6=i |θ~ib ).

(3.1)

These probabilities can be computed as the conditional of Pr(s,~θ t |b0 ,ϕt ), defined by (A.2).
In subsections 3.1.2–3.1.4, we formally prove that PE is a sufficient criterion to guarantee
that clustering is lossless. In the remainder of Section 3.1.1 we discuss some key properties of
the PE criterion in order to build intuition.
Note that the criterion can be decomposed into the following two criteria:
∀~θ6=i
∀~θ6=i ∀s

Pr(~
θ 6=i |θ~ia ) = Pr(~
θ 6=i |θ~ib ),

(3.2)

Pr(s|~
θ 6=i ,θ~ia ) = Pr(s|~
θ 6=i ,θ~ib ).

(3.3)

These criteria give a natural interpretation: the first says that the probability distribution
over the other agents’ AOHs must be identical for both θ~ia and θ~ib . The second demands that
the resulting joint beliefs are identical.
The above probabilities are not well defined without the initial state distribution b0 and
past joint policy ϕt . However, since we consider clustering of histories within a particular CBG
(for some stage t) constructed for a particular b0 ,ϕt , they are implicitly specified. Therefore
we drop these arguments, clarifying the notation.
Example 4. In Example 3, the types (oHL ,oHR ) and (oHR ,oHL ) of each agent are PE. To see this, note
that the rows (columns for the second agent) for these histories are identical in both Fig. 4a and
Fig. 4b. Thus, they specify the same distribution over histories of the other agents (cf. equation (3.2))
and the induced joint beliefs are the same (cf. equation (3.3)).

Probabilistic equivalence has a convenient property that our algorithms exploit: if it holds
for a particular pair of histories, then it will also hold for all identical extensions of those
histories, i.e., it propagates forwards regardless of the policies of the other agents.
465

Oliehoek, Spaan, Amato, & Whiteson

Definition 7 (Identical extensions). Given two AOHs θ~ia,t ,θ~ib,t , their respective extensions
θ~ a,t+1 = (θ~ a,t ,ai ,oi ) and θ~ b,t+1 = (θ~ b,t ,a′ ,o′ ) are called identical extensions if and only if
i

i

i

ai = a′i and oi = o′i .

i

i

i

Lemma 1 (Propagation of PE). Given θ~ia,t ,θ~ib,t that are PE, regardless of the decision rule
the other agents use (δ t6=i ), identical extensions are also PE:
∀ati ∀ot+1 ∀δt ∀st+1 ∀~θt+1
i

6=i

6=i

t+1
t
t+1 ~ t+1 ~ b,t t t+1 t
,θ 6=i |θi ,ai ,oi ,δ 6=i ) (3.4)
Pr(st+1 ,~
θ 6=i |θ~ia,t ,ati ,ot+1
i ,δ 6=i ) = Pr(s

Proof. The proof is listed in the appendix, but holds intuitively because if the probabilities
described above were the same before, they will also be the same after taking the same action
and seeing the same observation.
Note that, while the probabilities defined in (3.1) superficially resemble beliefs used in
POMDPs, they are substantially different. In a POMDP, the single agent can compute its
individual belief using only its AOH. It can then use this belief to determine the value of
any future policy, as it is a sufficient statistic of the history to predict the future rewards
(Kaelbling et al., 1998; Bertsekas, 2005). Thus, it is trivial to show equivalence of AOHs
that induce the same individual belief in a POMDP. Unfortunately, Dec-POMDPs are more
problematic. The next section elaborates on this issue by discussing the relation to multiagent
beliefs.
3.1.2 Sub-Tree Policies, Multiagent Beliefs and Expected Future Value
To describe the relationship between multiagent beliefs and probabilistic equivalence, we
must first discuss the policies an agent may follow and their resulting values. We begin
by introducing the concept of sub-tree policies. As illustrated in Fig. 2 (on page 456), a
(deterministic) policy πi can be represented as a tree with nodes labeled using actions and
edges labeled using observations: the root node corresponds to the first action taken, other
nodes specify the action for the observation history encoded by the path from the root node.
As such, it is possible to define sub-tree policies, γi , which correspond to sub-trees of agent
i’s policy πi (also illustrated in Fig. 2). In particular, we write
w
πi ~o t = γiτ =h−t
(3.5)
i

for the sub-tree policy of πi corresponding to w
observation history ~oit that specifies the actions
for the last τ = h − t stages. We refer to  as the policy consumption operator, since it
w
‘consumes’ the part of the policy corresponding to ~oit . Similarly we write γiτ =k ~o l = γiτ =k−l
i
(note that in (3.5), πi is just a τ = h-steps-to-go sub-tree policy) and use similar notation,
γ τ =k , for joint sub-tree policies. For a more extensive treatment of these different forms of
policy, we refer to the discussion by Oliehoek (2012).
Given these concepts, we can define the value of a τ = k-stages-to-go joint policy starting
from state s:
XX
w
Pr(s′ ,o|s,a)V (s′ , γ τ =k o ).
(3.6)
V (s,γ τ =k ) = R(s,a) +
s′

o

Here, a is the joint action specified by the roots of the individual sub-tree policies specified
by γ τ =k for stage t = h − k.
466

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

From this definition, it follows directly that the probability distribution over states s and
sub-tree policies over other agents γ 6=i is sufficient to predict the value of a sub-tree policy γi .
In fact, such a distribution is known as a multiagent belief bi (s,γ 6=i ) (Hansen, Bernstein, &
Zilberstein, 2004). Its value is given by
XX
V (bi ) = max
bi (s,γ 6=i )V (s,hγi , γ 6=i i),
(3.7)
γi

s

γ 6=i

and we refer to the maximizing γi as agent i’s best response for bi . This illustrates that a
multiagent belief is a sufficient statistic: it contains sufficient information to predict the value
of any sub-tree policy γi .
It is possible to connect action observation histories to multiagent beliefs by fixing the
policies of the other agents. Given that the other agents will act according to a profile of
policies π 6=i , agent i has a multiagent belief at the first stage of the Dec-POMDP: bi (s,π 6=i ) =
b0 (s). Moreover, agent i can maintain such a multiagent belief during execution. As such,
given π 6=i , each history θ~i induces a multiagent belief, which we will write as bi (s,γ 6=i |θ~i , π 6=i )
to make the dependence on θ~i , π 6=i explicit. The multiagent belief for a history is defined as
bi (s,γ 6=i |θ~i , π 6=i ) , Pr(s,γ 6=i |θ~i , b0 , π 6=i ),

(3.8)

and induces a best response via (3.7):
BR(θ~i |π 6=i ) , arg max
γi

XX
s

bi (s,γ 6=i |θ~i , π 6=i )V (s,γ 6=i ,γi ).

(3.9)

γ 6=i

From this we can conclude that two AOHs θ~ia ,θ~ib can be clustered together if they induce the
same multiagent belief.
However, this notion of multiagent belief is clearly quite different from the distributions
used in our notion of PE. In particular, to establish whether two AOHs induce the same
multiagent belief, we need a full specification of π 6=i . Nevertheless, we show that two AOHs
that are PE are also best response equivalent and that we can therefore cluster them. The
crux is that we can show that, if Criterion 1 is satisfied, the AOHs will always induce the
same multiagent beliefs for any π 6=i (consistent with the current past joint policy ϕ6=i ).
3.1.3 Best-Response Equivalence Allows Lossless Clustering of Histories
We can now relate probabilistic equivalence and the multiagent belief as follows.
Lemma 2 (PE implies multiagent belief equivalence). For any π 6=i , probabilistic equivalence
implies multiagent belief equivalence:


P E(θ~ia ,θ~ib ) ⇒ ∀s,γ6=i bi (s,γ 6=i |θ~ia , π 6=i ) = bi (s,γ 6=i |θ~ib , π 6=i )
(3.10)
Proof. See appendix.
This lemma shows that if two AOHs are PE, they produce the same multiagent belief.
Intuitively, this gives us a justification to cluster such AOHs together: since a multiagent
belief is a sufficient statistic we should act the same when we have the same multiagent belief,
but since Lemma 2 shows that θ~ia ,θ~ib induces the same multiagent beliefs for any π 6=i when
they are PE, we can conclude that we will always act the same in those histories. Formally,
we prove that θ~ia ,θ~ib are best-response equivalent if they are PE.
467

Oliehoek, Spaan, Amato, & Whiteson

Theorem 2 (PE implies best-response equivalence). Probabilistic equivalence implies bestresponse equivalence. That is


P E(θ~ia ,θ~ib ) ⇒ ∀π6=i BR(θ~ia |π 6=i ) = BR(θ~ib |π 6=i )
Proof. Assume any arbitrary π 6=i , then
BR(θ~ia |π 6=i ) = arg max

XX

bi (s,γ 6=i |θ~ia )V (s,γ 6=i ,γi )

= arg max

XX

bi (s,γ 6=i |θ~ib )V (s,γ 6=i ,γi ) = BR(θ~ib |π 6=i ),

γi

γi

s

s

γ 6=i

γ 6=i

where Lemma 2 is employed to assert the equality of bi (·|θ~ia ) and bi (·|θ~ib ).
This theorem is key because it demonstrates that when two AOHs θ~ia ,θ~ib of an agent are
PE, then that agent need not discriminate between them now or in the future. Thus, when
searching the space of joint policies, we can restrict our search to those that assign the same
sub-tree policy γi to θ~ia and θ~ib . As such, it directly provides intuition as to why lossless
clustering is possible. Formally, we define the clustered joint policy space as follows.
Definition 8 (Clustered joint policy space). Let ΠC ⊆ Π be the subset of joint policies that
is clustered: i.e., each πi that is part of a π ∈ ΠC assigns the same sub-tree policy to action
observation histories that are probabilistically equivalent.
Corollary 1 (Existence of an optimal clustered joint policy). There exists an optimal joint
policy in the clustered joint policy space:
max V (π) = max V (π)

π∈ΠC

π∈Π

(3.11)

Proof. It is clear that the left hand side of (3.11) is upper bounded by the right hand side,
since ΠC ⊆ Π. Now suppose that π ∗ = arg maxπ∈Π V (π) has strictly higher value than the
best clustered joint policy. For at least one agent i and one pair of PE histories θ~ia , θ~ib , π ∗ must
assign different sub-tree policies γia 6= γib (otherwise π ∗ would be clustered). Without loss of
generality we assume that there is only one such pair. It follows directly from Theorem 2 that
from this policy we can construct a clustered policy π C ∈ ΠC (by assigning either γia or γib
to both θ~ia , θ~ib ) that is guaranteed to have value no less than π ∗ , thereby contradicting the
assumption that π ∗ has strictly higher value than the best clustered joint policy.
This formally proves that we can restrict our search to ΠC , the space of clustered joint
policies, without sacrificing optimality.
3.1.4 Clustering with Commitment in CBGs
Though it is now clear that two AOHs that are PE can be clustered, making this result
operational requires an additional step. To this end, we use the abstraction layer provided
by Bayesian games. Recall that in the CBG for a stage, the AOHs correspond to types.
468

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

Therefore, we want to cluster these types in the CBG. To accomplish the clustering of two
types θia ,θib , we introduce a new type θic to replace them, by defining:
∀θ6=i Pr(θic ,θ 6=i ) , Pr(θia ,θ 6=i ) + Pr(θib ,θ 6=i )
∀j ∀a

u(hθic ,θ 6=i i ,a) ,

Pr(θia ,θ 6=i )u(hθia ,θ 6=i i ,a) + Pr(θib ,θ 6=i )u( θib ,θ 6=i ,a)
.
Pr(θia ,θ 6=i ) + Pr(θib ,θ 6=i )

(3.12)
(3.13)

Theorem 3 (Reduction through commitment). Given that agent i in collaborative Bayesian
game B is committed to selecting a policy that assigns the same action for two of its types
θia ,θib , i.e., to selecting a policy βi such that βi (θia ) = βi (θib ), the CBG can be reduced without
loss in value for any agents. That is, the result is a new CBG B ′ in which agent i employs a
policy βi′ that reflects the clustering and whose expected payoff is the same as in the original
′
CBG: V B (βi′ ,β 6=i ) = V B (βi ,β 6=i ).
Proof. See appendix.
This theorem shows that, given that agent i is committed to taking the same action for
its types θia ,θib , we can reduce the collaborative Bayesian game B to a smaller one B ′ and
′
translate the joint CBG-policy β ′ found
 for B back to a joint CBG-policy β in B. This does
not necessarily mean that β = βi ,β 6=i is also a solution for B, because the best-response of
agent i against β 6=i may not select the same action for θia ,θib . Rather βi is the best-response
against β 6=i given that the same action needs to be taken for θia ,θib .11
Even though Theorem 3 only gives a conditional statement that depends on an agent being
committed to select the same action for two of its types, the previous subsection discussed
when a rational agent can make such a commitment. Combining these results gives the
following corollary.
Corollary 2 (Lossless Clustering with PE). Probabilistically equivalent histories θ~ia ,θ~ib can
be clustered without loss in heuristic value by merging them into a single type in a CBG.
Proof. Theorem 3 shows that, given that an agent i is committed to take the same action
for two of its types, those types can be clustered without loss in value. Since θ~ia ,θ~ib are PE,
they are best-response equivalent, which means that the agent is committed to use the same
sub-tree policy γi and hence the same action ai . Therefore we can directly apply clustering
without loss in expected payoff, which in a CBG for a stage of a Dec-POMDP means no loss
in expected heuristic value as given by (2.7).
Intuitively, the maximizing action is the same for θ~ia and θ~ib regardless of what (future)
joint policies π 6=i the other agents will use and hence we can cluster them without loss in
heuristic value. Note that this does not depend on which heuristic is used and hence also
holds for an optimal heuristic (i.e., when using an optimal Q-value function that gives the
true value). This directly relates probabilistic equivalence with equivalence in optimal value.12
11. Although we focus on CBGs, these results generalize to BGs with individual payoff functions. Thus, they
could potentially be exploited by algorithms for general-payoff BGs. Developing methods that do so is an
interesting avenue for future work.
12. The proof originally provided by Oliehoek et al. (2009) is based on showing that histories that are PE will
induce identical Q-values.

469

Oliehoek, Spaan, Amato, & Whiteson

Algorithm 6 ClusterCBG(B)
Input: CBG B
Output: Losslessly clustered CBG B
1: for each agent i do
2:
for each individual type θi ∈ B.Θi do
3:
if Pr(θi ) = 0 then
4:
B.Θi ← B.Θi \θi
5:
continue
6:
end if
7:
for each individual type θi′ ∈ B.Θi do
8:
isProbabilisticallyEquivalent ← true
9:
for all hs,θ 6=i i do
10:
if Pr(s,θ 6=i |θi ) 6= Pr(s,θ 6=i |θi′ ) then
11:
isProbabilisticallyEquivalent ← false
12:
break
13:
end if
14:
end for
15:
if isProbabilisticallyEquivalent then
16:
B.Θi ← B.Θi \θi′
17:
for each a ∈ A do
18:
for all θ 6=i do
19:
u(θi ,θ 6=i ,a) ← min(u(θi ,θ 6=i ,a),u(θi′ ,θ 6=i ,a))
20:
Pr(θi ,θ 6=i ) ← Pr(θi ,θ 6=i ) + Pr(θi′ ,θ 6=i )
21:
Pr(θi′ ,θ 6=i ) ← 0
22:
end for
23:
end for
24:
end if
25:
end for
26:
end for
27: end for
28: return B

{Prune θi from B:}

{Prune θi′ from B:}
{ take the lowest upper bound }

Note that this result establishes a sufficient, but not necessary condition for lossless clustering.
In particular, given policies for the other agents, many types are best-response equivalent and
can be clustered. However, as far as we know, the criterion must hold in order to guarantee
that two histories have the same best-response against any policy of the other agents.
3.2 GMAA* with Incremental Clustering
Knowing which individual histories can be clustered together without loss of value has the
potential to speed up many Dec-POMDP methods. In this article, we focus on its application
within the GMAA* framework.
Emery-Montemerlo et al. (2005) showed how clustering can be incorporated at every stage
in their algorithm: when the CBG for a stage t is constructed, a clustering of the individual
histories (types) is performed first and only afterwards is the (reduced) CBG solved. The same
approach can be employed within GMAA* by modifying the Expand procedure (Algorithm 5)
to cluster the CBG before calling GenerateAllChildrenForCBG.
Algorithm 6 shows the clustering algorithm. It takes as input a CBG and returns the
clustered CBG. It performs clustering by performing pairwise comparison of all types of each
470

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

b
Algorithm 7 ConstructExtendedBG(B,β t−1 , Q)

Input: A CBG B for stage t − 1, and the joint BG policy followed β t−1 .
b ~θ,a).
Input: An admissible heuristic of the form Q(
′
Output: CBG B for stage t.
1: B ′ ← B
{make a copy of B that we subsequently alter}
2: for each agent i do
3:
B ′ .Θi = ConstructExtendedTypeSet(i)
{overwrite the individual type sets}
4: end for
5: B ′ .Θ ← ×i∈D Θi
{the new joint type set (does not have to be explicitly stored)}
6: for each joint type θ = (θ t−1 ,at−1 ,ot ) ∈ B ′ .Θ do
7:
for each state st ∈ S do
8:
Compute Pr(st |θ)
{from Pr(st−1 |θ t−1 ) via Bayes’ rule }
9:
end for
10:
Pr(θ) ← Pr(ot |θ t−1 ,at−1 ) Pr(θ t−1 )
11:
for each a ∈ A do
12:
q←∞
13:
for each history ~θ t represented by θ do
b ~θ t ,a))
b we can take the lowest upper bound }
14:
q ← min(q,Q(
{ if Q∗ ≤ Q
15:
end for
16:
B ′ .u(θ,a) ← q
17:
end for
18: end for
19: return B ′

agent to see if they satisfy the criterion, yielding O(|Θi |2 ) comparisons for each agent i. Each
comparison involves looping over all hs,θ 6=i i (line 9). If there are many states, some efficiency
could be gained by first checking (3.2) and then checking (3.3). Rather than taking the
average as in (3.13), on line 19 we take the lowest payoff, which can be done if we are using
upper bound heuristic values.
The following theorem demonstrates that, when incorporating clustering into GMAA*,
the resulting algorithm is still guaranteed to find an optimal solution.
Theorem 4. When using a heuristic of the form (2.9) and clustering the CBGs in GMAA*
using the PE criterion, the resulting search method is complete.
Proof. Applying clustering does not alter the computation of lower bound values. Also,
heuristic values computed for the expanded nodes are admissible and in fact unaltered as
guaranteed by Corollary 2. Therefore, the only difference with regular GMAA* is that the
class of considered joint policies is restricted to ΠC , the class of clustered joint policies: not
all possible child nodes are expanded, because clustering effectively prunes away policies that
would specify different actions for AOHs that are PE and thus clustered. However, Corollary 1
guarantees that there exists an optimal joint policy in this restricted class.
The modification of the Expand proposed above is rather naive. To construct B(b0 ,ϕt )
it must first construct all |Oi |t possible AOHs for agent i (given the past policy ϕti ). The
subsequent clustering involves pairwise comparison of all these exponentially many types.
Clearly, this is not tractable for later stages.
However, because PE of AOHs propagates forwards (i.e., identical extensions of PE histories are also PE), a more efficient approach is possible. Instead of clustering this exponentially
471

Oliehoek, Spaan, Amato, & Whiteson

Algorithm 8 Expand-IC(q, H). The expand operator for GMAA*-IC.
Input: q = hϕt , v̂i the search node to expand.
b ~θ,a).
Input: H the admissible heuristic that is of the form Q(
Output: QExpand the set containing expanded child nodes.
1: B(ϕt−1 ) ← ϕt−1 .CBG
{retrieve previous CBG, note ϕt = (ϕt−1 , β t−1 )}
t−1 b
t
t−1
2: B(ϕ ) ← ConstructExtendedBG(B(ϕ
),β , Q)
3: B(ϕt ) ← ClusterBG(B(ϕt ))
4: ϕt .CBG ← B(ϕt )
{store pointer to this CBG}
5: QExpand ← GenerateAllChildrenForCBG(B(ϕt ))
6: return QExpand

growing set of types, we can simply extend the already clustered types of the previous stage’s
CBG, as shown in Algorithm 7. That is, given Θi , the set of types of agent i at the previous
stage t − 1, and βit−1 the policy agent i took at that stage, the set of types at stage t, Θ′i , can
be constructed as

Θ′i = θi′ = (θi ,βit−1 (θi ),oti ) | θi ∈ Θi ,oti ∈ Oi .
(3.14)
This means that the size of this newly constructed set is |Θ′i | = |Θi | · |Oi | . If the type set Θi at
the previous stage t − 1 was much smaller than the set of all histories |Θi | ≪ |Oi |t−1 , then the
new type set Θ′i is also much smaller: |Θ′i | ≪ |Oi |t . In this way, we bootstrap the clustering
at each stage and spend significantly less time clustering. We refer to the algorithm that
implements this type of clustering as GMAA* with Incremental Clustering (GMAA*-IC).
This approach is possible only because we perform an exact, value-preserving clustering for
which Lemma 1 guarantees that identical extensions will also be clustered without loss in
value. When performing the same procedure in a lossy clustering scheme (e.g., as in EmeryMontemerlo et al., 2005), errors might accumulate, and a better option might be to re-cluster
from scratch at every stage.
Expansion of a GMAA*-IC node takes exponential time with respect to both the number
of agents and types, as there are O(|A∗ |n|Θ∗ | ) joint CBG-policies and thus child nodes in the
GMAA*-IC search tree (A∗ is the largest action set and Θ∗ is the largest type set). Clustering
involves a pairwise comparison of all types of each agent and each of these comparisons needs
to check O(|Θ∗ |n−1 |S|) numbers for equality to verify (3.1). The total cost of clustering can
therefore be written as
O(n |Θ∗ |2 |Θ∗ |n−1 |S|),
which is only polynomial in the number of types. When clustering decreases the number of
types |Θ∗ |, it can therefore significantly reduce the number of child nodes and thereby the
overall time needed. However, when no clustering is possible, some overhead will be incurred.
3.3 Improved Heuristic Representation
Since clustering can reduce the number of types, GMAA*-IC has the potential to scale to
larger horizons. However, doing so has important consequences for the computation of the
heuristics. Previous research has shown that the upper bound provided by QMDP is often
too loose for effective heuristic search (Oliehoek, Spaan, & Vlassis, 2008). However, the
space needed to store tighter heuristics such as QPOMDP or QBG grows exponentially with the
horizon. Recall from Section 2.2.2 (see Fig. 5) that there are two approaches to computing
472

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

b with minimum size.
Algorithm 9 Compute Hybrid Q
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

Qh−1 ← {R1 , . . . ,R|A| }
z ← |A| × |S|
for t = h − 2 to 0 do
~ t | × |A|
y ← |Θ
if z < y then
V ← VectorBackup(Qt+1 )
V ′ ← Prune(V)
Qt ← V ′
z ← |V ′ | × |S|
end if
if z ≥ y then
Qt ← TreeBackup(Qt+1 )
end if
end for

{vector representation of last stage}
{the size of the |A| vectors}
{size of AOH representation}

{From now on z ≥ y}

QPOMDP or QBG . The first constructs a tree of all joint AOHs and their heuristic values, which
is simple to implement but requires storing a value for each (~θ t , a)-pair, the number of which
grows exponentially with t. The second approach maintains a vector-based representation, as
is common for POMDPs. Though pruning can provide leverage, in the worst case, no pruning
is possible and the number of maintained vectors grows doubly exponentially with h − t, the
number of stages-to-go. Similarly, the initial belief and subsequently reachable beliefs can be
used to reduce the number of vectors retained at each stage, but as the number of reachable
beliefs is exponential in the horizon the exponential complexity remains.
Oliehoek, Spaan, and Vlassis (2008) used a tree-based representation for the QPOMDP and QBG heuristics. Since the
computational cost of solving the Dec-POMDP was the bottleneck, the inefficiencies in the representation could be overlooked. However, this approach is no longer feasible for the
longer horizons made possible by GMAA*-IC.

Hybrid

t=0

To mitigate this problem, we propose a hybrid represent=1
tation for the heuristics, as illustrated in Fig. 6. The main
insight is that the exponential growth of the two existing representations occurs in opposite directions. Therefore, we can
t=2
use the low space-complexity side of both representations: the
later stages, which have fewer vectors, use a vector-based representation, while the earlier stages, which have fewer histot=3
ries, use a history-based representation. This is similar to
the idea of utilizing reachable beliefs to reduce the size of the Figure 6: An illustration of
vector representation described above but, rather than stor- the hybrid representation.
ing vectors for the appropriate AOHs at each step, only the
values are needed when using the tree-based representation.
Algorithm 9 shows how, under mild assumptions, a minimally-sized representation can be
computed. Starting from the last stage, the algorithm performs vector backups, switching to
tree backups when they become the smaller option. For the last time step h − 1, we represent
473

Oliehoek, Spaan, Amato, & Whiteson

Qt by the set of immediate reward vectors13 , and variable z (initialized on line 2) keeps track
of the number of parameters needed to represent Qt as vectors for the time step at hand.
Note that z depends on how effective the vector pruning is, i.e., how large the parsimonious
representation of the piecewise linear and convex value function is. Since this is problem
dependent, z can be updated only after pruning has actually been performed (line 9). By
contrast y, the number of parameters in a tree representation, can be computed directly from
the Dec-POMDP (line 4). When z > y, the algorithm switches to tree backups.14

4. Incremental Expansion
The clustering technique presented in the previous section has the potential to significantly
speed up planning if much clustering is possible. However, if little clustering is possible, the
number of children in the GMAA* search tree will still grow super-exponentially. This section
presents incremental expansion, a complementary technique to deal with this problem.
Incremental expansion exploits recent improvements in effectively solving CBGs. First
note that during the expansion of the last stage t = h − 1 for a particular ϕh−1 , we are only
interested in the best child (ϕh−1 ,δ h−1,∗ ), which corresponds to the optimal solution of the
Bayesian game δ h−1,∗ ↔ β ∗ . As such, for this last stage, we can use new methods for solving
CBGs (Kumar & Zilberstein, 2010b; Oliehoek, Spaan, Dibangoye, & Amato, 2010) that can
provide speedups of multiple orders of magnitude over brute force search (enumeration).15
Unfortunately, the improvements to GMAA* afforded by this approach are limited: in order
to guarantee optimality, it still relies on expansion of all (child nodes corresponding to all)
joint CBG-policies β for the intermediate stages, thus necessitating a brute-force approach.
However, many of the expanded child nodes may have low heuristic values Vb and may therefore
never be selected for further expansion.
Incremental expansion overcomes this problem because it exploits the following key observation: if we can generate the children in decreasing heuristic order using an admissible
heuristic, we do not have to expand all the children. As before, an A* search is performed
over partially specified policies and each new CBG is constructed by extending the CBG for
the parent node. However, rather than fully expanding (i.e., enumerating all the CBG policies
of and thereby constructing all children for) each search node, we instantiate an incremental
CBG solver for the corresponding CBG. This incremental solver returns only one joint CBG
policy at a time, which is then used to construct a single child ϕt+1 = (ϕt , β). By revisiting
the nodes, only the promising child nodes are expanded incrementally.
Below, we describe GMAA*-ICE, an algorithm that combines GMAA*-IC with incremental expansion. We establish theoretical guarantees and describe the modifications to
BaGaBaB, the CBG solver that GMAA*-ICE employs, that are necessary to deliver the
child nodes in decreasing order.
13. Only in exceptional cases where a short horizon is combined with large state and action spaces will representing the last time step as vectors not be minimal. In such cases, the algorithm can be trivially adapted.
14. This assumes that the vector representation will not shrink again for earlier stages. Although unlikely in
practice, such cases would prevent the algorithm from computing a minimal representation.
15. Kumar and Zilberstein (2010b) tackle a slightly different problem; they introduce a weighted constraint satisfaction approach to solving the point-based backup in dynamic programming for Dec-POMDPs. However,
this point-based backup can be interpreted as a collection of CBGs (Oliehoek et al., 2010).

474

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

4.1 GMAA* with Incremental Clustering and Expansion
We begin by formalizing incremental expansion and incorporating it into GMAA*-IC, yielding GMAA* with incremental clustering and expansion (GMAA*-ICE). At the core of
incremental expansion lies the following lemma:
Lemma 3. Given two joint CBG policies β, β ′ for a CBG B(b0 ,ϕt ), if Vb (β) ≥ Vb (β ′ ), then
for the corresponding child nodes Vb (ϕt+1 ) ≥ Vb (ϕt+1′ ).
Proof. This holds directly by the definition of Vb (ϕt ) as given by (2.8):
Vb (ϕt+1 ) = V 0...(t−1) (ϕt ) + Vb (β)
≥ V 0...(t−1) (ϕt ) + Vb (β ′ ) = Vb (ϕt+1′ ).

It follows directly that, if for B(b0 ,ϕt ) we use a CBG solver that can generate a sequence
of policies β, β ′ , . . . such that
Vb (β) ≥ Vb (β ′ ) ≥ . . .

then, for the sequence of corresponding children

Vb (ϕt+1 ) ≥ Vb (ϕt+1′ ) ≥ . . . .

Exploiting this knowledge, we can expand only the first child ϕt+1 and compute its heuristic
value Vb (ϕt+1 ) using (2.8). Since all the unexpanded siblings will have heuristic values less
than or equal to that, we can modify GMAA*-IC to reinsert the node q into the open list L
to act as a placeholder for all its non-expanded children.
Definition 9. A placeholder is a node for which at least one child has been expanded. A
placeholder has a heuristic value equal to its last expanded child.
Thus, after expansion of a search node q’s child, we update q.v̂, the heuristic value of the
node, to Vb (ϕt+1 ), the value of the expanded child, i.e., we set q.v̂ ← Vb (ϕt+1 ). As such, we
can reinsert q into L as a placeholder. As mentioned above, this is correct because all the
unexpanded siblings (for which the parent node q now is a placeholder) have heuristic values
lower than or equal to Vb (ϕt+1 ). Therefore the next sibling q ′ represented by the placeholder
is always expanded in time: q ′ is always created before nodes with lower heuristic value are
selected for further expansion. We keep track of whether a node is a previously expanded
placeholder or not.
As before, GMAA*-ICE performs an A* search over partially specified policies. As in
GMAA*-IC, each new CBG is constructed by extending the CBG for the parent node and
then applying lossless clustering. However, rather than expanding all children, GMAA*-ICE
requests only the next solution β of an incremental CBG solver, from which a single child
ϕt+1 = (ϕt , β) is constructed. In principle GMAA*-ICE can use any CBG solver that is able
to incrementally deliver all β in descending order of Vb (β). We propose a modification of the
BaGaBaB algorithm (Oliehoek et al., 2010), briefly discussed in Section 4.3.
Fig. 7 illustrates the process of incremental expansion in GMAA*-ICE, with ϕt indexed
by letters. First, a CBG solver for the root node ha, 7i is created, and the optimal solution β ∗ is
computed, with value 6. This results in a child hb, 6i, and the root is replaced by a placeholder
node ha, 6i. As per Definition 5 (the node comparison operator), b appears before a in the
475

Oliehoek, Spaan, Amato, & Whiteson

Legend:
ϕt
v̂

t

a
7

a
6

Root node

a
6

β′

β∗
b
6

t+1

b
4
β∗

New B(a), Vb =6
c
4

t+2
hϕt , v̂i
in open list

ha, 7i

a
5.5

New B(b), Vb =4

ha, 6i
hc, 4i
hb, 4i

hb, 6i
ha, 6i

b
4

c
4

d
5.5
Next solution of
B(a), Vb =5.5

hd, 5.5i
ha, 5.5i
hc, 4i
hb, 4i

Figure 7: Illustration of incremental expansion, with the nodes in the open list at the bottom.
Past joint policies ϕt are indexed by letters. Placeholder nodes are indicated by dashes.
open list and hence is selected for expansion. Its best child hc, 4i is added and hb, 6i is replaced
by placeholder hb, 4i. Now the search returns to the root node, and the second best solution β ′
is obtained from the CBG solver, leading to child hd, 5.5i. Placeholder nodes are retained as
long as they have unexpanded children; only their values are updated.
When using GMAA*-ICE, we can derive lower and upper bounds for the CBG solution,
which can be exploited by the incremental CBG solver. The incremental CBG solver for
B(ϕt ) can be initialized with lower bound
vCBG = vGM AA − V 0...(t−1) (ϕt ),

(4.1)

where vGM AA is the value of the current best solution, and V 0...(t−1) (ϕt ) is the true expected
value of ϕt over the first t stages. Therefore, vCBG is the minimum value that a candidate
must generate over the remaining h − t stages in order to beat the current best solution. Note
that each time the incremental CBG solver is queried for a solution, vCBG is re-evaluated
(using (4.1)), because vGM AA may have changed.
When the used heuristic faitfully represents the immediate reward (i.e., is of the form
(2.9)), then, for the last stage t = h − 1, we can also specify an upper bound for the solution
of the CBG
v̄CBG = Vb (ϕh−1 ) − V 0...(h−2) (ϕh−1 ).
(4.2)
If this upper bound is attained, no further solutions will be required from the CBG solver.
The upper bound holds since by (2.8)
Vb (β) , Vb (ϕh ) − V 0...(h−2) (ϕh−1 )

= V (ϕh ) − V 0...(h−2) (ϕh−1 )
≤ Vb (ϕh−1 ) − V 0...(h−2) (ϕh−1 ).

In the first step, Vb (ϕh ) = V (ϕh ), because ϕh is a fully specified policy and the heuristic value
given by (2.8) equals the actual value when a heuristic that faithfully represents the expected
476

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

Algorithm 10 Expand-ICE(q, H). The expand operator for GMAA*-ICE.
Input: q = hϕt , v̂i the search node to expand.
b ~θ,a).
Input: H the admissible heuristic that is of the form Q(
Output: QExpand the set containing 0 or 1 expanded child nodes.
1: if IsPlaceholder(q) then
2:
B(ϕt ) ← ϕt .CBG
{reuse stored CBG}
3: else
4:
B(ϕt−1 ) ← ϕt−1 .CBG
{retrieve previous CBG, note ϕt = (ϕt−1 , β t−1 )}
t−1
b
5:
B(ϕt ) ← ConstructExtendedBG(B(ϕt−1 ),β , Q)
t
t
6:
B(ϕ ) ← ClusterBG(B(ϕ ))
7:
B(ϕt ).Solver ← CreateSolver(B(ϕt ))
8:
ϕt .CBG ← B(ϕt )
{store pointer to this CBG}
9: end if
{set lower bound for CBG solution}
10: vCBG = vGM AA − V 0...(t−1) (ϕt )
11: if t = h − 1 then
12:
v̄CBG = Vb (ϕh−1 ) − V 0...(h−2) (ϕh−1 )
{upper bound only used for last stage CBG}
13: else
14:
v̄CBG = +∞
15: end if
b (β t )i ← B(ϕt ).Solver.NextSolution(vCBG ,v̄CBG )
{compute next CBG solution}
16: hβ t , V
t
17: if β then
18:
ϕt+1 ← (ϕt , β t )
{create partial joint policy}
t
t+1
0...t−1
t
b
b
19:
V (ϕ ) ← V
(ϕ ) + V (β )
{compute heuristic value}
20:
q ′ ← hϕt+1 , Vb (ϕt+1 )i
{create child node}
21:
QExpand ← {q ′ }
22: else
23:
QExpand ← ∅
{fully expanded: exists no solution s.t. V (β h−1 ) ≥ vCBG }
24: end if
25: return QExpand

Algorithm 11 PostProcessNode-ICE(q, L): Post processing of a node in GMAA*-ICE.
Input: q the last expanded node, L the open list.
Output: q is either removed or updated.
1: L.Pop(q)
2: if q is fully expanded or depth(q) = h − 1 then
3:
Cleanup q
{delete the node and the associated CBG and Solver}
4:
return
5: else
6:
c ← last expanded child of q
7:
q.v̂ ← c.v̂
{update heuristic value of parent node}
8:
IsPlaceholder(q) ← true
{remember that q is a placeholder}
9:
L.Insert(q)
{reinsert at appropriate position}
10: end if

477

Oliehoek, Spaan, Amato, & Whiteson

immediate reward is used. This implies that Vb (β) itself is a lower bound. In the second step
V (ϕh ) ≤ Vb (ϕh−1 ), because Vb (ϕh−1 ) is admissible. Therefore, we can stop expanding when
we find a β with (lower bound) heuristic value equal to the upper bound v̄CBG . This applies
only to the last stage because only then the first step is valid.
GMAA*-ICE can be implemented by replacing the Expand and the PostProcessNode
procedures of Algorithms 8 and 4 by Algorithms 10 and 11, respectively. Expand-ICE first
determines if a placeholder is being used and either reuses the previously constructed incremental CBG solver or constructs a new one. Then, new bounds are calculated and the next
CBG solution is obtained. Subsequently, only a single child node is generated (rather than
expanding all children as in Algorithm 13). PostProcessNode-ICE removes the last node
that was returned by Select only when all its children have been expanded. Otherwise, it
updates that node’s heuristic value and reinserts it in the open list. See Appendix A.2 for
GMAA*-ICE shown as a single algorithm.
4.2 Theoretical Guarantees
In this section, we prove that GMAA*-IC and GMAA*-ICE are search-equivalent. As a direct
result we establish that GMAA*-ICE is complete, which means that integrating incremental
expansion preserves the optimality guarantees of GMAA*-IC.
Definition 10. We call two GMAA* variants search-equivalent if they select exactly the
same sequence of non-placeholder nodes corresponding to past joint policies to expand in the
search tree using the Select operator.
For GMAA*-IC and GMAA*-ICE we show that the set of selected nodes are the same.
However, the set of expanded nodes can be different; in fact, it is precisely these differences
that incremental expansion exploits.
Theorem 5. GMAA*-ICE and GMAA*-IC are search-equivalent.
Proof. Proof is listed in Section A.4 of the appendix.
Note that Theorem 5 does not imply that the computational and space requirements
of GMAA*-ICE and GMAA*-IC are identical. On the contrary, for each expansion,
GMAA*-ICE generates only one child node to be stored on the open list. In contrast,
GMAA*-IC generates a number of child nodes that is, in the worst case, doubly exponential
in the depth of the selected node.16 However, GMAA*-ICE is not guaranteed to be more
efficient than GMAA*-IC. For example, in the case where all child nodes still have to be
generated, GMAA*-ICE will be slower due to the overhead it incurs.
Corollary 3. When using a heuristic of the form (2.9) GMAA*-ICE is complete.
Proof. Under the stated conditions, GMAA*-IC is complete (see Theorem 4).
GMAA*-ICE is search equivalent to GMAA*-IC, it is also complete.

Since

16. When a problem allows clustering, the number of child nodes grows less dramatically (see Section 3).

478

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

4.3 Incremental CBG Solvers
Implementing GMAA*-ICE requires a CBG solver that can incrementally deliver all β in
descending order of Vb (β). To this end, we propose to modify the Bayesian game Branch and
Bound (BaGaBaB) algorithm (Oliehoek et al., 2010). BaGaBaB performs an A*-search
over (partially specified) CBG policies. Thus, when applied within GMAA*-ICE, it performs
a second, nested A* search. To expand each node in the GMAA* search tree, a nested A*
search computes the next CBG solution.17 This section briefly summarizes the main ideas
behind BaGaBaB (for more information, see Oliehoek et al., 2010) and our modifications.
BaGaBaB works by creating a search tree in which the nodes correspond to partially
specified joint CBG policies. In particular, it represents a β as a joint action vector, a vector
hβ(θ 1 ), . . . ,β(θ |Θ| )i of the joint actions that β specifies for each joint type. Each node g in the
BaGaBaB search tree represents a partially specified vector and thus a partially specified
joint CBG policy. For example, a completely unspecified vector h·, · , . . . ,·i corresponds to
the root node, while an internal node g at depth d (root being at depth 0) specifies joint
actions for the first d joint types g = β(θ 1 ), . . . , β(θ d ), · , · , . . . ,· . The value of a node V (g)
is the value of the best joint CBG-policy consistent with it. Since this value is not known in
advance, BaGaBaB performs an A* search guided by an optimistic heuristic.
In particular, we can compute an upper bound on the value achievable for any such
partially specified vector by computing the maximum value of the complete information joint
policy that is consistent with it (i.e., a non-admissible joint policy that selects the maximizing
joint actions for the remaining joint types). Since this value is a guaranteed upper bound on
the maximum value achievable by a consistent joint CBG policy, it is an admissible heuristic.
We propose a modification to BaGaBaB to allow solutions to be incrementally delivered.
The main idea is to retain the search tree after a first call of BaGaBaB on a particular CBG
B(ϕt ) and update it during subsequent calls, thereby saving computational effort.
Standard A* search terminates when a single optimal solution has been found. This
behavior is the same when incremental BaGaBaB is called for the first time on a B(ϕt ).
However, during standard A*, nodes whose upper bound is lower than the best known lower
bound can be safely deleted, as they will never lead to an optimal solution. In contrast, in
an incremental setting such nodes cannot be pruned, as they could possibly result in the k-th
best solution and therefore might need to be expanded during subsequent calls to BaGaBaB.
Only nodes returned as solutions are pruned in order to avoid returning the same solution
twice. This modification requires more memory but does not affect the A* search process
otherwise.
When asked it for the k-th solution, BaGaBaB resets its internal lower bound to the value
of the next-best solution that was previously found but not returned (or to vCBG as defined in
(4.1) if no such solution was found). Then it starts an A* search initialized using the search
tree resulting from the (k − 1)-th solution. In essence, this method is similar to searching for
the best k solutions, where k can be incremented on demand. Recently it was shown that, for
fixed k, such a modification preserves all the theoretical guarantees (soundness, completeness,
17. While GMAA*-ICE could also use any other incremental CGB solver, there are few that avoid enumerating
all β before providing the first result and thus have the potential to work incrementally. An exception may
be the method of Kumar and Zilberstein (2010b), which employs AND/OR branch and bound search with
the EDAC heuristic (and is thus limited to the two-agent case). As a heuristic search method, it may be
amenable to an incremental implementation though to our knowledge this has not been attempted.

479

Oliehoek, Spaan, Amato, & Whiteson

optimal efficiency) of the A* algorithm (Dechter, Flerova, & Marinescu, 2012), but the results
trivially transfer to the setting where k is allowed to increase.

5. Experiments
In this section, we empirically test and validate all the proposed techniques: lossless clustering
of joint histories, incremental expansion of search nodes, and hybrid heuristic representations.
After introducing the experimental setup, we compare the performance of GMAA*-IC and
GMAA*-ICE to that of GMAA* on a suite of benchmark problems from the literature.
Next, we compare the performance of the proposed methods with state-of-the-art optimal
and approximate Dec-POMDP methods, followed by a case study of the scaling behavior
with respect to the number of agents. Finally, we compare memory requirements of the
hybrid heuristic representation to those of the tree and vector representations.
5.1 Experimental Setup
The most well-known Dec-POMDP benchmarks are the Dec-Tiger (Nair et al., 2003) and
BroadcastChannel (Hansen et al., 2004) problems. Dec-Tiger was discussed extensively
in Section 2. In BroadcastChannel, two agents have to transmit messages over a communication channel, but when both agents transmit at the same time a collision occurs that is
noisily observed by the agents. The FireFighting problem models a team of n firefighters
that have to extinguish fires in a row of nh houses (Oliehoek, Spaan, & Vlassis, 2008). Each
agent can choose to move to any of the houses to fight fires at that location; if two agents are
in the same house, they will completely extinguish any fire there. The (negative) reward of
the team of firefighters depends on the intensity of the fire at each house; when all fires have
been extinguished, reward of zero is received. In the Hotel 1 problem (Spaan & Melo, 2008),
travel agents need to assign customers to hotels with limited capacity. They can also send a
customer to a resort but this yields lower reward. In addition, we also use the following problems: Recycling Robots (Amato, Bernstein, & Zilberstein, 2007), a scaled-down version of
the problem described in Section 2; GridSmall with two observations (Amato, Bernstein, &
Zilberstein, 2006) and Cooperative Box Pushing (Seuken & Zilberstein, 2007a), a larger
two-robot benchmark. Table 1 summarizes these problems numerically, listing the number of
joint policies for different planning horizons.
Experiments were run on an Intel Core i5 CPU running Linux, and GMAA*, GMAA*-IC,
and GMAA*-ICE were implemented in the same code-base using the MADP Toolbox (C++)
(Spaan & Oliehoek, 2008). The vector-based QBG representation is computed using a variation of Incremental Pruning (adapted for computing Q-functions instead of regular value functions), corresponding to the NaiveIP method as described by Oliehoek and Spaan (2012).
To implement the pruning, we employ Cassandra’s POMDP-solve software (A. R. Cassandra,
1998).
For the results in Sections 5.2 and 5.3, we limited each process to 2Gb RAM and a
maximum CPU time of 3,600s. Reported CPU times are averaged over 10 independent runs
and have a resolution of 0.01s. Timings are given only for the MAA* search processes, since
480

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

problem primitives

Dec-Tiger

num. π for h

n

|S|

|Ai |

|Oi |

2

4

6

2

2

3

2

7.29e2

2.06e14

1.31e60

BroadcastChannel

2

4

2

2

6.40e1

1.07e9

8.51e37

GridSmall

2

16

5

2

1.563e4

9.313e20

1.175e88

Cooperative Box Pushing

2

100

4

5

1.68e7

6.96e187

1.96e4703

Recycling Robots

2

4

3

2

7.29e2

2.06e14

1.31e60

Hotel 1

2

16

3

4

5.90e4

1.29e81

3.48e1302

FireFighting

2

432

3

2

7.29e2

2.06e14

1.31e60

Table 1: Benchmark problem sizes and number of joint policies for different horizons.
computation of the heuristic is the same for both methods and can be amortized over multiple
runs.18 All problem definitions are available via http://masplan.org.
5.2 Comparing GMAA*, GMAA*-IC, and GMAA*-ICE
We compared GMAA*, GMAA*-IC, and GMAA*-ICE using the hybrid QBG representation. While all methods compute an optimal policy, we expect GMAA*-IC to be more efficient
than GMAA* when lossless clustering is possible. Furthermore, we expect GMAA*-ICE to
provide further improvements in terms of speedup and scaling to longer planning horizons.
The results are shown in Table 2. For all entries where we report results, the QBG heuristics
could be computed, thanks to the hybrid representation. Consequently, the performance of
GMAA*-IC is much better than all previously reported results, including those of Oliehoek
et al. (2009), who were often required to resort to QMDP for larger problems and/or horizons.
The entries marked by ‘§’ show the limits when using QMDP instead of QBG : in most of these
problems we can reach longer horizons with QBG . Only for FireFighting can GMAA*-ICE
with QMDP compute solutions for higher h than is possible with QBG (hence the missing “§”,
and showing that GMAA*-ICE is more efficient using a loose heuristic than GMAA*-IC).
Furthermore, the “†” entries indicate that the horizon to which we can solve a problem with
a tree-based QBG representation is often much shorter.
These results clearly illustrate that GMAA*-IC leads to a significant improvement in
performance. In all problems, GMAA*-IC was able to produce a solution more quickly and
to increase the largest solvable horizon over GMAA*. In some cases, GMAA*-IC is able to
drastically increase the solvable horizon.
Furthermore, the results clearly demonstrate that incremental expansion allows for significant additional improvements. In fact, the table demonstrates that GMAA*-ICE significantly outperforms GMAA*-IC, especially in problems where little clustering is possible.
The results in Table 2 also illustrate the efficacy of a hybrid representation. For problems
like GridSmall, Cooperative Box Pushing, FireFighting and Hotel 1 neither the
tree nor vector representation is able to provide a compact QBG heuristic for the longer hori18. The heuristics’ computation time ranges from less than a second to hours (for high h in some difficult
problems). Table 4 presents some heuristic computation time results.

481

Oliehoek, Spaan, Amato, & Whiteson

h
2
3
4
5
6
7

V ∗ TGMAA* (s)
Dec-Tiger
−4.000000
≤ 0.01
5.190812
§≤ 0.01
4.802755
563.09
7.026451
−
10.381625

TIC (s)

TICE (s)

h

≤ 0.01
≤ 0.01
§0.27
†21.03
−

≤ 0.01
≤ 0.01
≤ 0.01
§†0.02
46.43
∗

2
3
4
5
6
10
15
18
20
30
40
50
60
70
80

FireFighting hnh = 3,nf = 3i
2 −4.383496
0.09
≤ 0.01
≤ 0.01
3 −5.736969
§3.05
§0.11
0.10
4 −6.578834
1001.53 †950.51
1.00
5 −7.069874
−
−
†4.40
6 −7.175591
0.08
0.07
7
#
#
GridSmall
2
0.910000
≤ 0.01
≤ 0.01
≤ 0.01
3
1.550444
§0.90
§0.10
≤ 0.01
4
2.241577
*
†1.77 §†≤ 0.01
5
2.970496
−
0.02
6
3.717168
−
0.04
7
#
#
Hotel 1
2 10.000000
§≤ 0.0
≤ 0.01
≤ 0.01
3 16.875000
*
≤ 0.01
≤ 0.01
4 22.187500
§†≤ 0.01 §†≤ 0.01
5 27.187500
≤ 0.01
≤ 0.01
6 32.187500
≤ 0.01
≤ 0.01
7 37.187500
≤ 0.01
≤ 0.01
8 42.187500
≤ 0.01
≤ 0.01
9 47.187500
0.02
≤ 0.01
10
#
#
Cooperative Box Pushing
2
17.600000
§0.02
≤ 0.01
≤ 0.01
3
66.081000
∗
§†0.11 †≤ 0.01
4
98.593613
∗ §313.07
5
#
#

2
3
4
5
6
7
10
20
25
30
40
50
53
100
250
500
600
700
800
900
1000

V ∗ TGMAA* (s) TIC (s) TICE (s)
Recycling Robots
7.000000
≤ 0.01 ≤ 0.01
≤ 0.01
10.660125
§≤ 0.01 ≤ 0.01
≤ 0.01
13.380000
713.41 ≤ 0.01
≤ 0.01
16.486000
− †≤ 0.01 †≤ 0.01
19.554200
≤ 0.01
≤ 0.01
31.863889
≤ 0.01
≤ 0.01
47.248521
§≤ 0.01
≤ 0.01
56.479290
≤ 0.01 §≤ 0.01
62.633136
≤ 0.01
≤ 0.01
93.402367
0.08
0.05
124.171598
0.42
0.25
154.940828
2.02
1.27
185.710059
9.70
6.00
216.479290
−
28.66
−
−
BroadcastChannel
2.000000
≤ 0.01 ≤ 0.01
≤ 0.01
2.990000
≤ 0.01 ≤ 0.01
≤ 0.01
3.890000
§≤ 0.01 ≤ 0.01
≤ 0.01
4.790000
1.27 ≤ 0.01
≤ 0.01
5.690000
− ≤ 0.01
≤ 0.01
6.590000
†≤ 0.01 †≤ 0.01
9.290000
≤ 0.01
≤ 0.01
18.313228
≤ 0.01
≤ 0.01
22.881523
≤ 0.01
≤ 0.01
27.421850
≤ 0.01
≤ 0.01
36.459724
≤ 0.01
≤ 0.01
45.501604
≤ 0.01
≤ 0.01
48.226420
§≤ 0.01 §≤ 0.01
90.760423
≤ 0.01
≤ 0.01
226.500545
0.06
0.07
452.738119
0.81
0.94
543.228071
11.63
13.84
633.724279
0.52
0.63
−
−
814.709393
9.57
11.11
−
−

Table 2: Experimental results comparing regular GMAA*, GMAA*-IC, and GMAA*-ICE.
Listed are the computation times of GMAA* (TGMAA* ), GMAA*-IC (TIC ), and
GMAA*-ICE (TICE ), using the hybrid QBG representation. We use the following symbols:
‘−’ memory limit violations, ‘∗’ time limit overruns, ‘#’ heuristic computation exceeded memory or time limits, ‘§’ maximum planning horizon using QMDP , ‘†’ maximum planning horizon
using tree-based QBG . Bold entries indicate that only the methods proposed in this article
have computed these results.
zons. Apart from Dec-Tiger and FireFighting, computing and storing QBG (or another
tight heuristic) for longer horizons is the bottleneck to further scalability.
Together, these algorithmic improvements lead to the first optimal solutions for many
problem horizons. In fact, for the vast majority of problems tested, we provide results for
longer horizons than any previous work (the bold entries). These improvements are quite sub482

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

h
2
3
4
5
2
3
4
5
6
2
3
4
2
3
4
5
6
7
8
9
2
3

|BGh−1 |

|cBGt |
Dec-Tiger
4 1.0, 4.0
16 1.0, 4.0, 9.0
64 1.0, 4.0, 9.0, 23.14
256 1.0, 4.0, 9.0, 16.0, 40.43
FireFighting hnh = 3,nf = 3i
4 1.0, 4.0
16 1.0, 4.0, 16.0
64 1.0, 4.0, 16.0, 64.0
256 1.0, 4.0, 16.0, 64.0, 256.0
1024 1.0, 1.0, 2.0, 3.0, 6.0, 10.0
GridSmall
4 1.0, 4.0
16 1.0, 4.0, 10.50
64 1.0, 4.0, 10.50, 20.0
Hotel 1
16 1.0, 4.0
256 1.0, 4.0, 16.0
4096 1.0, 4.0, 8.0, 16.0
65536 1.0, 4.0, 4.0, 8.0, 16.0
1.05e6 1.0, 4.0, . . . , 4.0, 8.0, 16.0
1.68e7 1.0, 4.0, . . . , 4.0, 8.0, 16.0
2.68e8 1.0, 4.0, . . . , 4.0, 8.0, 16.0
4.29e9 1.0, 4.0, . . . , 4.0, 8.0, 16.0
Cooperative Box Pushing
25 1.0, 4.0
625 1.0, 4.0, 25.0

h
2
3
4
5
10
15
18
20
30
40
50
60
2
3
4
5
6
7
10
20
25
30
40
50
100
900

|BGh−1 | |cBGt |
Recycling Robots
4 1.0, remaining stages
16 1.0, remaining stages
64 1.0, remaining stages
256 1.0, remaining stages
262144 1.0, remaining stages
2.68e8 1.0, remaining stages
1.72e10 1.0, remaining stages
2.75e11 1.0, remaining stages
2.88e17 1.0, remaining stages
1.0, remaining stages
1.0, remaining stages
1.0, remaining stages
BroadcastChannel
4 1.0 (for all t)
16 1.0 (for all t)
64 1.0 (for all t)
256 1.0 (for all t)
1024 1.0 (for all t)
4096 1.0 (for all t)
262144 1.0 (for all t)
2.75e11 1.0 (for all t)
2.81e14 1.0 (for all t)
2.88e17 1.0 (for all t)
1.0 (for all t)
1.0 (for all t)
1.0 (for all t)
1.0 (for all t)

≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0

Table 3: Experimental results detailing the effectiveness of clustering. Listed are the size of
the CBGs for t = h − 1 without clustering (|BGh−1 |), and the average CBG size for all stages
with clustering (|cBGt |).
stantial, especially given that lengthening the horizon by one increases the problem difficulty
exponentially (cf. Table 1).
5.2.1 Analysis of Clustering Histories
Table 3 provides additional details about the performance of GMAA*-IC, by listing the
number of joint types in the GMAA*-IC search, |cBGt |, for each stage t. These are averages
since the algorithm forms CBGs for different past policies, leading to clusterings of different
sizes.19 To see the impact of clustering, the table also lists |BGh−1 |, the number of joint types
in the CBGs constructed for the last stage without clustering, which is constant.
In Dec-Tiger, the time needed by GMAA*-IC is more than 3 orders of magnitude less
than that of GMAA* for horizon h = 4. For h = 5, this test problem has 3.82e29 joint
policies, and no other method has been able to optimally solve it. GMAA*-IC, however,
is able to do so in reasonable time. In Dec-Tiger, there are clear symmetries between the
19. Note that in some problem domains we report smaller clusterings than Oliehoek et al. (2009). Due to an
implementation mistake, their clustering was overly conservative, and did not in all cases treat two histories
as probabilistically equivalent, when in fact they were.

483

Oliehoek, Spaan, Amato, & Whiteson

observations that allow for clustering, as demonstrated by Fig. 4. Another key property is
that opening the door resets the problem, which may also facilitate clustering.
In FireFighting, for short planning horizons no lossless clustering is possible at any
stage, and as such, the clustering incurs some overhead. However, GMAA*-IC is still faster
than GMAA* because constructing the BGs using bootstrapping from the previous CBG
takes less time than constructing a CBG from scratch. Interesting counterintuitive results
occur for h = 6, which was solved within memory limits, in contrast to h = 5. In fact, using
QMDP we could compute optimal values V ∗ for h > 6, and it turns out that these are equal
to that for h = 6. The reason is that the optimal joint policy is guaranteed to extinguish all
fires in 6 stages. For subsequent stages, all the rewards will be 0. While this itself does not
influence clustering, the further analysis of Table 3 reveals that the CBG instances encountered
during the h = 6 search happen to cluster much better than those in h = 5, which is possible
because the heuristics vary with the horizon. In fact, π ∗ for h = 6 sends both agents to the
middle house at t = 0, while for h = 5, agents are dispatched to different houses. When both
agents fight fires at the same house, the fire is extinguished completely, and resulting joint
observations do not provide any new information. As a result, different joint types lead to the
same joint belief, which means they can be clustered. If agents visit different houses, their
observations do convey information, leading to different possible joint beliefs (which cannot
be clustered).
Hotel 1 allows for a large amount of clustering, and GMAA*-IC outperforms GMAA*
by a large margin, with the former reaching h = 9 and the latter h = 2. This problem
is transition and observation independent (Becker, Zilberstein, Lesser, & Goldman, 2003;
Nair, Varakantham, Tambe, & Yokoo, 2005; Varakantham, Marecki, Yabu, Tambe, & Yokoo,
2007), which facilitates clustering, as we further discuss in Section 5.5. Unlike methods
specifically designed to exploit transition and observation independence, GMAA*-IC exploits
this structure without requiring a predefined explicit representation of it. Further scalability
is limited by the computation of the heuristic.
For BroadcastChannel, GMAA*-IC achieves an even more dramatic increase in performance, allowing the solution of up to horizon h = 900. Analysis reveals that the CBGs
constructed for all stages are fully clustered: they contain only one type for each agent. The
reason is as follows. When constructing a CBG for t = 1, there is only one joint type for the
previous CBG so, given β 0 , the solution for the previous CBG, there is no uncertainty with
respect to the previous joint action a0 . The crucial property of BroadcastChannel is that
the (joint) observation reveals nothing about the new state, but only about what joint action
was taken (e.g., ‘collision’ if both agents chose to ‘send’). As a result, the different individual
histories can be clustered. In a CBG constructed for stage t = 2, there is again only one joint
type in the previous game. Therefore, given the past policy, the actions of the other agents
can be perfectly predicted. Again the observation conveys no information so this process repeats. Thus, the problem has a special property which could be described as non-observable
given the past joint policy. GMAA*-IC automatically exploits this property. Consequently,
the time needed to solve each CBG does not grow with the horizon. The solution time, however, still increases super-linearly because of the increased amount of backtracking. As in
FireFighting, performance is not monotonic in the planning horizon. In this case however,
clustering is clearly not responsible for the difference. Rather, the only explanation is that
for certain horizons, there are many near-optimal joint policies, leading to more backtracking
and a higher search cost.
484

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

10

Nodes at depth t

10

Dec−Tiger, h=6 − Full Exp.
Dec−Tiger, h=6 − Inc. Exp.
GridSmall, h=6 − Full Exp.
GridSmall, h=6 − Inc. Exp.
FireFighting, h=5 − Full Exp.
FireFighting, h=5 − Inc. Exp.

5

10

0

10

0

1

2
t

3

4

Figure 8: Number of expanded partial joint policies ϕt for intermediate stages t = 0, . . . ,h − 2
(in log scale).

5.2.2 Analysis of Incremental Expansion
In Dec-Tiger for h = 5, GMAA*-ICE achieves a speedup of three orders of magnitude and
can compute a solution for h = 6, unlike GMAA*-IC. For GridSmall, it achieves a large
speedup for h = 4 and fast solutions for h = 5 and 6, where GMAA*-IC runs out of memory. Similar positive results are obtained for FireFighting, Cooperative Box Pushing
and Recycling Robots. In fact, when using QMDP , GMAA*-ICE is able to compute
solutions well beyond h = 1000 for the FireFighting problem, which stands in stark contrast to GMAA*-IC that only computes solutions to h = 3 with this heuristic. Note that
BroadcastChannel is the only problem for which GMAA*-IC is (slightly) faster than
GMAA*-ICE. Because this problem exhibits clustering to a single joint type, the overhead
of incremental expansion does not pay off.
To further analyze incremental expansion, we examined its impact on the number of nodes
expanded for intermediate stages t = 0, . . . ,h − 2. Fig. 8 shows the number of nodes expanded
in GMAA*-ICE and the number that would be expanded for GMAA*-IC (which can be
easily computed since they are search-tree equivalent). There is a clear relationship between
the results from Fig. 8 and Table 2, illustrating, e.g., why GMAA*-IC runs out of memory on
GridSmall h = 6. The plots confirm our hypothesis that, in practice, only a small number
of child nodes are queried.
5.2.3 Analysis of Hybrid Heuristic Representation
Fig. 9 illustrates the memory requirements in terms of number of parameters (i.e., real numbers) for the tree, vector, and hybrid representations for QBG , where the latter is computed
following Algorithm 9. Results for the vector representation are omitted when those representations grew beyond limits. The effectiveness of the vector pruning depends on the problem
and the complexity of the value function, which can increase suddenly, as for instance happens in Fig. 9c. These results show that, for several benchmark Dec-POMDPs, the hybrid
representation allows for significant savings in memory, allowing the computation of tight
heuristics for longer horizons.
485

Oliehoek, Spaan, Amato, & Whiteson

h

MILP

DP-LPC

DP-IPG

GMAA — QBG
IC

ICE

heur

BroadcastChannel, ICE solvable to h = 900
2
0.38
≤ 0.01
0.09
≤ 0.01
3
1.83
0.50
56.66
≤ 0.01
4
34.06
∗
*
≤ 0.01
5
48.94
≤ 0.01

≤ 0.01
≤ 0.01
≤ 0.01
≤ 0.01

≤ 0.01
≤ 0.01
≤ 0.01
≤ 0.01

Dec-Tiger, ICE
2
0.69
3
23.99
4
∗
5

≤ 0.01
≤ 0.01
≤ 0.01
0.02

≤ 0.01
≤ 0.01
0.03
0.09

solvable to h = 6
0.05
0.32
60.73
55.46
−
2286.38
−

≤ 0.01
≤ 0.01
0.27
21.03

FireFighting (2 agents, 3 houses, 3 firelevels), ICE
2
4.45
8.13
10.34
≤ 0.01
3
−
−
569.27
0.11
4
−
950.51
GridSmall, ICE solvable to h = 6
2
6.64
11.58
0.18
3
∗
−
4.09
4
77.44

0.01
0.10
1.77

Recycling Robots, ICE solvable to h = 70
2
1.18
0.05
0.30
≤ 0.01
3
*
2.79
1.07
≤ 0.01
4
2136.16
42.02
≤ 0.01
5
−
1812.15
≤ 0.01
Hotel
2
3
4
5
9
10
15

1, ICE solvable to h = 9
1.92
6.14
0.22
315.16
2913.42
0.54
−
−
0.73
1.11
8.43
17.40
283.76

Cooperative Box Pushing
2
3.56
15.51
3
2534.08 −
4
−

≤ 0.01
≤ 0.01
≤ 0.01
≤ 0.01
0.02
#

solvable to h ≫ 1000
≤ 0.01 ≤ 0.01
0.10
0.07
1.00
0.65
≤ 0.01
≤ 0.01
≤ 0.01

≤ 0.01
0.42
67.39

≤ 0.01
≤ 0.01
≤ 0.01
≤ 0.01

≤ 0.01
≤ 0.01
0.02
0.02

≤ 0.01
≤ 0.01
≤ 0.01
≤ 0.01
≤ 0.01
#

0.03
1.51
3.74
4.54
20.26

(QPOMDP ), ICE solvable to h = 4
1.07
≤ 0.01 ≤ 0.01 ≤ 0.01
6.43
0.91
0.02
0.15
1138.61
*
328.97 0.63

Table 4: Comparison of runtimes with other methods. Total time of the GMAA* methods
is given by taking the time from the method column (‘IC’ or ‘ICE’) and adding the heuristic
computation time (‘heur’). We use the following symbols: ‘−’ memory limit violations, ‘*’
time limit overruns, ‘#’ heuristic computation exceeded memory or time limits.

486

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

10

Memory required

5

10

0

10

5

10

0

1

2

3
4
Horizon

5

10

6

(a) Dec-Tiger.

5

10

0

10

2

3
4
Horizon

5

10

6

(d) Recycling Robots.

10

0

1 2 3 4 5 6 7 8 9
Horizon

15

10

Tree
Vector
Hybrid

10

10

5

10

0

1 2 3 4 5 6 7 8 9 10
Horizon

Tree
Vector
Hybrid

(c) Hotel 1.

15

Tree
Vector
Hybrid

Memory required

Memory required

10

10

1

10

20

(b) FireFighting.

15

10

10

Tree
Vector
Hybrid

10

10
Memory required

Memory required

10

Tree
Vector
Hybrid

Memory required

10

10

Tree
Vector
Hybrid

10

10

5

10

0

1 2 3 4 5 6 7 8 9 10
Horizon

(e) BroadcastChannel.

10

1

2

3
4
Horizon

5

6

(f) GridSmall.

Figure 9: Hybrid heuristic representation. The y-axis shows number of real numbers stored
for different representations of QBG for several benchmark problems (in log scale).
5.3 Comparing to Other Methods
In this section, we compare GMAA*-IC and GMAA*-ICE to other methods from the literature. We begin by comparing the runtimes of our methods against the following state-ofthe-art optimal Dec-POMDP methods: MILP20 (Aras & Dutech, 2010) converts the DecPOMDP to a mixed integer linear program, for which numerous solvers are available. We
have used MOSEK version 6.0. DP-LPC21 (Boularias & Chaib-draa, 2008) performs dynamic programming with lossless policy compression, with CPLEX 12.4 as the LP solver.
DP-IPG (Amato et al., 2009) performs exact dynamic programing with incremental policy
20. The results reported here deviate from those reported by Aras and Dutech (2010). For a number of
problems, Aras et al. employed a solution method that solves the MILP as a series (a tree) of smaller
MILPs by branching on the continuous realization weight variables for earlier stages. That is, for each past
joint policy ϕt for some stage t, they solve a different MILP involving the subset of consistent sequences.
Additionally, for FireFighting and GridSmall, we use the benchmark versions standard to the literature
(Oliehoek, Spaan, & Vlassis, 2008; Amato et al., 2006), whereas Aras and Dutech (2010) use non-standard
versions. This explains the difference between our results and the ones reported in their article (personal
communication, Raghav Aras).
21. The goal of Boularias and Chaib-draa (2008) was to find non-dominated joint policies for all initial beliefs.
The previously reported results concerned run-time to compute the non-dominated joint policies, without
performing pruning on the full-length joint policies. In contrast, we report the time needed to compute the
actual optimal Dec-POMDP policy (given b0 ). This additionally requires the final round of pruning and
subsequently computing the value for each of the remaining joint policies for the initial belief. This additional overhead explains the differences in run time between what we report here and what was previously
reported (personal communication, Abdeslam Boularias).

487

Oliehoek, Spaan, Amato, & Whiteson

Problem
Dec-Tiger
Cooperative Box Pushing
GridSmall

h
6
3
5

m
7
3
3

VMBDP
9.91
53.04
2.32

V∗
10.38
66.08
2.97

Table 5: Comparison of optimal (V ∗ ) and approximate (VMBDP ) values.

generation that exploits known start state and knowledge about what states are reachable in
doing the DP backup.
Table 4, which shows the results of the comparison, demonstrates that, in almost all cases,
the total time of GMAA*-ICE (given by the sum of heuristic computation time and the time
for the GMAA*-phase) is significantly less than that of any other state-of-the-art methods.
Moreover, as demonstrated in Table 2, GMAA*-ICE can compute solutions for longer horizons for all these problems, except for Cooperative Box Pushing and Hotel 1.22 For
these problems, it is not possible to compute QBG for longer horizons. Overcoming this
problem could enable GMAA*-ICE to scale to further horizons as well.
The DP-LPC algorithm proposed by Boularias and Chaib-draa (2008) also improves the
efficiency of optimal solutions by a form of compression. The performance of their algorithm,
however, is weaker than that of GMAA*-IC. There are two main explanations for the performance difference. First, DP-LPC uses compression to more compactly represent the values
for sets of useful sub-tree policies, by using sequence form representation. The policies themselves, however, are not compressed: they still specify actions for every possible observation
history (for each policy it needs to select an exponential amount of sequences that make up
that policy). Hence, it cannot compute solutions for long horizons. Second, GMAA*-IC can
exploit knowledge of the initial state distribution b0 .
Overall, GMAA*-ICE substantially improves the state-of-the-art in optimally solving
Dec-POMDPs. Previous methods typically improved the feasible solution horizon by just
one (or only provided speed-ups for horizons that could already be solved). By contrast,
GMAA*-ICE dramatically extends the feasible solution horizon for many problems.
We also consider MBDP-based approaches, the leading family of approximate algorithms.
Table 5, which reports the VMBDP values produced by PBIP-IPG (Amato et al., 2009) (with
typical ‘maxTrees’ parameter setting m), demonstrates that the optimal solutions produced by
GMAA*-IC or GMAA*-ICE are of higher quality. PBIP-IPG was chosen because all other
MBDP algorithms with the same parameters achieve at most the same value. While not
exhaustive, this comparison illustrates that even the best approximate Dec-POMDP methods
in practice provide inferior joint policies on some problems. Conducting such analysis is
possible only if optimal solutions can be computed. Clearly, the more data that becomes
available, the more thorough the comparisons that can be made. Therefore, scalable optimal
solution methods such as GMAA*-ICE are critical for improving these analyses.
488

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

problem primitives

num. π for h

n

|S|

|A|

|O|

2

4

6

2

27

4

4

64

1.07e9

8.50e37

3

81

8

8

512

3.51e13

7.84e56

4

243

16

16

4.09e3

1.15e18

7.23e75

5

729

32

32

3.27e4

3.77e22

6.67e94

6

2187

64

64

2.62e5

9.80e55

6.15e113

Table 6: FireFightingGraph: the number of joint policies for different numbers of agents
and horizons, with 3 possible fire levels.
5.4 Scaling to More Agents
All of the benchmark problems in our results presented so far were limited to two agents. Here,
we present a case study on FireFightingGraph (Oliehoek, Spaan, Whiteson, & Vlassis,
2008), a variation of FireFighting allowing for more agents, in which each agent can only
fight fires at two houses, instead of at all of them. Table 6 highlights the size of these
problems, including the total number of joint policies for different horizons. We compared
GMAA*, GMAA*-IC, GMAA*-ICE (all using a QMDP heuristic), BruteForceSearch,
and DP-IPG, with a maximum run-time of 12 hours and running on an Intel Core i7 CPU,
averaged over 10 runs. BruteForceSearch is a simple optimal algorithm that enumerates
and evaluates all joint policies, and was implemented in the same codebase as the GMAA*
variations. DP-IPG results use the original implementation and were run on an Intel Xeon
computer. Hence, while the timing results are not directly comparable, the overall trends are
apparent. Also, since the DP-IPG implementation is limited to 2 agents, no results are shown
for more agents.
Fig. 10 shows the computation times for FireFightingGraph across different numbers of
of agents and planning horizons, while Table 7 lists the optimal values obtained. As expected,
the baseline BruteForceSearch performs very poorly, only scaling beyond h = 2 for 2
agents, while DP-IPG can only reach h = 4. On the other hand, regular GMAA* performs
relatively well, scaling to a maximum of 5 agents. However, GMAA*-IC and GMAA*-ICE
improve the efficiency of GMAA* by 1–2 orders of magnitude. As such, they substantially
outperform the other three methods, and scale up to 6 agents. The benefit of incremental expansion is clear for n = 3,4, where GMAA*-ICE can reach a higher horizon than GMAA*-IC.
Hence, although this article focuses on scalability in the horizon, these results show that the
methods we propose can also improve scalability in the number of agents.
5.5 Discussion
Overall, the empirical results demonstrate that incremental clustering and expansion offers
dramatic performance gains on a diverse set of problems. In addition, the results on Broad22. In Hotel 1, DP-IPG performs particularly well because the problem structure has limited reachability.
That is, each agent can fully observe its local state (but not that of the other agent) and in all local states
except one there is one action that dominates all others. As a result, DP-IPG can generate a small number
of possibly optimal policies.

489

Oliehoek, Spaan, Amato, & Whiteson

4

4

10

10

3

2

10

1

10

0

10

−1

10

−2

10

−3

10

6

5

4

3

2

2

3

4

5

6

7

8

9

10

3

10

computation time (s)

computation time (s)

computation time (s)

4

10

3

10

2

10

1

10

0

10

−1

10

−2

10

−3

10

6

5

4

h

#agents

3

2

2

3

5

4

7

6

8

9

10

10

2

10

1

10

0

10

−1

10

−2

10

−3

10

6

5

h

#agents

(a) GMAA* results.

3

2

2

3

4

5

7

8

9

10

h

#agents

(b) GMAA*-IC results.

4

(c) GMAA*-ICE results.

4

10

10

3

10

2

10

1

10

0

10

−1

10

−2

10

−3

10

6

5

4

3

2

2

3

4

5

6

7

8

9

10

computation time (s)

3

computation time (s)

4

6

10

2

10

1

10

0

10

−1

10

−2

10

−3

10

6
h

#agents

5

4

3

2

2

3

4

5

6

7

8

9

10

h

#agents

(d) BruteForceSearch results.

(e) DP-IPG results.

Figure 10: Comparison of GMAA*, GMAA*-IC, GMAA*-ICE, BruteForceSearch,
and DP-IPG on the FireFightingGraph problem. Shown are computation time (in log
scale) for various number of agents and horizons. Missing bars indicate that the method
exceeded time or memory limits. However, the DP-IPG implementation only supports 2
agents.

h
2
3
4
5
≥6

n=2
−4.394252
−5.806354
−6.626555
−7.093975
−7.196444

n=3
−5.213685
−6.654551
−7.472568

n=4
−6.027319
−7.391423
−8.000277

n=5
−6.846752

n=6
−7.666185

Table 7: Value V ∗ of optimal solutions to the FireFightingGraph problem, for different
horizons and numbers of agents.

castChannel illustrate a key advantage of our approach: when a problem possesses a property that makes a large amount of clustering possible, our clustering method exploits this
property automatically, without requiring a predefined explicit representation of it.
490

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

Of course, not all problems admit great reductions via clustering. One domain property
that allows for clustering is when the past joint policy encountered during GMAA* makes the
observations superfluous, as with BroadcastChannel and FireFighting. In Dec-Tiger,
we see that certain symmetries can lead to clustering. However clustering can occur even
without these properties. In fact, for all problems and nearly all horizons that we tested, the
size of the CBGs can be reduced. Moreover, in accordance with the analysis of Section 3.2,
the improvements in planning efficiency are huge, even for modest reductions in CBG size.
One class of problems where we can say something a priori about the amount of clustering
that is possible is the class of Dec-POMDPs with transition and observation independence
(Becker et al., 2003). In such problems, the agents have local states and the transitions are
independent, which for two agents can be expressed as
Pr(s′1 , s′2 |s1 , s2 , a1 , a2 ) = Pr(s′1 |s1 , a1 ) Pr(s′2 |s2 , a2 ).

(5.1)

Similarly, the observations are assumed to be independent, which means that for each agent
the observation probability depends only on its own action and local state: Pr(oi |ai , s′i ). For
such problems, the probabilistic equivalence criterion (3.1) factors too. In particular, due to
transition and observation independence23 , (3.2) holds true for any θ~ia ,θ~ib . Moreover, (3.3)
factors as the product Pr(s1 , s2 |θ~1 , θ~2 ) = Pr(s1 |θ~1 ) Pr(s2 |θ~2 ) and thus holds if Pr(s1 |θ~1a ) =
Pr(s1 |θ~1b ). That is, two histories can be clustered if they induce the same ‘local belief’. As
such, the size of the CBGs directly corresponds to the product of the number of reachable local
beliefs. Since the transition and observation independent Hotel 1 problem is also locally
fully observable, and the local state spaces consist of four states, there are only four possible
local beliefs (which is consistent with the CBG size of 16 from Table 3). Moreover, we see
that this maximum size is typically only reached at the end of search. This is because good
policies defer sending customers to the hotel and thus do not visit local states where the hotel
is filled in the earlier stages.
In more general classes of problems, even other weakly coupled models (e.g., Becker,
Zilberstein, & Lesser, 2004; Witwicki & Durfee, 2010), the criterion (3.1) does not factor,
and hence there is no direct correspondence to the number of local beliefs. As such, only
by applying our clustering algorithm can we determine how well such a problem clusters.
This is analogous to, e.g., state aggregation in MDPs (e.g., discussed in Givan, Dean, &
Greig, 2003) where it is not known how to predict a priori how large a minimized model will
be. Fortunately, our empirical results demonstrate that, in domains that admit little or no
clustering, the overhead is small.
As expected, incremental expansion is most helpful for problems which do not allow for
much clustering. However, the results for, e.g., Dec-Tiger illustrate that there is a limit to
the amount of scaling that the method can currently provide. The bottleneck is the solution
of the large CBGs for the later stages: the CBG solver has to solve these large CBGs when
returning the first solution in order to guarantee optimality, but this takes takes a long time.
We expect that further improvements to CBG solvers can directly add to the efficacy of
incremental expansion.
Our experiments also clearly demonstrate that the Dec-POMDP complexity results, while
important, are only worst-case results. In fact, the scalability demonstrated in our experiments
clearly show that in many problems we successfully scale dramatically beyond what would be
23. This assumes no ‘external’ state variable s0 .

491

Oliehoek, Spaan, Amato, & Whiteson

expected for a doubly-exponential dependence on the horizon. Even for the smallest problems,
a doubly-exponential scaling in the horizon implies that it is impossible to compute solutions
beyond h = 4 at all, as indicated by the following simple calculation: let n = 2, |Ai | = 2
actions, |Oi | = 2| observations, then
5

|Ai |(n∗(|Oi |

))

4

/|Ai |(n∗(|Oi |

))

= 4.2950e9.

Thus, even in the simplest possible case, we see an increase of a factor 4.2950e09 from h = 4
to h = 5. Similarly, the next increment, from h = 5 to h = 6, increases the size of the search
space by a factor 1.8447e19. However, our experiments clearly indicate that in almost all
cases, things are not so dire. That is, even though matters look bleak in the light of the
complexity results, we are in many cases able to perform substantially better than this worst
case.

6. Related Work
In this section, we discuss a number of methods that are related to those proposed in this
article. Some of these methods have already been discussed in earlier sections. In Section 3, we
indicated that our clustering method is closely related to the approach of Emery-Montemerlo
et al. (2005) but is also fundamentally different because our method is lossless. In Section 5.3,
we discussed connections to the approach of Boularias and Chaib-draa (2008) which clusters
policy values. This contrasts with our approach which clusters the histories and thus the
policies themselves, leading to greater scalability.
In Section 3.1.2, we discussed the relationship between our notion of probabilistic equivalence (PE) and the multiagent belief. However, there is yet another notion of belief, employed
in the JESP solution method (Nair et al., 2003), that is superficially more similar to the PE
distribution. A ‘JESP belief’ for an AOH θ~i is a probability distribution Pr(s,~o6=i |θ~i , b0 , π 6=i )
over states and observation histories of other agents given a (deterministic) full policy of all
the other agents. It is a sufficient statistic, since it induces a multiagent belief, thus it also
allows for the clustering of histories. The crucial difference with, and the utility of, PE lies in
the fact that the PE criterion is specified over states and AOHs given only a past joint policy.
That is, (3.1) does not induce a multiagent belief.
Our clustering approach also resembles a number of methods that employ other equivalence
notions. First, several approaches exploit the notion of behavioral equivalence (Pynadath &
Marsella, 2007; Zeng et al., 2011; Zeng & Doshi, 2012). They consider, from the perspective
of a protagonist agent i, the possible models of another agent j. Since j affects i only through
its actions, i.e., its behavior, agent i can cluster together all the models of agent j that lead
to the same policy πj for that agent. That is, it can cluster all models of agent j that are
behaviorally equivalent. In contrast, we do not cluster models of other agents j, but histories
of this agent i if all the other agents, as well as the environment, are guaranteed to behave the
same in expectation, thus leading to the same best response of agent i. That is, our method
could be seen as clustering histories that are ‘expected environmental behavior equivalent’.
The notion of utility equivalence (Pynadath & Marsella, 2007; Zeng et al., 2011) is closer to
PE because it also takes into account the (value of the) best-response of agent i (in particular,
it clusters two models mj and m′j if using BR(mj )—the best response against mj — achieves
the same value against m′j ). However, it remains a form of behavior equivalence in that it
clusters models of other agents, not histories of the protagonist agent.
492

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

There are also connections between PE and work on influence-based abstraction (Becker et
al., 2003; Witwicki & Durfee, 2010; Witwicki, 2011; Oliehoek et al., 2012), since the influence
(or point in parameter space, Becker et al., 2003) is a compact representation of the other
agents’ policies. Models of the other agents can be clustered if they lead to the same influence
on agent i. However, though more fine-grained, this is ultimately still a form of behavioral
equivalence.
A final relation to our equivalence notion is the work by Dekel, Fudenberg, and Morris
(2006), which constructs a distance measure and topology on the space of types with the
goal of approximating the infinite universal type space (the space of all possible beliefs about
beliefs about beliefs, etc.) for one-shot Bayesian games. Our setting, however, considers a
simple finite type space where the types directly correspond to the private histories (in the
form of AOHs) in a sequential problem. Thus, we do not need to approximate the universal
type space; instead we want to know which histories lead to the same future dynamics from
the perspective of an agent. Dekel et al.’s topology does not address this question.
Our incremental expansion technique is related to approaches extending A∗ to deal with
large branching factors in the context of multiple sequence alignment (Ikeda & Imai, 1999;
Yoshizumi, Miura, & Ishida, 2000). However, our approach is different because we do not
discard unpromising nodes but rather provide a mechanism to generate only the necessary
ones. Also, when proposing MAA*, Szer et al. (2005) developed a superficially similar approach that could be applied only to the last stage. In particular, they proposed generating
the child nodes one by one, each time checking if a child is found with value equal to its
parent’s heuristic value. Since the value of such a child specifies a full policy, its value is
a lower bound and therefore expansion of any remaining child nodes can be skipped. Unfortunately, a number of issues prevent this approach from providing substantial leverage in
practice. First, it cannot be applied to intermediate stages 0 ≤ t < h − 1 since no lower bound
values for the expanded children are available. Second, in many problems it is unlikely that
such a child node exists. Third, even if it does, Szer et al. did not specify an efficient way of
finding it. Incremental expansion overcomes all of these issues, yielding an approach that, as
our experiments demonstrate, significantly increases the size of the Dec-POMDPs that can
be solved optimally.
This article focuses on optimal solutions for Dec-POMDPs over a finite horizon. As part
of our evaluation, we compare against the MILP approach (Aras & Dutech, 2010), DPILP (Boularias & Chaib-draa, 2008) and DP-IPG (Amato et al., 2009), an extension of the
exact dynamic programming algorithm (Hansen et al., 2004). Research on finite-horizon DecPOMDPs has considered many other approaches such as bounded approximations (Amato,
Carlin, & Zilberstein, 2007), locally optimal solutions (Nair et al., 2003; Varakantham, Nair,
Tambe, & Yokoo, 2006) and approximate methods without guarantees (Seuken & Zilberstein,
2007b, 2007a; Carlin & Zilberstein, 2008; Eker & Akın, 2010; Oliehoek, Kooi, & Vlassis, 2008;
Dibangoye et al., 2009; Kumar & Zilberstein, 2010b; Wu et al., 2010a; Wu, Zilberstein, &
Chen, 2010b).
In particular, much research has considered the optimal and/or approximate solution of
subclasses of Dec-POMDPs. One such subclass contains only Dec-POMDPs in which the
agents have local states that other agents cannot influence. The resulting models, such as the
TOI-Dec-MDP (Becker et al., 2003; Dibangoye, Amato, Doniec, & Charpillet, 2013) and NDPOMDP (Nair et al., 2005; Varakantham et al., 2007; Marecki, Gupta, Varakantham, Tambe,
& Yokoo, 2008; Kumar & Zilberstein, 2009), can be interpreted as independent (PO)MDPs for
493

Oliehoek, Spaan, Amato, & Whiteson

each agent that are coupled through the reward function (and possibly an unaffectable state
feature). On the other hand, event-driven interaction models (Becker et al., 2004) consider
agents that have individual rewards but can influence each other’s transitions.
More recently, models that allow for limited transition and reward dependence have been
introduced. Examples are interaction-driven Markov games (Spaan & Melo, 2008), DecMDPs with sparse interactions (Melo & Veloso, 2011), distributed POMDPs with coordination locales (Varakantham et al., 2009; Velagapudi et al., 2011), event-driven interactions with
complex rewards (EDI-CR) (Mostafa & Lesser, 2011), and transition decoupled Dec-POMDPs
(Witwicki & Durfee, 2010; Witwicki, 2011). While the methods developed for these models often exhibit better scaling behavior than methods for standard Dec-(PO)MDPs, they typically
are not suitable when agents have extended interactions, e.g., to collaborate in transporting
an item. Also, there have been specialized models that consider the timing of actions whose
ordering is already determined (Marecki & Tambe, 2007; Beynier & Mouaddib, 2011).
Another body of work addresses infinite-horizon problems (Amato, Bernstein, & Zilberstein, 2010; Amato, Bonet, & Zilberstein, 2010; Bernstein, Amato, Hansen, & Zilberstein,
2009; Kumar & Zilberstein, 2010a; Pajarinen & Peltonen, 2011), in which it is not possible to
represent a policy as a tree. These approaches represent policies using finite-state controllers
that are then optimized in various ways. Also, since the infinite-horizon case is undecidable
(Bernstein et al., 2002), the approaches are approximate or optimal given a particular controller size. While there exists a boundedly optimal approach that can theoretically construct
a controller within any ǫ of optimal, it is only feasible for very small problems or a large ǫ
(Bernstein et al., 2009).
There has also been great interest in Dec-POMDPs that explicitly take into account communication. Some approaches try to optimize the meaning of communication actions without
semantics (Xuan, Lesser, & Zilberstein, 2001; Goldman & Zilberstein, 2003; Spaan, Gordon,
& Vlassis, 2006; Goldman, Allen, & Zilberstein, 2007) while others use fixed semantics (e.g.,
broadcasting the local observations) (Ooi & Wornell, 1996; Pynadath & Tambe, 2002; Nair et
al., 2004; Roth et al., 2005; Oliehoek, Spaan, & Vlassis, 2007; Roth, Simmons, & Veloso, 2007;
Spaan, Oliehoek, & Vlassis, 2008; Goldman & Zilberstein, 2008; Becker, Carlin, Lesser, & Zilberstein, 2009; Williamson, Gerding, & Jennings, 2009; Wu et al., 2011). Since models used
in the first category (e.g., the Dec-POMDP-Com) can be converted to normal Dec-POMDPs
(Seuken & Zilberstein, 2008), the contributions of this article are applicable to those settings.
Finally, there are numerous models closely related to Dec-POMDPs, such as POSGs
(Hansen et al., 2004), interactive POMDPs (I-POMDPs) (Gmytrasiewicz & Doshi, 2005),
and their graphical counterparts (Doshi, Zeng, & Chen, 2008). These models are more general in the sense that they consider self-interested settings where each agent has an individual
reward function. I-POMDPs are conjectured to also require doubly exponential time (Seuken
& Zilberstein, 2008). However, for the I-POMDP there have been a number of recent advances
(Doshi & Gmytrasiewicz, 2009). The current paper makes a clear link between best-response
equivalence of histories and the notion of best-response equivalence of beliefs in I-POMDPs.
In particular, this article demonstrates that two PE action-observation histories (AOHs) induce, given only a past joint policy, a distribution over states and AOHs of other agents, and
therefore will induce the same multiagent belief for any future policies of other agents. These
induced multiagent beliefs, in turn, can be interpreted as special cases of I-POMDP beliefs
where the model of the other agents are sub-intentional models in the form of a fixed policy
tree. Rabinovich and Rosenschein (2005) introduced a method that, rather than optimizing
494

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

the expected value of a joint policy, selects coordinated actions under uncertainty by tracking
the dynamics of an environment. This approach, however, requires a model of the ideal system
dynamics as input and in many problems, such as those considered in this article, identifying
such dynamics is difficult.

7. Future Work
Several avenues for future work are made possible by the research presented in this article.
Perhaps the most promising is the development of new approximate Dec-POMDP algorithms.
While this article focused on optimal methods, GMAA*-ICE can also be seen as a framework for approximate methods. Such methods could be derived by limiting the amount of
backtracking, employing approximate CBG solvers (Emery-Montemerlo, Gordon, Schneider,
& Thrun, 2004; Kumar & Zilberstein, 2010b; Wu et al., 2010a), integrating GMAA* methods for factored Dec-POMDPs (Oliehoek, Spaan, Whiteson, & Vlassis, 2008; Oliehoek, 2010;
Oliehoek et al., 2013), performing lossy clustering (Emery-Montemerlo, 2005; Wu et al., 2011)
or using bounded approximations for the heuristics. In particular, it seems promising to combine approximate clustering with approximate factored GMAA* methods.
Lossy clustering could be achieved by generalizing the probabilistic equivalence criterion,
which is currently so strict that little or no clustering may be possible in many problems. An
obvious approach is to cluster histories for which the distributions over states and histories of
other agents are merely similar, as measured by, e.g., Kullback-Leibler divergence. Alternately,
histories could be clustered if they induce the same individual belief over states:
Pr(s|θ~i ) =

X

Pr(s,~
θ 6=i |θ~i ).

(7.1)

~
θ 6=i

While individual beliefs are not sufficient statistics for history, we hypothesize that they
constitute effective metrics for approximate clustering. Since the individual belief simply
marginalizes out the other agents’ histories from the probabilities used in the probabilistic
equivalence criterion, it is an intuitive heuristic metric for approximate clustering.
While this article focuses on increasing scalability with respect to the horizon, developing
techniques to deal with larger number of agents is an important direction of future work. We
plan to further explore performing GMAA* using factored representations (Oliehoek, Spaan,
Whiteson, & Vlassis, 2008). In that previous work, we could only exploit the factorization at
the last stage, since earlier stages required full expansions to guarantee optimality. However,
for such larger problems, the number of joint BG policies (i.e., number of child nodes) is
directly very large (earlier stages are more tightly coupled); therefore incremental expansion
is crucial to improving the scalability of optimal solution methods with respect to the number
of agents.
Another avenue for future work is to further generalize GMAA*-ICE. In particular, it
may be possible to flatten the two nested A∗ searches into a single A∗ search. Doing so
could lead to significant savings as it would obviate the need to solve an entire CBG before
expanding the next one. In our work, we employed the plain A∗ algorithm as a basis, but a
promising direction of future work is to investigate what A∗ enhancements from the literature
(Edelkamp & Schrödl, 2012) can benefit GMAA* most. In particular, as we described in
our experiments, different past joint policies can lead to CBGs of different sizes. One idea
495

Oliehoek, Spaan, Amato, & Whiteson

is to first expand parts of the search tree that lead to small CBGs, by biasing the selection
operator (but not the pruning operator, so as to maintain optimality).
Yet another important direction for future work is the development of tighter heuristics.
Though few researchers are addressing this topic, the results presented in this article underscore how important such heuristics are for solving larger problems. Currently, the heuristic
is the bottleneck in four out of the seven problems we considered. Moreover, two of the
problems where this is not the bottleneck can already be solved for long (h > 50) horizons.
Therefore, we believe that computing tight heuristics for longer horizons is the single most
important research direction for further improving the scalability of optimal Dec-POMDP
solution methods with respect to the horizon.
A different direction is to employ our theoretical results on clustering beyond the DecPOMDP setting to develop new solution methods for CBGs. For instance, a well-known
method for computing a local optimum is alternating maximization (AM): starting from an
arbitrary joint policy, compute a best response for some agent given that other agents keep
their policies fixed and then select another agent’s policy to improve, etc. One idea is to start
with a ‘completely clustered’ CBG, where all agents’ types are clustered together and thus
a random joint CBG policy has a simple form: each agent just selects a single action. Only
when improving the policy of an agent do we consider all its actual possible types to compute
its best response. Subsequently, we cluster together all types for which that agent selects the
same action and proceed to the next agent. In addition, since our clustering results are not
restricted to the collaborative setting, it may also be possible to employ them, using a similar
approach, to develop new solution methods for general-payoff BGs.
Finally, two of our other contributions can have a significant impact beyond the problem of
optimally solving Dec-POMDPs. First, the idea of incrementally expanding nodes introduced
in GMAA*-ICE can be applied in other A∗ search methods. Incremental expansion is most
useful when children can be generated in order of decreasing heuristic value without prohibitive
computational effort, and in problems with a large branching factor such as multiple sequence
alignment problems in computational biology (Carrillo & Lipman, 1988; Ikeda & Imai, 1999).
Second, representing PWLC value functions as a hybrid of a tree and a set of vectors can have
wider impact as well, e.g., in online search for POMDPs (Ross, Pineau, Paquet, & Chaib-draa,
2008).

8. Conclusions
This article presented a set of methods that advance the state-of-the-art in optimal solution
methods for Dec-POMDPs. In particular, we presented several advances that aim to extend
the horizon over which optimal solutions can be found. These advances build off the GMAA*
heuristic search approach and include lossless incremental clustering of the CBGs solved by
GMAA*, incremental expansion of nodes in the GMAA* search tree, and hybrid heuristic
representations. We provided theoretical guarantees that, when a suitable heuristic is used,
both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent. Finally, we presented extensive empirical results demonstrating
that GMAA*-ICE can optimally solve Dec-POMDPs of unprecedented size. We significanty
increase the planning horizons that can be tackled—in some cases by more than an order of
magnitude. Given that an increase of the horizon by one results in an exponentially larger
search space, this constitutes a very large improvement. Moreover, our techniques also im496

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

prove scalability with respect to the number of agents, leading to the first ever solutions of
general Dec-POMDPs with more than three agents. These results also demonstrated how
optimal techniques can yield new insights about particular Dec-POMDPs, as incremental
clustering revealed properties of BroadcastChannel that make it much easier to solve. In
addition to facilitating optimal solutions, we hope these advances will inspire new principled
approximation methods, as incremental clustering has already done (Wu et al., 2011), and
enable them to be meaningfully benchmarked.

Acknowledgments
We thank Raghav Aras and Abdeslam Boularias for making their code available to us. Research supported in part by AFOSR MURI project #FA9550-09-1-0538 and in part by NWO
CATCH project #640.005.003. M.S. is funded by the FP7 Marie Curie Actions Individual
Fellowship #275217 (FP7-PEOPLE-2010-IEF).

Appendix A. Appendix
A.1 Auxiliary algorithms
Algorithm 12 implements the BestJointPolicyAndValue function, which prunes all child
nodes that are not fully specified. Algorithm 13 generates all children of a particular CBG.
Algorithm 12 BestJointPolicyAndValue(QExpand ): Prune fully expanded nodes from a set
of nodes QExpand returning only the best one and its value.
Input: QExpand a set of nodes for fully specified joint policies.
Output: the best full joint policy in the input set and its value.
1: v ∗ = −∞
2: for q ∈ QExpand do
3:
QExpand .Remove(q)
4:
hπ, v̂i ← q
5:
if v > v ∗ then
6:
v∗ ← v
7:
π∗ ← π
8:
end if
9: end for
10: return hπ ∗ , v ∗ i

A.2 Detailed GMAA*-ICE algorithm
The complete GMAA*-ICE algorithm is shown in Algorithm 14.
A.3 Computation of V 0...t−1 (ϕt )
The quantity V 0...t−1 (ϕt ) is defined recursively via:
V 0...t−1 (ϕt ) = V 0...t−2 (ϕt−1 ) + Est−1 ,~θt−1 [R(st−1 ,δ t−1 (~θ t−1 )) | b0 , ϕt ].
497

(A.1)

Oliehoek, Spaan, Amato, & Whiteson

Algorithm 13 GenerateAllChildrenForCBG(B(ϕt )).
Input: CBG B(ϕt ).
Output: QExpand the set containing all expanded child nodes for this CBG.
1: QExpand ← {}
2: for all jointP
CBG policies β for B do
3:
Vb (β) ← θ Pr(θ)u(θ,β(θ))
4:
ϕt+1 ← (ϕt , β t )
{create partial joint policy}
t
t+1
0...t−1
t
b
b
5:
V (ϕ ) ← V
(ϕ ) + V (β )
{compute heuristic value}
6:
q ′ ← hϕt+1 , Vb (ϕt+1 )i
{create child node}
7:
QExpand .Insert(q ′ )
8: end for
9: return QExpand

The expectation is taken with respect to the joint probability distribution over states and
joint AOHs that is induced by ϕt :
X
Pr(st ,~θ t |b0 ,ϕt ) =
Pr(ot |at−1 ,st ) Pr(st |st−1 ,at−1 ) Pr(at−1 |ϕt ,~θ t−1 ) Pr(st−1 ,~θ t−1 |b0 ,ϕt ).
st−1 ∈S

(A.2)
Here, ~θ t = (~θ t−1 ,at−1 ,ot ) and Pr(at−1 |ϕt ,~θ t−1 ) is the probability that ϕt specifies at−1 for
AOH ~θ t−1 (which is 0 or 1 in case of deterministic past joint policy ϕt ).
A.4 Proofs
Proof of Theorem 1
Substituting (2.9) in (2.7) yields
X
b ~θ t ,δ t (~θ t ))
Vb (β) = Vb (δ t ) =
Pr(~θ t |b0 ,ϕt )Q(
~
θt

=

X
~
θt



Pr(~θ t |b0 ,ϕt ) Est [R(st ,δ t (~θ t )) | ~θ t ] + E~θt+1 [Vb (~θ t+1 ) | ~θ t , δ t (~θ t )]

= Est ,~θt [R(st ,δ t (~θ t ) | b0 , ϕt ] + E~θt+1 [Vb (~θ t+1 ) | b0 , ϕt , δ t ]

≥ Est ,~θt [R(st ,δ t (~θ t ) | b0 , ϕt ] + E~θt+1 [Qπ∗ (~θ t+1 , π ∗ (~θ t+1 )) | b0 , ϕt+1 = (ϕt , δ t )]
= Est ,~θt [R(st ,δ t (~θ t ) | b0 , ϕt ] + H ∗,t+1...h−1 (ϕt+1 ),

where H ∗ is an optimal admissible heuristic. Substituting this into (2.8) we obtain
Vb (ϕt+1 = (ϕt , δ t )) = V 0...t−1 (ϕt ) + Est ,~θt [R(st ,δ t (~θ t ) | b0 , ϕt ] + E~θt+1 [Vb (~θ t+1 ) | b0 , ϕt , δ t ]
≥ V 0...t−1 (ϕt ) + Est ,~θt [R(st ,δ t (~θ t )) | b0 , ϕt+1 ] + H ∗,t+1...h−1 (ϕt+1 )

{via (A.1)} = V 0...t (ϕt+1 ) + H ∗,t+1...h−1 (ϕt+1 ),
which demonstrates that the heuristic value Vb (ϕt ) used by GMAA* via CBGs using heuristic
of a form (2.9) is admissible, as it is lower bounded by the actual value for the first t plus
an admissible heuristic. Since it performs heuristic search with this admissible heuristic, this
algorithm is also complete.
498

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

Algorithm 14 GMAA*-ICE
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:
39:
40:
41:
42:
43:
44:
45:
46:
47:

vGM AA ← −∞
ϕ0 ← ()
v̂ ← +∞
q 0 ← hϕ0 , v̂i
LIE ← {q 0 }
repeat
q ← Select(LIE )
{q = hϕt , v̂i}
IE
L .pop(q)
if IsPlaceholder(q) then
B(ϕt ) ← ϕt .CBG
{reuse stored CBG}
else
{Construct extended BG and solver:}
B(ϕt−1 ) ← ϕt−1 .CBG
{note ϕt = (ϕt−1 , β t−1 )}
t−1
t
t−1
B(ϕ ) ← ConstructExtendedBG(B(ϕ ),β )
B(ϕt ) ← ClusterBG(B(ϕt ))
B(ϕt ).Solver ← CreateSolver(B(ϕt ))
ϕt .CBG ← B(ϕt )
end if
{Expand a single child:}
vCBG = vGM AA − V 0...(t−1) (ϕt )
v̄CBG = +∞
if last stage t = h − 1 then
v̄CBG = Vb (ϕh−1 ) − V 0...(h−2) (ϕh−1 )
end if
hβ t , Vb (β t )i ← B(ϕt ).Solver.NextSolution(vCBG ,v̄CBG )
if not β t then
{fully expanded: no solution s.t. V (β h−1 ) ≥ vCBG }
delete q (and its CBG + solver)
continue
{(i.e. goto line 8)}
end if
ϕt+1 ← (ϕt , β t )
Vb (ϕt+1 ) ← V 0...t−1 (ϕt ) + Vb (β t )
if last stage t = h − 1 then
{Note that π = ϕt+1 , V (π) = Vb (ϕt+1 ) }
if V (π) > vGM AA then
vGM AA ← V (π)
{found new lower bound}
π⋆ ← π
LIE .prune(vGM AA )
end if
delete q (and its CBG + solver)
else
q ′ ← hϕt+1 , Vb (ϕt+1 )i
LIE .insert(q ′ )
q ← hϕt , Vb (ϕt+1 )i
{ Update parent node q, which now is a placeholder }
LIE .insert(q)
end if
until LIE is empty

499

Oliehoek, Spaan, Amato, & Whiteson

Proof of Lemma 1
t
t+1
t
t+1 and ~
Proof. Assume an arbitrary ati ,ot+1
θ 6=i ,at6=i ,ot+1
θ 6=i = (~
i , δ 6=i ,s
6=i )). We have that
t+1

~ a,t t t
Pr(st+1 ,~
θ 6=i ,ot+1
i |θi ,ai ,δ 6=i )
X
t
t
t+1 t t
t+1
θ 6=i |θ~ia,t )
Pr(ot+1
) Pr(st+1 |st ,ati ,at6=i ) Pr(at6=i |~
θ 6=i ,δ t6=i ) Pr(st ,~
=
i ,o6=i |ai ,a6=i ,s
st

=

X

t
t
t+1 t t
t+1
θ 6=i |θ~ib,t )
) Pr(st+1 |st ,ati ,at6=i ) Pr(at6=i |~
θ 6=i ,δ t6=i ) Pr(st ,~
Pr(ot+1
i ,o6=i |ai ,a6=i ,s

st

t+1
~ b,t t t
= Pr(st+1 ,~
θ 6=i ,ot+1
i |θi ,ai ,δ 6=i )
t+1

Because we assumed an arbitrary st+1 ,~
θ 6=i ,ot+1
i , we have that
∀st+1 ,~θt+1 ,ot+1
6=i

i

t+1
t+1 ~ t+1 t+1 ~ b,t t t
~ a,t t t
,θ 6=i ,oi |θi ,ai ,δ 6=i )
Pr(st+1 ,~
θ 6=i ,ot+1
i |θi ,ai ,δ 6=i ) = Pr(s

(A.3)

In general we have that
t+1

Pr(s

t+1

t+1
t
,~
θ 6=i |θ~it ,ati ,ot+1
i ,δ 6=i ) =

~t t t
Pr(st+1 ,~
θ 6=i ,ot+1
i |θi ,ai ,δ 6=i )
Pr(ot+1 |θ~ t ,at ,δ t )
i

=

i

i

6=i

t+1
~t t t
Pr(st+1 ,~
θ 6=i ,ot+1
i |θi ,ai ,δ 6=i )
P
t+1 t+1 t t t
t+1 ,~
~
θ
t+1 Pr(s
6=i ,oi |θi ,ai ,δ 6=i )
t+1
~
s
,θ
6=i

Now, because of (A.3), both the numerator and denominator are the same when substituting
θ~ia,t ,θ~ib,t in this equation. Consequently, we can conclude that
t+1
t
t+1 ~ t+1 ~ b,t t t+1 t
,θ 6=i |θi ,ai ,oi ,δ 6=i )
Pr(st+1 ,~
θ 6=i |θ~ia,t ,ati ,ot+1
i ,δ 6=i ) = Pr(s
t+1

t
t+1 , and ~
Finally, because ati , ot+1
θ 6=i were all arbitrarily chosen, we can conclude that
i , δ 6=i ,s
(3.4) holds.

Proof of Lemma 2
Proof. Assume an arbitrary π 6=i ,s and γ 6=i , then we have
bi (s,γ 6=i |θ~ia , π 6=i ) , Pr(s,γ 6=i |θ~ia , π 6=i ,b0 )
X
=
Pr(s,γ 6=i ,~
θ 6=i |θ~ia ,π 6=i ,b0 )
~
θ 6=i

{factoring the joint distribution}

=

X

Pr(s,~
θ 6=i |θ~ia ,π 6=i ,b0 ) Pr(γ 6=i |s,~
θ 6=i , θ~ia ,π 6=i ,b0 )

~
θ 6=i

500

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

{γ 6=i only depends on ~
θ 6=i ,π 6=i } =

X

Pr(s,~
θ 6=i |θ~ia ,π 6=i ,b0 ) Pr(γ 6=i |~
θ 6=i ,π 6=i )

{ s,~
θ 6=i only depend on ϕ6=i } =

X

Pr(s,~
θ 6=i |θ~ia ,ϕ6=i ,b0 ) Pr(γ 6=i |~
θ 6=i ,π 6=i )

{due to PE} =

X

Pr(s,~
θ 6=i |θ~ib ,ϕ6=i ,b0 ) Pr(γ 6=i |~
θ 6=i ,π 6=i )

~
θ 6=i

~
θ 6=i

~
θ 6=i

= [...] = Pr(s,γ 6=i |θ~ib ,π 6=i ,b0 ) = bi (s,γ 6=i |θ~ib , π 6=i )
We can conclude this holds for all π 6=i ,s and γ 6=i .
Proof of Theorem 5 (Search Equivalence)
To prove search equivalence, we explicitly write a node as a tuple q = hϕt , v̂, PHi, where ϕt
is the past joint policy, v̂ the node’s heuristic value, and PH a boolean indicating whether
it is a placeholder. We consider the equivalence of the maintained open lists. The open list
L maintained by GMAA*-IC contains only non-expanded nodes q. In contrast, the open
list LIE of GMAA*-ICE contains both non-expanded nodes q and placeholders (previously
expanded nodes), q̄. We denote the ordered subset of LIE containing non-expanded nodes
with Q and that containing placeholders with Q̄. We treat these open lists as ordered sets of
heuristic values and their associated nodes.
Definition 11. L and LIE are equivalent, L ≡ LIE if:
1. Q ⊆ L.
2. The q’s have the same ordering: L.remove(L \ Q) = Q.24
3. Nodes q in L but not Q have a placeholder q̄ that is the parent of and higher ranked
than q:
∀q=hϕt ,v̂q ,falsei∈(L\Q)

∃q̄=hϕt−1 ,v̂q̄ ,truei∈Q̄ s.t. (ϕt = (ϕt−1 , β) ∧ q < q̄).

4. There are no other placeholders.
Fig. 11 illustrates two equivalent lists in which the past joint policies are indexed with letters.
Note that the placeholders in LIE are ranked higher than the nodes in L that they represent.
Let us write IT-IC(L) and IT-ICE(LIE ) for one iteration (i.e., one loop of the main repeat
in Algorithm 1) of the respective algorithms. Let IT-ICE* denote the operation that repeats
IT-ICE as long as a placeholder was selected (so it ends when a q is expanded).
Lemma 4. If L ≡ LIE , then executing IT-IC(L) and IT-ICE*(LIE ) leads to new open lists
that are again equivalent: L ′ ≡ LIE′ .
Proof. When IT-ICE* selects a placeholder q̄, it generates child q ′ that was already present
in L (due to properties 3 and 4 of Definition 11) and inserts it. Insertion occurs at the
same relative location as IT-IC because both algorithms use the same comparison operator
(Definition 5). Together these facts guarantee that the insertion preserves properties 1 and 2.
24. A.remove(B) removes the elements of B from A without changing A’s ordering.

501

Oliehoek, Spaan, Amato, & Whiteson

LIE

L
Q
Vb

ϕt

7
5
4.5

c
d
e

3
3
2.5
1
0.5

f
g
h
i
j

Q̄

Vb

ϕt

5

d

3
3

f
g

Vb
8

4

ϕt
a

b

←

placeholder for {c,e,j}

←

same nodes: same position

←
o

placeholder for {h,i}
consistent ordering
for equal values

Figure 11: Illustration of equivalent lists. Past joint policies are indexed by letters. In this
example, a and b have been expanded earlier (but are not yet fully expanded in the ICE-case).
If there are remaining unexpanded children of q̄, IT-ICE* reinserts q̄ with an updated heuristic
value q̄.v̂ ← q ′ .v̂ that is guaranteed to be an upper bound on the value of unexpanded siblings
q ′′ since q ′ .v̂ = Vb (q ′ .ϕ) ≥ Vb (q ′′ .ϕ) = q ′′ .v̂ (preserving properties 3 and 4).
When IT-ICE* finally selects a non-placeholder q, it is guaranteed to be the same q as
selected by IT-IC (due to properties 1 and 2). Expansion in ICE generates one child q ′ (again
inserted at the same relative location as in IC) and inserts placeholder q̄ = hq.ϕ, q ′ .v̂, truei for
the other siblings q ′′ (again preserving properties 3 and 4).
Proof of Theorem 5. The fact that GMAA*-ICE and GMAA*-IC are search-equivalent follows directly from Lemma 4. Search equivalence means that both algorithms select the same
non-placeholders q to expand. Since both algorithms begin with identical (and therefore trivially equivalent) open lists, they maintain equivalent open lists throughout search. As such,
property 2 of Definition 11 ensures that every time IT-ICE* selects a non-placeholder, IT-IC
selects it too.




Online, sample-based planning algorithms for POMDPs have shown great promise in scaling to problems
with large state spaces, but they become intractable for large action and observation spaces. This is particularly problematic in multiagent POMDPs where the action and observation space grows exponentially with
the number of agents. To combat this intractability, we propose a novel scalable approach based on samplebased planning and factored value functions that exploits structure present in many multiagent settings. This
approach applies not only in the planning case, but also in the Bayesian reinforcement learning setting. Experimental results show that we are able to provide high quality solutions to large multiagent planning and
learning problems.

1

Introduction

Online planning methods for POMDPs have demonstrated impressive performance (Ross et al., 2008) on large
problems by interleaving planning with action selection. The leading such method, partially observable Monte
Carlo planning (POMCP) (Silver and Veness, 2010), achieves performance gains by extending sample-based
methods based on Monte Carlo tree search (MCTS) to solve POMDPs.
While online, sample-based methods show promise in solving POMDPs with large state spaces, they become intractable as the number of actions or observations grow. This is particularly problematic in the case
of multiagent systems. Specifically, we consider multiagent partially observable Markov decision processes
(MPOMDPs), which assume all agents share the same partially observable view of the world and can coordinate their actions. Because the MPOMDP model is centralized, POMDP methods apply, but the fact that the
number of joint actions and observations scales exponentially in the number of agents renders current POMDP
methods intractable.
To combat this intractability, we provide a novel sample-based online planning algorithm that exploits multiagent structure. Our method, called factored-value partially observable Monte Carlo planning (FV-POMCP),
is based on POMCP and is the first MCTS method that exploits locality of interaction: in many MASs, agents
interact directly with a subset of other agents. This structure enables a decomposition of the value function into
a set of overlapping factors, which can be used to produce high quality solutions (Guestrin, Koller, and Parr,
2001; Nair et al., 2005; Kok and Vlassis, 2006). But unlike these previous approaches, we will not assume a
factored model, but only that the value function can be approximately factored. We present two variants of FVPOMCP that use different amounts of factorization of the value function to scale to large action and observation
spaces.
Not only is our FV-POMCP approach applicable to large MPOMDPs, but it is potentially even more important for Bayesian learning where the agents have uncertainty about the underlying model as modeled by
Bayes-Adaptive POMDPs (BA-POMDPs) (Ross et al., 2011). These models translate the learning problem
to a planning problem, but since the resulting planning problems have an infinite number of states, scalable
sample-based planning approaches are critical to their solution.

1

We show experimentally that our approach allows both planning and learning to be significantly more
efficient in multiagent POMDPs. This evaluation shows that our approach significantly outperforms regular
(non-factored) POMCP, indicating that FV-POMCP is able to effectively exploit locality of interaction in both
settings.

2

Background

We first discuss multiagent POMDPs and previous work on Monte Carlo tree search and Bayesian reinforcement
learning (BRL) for POMDPs.

2.1

Multiagent POMDPs

An MPOMDP (Messias, Spaan, and Lima, 2011) is a multiagent planning model that unfolds over a number
of steps. At every stage, agents take individual actions and receive individual observations. However, in an
MPOMDP, all individual observations are shared via communication, allowing the team of agents to act in a
‘centralized manner’. We will restrict ourselves to the setting where such communication is free of noise, costs
and delays.
An MPOMDP is a tuple hI, S, {Ai }, T, R, {Zi }, O, hi with: I, a set of agents; S, a set of states with
designated initial state distribution b0 ; A = ×i Ai , the set of joint actions, using action sets for each agent, i;
0
T , a set of state transition probabilities: T s~as = Pr(s0 |s, ~a), the probability of transitioning from state s to s0
when actions ~a are taken by the agents; R, a reward function: R(s, ~a), the immediate reward for being in state s
and taking actions ~a; Z = ×i Zi , the set of joint observations, using observation sets for each agent, i; O, a set
0
of observation probabilities: O~as ~z = Pr(~z|~a,s0 ), the probability of seeing observations ~o given actions ~a were
taken and resulting state s0 ; h, the horizon.
An MPOMDP can be reduced to a POMDP with a single centralized controller that takes joint actions and
receives joint observations (Pynadath and Tambe, 2002). Therefore, MPOMDPs can be solved with POMDP
solution methods, some of which will be described in the remainder of this section. However, such approaches
do not exploit the particular structure inherent to many MASs. In Sec. 4, we present a first online planning
method that overcomes this deficiency.

2.2

Monte Carlo Tree Search for (M)POMDPs

Most research for (mutliagent) POMDPs has focused on planning: given a full specification of the model,
determine an optimal policy, π, mapping past observation histories (which can be summarized by distributions
b(s) over states
policy can be extracted from an optimal Q-value function,
P called beliefs)
P to actions. An optimal
0 0
0
Q(b,a) =
s R(s,a) +
z P (z|b,a) maxa Q(b ,a ), by acting greedily. Computing Q(b,a), however, is
complicated by the fact that the space of beliefs is continuous.
POMCP (Silver and Veness, 2010), is a scalable method which extends Monte Carlo tree search (MCTS) to
solve POMDPs. At every stage, the algorithm performs online planning, given the current belief, by incrementally building a lookahead tree that contains (statistics that represent) Q(b,a). The algorithm, however, avoids
expensive belief updates by creating nodes not for each belief, but simply for each action-observation history
h. In particular, it samples hidden states s only at the root node (called ‘root sampling’) and uses that state to
sample a trajectory that first traverses the lookahead tree and then performs a (random) rollout. The return of
this trajectory is used to update the statistics for all visited nodes. Because this search tree can be enormous,
the search is directed topthe relevant parts by selecting actions to maximize the ‘upper confidence bounds’:
U (h,a) = Q(h, a) + c log(N + 1)/n. Here, N is the number of times the history has been reached and n
is the number of times that action a has been taken in that history. POMCP can be shown to converge to an
-optimal value function. Moreover, the method has demonstrated good performance in large domains with a
limited number of simulations.

2

2.3

Bayesian RL for (M)POMDPs

Reinforcement learning (RL) considers the more realistic case where the model is not (perfectly) known in
advance. Unfortunately, effective RL in POMDPs is very difficult. Ross et al. (2011) introduced a framework,
called the Bayes-Adaptive POMDP (BA-POMDP), that reduces the learning problem to a planning problem,
thus enabling advances in planning methods to be used in the learning problem.
In particular, the BA-POMDP utilizes Dirichlet distributions to model uncertainty over transitions and observations. Intuitively, if the agent could observe states and observations, it could maintain vectors φ and ψ
of counts for transitions and observations respectively. Let φass0 be the transition count of the number times
state s0 resulted from taking action a in state s and ψsa0 z be the observation count representing the number of
times observation z was seen after taking action a and transitioning to state s0 . These counts induce a probability distribution over the possible transition and observation models. Even though the agent cannot observe
the states and has uncertainty about the actual count vectors, this uncertainty can be represented using the
POMDP formalism — by including the count vectors as part of the hidden state of a special POMDP, called a
BA-POMDP.
The BA-POMDP can be extended to the multiagent setting (Amato and Oliehoek, 2013), yielding the BayesAdaptive multiagent POMDP (BA-MPOMDP) framework. BA-MPOMDPs are POMDPs, but with an infinite
state space since there can be infinitely many count vectors. While a quality-bounded reduction to a finite state
space is possible (Ross et al., 2011), the problem is still intractable; sample-based planning is needed to provide
solutions. Unfortunately, current methods, such as POMCP, do not scale well to multiple agents.

3

Exploiting Graphical Structure

POMCP is not directly suitable for multiagent problems (in either the planning or learning setting) due to the
fact that the number of joint actions and observations are exponential in the number of agents. We first elaborate
on these problems, and then sketch an approach to mitigate them by exploiting locality between agents.

3.1

POMCP for MPOMDPs: Bottlenecks

The large number of joint observations is problematic since it leads to a lookahead tree with very high branching
factor. Even though this is theoretically not a problem in MDPs (Kearns, Mansour, and Ng, 2002), in partially
observable settings that use particle filters it leads to severe problems. In particular, in order to have a good
particle representation at the next time step, the actual joint observation received must be sampled often enough
during planning for the previous stage. If the actual joint observation had not been sampled frequently enough
(or not at all), the particle filter will be a bad approximation (or collapse). This results in sampling starting from
the initial belief again, or alternatively, to fall back to acting using a separate (history independent) policy such
as a random one.
The issue of large numbers of joint actions is also problematic: the standard POMCP algorithm will, at each
node, maintain separate statistics, and thus separate upper confidence bounds, for each of the exponentially
many joint actions. Each of the exponentially many joint actions will have to be selected at least a few times
to reduce their confidence bounds (i.e., exploration bonus). This is a principled problem: in cases where each
combination of individual actions may lead to completely different effects, it may be necessary to try all of
them at least a few times. In many cases, however, the effect of a joint action is factorizable as the effects of
the action of individual agents or small groups of agents. For instance, consider a team of agents that is fighting
fires at a number of burning houses, as illustrated in Fig. 1(a). The rewards depend only on the amount of water
deposited on each house rather than the exact joint action taken (Oliehoek et al., 2008). While this problem
lends itself to a natural factorization, other problems may also be factorized to permit approximation.

3.2

Coordination Graphs

In certain multiagent settings, coordination (hyper) graphs (CGs) (Guestrin, Koller, and Parr, 2001; Nair et al.,
2005) have been used to compactly represents interactions between subsets of agents. In this paper we extend
3

(a)

(b)

(c)

Figure 1: (a) Illustration of ‘fire fighting’ (b) Coordination graph with 4 houses and 3 agents (c) Illustration of
a sensor network problem on a grid that is used in the experiments.
this approach to MPOMDPs. We first introduce the framework of CGs in the single shot setting.
A CG specifies a set of payoff components E = {Qe }, and each component e is associated with a subset
of agents. These subsets (which we also denote using e) can be interpreted as (hyper)-edges in a graph where
the nodes are agents. P
The goal in a CG is to select a joint action that maximizes the sum of the local payoff
components Q(~a) = e Qe (~ae ).1 A CG for the fire fighting problem is shown in Fig. 1(b). We follow the
cited literature in assuming that a suitable factorization is easily identifiable by the designer, but it may also
be learnable. Even if a payoff function Q(~a) does not factor exactly, it can be approximated by a CG. For
the moment assuming a stateless problem (we will consider the case where histories are included in the next
section), an action-value function can be approximated by
X
Q(~a) ≈
Qe (~ae ),
(1)
e

We refer to this as the linear approximation of Q, since one can show that this corresponds to an instance of
linear regression (See Sec. 5).
P
Using a factored representation, the maximization max~a e Qe (~ae ) can be performed efficiently via variable elimination (VE) (Guestrin, Koller, and Parr, 2001), or max-sum (Kok and Vlassis, 2006). These algorithms are not exponential in the number of agents, and therefore enable significant speed-ups for larger number
of agents. The VE algorithm (which we use in the experiments) is exponential in the induced width w of the
coordination graph.

3.3

Mixture of Experts Optimization

VE can be applied if the CG is given in advance. When we try to exploit these techniques in the context of
POMCP, however, this is not
Pthe case. As such, the task we consider here is to find the maximum of an estimated
factored function Q̂(~a) = e Q̂e (~ae ). Note that we do not necessarily require the best approximation to the
entire Q, as in (1). Instead, we seek an estimation Q̂ for which the maximizing joint action ~aM is close to the
maximum of the actual (but unknown) Q-value: Q(~aM ) ≈ Q(~a∗ ).
For this purpose, we introduce a technique called mixture of experts optimization. In contrast to methods
based on linear approximation (1), we do not try to learn a best-fit factored Q function, but directly try to estimate
the maximizing joint action. The main idea is that for each local action ~ae we introduce an expert that predicts
the total value Q̂(~ae ) = E[Q(~a) | ~ae ]. For a joint action, these responses—one of each payoff component
e—
P
are put in a mixture with weights αe and used to predict the maximization joint action: arg max~a e αe Q̂(~ae ).
This equation is the sum of restricted-scope functions, which is identical to the case of linear approximation (1),
so VE can be used to perform this maximization effectively. In the remainder of this paper, we will integrate
the weights and simply write Q̂e (~ae ) = αe Q̂(~ae ).
1 Since we focus on the one-shot setting here, the Q-values in the remainder of this section should be interpreted as those for one specific
joint history h, i.e.: Q(~a) ≡ Q(h,~a).

4

~a 1

~a 2

~o 1

~o 1

~o 2

~a 2
~o 1

Figure 2: Factored Statistics: joint histories are maintained (for specific joint actions and observations specified
by ~a j and ~o k ), but action statistics are factored at each node.
The experts themselves are implemented as maximum-likelihood estimators of the total value. That is, each
expert (associated with a particular ~ae ) keeps track of the mean payoff received when ~ae was performed, which
can be done very efficiently. An additional benefit of this approach is that it allows for efficient estimation of
upper confidence bounds by also keeping track of how often this local action was performed, which in turns
facilitates easy integration in POMCP, as we describe next.

4

Factored-Value POMCP

This section presents our main algorithmic contribution: Factored-Value POMCP, which is an online planning
method for POMDPs that can exploit approximate structure in the value function by applying mixture of experts optimization in the POMCP lookahead search tree. We introduce two variants of FV-POMCP. The first
technique, factored statistics, only addresses the complexity introduced by joint actions. The second technique,
factored trees, additionally addresses the problem of many joint observations. FV-POMCP is the first MCTS
method to exploit structure in MASs, achieving better sample complexity by using factorization to generalize
the value function over joint actions and histories. While this method was developed to scale POMCP to larger
MPOMDPs in terms of number of agents, the techniques may be beneficial in other multiagent models and
other factored POMDPs.

4.1

Factored Statistics

We first introduce Factored Statistics which directly applies mixture of experts optimization to each node in the
POMCP search tree. As shown in Fig. 2, the tree of joint histories remains the same, but the statistics retained
at for each history is now different. That is, rather than maintaining one set of statistics in each node (i.e, joint
history ~h) for the expected value of each joint action Q(~h, ~a), we maintain a set of statistic for each component
e that estimates the values Qe (~h, ~ae ) and corresponding upper confidence bounds.
Joint actions are selected according to the maximum (factored) upper confidence bound:
X
max
Ue (~h, ~ae ),
~
a

e

p
Where Ue (~h, ~ae ) , Qe (~h, ~ae ) + c log(N~h + 1)/n~ae using the Q-value and the exploration bonus added for
that factor. For implementation, at each node for a joint history ~h, we store the count for the full history N~h as
well as the Q-values, Qe , and the counts for actions, n~ae , separately for each component e.
Since this method retains fewer statistics and performs joint action selection more efficiently via VE, we
expect that it will be more efficient than plain application of POMCP to the BA-MPOMDP. However, the
complexity due to joint observations is not directly addressed: because joint histories are used, reuse of nodes

5

...
~a 11
~o 11

e

...
~a 1|E|

~a 21
~o 11

~o 1|E|

~o 21

~a 2|E|

~o 1|E|

~a 21

~a 2|E|

~o 11

~o 1|E|

~o 2|E|

Figure 3: Factored Trees: local histories for are maintained for each factor (resulting in factored history and
action statistics). Actions and observations for component i are represented as ~a ji and ~o ki )
and creation of new nodes for all possible histories (including the one that will be realized) may be limited if
the number of joint observations is large.

4.2

Factored Trees

The second technique, called Factored Trees, additionally tries to overcome the large number of joint observations. It further decomposes the local Qe ’s by splitting joint histories into local histories and distributing them
over the factors. That is, in this case, we introduce an expert for each local ~he , ~ae pair. During simulations, the
agents know ~h and action selection is conducted by maximizing over the sum of the upper confidence bounds:
X
max
Ue (~he , ~ae ),
~
a

e

q
where Ue (~he , ~ae ) = Qe (~he , ~ae ) + c log(N~he + 1)/n~ae . We assume that the set of agents with relevant actions
and histories for component Qe are the same, but this can be generalized. This approach further reduces the
number of statistics maintained and increases the reuse of nodes in MCTS and the chance that nodes in the trees
will exist for observations that are seen during execution. As such, it can increase performance by increasing
generalization as well as producing more robust particle filters.
This type of factorization has a major effect on the implementation: rather than constructing a single tree,
we now construct a number of trees in parallel, one for each factor e as shown in Fig. 3. A node of the tree for
component e now stores the required statistics: N~he , the count for the local history, n~ae , the counts for actions
taken in the local tree and Qe for the tree. Finally, we point out that this decentralization of statistics has the
potential to reduce communication since the components statistics in a decentralized fashion could be updated
without knowledge of all observation histories.

5

Theoretical Analysis

Here, we investigate the approximation quality induced by our factorization techniques.2 The most desirable
quality bounds would express the performance relative to ‘optimal’, i.e., relative to flat POMCP, which converges in probability an -optimal value function. Even for the one-shot case, this is extremely difficult for
any method employing factorization based on linear approximation of Q, because Equation (1) corresponds to
aPspecial case of
P linear regression. In this case, we can write (1) in terms of basis functions and weights as:
Q
(~
a
)
=
a), where the he,~ae are the basis functions: he,~ae (~a) = 1 iff ~a specifies ~ae for
ae he,~
ae (~
e e e
e,~
ae we,~
component e (and 0 otherwise). As such, providing guarantees with respect to the optimal Q(~a)-value would
2 Proofs

can be found in Appendix B.

6

require developing a priori bounds for the approximation quality of (a particular type of) basis functions. This
is a very difficult problem for which there is no good solution, even though these methods are widely studied in
machine learning.
However, we do not expect our methods to perform well on arbitrary Q. Instead, we expect them to perform
well when Q is nearly factored, such that (1) approximately holds, since then the local actions contain enough
information to make good predictions. As such, we analyze the behavior of our methods when the samples of
Q come from a factored function (i.e., as in (1) ) contaminated with zero-mean noise. In such cases, we can
show the following.
Theorem 1. The estimate Q̂ of Q made by a mixture of experts converges in probability to the true value plus
p
a sample policy dependent bias term: Q̂(~a) → Q(~a) + B~π (~a). The bias is given by a sum of biases induced by
pairs e,e0 :
XX X
~π (~ae0 \e |~ae )Qe0 (~ae0 \e ,~ae0 ∩e ).
B~π (~a) ,
e e0 6=e ~
a

e0 \e

Here, ~ae0 ∩e is the action of the agents that participate both in e and e0 (specified by ~a) and ~ae0 \e are the actions
of agents in e0 that are not in e.
Because we observe the global reward for a given set of actions, the bias is caused by correlations in the
sampling policy and the fact that we are overcounting value from other components. When there is no overlap,
and the sampling policy we use is ‘component-wise’: ~π (~ae0 \e |~ae ) = ~π (~ae0 \e |~a0e ) = ~π (~ae0 \e ), this over counting
is the same for all local actions ~ae :
Theorem 2. When value components do not overlap and a component-wise sampling policy is used, mixture of
experts optimization recovers the maximizing joint action.
Similar reasoning can be used to establish bounds on the performance in the case of overlapping components, subject to assumptions about properties of the true value function. Let N (e) denote the neighborhood of
component e: the set of other components e0 which have an overlap with e (those that have at least one agent
participating in them that also participates in e).
Theorem 3. If for all overlapping components e,e0 , and any two ‘intersection action profiles’ ~ae0 ∩e ,~a0e0 ∩e for
their intersection, the true value function satisfies
∀~ae0 \e

Qe0 (~ae0 \e ,~ae0 ∩e ) − Qe0 (~ae0 \e ,~a0e0 ∩e ) ≤


,
~ e0 \e | · ~π (~ae0 \e )
|E| · |N (e)| · |A

(2)

~ e0 \e | the number of intersection action profiles, then mixture of experts optimization, in the limit, will
with |A
return a joint action whose value lies within  of the optimal solution.
The analysis shows that a sufficiently local Q-function can be effectively optimized when using a sufficiently
local sampling policy. Under the same assumptions, we can also derive guarantees for the sequential case. It is
not directly possible to derive bounds for FV-POMCP itself (since it is not possible to demonstrate that the UCB
exploration policy is component-wise), but it seems likely that UCB exploration leads to an effective policy that
nearly satisfies this property. Moreover, since bias is introduced by the interaction between action correlations
and differences in ‘non-local’ components, even when using a policy with correlations, the bias may be limited
if the Q-function is sufficiently structured.
In the factored tree case, we can introduce a strong result. Because histories for other agents outside the
factor are not included and we do not assume independence between factors, the approximation quality may
suffer: where ~h is Markov, this is not the case for the local history ~he . As such, the expected return for such a
local history depends on the future policy as well as the past one (via the distribution over histories of agents
not included in e). This implies that convergence is no longer guaranteed:
Proposition 1. Factored-Trees FV-POMCP may diverge.

7

Proof. FT-FV-POMCP (with c = 0) corresponds to a general case of Monte Carlo control (i.e., SARSA(1))
with linear function approximation that is greedy w.r.t. the current value function. Such settings may result in
divergence (Fairbank and Alonso, 2012).
Even though this is a negative result, and there is no guarantee of convergence for FT-FV-POMCP, in practice
this need not be a problem; many reinforcement learning techniques that can diverge (e.g., neural networks) can
produce high-quality results in practice, e.g., (Tesauro, 1995; Stone and Sutton, 2001). Therefore, we expect
that if the problem exhibits enough locality, the factored trees approximation may allow good quality policies
to be found very quickly.
Finally, we analyze the computational complexity. FV-POMCP is implemented by modifying POMCP’s
S IMULATE function (as described in Appendix A). The maximization is performed by variable elimination,
which has complexity O(n|Amax |w ) with w the induced width and |Amax | the size of the largest action set.
In addition, the algorithm updates each of the |E| components, bringing the total complexity of one call of
simulate to O(|E| + n|Amax |w ).

6

Experimental Results

Here, we empirically investigate the effectiveness of our factorization methods by comparing them to nonfactored methods in the planning and learning settings.
Experimental Setup. We test our methods on versions of the firefighting problem from Section 4 and on
sensor network problems. In the firefighting problems, fires are suppressed more quickly if a larger number of
agents choose that particular house. Fires also spread to neighbor’s houses and can start at any house with a
small probability. In the sensor network problems (as shown by Fig. 1(c)), sensors were aligned along discrete
intervals on two axes with rewards for tracking a target that moves in a grid. Two types of sensing could be
employed by each agent (one more powerful than the other, but using more energy) or the agent could do
nothing. A higher reward was given for two agents correctly sensing a target at the same time. The firefighting
problems were broken up into n − 1 overlapping factors with 2 agents in each (representing the agents on the
two sides of a house) and the sensor grid problems were broken into n/2 factors with n/2 + 1 agents in each
(representing all agents along the y axis and one agent along the x axis). For the firefighting problem with 4
agents, |S| = 243, |A| = 81 and |Z| = 16 and with 10 agents, |S| = 177147, |A| = 59049 and |Z| = 1024. For the
sensor network problems with 4 agents, |S| = 4, |A| = 81 and |Z| = 16 and with 8 agents, |S| = 16, |A| = 6561
and |Z| = 256.
Each experiment was run for a given number of simulations, the number of samples used at each step to
choose an action, and averaged over a number of episodes. We report undiscounted return with the standard
error. Experiments were run on a single core of a 2.5 GHz machine with 8GB of memory. In both cases, we
compare our factored representations to the flat version using POMCP. This comparison uses the same code
base so it directly shows the difference when using factorization. POMCP and similar sample-based planning
methods have already been shown to be state-of-the-art methods in both POMDP planning (Silver and Veness,
2010) and learning (Ross et al., 2011).
MPOMDPs. We start by comparing the factored statistics (FS) and factored tree (FT) versions of FV-POMCP
in multiagent planning problems. Here, the agents are given the true MPOMDP model (in the form of a simulator) and use it to plan. For this setting, we compare to two baseline methods: POMCP: regular POMCP applied
to the MPOMDP, and random: uniform random action selection. Note that while POMCP will converge to an
-optional solution, the solution quality may be poor when using small number of simulations.
The results for 4-agent and 10-agent firefighting problems with horizon 10 are shown in Figure 4(a). For the
4-agent problem, POMCP performs poorly with a few simulations, but as the number of simulations increases
it outperforms the other methods (presumably converging to an optimal solution). FT provides a high-quality
solution with a very small number of simulations, but the resulting value plateaus due to approximation error.
FS also provides a high-quality solution with a very small number of simulations, but is then able to converge
to a solution that is near POMCP. In the 10-agent problem, POMCP is only able to generate a solution that is
slightly better than random while the FV-POMCP methods are able to perform much better. In fact, FT performs

8

500 -93

1

100

1000

5000

10000

50000

POMCP (true)

-85.97

-77.74

-42.53

-28.48

-21.16

-21.26

-20.6

FS (true)

-85.97

-37.44

-27.14

-26.66

-24.67

-24.65

FT (true)

-85.97

-37.14

-37.7229

Random

-85.97

-100

0

10000
-37.3

20000
-37.6
FS (true)

Firefighting: 4 Agents

FT (true)

-23.36

-1749

50

-1765.95
FS
(true)

'1767.68 FT (true) '1765.91

5752.47
'3181.92
1.25479

10.6662
1.28973

1

'599.286

10.6662

1

'682.32

4997.73

10.8695

1

No learning

'1775.16 Random '1783.55

-5772.08
'1779.22

-5772.08
'1610.55

-5772.08
'804.615

-5772.08
4667

10.8695

1

Random

'1767.68

'1767.68

'1767.68

'1767.68

5600

-60

-86

3700

-75

-93

'611.47
0.948415

'1767.68

POMCP1100001
err
sims
7403.65

13.4053

Sensor Grid: 8 agents

4500

Sensor Grid:
Horizon 100
3000

Value

Value

Value

Value

1127.31
'5785.9
1.42464

'1746.04
-5772.08
0.948415

POMCP (true)

1500
0
-1500
-3000

1800
-4500

Random

Random
-100

100000

100

-100
10000

1000

Number of Simulations
POMCP (true)
FS (true)
-2000

FT (true)

Random
-6000

100000

100

1000

1000

FS

2000

FT

3000

4000

Number of Simulations
BA-POMCP

0

POMCP (true)

-26

Sensor Grid: Horizon 100
7500

POMCP (true)

POMCP (true)
5600

Value

-125

Value

FT (true)

5000

No learning

Firefighting: Horizon 50

Firefighting: Horizon 10

10000

Random
Number
of Simulations
POMCP (true)
FS (true)

FT (true)

0

(a) MPOMDP results

Value

36.3051
1.00768

'593.521

-79

-250

-58

3700

1800

-375

-74
-90

3907.14
1.25182

4247.21
'3220.85
1.24872

-45

-42

2715.05
'4460.1
1.20706

'603.59

7500

-10

'5814.7
2.01417

'1609.37

-72

Number of Simulations
POMCP (true)
FS (true)

5000
0
0.79721

-1664.6
-5772.08
0.948415

'1775.73

Firefighting: 10 Agents

10000

5000
'5488.18
1.99687

'1783.55

-30

1000

1000
500

1000
'5823.9
2.35085

'1692.69

Random
BA-POMCP

-65

100

100
100

500
'5796.7
1.14661

'1775.16

50000
-37.44

FT

-15

-90

10000 errors
1000

0
0
100
-5772.08
0.948415

'1767.68

30000 -36.91 FS40000

Number of Simulations
POMCP (true)

POMCP (true)

75000
0 POMCP (true)

-100

Random

Random
-500

0

125

FS

250

375

Number of Simulations

FT

BA-POMCP

500

0

50

FS

100

150

200

Number of Simulations

FT

BA-POMCP

Random

250

-2000

0

200

FS

400

600

800

1000

Number of Simulations
FT

BA-POMCP

(b) BA-MPOMDP results

Figure 4: Results for (a) the planning (MPOMDP) case (log scale x-axis) and (b) the learning (BA-MPOMDP)
case for the firefighting and sensor grid problems.
very well with a small number of samples and FS continues to improve until it reaches solution that is similar
to FT.
Similar results are seen in the sensor grid problem. POMCP outperforms a random policy as the number
of simulations grows, but FS and FT produce much higher values with 3the available simulations. FT seems
to converge to a low quality solution
(in both planning and learning) due to the loss of information about the
9
target’s previous position that is no longer known to local factors. In this problem, POMCP requires over 10
minutes for an episode of 10000 simulations, making reducing the number of simulations crucial in problems
of this size. These results clearly illustrate the benefit of FV-POMCP by exploiting structure for planning in
MASs.
BA-MPOMDPs. We also investigate the learning setting (i.e., when the agents are only given the BA-POMDP
model). Here, at the end of each episode, both the state and count vectors are reset to their initial values.
Learning in partially observable environments is extremely hard, and there may be many equivalence classes of
transition and observation models that are indistinguishable when learning. Therefore, we assume a reasonably
good model of the transitions (e.g., because the designer may have a good idea of the dynamics), but only a
poor estimate of the observation model (because the sensors may be harder to model).
For the BRL setting, we compare to the following baseline methods: POMCP: regular POMCP applied to
the true model using 100,000 simulations (this is the best proxy for, and we expect this to be very close to, the
optimal value), and BA-POMCP: regular POMCP applied to the BA-POMDP.
Results for a four agent instance of the fire fighting problem are shown in Fig. 4(b), for h = 10, 50. In both
cases, the FS and FT variants approach the POMCP value. For a small number of simulations FT learns very
quickly, providing significantly better values than the flat methods and better than FS for the increased horizon.
FS learns more slowly, but the value is better as the number of simulations increases (as seen in the horizon 10
case) due to the use of the full history. After more simulations in the horizon 10 problem, the performance of
the flat model (BA-MPOMDP) improves, but the factored methods still outperform it and this increase is less
9

1

1

visible for the longer horizon problem.
Similar results are again seen in the four agent sensor grid problem. FT performs the best with a small
number of simulations, but as the number increases, FS outperforms other methods. Again, for these problems,
BA-POMCP requires over 10 minutes for each episode for the largest number of simulations, showing the need
for more efficient methods. These experiments show that even in challenging multiagent settings with state
uncertainty, BRL methods can learn by effectively exploiting structure.

7

Related Work

MCTS methods have become very popular in games, a type of multiagent setting, but no action factorization
has been exploited so far (Browne et al., 2012). Progressive widening (Coulom, 2007) and double progressive
widening (Couëtoux et al., 2011) have had some success in games with large (or continuous) action spaces.
The progressive widening methods do not use the structure of the coordination graph in order to generalize
value over actions, but instead must find the correct joint action out of the exponentially many that are available
(which may require many trajectories). They are also designed for fully observable scenarios, so they do not
address the large observation space in MPOMDPs.
The factorization of the history in FTs is not unlike the use of linear function approximation for the state
components in TD-Search (Silver, Sutton, and Müller, 2012). However, in contrast to that method, due to our
particular factorization, we can still apply UCT to aggressively search down the most promising branches of the
tree. While other methods based on Q-learning (Guestrin, Lagoudakis, and Parr, 2002; Kok and Vlassis, 2006)
exploit action factorization, they assume agents observe individual rewards (rather than the global reward that
we consider) and it is not clear how these could be incorporated in a UCT-style algorithm.
Locality of interaction has also been considered previously in decentralized POMDP methods (Oliehoek,
2012; Amato et al., 2013) in the form of factored Dec-POMDPs (Oliehoek, Whiteson, and Spaan, 2013; Pajarinen and Peltonen, 2011) and networked distributed POMDPs (ND-POMDPs) (Nair et al., 2005; Kumar and
Zilberstein, 2009; Dibangoye et al., 2014). These models make strict assumptions about the information that
the agents can use to choose actions (only the past history of individual actions and observations), thereby
significantly lowering the resulting value (Oliehoek, Spaan, and Vlassis, 2008). ND-POMDPs also impose additional assumptions on the model (transition and observation independence and a factored reward function).
The MPOMDP model, in contrast, does not impose these restrictions. Instead, in MPOMDPs, each agent
knows the joint action-observation history, so there are not different perspectives by different agents. Therefore,
1) factored Dec-POMDP and ND-POMDP methods do not apply to MPOMDPs; they specify mappings from
individual histories to actions (rather than joint histories to joint actions), 2) ND-POMDP methods assume that
the value function is exactly factored as the sum of local values (‘perfect locality of interaction’) while in an
MPOMDP, the value is only approximately factored (since different components can correlate due to conditioning the actions on central information). While perfect locality of interaction allows a natural factorization of the
MPOMDP value function, but our method can be applied to any MPOMDP (i.e., given any factorization of the
value function). Furthermore, current factored Dec-POMDP and ND-POMDP models generate solutions given
the model in an offline fashion, while we consider online methods using a simulator in this paper.
Our approach builds upon coordination-graphs (Guestrin, Koller, and Parr, 2001), to perform the joint action
optimization efficiently, but factorization in one-shot problems has been considered in other settings too. Amin
et al. (Amin, Kearns, and Syed, 2011) present a method to optimize graphical bandits, which relates to our optimization approach. Since their approach replaced the UCB functionality, it is not obvious how their approach
could be integrated in POMCP. Moreover, their work, focuses on minimizing regret (which is not an issue in
our case), and does not apply when the factorization does not hold. Oliehoek et al. (Oliehoek, Whiteson, and
Spaan, 2012) present an factored-payoff approach that extends coordination graphs to imperfect information
settings where each agent has its own knowledge. This is not relevant for our current algorithm, which assumes
that joint observations will be received by a centralized decision maker, but could potentially be useful to relax
this assumption.

10

8

Conclusions

We presented the first method to exploit multiagent structure to produce a scalable method for Monte Carlo tree
search for POMDPs. This approach formalizes a team of agents as a multiagent POMDP, allowing planning
and BRL techniques from the POMDP literature to be applied. However, since the number of joint actions and
observations grows exponentially with the number of agents, naı̈ve extensions of single agent methods will not
scale well. To combat this problem, we introduced FV-POMCP, an online planner based on POMCP (Silver and
Veness, 2010) that exploits multiagent structure using two novel techniques—factored statistics and factored
trees— to reduce 1) the number of joint actions and 2) the number of joint histories considered. Our empirical
results demonstrate that FV-POMCP greatly increases scalability of online planning for MPOMDPs, solving
problems with 10 agents. Further investigation also shows scalability to the much more complex learning
problem with four agents. Our methods could also be used to solve POMDPs and BA-POMDPs with large
action and observation spaces as well the recent Bayes-Adaptive extension (Ng et al., 2012) of the self interested
I-POMDP model (Gmytrasiewicz and Doshi, 2005).

Acknowledgments
F.O. is funded by NWO Innovational Research Incentives Scheme Veni #639.021.336. C.A was supported by
AFOSR MURI project #FA9550-09-1-0538 and ONR MURI project #N000141110688.



Coordination of distributed agents is required for problems arising in many areas, including multi-robot systems, networking and e-commerce. As a formal framework for such
problems, we use the decentralized partially observable Markov decision process (DECPOMDP). Though much work has been done on optimal dynamic programming algorithms
for the single-agent version of the problem, optimal algorithms for the multiagent case have
been elusive. The main contribution of this paper is an optimal policy iteration algorithm
for solving DEC-POMDPs. The algorithm uses stochastic finite-state controllers to represent policies. The solution can include a correlation device, which allows agents to correlate
their actions without communicating. This approach alternates between expanding the
controller and performing value-preserving transformations, which modify the controller
without sacrificing value. We present two efficient value-preserving transformations: one
can reduce the size of the controller and the other can improve its value while keeping the
size fixed. Empirical results demonstrate the usefulness of value-preserving transformations
in increasing value while keeping controller size to a minimum. To broaden the applicability of the approach, we also present a heuristic version of the policy iteration algorithm,
which sacrifices convergence to optimality. This algorithm further reduces the size of the
controllers at each step by assuming that probability distributions over the other agents’
actions are known. While this assumption may not hold in general, it helps produce higher
quality solutions in our test problems.

1. Introduction
Markov decision processes (MDPs) provide a useful framework for solving problems of
sequential decision making under uncertainty. In some settings, agents must base their
decisions on partial information about the system state. In that case, it is often better to use
the more general framework of partially observable Markov decision processes (POMDPs).
Even more general are problems in which a team of decision makers, each with its own
c 2009 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Bernstein, Amato, Hansen, & Zilberstein

local observations, must act together. Domains in which these types of problems arise
include networking, multi-robot coordination, e-commerce, and space exploration systems.
The decentralized partially observable Markov decision process (DEC-POMDP) provides
an effective framework to model such problems. Though this model has been recognized for
decades (Witsenhausen, 1971), there has been little work on provably optimal algorithms
for it.
On the other hand, POMDPs have been studied extensively over the past few decades
(Smallwood & Sondik, 1973; Simmons & Koenig, 1995; Cassandra, Littman, & Zhang, 1997;
Hansen, 1998a; Bonet & Geffner, 2000; Poupart & Boutilier, 2003; Feng & Zilberstein, 2004;
Smith & Simmons, 2005; Smith, Thompson, & Wettergreen, 2007). It is well known that
a POMDP can be reformulated as an equivalent belief-state MDP. A belief-state MDP
cannot be solved in a straightforward way using MDP methods because it has a continuous
state space. However, Smallwood and Sondik showed how to implement value iteration by
exploiting the piecewise linearity and convexity of the value function. This work opened the
door for many algorithms, including approximate approaches and policy iteration algorithms
in which the policy is represented using a finite-state controller.
Extending dynamic programming for POMDPs to the multiagent case is not straightforward. For one thing, it is not clear how to define a belief state and consequently form a
belief-state MDP. With multiple agents, each agent has uncertainty about the observations
and beliefs of the other agents. Furthermore, the finite-horizon DEC-POMDP problem
with just two agents is complete for a higher complexity class than the single-agent version
(Bernstein, Givan, Immerman, & Zilberstein, 2002), indicating that these are fundamentally
different problems.
In this paper, we describe an extension of the policy iteration algorithm for single agent
POMDPs to the multiagent case. As in the single agent case, our algorithm converges
in the limit, and thus serves as the first nontrivial optimal algorithm for infinite-horizon
DEC-POMDPs. A few optimal approaches (Hansen, Bernstein, & Zilberstein, 2004; Szer,
Charpillet, & Zilberstein, 2005) and several approximate algorithms have been developed for
finite-horizon DEC-POMDPs (Peshkin, Kim, Meuleau, & Kaelbling, 2000; Nair, Pynadath,
Yokoo, Tambe, & Marsella, 2003; Emery-Montemerlo, Gordon, Schnieder, & Thrun, 2004;
Seuken & Zilberstein, 2007), but only locally optimal algorithms have been proposed for
the infinite-horizon case (Bernstein, Hansen, & Zilberstein, 2005; Szer & Charpillet, 2005;
Amato, Bernstein, & Zilberstein, 2007).
In our algorithmic framework, policies are represented using stochastic finite-state controllers. A simple way to implement this is to give each agent its own local controller. In
this case, the agents’ policies are all independent. A more general class of policies includes
those which allow agents to share a common source of randomness without sharing observations. We define this class formally, using a shared source of randomness called a correlation
device. The use of correlated stochastic policies in the DEC-POMDP context is novel. The
importance of correlation has been recognized in the game theory community (Aumann,
1974), but there has been little work on algorithms for finding correlated policies.
Each iteration of the algorithm consists of two phases. These are exhaustive backups,
which add nodes to the controller, and value-preserving transformations, which change the
controller without sacrificing value. We first provide a novel exposition of existing single90

Policy Iteration for DEC-POMDPs

agent algorithms using this two-phase view, and then we go on to describe the multiagent
extension.
There are many possibilities for value-preserving transformations. In this paper, we
describe two different types, both of which can be performed efficiently using linear programming. The first type allows us to remove nodes from the controller, and the second
allows us to improve the value of the controller while keeping its size fixed. Our empirical
results demonstrate the usefulness of value-preserving transformations in obtaining high
values while keeping controller size to a minimum.
We note that this work serves to unify and generalize previous work on dynamic programming for DEC-POMDPs. The first algorithm for the finite-horizon case (Hansen et al., 2004)
can be extended to the infinite-horizon case and viewed as interleaving exhaustive backups
and controller reductions. The bounded policy iteration algorithm for DEC-POMDPs (Bernstein et al., 2005), which extends a POMDP algorithm proposed by Poupart and Boutilier
(2003), can be viewed through the lens of our framework as repeated application of a specific
value-preserving transformation.
Because the optimal algorithm will not usually be able to return an optimal solution
in practice, we also introduce a heuristic version of the policy iteration algorithm. This
approach makes use of initial state information to focus policy search and further reduces
controller size at each step. To accomplish this, a forward search from the initial state
distribution is used to construct a set of belief points an agent would visit assuming the
other agents use given fixed policies. This search is conducted for each agent and then policy
iteration takes place while using the belief points to guide the removal of controller nodes.
The assumption that other agents use fixed policies causes the algorithm to no longer be
optimal, but it performs well in practice. We show that more concise and higher-valued
solutions can be produced compared to the optimal method before resources are exhausted.
The remainder of the paper is organized as follows. Section 2 introduces the formal
models of sequential decision making. Section 3 contains a novel presentation of existing
dynamic programming algorithms for POMDPs. In section 4, we present an extension of
policy iteration for POMDPs to the DEC-POMDP case, along with a convergence proof.
We discuss the heuristic version of policy iteration in section 5, followed by experiments
using policy iteration and heuristic policy iteration in section 6. Finally, section 7 contains
the conclusion and a discussion of possible future work.

2. Formal Model of Distributed Decision Making
We begin with a description of the formal framework upon which our work is based. This
framework extends the well-known Markov decision process to allow for distributed policy
execution. We also define an optimal solution for this model and discuss two different
representations for these solutions.
2.1 Decentralized POMDPs
A decentralized partially observable Markov decision process (DEC-POMDP) is defined for~ T, R, Ω,
~ Oi, where
mally as a tuple hI, S, A,
• I is a finite set of agents.
91

Bernstein, Amato, Hansen, & Zilberstein

Agent
Agent
a

s, r
System

a1

Agent
a

System

o, r
System

o1, r

a2

o2, r
Agent

(a)

(b)

(c)

Figure 1: (a) Markov decision process. (b) Partially observable Markov decision process.
(c) Decentralized partially observable Markov decision process with two agents.

• S is a finite set of states, with distinguished initial state s0 .
~ = ×i∈I Ai is a set of joint actions, where Ai is the set of actions for agent i.
• A
~ → ∆S is the state transition function, defining the distributions of states
• T : S×A
that result from starting in a given state and each agent performing an action.
~ → < is the reward function for the set of agents for each set of joint actions
• R : S ×A
and each state.
~ = ×i∈I Ωi is a set of joint observations, where Ωi contains observations for agent i.
• Ω
~ × S → ∆Ω
~ is an observation function, defining the distributions of observations
• O:A
for the set of agents that result from each agent performing an action and ending in
a given state.
The special case of a DEC-POMDP in which there is only one agent is called a partially
observable Markov decision process (POMDP).
In this paper, we consider the case in which the process unfolds over an infinite sequence
of stages. At each stage, all agents simultaneously select an action, and each receives the
global reward based on the reward function and a local observation based on the observation
function. Thus, the transitions, rewards and observations depend on the actions of all
agents, but each agent must act based only on local observations. This is illustrated in
Figure 1. The objective of the agents is to maximize the expected discounted sum of
rewards that are received, thus it is a cooperative framework. We denote the discount
factor β and require that 0 ≤ β < 1.
In a DEC-POMDP, the decisions of each agent affect all the agents in the domain, but
due to the decentralized nature of the model each agent must choose actions based solely on
local information. Because each agent receives a separate observation that does not usually
provide sufficient information to efficiently reason about the other agents, solving a DECPOMDP optimally becomes very difficult. For example, each agent may receive a different
92

Policy Iteration for DEC-POMDPs

(a)

(b)

Figure 2: A set of horizon three policy trees (a) and two node stochastic controllers (b) for
a two agent DEC-POMDP.

piece of information that does not allow a common state estimate or any estimate of the
other agents’ decisions to be calculated. These single estimates are crucial in single agent
problems, as they allow the agent’s history to be summarize concisely, but they are not
generally available in DEC-POMDPs. This is seen in the complexity of the finite-horizon
problem with at least two agents, which is NEXP-complete (Bernstein et al., 2002) and
thus in practice may require double exponential time. Like the infinite-horizon POMDP,
optimally solving an infinite-horizon DEC-POMDP is undecidable as it may require infinite
resources, but our method is able to provide a solution within  of the optimal with finite
time and memory. Nevertheless, introducing multiple decentralized agents causes a DECPOMDP to be significantly more difficult than a single agent POMDP.
2.2 Solution Representations
A local policy for an agent is a mapping from local action and observation histories to actions
while a joint policy is a set of policies, one for each agent in the problem. As mentioned
above, an optimal solution for a DEC-POMDP is the joint policy that maximizes the
expected sum of rewards that are received over the finite or infinite steps of the problem.
In infinite-horizon problems, the rewards are discounted to maintain a finite sum. Thus, an
optimal solution is a joint policy that provides the highest value starting at the given initial
state of the problem.
For finite-horizon problems, local policies can be represented using a policy tree as seen
in Figure 2a. Actions are represented by the arrows or stop figures (where each agent can
move in the given direction or stay where it is) and observations are labeled “wl” and “wr”
for seeing a wall on the left or the right respectively. Using this representation, an agent
takes the action defined at the root node and then after seeing an observation, chooses the
next action that is defined by the respective branch. This continues until the action at a
leaf node is executed. For example, agent 1 would first move left and then if a wall is seen
on the right, the agent would move left again. If a wall is now seen on the left, the agent
does not move on the final step. A policy tree is a record of the the entire local history for
an agent up to some fixed horizon and because each tree is independent of the others it can
93

Bernstein, Amato, Hansen, & Zilberstein

be executed in a decentralized manner. While this representation is useful for finite-horizon
problems, infinite-horizon problems would require trees of infinite height.
Another option used in this paper is to condition action selection on some internal
memory state. These solutions can be represented as a set of local finite-state controllers
(seen in Figure 2b). The controllers operate in a very similar way to the policy trees in
that there is a designated initial node and following the action selection at that node, the
controller transitions to the next node depending on the observation seen. This continues for
the infinite steps of the problem. Throughout this paper, controller states will be referred
to as nodes to help distinguish them from system states.
An infinite number of nodes may be required to define an optimal infinite-horizon DECPOMDP policy, but we will discuss a way to produce solutions within  of the optimal
with a fixed number of nodes. While deterministic action selection and node transitions are
sufficient to define this -optimal policy, when memory is limited stochastic action selection
and node transition may be beneficial. A simple example illustrating this for POMDPs
is given by Singh (1994), which can be easily extended to DEC-POMDPs. Intuitively,
randomness can help an agent to break out of costly loops that result from forgetfulness.
A formal description of stochastic controllers for POMDPs and DEC-POMDPs is given
in sections 3.2.1 and 4.1.1 respectively, but an example can be seen in Figure 2b. Agent 2
begins at node 1 and moves up with probability 0.89 and stays in place with probability
0.11. If the agent stayed in place and a wall was then seen on the left (observation “wl”), on
the next step, the controller would transition to node 1 and the agent would use the same
distribution of actions again. If a wall was seen on the right instead (observation “wr”),
there is a 0.85 probability that the controller will transition back to node 1 and a 0.15
probability that the controller will transition to node 2 for the next step. The finite-state
controller allows an infinite-horizon policy to be represented compactly by remembering
some aspects of the agent’s history without representing the entire local history.

3. Centralized Dynamic Programming
In this section, we cover the main concepts involved in dynamic programming for the single agent case. This will provide a foundation for the multiagent dynamic programming
algorithm described in the following section.
3.1 Value Iteration for POMDPs
Value iteration can be used to solve POMDPs optimally. This algorithm is more complicated
than its MDP counterpart, and does not have efficiency guarantees. However, in practice
it can provide significant leverage in solving POMDPs.
We begin by explaining how every POMDP has an equivalent MDP with a continuous
state space. Next, we describe how the value functions for this MDP have special structure
that can be exploited. These ideas are central to the value iteration algorithm.
3.1.1 Belief State MDPs
A convenient way to summarize the observation history of an agent in a POMDP is through
a belief state, which is a distribution over system states. As it receives observations, the
94

Policy Iteration for DEC-POMDPs

agent can update its belief state and then remove its observations from memory. Let b
denote a belief state, and let b(s) represent the probability assigned to state s by b. If an
agent chooses action a from belief state b and subsequently observes o, each component of
the successor belief state obeys the equation
P
P (o|a, s0 ) s∈S P (s0 |s, a)b(s)
0 0
b (s ) =
,
P (o|b, a)
where

"
P (o|b, a) =

X

#
0

P (o|a, s )

s0 ∈S

X

0

P (s |s, a)b(s) .

s∈S

Note that this is a simple application of Bayes’ rule.
It was shown by Astrom (1965) that a belief state constitutes a sufficient statistic for
the agent’s observation history, and it is possible to define an MDP over belief states as
follows. A belief-state MDP is a tuple hΠ, A, T, Ri, where
• Π is the set of distributions over S.
• A is the set of actions (same as before).
• T (b, a, b0 ) is the transition function, defined as
X
T (b, a, b0 ) =
P (b0 |b, a, o)P (o|b, a).
o∈O

• R(b, a) is a reward function, defined as
R(b, a) =

X

b(s)R(s, a).

s∈S

When combined with belief-state updating, an optimal solution to this MDP can be used
as an optimal solution to the POMDP from which it was constructed. However, since the
belief state MDP has a continuous, |S|-dimensional state space, traditional MDP techniques
are not immediately applicable.
Fortunately, dynamic programming can be used to find a solution to the belief state
MDP. The key result in making dynamic programming practical was proved by Smallwood
and Sondik (1973), who showed that the Bellman operator preserves piecewise linearity and
convexity of a value function. Starting with a piecewise linear and convex representation of
V t , the value function V t+1 is piecewise linear and convex, and can be computed in finite
time.
To represent a piecewise linear and convex value function, one need only store the value
of each facet for each system state. Denoting the set of facets Γ, we can store |Γ| |S|dimensional vectors of real values.PFor any single vector, γ ∈ Γ, we can define its value at
the belief state b with V (b, γ) = s∈S b(s)γ(s). Thus, to go from a set of vectors to the
value of a belief state, we use the equation
X
V (b) = max
b(s)γ(s).
γ

s∈S

95

Bernstein, Amato, Hansen, & Zilberstein

s1

s1

s2
(a)

s2
(b)

Figure 3: A piecewise linear and convex value function for a POMDP with two states (a)
and a non-minimal representation of a piecewise linear and convex value function
for a POMDP (b).

Figure 3a shows a piecewise linear and convex value function for a POMDP with two states.
Smallwood and Sondik proved that the optimal value function for a finite-horizon
POMDP is piecewise linear and convex. The optimal value function for an infinite-horizon
POMDP is convex, but may not be piecewise linear. However, it can be approximated
arbitrarily closely by a piecewise linear and convex value function, and the value iteration
algorithm constructs closer and closer approximations, as we shall see.
3.1.2 Pruning Vectors
Every piecewise linear and convex value function has a minimal set of vectors Γ that represents it. Of course, it is possible to use a non-minimal set to represent the same function.
This is illustrated in Figure 3b. Note that the removal of certain vectors does not change
the value of any belief state. Vectors such as these are not necessary to keep in memory.
Formally, we say that a vector γ is dominated if for all belief states b, there is a vector
γ̂ ∈ Γ \ γ such that V (b, γ) ≤ V (b, γ̂).
Because dominated vectors are not necessary, it would be useful to have a method for
removing them. This task is often called pruning, and has an efficient algorithm based
on linear programming. For a given vector γ, the linear program in Table 1 determines
whether γ is dominated. If variables can be found to make  positive, then adding γ to the
set improves the value function at some belief state. If not, then γ is dominated.
This gives rise to a simple algorithm for pruning a set of vectors Γ̃ to obtain a minimal
set Γ. The algorithm loops through Γ̃, removes each vector γ ∈ Γ̃, and solves the linear
program using γ and Γ̃ \ γ. If γ is not dominated, then it is returned to Γ̃.
It turns out that there is an equivalent way to characterize dominance that can be useful.
Recall that for a vector to be dominated, there does not have to be a single vector that has
value at least as high for all states. It is sufficient for there to exist a set of vectors such
that for all belief states, one of the vectors in the set has value at least as high as the vector
in question.
96

Policy Iteration for DEC-POMDPs

Variables: , b(s)
Objective: Maximize .
Improvement constraints:
X

∀γ̂

b(s)γ̂(s) +  ≤

s

X

b(s)γ(s)

s

Probability constraints:
X

∀s

b(s) = 1,

b(s) ≥ 0

s

Table 1: The linear program for testing whether a vector γ is dominated.

γ1

γ2
convex combination
γ3

s1

s2

Figure 4: The dual interpretation of dominance. Vector γ3 is dominated at all belief states
by either γ1 or γ2 . This is equivalent to the existence of a convex combination of
γ1 and γ2 which dominates γ3 for all belief states.

It can be shown that such a set exists if and only if there is some convex combination
of vectors that has value at least as high as the vector in question for all states. This is
shown graphically in Figure 4. If we take the dual of the linear program for dominance
given in the previous section, we get a linear program for which the solution is a vector of
probabilities for the convex combination. This dual view of dominance was first used in a
POMDP context by Poupart and Boutilier (2003), and is useful for policy iteration, as will
be explained later.
3.1.3 Dynamic Programming Update
In this section, we describe how to implement a dynamic programming update to go from a
value function Vt to a value function Vt+1 . In terms of implementation, our aim is to take
a minimal set of vectors Γt that represents Vt and produce a minimal set of vectors Γt+1
that represents Vt+1 .
97

Bernstein, Amato, Hansen, & Zilberstein

Each vector that could potentially be included in Γt+1 represents the value of an action a
and assignment of vectors in Γt to observations. A combination of an action and transition
rule will hereafter be called a one-step policy. The value vector for a one-step policy can
be determined by considering the action taken, the resulting state transitioned to and
observation seen and the value of the assigned vector at step t. This is given via the
equation
X
γit+1 (s) = R(s, α(i)) + β
P (s0 |s, α(i))P (o|α(i), s0 )γτt (i,o) (s0 ),
s0 ,o

where i is the index of the vector, α(i) is its action, and τ (i, o) is the index of the vector in
Γt to which to transition upon receiving observation o and β is the discount factor. More
details on the derivation and use of this formula are provided by Zhang and Zhang (2001).
There are |A||Γt ||Ω| possible one-step policies. A simple way to construct Γt+1 is to
evaluate all possible one-step policies and then apply a pruning algorithm such as Lark’s
method (Lark III, 1990). Evaluating the entire set of one-step policies will hereafter be
called performing an exhaustive backup. It turns out that there are ways to perform a
dynamic programming update without first performing an exhaustive backup. Below we
describe two approaches to doing this.
The first approach uses the fact that it is simple to find the optimal vector for any
particular belief state. For a belief state b, an optimal action can be determined via the
equation
"
#
X
P (o|b, a)V t (T (b|a, o)) .
α = argmaxa∈A R(b, a) + β
o∈Ω

For each observation o, there is a subsequent belief state, which can be computed using
Bayes’ rule. To get an optimal transition rule, τ (o), we take the optimal vector for the
belief state corresponding to o.
Since the backed-up value function has finitely many vectors, there must be a finite set
of belief states for which backups must be performed. Algorithms which identify these belief
states include Smallwood and Sondik’s “one-pass” algorithm (1973), Cheng’s linear support
and relaxed region algorithms (Cheng, 1988), and Kaelbling, Cassandra and Littman’s
Witness algorithm (1998).
The second approach is based on generating and pruning sets of vectors. Instead of
generating all vectors and then pruning, these techniques attempt to prune during the generation phase. The first algorithm along these lines was the incremental pruning algorithm
(Cassandra et al., 1997). Recently, improvements have been made to this approach (Zhang
& Lee, 1998; Feng & Zilberstein, 2004, 2005).
It should be noted that there are theoretical complexity barriers for DP updates. Littman
et al. (1995) showed that under certain widely believed complexity theoretic assumptions,
there is no algorithm for performing a DP update that is worst-case polynomial in all the
quantities involved. Despite this fact, dynamic programming updates have been successfully implemented as part of the value iteration and policy iteration algorithms, which will
be described in the subsequent sections.
98

Policy Iteration for DEC-POMDPs

3.1.4 Value Iteration
To implement value iteration, we simply start with an arbitrary piecewise linear and convex
value function, and proceed to perform DP updates. This corresponds to value iteration in
the equivalent belief state MDP, and thus converges to an -optimal value function after a
finite number of iterations.
Value iteration returns a value function, but a policy is needed for execution. As in the
MDP case, we can use one-step lookahead, using the equation
"
#
X
X
δ(b) = argmaxa∈A
R(s, a)b(s) + β
P (o|b, a)V (τ (b, o, a)) ,
s∈S

o∈Ω

where τ (b, o, a) is the belief state resulting from starting in belief state b, taking action a,
and receiving observation o. We note that a state estimator must be used as well to track
the belief state. Using the fact that each vector corresponds to a one-step policy, we can
extract a policy from the value of the vectors:
!
X
δ(b) = α argmaxk
b(s)γk (s)
s

While the size of the resulting set of dominant vectors may remain exponential, in many
cases it is much smaller. This can significantly simplify computation.
As in the completely observable case, the Bellman residual provides a bound on the
distance to optimality. Recall that the Bellman residual is the maximum distance across all
belief states between the value functions of successive iterations. It is possible to find the
maximum distance between two piecewise linear and convex functions in polynomial time
with an algorithm that uses linear programming (Littman et al., 1995).
3.2 Policy Iteration for POMDPs
With value iteration, a POMDP is viewed as a belief-state MDP, and a policy is a mapping
from belief states to actions. An early policy iteration algorithm developed by Sondik used
this policy representation (Sondik, 1978), but it was very complicated and did not meet
with success in practice. We shall describe a different approach that has performed better
on test problems. With this approach, a policy is represented as a finite-state controller.
3.2.1 Finite-State Controllers
Using a finite-state controller, an agent has a finite number of internal states. Its actions
are based only on its internal state, and transitions between internal states occur when
observations are received. Internal states provide agents with a kind of memory, which can
be crucial for difficult POMDPs. Of course, an agent’s memory is limited by the number
of internal states it possesses. In general, an agent cannot remember its entire history of
observations, as this would require infinitely many internal states. An example of a finitestate controller can be seen by considering only one agent’s controller in Figure 2b. The
operation of a single controller is the same as that for each agent in the decentralized case.
We formally define a controller as a tuple hQ, Ω, A, ψ, ηi, where
99

Bernstein, Amato, Hansen, & Zilberstein

• Q is a finite set of controller nodes.
• Ω is a set of inputs, taken to be the observations of the POMDP.
• A is a set of outputs, taken to be the actions of the POMDP.
• ψ : Q → ∆A is an action selection function, defining the distribution of actions
selected at each node.
• η : Q × A × Ω → ∆Q is a transition function, defining the distribution of resulting
nodes for each initial node and action taken.
For each state and starting node of the controller, there is an expected discounted sum
of rewards over the infinite horizon. It can be computed using the following system of linear
equations, one for each s ∈ S and q ∈ Q:


X
X
V (s, q) =
P (a|q) R(s, a) + β
P (o, s0 |s, a)P (q 0 |q, a, o)V (s0 , q 0 ) .
s0 ,o,q 0

a

Where P (a|q) is the probability action a will be taken in node q and P (q 0 |q, a, o) is the
probability the controller will transition to node q 0 from node q after action a was taken
and o was observed.
We sometimes refer to the value of the controller at a belief state. For a belief state b,
this is defined as
X
V (b) = max
b(s)V (s, q).
q

s

Thus, it is assumed that, given an initial state distribution, the controller is started in the
node which maximizes value from that distribution. Once execution has begun, however,
there is no belief state updating. In fact, it is possible for the agent to encounter the same
belief state twice and be in a different internal state each time.
3.2.2 Algorithmic Framework
We will describe the policy iteration algorithm in abstract terms, focusing on the key components necessary for convergence. In subsequent sections, we present different possibilities
for implementation.
Policy iteration takes as input an arbitrary finite-state controller. The first phase of an
iteration consists of evaluating the controller, as described above. Recall that value iteration
was initialized with an arbitrary piecewise linear and convex value function, represented by
a set of vectors. In policy iteration, the piecewise linear and convex value function arises
out of evaluation of the controller. Each controller node has a value when paired with each
state. Thus, each node has a corresponding vector and thus a linear value function over
belief state space. Choosing the best node for each belief state yields a piecewise linear and
convex value function.
The second phase of an iteration is the dynamic programming update. In value iteration, an update produces an improved set of vectors, where each vector corresponds to
a deterministic one-step policy. The same set of vectors is produced in this case, but the
100

Policy Iteration for DEC-POMDPs

Input: A finite state controller, and a parameter .
1. Evaluate the finite-state controller by solving a system of linear equations.
2. Perform a dynamic programming update to add a set of deterministic nodes to the
controller.
3. Perform value-preserving transformations on the controller.
4. Calculate the Bellman residual. If it is less than (1 − β)/2β, then terminate.
Otherwise, go to step 1.
Output: An -optimal finite-state controller.
Table 2: Policy Iteration for POMDPs.
actions and transition rules for the one-step policy cannot be removed from memory. Each
new vector is actually a node that gets added to the controller. All of the probability distributions for the added nodes are deterministic. That is, a exhaustive backup in this context
creates a new node for each possible action and possible combinations of observations and
deterministic transitions into the current controller. This results in the same one-step policies being considered as in the dynamic programming update described above. As there
are |A||Γt ||Ω| possible one-step polices, this number also defines the number of new nodes
added to the controller after an exhaustive backup.
Finally, additional operations are performed on the controller. There are many such
operations, and we describe two possibilities in the following section. The only restriction
placed on these operations is that they do not decrease the value for any belief state. Such
an operation is denoted a value-preserving transformation.
The complete algorithm is outlined in Table 2. It is guaranteed to converge to a finitestate controller that is -optimal for all belief states within a finite number of steps. Furthermore, the Bellman residual can be used to obtain a bound on the distance to optimality,
as with value iteration.
3.2.3 Controller Reductions
In performing a DP update, potential nodes that are dominated do not get added to the
controller. However, after the update is performed, some of the old nodes may have become
dominated. These nodes cannot simply be removed, however, as other nodes may transition
into them. This is where the dual view of dominance is useful. Recall that if a node is
dominated, then there is a convex combination of other nodes with value at least as high
from all states. Thus, we can remove the dominated node and merge it into the dominating
convex combination by changing transition probabilities accordingly. This operation was
proposed by Poupart and Boutilier (2003) and built upon earlier work by Hansen (1998b).
Formally, a controller reduction attempts to replace a node q ∈ Q with a distribution
P (q̂) over nodes q̂ ∈ Q \ q such that for all s ∈ S,
X
V (s, q) ≤
P (q̂)V (s, q̂).
q̂∈Q\q

101

Bernstein, Amato, Hansen, & Zilberstein

Variables: , x(γ̂)
Objective: Maximize 
Improvement constraints:
∀s

V (s, γ) +  ≤

X

x(γ̂)V (s, γ̂)

γ̂

Probability constraints:
X

∀γ̂

x(γ̂) = 1,

x(γ̂) ≥ 0

γ̂

Table 3: The dual linear program for testing dominance for the vector γ. The variable x(γ̂)
represents P (γ̂).

This can be achieved by solving the linear program in Table 3. As nodes are used rather
than vectors, we replace x(γ̂) with x(q̂) in the dual formulation which provides a probability distribution of nodes which dominate node q. Rather than transitioning into q, this
distribution can then be used instead. It can be shown that if such a distribution is found
and used for merging, the resulting controller is a value-preserving transformation of the
original one.
3.2.4 Bounded Backups
In the previous section, we described a way to reduce the size of a controller without
sacrificing value. The method described in this section attempts to increase the value of
the controller while keeping its size fixed. It focuses on one node at a time, and attempts to
change the parameters of the node such that the value of the controller is at least as high
for all belief states. The idea for this approach originated with Platzman (1980), and was
made efficient by Poupart and Boutilier (2003).
In this method, a node q is chosen, and parameters for the conditional distribution
P (a, q 0 |q, o) are to be determined. Determining these parameters works as follows. We
assume that the original controller will be used from the second step on, and try to replace
the parameters for q with better ones for just the first step. In other words, we look for
parameters which satisfy the following inequality:


X
X
V (s, q) ≤
P (a|q) R(s, a) + β
P (q 0 |q, a, o)P (o, s0 |s, a)V (s0 , q 0 )
a

s0 ,o,q 0

for all s ∈ S. Note that the inequality is always satisfied by the original parameters.
However, it is often possible to get an improvement.
The new parameters can be found by solving a linear program, as shown in Table 4.
Note that the size of the linear program is polynomial in the sizes of the POMDP and the
controller. We call this process a bounded backup because it acts like a dynamic programming
102

Policy Iteration for DEC-POMDPs

Variables: , x(a), x(a, o, q 0 )
Objective: Maximize 
Improvement constraints:

∀s

V (s, q) +  ≤

X



x(a)R(s, a) + β

x(a) = 1,

∀a, o

X

x(a, o, q 0 ) = x(a)

q0

a

∀a

x(a, o, q 0 )P (o, s0 |s, a)V (s0 , q 0 )

s0 ,o,q 0

a

Probability constraints:
X

X

x(a) ≥ 0,

∀a, o, q 0

x(a, o, q 0 ) ≥ 0

Table 4: The linear program to be solved for a bounded backup. The variable x(a) represents P (a|q), and the variable x(a, o, q 0 ) represents P (a, q 0 |q, o).

backup with memory constraints. To see this, consider the set of nodes generated by a DP
backup. These nodes dominate the original nodes across all belief states, so for every
original node, there must be a convex combination of the nodes in this set that dominate
the original node for all states. A bounded backup finds such a convex combination.
It can be shown that a bounded backup yields a value-preserving transformation. Repeated application of bounded backups can lead to a local optimum, at which none of the
nodes can be improved any further. Poupart and Boutilier (2003) showed that a local optimum has been reached when each node’s value function is touching the value function
produced by performing a full DP backup. This is illustrated in Figure 5.

4. Decentralized Dynamic Programming
In the previous section, we presented dynamic programming for POMDPs. A key part of
POMDP theory is the fact that every POMDP has an equivalent belief-state MDP. No
such result is known for DEC-POMDPs, making it difficult to generalize value iteration to
the multiagent case. This lack of a shared belief-state requires a new set of tools to be
developed for solving DEC-POMDP. As a step in this direction, we were able to develop an
optimal policy iteration algorithm for DEC-POMDPs that includes the POMDP version as
a special case. This algorithm is the focus of the section.
We first show how to extend the definition of a stochastic controller to the multiagent
case. Multiagent controllers include a correlation device, which is a source of randomness
shared by all the agents. This shared randomness increases solution quality while minimally
increasing representation size without adding communication. As in the single agent case,
policy iteration alternates between exhaustive backups and value-preserving transforma103

Bernstein, Amato, Hansen, & Zilberstein

value function after
DP update
value function for
controller

s1

s2

Figure 5: A local optimum for bounded backups. The solid line is the value function for the
controller, and the dotted line is the value function for the controller that results
from a full DP update.

tions. A convergence proof is given, along with efficient transformations that extend those
presented in the previous section.
4.1 Correlated Finite-State Controllers
The joint policy for the agents is represented using a stochastic finite-state controller for
each agent. In this section, we first define a type of controller in which the agents act
independently. We then provide an example demonstrating the utility of correlation, and
show how to extend the definition of a controller to allow for correlation among agents.
4.1.1 Local Finite-State Controllers
In a local controller, the agent’s node is based on the local observations received, and the
agent’s action is based on the current node. These local controllers are defined in the same
way as the POMDP controllers above, with each agent possessing its own controller that
operates independently of the others. As before, stochastic transitions and action selection
are allowed.
We formally define a local controller for agent i as a tuple hQi , Ωi , Ai , ψi , ηi i, where
• Qi is a finite set of controller nodes.
• Ωi is a set of inputs, taken to be the local observations for agent i.
• Ai is a set of outputs, taken to be the actions for agent i.
• ψi : Qi → ∆Ai is an action selection function for agent i, defining the distribution of
actions selected at each node of that agent’s controller.
• ηi : Qi × Ai × Ωi → ∆Qi is a transition function for agent i, defining the distribution
of resulting nodes for each initial node and action taken of that agent’s controller.
The functions ψi and ηi parameterize the conditional distribution P (ai , qi0 |qi , oi ) which represents the combined action selection and node transition probability for agent i. When
104

Policy Iteration for DEC-POMDPs

AB
BA
BB

AA

AA
AB
BA

+R

–R

–R

s1

s2

+R

BB

Figure 6: A DEC-POMDP for which a correlated joint policy yields more reward than the
optimal independent joint policy.

taken together, the agents’ controllers determine the conditional distribution P (~a, ~q 0 |~q, ~o).
This is denoted an independent joint controller. In the following subsection, we show that
independence can be limiting.
4.1.2 The Utility of Correlation
The joint controllers described above do not allow the agents to correlate their behavior
via a shared source of randomness. We will use a simple example to illustrate the utility
of correlation in partially observable domains where agents have limited memory. This
example generalizes the one given by Singh (1994) to illustrate the utility of stochastic
policies in partially observable settings containing a single agent.
Consider the DEC-POMDP shown in Figure 6. This problem has two states, two agents,
and two actions per agent (A and B). The agents each have only one observation, and
thus cannot distinguish between the two states. For this example, we will consider only
memoryless policies.
Suppose that the agents can independently randomize their behavior using distributions
P (a1 ) and P (a2 ). If the agents each choose either A or B according to a uniform distribution,
then they receive an expected reward of − R2 per time step, and thus an expected long-term
−R
reward of 2(1−β)
. It is straightforward to show that no independent policy yields higher
reward than this one for all states.
Next, let us consider the even larger class of policies in which the agents may act in a
correlated fashion. In other words, we consider all joint distributions P (a1 , a2 ). Consider
the policy that assigns probability 21 to the pair AA and probability 12 to the pair BB. This
yields an average reward of 0 at each time step and thus an expected long-term reward of
0. The difference between the rewards obtained by the independent and correlated policies
can be made arbitrarily large by increasing R.
105

Bernstein, Amato, Hansen, & Zilberstein

4.1.3 Correlated Joint Controllers
In the previous subsection, we established that correlation can be useful in the face of
limited memory. In this subsection, we extend our definition of a joint controller to allow for
correlation among the agents. To do this, we introduce an additional finite-state machine,
called a correlation device, that provides extra signals to the agents at each time step. The
device operates independently of the DEC-POMDP process, and thus does not provide
agents with information about the other agents’ observations. In fact, the random numbers
necessary for its operation could be determined prior to execution time and made available
to all agents.
Formally, a correlation device is a tuple hQc , ψc i, where Qc is a set of nodes and ψc :
Qc → ∆Qc is a state transition function. At each step, the device undergoes a transition,
and each agent observes its state.
We must modify the definition of a local controller to take the state of the correlation
device as input. Now, a local controller for agent i is a conditional distribution of the
form P (ai , qi0 |qc , qi , oi ). The correlation device together with the local controllers form a
joint conditional distribution P (~a, ~q 0 |~q, ~o), where ~q = hqc , q1 , . . . , qn i. We will refer to this
as a correlated joint controller. Note that a correlated joint controller with |Qc | = 1 is
effectively an independent joint controller. Figure 7 contains a graphical representation of
the probabilistic dependencies in a correlated joint controller.
The value function for a correlated joint controller can be computed by solving the
~
following system of linear equations, one for each s ∈ S and ~q ∈ Q:

V (s, ~q) =

X



P (~a|~q) R(s, ~a) + β

X

P (s0 , ~o|s, ~a)P (~q 0 |~q, ~a, ~o)V (s0 , ~q 0 ) .

s0 ,~
o,~
q0

~a

We sometimes refer to the value of the controller for an initial state distribution. For a
distribution b, this is defined as
V (b) = max
q~

X

b(s)V (s, ~q).

s

It is assumed that, given an initial state distribution, the controller is started in the joint
node which maximizes value from that distribution.
It is worth noting that correlation can increase the value of a set of fixed-size controllers,
but this same value can be achieved by a larger set of uncorrelated controllers. Thus,
correlation is a way to make better use of limited representation size, but is not required
to produce a set of optimal controllers. This is formalized by the following theorem, which
is proved in Appendix A. The theorem asserts the existence of uncorrelated controllers;
determining how much extra memory is needed to replace a correlation device remains an
open problem.
Theorem 1 Given an initial state and a correlated joint controller, there always exists
some finite-size joint controller without a correlation device that produces at least the same
value for the initial state.
106

Policy Iteration for DEC-POMDPs

a1

q1

q2

a2

q’2

o2

qc
o1

q’1

Figure 7: A graphical representation of the probabilistic dependencies in a correlated joint
controller for two agents.

In the example above, higher value can be achieved with two node uncorrelated controllers for each agent. If the problem starts in s1 , the first node for each agent would choose
A and transition to the second node which would choose B. The second node would then
transition back to the first node. The resulting policy consists of the agents alternating
R
between choosing AA and BB, producing an expected long-term reward of 1−β
which is
higher than the correlated one node policy value of 0. Thus, doubling memory for each
agent in this problem is sufficient to remove the correlation device.
4.2 Policy Iteration
In this section, we describe the policy iteration algorithm. We first extend the definitions of
exhaustive backup and value-preserving transformation to the multiagent case. Following
that, we provide a description of the complete algorithm, along with a convergence proof.
4.2.1 Exhaustive Backups
We introduced exhaustive backups in the section on dynamic programming for POMDPs.
We stated that one way to implement a DP update was to perform an exhaustive backup,
and then prune dominated nodes that were created. More efficient implementations were
described thereafter. These implementations involved interleaving pruning with node generation.
For the multiagent case, it is an open problem whether pruning can be interleaved
with node generation. Nodes can be removed, as we will show in a later subsection, but for
convergence we require exhaustive backups. We do not define DP updates for the multiagent
case, and instead make exhaustive backups a central component of our algorithm.
An exhaustive backup adds nodes to the local controllers for all agents at once, and
leaves the correlation device unchanged. For each agent i, |Ai ||Qi ||Ωi | nodes are added
to the
Q local controller, one for each one-step policy. Thus, the joint controller grows by
|Qc | i |Ai ||Qi ||Oi | joint nodes.
Note that repeated application of exhaustive backups amounts to a brute force search
in the space of deterministic policies. This converges to optimality, but is obviously quite
inefficient. As in the single agent case, we must modify the joint controller in between
107

Bernstein, Amato, Hansen, & Zilberstein

adding new nodes. For convergence, these modifications must preserve value in a sense that
will be made formal in the following section.
4.2.2 Value-Preserving Transformations
We now extend the definition of a value-preserving transformation to the multiagent case.
In the following subsection, we show how this definition allows for convergence to optimality
as the number of iterations grows.
The dual interpretation of dominance is helpful in understanding multiagent valuepreserving transformations. Recall that for a POMDP, we say that a node is dominated if
there is a convex combination of other nodes with value at least as high for all states. Though
we defined a value-preserving transformation in terms of the value function across belief
states, we could have equivalently defined it so that every node in the original controller
has a dominating convex combination in the new controller.
For the multiagent case, we do not have the concept of a belief state MDP, so we take
the second approach mentioned above. In particular, we require that dominating convex
combinations exist for nodes of all the local controllers and the correlation device. A transformation of a controller C to a controller D qualifies as a value-preserving transformation
if C ≤ D, where ≤ is defined below.
~ and R,
~ respectively. We
Consider correlated joint controllers C and D with node sets Q
say that C ≤ D if there exist mappings fi : Qi → ∆Ri for each agent i and fc : Qc → ∆Rc
such that
X
V (s, ~q) ≤
P (~r|~q)V (s, ~r)
~
r

~ Note that this relation is transitive as further value-preserving
for all s ∈ S and ~q ∈ Q.
transformations of D will also be value-preserving transformations of C.
~ → ∆R.
~ Examples
We sometimes describe the fi and fc as a single mapping f : Q
of efficient value-preserving transformations are given in a later section. In the following subsection, we show that alternating between exhaustive backups and value-preserving
transformations yields convergence to optimality.
4.2.3 Algorithmic Framework
The policy iteration algorithm is initialized with an arbitrary correlated joint controller. In
the first part of an iteration, the controller is evaluated via the solution of a system of linear
equations. Next, an exhaustive backup is performed to add nodes to the local controllers.
Finally, value-preserving transformations are performed.
In contrast to the single agent case, there is no Bellman residual for testing convergence to -optimality. We resort to a simpler test for -optimality based on the discount
rate and the number of iterations so far. Let |Rmax | be the largest absolute value of an
immediate reward possible in the DEC-POMDP. Our algorithm terminates after iteration
t+1 |R
max |
t if β 1−β
≤ . At this point, due to discounting, the value of any policy after step t is
less than . Justification for this test is provided in the convergence proof. The complete
algorithm is sketched in Table 5.
Before proving convergence, we state a key lemma regarding the ordering of exhaustive
backups and value-preserving transformations. Its proof is deferred to the Appendix.
108

Policy Iteration for DEC-POMDPs

Input: A correlated joint controller, and a parameter .
1. Evaluate the correlated joint controller by solving a system of linear equations.
2. Perform an exhaustive backup to add deterministic nodes to the local controllers.
3. Perform value-preserving transformations on the controller.
t+1

|Rmax |
4. If β 1−β
≤ , where t is the number of iterations so far, then terminate. Else go
to step 1.

Output: A correlated joint controller that is -optimal for all states.
Table 5: Policy Iteration for DEC-POMDPs.
Lemma 1 Let C and D be correlated joint controllers, and let Ĉ and D̂ be the results of
performing exhaustive backups on C and D, respectively. Then Ĉ ≤ D̂ if C ≤ D.
Thus, if there is a value-preserving transformation mapping controller C to D and both are
exhaustively backed up, then there is a value-preserving transformation mapping controller
Ĉ to D̂. This allows value-preserving transformations to be performed before exhaustive
backups, while ensuring that value is not lost after the backup. We can now state and prove
the main convergence theorem for policy iteration.
Theorem 2 For any , policy iteration returns a correlated joint controller that is -optimal
for all initial states in a finite number of iterations.
Proof: Repeated application of exhaustive backups amounts to a brute force search in
the space of deterministic joint policies. Thus, after t exhaustive backups, the resulting
controller is optimal for t steps from any initial state. Let t be an integer large enough that
β t+1 |Rmax |
≤ . Then any possible discounted sum of rewards after t time steps is small
1−β
enough that optimality over t time steps implies -optimality over the infinite horizon.
Now recall the above lemma, which states that performing value-preserving transformations before a backup provides at least as much value as just performing a backup. By an
inductive argument, performing t steps of policy iteration is a value-preserving transformation of the result of t exhaustive backups. We have argued that for large enough t, the value
of the controller resulting from t exhaustive backups is within  of optimal for all states.
Thus, the result of t steps of policy iteration is also within  of optimal for all states. 2
4.3 Efficient Value-Preserving Transformations
In this section, we describe how to extend controller reductions and bounded backups
to the multiagent case. We will show that both of these operations are value-preserving
transformations.
4.3.1 Controller Reductions
Recall that in the single agent case, a node can be removed if for all belief states, there is
another node with value at least as high. The equivalent dual interpretation is that a node
109

Bernstein, Amato, Hansen, & Zilberstein

can be removed is there exists a convex combination of other nodes with value at least as
high across the entire state space.
Using the dual interpretation, we can extend this to a rule for removing nodes in the
multiagent case. The rule applies to removing nodes either from a local controller or from the
correlation device. Intuitively, in considering the removal of a node from a local controller
or the correlation device, we consider the nodes of the other controllers to be part of the
hidden state.
More precisely, suppose we are considering removing node qi from agent i’s local controller. To do this, we need to find a distribution P (q̂i ) over nodes q̂i ∈ Qi \ qi such that for
all s ∈ S, q−i ∈ Q−i , and qc ∈ Qc ,
V (s, qi , q−i , qc ) ≤

X

P (q̂i )V (s, q̂i , q−i , qc ).

q̂i

where Q−i represents the set of nodes for the other agents. Finding such a distribution
can be formulated as a linear program, as shown in Table 6a. In this case, success is
finding parameters such that  ≥ 0. The linear program is polynomial in the sizes of the
DEC-POMDP and controllers, but exponential in the number of agents.
If we are successful in finding parameters that make  ≥ 0, then we can merge the
dominated node into the convex combination of other nodes by changing all incoming links
to the dominated controller node to be redirected based on the distribution P (q̂i ). At this
point, there is no chance of ever transitioning into qi , and thus it can be removed.
The rule for the correlation device is very similar. Suppose that we are considering the
removal of node qc . In this case, we need to find a distribution P (q̂c ) over nodes q̂c ∈ Qc \ qc
~
such that for all s ∈ S and ~q ∈ Q,
V (s, ~q, qc ) ≤

X

P (q̂c )V (s, ~q, q̂c ).

q̂c

~ for the set of tuples of local controller nodes,
Note that we abuse notation here and use Q
excluding the nodes for the correlation device. As in the previous case, finding parameters
can done using linear programming. This is shown in Table 6b. This linear program is also
polynomial in the the sizes of the DEC-POMDP and controllers, but exponential in the
number of agents.
We have the following theorem, which states that controller reductions are value-preserving
transformations.
Theorem 3 Any controller reduction applied to either a local node or a node of the correlation device is a value-preserving transformation.
Proof: Suppose that we have replaced an agent i node qi with a distribution over nodes
in Qi \ qi . Let us take fi to be the identity map for all nodes except qi , which will map to
the new distribution. We take fc to be the identity map, and we take fj to be the identity
map for all j 6= i. This yields a complete mapping f . We must now show that f satisfies
the condition given in the definition of a value-preserving transformation.
110

Policy Iteration for DEC-POMDPs

(a) Variables: , x(q̂i )
Objective: Maximize 
Improvement constraints:
∀s, q−i , qc

V (s, qi , q−i , qc ) +  ≤

X

x(q̂i )V (s, q̂i , q−i , qc )

q̂i

Probability constraints:
X

∀q̂i

x(q̂i ) = 1,

x(q̂i ) ≥ 0

q̂i

(b) Variables: , x(qc )
Objective: Maximize 
Improvement constraints:
∀s, ~q V (s, ~q, qc ) +  ≤

X

x(q̂c )V (s, ~q, q̂c )

qˆc

Probability constraints:
X

∀q̂c

x(q̂c ) = 1,

x(q̂c ) ≥ 0

q̂c

Table 6: (a) The linear program to be solved to find a replacement for agent i’s node qi .
The variable x(q̂i ) represents P (q̂i ). (b) The linear program to be solved to find a
replacement for the correlation node qc . The variable x(q̂c ) represents P (q̂c ).

Let Vo be the value function for the original controller, and let Vn be the value function
for the controller with qi removed. A controller reduction requires that
Vo (s, ~q) ≤

X

P (~r|~q)Vo (s, ~r)

~
r

~ Thus, we have
for all s ∈ S and ~q ∈ Q.

Vo (s, ~q) =

X

P (~a|~q) R(s, a) + β


X

P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)Vo (s0 , ~q 0 )

s0 ,~
o,~
q0

~a


≤

X
~a

P (~a|~q) R(s, a) + β


X

P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)

s0 ,~
o,~
q0

111

X
~
r0

P (~r|~q)Vo (s, ~r 0 )

Bernstein, Amato, Hansen, & Zilberstein


=

X

P (~a|~q) R(s, a) + β


X

P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)P (~r|~q)Vo (s, ~r 0 )

s0 ,~
o,~
q 0 ,~
r0

~a

~ Notice that the formula on the right is the Bellman operator
for all s ∈ S and ~q ∈ Q.
for the new controller, applied to the old value function. Denoting this operator Tn , the
system of inequalities implies that Tn Vo ≥ Vo . By monotonicity, we have that for all k ≥ 0,
Tnk+1 (Vo ) ≥ Tnk (Vo ). Since Vn = limk→∞ Tnk (Vo ), we have that Vn ≥ Vo . This is sufficient
for f to satisfy the condition in the definition of value-preserving transformation.
The argument for removing a node of the correlation device is almost identical to the
one given above. 2
4.3.2 Bounded Dynamic Programming Updates
In the previous section, we described a way to reduce the size of a controller without
sacrificing value. Recall that in the single agent case, we could also use bounded backups
to increase the value of the controller while keeping its size fixed. This technique can
be extended to the multiagent case. As in the previous section, the extension relies on
improving a single local controller or the correlation device, while viewing the nodes of the
other controllers as part of the hidden state.
We first describe in detail how to improve a local controller. To do this, we choose an
agent i, along with a node qi . Then, for each oi ∈ Ωi , we search for new parameters for the
conditional distribution P (ai , qi0 |qi , oi ).
The search for new parameters works as follows. We assume that the original controller
will be used from the second step on, and try to replace the parameters for qi with better
ones for just the first step. In other words, we look for parameters satisfying the following
inequality:


X
X
V (s, ~q) ≤
P (~a|~q) R(s, a) + β
P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)V (s0 , ~q 0 )
~a

s0 ,~
o,~
q0

for all s ∈ S, q−i ∈ Q−i , and qc ∈ Qc . The search for new parameters can be formulated as a
linear program, as shown in Table 7a. Its size is polynomial in the sizes of the DEC-POMDP
and the joint controller, but exponential in the number of agents.
The procedure for improving the correlation device is very similar to the procedure for
improving a local controller. We first choose a device node qc , and consider changing its
parameters for just the first step. We look for parameters satisfying the following inequality:


X
X
P (~a|~q) R(s, a) + β
P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)V (s0 , ~q 0 )
V (s, ~q) ≤
~a

s0 ,~
o,~
q0

~
for all s ∈ S and ~q ∈ Q.
As in the previous case, the search for parameters can be formulated as a linear program.
This is shown in Table 7b. This linear program is also polynomial in the sizes of the DECPOMDP and joint controller, but exponential in the number of agents.
The following theorem states that bounded backups preserve value.
112

Policy Iteration for DEC-POMDPs

(a) Variables: , x(qc , ai ), x(qc , ai , oi , qi0 )
Objective: Maximize 
Improvement constraints:
∀s, q−i , qc

X

V (s, ~q, qc ) +  ≤

P (a−i |qc , q−i )[x(qc , ai )R(s, ~a) +

~a

β

X

0
x(c, ai , oi , qi0 )P (q−i
|qc , q−i , a−i , o−i )

s0 ,~
o,~
q 0 ,qc0

· P (~o, s0 |s, ~a)P (qc0 |qc )V (s0 , ~q 0 , qc0 )]

Probability constraints:
X
∀qc
x(qc , ai ) = 1,

∀qc , ai , oi

x(qc , ai , oi , qi0 ) = x(qc , ai )

qi0

ai

∀qc , ai

X

x(qc , ai ) ≥ 0,

∀qc , ai , oi , qi0

x(qc , ai , oi , qi0 ) ≥ 0

(b) Variables: , x(qc0 )
Objective: Maximize 
Improvement constraints:
∀s, ~q V (s, ~q, qc ) +  ≤

X

P (~a|qc , ~q)[R(s, ~a) + β

X

P (~q 0 |qc , ~q, ~a, ~o)

s0 ,~
o,~
q 0 ,qc0

~a
0

· P (s , ~o|s, ~a)x(qc0 )V (s0 , ~q 0 , qc0 )]

Probability constraints:
∀qc0

X

x(qc0 ) = 1,

∀qc0

x(qc0 ) ≥ 0

qc0

Table 7: (a) The linear program used to find new parameters for agent i’s node qi . The
variable x(qc , ai ) represents P (ai |qi , qc ), and the variable x(qc , ai , oi , qi0 ) represents
P (ai , qi0 |qc , qi , oi ). (b) The linear program used to find new parameters for the
correlation device node qc . The variable x(qc0 ) represents P (qc0 |qc ).

113

Bernstein, Amato, Hansen, & Zilberstein

Theorem 4 Performing a bounded backup on a local controller or the correlation device
produces a new correlated joint controller which is a value-preserving transformation of the
original.
Proof: Consider the case in which some node qi of agent i’s local controller is changed.
We define f to be a deterministic mapping from nodes in the original controller to the
corresponding nodes in the new controller.
Let Vo be the value function for the original controller, and let Vn be the value function
for the new controller. Recall that the new parameters for P (ai , qi0 |qc , qi , oi ) must satisfy
the following inequality for all s ∈ S, q−i ∈ Q−i , and qc ∈ Qc :


X
X
Vo (s, ~q) ≤
P (~a|~q) R(s, a) + β
P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)Vo (s0 , ~q 0 ) .
~a

s0 ,~
o,~
q0

Notice that the formula on the right is the Bellman operator for the new controller, applied
to the old value function. Denoting this operator Tn , the system of inequalities implies
that Tn Vo ≥ Vo . By monotonicity, we have that for all k ≥ 0, Tnk+1 (Vo ) ≥ Tnk (Vo ). Since
Vn = limk→∞ Tnk (Vo ), we have that Vn ≥ Vo . Thus, the new controller is a value-preserving
transformation of the original one.
The argument for changing nodes of the correlation device is almost identical to the one
given above. 2
4.4 Open Issues
We noted at the beginning of the section that there is no known way to convert a DECPOMDP into an equivalent belief-state MDP. Despite this fact, we were able to develop
a provably convergent policy iteration algorithm. However, the policy iteration algorithm
for POMDPs has other desirable properties besides convergence, and we have not yet been
able to extend these to the multiagent case. Two such properties are described below.
4.4.1 Error Bounds
The first property is the existence of a Bellman residual. In the single agent case, it
is possible to compute a bound on the distance to optimality using two successive value
functions. In the multiagent case, policy iteration produces a sequence of controllers, each
of which has a value function. However, we do not have a way to obtain an error bound
from these value functions. For now, to bound the distance to optimality, we must consider
the discount rate and the number of iterations completed.
4.4.2 Avoiding Exhaustive Backups
In performing a DP update for POMDPs, it is possible to remove certain nodes from
consideration without first generating them. In Section 3, we gave a high-level description
of a few different approaches to doing this. For DEC-POMDPs, however, we did not define
a DP update and instead used exhaustive backups as the way to expand a controller. Since
exhaustive backups are expensive, it would be useful to extend the more sophisticated
pruning methods for POMDPs to the multiagent case.
114

Policy Iteration for DEC-POMDPs

Input: A joint controller, the desired number of centralized belief points k, initial state
b0 and fixed policy for each agent πi .
1. Starting from b0 , sample a set of k belief points for each agent assuming the other
agents use their fixed policy.
2. Evaluate the joint controller by solving a system of linear equations.
3. Perform an exhaustive backup to add deterministic nodes to the local controllers.
4. Retain nodes that contribute the highest value at each of the belief points.
5. For each agent, replace nodes that have lower value than some combination of other
nodes at each belief point.
6. If controller sizes and parameters do not change then terminate. Else go to step 2.
Output: A new joint controller based on the sampled centralized belief points.
Table 8: Heuristic Policy Iteration for DEC-POMDPs.

Unfortunately, in the case of POMDPs, the proofs of correctness for these methods all
use the fact that there exists a Bellman equation. Roughly speaking, this equation allows us
to determine whether a potential node is dominated by just analyzing the nodes that would
be its successors. Because we do not currently have an analog of the Bellman equation for
DEC-POMDPs, we have not been able to generalize these results.
There is one exception to the above statement, however. When an exhaustive backup
has been performed for all agents except one, then a type of belief state space can be
constructed for the agent in question using the system states and the nodes for the other
agents. The POMDP node generation methods can then be applied to just that agent. In
general, though, it seems difficult to rule out a node for one agent before generating all the
nodes for the other agents.

5. Heuristic Policy Iteration
While the optimal policy iteration method shows how a set of controllers with value arbitrarily close to optimal can be found, the resulting controllers may be very large and many
unnecessary nodes may be generated along the way. This is exacerbated by the fact that
the algorithm cannot take advantage of an initial state distribution and must attempt to
improve the controller for any initial state. As a way to combat these disadvantages, we
have developed a heuristic version of policy iteration that removes nodes based on their
value only at a given set of centralized belief points. We call these centralized belief points
because they are distributions over the system state that in general could only be known
by full observability of the problem. As a result, the algorithm will no longer be optimal,
but it can often produce more concise controllers with higher solution quality for a given
initial state distribution.
115

Bernstein, Amato, Hansen, & Zilberstein

5.1 Directed Pruning
Our heuristic policy iteration algorithm uses sets of belief points to direct the pruning process of our algorithm. There are two main advantages of this approach: it allows simultaneous pruning for all agents and it focuses the controller on certain areas of the belief space.
We first discuss the benefits of simultaneous pruning and then mention the advantages of
focusing on small areas of the belief space.
As mentioned above, the pruning method used by the optimal algorithm will not always
remove all nodes that could be removed from all the agents’ controllers without losing
value. Because pruning requires each agent to consider the controllers of other agents, after
nodes are removed for one agent, the other agents may be able to prune other nodes. Thus
pruning must cycle through the agents and ceases when no agent can remove any further
nodes. This is both time consuming and causes the controller to be much larger than it
needs to be.
Like the game theoretic concept of incredible threats1 , a set of suboptimal policies for
an agent may be useful only because other agents may employ similarly suboptimal policies. That is, because pruning is conducted for each agent while holding the other agents’
policies fixed, polices that are useful for any set of other agent policies are retained, no
matter the quality of these other agent policies. Some of an agent’s policies may only be
retained because they have the highest value when used in conjunction with other suboptimal policies of the other agents. In these cases, only by removing the set of suboptimal
policies simultaneously can controller size be reduced while at least maintaining value. This
simultaneous pruning could further reduce controller sizes and thus increase scalability and
solution quality. While it may be possible to define a value-preserving transformation for
these problems, finding a nontrivial automated way to do so while maintaining the optimality of the algorithm remains an open question.
The advantage of considering a smaller part of the state space has already been shown
to produce drastic performance increases in POMDPs (Ji, Parr, Li, Liao, & Carin, 2007;
Pineau, Gordon, & Thrun, 2003) and finite-horizon DEC-POMDPs (Seuken & Zilberstein,
2007; Szer & Charpillet, 2006). For POMDPs, a problem with many states has a belief
space with large dimensionality, but many parts may never be visited by an optimal policy.
Focusing on a subset of belief states can allow a large part of the state space to be ignored
without significant loss of solution quality.
The problem of having a large state space is compounded in the DEC-POMDP case.
Not only is there uncertainty about the state, but also about the policies of the other agents.
As a consequence, the generalized belief space which includes all possible distributions over
states of the system and current policies of the other agents must be considered to guarantee
optimality. This results in a huge space which contains many unlikely states and policies.
The uncertainty about which policies other agents may utilize does not allow belief updates
to normally be calculated for DEC-POMDPs, but as we showed above, it can be done by
assuming a probability distribution over actions of the other agents. This limits the number
of policies that need to be considered by all agents and if the distributions are chosen well,
may permit a high-valued solution to be found.
1. An incredible threat is an irrational strategy that the agent knows it will receive a lower value by choosing
it. While it is possible the agent will choose the incredible threat strategy, it is irrational to do so.

116

Policy Iteration for DEC-POMDPs

Variables: , x(q̂i ) and for each belief point b
Objective: Maximize 
Improvement constraints:

∀b, q−i

X

X

b(s)
x(q̂i )V (q̂i , q−i , s) − V (~q, s) ≥ 

s

X

Probability constraints:

q̂i

x(q̂i ) = 1 and ∀q̂i x(qi ) ≥ 0

q̂i

Table 9: The linear program used to determine if a node q for agent i is dominated at
each point b and all initial nodes of the other agents’ controllers. As node q
may be dominated by a distribution of nodes, variable x(q̂i ) represents P (q̂i ), the
probability of starting in node q̂ for agent i.

5.2 Belief Set Generation
As mentioned above, our heuristic policy iteration algorithm constructs sets of belief points
for each agent which are later used to evaluate the joint controller and remove dominated
nodes. To generate the belief point set, we start at the initial state and by making assumptions about the other agents, we can calculate the resulting belief state for each action and
observation pair of an agent. By fixing the policies for the other agents, this belief state update can be calculated in a way very similar to that described for POMDPs in section 3.1.1.
This procedure can be repeated from each resulting belief state until a desired number of
points is generated or no new points are visited.
More formally, we assume the other agents have a fixed distribution of action choice for
each system state. That is, if we know P (~a−i |s) then we can determine the probability any
state results given a belief point and an agent’s action and observation. The derivation of
the likelihood of state s0 , given the belief state b, and agent i’s action ai and observation oi
is shown below.

P (s0 |ai , oi , b) =

X

P (s0 , ~a−i , ~o−i , s|ai , oi , b)

~a−i ,~
o−i ,s

o|s, b, ~a, s0 )P (s0 , s, ~a, b)
~a−i ,~
o−i ,s P (~

P
=

P (oi , ai , b)
o|s, ~a, s0 )P (s0 |s, ~a, b)P (~a, s, b)
~a−i ,~
o−i ,s P (~

P
=

P (oi , ai , b)
o|s, ~a, s0 )P (s0 |s, ~a)P (~a−i |a, s, b)P (~a, s, b)
~a−i ,~
o−i ,s P (~

P
=

P (oi , ai , b)
0 )P (s0 |s, ~
P
(~
o
|s,
~
a
,
s
a)P (~a−i |ai , s, b)P (s|ai , b)P (ai , b)
~a−i ,~
o−i ,s

P
=

P (oi , ai , b)
117

Bernstein, Amato, Hansen, & Zilberstein

P
=

o|s, ~a, s
~a−i ,~
o−i ,s P (~

0 )P (s0 |s, ~
a)P (~a−i |s)b(s)

P (oi |ai , b)

where
X

P (oi |ai , b) =

P (~o|s, ~a, s0 )P (s0 |s, ~a)P (~a−i |s)b(s)

a−i ,o−i ,s,s0

Thus, given the action probabilities for the other agents, −i, and the transition and observation models of the system, a belief state update can be calculated.
5.3 Algorithmic Framework
We provide a formal description of our approach in Table 8. Given the desired number
of belief points, k, and random action and observation selection for each agent, the sets
of points are generated as described above. The search begins at the initial state of the
problem and continues until the given number of points is obtained. If no new points are
found, this process can be repeated to ensure a diverse set is produced. The arbitrary initial
controller is evaluated and the value at each state and for each initial node of any agent’s
controller is retained. The exhaustive backup procedure is exactly the same as the one used
in the optimal algorithm, but updating the controller takes place in two steps. First, for
each of the k belief points, the highest-valued set of initial nodes is found. To accomplish
this, the value of beginning at each combination of nodes for all agents is calculated for each
of these k points and the best combination is kept. This allows nodes that do not contribute
to any of these values to be simultaneously pruned. Next, each node of each agent is pruned
using the linear program shown in Table 9. If a distribution of nodes for the given agent has
higher value at each of the belief points for any initial nodes of the other agents’ controllers,
it is pruned and replaced with that distribution. The new controllers are then evaluated
and the value is compared with the value of the previous controller. This process of backing
up and pruning continues while the controller parameters continue to change.
Similar to how bounded policy updates can be used in conjunction with pruning in the
optimal policy iteration algorithm, a nonlinear programming approach (Amato et al., 2007)
can be used to improve solution quality for the heuristic case. To accomplish this, instead of
optimizing the controller for just the initial belief state of the problem, all the belief points
being considered are used. A simple way to achieve this is to maximize over the sum of
the values of the initial nodes of the controllers weighted by the probabilities given for each
point. This approach can be used after each pruning step and may further improve value
of the controllers.

6. Dynamic Programming Experiments
This section describes the results of experiments performed using policy iteration. Because
of the flexibility of the algorithm, it is impossible to explore all possible ways of implementing
it. However, we did experiment with a few different implementation strategies to gain an
idea of how the algorithm works in practice. All of these experiments were run on a 3.40GHz
Intel Pentium 4 with 2GB of memory. Three main sets of experiments were performed on
a single set of test problems.
118

Policy Iteration for DEC-POMDPs

Our first set of experiments focused on exhaustive backups and controller reductions.
The results confirm that value improvement can be obtained through iterated application of
these two operations. Further improvement is demonstrated by also incorporating bounded
updates. However, because exhaustive backups are expensive, the algorithm was unable to
complete more than a few iterations on any of our test problems.
In the second set of experiments, we addressed the complexity issues by using only
bounded backups, and no exhaustive backups. With bounded backups, we were able to
obtain higher-valued controllers while keeping memory requirements fixed. We examined
how the sizes of the initial local controllers and the correlation device affected the value of
the final solution.
The third set of experiments examined the complexity issues caused by exhaustive backups by using the point-based heuristic. This allowed our heuristic policy iteration algorithm
to complete more iterations than the optimal algorithm and in doing so, increased solution
quality of the largest solvable controllers. By incorporating Amato et al.’s NLP approach,
the heuristic algorithm becomes slightly less scalable than with heuristic pruning alone, but
the amount of value improvement per step increases. This causes the resulting controllers
in each domain to have the highest value of any approach.
6.1 Test Domains
In this section, we describe three test domains, ordered by the size of the problem representation. For each problem, the transition function, observation function, and reward
functions are described. In addition, an initial state is specified. Although policy iteration
does not require an initial state as input, one is commonly assumed and is used by the
heuristic version of the algorithm. A few different initial states were tried for each problem,
and qualitatively similar results were obtained. In all domains, a discount factor of 0.9 was
utilized.
As a very loose upper bound, the centralized policy was calculated for each problem in
which all agents share their observations with a central agent and decisions for all agents are
made by the central agent. This results in a POMDP with the same number of states, but
the action and observation sets are Cartesian products of the agents action and observation
sets. The value of this POMDP policy is provided below, but because DEC-POMDP policies
are more constrained, the optimal value may be much lower.
Two Agent Tiger Problem
The two agent tiger problem consists of 2 states, 3 actions and 2 observations (Nair et al.,
2003). The domain includes two doors, one of which leads to a tiger and the other to a large
treasure. Each agent may open one of the doors or listen. If either agent opens the door
with the tiger behind it, a large penalty is given. If the door with the treasure behind it is
opened and the tiger door is not, a reward is given. If both agents choose the same action
(i.e., both opening the same door) a larger positive reward or a smaller penalty is given to
reward cooperation. If an agent listens, a small penalty is given and an observation is seen
that is a noisy indication of which door the tiger is behind. While listening does not change
the location of the tiger, opening a door causes the tiger to be placed behind one of the
119

Bernstein, Amato, Hansen, & Zilberstein

door with equal probability. The problem begins with the tiger equally likely to be located
behind either door. The optimal centralized policy for this problem has value 59.817.
Meeting on a Grid
In this problem, with 16 states, 5 actions and 4 observations, two robots must navigate on a
two-by-two grid. Each robot can only sense whether there are walls to its left or right, and
their goal is to spend as much time as possible on the same square as the other agent. The
actions are to move up, down, left, or right, or to stay on the same square. When a robot
attempts to move to an open square, it only goes in the intended direction with probability
0.6, otherwise it either goes in another direction or stays in the same square. Any move
into a wall results in staying in the same square. The robots do not interfere with each
other and cannot sense each other. The reward is 1 when the agents share a square, and
0 otherwise. The initial state places the robots diagonally across from each other and the
optimal centralized policy for this problem has value 7.129.
Box Pushing Problem
This problem, with 100 states, 4 actions and 5 observations consists of two agents that
get rewarded by pushing different boxes (Seuken & Zilberstein, 2007). The agents begin
facing each other in the bottom corners of a four-by-three grid with the available actions of
turning right, turning left, moving forward or staying in place. There is a 0.9 probability
that the agent will succeed in moving and otherwise will stay in place, but the two agents
can never occupy the same square. The middle row of the grid contains one large box in the
middle of two small boxes. The small boxes can be moved by a single agent, but the large
box can only be moved by both agents pushing at the same time. The upper row of the
grid is considered the goal row, which the boxes are pushed into. The possible deterministic
observations for each agent consist of seeing an empty space, a wall, the other agent, a small
box or the large box. A reward of 100 is given if both agents push the large box to the
goal row and 10 is given for each small box that is moved to the goal row. A penalty of -5
is given for each agent that cannot move and -0.1 is given for each time step. Once a box
is moved to the goal row, the environment resets to the original start state. The optimal
centralized policy for this problem has value 183.936.
6.2 Exhaustive Backups and Controller Reductions
In this section, we present the results of using exhaustive backups together with controller
reductions. For each domain, the initial controllers for each agent contained a single node
with a self loop, and there was no correlation device. For each problem, the first action
of the problem description was used. This resulted in the repeated actions of opening the
left door in the two agent tiger problem, moving up in the meeting on a grid problem and
turning left in the box pushing problem. The reason for starting with the smallest possible
controllers was to see how many iterations we could complete before running out of memory.
On each iteration, we performed an exhaustive backup, and then alternated between
agents, performing controller reductions until no more nodes could be removed. For bounded
dynamic programming results, after the reductions were completed bounded updates were
also performed for all agents. For these experiments, we attempted to improve the nodes of
120

Policy Iteration for DEC-POMDPs

Iteration
0
1
2
3

Two Agent Tiger, |S| = 2, |Ai | = 3, |Ωi | = 2
Exhaustive Sizes
Controller Reductions
Bounded Updates
(1, 1)
-150 (1,1 in 1s)
-150 (1,1 in 1s)
(3, 3)
-137 (3,3 in 1s)
-20 (3,3 in 12s)
(27, 27)
-117.8 (15, 15 in 7s)
-20 (15, 15 in 89s)
(2187, 2187)
-98.9 (255, 255 in 1301s) -20* (255, 255 in 3145s)

Iteration
0
1
2

Meeting on a Grid, |S| = 16, |Ai | = 5, |Ωi | = 4
Exhaustive Sizes
Controller Reductions
Bounded Updates
(1, 1)
2.8 (1,1 in 1s)
2.8 (1,1 in 1s)
(5, 5)
3.4 (5,5 in 7s)
3.8 (5,5 in 145s)
(3125, 3125)
3.7 (80,80 in 821s)
4.78* (125,125 in 1204s)

Iteration
0
1
2

Box Pushing, |S| = 100, |Ai | = 4, |Ωi | = 5
Exhaustive Sizes
Controller Reductions
Bounded Updates
(1, 1)
-2 (1,1 in 4s)
-2 (1,1 in 53s)
(4, 4)
-2 (2,2 in 108s)
6.3 (2,2 in 132s)
(4096, 4096)
12.8 (9,9 in 755s)
42.7* (16,17 in 714s)

Table 10: Results of applying exhaustive backups, controller reductions and bounded updates to our test problems. The second column contains the sizes of the controllers
if only exhaustive backups had been performed. The third column contains the
resulting value, sizes of the controllers, and time required for controller reductions
to be performed on each iteration. The fourth column displays these same quantities with bounded updates also being used. The * denotes that a backup and
pruning were performed, but bounded updates exhausted the given resources.

each agent in turn until value could not be improved for any node of any agent. For each
iteration, we recorded the sizes of the controllers produced, and noted what the sizes would
be if no controller reductions had been performed. In addition, we recorded the value from
the initial state and the total time taken to reach the given result.
The results are shown in Table 10. Because exhaustive backups add many nodes, we
were unable to complete many iterations without exceeding memory limits. As expected,
the smallest problem led to the largest number of iterations being completed. Although
we could not complete many iterations before running out of memory, the use of controller
reductions led to significantly smaller controllers compared to the approach of just applying
exhaustive backups. Incorporating bounded updates requires some extra time, but is able
to improve the value produced at each step, causing substantial improvement in some cases.
It is also interesting to notice that the controller sizes when using bounded updates are
not always the same as when only controller reductions are completed. This can be seen
after two iterations in both the meeting on a grid and box pushing problems. This can
occur because the bounded updates change node value and thus change the number and
location of the nodes that are pruned. In the box pushing problem, the two agents also
121

Bernstein, Amato, Hansen, & Zilberstein

have different size controllers after two steps. This can occur, even in symmetric problems,
when a set of actions is only necessary for a single agent.
6.3 Bounded Dynamic Programming Updates
As we saw from the previous experiments, exhaustive backups can fill up memory very
quickly. This leads naturally to the question of how much improvement is possible without
exhaustive backups. In this section, we describe an experiment in which we repeatedly
applied bounded backups, which left the size of the controller fixed. We experimented with
different starting sizes for the local controllers and the correlation device.
We define a trial run of the algorithm as follows. At the start of a trial run, a size is
chosen for each of the local controllers and the correlation device. The action selection and
transition functions are initialized to be deterministic, with the outcomes drawn according
to a uniform distribution. A step consists of choosing a node uniformly at random from the
correlation device or one of the local controllers, and performing a bounded backup on that
node. After 200 steps, the run is considered over. In practice, we found that values often
stabilized in fewer steps.
We varied the sizes of the local controllers while maintaining the same number of nodes
for each agent, and we varied the size of the correlation device from 1 to 2. For each domain,
we increased number of nodes until the required number of steps could not be completed
in under four hours. In general, runs required significantly less time to terminate. For each
combination of sizes, we performed 20 trial runs and recorded the best value over all runs.
For each of the three problems, we were able to obtain solutions with higher value than
with exhaustive backups. Thus, we see that even though repeated application of bounded
backups does not have an optimality guarantee, it can be competitive with an algorithm that
does. However, it should be noted that we have not performed an exhaustive comparison.
We could have made different design decisions for both approaches concerning the starting
controllers, the order in which nodes are considered, and other factors.
Besides comparing to the exhaustive backup approach, we wanted to examine the effect
of the sizes of the local controllers and the correlation device on value. Figure 8 shows
a graph of best values plotted against controller size. We found that, for the most part,
the value increases when we increase the size of the correlation device from one node to
two nodes (essentially moving from independent to correlated). It is worth noting that the
solution quality had somewhat high variance in each problem, showing that setting good
initial parameters is important for high-valued solutions.
For small controllers, the best value tends to increase with controller size. However, for
very large controllers, this not always the case. This can be explained by considering how a
bounded backup works. For new node parameters to be acceptable, they must not decrease
the value for any combination of states, nodes for the other controllers, and nodes for the
correlation device. This becomes more difficult as the numbers of nodes increase, and thus
it is easier to get stuck in a local optimum. This can be readily seen in the two agent tiger
problem and to some extent the meeting on a grid problem. Memory was exhausted before
this phenomenon takes place in the box pushing problem.
122

Policy Iteration for DEC-POMDPs

(a)

(b)

(c)
Figure 8: Best value per trial run plotted against the size of the local controllers, for (a)
the two agent tiger problem, (b) the meeting in a grid problem and (c) the box
pushing problem. The solid line represents independent controllers (a correlation
device with one node), and the dotted line represents a joint controller including a
two-node correlation device. Times ranged from under 1s for one node controllers
without correlation to four hours for the largest controller found with correlation
in each problem.

6.4 Heuristic Dynamic Programming Updates
As observed above, the optimal dynamic programming approach can only complete a small
number of backups before resources are exhausted. Similarly, using bounded updates with
fixed size controllers can generate high value solutions, but it can be difficult to pick the
correct controller size and initial parameters. As an alternative to the other approaches, we
also present experiments using our heuristic dynamic programming algorithm.
Like the optimal policy iteration experiments, we initialized single node controllers for
each agent with self loops and no correlation device. The same first actions were used as
above and backups were performed until memory was exhausted. The set of belief points
for each problem was generated given the initial state distribution and a distribution of
actions for the other agents. For the meeting on a grid and box pushing problems, it was
123

Bernstein, Amato, Hansen, & Zilberstein

assumed that all agents chose any action with equal probability regardless of state. For the
two agent tiger problem, it was assumed that for any state agents listen with probability 0.8
and open each door with probability 0.1. This simple heuristic policy was chosen to allow
more of the state space to be sampled by our search. The number of belief points used for
the two agent tiger and meeting on a grid problems was ten and twenty points were used
for the box pushing problem.
For each iteration, we performed an exhaustive backup and then pruned controllers as
described in steps four and five of Table 8. All the nodes that contributed to the highest
value for each belief point were retained and then each node was examined using the linear
program in Table 9. For results with the NLP approach, we also improved the set of
controllers after heuristic pruning by optimizing a nonlinear program whose objective was
the sum of the values of the initial nodes weighted by the belief point probabilities. We
report the value produced by the optimal and heuristic approaches for each iteration that
could be completed in under four hours and with the memory limits of the machine used.
The nonlinear optimization was performed on the NEOS server, which provides a set of
machines with varying CPU speeds and memory limitations.
The values for each iteration of each problem are given in Figure 9. We see the heuristic policy iteration (HPI) methods are able to complete more iterations than the optimal
methods and as a consequence produce higher values. In fact, the results from HPI are
almost always exactly the same as those for the optimal policy iteration algorithm without
bounded updates for all iterations that can be completed by the optimal approach. Thus,
improvement occurs primarily due to the larger number of backups that can be performed.
We also see that while incorporating bounded updates improves value for the optimal
algorithm, incorporating the NLP approach into the heuristic approach produces even higher
value. Optimizing the NLP requires a small time overhead, but substantially increases
value on each iteration. This results in the highest controller value in each problem. Using
the NLP also allows our heuristic policy iteration to converge to a six node controller for
each agent in the two agent tiger problem. Unfortunately, this solution is known to be
suboptimal. As an heuristic algorithm, this is not unexpected, and it should be noted that
even suboptimal solutions by the heuristic approach outperform all other methods in all
our test problems.
6.5 Discussion
We have demonstrated how policy iteration can be used to improve both correlated and
independent joint controllers. We showed that using controller reductions together with
exhaustive backups is more efficient in terms of memory than using exhaustive backups
alone. However, due to the complexity of exhaustive backups, even that approach could
only complete a few iterations on each of our test problems.
Using bounded backups alone provided a good way to deal with the complexity issues.
With bounded backups, we were able to find higher-valued policies than with the previous
approach. Through our experiments, we were able to understand how the sizes of the local
controllers and correlation device affect the final values obtained.
With our heuristic policy iteration algorithm, we demonstrated further improvement by
dealing with some of the complexity issues. The heuristic approach is often able to continue
124

Policy Iteration for DEC-POMDPs

(a)

(b)

(c)
Figure 9: Comparison of the dynamic programming algorithms on (a) the two agent tiger
problem, (b) the meeting in a grid problem and (c) the box pushing problem.
The value produced by policy iteration with and without bounded backups as
well as our heuristic policy iteration with and without optimizing the NLP were
compared on each iteration until the time or memory limit was reached.

improving solution quality past the point where the optimal algorithm exhausts resources.
More efficient use of this limited representation size is achieved by incorporating the NLP
approach as well. In fact, the heuristic algorithm with NLP improvements at each step
provided results that are at least equal to the highest value obtained in each problem and
sometimes were markedly higher than the other approaches. Furthermore, as far as we
know, these results are the highest published values for all three of the test domains.

7. Conclusion
We present a policy iteration algorithm for DEC-POMDPs. The algorithm uses a novel policy representation consisting of stochastic finite-state controllers for each agent along with
a correlation device. We define value-preserving transformations and show that alternating
between exhaustive backups and value-preserving transformations leads to convergence to
125

Bernstein, Amato, Hansen, & Zilberstein

optimality. We also extend controller reductions and bounded backups from the single agent
case to the multiagent case. Both of these operations are value-preserving transformations
and are provably efficient. Finally, we introduced a heuristic version of our algorithm which
is more scalable and produces higher values on our test problems. Our algorithm serves as
the first nontrivial exact algorithm for DEC-POMDPs, and provides a bridge to the large
body of work on dynamic programming for POMDPs.
Our work provides a solid foundation for solving DEC-POMDPs, but much work remains
in addressing more challenging problem instances. We focused on solving general DECPOMDPs, but the efficiency of our approaches could be improved by using structure found
in certain problems. This would allow specialized representations and solution techniques
to be incorporated. Below we describe some key challenges of our general approach, along
with some preliminary algorithmic ideas to extend our work on policy iteration.
Approximation with Error Bounds Often, strict optimality requirements cause computational difficulties. A good compromise is to search for policies that are within some
bound of optimal. Our framework is easily generalized to allow for this.
Instead of a value-preserving transformation, we could define an -value-preserving transformation, which insures that the value at all states decreases by at most . We can perform
such transformations with no modifications to any of our linear programs. We simply need
to relax the requirement on the value for  that is returned. It is easily shown that using
an -value-preserving transformation at each step leads to convergence to a policy that is
β
within 1−β
of optimal for all states.
For controller reductions, relaxing the tolerance may lead to smaller controllers because
some value can be sacrificed. For bounded backups, it may help in escaping from local
optima. Though relaxing the tolerance for a bounded backup could lead to a decrease in
value for some states, a small “downward” step could lead to higher value overall in the
long run. We are currently working on testing these hypotheses empirically.
General-Sum Games In a general-sum game, there is a set of agents, each with its own
set of strategies, and a strategy profile is defined to be a tuple of strategies for all agents.
Each agent assigns a payoff to each strategy profile. The agents may be noncooperative, so
the same strategy profile may be assigned different values for each agent.
The DEC-POMDP model can be extended to a general-sum game by allowing each
agent to have its own reward function. In this case, the strategies are the local policies, and
a strategy profile is a joint policy. This model is often called a partially observable stochastic
game (POSG). Hansen et al. (2004) presented a dynamic programming algorithm for finitehorizon POSGs. The algorithm was shown to perform iterated elimination of dominated
strategies in the game. Roughly speaking, it eliminates strategies that are not useful for an
agent, regardless of the strategies of the other agents.
Work remains to be done on extending the notion of a value-preserving transformation
to the noncooperative case. One possibility is to redefine value-preserving transformations
so that value is preserved for all agents. This is closely related to the idea of Pareto
optimality. In a general-sum game, a strategy profile is said to be Pareto optimal if there
does not exist another strategy profile that yields higher payoff for all agents. It seems that
policy iteration using the revised definition of value-preserving transformation would tend
to move the controller in the direction of the Pareto optimal set. Another possibility is
126

Policy Iteration for DEC-POMDPs

to define value-preserving transformations with respect to specific agents. As each agent
transforms its own controller, the joint controller should move towards a Nash equilibrium.
Handling Large Numbers of Agents The general DEC-POMDP representation presented in this paper grows exponentially with the number of agents, as seen in the growth
of the set of joint actions and observations as well as the transition, reward and observation
functions. Thus this representation is not feasible for large numbers of agents. However,
a compact representation is possible if each agent interacts directly with just a few other
agents. We can have a separate state space for each agent, factored transition probabilities,
and a reward function that is the sum of local reward functions for clusters of agents. In
this case, the problem size is exponential only in the maximum number of agents interacting
directly. This idea is closely related to recent work on graphical games (La Mura, 2000;
Koller & Milch, 2003).
Once we have a compact representation, the next question to answer is whether we
can adapt policy iteration to work efficiently with the representation. This indeed seems
possible. With the value-preserving transformations we presented, the nodes of the other
agents are considered part of the hidden state of the agent under consideration. These
techniques modify the controller of the agent to get value improvement for all possible
hidden states. When an agent’s state transitions and rewards do not depend on some other
agent, it should not need to consider that agent’s nodes as part of its hidden state. A
specific compact representation along with extensions of different algorithms was proposed
by Nair et al. (2005).

Acknowledgments
We thank Martin Allen, Marek Petrik and Siddharth Srivastava for helpful discussions of
this work. Marek and Siddharth, in particular, helped formalize and prove Theorem 1. The
anonymous reviewers provided valuable feedback and suggestions. Support for this work
was provided in part by the National Science Foundation under grants IIS-0535061 and
IIS-0812149, by NASA under cooperative agreement NCC-2-1311, and by the Air Force
Office of Scientific Research under grants F49620-03-1-0090 and FA9550-08-1-0181.

Appendix A. Proof of Theorem 1
A correlation device produces a sequence of values that all the agents can observe. Let X
be the set of all possible infinite sequences that can be generated by a correlation device.
Let Vx (~q0 , s0 ) be the value of the correlated joint controller with respect to some correlation
sequence x ∈ X, initial nodes ~q0 of the agent controllers, and initial state s0 of the problem.
We will refer to Vx (~q0 , s0 ) simply as Vx – the value of some sequence x, given the controllers
for the agents. We define a regular sequence as a sequence that can be generated by a
regular expression. Before we prove Theorem 1, we establish the following property.
Lemma 2 The value of any sequence, whether regular or non-regular, can be approximated
within any  by some other sequence.
Proof: The property holds thanks to the discount factor used in infinite-horizon DECPOMDPs. Given a sequence x with value Vx , we can determine another sequence x0 such
127

Bernstein, Amato, Hansen, & Zilberstein

that |Vx0 − Vx | < . The sequence x0 is constructed by choosing the first k elements of x,
and then choosing an arbitrary regular or non-regular sequence for the remaining elements.
kR
max
As long as k is chosen such that  ≥ β(1−β)
, then |Vx0 − Vx | < . 2
Theorem 1 Given an initial state and a correlated joint controller, there always exists
some finite-size joint controller without a correlation device that produces at least the same
value for the initial state.
Proof: Let E represent the expected value of the joint controller with the correlation
device. Let V = {Vx | x ∈ X} be the set of values produced by all the possible correlation
device sequences. Let inf and sup represent the infimum and supremum of V respectively.
We break the proof into two cases, depending on the relation of the expectation versus
the supremum. We show in each case that a regular sequence can be found that produces
at least the same value as E. Once such a regular sequence is found, then that sequence can
be generated by a finite-state controller that can be embedded within each agent. Thus, a
finite number of nodes can be added to the agents’ controllers to provide equal or greater
value, without using a correlation device.
Case (1) inf ≤ E < sup
Based on Lemma 2, there is some regular sequence x that can approximate the supremum
within . If we choose  = sup −E, then Vx ≥ sup − = E.
Case (2) E = sup
If there is a regular sequence, x, for which Vx = E, we can choose that sequence. If no
such regular sequence exists, we will show that E 6= sup. We give a somewhat informal
argument, but this can be more formally proven using cylinder sets as discussed by Parker
(2002). We begin by first choosing some regular sequence. We can construct a neighborhood around this sequence (as described in Lemma 2) by choosing a fixed length prefix
of
A prefixP
of length k has a well-defined probability that is defined as
P the sequence.
P
0)
1 |q 0 ) . . .
k−1 |q k−2 ) where P (q 0 ) is the probability distribution
P
(q
P
(q
c
c c
c
c
qc0
qc1
qck−1 P (qc
of initial node of the correlation device and P (qci |qci−1 ) represents the probability of transitioning to correlation device node qci from node qci−1 . The set of sequences that possess this
prefix has probability equal to that of the prefix. Because we assumed there exists some
regular sequence which has value less than the supremum, we can always choose a prefix
and length such that the values of the sequences in the set are less than the supremum.
Because the probability of this set is nonzero and the value of these sequences is less than
the supremum, then E 6= sup, which is a contradiction.
Therefore, some regular sequence can be found that provides at least the same value as
the expected value of the correlated joint controller. This allows some uncorrelated joint
controller to produce at least the same value as a given correlated one. 2

Appendix B. Proof of Lemma 1
For ease of exposition, we prove the lemma under the assumption that there is no correlation
device. Including a correlation device is straightforward but unnecessarily tedious.
128

Policy Iteration for DEC-POMDPs

Lemma 1 Let C and D be correlated joint controllers, and let Ĉ and D̂ be the results of
performing exhaustive backups on C and D, respectively. Then Ĉ ≤ D̂ if C ≤ D.
Proof: Suppose we are given controllers C and D, where C ≤ D. Call the sets of joint
~ and R,
~ respectively. It follows that there exists a function
nodes for these controllers Q
~
fi : Qi → ∆Ri for each agent i such that for all s ∈ S and ~q ∈ Q
V (s, ~q) ≤

X

P (~r|~q)V (s, ~r).

~
r

We now define functions fˆi to map between the two controllers Ĉ and D̂. For the old
nodes, we define fˆi to produce the same output as fi . It remains to specify the results of fˆi
applied to the nodes added by the exhaustive backup. New nodes of Ĉ will be mapped to
distributions involving only new nodes of D̂.
To describe the mapping formally, we need to introduce some new notation. Recall that
the new nodes are all deterministic. For each new node ~r in controller D̂, the node’s action
is denoted ~a(~r), and its transition rule is denoted ~r 0 (~r, ~o). Now, the mappings fˆi are defined
such that
P (~r|~q) = P (~a(~r)|~q)

YX

P (~q 0 |~q, ~a(~r), ~o)P (~r 0 (~r, ~o)|~q 0 )

q~ 0

~
o

for all ~q in controller Ĉ and ~r in controller D̂.
We must now show that the mapping fˆ satisfies the inequality given in the definition
of a value-preserving transformation. For the nodes that were not added by the exhaustive
backup, this is straightforward. For the new nodes ~q of the controller Ĉ, we have for all
s ∈ S,

V (s, ~q) =

X

P (~a|~q) R(s, ~a) +


X

P (s0 , ~o|s, ~a)P (~q 0 |~q, ~a, ~o)V (s0 , ~q 0 )

~
o,s0 ,~
q0

~a




≤

X

P (~a|~q) R(s, ~a) +

X

P (s0 , ~o|s, ~a)P (~q 0 |~q, ~a, ~o)

~
o,s0 ,~
q0

~a

X

P (~r 0 |~q 0 )V (s0 , ~r 0 )

~
r0


=

X

P (~a|~q) R(s, ~a) +


X

P (s0 , ~o|s, ~a)P (~q 0 |~q, ~a, ~o)P (~r 0 |~q 0 )V (s0 , ~r 0 )

~
o,s0 ,~
q 0 ,~
r0

~a


=

X

=

X

P (~r|~q) R(s, ~a(~r)) +


X

P (s0 , ~o|s, ~a(~r))V (s0 , ~r 0 (~r, ~o))

~
o,s0

~
r

P (~r|~q)V (s, ~r).

~
r

2
129

Bernstein, Amato, Hansen, & Zilberstein


