

We present a mechanism for constructing
graphical models, specifically Bayesian net­
works, from a knowledge base of general
probabilistic information. The unique fea­
ture of our approach is that it uses a power­
ful first-order probabilistic logic for express­
ing the general knowledge base. This logic
allows for the representation of a wide range
of logical and probabilistic information. The
model construction procedure we propose
uses notions from direct inference to identify
pieces of local statistical information from
the knowledge base that are most appropri­
ate to the particular event we want to reason
about. These pieces are composed to gener­
ate a joint probability distribution specified
as a Bayesian network. Although there are
fundamental difficulties in dealing with fully
general knowledge, our procedure is practical
for quite rich knowledge bases and it supports
the construction of a far wider range of net­
works than allowed for by current template
technology.
1

Introduction

The development of graphical representations for prob­
abilistic and decision-theoretic models [Pea88, 0890]
has vastly increased the range of applicability of such
models in AI. However, it appears that current graph­
ical representations are limited to specialized domains
of knowledge--somewhere around the scope of modern
expert systems. For a number of reasons, it seems im­
possible to use such models to represent, say, the gen­
eral medical knowledge possessed by a typical physi­
cian.
A major limitation of current graphical representa­
tions is that they are propositional . That is, they
*This work was supported by NSERC under its Operat­
ing Grants program and by the IRIS network. The author's
e-mail address is fbacchus@logos. uwaterloo. ca

lack quantifiers, which are essential for representing
general knowledge. W ith quantifiers one can repre­
sent an assertion about a whole class of individuals
using a single sentence, while in a propositional lan­
guage this would require a separate sentence for each
individual. As a result, important knowledge structur­
ing techniques, like taxonomies, cannot be applied to
propositional representations.
However, graphical representations have important ad­
vantages of their own. In particular, they support ef
ficient reasoning algorithms. These algorithms are fa
more efficient than the symbolic reasoning mechanism�
typical of more general representations.
This dichotomy of utility has lead to proposals fo'
hybrid uses of general and graphical representations
In particular, Breese et al. (BGW91] have proposul
the technique of knowledge based model construe
tion ( KBMC) : the automatic construction of propo
sitionalj graphical models for specific problems from '
larger knowledge base expressed in a general repre
sentation. Breese et al. provide a number of moti­
vations for this approach that extend the arguments
given above.
We refer the reader to [B GW91] for this motivation,
and take as our starting point that KBMC is a po­
tentially useful technique, certainly worth examining
in more detail. Our contribution, then, is to look
more closely at a particular mechanism for perform­
ing KBMC. In particular, we develop a mechanism in
which a first-order probability logic [Bac90b] is used
to represent the general knowledge base, and model
construction is performed using ideas arising from thr'
study of direct inference. Direct inference involve.
reasoning from general statistical knowledge to prob
abilities assigned to particular cases and has beei
worked on by a number of authors including [ BGHK92
Bac90b, Kyb61, Kyb74, Lev80, Lou87, Pol90, Sal71 ]
Our mechanism brings to light the important role ex
pressive-first-order probability logics can play in rep
resenting general probabilistic knowledge, and the im
portant relationship between KBMC and direct infer
ence.
In the sequel, we first introduce a probability logi•

220

Bacchus

that can be used for the representation of general
probabilistic and logical knowledge, and demonstrate
that it is capable of representing any Bayesian net­
work [Pea86]-pe'rhaps the most important of current
graphical representations. Then we discuss how ideas
from direct inference can be used to specify a model
construction procedure that can construct graphical
models for particular problems. We point out how
this idea is related to, but strictly more general than,
template models. Throughout our discussion we try to
point out various insights about the process of KBMC
offered by our approach. Finally, we close with some
conclusions and indications for future work.
2

Representing Gener al Prob abilistic
Knowledge

KBMC requires a mechanism for representing general
knowledge. This representation should be declarative,
for a number of good reasons that are beyond the scope
of this paper to discuss. Furthermore, the representa­
tion should have a precise semantics, so that we can
specify exactly the meaning of the expressions in the
knowledge base. Without precise semantics it would
be impossible to verify the accuracy of the knowledge
base.

tics of our language here ( see [Bac90b] for all such
details) . The specification simply formalizes the fol­
lowing notion: a formula with free variables might be­
come true or false depending on how the variables an:
instantiated. For example, bird(x) might be true when
x = Tweety but false when x =Clyde. A proportion
term, then, simply evaluates to the proportion of pos­
sible instantiations that make the formula true.
This language can express an wide variety of statistical
assertions ( [Bac90b] gives an extensive collection of ex­
amples) . It can also express whatever can be expressed
in first-order logic, so essential structuring mechanisms
like taxonomies can be applied.
Example 1 Let the domain contain, among other
things, a collection of coins, and a collection of coin­
tossing events. 1 In addition to some obvious symbols,
let our language include the predicate CoinToss(e)
which is true of an individual e iff e is an coin-tossing
event; Goin(x) which is true of x iff xis a coin; and
Object(e,x) which holds of the individuals e and x iff
e is an event and x is the object of that event: the ob­
ject of a coin-tossing event is the particular coin that
is tossed. Now we can express the following:

1. Ve,x.GoinToss(e) 1\ Object(e,x) ---> Goin(x).
That is, the object of any coin toss is always a.
coin.

Since logical representations meet our desiderata, we
propose as a representation mechanism a first-order
logic for statistical information, developed by Bacchus
[Bac90a] . This logic is basically first-order logic aug­
mented to allow the expression of various assertions
about proportions.
Syntactically, we augment an ordinary collection of
first-order symbols with symbols useful for express­
ing numeric assertions, e.g., '1', '+', ';::: '. In ad­
dition to allowing the generation of ordinary first­
order formulas we also allow the generation of nu­
meric assertions involving proportions. For example,
[P(x)]x = 0.75, expresses the assertion that 75% of
the individuals in the domain have property P, while
0.45 ::; [R(x,y)](x,y} ::; 0.55 expresses the assertion
that between 45% and 55% of all pairs of domain in­
dividuals stand in relation R. In general, if a is an
existent formula and x is a vector of n variables, the
proportion term [a] x denotes the proportion of n-ary
vectors of domain individual that satisfy the formula
a. Most of the statistical information we wish to ex­
press will in fact be statements of conditional probabil­
ity denoting relative proportions. For example, [ai .Bl x
will denote the proportion of n-ary vectors of domain
individuals among those that satisfy ,B which also sat­
isfy a. We can then express various statistical asser­
tions by expressing various constraints on the values
that these proportion terms can take. For example,
by asserting that [Q(x)IP(x)]x = 0.5 we are asserting
that the domain we are considering is such that 1/2 of
the P's are Q's.
We will not give a formal specification of the seman-

2. Vx.Fair(x)
+-+
[Heads(e)IGoinToss(e) J'
Object(e,x)]e E (.49,.51). We agree to call an;,
coin x fair iff approximately 50% of the events in
which it is tossed result in heads. This example
demonstrates the useful interplay between univer·
sal quantification and the proportion terms.
3. [[Heads(e)IGoinToss(e) 1\ Object(e,x)]e
E
(0.49,0.51)1Goin(x)Jx = 0.95. This formula says
that 95% of all coins are such that approximately
50% of the events in which they are tossed re­
sult in heads. That is, 95% of the coins in the
domain are approximately fair. This example
demonstrates the useful ability to nest proportion
statements.
3

Representing Bayesian Networks

Using the logic described in the previous section we
can represent a large knowledge base of general logi­
cal and statistical information by a collection of sen.
tences. It is not difficult to see that any discrete val­
ued Bayesian network can easily be represented in thf
1The explicit inclusion of events in the domain of indi­

viduals is similar to the inclusion of other abstract object,:
like time points or situations (as in the situation calculu.>

[MH69]). There may be philosophical objections, but tech­
nical difficulties can be avoided if we restrict ourselves t;
a finite collection of distinct e vents.

221

Probability Log ics for KBMC

logic.2 Here we will give a particular scheme for repre­
senting an arbitrary network, although there are many
other schemes possible.

its parents Xf(i,l),...,Xf(i,qi)· This matrix of cone
tional probabilities consists of a collection of individm
equations each of the form

Any Bayesian network is completely specified by two
pieces of information: ( 1 ) a product decomposition of
the joint distribution which specifies the topological
structure of the network, and (2) matrices of condi­
tional probability values which parameterize the nodes
in the network [Pea88J . Consider an arbitrary network
B. Let the nodes in B be the set {X1,...,Xn}- Each
node Xi has some set of parents {XJ(i,l)> ...,Xf(i,qi)},
where f ( i, j) gives the index of node Xi's j-th parent,
and qi is the number of parents of Xi- Furthermore
each node Xi can take one of some discrete set of val­
ues {v1,...,Vki}, where ki is the number of different
values for xi.

Pr(Xi

The topological structure of B is completely specified
by the equation
Pr(X1,..., Xn)= Pr(X1IX!(l,l),...,Xf(l,ql))x
···X Pr(XniX t(n,l)> ...,Xf(n,q n))That is, the topological structure of B is equivalent
to an assertion about how the joint distribution over
the nodes X1-Xn can be decomposed into a product
of lower-order conditionals. Actually, this equation is
shorthand. Its full specification is that this product
decomposition holds for every collection of values the
nodes XcXn can take on.
We can translate this equation into a sentence of our
logic by creating a fnnction symbol for every node Xi;
for convenience we use the same symbol Xi. Now the
above structure equation can be rewritten as the sen­
tence

Vz1,···,Zn.[X1(e)=z1/\ ···1\Xn(e)=zn]e=
Xt(1,1)=Z£(1,1) 1\ . ..
X1(e)= z1
1\Xt(1,q1) =Zf(1,q1) e x

1

[

X

[

Xn(e) =Zn

J

I

.. ]

X f(n,1) = Zf(n,1)1\ .
1\X f(n,qn) =Zf(n,qn)

.
e

Here we have treated the multi-valued nodes as func­
tion symbols XrXn in our language. Our translated
sentence asserts that for every particular set of val­
ues the X1-Xn can take on, the proportion of events e
that achieve that set of values can be computed from
the lower-order relative proportions. The universal
quantification ensures that this product decomposition
holds of every collection of values.
Having completely specified the topological structure
of B, we can equally easily specify the conditional
probability parameters in our language. For each node
Xi, B provides the probability of Xi taking on any of
its allowed values under every possible instantiation of
2It is also possible, with a few technical caveats, to rep­
resent networks with continuous valued nodes. But here
we restrict our attention to discrete valued nodes.

=

tiiXt(i,l) =t f(i,l),···,Xf(i, qi)

=

tf(i,q i))=p,

where tj is some value for variable Xj, and p is some
numeric probability value.
To translate these equations into sentences of our logic
we create new constant symbols for every possible
value ti of every node Xi; for convenience we use the
same symbol ti. Now the above equation can be rewrit­
ten as the sentence

[Xi(e)= ti l

Xt (i,l)(e) � t f(i:! )1\ · ··
1\xf(l,ql) ( e ) -_ t f(l_ ,ql.)

]=
e

p.

Here we have simply rewritten the conditional proba
bility equations as equations involving the proportiu
of events in which Xi takes on value ti.
The above procedure can be applied to any networ�
Thus we make the following observation. Any discret.
valued Bayesian network can be represented as a Cur
lection of sentences in the knowledge base.
W hat is important to point out about this transla
tion is that the translated assertions represent templat.:
networks. As pointed out in [BGW91] most probabilis
tic networks in use in consultation systems are actuall}'
template models. That is, the nodes represent gener­
alized events which get instantiated to the particular
event under consideration. For example, a node rep­
resenting "Disease D" will be instantiated to "Patient
John R. Smith has disease D, " a node representing
"Blood test shows low white cell count" will be instan­
tiated to "Blood test T0906 for patient John R. Smith
shows low white cell count," etc. In our representation
the template nature of the networks is made explicit:
our formulas refer to proportions over classes of sim­
ilar events not particular events. As we will see this
is not a limitation in representational power, rather it
is simply a more accurate representation which allows
for greater modularity. Propositional networks refer
ring to particular events are to be generated from the
knowledge base via model construction techniques.
4

Simple Model Construction

To introduce the basic ideas that underlie our mode'
construction technique consider a knowledge base thai
consists simply of a collection of template Bayesian
networks, each one applicable to different types of
events.
To specify that each different decomposition, and col­
lection of conditional probability parameters, is appli­
cable to a different class of events we only need add the
event type as an extra conditioning formula. For ex­
ample, say that we have two networks both suitable for
diagnosing abdominal pain. However, one of the net
works is designed for women in late-term pregnane}.

222

Bacchus

\fz1, z2, zs.[Xl (e)=z1 1\X2(e) z2 1\Xs(e)=zsiAbdominalPain(e) 1\-.Pregnancy(e)]e
= [X1(e)=z1 IAbdominalPain(e) 1\-.Pregnancy(e)]e
x [X2(e)
z2 IX1(e) z1 1\AbdominalPain(e) 1\--.Pregnancy(e)]e
x [Xs(e)= zsiX1(e)=Z1 1\X2(e) =z2 1\AbdominalPain(e) 1\-.Pregnancy(e)]e,
=

(1)

=

=

\fz1,Z 2,zs.[Y1(e)=z1 1\Y2(e)=z2 1\Ys(e) zsiAbdominaiPain(e) 1\Pregnancy(e)]e
= [¥1(e)= z1IAbdominalPain(e) 1\Pregnancy(e)]e
X [Y2(e) = z2 IY1(e) =z1 1\AbdominalPain(e) 1\Pregnancy(e)]e
x [Ys(e) = zs1Y1(e)=z1 1\AbdominalPain(e) 1\Pregnancy(e)]e·
=

(2)

.

F igure 1: Alternate Structures for Abdominal Pain
while the other is suitable for other patients with ab­
dominal pain. Our general knowledge base might con­
tain the two formulas (Equations 1 and 2) shown in
F igure 1.
In this example the events involving abdominal pain
and pregnancy have a different network models (i.e.,
structural decompositions) with entirely different vari­
ables than the events where there is no pregnancy. In
a similar manner we can represent a whole collection
of disjoint types of events, where each event type is
modeled by a different probabilistic structure.
In this case the model construction technique in
this case would simply locate the appropriate tem­
plate model using information about the particular
event being reasoned about. For example, if the
event is EOOl and we know AbdominalPain(EOOl) 1\
Pregnancy(EOOl ), i.e., the event being reasoned about
involves adominal pain in a pregnant patient, we would
construct a network model for reasoning about EOOl
using the second template model. This network would
have the structure
Pr(Y1, Y2 , Y3)

Pr(Y1) x Pr(Y2 1Yl) x Pr(Y31Yl),
and would be parameterized by the values specified
in the knowledge base for the Yi variables. Since
the constructed network is now specific to event EOOl
we can drop the extra condition AbdominalPain(e) 1\
Pregancy(e) as we know that EOOl satisfies these con­
ditions. Now we have an event specific network that
can be used to reason about the probable values of the
variables Yi in the particular event.
=

We can see that the model constructor is simply "in­
stantiating" the general template model with the par­
ticular event EOOl. By using the same structure and
probability parameters as the class of abdominal pain­
pregnancy events we are assigning probabilities to the
particular event EOOl that are identical to the statis­
tics we have about that general class of events. This
is an example of direct inference, where we use statis­
tics over a class of similar events to assign probabilities
to a particular event. For example, when we assign a
probability of 1/2 to the event of heads on a particu­
lar coin toss based on statistics from a series of coin
tosses we are performing direct inference. This kind of
inference is pervasive in reasoning under uncertainty.3
3
See Kyburg

[K yb83a]

for further arguments pointing

Simple model construction of this kind is not tha,
interesting however. We could easily accomplish the
same thing with a control structure that chooses frou
some collection of networks. The main difference b
that here we have an explicit, declarative, represen
tation of which network is applicable to what type cr
event. Furthermore, it also serves to illustrate the b<1
sic idea behind our approach to KBMC.
5

More General Model Construction

In general we will not have explicit template models
in our knowledge base for all of the events we wish
to reason about. Indeed, this is exactly the point of
the KBMC approach: we want to deal with situation:>
beyond the ability of template models.
Our knowledge base will more likely contain informa
tion about conditional probabilities isolated to neigh
borhoods of related variables. For example, instead
of having an explicit product decomposition for all of
the relevant variables, as in the above examples, th•·
knowledge base might simply contain the individua.
product terms, i.e., the neighborhood information, in
isolation. It will be up to the model construction pro­
cedure to link these individual terms into a joint dis
tribution. Consider Pearl's classic Holmes's burglar:··
example. It is unlikely that Holmes has in his knowl­
edge base an explicitly represented decomposition of
the form shown in Equation 3 (Figure 2). Such a de
composition is simply far too specific. Rather Holme­
would more typically have information like that shown
in Equation 4 (Figure 2). In this case Holmes has the
knowledge (a) in 75% of the events in which a house
with an alarm is burglarized, the alarm will sound;
(b) in 45% of the events in which an alarm sounds
near where a person lives that person will report the
alarm; (c) the specific knowledge that Watson lives
near Holmes's house and that Holmes's house has an
alarm. The advantage of knowledge in this more gen­
eral form is that it can be used to reason about many
other types of events. For example, the statistic�!
knowledge (a) can be used to reason about any alarn1
in any house, e.g., if Holmes learns that his parent&
house alarm has been tripped; similarly (b) can b
out the prevalence of "direct inference" in probabilisti
reasoning.

Probability Logics for KBMC

(3)

(4)

223

[Burglary(e,MyHouse) A AlarmSound(e,MyHouse) A ReportsAlarm( e,Watson,MyHouse)]e
[AlarmSound(e,MyHouse) !Burglary(e,MyHouse)]e
x [ReportsAlarm(e,Watson,MyHouse) IAlarmSound(e,MyHouse)]e=

( a)
(b)
( c)

[AlarmSound(e,x)jBurglary(e,x) AHouseWithAlarm(x)](e,x) .75
[ReportsAlarm(e,y,x)l
AlarmSound(e,x) A HouseWithAlarm(x) A LivesNear(x,y)](e,x,y)
LivesNear(MyHouse,Watson) A HouseWithAlarm(MyHouse)
=

=

0 45
.

Figure 2: An overly Specific Decomposition vs. General Information
used for reasoning about reports from any neighbor,
e.g., if Mrs. Gibbons reported the alarm instead of Dr.
Watson.
Holmes will also have other pieces of statistical infor­
mation, e.g. , statistics about the event that a house
has been burglarized given that a police car is parked
outside, and other pieces of information specific to the
particular event being reasoned about. The task, then,
of a model construction procedure is to use the infor­
mation specific to the particular event being reasoned
about to decide which local pieces of statistical infor­
mation are relevant and how they should be linked
into a Bayesian network representation. Once a net­
work has been constructed it can be used to quickly
perform a range of complex reasoning about the par­
ticular event.
There are three issues that arise when constructing a
Bayesian network model of the particular event we are
reasoning about. First, the model construction proce­
dure must have some information about the variables
(properties of the event in question) that we wish to in­
clude in the constructed network. Second, we must use
information about the particular event to locate ap­
propriate pieces of local statistical information in the
knowledge base. And third, we must combine these
local pieces of information into a network.
5.1

The Set of Variables

Some information must be supplied about what collec­
tion of variables we want to model in the constructed
network. In the simplest case we will just supply
a query about the particular event under consider­
ation along with some additional information about
that event. For example, we might be reasoning
about event E002 and the guery might be expressed
as Burglary(E002)?; i.e., did a burglary occur as
part of this event? We might also have the informa­
tion ReportsAlarm(E002,Watson,MyHouse), i.e. , Dr.
Watson reported an alarm at Holmes's house during
this event. If the knowledge base is similar to that
given above, the procedure could determine that it can
chain probabilistic influence from a report by Watson
to belief in the alarm sounding, and then from there
to a belief in a burglary, i.e. , to an inference about
the query. Given that this is the only chain of influ­
ence it can find in the knowledge base linking alarm
reports and burglaries, the constructed network will

only contain a burglary node, an alarm sound nod< ,
and an alarm report node. That is, in a strictly query
driven KBMC procedure the constructed model will
only contain variables relevant to the particular quen
Alternately, we could supply the procedure with mor
information. For example, we could specify a set c
variables that we wish to include in the constructed
model. For example, we could specify that we are also
interested in reasoning about earthquakes and radio
broadcasts. If the knowledge base has local statistics
about the frequency of alarms sounding given earth­
quakes, and radio reports given earthquakes, a larger
Bayesian network could be constructed that includes
nodes for these variables. The links between these
variables would be determined by the local statistic··
contained in the knowledge base. For example, if w
know the frequency of alarm triggers given earthquak'
events, we would plac�a link from the earthquake noel-)
to the alarm node in the constructed network.
As in the simple query driven case, however, the proc.o
dure would still be able to add additional intermediat. ·
variables that link the variables in the set of inter
est. These intermediate variables would be found b:·
looking through the knowledge base for chains of in
ftuences between the specified variables. For example,
if we inform the procedure to build a model of some
set of diseases {D1,...,Dn} and some set of symp
toms {81,...,Bm}, it can search for chains of local
conditional probabilities linking members of these tw•>
sets. Hence, the constructed network will generally
contain additional intermediary nodes describing the
causal processes known to link the diseases with the
various symptoms, just as the alarm sound informa­
tion linked burglaries and alarm reports in the query
driven case.
It seems likely that we would want to amortize the ef­
fort of constructing the Bayesian network over a whole
range of queries. Hence, we will probably want to sup­
ply the model constructor with more information than
just a single query.
5.2

Locating the Appropriate Local Statistic.

Inform�tion about the particular event will help deter
mine which collection of local statistics are appropri
ate. The issue of choosing appropriate statistics is a
the heart of the difficulties in direct inference. Q],

224

Bacchus

approaches to direct inference revolved around try­
ing to find appropriate reference classes from which
statistics can be. drawn [Kyb83b]. More recent work
has taken an approach based on the principle of indif­
ference that dispenses with the notion of a. reference
class altogether [BGHK92]. In general, however, de­
termining the probabilities to assign to a particular
event given a collection of statistical information about
classes of similar events is a very difficult problem. For
a practical enterprise like KBMC, however, we can use
the work on direct inference to derive general guide­
lines as to what statistics to consider. For example,
all approaches to direct inference validate the subset
or specificity preference: one should choose the most
specific statistics applicable to the event in question.
Similarly, if we have statistical information about a
specific individual involved in the event we should use
that.
Information about the particular event can alter both
the parameterization and the structure of the con­
structed Bayesian network. This flexibility is not pos­
sible with simple template models. Consider the fol­
lowing example.
Example 2 Say that the local information shown in
F igure 3 was contained in the knowledge base. And
say that our information about the particular event
was ReportsAlarm(E002, Watson, MyHouse). If it is
decided that AlarmSound should be placed in the con­
structed network, either because it is a variable of in­
terest or because it is in a chain of influences to a
variable of interest, then the procedure would have to
choose how to parameterize the link from the MyHouse
alarm sound node and the Watson alarm report node.

The only statistic we have about the chance of an
alarm report given an alarm concerns the class of peo­
ple who live near the house whose alarm sounded. In
this case we know Dr. Watson is a member of this class,
i.e., LivesNear(MyHouse, Watson), so item 1 gives the
most specific known probability of a report given an
alarm. However, we do have a more specific statistic
for Dr. Watson, item 3, in the case of a report when
there is no alarm, indicating that Watson is a bit of a
practical joker. Hence, this more specific value would
be used for the probability of a report given no alarm.
On the other hand if the event in question involved
a report by Mrs. Gibbons, we would be forced to use
the more general statistics 1 and 2 to parameterize the
alarm-report/alarm-sound link as we have no specific
statistics for Mrs. Gibbon's alarm reports.
Example 3 Let the knowledge base be as in F igure 3,

except augmented by the additional statistical infor­
mation shown in F igure 4. That is, in this case Holmes
has a special alarm installed by a security company
AlarmMonitorCompany with a direct line to their of­
fice, and from the company's literature about the ac­
curacy of their alarm systems Holmes has come to ac­
cept the above statistical assertion about the reliabil­
ity of their alarm reports. Now if the event was Re-

portsAlarm(E003, AlarmMonitorCompany, MyHous<

there would be no need for the model constructio '
procedure to include an intermediary node of alan ,
sound, nor would the direction of the links be require.
to go from burglaries towards alarm reports. Instea' I
it could use this statistic, as the particular event EOO
is a member of this class of events, to link the alan
report node directly to the burglary node, and a quit·
different network structure would result.

J

5.3

Linking the Local Pieces

Once appropriate local statistics are obtained from the
·database we have enough information to link various
nodes in the network. That is, each local statistic will
serve to parameterize a link between two nodes in the
network. A difficulty that arises here is justifying thi ,
composition.
All we really know about the probability distributio: .
describing the interaction between the variables ar
the local conditional probabilities. There will in gen
eral be many different joint probability distribution",
that are consistent with these local conditional prob
abilities. In linking up the nodes in a manner d€
termined solely by the local information we are cou
structing a particular joint distribution, one in whie1
the local conditional probabilities determine a produ..;
decomposition. An important question is: to what ex
tent is such a procedure justified? Lewis [LI59] prow" I
some results which show that by taking the produc
of local conditional probabilities one obtains a best es
timator in the sense of Kullback-Leibler cross-entrop)
[KL51]. But his results do not cover all of the cases
that might occur. Another justification comes from re­
cent work that applies the principle of indifference to
reasoning about change [BGHK93]. For an enterprise
like KBMC, however, we will again want to use gen ­
eral principles derived from such work. One general
principle arising from [BGHK93], and earlier work by
Hunter [Hun89], is that when the variables are causally
related, as compared to being simply correlated, using
the product of the local conditional probabilities can
be justified by principles of indifference.
A related difficulty occurs when we have some but nm
all of the information required to specify the parame
terization of the network. For example, we might hav
statistics about a number of distinct causes for an d
feet, but we might not have statistics about their join.
effect. Pearl [Pea88] has suggested the use of "proto
typical structures" like noisy OR gates. There is P.J
underlying probabilistic model from which noisy 0 I
gates arise, and when it is reasonable to assume tha'
this model holds in a domain, prototypical structure ;
of this form could be used. Alternately, the indiffer
ence considerations of [BGHK93, Hun89] can also t,.
used in certain cases to complete the joint distribution
over the different causes.

Probability Logics for KBMC

225

1.

[ReportsAlarm(e,y,x)l
AlarmSound( e, x) 1\ HouseWithAlarm(x) 1\ LivesNear(x,y)](e,x,y) 0.45
2. [ReportsAlarm(e,y,x)l
·AlarmSound(e,x) 1\ HouseWithAlarm(x) 1\ LivesNear(x,y)](e,x,y) 0.05
3. [ReportsAlarm(e,Watson,x)l
·AlarmSound(e,x) 1\ HouseWithAlarm(x) 1\ LivesNear(x,Watson)](e,x) 0.15
4. HouseWithAlarm(MyHouse) 1\ LivesNear(MyHouse,Watson)
5. LivesNear(MyHouse, Gibbons)
=

=

=

F igure 3: Knowledge Base for Example 2
6.
7.

[Burglary(e,MyHouse) IReportsAlarm(e,AlarmMonitorCompany,MyHouse)]e
[Burglary(e,MyHouse) I•ReportsAlarm(e,AlarmMonitorCompany, MyHouse)]e

=

0.90

=

0.05

Figure 4: Additional Knowledge for Example 3
6

Conclusions and Future Work

We have outlined a mechanism for KBMC of Bayesian
networks from a knowledge base expressed in a first­
order probabilistic logic. Although we have only been
able to present a sketch of how the mechanism works
we have discussed the main ideas behind the proposal:
( 1) identify the variables of interest either through a
query driven process or through information provided
by the user; ( 2) locate local statistics, relevant to the
particular event being reasoned about, by using prin­
ciples from work on direct inference, like specificity, to
prefer certain local st�tistics over others; ( 3) construct
chains of probabilistic influence from these local statis­
tics; (4) construct an event specific network by using
the chains of probabilistic influence to specify the arcs
in the network, and by using the local statistics to
parameterize the nodes, perhaps filling in missing pa­
rameters by using prototypical structures or principles
of indifference. The resulting network can then be used
to reason probabilistically about the particular event.
The mechanism can be actualized fairly easily in
straightforward cases. In such cases the chains of influ­
ence are easy to locate: the individual links are explic­
itly expressed in the knowledge base. If the statistics
in the knowledge base are of a form such that select­
ing the most appropriate statistics reduces to simple
specificity considerations and if we have sufficient sta­
tistical information, we can easily parameterize the re­
sulting structure. Such a mechanism, although limited
in some ways, already offers a considerable increase in
flexibility over current template models.
One issue we have not addressed here is a mechanism
for representing temporal information, but as shown
by Bacchus et al. [B TH91] first-order logic is suffi­
cient for representing a range of temporal ontologies.
Hence, once an appropriate temporal ontology is de­
cided upon, it is possible that the representation could
be extended to allow for temporal information. If
the temporal structure is discrete we could also al-

low the formation of proportion statements over time
points, thus allowing the expression of various asser­
tions about discrete stochastic processes. A related
issue that can be addressed is the representation of
utilities. Extending our representation to utilities and
temporal information, and the KBMC procedure WP
proposed to generate, e.g., influence diagrams, is hU
interesting area for future research. Current work uH
this model is focused on filling in the details of th.
mechanism we have sketched, and on building a pro
totype system.
In conclusion, we feel that our proposal is a wori<
able one, that, with sufficient resources, can be turnt'•:
into a prototype implementation. Work on this is con
tinuting. Such an implementation holds the promis
of a useful KBMC procedure that would be far mor,
general than current template models. There are, o
course, limitations to the approach, limitations that.
stem mainly from problems that arise during dire.�r
inference. Given a very general knowledge base of sta­
tistical information it will not always be possible tl)
choose the "most appropriate" statistical information
for an event. For example, we might have conflict­
ing statistical information that cannot be resolved by
specificity. Nevertheless, we can still obtain useful re­
sults in less general but, we hope, still practical, con­
texts.



is being used, new data about the domain can be ac­
cumulated, perhaps from the cases the network is used

We explore the issue of refining an exis­
tent Bayesian network structure using new
data which might mention only a subset of
the variables. Most previous works have
only considered the refinement of the net­
work's conditional probability parameters,
and have not addressed the issue of refin­
ing the network's structure. We develop

on, or from other sources. Hence, it is advantageous
to be able to improve or refine the network using this
new data. Besides improving the network's accuracy,
refinement can also provide an adaptive ability. In par­
ticular, if the probabilistic process underlying the re­
lationship between the domain variables changes over
time, the network can be adapte d to these changes and
its accuracy maintained through a process of refine­
ment. One way the underlying probabilistic process
can change is via changes to variables not represented
in the model. For example, a model of infectious dis­
ease might not include variables for public sanitation,

a new approach for refining the network's
structure. Our approach is based on the
Minimal Description Length (MDL) princi­
ple, and it employs an adapted version of

but changes in these variables could well affect the
variables in the modeL

a Bayesian network learning algorithm de­
veloped in our previous work. One of the
adaptations required is to modify the previ­
ous algorithm to account for the structure
of the existent network. The learning algo­
rithm generates a partial network structure
which can then be used to improve the exis­

A number of recent works have addressed the issue of

refining a network, e.g., [Bun91, Die93, Mus93, SL90,

SC92]. However, most of this work has concentrated
on improving the conditional probability parameters
associated with the network. There has been very lit­
tle work on actually improving the structure of the
network. Clearly errors in the construction of the
original network could have just as easily yielded in­

tent network. We also present experimental
evidence demonstrating the effectiveness of
our approach.

1

Introduction

A number of errors and inaccuracies can occur dur­
ing the construction of a Bayesian net. For example,
if knowledge is acquired from domain experts, mis­
communication between the expert and the network
builder might result in errors in the network model.
Similarly, if the network is being constructed from raw
data, the data set might be inadequate or inaccurate.
Nevertheless, with sufficient engineering effort an ade­
quate network model can often be constructed. Such a
network can be usefully employed for reasoning about
its domain. However, in the period that the network
*Wai Lam's work was supported by an OGS schol­

arship.

Fahiem

the Canadian

Bacchus's

work

was

supported

by

Govenunent through the NSERC and
IRIS programs.
The authors' e-mail addresses are

[wlamllfbacchus]tlogos.uwaterloo.ca.

accurate structures as inaccurate probability param­
eters. Similarly, changes to variables not included in
the network could result in structural as well as para­
metric changes to the probabilistic process underlying
the network variables. This paper presents a new ap­
proach to the problem of structural refinement. The
approach uses new data, which might be only partially
specified, to improve an existent network making it
more accurate and more useful in domains where the
underlying probabilistic process changes over time.
It is not uncommon that practical Bayesian network
structures involve a fairly large number (e.g.,> 100) of
nodes. However, inaccuracies or changes might affect
only some subset of the variables in the network. Fur­
thermore, new data collected about the domain might
be only partial. That is, the data might contain infor­
mation about only a subset of the network variables.
When the new data is paitial it is only possible to re­
fine the structural relationships that exist between the

Lam and Bacchus

384

variables mentioned in the new data. Our approach
performs refinement locally; i.e., it uses an algorithm
that only examines a node and its parents. This al­
lows us to employ partial data to improve local sections
of the network. Furthermore, by taking advantage of
locality our approach can avoid the potentially very
expensive task of examining the entire network.
Our approach makes use of the Minimal Description
Length MDL principle Ris89 . The MDL principle
is a machine learning paradigm that has attracted the
attention of many researchers, and has been success­

{

)

[

]

fully applied to various learning problems, see, e.g.,
GL89, QR89, LB92 . Specifically, we adapt the MDL
learning algorithm developed in our previous work
LB93 to the refinement task, and perform experi­
ments to demonstrate its viability.

[

]

[

]

In the subsequent sections we first present, more for­
mally, the problem we are trying to solve. After an
overview of our approach we show how the MDL prin­
ciple can be applied to yield a refinement algorithm.
This requires a further discussion of how the compo­
nent description lengths can be computed. The refine­
ment algorithm is then presented. Finally, we present
some results from experiments we used to evaluate our
approach. Before, turning to these details, however let
us briefly discuss some of the relevant previous
ork
on refinement.

�

[SL90,

Spiegelhalter et al.

]

SC92 developed a method

to update or refine the probability parameters of a
Bayesian networks using new data. Their method was
subsequently extended by other researchers Die93,
Mus93 . However, all of these approaches were re­
stricted to refining the probability parameters in a
fixed network. In other words, they were not capable

[

]

of refining or improving the network's structure. Bun­

[

]

tine Bun91 proposed a Bayesian approach for refin­
ing both the parameters and the structure. The initial
structure is acquired from prior probabilities associ­
ated with each possible arc in the domain. Based on
the new data, the structure is updated by calculating
the posterior probabilities of these arcs. Buntine's ap­
proach can be viewed as placing a prior distribution on
the space of candidate networks which is updated by
the new data. One can also view the MDL measure as
placing a prior distribution on the space of candidate
networks. However, by using an MDL approach we
are able to provide an intuitive mechanism for specify­
ing the prior distribution that can be tailored to favour
preserving the existent network to various degrees. Fi­

[

]

nally, Cowell et al. CDS93 have recently investigated
the task of monitoring a network in the presence of
new data. A major drawback of their approach that
it can only detect discrepancies between the new data
and the existent network, it cannot suggest possible
improvements; i.e., it cannot perform refinement.

2

The Refinement Problem

The refinement problem we address in this work is as
follows: Given a set of new, partially specified data
and an existent network structure, the objecti e is t
produce a new, refined, network. The refined network
should more accurately reflect the probabilistic struc­
ture of the new data, and at the same time retain as

�

�

much of the old network structure as is consistent with
the new data. The refinement process can be naturally
viewed as a learning task. Specifically, the source data
for the learning task consists of two pieces of informa­
tion, namely the new data and the existent network
structure. The goal of the learning task is to discover
a more useful structure based on these two sources of
information.
There are a number of features we desire of the learned
network structure. First, the learned structure should
accurately represent the distribution of the new data.
Second, it should be similar to the existent structure.
Finally, it should be as simple as possible. The jus­
tification of the first feature is obvious. The second
arises due to the fact that the task is refinement, which
carries with it an implicit assumption that the exis­
tent network is already a fairly useful, i.e., accurate,
model. And the last feature arises from the nature of
Bayesian network models: simpler networks are con­
ceptually and computationally easier to deal with ( see

[LB92]

)

for a more detailed discussion of this point .

It is easily observed that in many circumstances these
requirements cannot be fulfilled simultaneously. For
example, a learned structure that can accurately rep­
resent the distribution of the new data may possess a
large topological deviation from the existent network
structure, or it may have a very complex topologi­
cal structure. On the other hand, a learned structure
that is close to the existent structure might represent a
probability distribution that is quite inaccurate with
respect to the new data. Thus, there are tradeoffs
between these criteria. In other words, the learned
network structure should strike a balance between its
accuracy with respect to the new data, its closeness to
the existent structure, and its complexity. The advan­
tage of employing the MDL principle in this context is
that it provides an intuitive mechanism for specifying
the tradeoffs we desire.
2.1

The form of the new data

We are concerned with refining a network containing
n nodes.
These nodes represent a collection of do­
main variables X = {X1, . . , X.,}, and the structure
and parameters of the network represents a distribu­
tion over the values of these variables. Our aim is to
construct a refinement algorithm that can refine parts
of the original network using new data that is partial.
Specifically, we assume that the data set is specified
as a p-cardinality table of cases or records involving
.

a subset

Xp

of the variables in

X (i.e., Xp � X

and

Using New Data to Refine a Bayesian Network

385

I!Xpll =: p:::; n) . Each entry in the table contains an
instantiation of the variables in X . the results of a
p where n = 100,
single case. For example, in a domain
suppose that each variable X; can tale on one of the
five values {a, b, c, d, e }. The following is an example
of a data. set involving only 5 out of the 100 variables.

In this paper, we extend our learning approach to take
into account the existent network structure. Through
the MDL principle, a natural aggregation of the new
data and the existent network structure is made dur­
ing the learning process. At the same time, a natural
compromise will be sought if the new data and the
existent structure conflict with each other.

X2o

The MDL principle provides a metric for evaluating
candidate network structures. A key feature of our ap­
proach is the localization of the evaluation of the MDL

x4
b

x12
a

b
b

b

d
b

a

:

:

:

:

:

:

a

X21
e
c

b
:
:

X4s
c

a
b
:
:

Using this data set we can hope to improve the original
network by possibly changing the structure (the arcs)
between the variables x4, x12, X:w, x21! and x4S·
The data set could possibly be used to improve the
rest of the network also, if we employed techniques
from missing data analysis, e.g., [SDLC93]. However,
here we restrict ourselves to refinments ofthe structure
between the variables actually mentioned in the data
set.
3

Our Approach

As mentioned above, we employ the MDL principle in
our refinement algorithm. The idea is to firs t learn a
partial network structure from the new data and the
existent network via an MDL learning method. This
partial network structure is a Bayesian network struc­
ture over the subset of variables contained in the new
data. Thus, it captures the probabilistic dependencies
or independencies among the nodes involved. Based on
this partial network, we analyze and identify particu­
lar spots in the original network that can be refined.
The process of constructing the partial network struc­
ture is like an ordinary learning task. The new data
contains information about only a partial set of the
original network variables. However, it is complete
with respect to the partial network structure; i.e., it
contains information about every variable in the par­
tial structure. Hence, constructing the partial struc­
ture is identical to a learning task of constructing a
Bayesian network given a collection of complete data
points and additionally an existent network structure
over a superset of variables.
In our previous work, we developed a MDL approach

for learning Bayesian network structures from a col­
lection of complete data points [LB93]. Unlike many
other works in this area, our approach is able to make
a tradeoff between the complexity and the accuracy
of the learned structure. In essence, it prefers to con­
struct a slightly less accurate network if more accurate
ones require significantly greater topological complex­
ity. Ou:r approach also employed a best-first search
algorithm that did not require an input node order­
ing.

metric. We develop a scheme that makes .it possible to

evaluate the MDL measure of a candidate network by
exa.ming the local structure of each node.
3.1

Applying the MDL Principle

The MDL principle states that the best model to be
learned from a set of source data is the one the min­
imizes the sum of two description (encoding) lengths:
(1) t he description lengt h of the model, and (2) the
description length of the source data given the model.
This sum is known as the total description {encoding)
length. For the refinement problem, the source data
consists of two components, the new data and the ex­
istent network structure. Our objective is to learn a
partial network structure Hp from these two pieces of
information. Therefore, to apply the MDL principle
to the refinement problem, we must find a network Hp
(the model in this context) that minimizes the sum of
the following three items:
its own description length, 1.e., the description
length of Hp,
2. the description length of the new data given the
network Hp, and
3. the description length of the existent network
structure given the network Hp.
1.

The sum of the last two items correspond to the de­
scription length of the source data given the model
(item 2 in the MDL principle). We are assuming that
these two items are independent of each other given
Hp, and thus that they can be evaluated separately.
The desired network structure is the one with the min­
imum total description (encoding) length. Further­
more, even if we cannot find a minimal network, struc­
tures with lower total description length are to be pre­
ferred. Such structures are superior to structures with
larger total description lengths, in the precise sense
that they are either more accurate representations of
the distribution of the new data, or are topologically
less complex, or are closer in structure to the original
network. Hence, the total description length provides
a metric by which alternate candidate structures can
be compared.
We have developed encoding schemes for representing
a given network structure (item 1) , as well as for rep­
resenting a collection of data points given the network
(item 2) (see [LB92] for a detailed discussion of these

386

Lam and Bacchus

encoding schemes). The encoding scheme for the net­
work structure has the property that the simpler is the
topological complexity of the network, the shorter will
be its encoding. Similarly, the encoding scheme for
the data has the property that the closer the distribu­
tion represented by the network is to the underlying
distribution of the data, the shorter will be its encod­
ing (i.e., networks that more accurately represent the
data yield shorter encodings of the data). Moreover,
we have developed a method of evaluating the sum of
these two description lengths that localizes the com­
putation. In particular, each node has a local measure
known as its node description length. In this paper,
we use DLi1d to denote the measure of the i-th node.1
The measure DLf1d represents the sum of items 1 and
2 (localized to a particular node i), but it does not
take into account the existent network; i.e., it must
be extended to deal with item 3. We turn now to a
specification of this extension.
The Existent Network Structure

4

Let the set of all the nodes (variables) in a domain be
X == {Xt, ... , Xn}, and the set of nodes in the new
data be Xp � X containing p :S: n nodes. Suppose
the existent network structure is H,.; Hn contains of
all the nodes in X. Through some search algorithm,
a partial network structure Hp containing the nodes
Xp is proposed. We seek a mechanism for evaluating
item 3, above; i.e., we need to compute the description
length of Hn given Hp.
To describe Hn given that we already have a descrip­
tion of Hp, we need only describe the differences be­
tween H., and Hp. If Hp is similar to Hr., a description
of the differences will be shorter than a complete de­
scription of Hn, and will still enable the construction
of H.,. given our information about Hp. To compute
the description length of the differences we need only
develop an encoding scheme for representing these dif­
ferences.
What information about the differences is sufficient to
recover Hn from Hp? Suppose we are given the struc­
ture of Hp, and we know the following information:
•

•

•

a listing of the reversed arcs (i.e., those arcs in Hp
that are also in H.,. but with opposite direction),
the additional arcs of H., (i.e., those arcs in H,.
that are not present in Hp), and
the missing arcs of H.,. (i.e., those arcs that are
in Hp but are missing from H.,).

It is clear that the structure of H,. can be recovered
from the structure of Hp and the above arc informa­
tion. Hence, the description length for item 3, above,
1This

[LB93].

was

denoted

as

simply DL, in our previous paper

can be taken to be simply the length of an encoding
of this collection of arc information.
A simple way to encode an arc is to describe its two
associated nodes (i.e., the source and the destination
node). To identify a node in the structure Hn, we
need log n bits. Therefore, an arc can be described
using 2log n bits. Let r, a, and m be respectively
the number of reversed, additional and missing arcs
in Hn with respect to a proposed network Hp. The
description length Hn given Hp is then given by:

( r +a+ m)2log n.

(1)

Note that this encoding allows us to recover Hn from

Hp.

This description length has some desirable features.
In particular, the closer the learned structure Hp is to
the existent structure H.. , in terms of arc orientation
and number of arcs in common, the lower will be the
description length of H.. given Hp. Therefore, by con­
sidering this description length in our MDL metric, we
take into account the existent network structure, giv­
ing preference to learning structures that are similar
to the original structure.
Next, we analyze how to localize the description length
of Equation 1. Each arc can be uniquely assigned to
its destination node. For a node X; in Hn let r;, a; and
m; be the number of reversed, additional, and missing
arcs assigned to it given Hp· It can easily be shown
that Equation 1 can then be localized as follows:

L (r; +a;+ m;)2log n.

(2)

X,EX

Note that each of the numbers r;, a,, m;, can be com­
puted by examining only X; and its parents. Specifi­
cally, at each node X; we need only look at its incom­
ing arcs (i.e., its parent nodes) in the structure Hn and
compare them with its incoming arcs in the structure
Hp, the rest of H.,. and Hp need not be examined.
Based on new data, i can be partitioned into two
disjoint sets namely Xp and Xq, where Xq is the set
of nodes that are in X but not in Xp· Equation 2 can
lienee be expressed as follows:

L

(r;+a;+m;)2logn+

L

(r;+a;+m;)2logn.

The second sum in the above equation specifies the
description lengths of the nodes in X,. Since these
nodes are not present in the new data (i.e., they are
not in Xp), the corresponding r;'s and m;'s must be
0. Besides, the a; 's in the second sum are not affected
by the partial network structure Hp. That is, if we are
searching for a good partial network, this part of the
sum will not change as we consider alternate networks.
As a result, the localization of the description length
of the existent network structure (i.e., Equation 1) is

Using New Data to Refine a Bayesian Network

given by:
p. +

L:

X;EXp

h

(3)

+a.;+ 111-;)2log n

5

Given the new data and an existent network struc­
ture, a partial network structure can be learned via
the MDL principle by searching for a network with low
total description length. The search algorithm evalu­
ates the total description length of candidate networks
using this to guide its search. As pointed out in Sec­
tion 3.1, in previous work [LB93] we have been able to
localize the computation of the first two components
of the description length, generating a node measure
function
Similarly, in the previous section we
have shown how the third component of the descrip­
tion length can be localized. Combining these results,
we introduce a new node description length measure
for the refinement task. This is a local measure that
assigns a weight to each node, and can be computed
by examining only the node and its parents. The total
description length of the network is then computed by
simply summing over the nodes.

DLold.

Xi

DLi

=

is defined as:

DL'ild + (r,: + a,:+ 111-;)2logn,

DLi

for
(4)

where
is the local measure given in our previous
work [LB93].

DLfd

Note that any constant terms can be dropped as they
do not play a role in discriminating between alternate
proposed partial network structures Hp. Now the total
de!lcription length of a proposed network structure Hp
is simply (modulo a constant factor) the LX;EXp

DLi.

To obtain the desired partial network structure, we
need to search for the structure with the lowest to­
tal description length. However, it is impossible to
search every possible network: there are exponentially
many of them. A heuri!!tic search procedure was devel­
oped in our previous work [LB93] that has preformed
successfully even in fairly large domains. This search
algorithm can be applied in this problem to learn par­

tial network structures by simply substituting our new
description length function DL for the old one

DLold

6

Refining Network Structures

Once we have learned a good partial network structure
we can refine the original network H,. by using

H'P

{Xl! x2, ...I Xn}

be the nodes in an exis­

tent Bayesian network H,., Xp be any subset of X,
and
::::: Z:x,EX
where
is defined by
p

DLx.,

DLi,

DL;

Equation 4. Suppose we find a new topology for the

stituted into

Structure

the node

=

subgraph formed by the nodes in X'P such that this
new topology does not create any cycles when sub­

Learning the Partial Network

Definition 5.1 The node description length

information contained in Hp. The manner in which Hp
can be used is based on the following theorem about
the node description length measure.
Let X

where p. is a constant that can be ignored when com­
paring the total description lengths of candidate net­
work structures.

387

H,.;

i.e., we find a new way to connect

nodes in Xp that does not create any cycles with the
rest of H,.. This new topology will alter the node de­
scription lengths of the nodes in Xp. Let

DLj/'JJ
p

be

the sum of the node description lengths of Xp under
this new topology. Let n;:•w denote the new network
formed from Hn by replacing the connections between

the nodes in Xp by their new topology.

Theorem 6.1

If DLn.ew
x,

<

DLx-"

then HJ:•w will

ha1!e a lower total description length than H,..

A proof of this theorem is given in [Lam94].
This theorem says that we can improve a network (i.e.,
find one with a lower description length) by improv­
ing one of its subgraphs. The only restriction is that

the resulting network must remain acyclic. The the­
orem demonstrates the importance of our localization
of the total description length metric into a node de­
scription length metric. The node description length
metric allows us to refine a particular part of the net­
work without having to evaluate the total description
length of the entire network; a potentially expensive
task if the network is very large.

Despite the fact that the new data only mentions a
subset Xp of observed nodes from X, it still repre­

sents a probability distribution over the nodes in Xp.
Hence, it contains information about the probabilistic

dependencies or independencies among the nodes in

Xp, and as we have demonstrated, a partial network
structure Hp can be learned from the new data and
the original network. In general, the structure Hp is
not a subgraph of the original network. Nevertheless,

it contributes a considerable amount of new informa­
tion regarding the interdependencies among the nodes

in Xp. In some cases, Hp provides information that
allo ws us to refine the original network, generating
a better network with lower total description length.
An algorithm performing this task is discussed below.
In other cases, it can serve as an indicator for locat­
ing particular areas in the existent network that show
dependency relationships contradicting the new data.
These areas are possible areas of inaccuracy in the orig­
inal network. This issue of using new data to monitor
a network will be explored in future work.

388

6.1

Lam and Bacchus

A Refinement Algorithm

Suppose the existent network structure is

H.. ,

and

the learned partial structure is Hp. The objective of
the refinement process is to obtain a. refined structure
of lower total description length ( hopefully minimum )
with the aid of the existent structure Hn and the par­
tial structure Hp.
Say we have a. node X,.,. in Hp. In Hp this node has
some set of parents Par( X,.,., Hp)1 and a. its description
length DLi Defn. 4 in Hp can be computed. In the
existent network H.., however, X,.,. will in general have
a. different set of parents Par(Xm., Hn) and a different

description length. If Par(Xm., Hn) rt. Xp, then these
two description lengths are incomparable. In this case
Xm has a parent in Hn that does not appear in the
new data; hence the new data cannot tell us anything
about the effect of that parent on Xm 's description
length. We identify all of the nodes Xm whose parents
in H,. are also in Hp and call these the set of marked
nodes.
Suppose for a certain marked node Xm, we decide to
substitute the parents of Xm in H .. with the parents
of Xm in Hp. After the substitution, a. new structure
Hn1 is obtained. Usually the total description length
of H,.1 can be calculated simply by adding the total
description length of the old structure Hn to the dif­
ference between the local description lengths of Xm in

Hn and Hp. The new total description length of H n1
can be evaluated in this way if the substitution of the
parents of X,.,. in Hn does not affect the local descrip­
tion lengths of any other node in H .. . In fact, the only
situation where this condition fails is when the parents
of Xm in Hp contain a reversed arc ( as compared to
Hn)· Under this circumstance, we need to consider
the node X,. associated with this reversed arc. If X,.
is also a marked node, we need to re-evaluate its lo­
cal description length since it will be affected by the
substitution of Xm 's parents. Recursively, we must
detect any other marked nodes that are, in turn, af­
fected by the change in X,. 's description length. It can
be easily observed that these affected nodes must all
be connected. As a result, we can identify a marked
subgraph unit that contains only marked nodes and
which can be considered together as an unit when the
replacement is performed.
Actually, we can obtain the same subgraph unit if we
had started off at any node in the subgraph due to
the symmetrical nature of the influence between the
nodes in the subgraph. For instance, returning to the
previous example, if we considered X,. first, we would
have detected that the local description length of Xm
would be affected by the substitution of X,. 's parents.
The process would have continued and we would have
obtained the same subgraph.
Figure 1 shows the algorithm for the identification of a
marked subgraph unit with respect to Xm. Initially, Q
is a set containing the single node

Xm.

and it grows as

construct-subgraph ( Q ,Xm,M)
Let R be a set of reversed arcs from
set in Hp
For each X,. in R

Xm 's

parent

M=M- {X,.}
X,. is "marked" and Xr 1- Q then
Q = Qu{X,.}
construct-subgraph (Q,Xr,M)

If

Figure 1: The Algorithm for Identification of a Marked
Subgraph Unit

partition-into-subgraph (M,
while M #- 0
X,. is a node from M

M = M-{Xm}
Q = {Xm}

construct-subgraph

S=SU{Q}

S)

(Q,Xm,M)

Figure 2: The Algorithm for Identification of All Sub­
graph Units

the algorithm progresses. Q will contain the required
marked subgraph when the algorithm terminates. Ini­
tially, M is a set containing some nodes that could
be transferred to Q. It shrinks as the algorithm pro­
gresses and contains the remaining marked nodes that
are not included in Q.
Now, we can identify all marked subgra.ph units in Hp·
Parent substitution is to be done for all the nodes in
the subgraph if this subgra.ph is chosen for refinement.
A useful property of the subgraph is that the change
in description length of each subgraph is independent
of all other subgraphs. Figure 2 shows the algorithm
for identifying all marked subgraph units in Hp. Ini­
tially

M contains

all of the marked nodes and

§ = 0.

All subgraph units will be contained in S when the
algorithm terminates. Q is a local variable containing
the nodes for the current subgraph unit.
The refinement problem now is reduced to choosing
appropriate subgraphs for which we should perform
parent substitution in order to achieve a refined struc­
ture of lowest total description length. Although each
subgraph substitution yields an independent reduction
in description length, these substitutions cannot be
preformed independently as cycles may arise.
We use best-first search to find the set of subgraph
units that yields the best reduction in description
length without generating any cycles. To assist the

search task, we construct a list §
{S1, S2, , S1} by
ranking all subgraphs in ascending order of the benefit
gained if parent substitution was to be performed us­
ing that subgraph. The OPEN list contains search ele­
ments which consist of two components ( H, S), where
=

. . •

Using New Data to Refine a Bayesian Network

389

H is a refined network structure and S is the next
subgraph unit to be substituted into H. The elements
in the OPEN list are ordered by the sum of the de­
scription length of H and the benefit contributed by
the subgraph unit S. The initial OPEN list consists
of the search elements {Hi, Si+l) where Hi is obtained
by substituting Si into the existent structure Hn for
i = 1 tot-l.
1. Extract the first element from the OPEN list. Let

it be

{H, S;).

Put H on to the CLOSED list.

2. Construct a new refined structure
porating S; into H.

Hnew

by incor­

3. Insert the element {H, Si+t} into the OPEN list.
If Hnew is acyclic, we also insert the element
{Hnew1 SHt} into the OPEN list.

in existent s.trocture�
but not in true $tr·ucture
)II
arc

arc

in true structure.

but not in existent structure
······ ------�--

o-

Figure 4: The Existent Structure for the First Exper­
iment

4. Terminate if our resource limits are exceeded.

7

Experimental Results

8

Figure 5: The Learned Partial Network Structure for
the First Experiment

8

network, Figure 4, so that it became identical to the
true network, i.e., Figure 3, correcting all errors in the
structure.
The second experiment used new data mentioning 24

Figure 3: The True Structure Used To Generate The
Data Set
Two experiments were conducted to verify our ap­
proach to refinement. The data used in these experi­
ments was extracted from collection of complete data
points that were generated from the ALARM struc­
ture shown in Figure 3. For the first two experiments,
the new partially specified data was obtained by ex­
tracting from the complete data points the values of a
subset of the variables. The extraction of partial data
corresponds to performing a relational algebra "pro­
jection" on the full data set.
The first experiment used new data mentioning 17
nodes. The specific variables extracted for this ex­
periment were variables number 1, 2, 3, 6, 7, 8, 9, 10,
17, 18, 21, 25, 26, 27, 28, 29, and 30.
The existent network structure before refinement was
as shown in Figure 4. Note that we deliberately chose
a slightly different topology from the correct one. The
partial network structure recovered after the learning
process is shown in Figure 5. Our refinement algorithm
was then invoked. It succeeded in refining the existent

nodes, specifically nodes 4, 5, 6, 10, 11, 13, 14, 16, 19,
20,21,22, 23,24, 26, 27, 29, 31, 32, 33, 34,35, 36,and
37. The existent network structure before refinement
was shown in Figure 6. After the refinement process,
the structure was improved to the point where it be­
came identical to the true network, Figure 3, except
for the arc between nodes 10 and 21, which remained
reversed. This result, in fact, demonstrates the capa­
bility of our approach to optimize different features.
If the issue of accuracy with respect to the new data
was the only one considered, the arc connecting node
10 and 21 could be assigned in either direction: both
directions yield the same accuracy. Any distribution
that can be represented by the true structure, Figure 3,
can be equally well represented by a structure in which
the arc between 10 and 21 is reversed but is otherwise
identical. This follows from the results of Verma and
Pearl [VP90] . However, under the MDL metric used
in our refinement approach, the direction from node
10 to 21 (i.e., 10 -+ 20 ) is preferred due to the bias
from the existent structure, Figure 6. In other words,
although accuracy with respect to the data is unable
to decide a direction for this arc, the bias from the
existent network makes our algorithm prefer to pre-

390

Lam and Bacchus

[LB93]

W. Lam and F. Bacchus. Using causal infor­
mation and local measure to learn Bayesian
networks. In Proceedings of the Confer­
ence on Uncertainty in Artificial Intelli­
gence, pages 243-250, 1993.

(Mus93]

R. Musick. Minimal assumption distribu­
tion propagation in Belief networks.
In
Proceedings of the Conference on Uncer­
tainty in Artificial Intelligence, pages 251258, 1993.

but not ln true structure

but not in existent structure
······· ·----·--

(QR89]

J. R. Quinlan and R. 1. Rivest.

[Ris89]

J. Rissanen. Stochastic Complezity in Sta­
tistical Inquiry. World Scientific, 1989.

(SC92]

D.J. Spiegelhalter and R.G. Cowell. Learn­
In
ing in probabilistic expert systems.
Bayesian Statistics 4, pages 447-465. Ox­
ford University Press, 1992.

>

Figure 6: The Existent Structure for the Second Ex­
periment

serve the causal structure of the existent network if no
information in the data contradicts this choice.

Acknowledgments
We would like to thank E. Herskovits and M. Singh
for providing the ALARM network database, and the
referees for some insightful comments.

[SDLC93] D.J. Spiegelhalter, A.P. Da.wid, S.L. Lau­
ritzen, a.nd R.G Cowell. Bayesian analy­
sis in expert systems. Statistical Science,
8(3):219-283, 1993.
[5190]

D.J. Spiegelhalter and S.L. Lauritzen. Se­
quential updating of conditional probabilities on directed graphical structures. Net­
works, 20:579-605, 1990.

[VP90j

T. Verma and J. Pearl. Equivalence and
synthesis of causal models. In Proceedings
of the Conference on Uncertainty in Artifi­




In previous work we developed a method of
learning Bayesian Network models from raw
data. This method relies on the well known
minimal description length (MDL) principle.
The MDL principle is particularly well suited
to this task as it allows us to tradeoff, in a
principled way, the accuracy of the learned
network against its practical usefulness. In
this_ paper we present some new results that
have arisen from our work. In particular, we
present a new local way of computing the
description length. This allows us to make
significant improvements in our search algo­
rithm. In addition, we modify our algorithm
so that it can take into account partial do­
main information that might be provided by
a domain expert. The local computation of
description length also opens the door for lo­
cal refinement of an existent network. The
feasibility of our approach is demonstrated
by experiments involving networks of a prac­
tical size.
1

Introduction

Bayesian networks, advanced by Pearl [Pea86], have
become an important paradigm for representing and
reasoning under uncertainty. Systems based on
Bayesian networks have been constructed in a num­
ber of different application areas, ranging from medi­
cal diagnosis [BBS91), to oil price reasoning [Abr91).
Despite these successes, a major obstacle to using
Bayesian networks lies in the difficulty of constructing
them in complex domains: there is a knowledge engi­
neering bottleneck. Clearly, it would be extremely use­
ful if the construction process could be fully or partly
*Wai Lam's work was supported by an OGS scholarship.
His e-mail address is wlam11Dmath. uwaterloo. ca
tFahiem Bacchus's work was supported by NSERC and
by IRIS. His e-mail address is
fbacchus�Dlogos.uwaterloo.ca

Fahiem Bacchus t

Department of Computer Science
University of Waterloo
Waterloo, Ontario,
Canada, N2L 3G 1
automated. A useful approach, that has recently be­
ing pursued by a number of authors, is to attempt to
build, or learn, a network model from raw data. In
practice, raw data is often available from databases of
records.
We have developed a new approach to learning
Bayesian network models [LB93b]. Our approach
is based on Rissanen's Minimal Description Length
(MDL) [Ris78] principle. The MDL principle offers a
means for trading off model complexity and accuracy,
and our experiments have demonstrated its suitabil­
ity for this task. In this paper we present some sig­
nificant improvements to our original system [LB93b:
which (1) make it more efficient, (2) allow it to take
into consideration domain information about causa
tion and ordering, and (3) allow local refinement of an
existing network.
These improvements are mainly based on a new anal
ysis of the description length parameter that show,,;
how we can evaluate the description length of a pro
posed network via local computations involving only
a node and its parents. This localized evaluation of
description length allows us to develop an improved
searching mechanism that performs well even in fairly
large domains. In addition, it allows us to modify our
search procedure so that it can take into consideration
domain knowledge of direct causes as well as partial or­
derings among the variables. Such partial information
about the structure of the domain is quite common
and in many cases it can reduce the complexity of the
searching process during learning.
The localized evaluation of description length also al­
lows us to modify an existing Bayesian network by
refining a local part of it. By refining the network we
obtain a more accurate model, or adapt an existing
model to an environment that has changed over time
In the sequel we will first describe, briefly, the key fea
tures of our previous work, concentrating in particula
on the advantages of the MDL approach. Then we de­
rive a new localized version of the description lengtl:
computation. Using this we develop an algorithm tha,
searches for a good network model, taking into consid
eration causal and ordering information about the do

244

Lam and Bacchus

main. Finally, we discuss the results of various exper­
iments we have run that demonstrate the effectiveness
of our approach. The experimental results of our work
on local refinement of an existing network are not yet
complete, but we will close with a brief discussion of
the method. The experiment results will be reported
in our full report [LB93a].
Learning Bayesian Networks

2

Much early work on learning Bayesian networks shares
the common disadvantage of relying on assumptions
about the underlying distribution being learned. For
example, Chow and Liu [CL68] developed methods
that construct tree structured networks; hence their
method provides no guarantees about the accuracy
of the learned structure if the underlying distribution
cannot be expressed by a tree structure. The ap­
proach of Rebane and Pearl [RP87], as well as that
of Geiger et. al. [GPP90], suffers from the same criti­
cism, except that they are able to construct singly con­
nected networks. Sprites et al.[SS90] as well as Verma
and Pearl [VP90, PV91] develop approaches that are
able to construct multiply connected networks, but
they require the underlying distribution to be dag­
isomorphic.1
The problem with making an assumption about the
underlying distribution is that generally we do not
have sufficient information to test our assumption.
The underlying distribution is unknown; all we have is
a collection of records in the form of variable instantia­
tions. Hence, in practice these methods offer no guar­
antees about the accuracy of the learned model except
in the rare circumstances where we know something
about the underlying distribution.
Our approach can construct an accurate model from
an unrestricted range of underlying distributions, and
it is capable of constructing networks of arbitrary
topology, i.e., it can construct multiply connected net­
works. The ability to construct a multiply connected
networks is sometimes essential if the network is to be
a sufficiently accurate model of the underlying distri­
bution.
Although multiply connected networks allow us to
more accurately model the underlying distribution
they have computational as well as conceptual dis­
advantages. Exact belief updating procedures are, in
the worst case, computationally intractable over mul­
tiply connected networks [Coo90]. Moreover, even if
an approximation algorithm is used, e.g., the stochas­
tic simulation methods of [CC90, Pea87, SP90], highly
connected networks still require the storage and esti­
mation of an exponential number of conditional prob­
ability parameters. 2 Hence, even if a highly connected
A distribution is dag-isomorphic if there is some dag
that displays all of its dependencies and independencies
[Pea88].
2The number of parameters required is exponential in
1

network is more accurate, in practice it might not be
as useful a model as a simpler albeit slightly less ac­
curate model. In addition to the computational dis­
advantages the causal relationships between the vari­
ables are conceptually more difficult to understand in
a complex network.
Hence, we are faced with a tradeoff. More complex
networks allow for more accurate models, but at the
same time such models may be of less practical use
than simpler models. The MDL principle allows us
to balance this tradeoff: our method will learn a less
complex network if that network is sufficiently accu­
rate, and at the same time it is still capable of learning
a complex network if no simpler one is sufficiently ac­
curate. This seems to be a particularly appropriate
approach to take in light of the fact that we only have
a sample of data points from the underlying distribu­
tion. That is, it seems inappropriate to try to learn
the "most accurate" model of the underlying distribu··
tion given that the raw data only provides us with an
approximate picture of it.
Among other works on learning Bayesian networks, th·
most closely related is that of Cooper and Herskovit•
[CH91]. They use a Bayesian approach that, like ours,
is capable of learning multiply connected networks.
However, as with all Bayesian approaches they must
choose some prior distribution over the space of possi­
ble networks. One way of viewing the MDL principle is
as a. mechanism for choosing a. reasonable prior that is
biased towards simpler models. Cooper and Herskovits
[CH91] investigate a number of different priors, but it
is unclear how any particular choice will influence the
end result. The MDL principle, on the other hand,
allows the system designer (who can choose different
ways of encoding the network) to choose a. prior based
on principles of computational efficiency. For exam·
ple, if we prefer to learn networks in which no node
has more than 5 parents, we can choose an encoding
scheme that imposes a high penalty on networks that
violate this constraint.
2.1

Applying the MDL Principle

The MDL principle is based on the idea that the best
model representing a collection of data items is the
model that minimizes the sum of
1. the length of the encoding of the model, and
2. the length of the encoding of the data given the
model,
both of which can be measured in bits. A detailed
description of the MDL principle with numerous ex­
amples of its application can be found in [Ris89).
To apply the MDL principle to the task of learning
Bayesian networks we need to specify how we can per­
form the two encodings, the network itself (item 1) and
the maximum number of parents of node.

Using Causal Information and Local Measures to Learn Bayesian Networks

the raw data given a network (item

2).

Encoding the Network Our encoding scheme for
the networks has the property that the higher the
topological complexity of the network the longer will
be its encoding. To represent the structure of a
Bayesian network we need for each node a list of its
parents and a list of its conditional probability param­
eters.

Suppose there are n nodes in the problem domain. For
a node with k parents, we need k log 2 (n) bits to list
its parents. To represent the conditional probabilities,
the encoding length will be the product of the number
of bits required to store the numerical value of each
conditional probability and the total number of con­
ditional probabilities that are required. In a Bayesian
network, a conditional probability is needed for every
distinct instantiation of the parent nodes and node it­
self (except that one of these conditional probabilities
can be computed from the others due to the fact that
they all sum to 1). For example, if a node that can
take on 4 distinct values has 2 parents each of which
can take on 3 distinct values, we will need 3 2 X ( 4- 1)
conditional probabilities.
Hence, the total description length for a particular net­
work will be:
i=l

iEF;

where there are n nodes; for node i, ki is the number
of its parent nodes, Si is the number of values it can
take on, and Fi is the' set of its parents; and d repre­
sents the number of bits required to store a numerical
value. For a particular problem domain, n and d will
be constants. This is not the only encoding scheme
possible, but it is simple and it performs well in our
experiments.
By looking at this equation, we see that highly con­
nected networks require longer encodings. First, for
many nodes the list of parents will become larger, and
second the list of conditional probabilities we need to
store for that node will also increase. In addition, net­
works ir.. which nodes that have a larger number of
values have parents with a large number of values will
require longer encodings. Hence, the MDL principle,
which is trying to minimize the sum of the encoding
lengths, will tend to favor networks in which the nodes
have a smaller number of parents (i.e., networks that
are less connected) and also networks in which nodes
taking on a large number of values are not parents of
nodes that also take on a large number of values.
In Bayesian networks the degree of connectivity is
closely related to the computational complexity of
using the network, both space and time complexity.
Hence, our encoding scheme generates a preference for
more efficient networks. That is, since the encoding
length of the model is included in our evaluation of
description length, we are enforcing a preference for

245

networks that require the storage of fewer probabiliij
parameters and on which exact algorithms are more
efficient.
Encoding the Data Using the Model The task is
to learn the joint distribution of a collection of random
variables X = {Xt, ... ' Xn}· Each variable xi has
an associated collection of values {x}, ..., xi'} that it
can take on, where the number of values Si depends on
i. Every distinct choice of values for all the variables
in X defines an atomic event in the underlying joint
distribution and is assigned a particular probability by
that distribution.

We assume that the data points in the raw data are
all atomic events. That is, each data point specifies a
value for every random variable in X. Furthermore,
we assume that the data points are the result of in­
dependent random trials. Hence, we would expect,
via the central limit theorem, that each particular in­
stantiation of the variables would eventually appear in
the database with a relative frequency approximately
equal to its probability. These are standard assump­
tions.
Given a collection of N data points we want to encode,
or store, the data as a binary string. There are various
ways in which this encoding can be done, but here we
are only interested in using the length of the en cod·
ing as a metric, via item 2 in the MDL principle, for
comparing the merit of candidate Bayesian Networks.
Hence, we can limit our attention to character codes
[CLR89, pp. 337]. W ith character codes each atomic
event is assigned a unique binary string. Each of the
data points, which are all atomic events, is converted
to its character code, and the N points are represented
by the string formed by concatenating these character
codes together. To minimize the total length of the
encoding we assign shorter codes to events that oc­
cur more frequently. This is the basis for Huffman's
encoding scheme. It is well known that Huffman's al­
gorithm yields the shortest encoding of the N data
points [LH87].
Say that in the underlying distribution each atomi,'
event ei has probability Pi and we construct, via some
learning scheme, a particular Bayesian network from
the raw data. This Bayesian network acts as a model of
the underlying distribution and it also assigns a prob­
ability, say qi, to every atomic event ei. Of course,
in general qi will not be equal to Pi, as the learning
scheme cannot guarantee that it will construct a per
fectly accurate network. Nevertheless, the aim is for q.
to be close to Pi,- and the closer it is the more accurate
is our model.
The constructed Bayesian network is intended as our
best "guess" representation of the underlying distribu­
tion. Hence, given that the probabilities qi determined
by the network are our best guess of the true values Pi
it makes sense to design our Huffman code using thesf
probabilities. Using the qi probabilities the Huffman

246

Lam and Bacchus

algorithm will assign event ei a codeword of length ap­
proximately -log2(qi)· If we had the true probabilities
Pi, the algorithm would have assigned ei and optimal
codeword of length -l og2 (Pi ) instead. Despite our use
of the values qi in assigning codewords, the raw data
will continue to be determined by the true probabil­
ities Pi· That is, we still expect that for large N we
will have NPi occurrences of event ei 1 as Pi is the true
probability of ei occurring. Therefore, when we use
the learned Bayesian network to encode the data the
length of the string encoding the database will be ap­
proximately
{2)

where we are summing over all atomic events. How
does this encoding length compare to the encoding
length if we had access to the true probabilities Pi?
An old theorem due originally to Gibbs gives us the
answer.
(Gibbs) Let Pi and qi, i = 1, .. . , t, be
non-negative real numbers that sum to 1. Then

Theorem 2.1
t

t

- 2:Pi log 2(Pi) � - 2:Pi log2 (qi) ,
i =l
i=l
with equality holding if and only if \fi.pi = qi. In the
summation we take Olog2 (0) to be 0.

In other words, this theorem shows that the encoding
using the estimated probabilities qi will be longer than
the encoding using the true probabilities Pi· It also
says that the true probabilities achieve the minimal
encoding length possible.
The MDL principle says that we must choose a net­
work that minimizes the sum of its own encoding
length, which depends on the complexity of the net­
work, and the encoding length of the data given the
model, which depends on the closeness of the proba­
bilities qi determined by the network to the true prob­
abilities Pi, i.e., on the accuracy of the model.
We could use Equation 2 directly to evaluate the the
encoding length of the data given the model. How­
ever, the equation involves a summation over all the
atomic events, and the number of atomic events is ex­
ponential in the number of variables. Instead of trying
to use Equation 2 directly we investigate the relation­
ship between encoding length and network topology.
Let the underlying joint distribution over the variables X = {X11 , Xn} be P. Any Bayesian network
model will also define a joint distribution Q over these
variables. We can express Q as [Pea88]:
• • •

Q(X)

P(Xt I Fx1)P(X2 I Fx.) ...P(Xn I Fx,J,
(3)
where Fx; is the, possibly empty, set of parents of Xi
in the network. Note that P appears on the right hand
side instead of Q. We obtain the conditional proba­
bility parameters on the right from frequency counts
=

taken over the data points. By the law of large num­
bers we would expect that these frequency counts will
be close to the true probabilities over P.3
We can now prove the following new result that is the
basis for our new localized description length compu­
tations:
The encoding length of the data (Equa­
tion 2) can be ezpressed as:
n
n
-N '2:W(Xi,Fx;)+N '2::[- '2:P(X.)log2(P(X•))]
i=l
i=l
(4)

Theorem 2.2

where the second sum is taken over all possible instan­
tiations of Xi. The term W(Xi, Fx;) given by
W(Xi,Fx,)

=

P(Xi,Fx,)
�
L.J P(Xi,Fx,) log2

P(Xi)P(Fx;)

Xi,Fxi

(5)
where the sum is taken over all possible instantiations
of xi and its parents Fxi I and we take w(Xi I Fxi ) 0
if Fx, = 0. The proof of this, and all other theorems,
is presented in our full report [LB93a].
=

Given some collection of raw data, the last term in
Equation 4 is independent of the structure of the net­
work. Furthermore, the weight measure, the first term
in Equation 4, can be calculated locally.
Localization of the Description

3

Length

To make use of the MDL principle, we need to evaluak
the total description length {item 1 + item 2) given a
Bayesian network. Adding Equation 1 and 4, the total
description length is:
n

n

L[k,log2(n)+(s•-1)(
i=l

II
n

SJ)d]-NLW(Xi,Fx,)
i=l

+NL[-LP(Xi)log2(P(Xi))]
i=l
Xi
n

=

L[[kilog2(n)+ (si -1)(
i=l

II

s;)d].- NW(Xi,FxJ]

n

+NL[-LP(Xi)log2(P(Xi))]
x.
i=l

(6)

The last term in Equation 6 remains constant for a
fixed collection of raw data. Therefore, the first term
is sufficient to compare the total description lengths of
alternative candidate Bayesian networks.
3It might not be the case that Pis equal to this decom­
position. The approximation introduced by our network
model is precisely the asswnption of such a decomposition

Using Causal Information and Local Measures to Learn Bayesian Networks

The node description length DLi for
the node Xi, with respect to its parents Fx., is defined
as:

Definition 3.1

{7)
Definition 3.2 The relative total description length
for a Bayesian network, defined as the summation of
the node description length of every node in the net­
work, is given by:
n

{8)

i=l
As a result, the relative total description length is ex­
actly equivalent to the first term in Equation 6, and
thus is sufficient for comparing candidate networks.
Moreover, it can be calculated locally since each DLi
depends only on the set of parent nodes for a given
node xi.
Given a collection of raw data, an
optimal Bayesian network is a Bayesian network for
which the total description length is minimum.

Definition 3.3

Clearly, one or more optimal Bayesian networks must
exist for any collection of raw data. Furthermore, we
have the following result.
Given a collection of raw data, the rel­
ative total description length of an optimal Bayesian
network is minimum. Also, for a given node Xi in an
optimal Bayesian network, DLi is minimum among
those parent sets creating no cycle and not making the
network disconnected. That is, we cannot reduce DLi
by modifying the network to change xi 's parents.

Theorem 3.4

This theorem says that in an optimal network no sin­
gle node can be locally improved. It is possible, how­
ever, that a non-optimal network could also possess
this property. In such a case the parent sets of a num­
ber of nodes would have to be altered simultaneously
in order to reduce its description length.
4

Incorporating Partial Domain
Knowledge

247

between the other variables. This kind of informatio
might be provided by, e.g., domain experts, and we ca,
use it when generating the network model. In particu­
lar, we can require that in the learned model Xi be one
of Xj 's parents, thus ensuring that the model validates
the direct causation. More generally, the domain e:x.­
perts might be able to construct a skeleton of the net­
work, involving some, but not all, of the variables. The
arcs in the skeleton can be specified as direct causation
specifications to our system, which will then proceed
to fill in the skeleton placing the remaining variables
in appropriate positions.
Partial ordering information, on the other hand, spec­
ifies ordering relationships between two nodes. Such
information might, for example, come from knowledge
about the temporal evolution of events in our domair,.
For instance, if we know that Xi occurs before x3, the
network model should not contain a path from Xj b
Xi as no causal influence should exist in that dire.:
tion. Note that a total ordering among the variable�,
as required by Cooper and Herskovits [CH91], is just
a special case of our partial ordering specifications.
Subject to the condition that the direct causation and
partial ordering specifications not entail any transitiv
ity violations (e.g., we cannot have a circular sequence
of direct causations as input to the system), our sys­
tem can ensure that the constructed network validate3
these specifications. Furthermore, information of thi;
sort can in fact lead to increased efficiency: it will con­
strain our search for an appropriate network model.
To incorporate this information, we define a
strained Bayesian network as follows:

o ­

c n

Definition 4.1 A constrained Bayesian network is an
ordinary Bayesian network whose topology includes all
the arcs specified by the direct causation specification;
and does not violate any partial ordering specifica­
tions.

It can be shown that Theorem 3.4 still holds, with
the obvious modifications, if we consider constrained
Bayesian networks instead of ordinary networks.
5

Searching for the Best Constrained
Network

Although we might not know the underlying joint dis­
tribution governing the behavior of the domain vari­
ables, we could possibly have other, partial, informa­
tion about the domain. In particular, our new system
can consider two types of domain knowledge: direct
causation specifications and partial ordering specifications.

Although our expression for the relative total descrip
tion length allows us to evaluate the relative merit of
candidate network models, we cannot consider all pos­
sible networks: there are simply too many of them
(an exponential number in fact). Hence, to apply th�;
MDL principle we must engage in a heuristic search
that tries to find a good (i.e., low description length),
but not necessarily optimal, network model.

By direct causation information we mean information
of the form "Xi is a direct cause of X/'. That is, we
might know of a direct causal link between two vari­
ables, even if we do not know the causal relationships

In this section we describe our search algorithm which
attempts to find a good network by building one up
arc by arc. The first step is to rank the possible arcr.
so that "better" arcs can be added into the candidat·

·

248

Lam and Bacchus

networks before others. The arcs are ranked by cal­
culating the node description length for Xj given the
arc Xi ----+ Xj, i =/= j, using Equation 7 and treating Xi
as the single parent. This node description length is
assigned as the "description length" of arc Xi ----+ Xj.
A list of arcs PAIRS is created sorted so that the first
arc on PAIRS has lowest description length. PAIRS will
contain all arcs except for those violating the direct
causation or partial ordering specifications. Looking
at Equation 7 we can see that if Xi and Xj are highly
correlated (as measured by W(Xj, Xi), Equation 5)
the description length will be lower, and an arc be­
tween them will be one of the first that we will try to
add to the candidate networks.
Search is performed by a best-first algorithm that
maintains OPEN and CLOSED lists each containing
search elements. The individual search elements have
two components (G,L): a candidate network G, and
an arc L which could be added to the candidate net­
work without causing a cycle or violating the partial
ordering and direct causation specifications. OPEN is
ordered by heuristic value, which is calculated as the
relative total description length (Equation 8) of the
element's network, plus the description length of the
element's arc (calculated during the construction of
PAIRS). Therefore, the lower the heuristic value, the
shorter the encoding length. Initially, we construct a
network Ginit containing only those arcs included in
the direct causation specifications. Then, the initial
OPEN list is generated by generating all of the search
elements (Ginit,L) for all arcs L E PARIS. Best-first
search is then executed with the search element at the
front of OPEN expanded as follows.
1. Remove the first element from OPEN and copy it
onto CLOSED. Let the element's network be Gold
and the element's arc beL.
2. Invoke the ARc-ABSORPTION procedure on Gold
and L to obtain a new network Gnew with the
arc L added. The ARc-ABSORPTION procedure,
described below, might also reverse the direction
of some other arcs in Gold·
3. Next we make a new search element consisting of
Gnew and the first arc from PAIRS that appears
after the old arcL which could be added to Gnew
without generating a cycle or violating a partial
ordering specification. This new element is placed
on OPEN in the correct order according to the
heuristic function.
4. Finally, we make another new search element con­
sisting of Gold and the first arc from PAIRS that
appears after L which could be added to Gold
without generating a cycle or violating a par­
tial ordering specification. Again, this element
is placed on OPEN in the correct order.
Now we describe the ARC-ABSORPTION procedure
which finds a locally optimal way to insert a new arc
into an existing network. To minimize the description

length of the resulting network, the procedure might
also decide to reverse the direction of some of the old
arcs.
A network Gold·
An arc ,(Xi ----+ Xj) to be added.
Output : A new network Gnew with the arc added
and some other arcs possibly reversed.

Input

1. Create a new network by adding the arc (Xi -->
Xj) to Gold· In the new network we then search
locally to determine if we can decrease the relative
total description length by reversing the direction
of some of the arcs. This is accomplished via the
following steps.
2. Determine the optimal directionality of the arcs
attached directly to Xj by examining which di­
rections minimize the relative total description
length. Some of these arcs may be reversed by
this process.4 Furthermore, we do not consider
the reversal of any arcs that would result in the
violation of the direct causation or partial order­
ing specifications.
3. If the direction of an existing arc is reversed thel.<
perform the above directionality determinatim,
step on the other node affected.
The search procedure is mainly composed of the ARc
ABSORPTION procedure, a cycle checking routine, an•!

·

a partial order checking routine. The complexity of
cycle checking and partial order checking are O(n) and
O(n2) respectively, where n is the number of nodes
We have found that the search can arrive at a very
reasonable network model if provided with a resource
bound of O(n2) search elements expansions. Under
this resource bound, we have found that in practice the
overall complexity of the search mechanism remains
polynomial in the number of nodes n.
We can further observe that when the amount of do­
main information increases, the search time to find a
good network model decreases. This arises from the
fact that such information places constraints on the
space of allowable models making search easier. For
example, if a total ordering among the nodes in the
domain is given, the search time will be reduced by a
factor of O(n2): there is no need to perform the cycle
or partial order checking, and the arc reversal step in
ARc-ABSORPTION is no longer needed.
6

Experiments

Following (CH91] we test our approach by constructing
an original network and using Henrion's logical sam
pling technique (Hen87] to generate a collection of raw
4Note that it is sufficient to compute the node descrip·
tion length (Equation 7} of those nodes whose parents have
been changed. The relative total description length (Equa­
tion 8} of the whole network need not be computed.

Using Causal Information and Local Measures to Learn Bayesian Networks

249

P(c1lb1)=0.8 P(c1lb0)=0.2

conditional
probabi1ity
parameters

P(e1la1,b1,c1,d1)=0.9 P(e11aO,b1,c1,d1)=0.1
P(e1la1,bO,c1,d1)=0.15 P(e1laO,bO,c1,d1)=0.1
P(e1la1,b1,cO,d1)=0.1 P(e1laO,b1,cO,d1)=0.1
P(e1la1,bO,cO,d1)=0.08 P(e1laO,bO,cO,d1)=0.1
P(e1la1,b1,c1,d0)=0.1 P(eilaO,bi,c1,d0)=0.1
P(eila1,bO,c1,d0)=0.1 P(e1laO,bO,c1,d0)=0.1
P(eila1,b1,cO,d0)=0.1 P(eilaO,b1,cO,d0)=0.1
P(e1la1,bO,cO,d0)=0.1 P(e1laO,bO,cO,d0)=0.1

same as the
£irst except
£or:
P(e1la1,bO,c1,d1)
=0.8

same as the
£irst except
£or:
P(e1la1,b1,cO,d1)
=0.8

same as the
£irst except
£or:
P(c1lb0)=0.9

B
1earned
structures

��D
E

E

Figure

1:

The Quality of Learned Networks

data. We then apply our learning mechanism to the
raw data to obtain a learned network. By comparing
this network with the original we can determine the
performance of our system.
In the first set of experiments, the original Bayesian
network G consisted of 5 nodes and 5 arcs. We varied
the conditional probability parameters during the pro­
cess of generating the raw data obtaining four different
sets of data. Exhaustive searching, instead of heuris­
tic searching, was then carried out to find the net­
work with minimum total description length for each
of these sets of raw data resulting in different learned
structures in each case. The experiment demonstrates
that our algorithm does in fact yield a tradeoff between
accuracy and complexity of the learned structures: in
all cases where the original network was not recovered
a simpler network was learned. The type of structure
learned depends on the parameters, as each set of pa­
rameters, in conjunction with the structure, defines a
different probability distribution. Some of these distri­
butions can be accurately modeled with simpler struc­
tures. In the first case, the distribution defined by the
parameters did not have a simpler model of sufficient
accuracy, but in the other cases it did. We have also
developed measures of the absolute accuracy of the
learned structures (see (LB93b] for a full description)
that indicate in all cases that the learned structure was
very accurate even though it might possess a different
topology.
The second experiment consisted of learning a
Bayesian network with a fairly large number of vari­
ables (37 nodes and 46 arcs). This network was de­
rived from a real-world application in medical diagno­
sis [BSCC89] and is known as the ALARM network
(see (LB93b) for a diagram of this network). After ap­
plying our heuristic search algorithm, we found that

the learned network is almost identical to the original
structure with the exception of one different arc and
one missing arc. One characteristic of our heuristic
search algorithm is that we did not require a user sup­
plied ordering of variables (cf. Cooper and Herskovits
(CH91]). This experiment demonstrates the feasibility
of our approach for recovering networks of practical
size.
Besides being able to use extra domain information
our new search mechanism is faster and more accurate
than the mechanism first reported in (LB93b] which
was developed without the local measure of descrip­
tion length. To investigate how our search mechanism
behaves when domain information is supplied, we con­
ducted some further experiments. Using the same set
of raw data derived from the ALARM model in con
junction with varying amounts of domain information,
we applied our learning algorithm and recorded the
search time required to obtain an accurate network
model. The following two tables depict the relative
time required by the search algorithm when varying
amounts of direct causation and partial orderings spec­
ifications are made available. In general, the search
time decreases as the amount of causal information
Increases.

time

time

no p artial
ordering
100%

10 partial
orderings
84%

no direct
causal
specification
100%

20 partial
orderings
60%

10 direct
causal
sp ecifications
74%

total
ordering _i
20%

20 direct
causal
specifications
25%

I

250

Lam and Bacchus

Refinement of Existent Networks

(Coo90]

Besides the advantages outlined above our new local com­
putation of description length also allows for the possibil­
ity of refining an existing network by modifying some local
part of it. Refinement is based on the following theorem.

[GPP90]

7

Theorem 7.1 Let

X= {Xl' x2, .. . ,X....}

be the nodes in

an existent Bayesian network, X' be a subset of X, and
DLx' be the total node description lengths of all the nodes
in X' (i.e., DLx' = l:x,ex' DL,). Suppose we find a new

set of parents for every node in X' that does not create any
cycles or make the network di.sconnected. Let the new total
node description lengths of all the nodes in X' be DLnewX'·
Then we can construct a new network in which the parents
of the nodes in X' are replaced by their new parent sets,
such that the new network will have lower total description
length if DLnewX' < DLx'·

This theorem provides a means to improve a Bayesian net­
work without evaluating the total description length of the
whole Bayesian network, a potentially expensive task if the
network is large. We can isolate a subset of nodes and try to
improve that collection locally, ignoring the rest of the net­
work. Algorithms for performing such a refinement, based
on this theorem, have been developed and experiments are
being performed. We hope to report on this work in the
near future [LB93a].

·

[Hen87]

[LB93a]
[LB93b]

[LH87]

·

[Pea86]

[Pea87]



In previous work
studied the

[BGHK92, BGHK93], we have
random-worlds approach�a particul ar

(and quite powerful) method for generating degrees

of belief (i.e., subjective probabilities) from a knowl­
edge base consisting of objective (first-order, slalisti­

cal, and defaull) infonnation. But all owing a k n ow l
edge base to contain only objective in form ation is

­

sometimes limiting. We occa<;ionally wish to include
infonnation about d egrees of belief in the knowledge
base as well, because there are contex t s in which old

be1iefs represent importan t information that should
influence new beliefs. In this paper, we describe three
quite general techniques for extending a method that
generates degrees of belief from objective informa­
tion to one that can make use of d egrees of belief as
well. All of our techniques

are

ha<;ed on well-known

approaches, such a5 cross-emropy. We di sc uss gen­

eral connections between the techniques and in partic

­

ular show that, although concept u ally and technically
quite different, all of the t echniques give the same
answer when applied to the random-worlds method.

1

halpern@almaden.ihm.com

Daphne Koller

Computer Science Division
University of California, Berkeley
Berkeley, CA 94720
daphne@cs.berkeley.edu

principle determine if the agent's objective information is
cmTect (by ex amin ing what is actually the case in its envi­
ronment), we cannot so ea<;ily say that its subjective beliefs
are '-Xmcct. The truth or falsity of these pieces of informa­
tion is not detc nni n ed by the state of the environment.
Although su b j ecti ve infonnation could take many differ­

ent l(mns. we will concentrate here on

degrees of belief

arc probabilities that are assigned to fonnulas ex­
pressing objective a sse rti ons. For example, the assertion
"the weather is warm in New York" is an objective one: it

These

is either true or false in the agent's environment. But when

we <L..; sig n a degree of belief to this assertion, as above, we

obtain

a

subjective a�sertion: it becomes a statement about

the state of the agent's be l ie fs. In tl1e context of probability

d is tinction between subjective and objective can

theory the

appear somewhat subtle. be cau se some form of objective
information (such <L<; proportions

or frequencies) obey the

laws of probahility,j ust a<; d o degr ees of belief. Yet the dis­

tincrion can be a significant one if we want to use or interpret

a pmbahilistic theory conectly. Camap's work

[Car50]

is

notewort11y for its care ful distinction between, and study
of, both st at i s tical

degree of belief

probabilities, which are objective, and
probabil it ies, which are subjective.

In order to und erstand

Introduction

vide

a

this distinction, it is useful to pro­

formal semantics for degrees of belief that captures

the difference between them and objective information. As

When we examine the knowledge or information possessed
by an agent, it is useful to distinguish between

subjective

objective information. Objective information is infor­
mation about me environment, whcrca� su�iective informa­
and

tion is infonnation about the state of the agen t s hcliefs. For
example, we might c hara ct erize the infonnation of an agent
'

travelling from San Francisco

to New York as con�isting of
the objective infonnation tlmt the weather is warm in San
Francisco, and the subjecti ve infotmation that the proba­

bility rhat the weather is wann in New York is 0.2. The
important thing to notice here i s tlmt altl10ugh we can in
*This research has been supported in p art by the Canadian

Government through their NSERC nnd IRIS programs, by the

Air Force Office of Scientific Research (AFSC) under Contract
F49620-91-C-0080, and by a University of California President's

Postdoctoral Fellowship. The United States Government is au­
thorized to reproduce nnd distribute reprints for govemmentJI
purposes.

,Joseph Y. Halpern
IDM Almaden Research Center
650 Han·y Road
San Jose, CA 95120-6099

dcmonstraled by Halpern [Hal90], a natural, and very gen­

eral . way to give a
d efi nin g
worlds.1

semantics

to degrees of belief is by

probabi li ty distribution over a set of possible
The degree of bel ie f in a fonnul a r.p is then the

a

probab il it y of the set of worlds where r.p is true. In this
framework we can characterize o�j ective infonnation as
consisting of assertions ( ex pressed as fmmulas} that can
he assi gned a truth value by a single world. For example,
in any given world Tweety the bird does or does not fly.
Hence, the fonnula Fly(Tweety) is objective. Statistical
assertions such as IIFly(x)IBird(J:)II'"

i m atel y

80% of

�

0.8, read "approx­

birds fly", are also objective. On the other

hand, Pr( Fly(Tweety))

=

0.8, expressing the assertion that

1Conceptually. this notion of world is just as in classical
··possibl�-worlds semantics": a complete picture or description
of tht! way the world might he. Formally, we take a world to be
an intcrprctntion (model) for first-order logic.

Bacchus, Grove, Halpern, and Koller

38

the agent's degree of belief in Tweety !l yi ng

is 0.8, is n ot

objective, as its truth is determined by whether or not tlle
pr obability of the set of worlds where Tweety flies is

0.8.

Although we cannot easily characterize an agent's degr ees
of beliefs as being correct or incorrect, it is nevertlleless
clear that these beliefs should have some rel ation to objec­
tive reality. One way of guaranteeing this is to actually

generate them from the objective information available to
the agent. Several ways of doing this h ave been consid­
ered in the literature; for example, [BGHK92, PV92] each

discuss several possibilities. The appr oach es in {BGHK92)
are based in a very natural way on tlle semantics des cr ib e d
above. Assume we have a (prior) probability distribution
over some set of worlds.

We cru1 t11e n generate degr ees

of belief from an objective know ledge ba�e KB by u sin g
standard Bayesian conditioning: to t11e formul a 'P we a�­
sign as its degree of belief the conditional prob ab i lity

of

r.p given KB. In [BGHK92] we considered three particu­
lar choices for a prior, and i nve stigat ed the properties of
the resulting inductive inference sy stem s.

In

[BGHK93]

we concentrated on the simp lest of these methods-the
choice of prior is essen­

random-worlds m eth od-whose

tially the uniform prior over the se t of possible worlds.

More precisely, suppose we re strict our attention to worlds
(i.e., interpretations of ru1 appropriate vocabul ary for first­

order logic) with t11e domain

{ I, . . . , N}.

Assumin g we

have a finite vocabulary, there will be only finitely many

such worlds. Random worlds ta kes a'> the set of worlds all

of these worlds, and uses p erh ap s the s i mpl e st probability
distribution over them-the uniform distribution�thus as­
suming that each of t11e worlds is eq u ally likely. This gi ves
a prior distribution on the set of pos sibl e worlds. We can
now induce a degree of belief i n 'P gi ven KB by using the
conditional probability of 'P

given Kll with respect to this
of

uniform distribution. II is ea-;y to see that the degree

belief in r.p given KB is then s i mply tJ1e fraction of possible
worlds satisfying KB that also sati s fy 'P· In general . how­

ever, we do not know the domai n size N; we know only
that it is typically large. We cm1 therefore appro x i mate the

degree

of belief for the true but unknown

N

by computing

the limiting value of this degree of be l ief as N g rows large.
This limiting value (if it exists, which it may not) is denoted

�( r.piKB), and it is what the rando m -w orld method takes

to be the degree of belief in 'P given

KB. In [BGHK93], we

showed that U1is method posse ss es a number of anractive
properties,

such. as a prefe renc e for more

specific infonna­

tion and the ab i lity to ignore inelevant information.
The random-worlds method Clli1 generate degrees of be­
Iief from rich k nowledge b ases that may contain first-order.
statistical , and default information. However, w; with any

conditioning process, is limited to deal ing

with objective

information. When we add subje cti ve formulas to

KB. we

can no longer simply c ond iti o n on KB: the conditioning
process eliminates those worlds inconsistent with our infor­

mation, while U1e truth of a subjective formula cannot be
determined by a single world.2

Hence, we would like to

2
In the context of random worlds (and in other cases where
the degrees of belief are de termined using

a

prior

on

the

set

of

extend the nmdom-worlds method so as to enable it to deal
with both objective and subjective information.
Why do we wrun to take into account subjective beliefs?
There are a number of situations where this seems to make
sense. For example, suppose a birdwa tcher is interested
in a domain of birds, m1d has an obj ective knowledge base
KBbird con si sting of t11e statistical inf ormation

IICardinal(x)i-.Red(.1�)11x � 0.1
IICardina/(;t)IRed(x)llx � 0.7.

1\

Now the b irdw at cher catches a glimpse of a bird

b flying by

that seems to be r ed. The birdwatcher is trying to decide

if b is a cru·dinal.

By the results of [BGHK93], if the

birdwatcher assumes U1at U1e bird is n ot red, random-worlds

gi ves

1\

Prr�(Cardinal(b)IKBbird

-.Red(b))

::::: 0.1. On the

other hand, if she assumes tlmt the bird is red, we get

Red( b))::::: 0.7.

But it does not

to he able to generate a d eg ree of b eli ef

in Cardinal( b)

P•J.:::(Conlinal(b)IKB,;,-rJ

1\

seem ap prop riate for her to do either; rather we would like
that takes into acco u nt the birdwatcher's degree of belief in

Red( b).

For example, if this degree of

be lief is 0.8, then

we would like to use a kn owled ge ba<>e such as KBb;rd 1\
Pr ( Red( b ) ) = O.R. It seems re asonable to expect that the

resul t ing degree of oclief in

Cardinal(b)

somewhere bet wee n the two e xtremes of

would then be

0. 7 and 0.1.

As ano ther example, suppose we have reason to believe

that two sensors are independent. For simplicity, suppose

the sensors measure temperature, and report it to be either

high, h. or low. I. We can imagine t11ree unary predicates:
S/(.r), indica ting that sensor 1 reports the value x; S2(x),

and Actual ( x ) , i ndi cating
That the sensors are inde­
pen d ent (given the actual value) can be represented by the
:11
co nj unct i o n over all choices for .r:, .1J, and J
i n {I, h} of:

a similar predicate for s ensor
that the act u al

Pr(S/(:t')
=

2;

temperature is ;r:.

1\ S2(.r")IActual(;r))

Pr(Sl(:t')IActua/(;r))

x

Pr(S2(x")IActual(x)).

It co u ld he tha t we h ave dctennined that ilie sensors are
independent through the observation
readings.

of a n umber

of test

Such e mpirical evidence c oul d be summarized

by a stati s t ical assertion �md thus added to our knowledge

base without req u i ring a degree of be lie f statement like ilie

ab ove. However. this is not the normal situation. Rather, we

arc more likely to have based our belief in independence

on other i n form ation. such a'>

our

beliefs

about causality.

For example, the sensors may h ave been built by different
manufacturers.

In this ca�e. it seems most reasonable to

represent this kind of informa tion using an assertion about
degrees of belief.

How, t hen , can we incorp or at e information about degrees of
belief into the ran d om -worlds framework? More generally,
3
given any inference proce,u -i.e., a method for generat­
ing degrees of belief from objective infonnat i on-we would
worlds). this problem can be v iewed as an instance of the general
problem of conditioning a distribution on uncertain evidence.
3The term ··inference process" is taken hom Paris and Vencov­

sko I PVX91. Our framework is slightly different from theirs, but
we think t h i s usage of the term is consistent with their intent.

Generating New Beliefs from Old

like to extend it so that it can also deal with subjective infor­
mation. This is an issue that has received some attention re­
cently [PV92, Jae94b, Jae94a]. We discuss lhree techniques

39

by us in g cross-entropy [KL51]. Given two probability dis­
tributions p and p', the cross-entropy of p1 relative to J.l,
denoted C(1/, 1•), is a meao;;ure of how "far" J.t1 is from J.l

here, and consider their application in the specific context
of random worlds. As we shall see, all of our techniques

a p rior

are very closely based on well-known ideas in the litera­
ture. Two make use of cross-entropy, while the t11ird is a

can t11en find the distribution on worlds satisfying the con­
s train ts tllat minimizes cross-entropy relative to the prior,

generalization of a method considered by Paris and Vencov­

ska [PV92]. They are conceptually and

formally d istin ct

,

yet there are some interesting connections between them.
In particular, in the context of random-worlds they gener­
ally yield the same answers (where the compruison makes
sense; the various methods have different nmges of applica­
bility). Many of the results we discuss are, in general terms

if not in specific details, already known. Nevertlleless, th eir
combination is quite interesting.
We now describe the three methods in a lillie more detail.
The first method we exrunin e is

perhaps the simplest to ex­
t1Je context of random worlds.
Fix N. Random worlds considers all of the worlds that have
domain { 1, ... , N}, ru1d assumes they are equal ly likely,
which seems reasonable in the ab sence of information to
the contrary. But now suppose that we have a degree of
belief such as Pr(Red(b)) = 0.8. In thi s case it is no longer
reasonable to assume that all worlds are equally likely; our
knowledge base tells us that the worlds where b is red are
plain. We consider it first in

[SJ80, Sho86]. Given an inference method that generates

and

and a set of constraints determined by the KB, we

then use t11is new distribution to compute degrees of
We call this method CEW (for cross-entropy on

belief.

worlds).
The next method we consider also uses cross-entropy, but
in

a completely different way. Suppose we have the (ob­

KBbird given above, and a separate
(Pr(Red(b)) = 0.8). As we sug­
gested, if the birdwatcher were sure Umt b was red, random
worlds would give a degree of belief of 0.7 in Cardinal( b);
similarly, if she were sure that b was not red, random worlds
would give 0.1. Given that her degree of belief in Red{b)
jective) knowledge base

."

beli e f

ha<;e" BBbird

is O.R. it seems reasonable to a�sign a degree of belief of
O.R x 0.7 + 0.2 x 0.1 to Cardinal(b). In fact, if we consider
any inference process I ( not necessarily one that generates
a prior prnhahility on possihle worlds), it seems reasonable
to ddine
I (Cardinal( b) I KBhird 1\
=

he equally likely, we
divide the worlds into two sets: those which satisfy Red(b)
and those which satisfy -.Red( b). Our beliefs require that
the first set have probability 0.8 �md the second probability
0.2. But otherwise we can make the worlds within each set
equally likely. This is consi ste nt with the random worlds
approach of making all worlds equally likely. Intuitively,
we are considering the probability dis t ibut ion on the worlds
that is as close as possible to our ori g i nal uniform distribu­
tion subject to the constraint that the set of worlds where
Red( b) holds should have probability 0.8.
tion. Rather than taking all worlds to

r

What do we do if we have ru1 inference

process other than

random worlds? As long as it also proceeds by generating
a prior on a set of possible worlds and then conditioning,
we can deal with at least tl1i s examp le.

We simply use the
ao;;sign relative weights to
the worlds in the sets determined hy Red(b) and -.Red( b),
and then scale these weights wi thin each set so that the sets
are assigned probabi l i ty 0.8 ru1d 0.2 respectively (Readers
familiar with Jeffrey s rule [Jer92] will realize that this is es
sentially an application of that rule.) Again, intuitively, we
are considering the distribution closest to Ute original prior
that gives the set of worlds satisfying Red(b) probabi l ity
prior generated by the method to

.

­

'

0.8.
Unfortunately, the knowledge bao;;e is rarely this simple.
Our degrees of belief often place co mple x constraints on
the probability distribution

over possihle worlds. Never­

theless, we would like to maintain the intuition that we are
considering the distribution closest to the original prior
"

"

that satisfies the constnunts imposed by the KB. But how
do we determine the "closest" distribution? One way is

0.8

x

+

more likely than the worlds where b is not red. Nevenheless,

there is a straightforward way of incorporating this informa­

=

BBbird)
I(Cardinal(b)JKBbirdi\Red(b))
0.2 x I(Cardinal(b)JKBbird A -.Red( b)).

More

generally, we might hope that given an inference
and a knowledge bao;;e of t11e form KB A BB,
we can gen erate from i t a collection of objective knowl­
edge hase� KD i, .. , KB,. suc h tlmr I( 'PIKB /\ BB) is a
weighted average of I(;pJKBI), ..., I(lf'IKBm). as in the
example. In general. ho wever achieving this in a reasonable
fas h ion is not so easy. Consider t11e belief base BB�;rd =
(Pr(Red(b)) = 0.8) 1\ (Pr(.'inw/!(b)) = 0.6). In this case,
we would like to d efi ne 1 (Cardinal( b )JKBbird 1\ BB�;rd) us­
ing a weighted average of I(Cardinal(b)JKBbird /\Red(b) 1\
Snwll(b)), I(Cardinal(b)IKBbint 1\ Red(b) 1\ --,Smnll(b)),
etc. As in the simple example, it seems reasonable to take
the we i ght of the tenn l(Cardinal(b)JKBbird 1\ Red(b) 1\
Small( b)) to he the degree of belief in Red( b) 1\ Smnll(b).
Unfortunately, while ss;,;rd tells us t11e degree of belief in
Red( /1) and Small( b) separately, it does not give us a degree
of hel ief for their conjunction. A superficially plausible
heu ri sti c would he to ao;; sume that Red( b) and Smnll(b) are
independent. and thus assi gn degree of belief 0.8 x 0.6 to
their conjunction. While this seems reasonable in this case,
at other times it is completely inapp rop riate For example,
if our knowledge base asser ts that all small things are red,
then Red( b) and Snwll( b) cannot be independent, and we
should dearly take the degree of belief in Red(b) 1\ Smnll( b)
to he the same a� the degree of belief in Small( b), namely,
0.6. In general, our new degre e of belief for the formula
Red(b) 1\ Small(b) may depend not only on the new de­
grees of be lief for the two conjuncts, but also on our old
process I

.

,

.

degree of belief

!(Red( b)

1\

Small(b )JKBb;rd).

One reason­

to computing t11ese degrees of belief is to
make the smaflesr c hange possible to achieve consistency
with the hclief hase. Here, as before, cross-entropy is a
u seful tool. Indeed, a� we shall show, there is a way of
able approach

40

Bacchus, Grove, Halpern, and Koller

applying cross-entropy in this context to gi ve us a general
approach. We call Ibis method CEF, for cross-entropy on
formulas. Altbough both CEW and CEF use cross-entropy,
they use it in conceptually d i fferen t ways. As the names
suggest, CEW uses cross-entropy to compare two probabil­
ity distributions over possible worlds, while CEF uses it to
compare two probability distributions over formulas. On
the other hand, any probability distribution on worlds gen­
erates a probability distribution on formulas in th e obvious
way (tbe probability of a formula is the p rob abil i ty of the set
of worlds where it is true), and so we can use a well-known
property of the cross-entropy function to observe that the
two approaches are in fact equiv ale nt when they can hmh

be applied.

It is worth noting that the two approach es are actua.lly in­
comparable in their scope of application. Because CEF is
not restricted to inference processes tlJat generate a pri or
probability on a set of possible worlds, it can be app lied
to more inference processes than CEW. On the other hand,
CEW is applicable to arbitrary KB's while, as we shall see,
for CEF to apply we need to make mo re restrictions on the
form of the KB.

the metho ds also agree when appli ed to our version of the
ME process and when applied to random worlds. Putting
t11e results together, we can show that all these metbods­
CEW, CEF, and RS-agree when applied to random worlds
and in fact, CEW and CEF agree in general. In addition,
,

t11e r esu lt ing extension of random worlds agrees with the
approach obtained when we apply CEF and RS to the ME

process.

The rest of this paper is organi zed as follows. In the next

sect i on we r evi ew the formal model of [Ha190] for degrees

of belief and statistical information, and some material from
[13GHK93] regarding the random-worlds method. We give
the formal definitions of t11e three metbods we consider in
S ecti on 3, and discuss their equivalence. In passing, we
also d i sc uss the conn ecti on to Jeffrey's rule, which is an­
other very well known met11od of updating by uncertain
infonnation. We conclude in Section 4 witb some discus­
sion or computational issues and possible generalizations
of lhesc approaches.

2

Technical preliminaries

In Ibis paper, we focus on two instantiations of CEF. The
first applies it to the random-worlds method. The seco n d

2.1

used by Paris and Venco vska [PV89] ( an d simililr in spirit to
the metbod used by Jaeger [Jae94b)). which we henceforth
call the ME (inference) process. Using results of [GHK92,
PV89], we prove that t11ese two instm ll ia l ions are equivalent.

and reaso n with hoth statistical information and degrees of
belief. We hriclly review the relevant mmerial here. We
sta rt with a standard first-order la ngu a ge over a finite vo­
cabulary <�, and a u gment it with proportion expressions
and belief expressions. A basic proportion expression has
the form 1!1/•(;r)!O(J:)IIx and denotes the proportion of do­

applies it to a variant of the maximum-entropy approach

The third method we consider also app l i es only to certain
types of in ference processes. In panicu1ar, it take� a.<. its
basic intuition that all degrees of belief must ultimately he
the result of some statistical process. Hence, it re q u ires an
inference process tlmt cm1 gene rate degrees of belief from
statistics, like random-worlds. S uppo se we have the belief
Pr(Red(b)) = 0.8. If we view th is belief as havin g arisen
from some statistical sampling process, then we can regard
it as an abbreviation for statistical information ahout the
class of individuals who are '�just li ke b". For example, say
that we get only a quick glm1ce at b, so we are not certain
it is red. The above asser ti on could be con strued as being
an abbreviated way of saying that 80% of the objects that
give a similar sense perception are red. To capture this
idea formally we can view b as a membe r of a small set
of (possibly fictional) ind ividual s S that are "'just like b" to

the best of our knowledge, and ass u me that our degrees of
belief about b actually represents the statistical information
aboutS: IIRed(x)IS(x)llr � 0.8. Once all degree ofhelief
assertions have been convert e d into statistical assertions.
we can tben apply any method for in fer r in g degn:es of
belief from statistical knowledge bases. We call this the
RS method (for representative set). The general intuition
for this metbod goes back to s tatistical mechanics [Lan80J.
It was also defined (independently it seems) by Paris and
Vencovska [PV 92); we follow their presentation here.
Paris and Vencovska showed t11 a t the RS method and the
CEF metbod agree when app li ed to their version of the ME
process. Using results of[GHK92, PV89], we can show that

A fit·st-ol'<ler

logic of probahility

In [Hal90), a l o g ic is presente d that allows us to represent

main elements s atis fyin g 1j; from among those elements
satisfyin g B. (We take 111/;(.1:)11., to be an abbreviation for
11�1·( ;r) lmw(.r )11., . ) On the other hm1d, a basic beliefexpres­
.

.l.ion has

the form

Pr( 1/J IO) m1d denotes tbe agent's degree
0. The set of p roport ion (resp. belief)

or belief in 11• g i ven

expressions is formed by ad din g the rational numbers to the
set of basic proportion (resp. belief) expressions and then
closing off und er addition m1d mu lt ip lica tio n
.

We compm·e two pro por ti on expression s using the approx­

( ap pro xim at ely less than or equal");
the result is a proportion formu l a. We use { � {' as an
abbreviation for ({ ::5 e) 1\ ({' ::5 0· Thus, for example,
we can express the stateme nt "90% of birds fly" using the
proportion formula IIFly(x)IBird(x)llx � 0.9.4 We com­
pa re two belief expressions using standard �; the result is
a basic belief{ormula. For example, Pr(Re d(b)) � 0.8 is a
ba�ic belief form ula . (Of course Pr(Red(b))
0.8 can be
expressed as the o hv io us c onjunct ion ) In the full language
£ we allow arbitrary llrst-order qu ant i fication and nesting
of belief and proportion formula'>. For example, complex
imate connective ::5

"

,

=

.

fonnulas like

in£.

Pr{V:t(I!Knmvs(:r., y)IIY ::5

0.3 ))

� 0.5

are

4We remark that in [Hal90] there was no use of approximate
equality

(�).

We use it here since, as argued in [BGHK93], its

use is crucial in our intended applications. On the other hand, in

[DGI !K93], we used a whole family of approxima te equality func­

tions of the form�;. i
we use only one here.

=

1, 2, 3, . .

..

To simplify the presentation,

Generating New Beliefs from Old

We will also be interested in various sublanguages of £ . A
fonn ul a in which the "Pr" operator does not appear is m1
objective formula. Such fonnulas are assigned truth values
by single worlds. The sublanguage restricted to objective
fonnulas is den oted by c obj . The standard rand om- worl ds
method is restricted to knowledge bases expressed in cobj .
l
The set of belief formulas, c oe is fanned by starting with
,
basic belief formulas and closing off under conju nction ,
n eg ati on , and first-order quantification. In contra<;t to ob­
j ective fonnulas, the truth value of a be l ief fonnula is com­
pletely i ndepen dent of the world where it is evaluat ed . A
flat formula is a B ool ean combination of belief formulas,
such that in each belief expression Pr( 'P ) , the formula 'P is a
closed (i.e., con taining no free variables) obj ective form u la.
(Hence we have no nesti nr, of "Pr" in nat formulas nor any
"quantifying in".) Let £! at be the language consisting of
the fiat fonnulas.
give semantics to botl1 proportion form u l a-; and belief
formulas, we use a special case of what were called in
[Hal90] type-3 structures. In part i cu l ar, we consider type-:�
structures of the fonn (WN , p.), where WN co n si s t s of al l
worlds (first-order models) with domain { I , . . . , A' } over
the vocabu l ary <ll , and Jl is a probabi l i t y distribution over
WN .5 Given a structure and a world i n that structure. we
eval uate a proportion expression 1 1 1/•( :�: ) IO ( .r ) I I .,· as the frac­
tion of domain el emen ts sat isfying 1,1>( ;I' ) among those sat­
isfying O(x ) . We evaluate a belief form u l a us i n g our proha­
bi1ity distribution over tl1e set of possib l e worlds. More pre­
cisely, given a structure M = (WN , 11), a world w E WN ,
a tolerance r E (0, I] (u sed to i n te rpre t ::::::: and ::S ) , and a
val u ati on V (used to i n terpret the free variables), we asso­
ciate with each fonnula a tru tll value and with each belief
expression or proportion expression ( a numher [(j M , ,v, r .
We give a few representative clauses here:

To

w

•

•

If ( is tlle proporti on expression I I \0( .1:) I �{1:) I I ,. , then
[(]M,w , v , r is the num ber of domai n elements i n w sat i s­
fy i ng \0 1\ 1/; d i v i de d hy the numher sat i s fyi n g ''' · (Note
that these n u m bers may depend on w . ) We take this
fraction to be 1 if n o domain elements sat i s fies ·� · .
I f ( is

the belief expression Pr( 'P I I/' ) , then
_

[(] M,w ,V,r Again,
•

p { tv1 : ( M , w' , V, r ) )= 'P 1\ !}• }
p { w' : ( M, w1 , V, r ) )= 1}1}

·

we take this to he 1 i f the denominator is 0.

( m1d ( ' are two proportion
( M , w , T , V) I= ( ::S (' iff

If

expressions.

[(]M,w , r , V :S [(1] M , w , r ,V

+

th e n

T.

That is, approximate less than or eq ual allows a tolerance
of r .

( is a belief expression, th en i t s value i s
in dependen t of the world w. Moreover, i f it is cl osed then
its value is in dep enden t of the val u a t i on V. Thus, we can
write [(]M,, in tllis ca..o;e. Similarly, if <p E £11'1 is a cl o sed

Notice that if

5

In general, type-3 structures

addition a lly allow for a distribu­
{ l , . . . , N} ) . Here, we always

tion over the domain (in this case,

use the unifonn d isui b u ti on over the doma in.

belief form u l a,

its truth depends only on M and r,
)= cp i n this ca<;e.

can write ( M, r )
2.2

41

so we

The random-worlds method

Given these semantics, tlle random-worlds method is now
easy to describe. S uppose we have a KB of objective for­
mula�. and we wrull to a<;sign a degree of belief to a fonnula
tp . Let p'f.v be the uniform distribution over WN , and let
w
M!V = (WN , Jlf.r ). Let Pr»r (cpi K B) = [Pr(cpjKB)]MN , r ·
Typically, we know on l y tl1at N i s large and that r is small.
Hence. we approximate the value for the true N and r by
ddining

P1-: ( 10lKB)

=

lim lim Pr»rw(cpjKB),
N -+-CXJ

-r - 0

a-;suming the limit exists. P1-: ( cpiKB) is the degree ofbelief
in 'P given KI3 acc ord i ng to the random-worlds method.
2.3

Maxi m u m entl'Opy and cross-entropy

The entropy or a probab i l i t y d is tribu ti on Jl over a finite space

n

Lw E n /l ( w ) ln(,l(w ) ) . It ha� been argued [Jay78]
the amou n t of "information" i n a
probabi l i t y di stri b u t i on, i n t h e sense of information tlleory.
The uniform distri bution has the maximum possible en­
t rop y. I n general, g i ven som e constraints on the probability
is -

that entropy measures

with maximum entropy that
viewed as the one that i ncor­
i nfonnation above and beyond

distributions, the distri b u t i o n

satisfies t h e c onst rai n ts cllil be
porates the least addi t i onal
the constrai nts.

The related cross-entropy function measures the additional
information gained by mov i n g from one di stribu ti on Jl to
another uistri hu t ion Jl1:
( ' ( I' , , Jt )
.. ·

=

'\"" '
tl (w)
)
-.
L 1'· ( w In p (w )

wEn

Various arguments have been pre sent ed showing that cross­
meas u res how close one probability distribution is
to ano t he r [S.TRO, S ho86) . Thus, given a prior distribution
I' and a set S' of add i tio nal constraints, we are typically
i n t ere st c u i n th e unique distribution 1/ tllat satisfies S and
minimizes C(p' , p ) . It is well -known tlmt a su fficient con­
dition for such a u n i q u e distribution to exist is that the set
of di stribu tions sat i sfyi n g S form a convex set, and that
t here be at le<L<;t one d i stribution p" sati sfying S such that
C(IJ'' , I ' ) is fi n i te . These conditions often h old in practice.

entropy

3
3.1

The three methods
CEW

As w e ment ioned in

the introduction, our first metllod,

CEW, assumes as input an inference process I that proceeds
by generati ng a pri or Jl f on a se t of possible worlds W1 and
t h e n conditioning on th e objective i n formati on . Given such
an inference process I , a k n owl ed ge base KB (that can con­
tain subjective infonnation) and an objective formula cp, we
wish t o co m p u te CEIV ( I)(<piKB), where CEW(I) i s a

42

Bacchus, Grove, Halpern, and Koller

new degree of belief generator that can handle knowledge
bases that can include subjective infmmation.

We say that an inference process I is world-based if there is
some structure M1 = (Wr , llt )and a tolerance r such that

l(�PIKB) = [Pr(lf'IKB)]M 1,T . Notice that Pr;_;rw is world­
based for each N (where the structure corresponding to
Pr;_;rw is MjV ). �, on the other hand, is not world-ba<;ed;
we return to this point shortly.

Given a world-based inference process I, we define
as follows: Given a knowledge base KB which
can be an arbitrary formula in the full language .C, let Jt.}8
be the probability distribution o n WI su ch t11at C(Jl}8, JlJ )
is minimized (if a unique such distribution exists) among all
distributions tl such that (WI , p' , T ) I= Pr(KB) = l. Intu­
itively, pf8 is the probability distribution closest to t he prior
Jli that gives KB probability l . Let Mf8 = ( WI , JLf8). We
can then define CEW(I)( c.oiKB ) = [Pr( cp)J.�1Ks ' T .

CEW(I)

I

The first thing to observe is that i f KB is objective, then
standard properties of cross-entropy can he used to show
that pf8 is t11e condilional distri b u t i o n I'· I C l KI3 ) . We thus
immediately get:
Proposition 3.1: lfKB is objective, then CEW( I ) ( <.p j K I3 )
l(lf' I KB ) . Thus, CEW(I) is a true extension of !.

=

Another important property of CEW fo llow s from the wel l ­
known fact that cross-entropy generalizes Jeffrey 's rule
[Jef92]. Standard probability theory tells us that if we
start with a probability function p an d observe that event
E holds, we should update to the conditional probabi lity
function Jl. ( · I E). Jeffrey's rule is mean t to ueal w i th the
possibility that rather than get ting certain i n formation. we
only get partial information, such a<; that E hold s with proh­
ability o:. Jeffrey's rule suggests that in tl1 i s case. we should
update to the probability function p' such that

tt'(A)

=

o:JI.( A I E ) + ( I - rr )p ( A IE) .

where E denotes the complement of E. 111 i s rul e uniformly
rescales the probabilities within E and (separately) those
within E so as to satisfy the constrain t Pr( E) = a. Clearly,
if o: = 1 , then Jl1 is j ust the conditional prohab i l i t y p( · I E ) .
This rule can be generalized i n a strai ght forward fashion .
If we are given a family of mutually ex c l u si ve and ex­
haustive events £1 ,
, Ek with de si red new prohahi l i t ies
0: 1 , . .
, ak (necessarily L:i c.r; = 1 ) , t h en we can define:
.

.

•

.

p'(A)

=

a , lt(A I El )

+

·

·

·

+

ni\ I'·( A I Ek) .

Suppose our knowledge base ha�; t he form ( Pr( 'PI ) = n 1 ) A
· · · A (Pr( lf'k ) = a k ) , where the If'; 's are mu tually exclusive
and exhaustive objective fonnulas and c.r 1 + · · + cr., = I .
The formulas lf' l , . . . , lf't.: conespond to mu tually ex clu sive
and exhaustive events. Tilus, Jeffrey's rule would suggest
that to compute the degree of belief in If' g i ven this knowl­
edge base, we should compute tile degree of belief in 'P
given each of the If'; separately, and then take th e linear
combination. Using t11e fact Umt cross-entropy generalizes
Jeffrey's rule, it is immediate that CEW i n fact docs t h i s .
·

Pr·oposition 3.2: Suppose that I is a world-based inference
process and that KB ' is of the form KB A BB, where KB
is objective and B B has the form (Pr( 1;? 1 ) = o: t ) A · A
(Pr( lf'k ) = a t.: ) , where the lf'i 's are mutually exclusive and
exhaustive objective formulas and cq + · · · + o:k = 1. Then
·

·

k

CEW( I )(c,oi KB ' )

=

L o:; I ( If'I KB 1\ cp; ) .
i=l

As we observed above, CEW as stated does not apply di­
rectly t o U1e random-worlds method Pr:. since it is not
wor l d - based . It is, however, the limit of world-based meth­
ods . (This is also true for the ot11er methods considered in
[BGH K92].) We can ea<;ily extend CEW so that it applies
to limits of world-ba<;ed methods by taking limits in the
obvious way. In particular, we define
CEW( Pr':;;;; H 'f' I K B )

=

l i m lim
r - 0 N - oo

CEW(Pr;,;rw)(�PIKB ) ,

provided t he limit exists. For convenience, we abbreviate
CEW ( Pr:; ) as Pr�w .

note that the distribution defined by
d i s tri bu t i on of maximum entropy that
sati sfies the constraint Pr( KB) = l . 1l1is follows from the
observation that the distri bution that minimizes the cross­
entropy from the u n i form distribution among those distri­
butions sa t i s fy i ng some constraints S', is exactly the distri­
bution or max i m um entropy satisfying 5.6 This maximum­
entropy characterization demonstrates Umt Pr�w extends
ran dom worlds by making the probabilities of the possible
worlds "<t<; equal as possi b l e" given the constraints.
It is i n teresting to

CEW( Prrt\� )

3.2

is the

CEF

Paris and Vencovska [PV89] consider inferences processes
that arc n o t world-based, so CEW cannot be applied to
them. The method CEF we now define applies to arbitrary

but requ i res that the knowledge base
form. For the remainder of this section,
we assume that the knowledge ba..;e has the fonn KB 1\ BB,
where KB i s an objective formula and B B (which we call
the hclicr base) is in £11" ' .
inference processes.
be or a re st ri c t ed

BB is of UJe form Pr( th ) =
U1e t/J; 's were mutually exclu­
sive, then we could define C E F ( I)( lf'IBB) so that Propo­
si t io n 3 . 2 he l d . B u t what i f the �;; 's are not mutually exclu­

First. suppose for simplicity that

d1

!\

·

·

·

A

Pr( V-'d

=

Pk · If

sive?

2 !' atoms over �!J ,

, 1/Jt.:. i .e., those
where each 1/J: is
ei t her ·rJ>; or · ·I/J1 • Atoms are always mutually exclusive
and ex h au s t i ve ; so, if we could find appropriate degrees
of helier for these at om s, we could again define things so
that Propo s i t i o n 3.2 holds. A simple way of doing this
Consider the r\·

conj u nct ions o r

=

the form '1/•;

1\

. . .

. . . A l/JI, ,

6We remark that in [GHK92, PV89] a connection was es­
lahlished between random worlds and maximum entropy. Here
maximum entropy is playing a different role. It is being used here
to extend rnndom worlds rather than to characterize properties of
random worlds as in [GHK92, PV89).

Generating New Beliefs from Old

would be to assume that, after conditioning, the a<>sertions
'if;; are independent. But, as we observed in the i ntroduction,
assuming independence is inappropriate in general .
Our solution is to first employ cross-en trop y to find appro­
priate probabi l i ti es for these atoms. We proceed as follows.
Suppose I is an arbitrary i n feren c e process, 1313 E C f1a t ,
and 1/J1 , . . . , 1/Jk are the formulas that appear in subexpres­
sions of the form Pr( 'if;) in B B . We form the K = 2k at oms
generated by the 'if;; , de no ting them by A 1 , . . . , A K . Con ­
sider the probab i l ity Jl defin ed on the space of atoms via
t-t(Aj ) = I(Aj [ KB ) .7 There is an obvious way of defining
whether the formula B B is satisfied by a probability distri­
bution on the atoms A 1 , . . . , Ak (we defer t11e formal details
to the full paper), but i n gen eral B B will not be sat isfied by
the distribu tion 11. For a si mpl e example, if we take the
in ference procedure to be random worlds and consider the
knowledge base KBb;nJA(Pr(Red(b)) = 0 . 8 ) from the intro­
duction, it turns out that P1-: ( Red ( b ) [ K B ,;,-d ) is around 0 . 5 7 .
Clearly, the di stri bu tio n 11 such that p.(Red( b ) ) i s around
0.57 does not satisfy t11e constraint Pr(Red( b ) ) = 0 . 8 . Let
Jl' be the probability d i stribution over the atoms that mini­
mizes cross-entropy relative to p among those t hat satisfy
BB, provided tl1ere is a unique such distribution. We then
define

CEF(I) ( <p[KB A B B )

=

1-" ( A l )I( 'P [ KB A A ! )

+ · · ·+

JJ.' ( A K ) ! ( <p [ KB A A g )

It is immediate from the definition that CEF( I )
Formally, w e h ave
Proposition 3.3:

I('P [ KB).

lf KB, 'P

E

_

extends

/.

c�<j , then CEF( / ) ( ;p [ K B )

=

Both CEW and CEF use cross-entropy. However, t h e two
applicatio ns are quite different. In the cm;e of CEW, we
apply cro ss- en trop y witl1 respect to probabi lity di s t ri bu ­
tions over pos si ble worlds, whereas with CEE we apply i t
to probabi lity distributions over formulas. Nevert hel e ss , as
we mentioned in the i n trod u c ti o n , there is a tight connection

between the approaches, since any probahi l i t y distri bution
over worlds defines a proba bi l i t y distribution over fonn u ­
las. In fact the following eq u i val e n c e can be proved, usi n g
simple properties of tJJe cross-emropy fu nct i on.
Theorem 3.4 :
Suppose I i s a world-based i�[erence
process, KB , <p E co"i , and B B E [fl•< � .
Then
CEW(I)(�P I KB A B B ) = CEF( / ) ( 'P [ KB A B B ).

Thus, CEF and CEW agree
defined.

i n contexts w h ere both are

By analogy to tl1e defin iti on for CEW, we define
Pr��' (cp!KB A BB)

=

lim

lim

r-O N - ro

CEF( Pr�rw ) ( <p [ KB A BB ) .

It immediately follows from Theorem 3 .4 that
7

S ince

BB

Pr cunnot be nested in
1/J'S are necessa1ily objective, an d �o are the

E c,Jlat by assumption, an d

a flat b e lief base, the

atoms they generate. Thus,

l( A1 [KB)

is well defined.

43

Corollary 3.5: {[ KB , IP E Co"i , and BB E [flat, then

Pr�w ( 'P [ K B A B B )

=

Pr;;r(�t?[KB A BB).

As the no t at i on suggests, we view
of rr: obt ai n ed by applying CEF.

Pr�F as the extension

Why did we not define
Pr�F as CEF(Pr: )? Cl earl y CEF(rr,:; ) and Pr�" are
closely related. Indeed, if both are defined, then they are
equal .

ff both CEF(Pr: )(<piKB A BB) and
Pr�r ( 'P I KB A B B ) are defined then they are equal.

Theon:m 3.6 :

It is q u i te p o ssi ble , in ge n e ral , that eitller one of Pr�" and
defmed while the other is not. The fo l l owin g

CEF( Pr� ) is

example demonstrates one type of situation where Pr�"
is dell ned and CEF( PrC: ) is no t. The converse situation
typical ly ru·ises only in pathological examples. In fact, as
we show in Theorem 3 .8, there is an i mportan t class of cases
where the existence of CEF(Pr:; ) g u arant ees that of Pr�F.
Suppose KB is [[Fly( x ) [Bird(x)[[ r �
I A Rird ( Tweety) and B B is Pr(Fly(Tweety) = 0) A
Pr(Red( Tweety) = 1 ) . Then , just as we would expect,
Pr�,"'(Red( lll'eet v) [ KB 1\ BB) = 1 . On the other hand,
CEF( P1�.,: ) (Red( 1\veety ) [ KB A BB) is undefined. To see
why, let ;t be the probabi lity distribution on the four atoms
ucfineu by Fly( 7\veery) and Red ( Tweety) determined by
Pts-:;:: ( · I K B ) . S i nce Pts� ( Fly( Tweety)IKB) = I , it must
he t he case that 11( Fly( Tweety) ) = 1 (or, more accu­
rately, ; t ( Fiy(7iveety) A Red( Tweety ) ) + p(Fly(Tweety) A
-.Red( liveetv) ) = l ) . On th e other hand, any distri­
but ion 1/ over the four atoms defined by Fly(Tweety)
and Red( 7iveety) that sati s fi es BB mu st be s uch that
it' ( Fiy( 7iveety ) ) = 0. It ea<;ily fo l low s that if p' sat­
isfies B B , then C ( p ' , I') = oo .
Thus, there is not a
unique u i st ri h u t i o n over the atom s that satisfies BB and
mi n i m i zes cross-entropy relative to p . This means that
CEI,.( Prr;: ) ( Red( 7\veety ) I KB 1\ B B ) i s undefined. I
Example 3.7 :

We nex t consider what happen s

when we instantiate CEF
process considered by Paris and
VL:ncovska that u ses max i mum e nt rop y [PV89]. Paris and
Vcncovsk a restrict at t e n t i on to rather simple lang u ag es , cor­
respond ing to the notion of " ess e n tial l y propositional" for­
m u l as de f i n ed below. When co ns i dering (our variant) of
the i r method we shal l make the same restriction.
w i th a p art i c ul a r inference

We say that 1/• ( :r ) is an essentially propositional formula
if it i s a quantifier-free fi rst - ord e r formula that mentions
only u nary predicates (and no con stan t or function sym­
bols), whose onl y free variable is x . A simple kno wl ­
edge base K B abollt c ha<> t he form ll'Pt ( x ) IO, ( x ) ll r ::S
o ,

1\

.,_., , , .

. .

. .

- A [ [ <p k ( :r ) [ fh ( x ) [ [ x ::S O:k 1\ 1/;(c), where
. . . , Ok , .,_;, are all essen t ial ly propositional.8

, 'P k , O , ,

The M E i n fe re nce process

is only defined for

a

simple

M Nolice that II'P( x ) fB ( 1: )f[.
t o- is expressible as
11-.'P{-" ) [ B ( :r: ) f l x :::: I - a ; th is means we can also express :::::: .
I Iowcvcr. because of the fact that we disallow negations in a sim­
ple KB . we cannot ex press s trict i ne qu al i ty. This is an important

rcstric lion.

44

Bacchus, Grove, Halpern, and Koller

knowledge base about c and an essen t i al l y propositional
query <p( c) about c. Let KB :::: KB' 1\ ¢ ( c ) be �m essen t ial l y
propositional knowledge base abou t c (where KB' is the
part of the knowledge base that does not mention c). I f the
unary predicates that appear in KB are P = { Pt , . . . , Pk } ,
then KB' can be viewed as pu tti ng constraints on the 21:
atoms over P .9 The fonn of KB' en s ures that U1ere will
be a unique distribution llme over Ulese atoms Ulat maxi­
mizes entropy and sati sfies the constraints. We then de­
fine ME( <p (c) I KB' 1\ Jj;(c)) to be P me ( lf' l 1/! ) . I n t u i t ively, we
are choosing the d istribution of maximum entropy over the
atoms that satisfies KB' , and treating c a<; a "nmdom" ele ­
ment of Ule domain, assuming i t satisfies each at om over P
wiili Ule probabil i ty dictated by Jlme ·
To apply CEF to ME, we also need to put restrictions
on tlle belief base. We say that D B E .c Jra t is an es·

sentially propositional belief base about c i f every basic
proportion expressi on has the fo nn Pr( y( c ) I O ( c ) ) , w here
<p and B are essen t ial ly p roposi t ional (In p art icu lar, this
.

disallows statistical fotmul a<;
simp le belief base about c is

Pr(<pt (c)IBt (c)) ::;

O'J

in the

a

scope
conjunction

1\ · · · Pr( <pl: ( c) I Ok { c) )

of

Pr.)

o f the

::;

ok.

A

form

w h ere

all of tlle fonnulas th at appear are essential l y proposi tional.
We can only apply CEF to ME if the knowledge ba�e has
the fonn KB 1\ BB, where KB is a simple knowledge hase
about c and B B i s a simp l e belief ba�e abou t c. It fo l lows
from results of [GHK92, PV89] that random worlds and
ME give the same results on th ei r common domain. Hence,
they are also equal after we appl y tl1e CEF transformation.
Moreover, on Ulis domain, if CEF(Pr� ) is de fi ned , then
so is Pr�F. (The con verse docs not hold, as shown hy
Example 3 .7.) Thus, we get
Theorem 3.8: If KB is a simple knowledge base about (
BB is a simple belief base about c, and '-P is on essenTially
propositionalformu!a, then
",

CEF(ME)(<p(c) I KB A B B ) :::: CEF( Prr,.: ) ( '-P ( c ) I K D A I3 I3 ) .
Moreover, ifCEF(ME)( <p ( c ) I KB
CEF(ME) (<p(c) IKB A BB)

=

A

B B ) is defined, then

Pr�F('-P( c ) I KB A B B ) .

3.3 RS
The last method we consider, RS, i s based on t he intu­
ition that degree of belief a�sertions must u l timately ari se
from statistical statements. This general idea goes hack to
work in Ule field of statistical mechanics [LanRO), whe re
it has been applied to the problem of reasoning abou t the
total energy of physical systems. If the system consi sts of
many particles then what is, in essence, a random-worlds
analysis can be appropriate. If the energy of the system is
known exactly no conceptual problem arises: some possibl e
configurations have the specified energy, while others are
impossible because they do not. H owever it turns out that i t
is frequently more appropriate to assume that all we know i s
the expected energy. Unfortunately, i t questionable whether
,

9 An

mulas

atom over P is an atom (as de fi ned ahove ) over the for­

P1 ( x ) . . . . , Pk ( x ) .

this i s real l y an "objective" a-;sertion about th e system in
que.� tion , 1 0 and in fact the physicist<; encounter a problem

analogous to that which motivated our paper. Like us, one
response they have con si dered is to modify the assump­
t i o n of unifonn probability and move to maximum entropy
(thus u si n g essent i al l y, an in st ance of our CEW applied to
a uniform prior). But another response is the fol lowing.
Ph ysi cal l y, expected energy is appropriate for systems in
th ermal equilibrium (i e , at a con stant temperature). But
in practice this means Ulat Ule s yste m is in Ulennal contact
with a (general l y much larger) system, sometimes called a
heat bath. So another approach is to model the system of
i n teres t a> being part of a much larger system, including
the h e a t bath, whose total energy is truly fixed. On Ulis
l arger scale, random-worlds is on ce again applicable. B y
ch oosi ng the energy for the total system appropriately, Ule
ex pec t ed energy of the sm all subsystem will be as speci­
fied . Hence. we have converted s u bjective statements into
objective ones, so that we are able to u se our standard tech­
niques. In this domain, there is a clear physical intuition
for the con nec t i on between t11e objective infonnation (Ule
,

.

energy of the

exp ec t ed

.

heat bat h ) and the subjective infonnation (the

energy of the

small system).

A more recent, and q u i t e

di fferent, appearance of Ulis intu­

ition is in t he work of Paris and Vencovska [PV92) . They
dell ned t h ei r method so that i t ha<> the same restricted scope

ME method. We presen t a more general version
here, that can handle a somewhat richer set of knowledge

as th e

bases. al though i t s scope

is still more restricted than CEF. It

can deal with arbitrary inference proces ses,

but Ule knowl­
BE, where KB is
o bj ec ti ve and D B is an essentially p ro po s i ti onal belief base
ah o u t some co n s tant c . The first step in the method is to
transform 13 I3 in t o an objective formula. Let S be a new
unary predicate, representing the set of individuals ·�ust
l i ke c" . We transform BD to KBnn by replacing all tenus
of the form Pr( 1/J( c)ID(c)) by 1 1 1/! ( x ) IB(x) 1\ S(x)ll:t-. and
rep l aci n g all occmTences of ::; by � . We then add Ule
cnnj u ncts I I S( :r l l l, ::::: 0 <md S(c), s ince S is assumed
For example,
to he a small set and c must be in S.
if D B is Pr(Red(c)) ::; 0.8 A Pr(Small(c)) :::: 0.6, then
the nmcsponding KBnn is ( I I Red(x) I S(x)llx � 0.8) 1\
( /ISmu//( :r J IS(:r ) ll, � 0.6) 1\ ( / I S(.z: ) l /�· ::::: 0) I\ S(c). We
then define R S ( ! ) ( <p( c ) IKB A B B ) = I( IP ( c) I KB A KB aa ) .
I t is al most imme d ia t e from the d efin i ti ons that if B B is
a si mple belief base ahout c , then RS(Pr,: )(<p(c) I KB A
D B ) = li mr o limN - = RS(Pr;r)(<piKB). We abbrevi­
a t e R S ( Prr;;: ) as Pr� .
e d g e hase must have the form

KB

1\

-

RS and

CEF are distinct.

This observa­
of [PV92) concerning an infer­
ence process CM, show i ng that RS(CM) cannot be equal
to CEF( CM ) . On th e other hand, Uley show Ulat, in
the restricted setting in which ME applies, RS(ME)
CEF( ME ) . S i nce ME = Pt� in this se t t ing we have:
In

general,

tion fol l ows from results

,

if'lf it is objective. it is most plausibly a statement about the
average energy over time. While this is a reasonable viewpoint, it

does not rea l l y escape from philosophical or technical problems

either.

Generating New Beliefs from Old

Theorem 3.9: If KB

is a simple knowledge base about

c,

tiona! Conference on Artificial Intelligence
(AAA! '92), pages 602-608, 1 992.

B B is an essentially propositional knowledge base about c,
and tf; is an essentially propositional formula, then

CEF(rc:)(<p(c) IKB /\ B B ) "" CEF(ME)(<p(c) ! KB II B B )
= RS(ME)(<p(c) ! KB II B B ) = Pr� (<p(c) I KB II B B ) .
4

Discussion

A . .T. Grove, J . Y. Halpern, and
D. Koller. Statistical foundations for default
rea<;oning. In Proc. Thirteenth International
Joint Conference on Artificial Intelligence (!J­
CA! '93), 1 993.

[BGHK93] F. B acchus ,

[Car50]

that they can deal with degrees of belief. We

view the fact that the three methods essentially agree when

[GHK92]

�Uld RS a;;su m e cert ain res t ri c ­
tions on the form of the knowl e d ge base, which are not
assumed in CEW. Is it possible to ex t en d these methods
so that they apply to more gen eral knowledge bas e s ? In

M . J aeger. A

350,

facts to KB

[Jac1J4b1

prevents

appl icatio n of

U1is idea

belief bases about some const an t
lems we have found trying to

that

prob­
do this seem dirtlcult but

be viewed as
U1ru1 seq ue ntial updat ing here. Sup­
pose our knowledge base co ntains two constraints:
Pr(<pt ) = u 1 1\ Pr(<p2 ) = o·2 · Although we cannot u s u ­
a l l y apply Jeffrey's rule to such a conj unction, we can a p ­
ply the rule seq uentially, first u pdating by Pr( <p1 )
and then by

Pr( <p2)

=

et 2 • We have

= n1 ,

described o u r meth­

ods in ilie context of u p d a t i ng by any set of constrai n t s
at once, but they can also b e defined

to update b y con­
straints one at a time. TI1e two possibilitie.� usual ly give
different results. Sequential updating may not p reserve
any but the last constraint used, and in general is order
dependent. Whether this should be seen

a-; a problem

doing

is wh y this issue can

be

ig nored when

B ayesian condi tionin g in general, and in ordinary

[ KL5 1 l

S . K u l lback and R . A. Leibler. On informa·
t i o n and su fficiency. Annals of Mathematical

bridge, 1992.

Swtistics, 22: 76-86, 195 1 .

[LanXO]

[BGHK92]

F. B acchus,

Physics, volume 1.

.1 . B . Paris and A. Yencovska. On the appli­
ca bi l i t y of max i m u m entropy to inexact rea­
son i n g . International Jo urn a l of Approximate

[PY02]

J.

[ S ho86 ]

J.

Reasoning, 3 : 1 -34, 1 989.

B. Paris and A . Vencovska. A method for up­
dat ing j u st i fy ing minimum cross entropy. In­
ternational Journal qf Approxi mate Reason·
ing, 1 -2: 1 - 1 8, 1 99 2.

E. Shore. R el ative entropy, probabilistic in­
ference, and AI. In L. N . Kanal and J . F. Lem·

mer, edi tors, Uncertainty in Artificial intelli­
gence. North -Holland, Amsterdam, 1 986.

[S.I80]



We propose a directed graphical representation of util­
ity functions, called UCP-networks, that combines as­
pects of two existing preference models: generalized
additive models and CP-networks. The network de­
composes a utility function into a number of additive
factors, with the directionality of the arcs reflecting
conditional dependence in the underlying (qualitative)
preference ordering under a ceteris paribus interpreta­
tion. The CP-semantics ensures that computing opti­
mization and dominance queries is very efficient. We
also demonstrate the value of this representation in de­
cision making. Finally, we describe an interactive elic­
itation procedure that takes advantage of the linear na­
ture of the constraints on "tradeoff weights" imposed
by a UCP-network.

1

Introduction

Effective representations for preferences and utility func­
tions are critical to the success of many AI applications.
A good preference or utility representation should capture
statements that are natural for users to assess, or are easy
to learn from data; it should offer the compact expression
of preferences or utilities; and it should support effective
inference.
·

A useful design stance for such representation is to exploit
the structure of utility functions using notions from multi­
attribute utility theory, such as conditional preferential in­
dependence, mutual utility independence, etc. [10]. Re­
cent work has exploited such structure to develop graphical
models: Bacchus and Grove [1, 2] propose an undirected
network representation for (quantitative) utility that cap­
tures conditional additive utility independencies; Boutilier,
Brafman, Hoos and Poole [3] propose a directed network
representation for (qualitative) preference functions that
captures conditional preference statements under a ceteris
paribus (all else equal) assumption. La Mura and Shoham
[II] describe a hybrid representation for combining both
probabilistic and utility information in a Wldirected graph­
ical model representing expected utilities directly.

In this paper, we propose a new directed network represen­
tation for utility functions that combines certain aspects of

the first two of these approaches. The UCP-network for­
malism can be viewed as an extension of the CP-network
model [3] that allows one to represent quantitative utility
information rather than simple preference orderings. The
formalism also utilizes the notion of generalized additive
independence (GAl) [1]. By employing a directed graph,
UCP-nets allow one to make more powerful statements that
are often more natural and lead to more effective infer­
ences. In particular, we will show that dominance and op­
timization queries can be answered directly in UCP-nets.
In addition, the formalism can be used in an interactive
elicitation process to determine relevant parameters of the
UCP model in a specific decision scenario. We propose a
technique for elicitation-much like that proposed by Cha­
jewska, Koller, and Parr [4]-that exploits the linear con­
straints imposed by a partially-specified UCP-model to de­
termine an "optimal" sequence of queries.
The rest of this paper is organized as follows: Section 2
provides necessary background. Section 3 describes UCP­
nets, their properties, and their relation to GAl decomposi­
tions of utility functions and CP-networks. Section 4 dis­
cusses the problem of optimization in the context of UCP­
nets, and shows the advantage of this representation tool.
Section 5 explains how elicitation and optimization can be
performed concurrently in order to recognize near-optimal
choices with minimal questioning. We conclude in Sec­
tion 6 with a discussion of future work.
2

Background Concepts

We begin with an outline of some relevant notions from
multiattribute utility theory [10]. We assume a set of ac­
tions A is available to a decision maker, each action hav­
ing one of a number of possible outcomes. The set of all
outcomes is designated 0. A preference ranking is a total
preorder t over the set of outcomes: o 1 t o2 means that
outcome 01 is equally or more preferred by the decision
maker than o2. A utility function is a bounded, real-valued
function u : 0 f-t R. A utility function u induces a pref­
erence ordering t such that 01 t 02 iffu(ot) � u(o2). A
utility function also induces preferences over lotteries, or
distributions over outcomes, where one lottery is preferred
to another when its expected utility is greater. When actions

BOUTILIER ET AL.

UAI 2001

57

have uncertain outcomes, thereby generating a distribution
over outcomes, preferences for actions can be equated with
preferences for the corresponding lotteries [12].
One difficulty encountered in eliciting, representing, and
reasoning with preferences and utilities is the size of
the outcome space, which is generally determined by a
set of variables. We assume a set of variables V
{X1, . . . , X n} characterizing possible outcomes. Each
variable X; h as domain D o m (X ; ) = {x i , ... ,x . } . The
set of outcomes is 0 = Dom(V)
Dom(X1) x · · · x
Dom(Xn)· Thus direct assessment of a preference func­
tion is generally infeasible due to the exponential size of
0. Fortunately, a preference function can be specified con­
cisely if it exhibits sufficient structure. We describe certain
standard types of structure here (see [10] for further de­
tails).
=

Figure 1: A CP-Network

�

=

We denote a particular assignment of values to a set X � V
as x, and the concatenation of two non-intersecting partial
assignments by xy. If XU Y
V, xy is a complete
outcome, and xy is a completion of the partial assignment
x. Comp(x) denotes the set of completions ofx.
=

A set of features X is preferentially independent of its com­
plementY = V- X iff, for all x1, xz, y1, yz, we have

X1 Y1 � X2Y1

iff

X1Y2 � XzYz

We denote this as PI(X, Y). In other words, the structure
of the preference relation over assignments to X, when all
other features are held fixed, is the same no matter what
values these other features take. If PI(X,Y) and X 1Y �
x2y for any assignment y to V- X, then we say that x1
is preferred to x 2 ceteris paribus. Thus, one can assess the
relative preferences over assignments to X once, knowing
these preferences do not change as other attributes vary. We
define conditional preferential independence analogously.
Let X, Y, and Z be nonempty sets that partition V. X
and Y are conditionally preferentially independent given
an assignment z to Z (denoted CPI(X, z,Y)) iff, for all
XI,Xz,YI,Y2, we have

In other words, the preferential independence of X andY
holds when Z is assigned z. If we have CPI(X, z,Y) for
all z E Dom(Z), then X andY are conditionally preferen­
tially independent given Z, denoted CPI(X, Z,Y).
Decomposability of a preference function often allows one
to identify the most preferred outcomes rather readily. Un­
fortunately, the ceteris paribus component of these defini­
tions means that the CPI statements are relatively weak.
In particular, they do not imply a stance on specific value
tradeoff's. For instance, suppose PI( A,B) and PI(B, A)
so that the preferences for values of A and B can be as­
sessed separately, with a1 >- az and b1 >- bz. Clearly, a1b1
is the most preferred outcome and a2b2 is the least; but if
feasibility constraints make a 1 b1 impossible, we must be
satisfied with one of a1b2 or a2b1. With just preferential
independence we cannot tell which is most preferred us­
ing these separate assessments. Stronger conditions (e.g.,

mutual preferential independence [ 1 0]) are required before
such tradeoff's can be easily evaluated.
CP-nets [3] are a graphical representation for structuring
In particular, CP-nets are directed acyclic
graphs whose nodes are the variables of V. We associate a

CPI statements.

conditionalpreference table (CPT) with each node X spec­
ifying a preference order over X's values given each in­
stantiation of its parents U, and require that CPI(X, U, Z)
hold, where Z = V- (U U {X}). CP-nets structure these
CPI statements so as to support useful inferences about the
underlying preference order [3]. In Fig. 1 we see a CP-net
defined over a set of four boolean variables, where, e.g.,
the CPT for
specifies that c is preferred to c when a and
b hold. An important property of CP-nets is the induced
importance it assigns to different variables: nodes "higher­
up" in the graph are more important than their descendants.
Thus, it is more important to obtain preferred values for a
node than for any one of its descendants. For example, in
the CP-net above, we can see that abed (in which a less pre­
ferred value of
appears) is preferred to abed (in which a
less preferred value of A appears). This property plays an
important role in UCP-nets.

C

C

Let X1, . . . , Xk be sets of not necessarily disjoint vari­
ables such that V
uiX;. X1, .. . ,Xk are generalized
additive independent (GAl) for an underlying utility func­
tion u if, for any two probability distributions Pr1 and Pr2
over Dom(V) that have the same marginals on each of the
sets of variables Xi, u has the same expected value under
Pr1 and Pr2• In other words, the expected value of u is not
affected by correlations between the X i. It depends only
on the the marginal distributions over each the X i.
=

xk are GAl iff u can be
J;(Xi). That is, u can be de­

It can be shown [1] that xl,.'.

L:�=l

I

written as u(V) =
composed into a sum of factors over each of these sets of
variables. This property generalizes the standard definition
of additive utility independence, which requires that the Xi
partition V. For UCP-nets the ability to deal with overlap­
ping sets of variables is critical.

3

Adding Utilities to CP-Nets

As noted above, the precision of a utility function (as op­
posed to a preference ordering) is often needed in decision
making contexts where uncertainty is a factor. The rep­
resentation of utility functions should be natural, easy to
elicit, compact (in typical cases), and support effective in­
ference. There are two basic types of queries with regard

BOUTILIER ET Al.

58

h(a,b,c)

UA12001

=

0.6, while h(a,b,c)

=

0.1. Thus, the CPT

tables along with the GAJ i nterp r eta tion provide a full spec­

ification of the utility function. For example, we have that

u(a,b,c,d) = fi(a) + h(b)
5 + 5 + 0.1 + 0.3 = 10.4.
Figure 2: A UCP-Network
to outcomes one will often ask: 1
(a) Dominance queries: does one outcome have higher
utility than another (i.e., u(vt) :?: u(v2))?

(b) Outcome optimization queries: what outcome has
maximum utility given some partial assignment (i.e.,
what is ar g m ax {u(v ) : v E Comp(x)})?
GAl-models allow dominance testing to be performed very
effectively: the utility of outcome v is readily determined
by looking up the value /i(v) of each factor applied to v,
and summing them to obtain u(v). In contrast, CP-nets
do not allow straightforward dominance tests, generally re­
quiring reasonably sophisticated search techniques for all
but the simplest network topologies. The relative attrac­
tiveness of the two approaches is, however, reversed when
one considers optimization queries. In CP-nets, determin­
ing the (conditional) maximal outcome in a preference re­
lation is straightforward. In contrast , maximization in a
GAl model requires the use of variable elimination [6, 13],
whose complexity depends on the structure of the modeL 2

+

h(a,b,c)

+

j4(c,d)

=

Notice however that the factors h and h are redundant in
the sense that they refer to variables that are included in
!J. Thus, this utility function could be represented more
concisely using a GAl decomposition containing two fac­
tors: j4(C,D) and j5(A,B,C) = JI(A) + h(B) +
h(A, B, C). The directionality of the utility-augmented
CP-network, on the surface, seems to serve no purpose
other than to break up the GAl-factors unnecessarily.
However, we can use this directionality to represent CP
conditions on the utility function u, and thus provide a sim­
ple and natural interpretation for the individual factors of u.
In particular, we interpret the fact that A and B are parents
ofC as asserting that CPJ(C, {A, B}, D), and thus the fac­
tor h(A, B,C) specifies the utility of C given A and B.
The fact that each node is isolated from the rest of the net­
work given the values of its parents greatly simplifies utility
assessment. Furthermore , this structure supports more ef­
ficient inference for certain queries than the standard GAl
representation.
Definition 1 Let u(X 1, . .. , Xn) be a utility function with
induced preference relation�- A UCP-networkfor u is a
DAG Gover X1, ... , Xn and a quantification (i.e., a set of
factors /;(X;, Ui) where Ui are the parents of Xi) s.t.:

(a) u(Xt, ... ,Xn)
Ldi(X;, U;)
(b) The DAG G is a valid CP-network for �; i.e.,
satisfies CPI(Xi, U;, Z;) for each X;, where Z;
=

We propose in this section a new network representation
for utilities that combines aspects of both CP-nets and GAl
models. The model is directed, like CP-nets, but prefer­
ences are quantified with utilities. The semantics is given
by generalized additive independence along with the con­
straint that the directed influences reflect the ceteris paribus
condition underlying CP-nets. By extending CP-nets with
quantitative uti lity information, expressive power is en­
hanced and dominance queries become computationally ef­
ficient. By introducing directionality and a ceteris paribus
semantics to the GAl model, we allow utility functions
to be expressed more naturally, and permit optimization
queries to be answered much more effectively.
A UCP-net extends a CP-net by allowing quantification of
nodes with conditional utility information. Semantically,
we treat the different factors as generalized additive in­
dependent of one another. For example, the network in
Fig. 1 can be extended with utility information by includ­
ing a factor for each family in the network, specifically,
11 (A), h(B), h(A, B, C), and j4(C,D) (see Fig. 2).
We interpret this network using GAl: u(A,B,C, D)
!I(A) + h(B) + h(A,B,C) + j4(C,D). Each of these
factors is quantified by the (now quantitative) CPT ta­
bles in the network. For example, in Fig. 2 we have that

V- (U;

u

>­

{Xi})

Condition (a) means that every UCP-net specifies a GAl
decomposition of the underlying utility function u. How­
ever, the acyclic restriction means that not every GAl de­
composition can be represented in a UCP-net. For exam­
ple, the GAl decomposition u(A, B, C)
h (A, B,C) +
h(C,B) would have to converted to the decomposition
u(A,B,C) = h(A,B,C), where h
!I + /2, before
it could be represented as a UCP-net. Nevertheless, there
is a simple case where a GAl decomposition can easily be
seen to be representable using a UCP-net topology.
=

=

Proposition 1 If there exists a ordering of the variables
such that under this ordering the last variable in every GAl
factor is unique (i.e., no variable is last in more than one
factor), then the GAl decomposition can be represented
with a UCP-net topology.

=

1
Queries regarding optimization with respect to actions are
discussed in the next section.

2GAI optimization can be effected using cost networks

[7).

To construct the UCP-net in this case, for every factor we
make every "last" variable a child and all of the prior vari­
ables its parents.
Even if we can represent a GAl decomposition in a UCP­
net topology and we parameterize the net using the GAl

BOUTILIER ET AL.

UAI2001

59

dren. It requires that we check that, for each instantiation
of X's children and the parents of its children, whether the
decrease in local utility (i.e., in factor f x) dominates the
(potential) increase it causes in the local utilities of its chil­
dren.
With this definition, we can specify a straightforward nec­
essary and sufficient condition that ensures a DAG satisfies
the CP conditions required by the definition of a UCP-net.

Figure 3: The Domination Relation

factors, the the result might not be a UCP-net, since the
utility function might not satisfy the CP requirements of
condition (b). For example , let u be a utility function over

the boolean variables A and B with u(ab)
9, u(ab)
1, u(ab)
2 and u(iib) = 8. No UCP-net can represent
=

=

=

To see this, first notice PI( A, B) fails to holds, since
preferences for B depend on A. But we cannot make A a

u.

B, since the preferences for A depend on B;
can we make B a parent of A for a similar reason. 3

parent of

nor

This example shows that, for a fixed set of variables, UCP­
nets define a proper subset of all utility functions; but this
subset has certain attractive computational properties as
well as pragmatic advantages when it comes to elicitation.
Given a UCP-network U for utility function u, verifying
that it satisfies the CP-relationships among variables re­
quired by its definition can be accomplished by tests in­
volving the local neighborhoods of each node in the net­
work.

Definition 2 Let X be a variable in a quantified DAG with
parents U and children Y = {Y1, . .. ,Yn}, and let Z;
be the parents of Y;, excluding X and any elements of
U. Let Z
U Z;. Let U; be the subset of variables
in U that are parents of Y; (the relationships among these
=

variables is shown in Fig. 3).

We say X dominates its
if, for all x 1, x2 such that

children given u E Dom(V)
fx(x!, u) ;:::: /x(x2, u), for all z
(y1, ... ,yn) E Dom(Y):

E

Dom(Z),

and for all

fx(xl, u)- fx(x2, u);::::

L jy, (y;, X2, u;, z;) - jy, (y;, x1, u;, z;)
i

X dominates its children if this holds for all u

E

Dom(V).

Testing whether X dominates its children is a local test, in­
volving only the factor for variable X and those of its chil3

The example above can be accommodated by clustering the
dependent variables. That is, we can define a new variable C with
Dom(C) = Dom(A) x Dom(B). In general, any utility function
can be represented in a UCP-net by clustering appropriate sets of
variables. This can be a reasonable approach if the clusters remain
relatively small. It is also possible to generalize the definition of
a UCP-net to allow cycles. This example could be represented by
allowing A and B to be parents of one another. Cycles allow one
to express a larger set of utility functions, but still do not permit all
utility functions to be represented. They also may admit inconsis­
tency: certain network structures with cycles do not correspond
to a consistent utility function satisfying the CP·constraints (an
impossibility in acyclic graphs). We refer to [3] for details.

Proposition 2 Let G be a DAG over {Xi} whose factors
fi reflect the GAl-structure of utility function u. Then G is
a UCP net iff each variable Xi dominates its children.
Proof: We need only show that the CP-condition holds for
iff X dominates its children. Assume the same vari­
ables stand in relation to X as in the definition of domina­
tion above, and let W
V- (U U {X} U Y U Z) (i.e.,
all of the other variables in the network). X satisfies the
CP-condition iff, for all x1, x2, u: x1 uyzw t x2uyzw
implies x1u(yzw)' � x2u(yzw)' for all (yzw)'. Now
x1 uyzw t x2uyzw iffu(x1 uyzw) � u(x2uyzw), iff

X

=

fx(x!, u)- !x(x2, u);::::

L jy, (yi, x2,

u;,

zi)

-

jy, (yi, x1, u;, z;)

since the only factors whose values can vary between these
two terms are f x and the fy,. By definition this relation
holds for all values of

y

and

X dominates its children. <IIIII

z

(and trivially for all w) iff

Determining if a quantified network is in fact a UCP­
network requires a case-by-case analysis for each "ex­
tended family" in the network involving a number of tests
exponential in the size of the extended families (by this we
refer to a variable, its parents, its children, and its children's
parents). Several stronger sufficient conditions exist that
are easier to test. Here we present a particularly simple
one.

Proposition 3 Let G be a quantified DAG over the set of
variables V. For each variable X let U be its set ofparents
in G. For X1, X2 E Dom(X), let

Minspan(x1,x2)
Minspan(X)

=

=

min

uEDom(U)
min

xl,x2EDom(X)

(lfx(xt,u)- fx(x2,u)l),

Minspan(x1, x2).

Define Maxspan(X) analogously with max replacing min.
Then G is a UCP-net ifMinspan(X) ;:::: Li Maxspan(Y;)
where the Y; are the children ofX.
The values of Maxspan and Minspan can be computed for
each variable X in 0(IDom (X) II f xI) time. Thus, this con­
dition can be checked in time polynomial in the number of
network parameters.
For purposes of elicitation and computation, it is often con­
venient to normalize utilities over the range [0, 1]. Simi­
larly, it is useful to normalize the individual factors of a

BOUTILIER ET AL.

60

UAI2001

UCP-net. In Section 5 we will consider a normalized vari­
ant of the UCP-net model in which the "rows" of each fac­
tor are normalized and "tradeoff' weights are used to cali­

instantiation of its parents in v i-1, with all other variables
retaining their values from Vi-1; (c) if Xi E Z, then

brate them. Specifically:

by the forward sweep
algorithm (assuming ties are broken in the same way as

(a) For each variable X, with parents U and factor f x,
and each u E Dom(U), we normalize the function
fx(x, u) so that its values lie in the range [0, 1].
That is, we insist that minx fx(x, u) = 0 and
1. We denote the normalized func­
maxx fx(x, u)
tion v x and call it the local value function for X given
=

u.

(b) For each

X and instantiation of its

ify a multiplicative tradeoffweight

tradeoffweight a-x.

parents
1r

u,

we spec­

x, and an additive

parents. It is not hard to see, by the usual transformation

results in utility theory, that every UCP-net has an equiva­
lent normalized representation.

in the procedure). since v n t vz for any outcome v z
consistent with the evidence, the forward sweep procedure
yields an optimal outcome. <ill

This algorithm illustrates the sharp contrast between UCP­
nets and GAl representations. Effective outcome optimiza­
tion in a GAl model requires that one use a dynamic pro­
gramming algorithm like variable elimination. As a conse­
quence, the complexity of such an algorithm-exponential
in the induced tree width of the GAl model-depends crit­

Thus, UCP-nets offer a valuable restriction of GAl models
and generalization of CP-nets. In particular, they impose
restrictions on the relative strength of the GAl factors, and
generalize CP-nets to allow for the representation of quan­
titative utilities. But we preserve the convenient graphical
representation of CP-nets, and gain considerable computa­
tional benefits over both models.
One of the main reasons to move from qualitative to quan­

Optimization Algorithms

The two types of queries discussed above, dominance
queries and optimization queries, can both be answered di­
rectly in UCP nets. Dominance queries can be answered
trivially: determining whether u(vl) ;=: u(v2) for two
complete outcomes simply requires that one extract and
sum the values of each factor in the network and then com­
pare the sums. This can be done in time linear in the size of
the network. Thus UCP-nets offer the advantages of other
additive decompositions. In contrast, dominance testing in
CP-nets is computationally difficult precisely because the
tradeoffs between the (conditional) preference levels for
different variables have not been specified.
Outcome optimization queries can also be answered di­
rectly given a UCP-net, taking linear time in the network

z E Dom(Z), deter­
Comp(z)) can be effected

size. Given a partial instantiation

mining argmax{u(o) : o E
by a straightforward sweep through the network.

Let

X1, ... , Xn

be any topological ordering of the network
variables excluding Z. We set Z = z, and instantiate each
xi in turn to its maximal value given the instantiation of its
parents. This procedure exploits the considerable power of
the CP semantics to easily find an optimal outcome given

certain observed evidence (or imposed constraints).

Proposition 4 The forward sweep procedure constructs

the optimaloutcomeargmax{u(v): v

Proof: Let

Yi-1· The last outcome

ically on the "topology'' of the model and the ability to find
good elimination orderings.

The semantics of such a normalized UCP-net is as follows:
the utility of any outcome is given by the sum of the terms
(for each variable X) 7rxvx(x) + a-x, where x is the in­
stantiation of variable X and u is the instantiation of its

4

v i = Vj-1. By construction, Vi t
vn is precisely that constructed

E

Comp(z)}.

vz be any outcome in the set of completions

of z. Define a sequence of outcomes vi, 0 � i � n, as
follows: (a) Vo = Vz; (b) if xi 'it Z, Vj is constructed by
setting the value of Xi to its most preferred value given the

titative preference models is to support decisions under
(quantified) uncertainty. Naturally, given a decision prob­
lem and a fully specified UCP-network, determining an
optimal course of action is (conceptually) straightforward.
W hen the distributions induced by actions can be structured
in a Bayes net, UCP-nets can be used to help structure the
decision problem. Suppose that the distribution over vari­
ables V determined by an action a is represented as a Bayes
net (possibly with a choice node if we wish to represent all
actions) and the utility function over outcome space is de­
termined by a UCP-net.4 To compute the optimal action,
we can construct an influence diagram by adding one utility
variable Fi for each (nonconstant) factor f x, in the UCP­
network.

Fi

has as parents both

X,

and the parents of Xi

in the UCP-net, and is quantified using factor f x, from the

UCP-net. Variable elimination (e.g., Dechter's

[6]

MEU

variant) can be used to determine the optimal action. 5

This approach uses the GAl factorization of utilities af­
forded by the UCP-net, but not the CP-semantics. We can
improve upon these ideas by noticing that our goa is to
select the optimal action, not (necessarily) compute tts ex­

�

pected utility. In any GAl representation, we can bound

the error associated with ignoring a utility variable Fi with
parents U as follows. L et ei
m axu Fi{u)- minu Fi(u).
=

The expected value EV(a) of any action

a

is given by

L Pr(vla)(l: Fi(v)) L(L Pr(vla)F,(v))
=

vEV

i

i

vEV

40ften utilities are elicited only over variables that are directly
related to preferences. The variables in the Bayes net may include
variables not contained in the UCP-net.
50ne might also consider how expected utility networks [l l]
might be used in this regard.

UA12001

BOUTILIER ET AL.

Let EV-i(a) be the expected value of action a with re­
spect to all utility variables except Fi. Then IEV(a) -

61

tion problem. Given a specific set of questions that can

such that

be posed to the user, the (myopic) value of information for
each question with respect its reduction of the minimax re­

a•

gret can also be computed by solving a linear program. As

is optimal without having to compute the i'th term in the
above summation. Analogous statements hold for ignoring
any subset F of the utility variables, setting ep = LiEFej.

such, an incremental procedure can be used to compute a
greedy query plan that will ask just enough questions of the
user to decide on a course of action whose regret is below
some prespecified threshold (if this threshold is set to zero,
then the optimal action will be recommended).

EV-i(a) I ::; e;. Thus
EV-;(a*) - EV_;(a) �

if there is some

€; for all

a

-::j:. a•,

a•

we know

This suggests an incremental technique for computing
an optimal (or near-optimal) action that exploits the CP­
semantics of a UCP-net. Let X 1, . . . Xn be a topological
ordering of the variables in the UCP-net. Our technique
runs in (at most)

n

stages, where at stage

k,

we compute

EV_{i>k}(a) for each action a. If for some a* we have
EV_{i>k} (a*)- EV -{i>k} (a) 2: E{i>k} for all a -::j:. a*, we
know a• is optimal and we can terminate without comput­
ing any further terms. Furthermore, we can remove from
consideration at subsequent stages any action whose partial
utility differs by more thane {i>k} from the (estimated) op­
timal action at this stage. The motivation for this approach
lies in the fact that in a UCP network, variables near the top
of the UCP-net have a larger impact on utility, and are thus
more likely to lead to the separation of actions than factors
lower in the net. For example, if action a has high probabil­
ity of making the most important variable

X1

take its most

desired value, while action b is likely to ensure its least de­
sired value, we may be able to eliminate b from considera­
tion by just computing the first term of above summation.

We can also terminate when the error associated with a* is
below some threshold, even if it is not optimal.
The computational benefits arise when one considers that
computing EV -{i2":k} (a) requires one to do inference only

on those variables that are relevant to predicting F {i>k}.
Furthermore, at each each stage we need only compute the
expected value with respect to the newly added utility vari­
_

able. In a problem with no evidence, for example, this can
be accomplished by considering only ancestors of the vari­
ables F; in the Bay es net. An important issue with this iter­

To make this more precise, we define a
to be a set of actions A

out too much difficulty.

Structure elicitation, involving

questions regarding the relative importance of different at­
tributes, as well as the dependence of these assessments on
other attributes should not be especially onerous. Eliciting

value function v x requires ordering a small number of val­
ues, given a specific parent context, and calibrating these
value using (local) standard gambles, again, a relatively un­

problematic task.
Although the structure and value functions are thus de­
termined, the tradeoff weights for the normalized UCP­
network remain unknown. The utility of any outcome v
is a linear function of these weights: specifically, ifv in­
stantiates each variable

Xi

to Xi and parent set

Ui

to ui,

then

u(v)

=

"\'"" 7TU;
L._...
X; VU;
X; ( x·� )

+

(JUi

i

X;

Let W be the set of possible instantiations of the tradeoff
weights, and u(v, w ) denote the utility ofv for a particular

instantiation w E W. The expected value of any action a i
is also a linear function of the tradeoff weights:

EV(a;, w )

rithmic question.

Elicitation with UCP-nets

decision scenario
where each action

the elicitation of both structure and local value functions
is something that users will often be able to provide with­

results---determining such orderings is an interesting algo­

5

, an,

As a starting point, we assume that we have been provided
with a normalized UCP-network, whose structure and local
value functions v x have been provided, but whose trade­
off weights 1Tx and ox remain unspecified. We feel that

reusing information computed in earlier iterations. This is

VE is used it might be possible to find variable orderings
for each computation that facilitate the reuse of previous

. ..

ai E A induces a distribution Pri(V) over outcomes. Let
0; denote the support set for Pr; (i.e., the set of outcomes
v for which Pr i(V) > 0). We generally assume that Oi is
small relative to Dom(V).

ative procedure is how to minimize overall computation by
plausible since (utility) factors generally overlap, and these
factors generally have common influences. For example, if

= a1,

=

L

vEO;

Pr i (v ) u (v, w )

Note that by assumption !Oil is relatively small, so this sum

--;

should contain only a small number of terms.
One of the key problems facing the use of decision­
theoretic models is the elicitation of preference informa­
tion. In this section we describe one possible procedure for
exploiting the structure and semantics of a UCP-network
to facilitate an incremental elicitation process. More pre­
cisely, given a specific decision scenario--i.e., a set of pos­
sible actions and the corresponding distributions over out­
comes they induce-and a set of constraints on the tradeoff
weights of a normalized UCP-network, the

regret

of the

best action can be computed as a simple linear optimiza-

We define the optimal action a :., with respect to an instan­
tiation w of the tradeoff weights to be

a:_.= argmaxEV(a;, w )
a;

utility function were known to have weights w, a;,
would be the correct choice of action. The regret of action
ai with respect to w is
If the

R(a;, w )

=

EV(a;,, w ) - EV(ai, w )

BOUTILIER ET AL.

62

i.e., the loss associated with executing ai instead of acting
optimally. Let C be a subset of the set of possible instanti­
ations of the tradeoff weights, W. We define the maximum
regret of action ai with respect to C to be

Finally, we define the action a0 with minimax regret with
respect to C:
a0

=

ar g min
a;

MR (ai , C)

The (minimax) regret level of weight set C is MMR( C)
MR(aC:, C). If the only information we have about a user's
utility function is that it lies in the set C, then a 0 is a
reasonable choice of action. Specifically, without distribu­
tional information over the set of possible utility functions,
choosing (or recommending) a c minimizes the worst case
loss with respect to possible realizations of the utility func­
tion.
=

If C is defined by a set of linear constraints on the weights,
then a0 as well as MMR(C) can be computed using a set
of linear programs. First note that we can compute

for any pair of actions ai and ai using a linear program:
we are maximizing a linear function of the weights sub­
ject to the linear constraints that define C. Solving O(n 2)
such linear programs, one for each ordered pair of actions,
allows us to identify the action a0 that achieves minimax
regret and to determine the minimax regret level MMR( C).

MMR(C) tells us how bad off we could be recommend­
ing a0. In particular if MMR(C) = MR(a0, C) = 0 then
a0 dominates all other actions: it is at least as good as any
other action for every feasible set of tradeoff weights. How­
ever, unless C is very refined (i.e., is defined by strong con­
straints), multiple actions will potentially be optimal (i.e.,
will be maximal in certain regions of weight space). To de­
termine which of these actions to recommend, we need to
refine the constraints defining C further.
C can contain a range of different linear constraints. One
class of constraints in C is imposed by the structure of
the network. In particular, each variable must dominate its
children: this is a necessary condition in any UCP-net. Us­
ing the same notation for variable X's neighborhood as in
Defn. 2, domination imposes the following linear constraint
on the weights: for each u, z, y and pair x l> x2 E Dom ( X)
such that v� ( xt ) 2 v� ( x2 ) , we must have:

Another class of constraints is the set of bounds that re­
strict each tradeoff weight 1r� and u� to a specific range.
Such bounds are required in order to keep the LP problems

UAI2001

we have proposed using bounded. Eliciting such bounds
from the user is not a difficult task, as one can always start
with very loose bounds. For example, the minimum and
maximum utility of any possible outcome is a simple uni­
form bound on the tradeoff weights. Besides these required
constraints, C could contain other nonstructurallinear con­
straints provided by the user, e.g., constraints on the rela­
tive magnitudes of different weights (reflecting degree of
importance) or constraints on the relative expected utility
of different actions in certain fixed contexts.
If minimax regret is zero, or lies below some acceptable
threshold, the action a0 can be recommended. Otherwise,
questions can be asked of the user to help differentiate be­
tween possible actions. The solution to the above set of
O(n2) linear programs can provide some guidance. For
example, the linear program for solving MR(ai, C) also
yields a solution to the dual problem. This solution pro­
vides a multiplicative factor associated with every inequal­
ity in C that tells us how much of a change we can produce
in MR( ai, C) (in the neighborhood of the optimal solution)
by modifying the inequality. Say that the k-th inequality is
the upper bound 1r� � 290, and that the value of the k-th
variable in the dual solution is 100. This tells us that if we
can get the user to tighten their upper bound on 1r Jl by 9
units this might yield a 900 unit decrease in MR(ai, C). By
examining the factors associated with the bounds imposed
on the weights, those weights that have the most potential
to influence MMR( C) could be identified. Furthermore, af­
ter we have queried the user and obtained an updated bound
we need not resolve the linear programs from scratch to re­
compute the maximum regret of each action. There are
many techniques in the LP literature on sensitivity analysis
that can be employed to minimize the amount of computa­
tion that needs to be performed [5].
However, generally for realistic elicitation we cannot rely
solely on the recommendations of the linear programs. In
particular, sharpening the inequality recommended by the
dual solution might not be an easy task for the user. The
types of questions that can (reasonably) be asked will be
domain dependent, and influenced by factors such as the
complexity of the domain (e.g., if the number of attributes
is manageable, asking a user to compare full outcomes may
be acceptable, but otherwise not), the sophistication of the
user, and the importance of the decision to be made. To
address the general problem here, we will assume a (finite)
set of possible questions Q = {q1, ... ,qk}, with each Qi
having m possible responses r } , ... , rf' (we fix the num­
ber of responses simply to streamline the presentation). We
suppose that every response adds an additional linear con­
straint to C (this subsumes the case of sharpening an ex­
isting constraint). Let C(d) denote the set of weights that
satisfy C U {r{ } . Then asking a question qi and receiving
a response r{ will reduce minimax regret by the amount
MMR(C)- MMR(C(r{)).
This suggests a querying strategy in which questions that
have the ability to reduce minimax regret the most are
asked first. In a certain sense, asking questions that reduce

UAI2001

BOUTI LIER ET AL.

63

determine if an optimal decision can be made. If no action

minimax regret can be seen as a distribution-free analog
of traditional value of information approaches to query­

dominates all others, further preferences are elicited. Our

ing. Specifically, the procedure we suggest strongly par­
allels the elicitation method proposed in [4], where a dis­

approach extends this basic viewpoint in a number of ways,
including the utilization of richer utility models and a min­

tribution over possible utility functions is used to guide

imax regret model that supports decisions even when no

the interactive elicitation process. In our distribution-free
model, we cannot define the expected value of a ques­
tion, but instead use the worst-case response to define

action is dominant. ISMAUT does not generally describe
means for generating queries automatically or making de­
cisions when no action is dominant.

the minimal improvement we can obtain from some ques­
tion. The minimax improvement of question q i , Ml(q;)

{

is mini MMR(C) - MMR(C(r ) ) . The minimax optimal
query with respect to C is that query with maximal im­
provement M/(qi) .6 We note that the improvement M/(qi)
for any query must be nonnegative, since q ; will always re­
duce the size of the feasible weight space, and generally
will be nonzero.
This suggests the following abstract elicitation strategy,

rf is obtained,
refined weight space C(r{). Then

imal improvement is asked, and response

MMR(C(rt ) ) is computed by solving the previously spec­

rf .

ified linear programs with the added constraint
Tech­
niques for sensitivity analysis can be utilized to minimize
the work involved in doing these computation. This pro­
cess is repeated until one of two conditions is met: (a)
the current weight space admits an action with regret less

than some threshold r; or (b) no query has an improve­
ment score greater than the cost of that query. This latter
condition is typically important in interactive elicitation:
while one could ask many questions to narrow down a util­
ity function so that a (near) optimal decision can be made,
one must account for the cost of these questions (e.g., the
burden they impose on the user).
Making this procedure concrete requires having a set of
possible questions whose responses induce linear con­
straints on weight space.

As pointed out above, such

questions will in general be domain dependent. However,
they might include asking the user to quantify the relative
strengths of various tradeoff weights associated with a sin­
gle variable ax and 1r x . For example, asking the user for
a value of k such that a�1 � ka�2 • Since this involves the
outcomes of a common variable it should be relatively easy
for the user to answer. Assessing relative tradeoff weights
associated with different variables is a similar, albeit more
difficult, question. Sharpening an upper or lower bound on
a weight was addressed above. One more type of ques­

tion might be to ask the user which of actions a i or aj she
would prefer in a specific context. The answer to this ques­
tion again imposes a linear constraint on weight space.
This approach is very similar to that utilized in imprecisely
specified multiattribute utility theory (ISMAUT) [9]. In
such work, standard additive independent utility models are
assumed and constraints on tradeoffs weights are used to
6

The

sequences

of

questions that reduce regret may not be considered if the
individual questions in the sequence do not. Circumvent­
ing this requires l ookahead or some form of dynamic pro­
gramming. This problem is common to most value of in­
formation approaches (e.g., [4]). The use of minimax re­
gret to select actions should be viewed as reasonable in the

which myopically attempts to improve minimax regret.
Given a set of feasible weights C, the query q; with max­
resulting in a more

Our elicitation procedure has several drawbacks.
greedy nature of the algorithm means that

Queries can be ranked simply using their worst-case minimax
regret, rather than their worst-case improvement, since the term
MMR( C) is common to all queries and responses.

absence of distributional information. However, selecting
queries so that the worst response has maximal improve­
ment may not always be appropriate; and comparing this
worst case improvement to the cost of the query may also
be questioned, but other strategies are possible.
We are currently exploring the use of distributions over
weight space (i.e., over utility functions) to guide the elici­
tation process. In the abstract, such a model would be sim­
ilar to that of Chajewska, Koller, and Parr [4].

The dif­

ferences lie in the use o f UCP-nets as the underlying util­
ity representation, and the use of dynamic programming to
construct optimal query sequences (rather than using my­
opic value of information). Integrating this with linear op­
timization poses some interesting challenges.

6

Concluding Remarks

We have proposed a new directed graphical model for rep­
resenting utility functions that combines appealing aspects
of both CP-nets and GAl models. The UCP-net formal­
ism has a number of conceptual and computational advan­
tages over these models, providing leverage with respect
to inference and elicitation. Clearly, practical experience
and empirical studies are needed to gauge the ultimate ef­
fectiveness of UCP-nets. However, we are encouraged by
the widespread use of additive models, and more recently,
by the successful application of CP-nets to the problem of
preference-based Web page content configuration [8].
We are currently in the process of implementing the inter­

active elicitation algorithm described in Section 5. Future
research includes the inclusion of distributional informa­
tion over utility functions (or tradeoff weights), and devel­

oping algorithms that compute and use value of informa­
tion to construct optimal query plans. Another fundamental
question pertains to determining optimal query plans when
the query space is large or infinite, involving parameterized
queries (e.g., standard gambles). We expect that the con­
siderable structure exhibited by the problem (e.g., the fact

that the set of actions divides weight space W into a set
of convex regions where each action is optimal) will allow
optimization over each query type to be effected, without
explicit enumeration of all instances.

64

BOUTILIER ET AL.

UAI2001

The investigation of the suggested optimization algorithms,

and trade-off weight determination under uncertainty.

specifically, empirical validation of the incremental vari­
able elimination approach to decision making described in
Section 4, is a high priority.

IEEE Transactions on Systems, Man and Cybernetics,

Acknowledgements

[ 1 0] R. L. Keeney and H. Raiffa.

Decisions with Multiple
Objectives: Preferences and Value Trade-offs. Wiley,

New York, 1 976.

Craig Boutilier and Fahiem Bacchus were supported by
Communications and Information Technology Ontario, the
Institute for Robotics and Intelligent Systems, and the
Natural Sciences and Engineering Research Council of
Canada. Ronen Brafman was supported by the Paul lvanier
Center for Robotics and Production Management.




of the logic, and the set consists of all unique pos­
sible worlds. A probability distribution is placed
over this set. Probabilities are then assigned to
the sentences by giving each sentence a proba­
bility equal to the probability of the subset of
possible worlds in which it is true.
Although this approach is unproblematic when
applied to propositional languages, certain diffi­
culties arise when dealing with first order lan­
guages. By taking a different tack these difficul­
ties can be overcome, and indeed, it has already
been demonstrated that probabilities can be co­
herently assigned to the sentences of any first
order language ( Gaifman [2] , Scott and Krauss
[3] ).
While the method of assigning probabilities to
logical formulas is capable of representing prob­
abilistic degrees of belief, it is incapable of ef­
fectively representing statistical assertions. It is
argued that many types of defaults have a natu­
ral statistical interpretation, but cannot be rep­
resented by probabilities over logical formulas,
because of this limitation. Some authors have
attempted to represent default.s by ( conditional )
probabilities over logical formulas ( Geffner and
Pearl [4] , Pearl [5]), and the difficulties this
causes can be demonstrated.
It is pointed out that although probabilities
over logical formulas fails to do the job, statis­
tical assertions can be efficiently represented in
other types of probability logics, logics which go
beyond the simple device of assigning probabili-

In Probabilistic Logic Nilsson uses the device of
a probability distribution over a set of possible
worlds to assign probabilities to the sentences of
a logical language. In his paper Nilsson concen­
trated on inference and associated computational
issues. This paper, on the other hand, exam­
ines the probabilistic semantics in more detail,
.particularly for the case of first order languages,
and attempts to explain some of the features and
limitations of this form of probability logic. It is
pointed out that the device of assigning proba­
bilities to logical sentences has certain expressive
limitations. In particular, statistical assertions
are not easily expressed by such a device. This
leads to certain difficulties with attempts to give
probabilistic semantics to default reasoning us­
ing probabilities assigned to logical sentences.
1

Introduction

Nilsson [1] describes a method of assigning prob­
abilities to the sentences of a logic through a
probability distribution over a set of possible
worlds. Each possible world in this set is a consis­
tent assignment of truth values to the sentences
•This research was supported by a Post-Doctoral fel­
lowship funded by the U.S. Army Signals Warfare Labo­
ratory, while the author was a researcher at the University
of Rochester.

I
I

I;

15

I
I
Given a probability distribution over the set
of possible worlds it is possible to assign a prob­
ability to each sentence of the language. Each
sentence is given a probability equal to the prob­
ability of the set of worlds in which it is true. So,
for example, the sentence A v B is true in worlds
1, 2, and 3. Hence, its probability will be equal
to the probability of the set of worlds { 1, 2, 3}.
Equivalently, a probability distribution can be
placed directly ov-er the sentences of the logic,
more precisely over the Lindenbaum-Tarski alge­
bra of the language. This algebra is generated by
grouping the sentences into equivalence classes.
Two sentences, a and /3, are in the same equiv­
alence class iff f-o a +-+ f3, where f-0 indicates
deducible from the propositional axioms.
This technique is not limited to languages with
a finite number of atomic symbols.. However,
when the language is finite the atoms will be
sentences of the language, and the probability
distribution can be completely specified by the
probabilities of the atoms (the e-classes of). Any
sentence can be written as a disjunction of a
unique set of atoms, and its probability will be
the sum of the probabilities of these atoms. For
example, if we specify the probabilities {A 1\ B =
.5, Af\-,B= .1, -,Af\B= . 2, -,Af\...,B= 2 } , then
the sentence A VB will have probability 0.8 as it
can be written as (AI\ B) V (AI\ -,B) v (-,AI\ B).

ties to first order sentences.
2

The Propositional Case

A natural semantic model for a propositional lan­
guage is simply a subset of the set of atomic
symbols (Chang and Keisler [6]). This subset
is the set of atomic symbols which are assigned
the truth value true (t). In the propositional
case Nilsson's concept of possible worlds, i.e., a
set of consistent truth value assignments, has a
natural correspondence with the set of semantic
models. Each possible world is completely de­
termined by its truth value assignments to the
atomic symbols of the language, and the truth
value assignments to the atomic symbols can be
viewed as being the characteristic function of a
semantic model (with t= 1, f= 0).
For example, in a propositional language with
two atomic symbols {A, B} there are four possi­
ble worlds with corresponding semantic models
(u is used to indicate the truth function).
1. {Au= t, Bu= t} or {A, B}.
2. {Au= t, Bu= f} or {A}�
3. {Au= f, Bu= t} or {B}.
4.

.

{Au= f, Bu= f} or {}, i.e., the empty set.

Another way of looking at possible worlds,
which will turn out to be more useful when we
move to first order languages, is to consider the
atoms1 of the language. When the language has
a finite number of atomic symbols each possible
world can be represented as a single sentence: a
sentence formed by conjoining each atomic sym­
bol or its negation, such a sentence is called an
atom. Corresponding to the four worlds above
we have the four atoms A 1\ B ' A 1\ -,B ' -,A 1\ B '
and ...,A 1\ -,B.

3

First Order Languages

When the move is made to first order languages
certain problems arise. The first problem is that
we lose the nice correspondence between possible
worlds and semantic morlf'll'l 'T'hf' normal �Pman­
tic model for a first order language is consider­
ably more complex than the model for a proposi­
tional language, and the truth values of the sen­
tences in a first order "language are determined
both by the model and by an interpretation (i.e.,
the mapping from the symbols to the semantic
entities). For a given truth value assignment

1 An atom in a Boolean algebra is a minimal non-zero
element ( Bell and Machover, [71).

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

16

I

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

guage. What this means is that each existentially
quantified sentence is equal to the supremum of
all its instantiations. This implies that the prob­
ability of any existentially quantified sentence
must be greater than or equal to the probability
of any of its instances. Similarly, the probability
of any universally quantified sentence must be
less than or equal to the proba bility of any of its
instances.
This interpretation also makes sense in terms
of Nilsson's possible worlds. In any single possi­
ble world the existential must be true if any of
its instantiations are. Hence, the set of possible
worlds in which the existential is true includes
the set of possible worlds in which any instanti­
ation is true, and thus the existential must have
a probability greater than or equal to the prob­
ability of any of its instances.

to the sentences (possible world) there will be
many different (in fact an infinite number) of
model/interpretation pairs which will yield the
same truth values. Hence, the semantic struc­
ture of the possible worlds is unclear.
Another difficulty, which Nilsson is aware of, is
that Nilsson's techniques depend on being able
to generate consistent truth value assignments
for a set of sentences. These are used as 0/ 1
column vectors in his V matrix. This technique
is limited to languages in which the consistency
of a finite set of sentences can be established.
The consistency of a set of first order sentences
is not decidable, except in special cases (see Ack­
ermann [8] for an interesting survey).
These difficulties can be avoided if instead
of probability distributions over possible worlds
we consider probability distributions over the
Lindenbaum-Tarski (L-T) algebra of the lan­
guage. It has already been demonstrated by
Gaifman [2] that a probability measure can be
defined over the L-T algebra of sentences of a
first order language. Every sentence in the lan­
guage will have a probability equal to the prob­
ability of its equivalence class, and furthermore,
the probabilities will satisfy the condition

4

tistical Know ledge
Probabilities attached to logical sentences can be

interpreted as degrees of belief in those sentences.
Instead of either asserting a sentence or its nega­
tion, as in ordinary logic, one can attach some
intermediate degree to it, a degree of belief. So,
for example, one could represent a degree of be­
lief of greater than 0.9 in the assertion "Tweety
can fly" by assigning the sentence Fly(Tweety) a
probability > 0.9. However, it is not easy to rep­
resent statistical information, for example, the
assertion "More than 90% of all birds fly." 2
First, propositional languages do not seem to
possess sufficient power to represent these kinds
of statements. This particular statistical stat.e-

-.(a 1\ (3) then p[a v (3J p[aJ + p[(3],
where 1- indicates deducible from the first order
axioms. This means that the probability mea­
sure preserves the partial order of the algebra.
In this partial order we have a < f3 iff a 1\ f3 a;
hence, p[a] ::; p[a 1\ f3] + p[-.a 1\ /3]
p[,B] (by
the above condition). Under this partial order
the conjunction and disjunction operators gener­
ate the greatest lower bound (infimum) and least
upper bound (supremum).
To examine what happens to quantified sen­
tences under such a probability measure it is suf­
ficient to note that for L-T algebras we have that
If

1-

=

=

=

( * ) l3xal

=

·- ·

·-

.

�It is the case that first order logic is in some sense uni­
versally expressive. That is, set theory can be constructed
in first order lo gi c , and thus sufficient mathematics can
be built up inside the hl.nguage to represent statements
of this form. This is not, however, :tn efficient repre� en­
tation, nor is there any direct reflection in the s emantics
of the statistical information. I am concerned here with
efficient representations.

VtET la(x/t)l,

where I • I indicates the equivalence class of the
formula, and T is the set of terms of the Ian-

I
I

The Representation of Sta­

17

I
I
formula of a first order language (unconditional
probability assertions like "p[fly(x)]" are not
first order formulas either ) . This statement is
intended to assert that for every term, t, in the
first order language the conditional probability of
the sentence Fly(x/t), with the variable x sub­
stituted by the term t, given Bird(x/t) is > 0.9.
However, this formulation also falls prey to any
know exception. Say that there is some individ­
ual, denoted by the constant c, who is thought
to be a bird, i.e., p[Bird(c)] is high, and for some
reason or the other is also believed to be unable
to fly, i.e., p[Fly(c)] is low, then clearly this state­
ment cannot be true for the instance when x is
c; hence, the meta-level universal statement can­
not be true: it is not true for the instance c. It is
important to note that it does not matter what
other things are known about the individual c.
For example, c could be known to be an ostrich,
and thus there may be a good reason why c is
unable to fly. However, it will still be the case
that the conditional probability of Fly(c) given
just Bird(c) will not be > 0.9. The universal
statement will fail for c. That is, this problem is
not resolved by conditioning on more knowledge
as claimed by Cheeseman [9].
There is no way that the statistical statement
"More than 90% of all birds fly" can be repre­
sented by the assertion that the conditional prob­
ability is greater than 0.9 for all substitutions of
x: this assertion will be false for certain substi­
tutions. The problem here is that the statistical
statement implies that p[Fly(x)IBird(x)] > 0.9
for a random x , but a universally quantified x
is not the same as a random variable x; further­
more, the simple device of assiJ?;ninl!; prnhahilit.ies
to sentences of a logical language does not give
you access to random variables. This point has
also been raised by Schubert [10].
One can choose to interpret the variable 'x' as
being a random variable. However, simply choos­
ing such an interpretation is not sufficient; it does
not provide any formal meaning. That is, the se­
mantics and behaviour of such a random x must

ment is an assertion which indicates some rela­
tionship between the properties of being a bird
and being able to fly, but it is not an assertion
about any particular bird. This indicates that
some sort of variable is required. Propositionaf
languages do not have variables, and so are in­
adequate for this task even when they are gener­
alized to take on probabilities instead of just 1/0
truth values.
When we move to first order languages we do
get access to variables, variables which can range
over the set of individuals. A seemingly reason­
able way to represent this statement is to con­
sider the probabilistic generalization of the uni­
versal sentence 'VxB£rd(x) � Fly(x). The uni­
versal in 1/0 first order logic says that all birds
fly, so if we attach a probability of > 0.9 per­
haps we will get what we need. Unfortunately,
this does not work. If there is single bird who is
thought to be unable to fly, this universal will
be forced to have a probability close to zero.
That is, the probability of this universal must
be 1- p[3xB£rd(x) !v-.,Fly(x)]. Hence, if one be­
lieves to degree greater that 0.1 that a non-flying
bird exists, then the probability of the universal
must be < .9.
Since universal quantification or its dual exis­
tential quantification are the only ones available
in a first order language, it does not seem that
moving to first order languages allows us to rep­
resent statistical assertions. There is, however,
one more avenue available: conditional proba­
bilities. We have probabilities attached to sen­
tences hence with two sentences we can form
conditional probabilities. It has been suggested
(Cheeseman [9]) that meta-quantified statements
of the following form can be used to capture sta­
tistical statements, in particular for the state­
ment about birds:
('Vx)p[Fly(x) IB£rd(x)]

>

0.9.

The reason that this is a meta-quantification is
that the universal quantifier is quantifying over
a formula "p[Fly(x)IBird(x)]" which is not a

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

18

I

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

have universal statements like 'rlx Penguin(x) Bird(x). The probability of these universals is
one; thus, as discussed above, every instantia­
tion must also have probability one.
To examine the difficulties which arise from
this approach consider the following exam­
Say that we have a logical language
ple.
with the predicates Bird, Fly, and Penguin,
some set of terms { ti}, and a probability
distribution over the sentences of the lan­
guage which satisfies the default rules, i.e.,
for all terms ti, p[Fly(ti)JBird(ti)J � 1 and
p[---.Fly(ti)jPengut'n(ti)] � 1, and in which the
universal 'rlx Penguin(x) --+ Bt'rd(x), has proba­
bility one. Some simple facts which follow from
the universal having probability one are that for
all terms ti, p[Bird(ti)] 2: p[Penguin(t;)], and
p[B£rd(ti) /\ Penguin(ti)] p[Penguin(ti)]).
Consider the derivation in figure 1.
The constraints imply that for any term ti that

also be specified. It is possible to formalize such
an interpretation. A logic can be constructed
with random variables which have a formal se­
mantics and a proof theory which specifies their
behaviour. Such a logic has been constructed
(Bacchus [11,12]). However, its structure is quite
different from probability logics which attach
probabilities to first order sentences.
5

The Representation of De­
faults

There are many different defaults which have a
natural statistical justification, the famous ex­
ample of "Birds fly" being one of them. A nat­
ural reason for assuming by default that a par­
ticular bird can fly is simply the fact that, in a
statistical sense, most birds do fly. This is not to
say that all defaults have a statistical interpreta­
tion: there are many different notions of typical­
ity which do not have a straightforward statis­
tical interpretation, e.g., "Dogs give live birth"
(Carlson [13] , Nutter [14], also see Brachman [15]
for a discussion of different notions of typicality) .
Since probabilities attached to the sentences
of a logic do not offer any easy way of represent­
ing statistical assertions, it is not surprising that
attempts to use this formalism to give meaning
to defaults leads to certain difficulties.
Recently Geffner and Pearl [4] have pro­
posed giving semantics to defaults through meta­
quantified conditional probability statements
( also Pearl [5]3}.
For example, the default
"Birds fly" is given meaning through the meta­
quantified statement Vx p[Fly(x)jB£rd(x)! �
1 In order to allow penguins to be non­
flying birds they have the separate default rule:
Vx p[---.Fly(x)/Pengut'n(x)J � 1.
They also

=

p[Pengut'n(t;) :::; (�)p[---.Penguin(t;)!;

hence p[Pengut' n (ti ) J cannot be much greater
than 0.5. Since � 0.5 is an upper bound on
the probability of all instances of the formula
Penguin(x), it must also be the case that it is an
upper bound on the probability of the sentence
3x Penguin(x), by equation*·
That is, if we accept the defaults we are must
reject any sort of high level of belief in the exis­
tence of penguins.
To be fair to Geffner and Pearl their system
does provide a calcu.lus for reasoning with de·
faults. However, this result indicates that se­
mantically their particular probabilistic interpre­
tation of defaults causes anomalies. It does not
capture the statistical notion that birds usually
fly.

3Pearl uses a slightly different notion of probabilities
within € of one. The technical d ifferences between this
approach and that of Geffner and Pearl do not make any
difference to the following d iscussion; the anomalies pre­
sented also appear in Pearl's system.

6

It has been demonstrated that although proba­
bilities can be assigned to the sentences of any

I
I

Conclusions

19

I
I
1

::::;

I

p[Fly(ti)IBird(ti)]
p[Fly(ti) 1\ Bird(ti) 1\ --,Peng(ti)] p[Fly(ti) 1\ Bird(ti) 1\ Peng(ti)]
+
p[Bird(ti)]
p[Bird(ti)]
p[Fly(ti) 1\ Bird(ti) 1\ ....,Peng(ti)] p[Fly(ti) 1\ Bird(ti) 1\ Peng(ti)]
+
p[Penguin(t;)]
p[Penguin(ti)]
p[-,Penguin(ti)] p[Fly(ti) 1\ Penguin(ti)]
+
p[Penguin(ti)J
p[Penguin(ti)]
p[...., Penguin(ti)]
�--����+ ::::; 0
p[Penguin(ti)]

I

.

<

<

I
I
I

Figure 1: Conditional Probabilities close to one.
feasible conclusions, conclusions which can be
defeated by new information. Kyburg uses an
object language/meta-language formalism, and
has explored the inductive formation of defeasi­
ble conclusions in greater detail [17].

first order language, the resulting probability
logics are not powerful enough to efficiently rep­
resent statistical assertions. It h�s also been
demonstrated that attempts to give defaults a
probabilistic semantics using these types of prob­
ability logics leads to certain semantic anomalies.
Statistical facts, it has been argued, give a
natural justification to many default inferences.
This implies that probabilities might still be use­
ful for giving semantics to default rules and a jus­
tification to default inferences. For example, the
default rule "Birds fly" could be represented as
a statistical assertion that some large percentage
of birds fly, and the default inference "Tweety
flies" could be given the justification that Tweety
probably does fly if to the best of our knowledge
Tweety was a randomly selected bird.
Probability logics which accomplish this have
already been developed (Bacchus [11,12], Kyburg
[16]), but these logics go beyond the simple de­
vice of assigning probabilities to the sentences
of a logical language. Bacchus uses a logic which
has random variables as well as universally quan­
tified variables, this logic is capable of express­
ing statistical informa-tion, and possesses a sound
and complete proof theory capable of reasoning
with statistical facts. Default inferences are han­
dled by an inductive mechanism which forms de-

1

I
I
I

Acknowledgement

The author is thankful to Henry Kyburg for his
helpful suggestions.

I



Inference in Bayes Nets (BAYES) is an important problem with numerous applications in probabilistic reasoning. Counting the number of satisfying assignments of a propositional formula
(#S AT) is a closely related problem of fundamental theoretical importance. Both these problems,
and others, are members of the class of sum-of-products (S UM P ROD) problems. In this paper
we show that standard backtracking search when augmented with a simple memoization scheme
(caching) can solve any sum-of-products problem with time complexity that is at least as good
any other state-of-the-art exact algorithm, and that it can also achieve the best known time-space
tradeoff. Furthermore, backtracking’s ability to utilize more flexible variable orderings allows us to
prove that it can achieve an exponential speedup over other standard algorithms for S UM P ROD on
some instances.
The ideas presented here have been utilized in a number of solvers that have been applied to
various types of sum-of-product problems. These system’s have exploited the fact that backtracking
can naturally exploit more of the problem’s structure to achieve improved performance on a range
of problem instances. Empirical evidence of this performance gain has appeared in published works
describing these solvers, and we provide references to these works.

1. Introduction
Probabilistic inference in Bayesian Networks (BAYES) is an important and well-studied problem
with numerous practical applications in probabilistic reasoning (Pearl, 1988). Counting the number
of satisfying assignments of a propositional formula (#S AT) is also a well-studied problem that is
of fundamental theoretical importance. These two problems are known to be closely related. In
particular, the decision versions of both #S AT and BAYES are #P-complete (Valiant, 1979b, 1979a;
Roth, 1996), and there are natural polynomial-time reductions from each problem to the other
(Darwiche, 2002; Sang, Beame, & Kautz, 2005b; Chavira, Darwiche, & Jaeger, 2006).
A more direct relationship between these two problems arises from the observation that they
are both instances of the more general “sum of products” problem (S UM P ROD). Perhaps the most
fundamental algorithm for S UM P ROD (developed in a general way by Dechter 1999) is based on
the idea of eliminating the variables of the problem one by one following some fixed order. This
algorithm is called variable elimination (VE), and it is the core notion in many state-of-the-art exact
algorithms for S UM P ROD (and BAYES).
SAT, the problem of determining whether or not a propositional formula has any satisfying
assignments, is also an instance of S UM P ROD, and the original Davis-Putnam algorithm (DP) for
determining satisfiability (Davis & Putnam, 1960) which uses ordered resolution is a version of
c 2009 AI Access Foundation. All rights reserved.

BACCHUS , DALMAO , & P ITASSI

variable elimination. However, DP is never used in practice as its performance is far inferior to
modern versions of the backtracking search based DPLL algorithm (Davis, Logemann, & Loveland,
1962). In fact DP is provably less powerful than modern versions of DPLL equipped with clause
learning (Hertel, Bacchus, Pitassi, & van Gelder, 2008).
This performance gap naturally raises the question of whether or not backtracking search could
be used to solve other types of S UM P ROD problems more efficiently than variable elimination. In
this paper, we present a general algorithmic framework for using backtrack search methods (specifically DPLL) to solve S UM P ROD and related problems.1 We first show that a straightforward adaptation of backtracking for solving S UM P ROD is insufficient. However, by examining the sources
of inefficiency we are able to develop some simple caching schemes that allow our backtracking
algorithm, #DPLL-Cache, to achieve the same performance guarantees as state-of-the-art exact algorithms for S UM P ROD, in terms of both time and space. Furthermore, we prove that backtracking’s
natural additional flexibility allows it to sometimes achieve an exponential speedup over other existing algorithms. Specifically, we present a family of S UM P ROD instances where #DPLL-Cache
achieves an exponential speedup over the original versions of three prominent algorithms for S UM P ROD.
Besides these theoretical results, there are also good reasons to believe that backtracking based
algorithms have the potential to perform much better than their worst case guarantees on problems
that arise from real domains. In fact, subsequent work has investigated the practical application of
the ideas presented here to the problem of counting satisfying assignments, BAYES, and constraint
optimization with very successful results (Sang, Bacchus, Beame, Kautz, & Pitassi, 2004; Sang
et al., 2005b; Sang, Beame, & Kautz, 2005a, 2007; Davies & Bacchus, 2007; Kitching & Bacchus,
2008).
An outline of the paper follows. In Section 2, we define S UM P ROD ; demonstrate that #S AT ,
BAYES, and other important problems are instances of this class of problems; discuss various graphtheoretic notions of width that can be used to characterize the complexity of algorithms for S UM P ROD; and review some core state-of-the-art exact algorithms for S UM P ROD. In Section 3, we
discuss DPLL-based algorithms with caching for solving #S AT and S UM P ROD and provide worst
case complexity bounds for these algorithms. These bounds are the same as the best time and space
guarantees achieved by currently known algorithms. In Section 4, we provide a framework for
comparing our algorithms with other algorithms for S UM P ROD and prove that with caching DPLL
can efficiently simulate known exact algorithms while sometimes achieving super-polynomially
superior performance. In Section 5 we discuss some of the work that has used our algorithmic
ideas to build practical solvers for various problems. Finally, we provide some closing remarks in
Section 6.

2. Background
In this section, we first define the sum-of-products (S UM P ROD) class of problems, and then illustrate how BAYES, #S AT, and some other important problems are instances of S UM P ROD. As we
will show in the rest of the paper, backtracking search equipped with different caching schemes is
1. The notion of “backtracking” over a previous set of commitments can be utilized in other contexts, including in other
algorithms for S UM P ROD. However, here we are referring to the standard algorithmic paradigm of backtracking
search that explores a single tree of partial variable assignments in a depth-first manner. This algorithm has an
extensive history that stretches back over a hundred years (Bitner & Reingold, 1975).

392

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

well suited for solving S UM P ROD. The key computational structure that is exploited by all algorithms for S UM P ROD is then explained and the graph theoretic notion of width that captures this
structure is identified. Different notions of “width” exist, and we present three different definitions
and show that they all yield essentially equivalent measures of complexity. The different definitions
are however very useful in that different algorithms are most easily analyzed using different definitions of width. Finally, we briefly review some of the most important exact algorithms for solving
S UM P ROD and related problems.
2.1 Sum-of-Products
Dechter (1999) has been shown that BAYES and many other problems are instances of a more
general problem called S UM P ROD (sum-of-products). An instance of S UM P ROD is defined by
the tuple hV, F, ⊕, ⊗i, where V is a set of discrete valued variables {X1 , . . . , Xn }, F is a set of
functions {f1 , . . . , fm } with each fi defined over some set of variables Ei ⊆ V, ⊕ is an addition
operator, and ⊗ is a multiplication operator. The range of the functions in F depends on the problem,
with ⊕ and ⊗ being operators over that range such that both are commutative, associative, and ⊗
distributes over ⊕. Typical examples involve functions that range over the boolean domain, with
⊕ being disjunction ∨ and ⊗ being conjunction ∧, or over the reals, with ⊕ and ⊗ being ordinary
addition and multiplication.
Definition 1 (S UM P ROD) Given hV, F, ⊕, ⊗i the S UM P ROD problem is to compute
MM
X1 X2

···

m
MO

fi (Ei ),

Xn i=1

i.e., the sum (⊕) over all values (assignments) of the variables V of the product (⊗) of the functions
F evaluated at those assignments.
A number of well known problems are instances of S UM P ROD. We describe some of them
below.
2.1.1 BAYES :
BAYES is the problem of computing probabilities in a Bayesian Network (BN). Developed by Pearl
(1988), a Bayesian network is a triple (V, E, P) where (V, E) describes a directed acyclic graph,
in which the nodes V = {X1 , . . . , Xn } represent discrete random variables, edges represent direct
correlations between the variables, and associated with each random variable Xi is a conditional
probability table CPT (or function), fi (Xi , π(Xi )) ∈ P, that specifies the conditional distribution of
Xi given assignments of values to its parents π(Xi ) in (V, E). A BN represents a joint distribution
over the random variables V in which the probability of any assignment (x1 , . . . , xn ) to the variables
Q
is given by the equation Pr (x1 , . . . , xn ) = ni=1 fi (xi , π(xi )), where fi (xi , π(xi )) is fi evaluated
at this particular assignment.
The generic BAYES problem is to compute the posterior distribution of a variable Xi given a
particular assignment to some of the other variables α: i.e., Pr (Xi |α). Since Xi has only a finite set
of k values, this problem can be further reduced to that of computing the k values Pr (Xi = dj ∧ α),
j = 1, . . . , k and then normalizing them so that they sum to 1. The values Pr (Xi = dj ∧ α) can
be computed by making all of the assignments in α as well as Xi = dj , and then summing out the
393

BACCHUS , DALMAO , & P ITASSI

other variables from the joint distribution Pr (x1 , . . . , xn ). Given the above product decomposition
of Pr (x1 , . . . , xn ), this is equivalent to reducing the functions fi ∈ P by setting the variables
assigned in α and Xi = dj , and then summing their product over the remaining variables; i.e., it is
an instance of S UM P ROD.
Computing all Marginals It is common when solving BAYES to want to compute all marginals.
That is, instead of wanting to compute just the marginal Pr(Xi |α) for one particular variable Xi ,
we want to compute the marginal for all variables not instantiated by α.
2.1.2 M ARKOV R ANDOM F IELDS
Markov Random Fields or Markov Networks (MN) (Preston, 1974; Spitzer, 1971) are similar to
Bayesian Networks in that they also define a joint probability distribution over a set of discrete
random variables V = {X1 , . . . , Xn } using a set of functions fi , called potentials, each over some
set of variables Ei ⊆ V. In particular, the probability of any assignment (x1 , . . . , xn ) to the variables
is given by the normalized product of the fi evaluated at the values specified by the assignment:
Q
i fi (Ei [x1 , . . . , xn ]). The difficulty is to compute the partition function, or normalizing constant:
Z=

X

···

m
XY

fi (Ei ).

Xn i=1

X1

Computing the partition function is thus an instance of S UM P ROD.
2.1.3 M OST P ROBABLE E XPLANATION
Most Probable Explanation (MPE) is the problem of finding the most probable complete assignment
to the variables in a Bayes net (or Markov net) that agrees with a fixed assignment to a subset of the
variables (the evidence). If the evidence, α, is an instantiation of the variables in E ⊂ V, then MPE
is the problem of computing
max
V −E

m
Y

fi |α (Ei − E),

i=1

where fi |α is the reduction of the function fi by the instantiations α to the variables in E (yielding
a function over the variables Ei − E).
2.1.4 S AT
Let V = {X1 , X2 , . . . , Xn } be a collection of n Boolean variables, and let φ(V) be a k-CNF
Boolean formula on these variables with m clauses {c1 , . . . , cm }. An assignment α to the Boolean
variables V is satisfying if it makes the formula True (i.e., φ(α) = 1). S AT asks, given a Boolean
formula φ(V) in k-CNF, does it have a satisfying assignment? By viewing each clause ci as being a
function of its variables Ei (i.e., it maps an assignment to these variables to TRUE if that assignment
satisfies the clause and to FALSE otherwise), we can see that S AT is equivalent to the instance of
S UM P ROD hV, {c1 , . . . , cm }, ∨, ∧i:
_
X1

···

m
_^

Xn i=1

394

ci (Ei ).

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

2.1.5 #S AT
Given a k-CNF formula φ(V) on the boolean variables V = {X1 , . . . , Xn }, as above, #S AT is
the problem of determining the number of satisfying assignments for φ. By viewing each clause
ci as being a function from its variables Ei to {0, 1} (i.e., it maps satisfying assignments to 1
and falsifying assignments to 0), we can see that #S AT is equivalent to the instance of S UM P ROD
hV, {c1 , . . . , cm }, +, ×i:
X

···

X1

2.1.6 O PTIMIZATION

WITH

m
XY

ci (Ei ).

Xn i=1

D ECOMPOSED O BJECTIVE F UNCTIONS

Let V = {X1 , . . . , Xn } be a collection of finite valued variables, the optimization task is to find
an assignment of values to these variables that maximizes some objective function O(V) (i.e., a
function that maps every complete assignment to the variables to a real value). In many problems
O can be decomposed into a sum of sub-objective functions {f1 , . . . , fm } with each fi being a
function of some subset of the variables Ei . This problem can then be cast as the S UM P ROD
instance hV, {f1 , . . . , fm }, max, +i
max · · · max
X1

Xn

m
X

fi (Ei ).

i=1

2.2 The Computational Complexity of S UM P ROD
S UM P ROD is a computationally difficult problem. For example, #S AT is known to be complete for
the complexity class #P (Valiant, 1979b, 1979a) as is BAYES (Roth, 1996). Many special cases that
are easy for S AT remain hard for #S AT, e.g., Valiant showed that the decision version of #S AT is #P
hard even when the clause size, k, is 2, and Roth (1996) showed that the problem is hard to even
approximate in many cases where S AT is easy, e.g., when φ(V) is monotone, or Horn, or 2-CNF.
Despite this worst case intractability, algorithms for S UM P ROD, e.g., the variable elimination
algorithm presented by Dechter (1999), can be successful in practice. The key structure exploited
by this algorithm, and by most algorithms, is that the functions fi of many S UM P ROD problems are
often relatively local and fairly independent. That is, it is often the case that the sets of variables
Ei that each function fi depends on are small, so that each function is dependent only on a small
“local” set of the variables, and that these sets share only a few variables with each other, so that the
functions fi are fairly independent of each other. The graph theoretic notion of Tree Width is used
to make these intuitions precise.
2.3 Complexity Measures and Tree width
There is a natural hypergraph, H = (V, E), corresponding to any instance hV, F, ⊕, ⊗i of S UM P ROD. In the hypergraph, V corresponds to the set V of variables, and for every function fi with
domain set Ei , there is a corresponding hyperedge, Ei .
The “width” of this hypergraph is the critical measure of complexity for essentially all state-ofthe-art algorithms for #S AT , BAYES, and S UM P ROD. There are three different (and well known)
notions of width that we will define in this section. We will also show that these different notions of
width are basically equivalent. These equivalences are known, although we need to state them and
395

BACCHUS , DALMAO , & P ITASSI

prove some basic properties, in order to analyze our new algorithms, and to relate them to standard
algorithms.
Definition 2 (Branch width) Let H = (V, E) be a hypergraph. A branch decomposition of H is
a binary tree T such that each node of T is labelled with a subset of V . There are |E| many leaves
of T , and their labels are in one-to-one correspondence with the hyperedges E. For any other node
n in T , let A denote the union of the leaf labeling of the subtree rooted at n, and let B denote the
union of the labelings of the rest of the leaves. Then the label for n is the set of all vertices v that
are in the intersection of A and B. The branch width of a branch decomposition T for H is the
maximum size of any labeling in T . The branch width of H is the minimum branch width over all
branch decompositions of H.
Example 1 Figure 1 shows a particular branch decomposition Tbd for the hypergraph H = (V, E)
where V = {1, 2, 3, 4, 5} and E = {{1, 2, 3}, {1, 4}, {2, 5}, {3, 5}, {4, 5}}. Tbd has branch width
3.
{}
H
 HH
HH



{3, 4, 5}

{3, 4, 5}

H
 H

H

HH

{2, 3, 4}

{2, 5}

{3, 5}

{4, 5}

H

H

H

{1, 2, 3}

{1, 4}

Figure 1: A branch decomposition of branch width 3 for H = {(1, 2, 3), (1, 4), (2, 5), (3, 5),
(4, 5)}.

Definition 3 (Elimination width) Let H = (V, E) be a hypergraph, and let π = v1π , . . . , vnπ be an
ordering of the vertices in V , where viπ is the ith element in the ordering. This induces a sequence
of hypergraphs Hn , Hn−1 , . . . , H1 where H = Hn and Hi−1 is obtained from Hi as follows. All
edges in Hi containing viπ are merged into one edge and then viπ is removed. Thus the underlying
π . The induced width of H under π is the size of the largest edge in
vertices of Hi are v1π , . . . vi−1
all the hypergraphs Hn , . . . , H1 . The elimination width of H is the minimum induced width over
all orderings π.
Example 2 Under the ordering π = h1, 2, 3, 4, 5i the hypergraph H of Example 1 produces the
following sequence of hypergraphs:
H5 = {(1, 2, 3), (1, 4), (2, 5), (3, 5), (4, 5)}
H4 = {(2, 3, 4), (2, 5), (3, 5), (4, 5)}
H3 = {(3, 4, 5), (3, 5), (4, 5)}
H2 = {(4, 5), (4, 5)}
H1 = {(5)}
396

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

The induced width of H under π is 3—the edges (1, 2, 3) ∈ H1 , (2, 3, 4) ∈ H2 and (3, 4, 5) ∈ H3
all achieve this size.
Tree width is the third notion of width.
Definition 4 (Tree width) Let H = (V, E) be a hypergraph. A tree decomposition of H is a
binary tree T such that each node of T is labelled with a subset of V in the following way. First,
for every hyperedge e ∈ E, some leaf node in T must have a label that contains e. Secondly, given
labels for the leaf nodes every internal node n contains v ∈ V in its label if and only if n is on a path
between two leaf nodes l1 and l2 whose labels contain v.2 The tree width of a tree decomposition
T for H is the maximum size of any labeling in T minus 1, and the tree width of H is the minimum
tree width over all tree decompositions of H.
Example 3 Figure 2 shows Ttd a tree decomposition for H of Example 1. Ttd has tree width 3.
{3, 4, 5}
H
 HH
HH



{2, 3, 4, 5}
H
HH


{1, 2, 3, 4}

{2, 5}

{3, 4, 5}
HH

{3, 5}

{4, 5}

H

H

H

{1, 2, 3}

{1, 4}

Figure 2: Tree decomposition of tree width for 3 for H of Example 1.
The next three lemmas show that these three notions are basically equivalent. The proofs of
Lemmas 2 and 3 are given in the appendix.
Lemma 1 (Robertson & Seymour, 1991) Let H be a hypergraph. Then the branch width of H is
at most the tree width of H plus 1, and the tree width of H is at most 2 times the branch width of H.
Lemma 2 Let H = (V, E) be a hypergraph with a tree decomposition of width w. Then there is an
elimination ordering π of the vertices V such that the induced width of H under π is at most w.
Lemma 3 Let H be a hypergraph with elimination width at most w. Then H has a tree decomposition of tree width at most w.
Letting TW (H), BW (H), and EW (H) represent the tree width, branch width and elimination
width of the hypergraph H, the above lemmas give the following relationship between these three
notions of width: for all hypergraphs H
BW (H) − 1 ≤ TW (H) = EW (H) ≤ 2BW (H).
2. Since the labels of internal nodes are determined by the labels of the leaf nodes in this way, it can be seen that for any
pair of nodes n1 and n2 in the tree decomposition every node lying on the path between them must contain v in its
label if v appears in both n1 ’s and n2 ’s labels. This is commonly known as the running intersection property of tree
decompositions.

397

BACCHUS , DALMAO , & P ITASSI

{4, 5}
H
 HH
H


{3, 4, 5}

{4, 5}

H
H

HH


{2, 3, 4, 5}

{3, 5}

H
 H

H

{1, 2, 3, 4}

{2, 5}

H

H

H

{1, 2, 3}

{1, 4}

Figure 3: Tree decomposition of the hypergraph H of Example 1 that has been constructed from
the ordering π = h1, 2, 3, 4, 5i.

Example 4 The tree decomposition Ttd of H = {(1, 2, 3), (1, 4), (2, 5), (3, 5), (4, 5)} given in Figure 2 has the property that it has tree width no more than twice the branch width of the branch
decomposition Tbd of H given in Figure 1. From Ttd we can obtain the ordering π = h1, 2, 3, 4, 5i
that was used in Example 2. (The proof of Lemma 2, given in the appendix, shows how a elimination ordering can be constructed from a tree-decomposition.) As shown in Example 2, π has
induced width 3, equal to the tree width of tree decomposition Ttd from which it was constructed.
Finally, from the ordering π we can construct a new tree decomposition for H shown in Figure 3.
(The proof of Lemma 3 shows how a tree decomposition can be constructed from an elimination
ordering). π has induced width 3 and, as indicated by Lemma 3 the tree decomposition constructed
from it has equal tree width of 3.
It can be noted that our definition of tree decompositions varies slightly from other definitions
that appear in the literature, e.g., (Bodlaender, 1993). Following Robertson and Seymour (1991)
we have defined tree decompositions over hypergraphs, rather than over graphs, and we have made
two extra restrictions so as to simplify the proofs of our results. First, we have restricted tree
decompositions to be binary trees, and second we have required that each hyperedge be contained
in the label of some leaf node of the tree decomposition. Usually tree decompositions are not
restricted to be binary trees, and only require that each hyperedge be contained in some node’s label
(not necessarily a leaf node).
It is not difficult to show that any tree decomposition that fails to satisfy our two restrictions
can be converted to a tree decomposition satisfying these restrictions without changing its width.
However, it is more straight forward to observe that with or without these two restrictions tree width
is equal to elimination width. Hence, our restrictions do not change the tree width.
2.4 Exact Algorithms for S UM P ROD
Next we briefly review three prominent exact algorithms for BAYES. These algorithms solve the
more general problem S UM P ROD. All of these algorithms are in fact nondeterministic algorithms
that should be considered to be families of procedures, each member of which is a particular deterministic realization.
398

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

2.4.1 VARIABLE E LIMINATION :
Variable or bucket elimination (VE) (Dechter, 1999) is a fundamental algorithm for S UM P ROD.
Variable elimination begins by choosing an elimination ordering, π for the variables V = {X1 ,
. . ., Xn }: Xπ(1) , . . ., Xπ(n) . (This is the nondeterministic part of the computation). In the first
phase, all functions involving Xπ(1) , are collected together in the set FXπ(1) , and a new function,
F1 is computed by “summing out” Xπ(1) . The new function sums the product of all the functions in
FXπ(1) over all of Xπ(1) ’s values. Specifically, F1 is a function of all of the variables of the functions
in FXπ(1) except for Xπ(1) , and its value on any assignment α to these variables is
F1 (α) =

X

Y

f (α, Xπ(1) = d).

d∈vals(Xπ(1) ) f ∈FXπ(1)

Summing out Xπ(1) induces a new hypergraph, H1 , where the hyperedges corresponding to the set
of functions FXπ(1) are replaced by a single hyperedge corresponding to the new function F1 . The
process then continues to sum out Xπ(2) from H1 and so on until all n variables are summed out.
Note that the sequence of hypergraphs generated by summing out the variables according to π is the
same the sequence of hypergraphs that defines the induced width of π (Definition 3).
The original Davis-Putnam algorithm (Davis & Putnam, 1960) based on ordered resolution is an
instance of variable elimination. Consider applying variable elimination to the formulation of S AT
given above. For S AT, the new functions Fi computed at each stage need only preserve whether or
not the product of the functions in FXπ(i) is 0 or 1, the exact number of satisfying assignments need
not remembered. This can be accomplished by representing the Fi symbolically as a set of clauses.
Furthermore, this set of clauses can be computed by generating all clauses that can be obtained
by resolving on Xπ(i) , and then discarding all old clauses containing Xπ(i) . This resolution step
corresponds to the summing out operation, and yields precisely the Davis-Putnam (DP) algorithm
for satisfiability.3
2.4.2 R ECURSIVE C ONDITIONING :
Recursive conditioning (RC) (Darwiche, 2001) is another type of algorithm for S UM P ROD. Let
S = hV, F, ⊕, ⊗i be an instance of S UM P ROD and H be its underlying hypergraph. RC is a
divide and conquer algorithm that instantiates the variables of V so as to break the problem into
disjoint components. It then proceeds to solve these components independently. The original spaceefficient version of recursive conditioning, as specified by Darwiche (2001), begins with a branch
decomposition T of H of width w and depth d, and an initially empty set of instantiated variables
ρ. (Choosing T is the nondeterministic part of the computation.) We call this algorithm RC-Space
and show it in Algorithm 1.
The branch decomposition T specifies a recursive decomposition of the problem and is used by
RC-Space as follows. Let label (n) be the label of a node in T , and let ST be the S UM P ROD problem
defined by the variables and functions contained in T . (In the initial call T is the complete branch
decomposition containing all variables and functions of S, so that initially ST = S). Starting at r,
the root of T , RC-Space solves the reduced S UM P ROD ST |ρ∪α for all assignments α to the variables
3. Rish and Dechter (2000) have previously made a connection between DP and variable elimination. They were thus
able to show, that DP runs in time nO(1) 2O(w) , where w is the branch width of the underlying hypergraph of the SAT
instance.

399

BACCHUS , DALMAO , & P ITASSI

in label (left(r)) ∩ label (right(r)) not yet instantiated by ρ, where left(r) and right (r) are the left
and right children of r. The sum over all such α is the solution to the inputed instance ST |ρ .
Each α renders the set of functions in the subtree below leftChild (r) (i.e., the leaf labels) disjoint
from the functions below rightChild (r). Thus for each α, RC-Space can independently solve the
subproblems specified by leftChild (r)|ρ∪α and rightChild (r)|ρ∪α (i.e., the sum of the products
of all of the functions below the left/right subtree conditioned on the instantiations in ρ ∪ α) and
multiply their answers to obtain the solution to ST |ρ∪α . At the leaf nodes, the function fi associated
with that node has had all of its variables instantiated, so the algorithm can simply “LOOKUP” fi ’s
current value.
Algorithm 1: RC-Space—Linear Space Recursive Conditioning
1
2
3
4
5
6
7
8
9
10
11

RC-Space (T, ρ)
begin
if T is a leaf node then
return LOOKUP(value of function labeling the leaf node)
p = 0; r = root (T )
~x = variables in label (left(r)) ∩ label (right(r)) uninstantiated by ρ
forall α ∈ {instantiations of ~x} do
p = p + RC-Space (leftChild (T ), ρ ∪ α) × RC-Space (rightChild (T ), ρ ∪ α)
end
return p
end

A less space-efficient but more time-efficient version of recursive conditioning, called RCCache, caches intermediate values that can be reused to reduce the computation. Algorithm 2
shows the RC-Cache algorithm. Like RC-Space, each invocation of RC-Cache solves the subproblem specified by the variables and functions contained in the passed subtree T . Since the functions
below T only share the variables in label (root(T )) with variables outside of T , only the instantiations in the subset, y, of ρ intersecting label (root(T )) can affect the form of this subproblem.
Hence, RC-Cache will return the same answer if invoked with the same T and same y, even if other
assignments in ρ have changed. RC-Cache, can thus use T and y to index a cache, storing the computed result in the cache (line 13) and returning immediately if the answer is already in the cache
(line 7).
Propagation Since RC instantiates the problem’s variables, propagation can be employed. That
is, RC can perform additional inference to compute some of the implicit effects each assignment
has on the remaining problem ST |ρ∪α . For example, if the functions of the S UM P ROD problem
are all clauses (e.g., when solving #S AT) unit propagation can be performed. Propagation can
make recursive conditioning more effective. For example, if one of the remaining clauses becomes
falsified through unit propagation, recursive conditioning can immediately move on to the next
instantiation of the variables ~x. Similarly, unit propagation can force the value of variables that will
be encountered in subsequent recursive calls, thus reducing the number of different instantiations α
that must be attempted in that recursive call. It can be noted that propagation does not reduce the
worst case complexity of the algorithm, as on some S UM P ROD problems propagation is ineffective.
It can however improve the algorithm’s efficiency on some families of problems.
400

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

Algorithm 2: RC-Cache—Recursive Conditioning with caching
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

RC-Cache (T, ρ)
begin
if T is a leaf node then
return LOOKUP(value of function labeling the leaf node)
y = ρ ∩ label (root(T ))
if InCache(T, y) then
return GetValue(T, y)
p = 0; r = root (T )
~x = variables in label (left(r)) ∩ label (right(r)) uninstantiated by ρ
forall α ∈ {instantiations of ~x} do
p = p + RC-Cache (leftChild (T ), ρ ∪ α) × RC-Cache (rightChild (T ), ρ ∪ α)
end
AddToCache((T ,y), p)
return p
end

RC-Cache+ A simple extension of RC that is used in practice is to set the variables ~x ⊆
label (left(r) ∩ label (right (r)) (line 10 of Algorithm 2) iteratively rather than all at once. That
is, rather than iterate over all complete assignments α to ~x we can instantiate these variables one at
a time, performing propagation after each assignment. This can make propagation more effective,
since, e.g., an empty clause might be detected after instantiating only a subset of the variables in ~x
and thus the number of iterations of the for loop might be reduced.
Once the variables of ~x are being set iteratively the order in which they are assigned can vary.
Furthermore, the order of assignment can vary dynamically. That is, depending on how the values assigned to the first k variables of ~x, the algorithm can make different choices as to which
unassigned variable of ~x to assign next.
We call the extension of RC-Cache that uses incremental assignments and dynamic variable ordering within set ~x, RC-Cache+ . That is RC-Cache+ uses the same caching scheme as RC-Cache,
but has more flexibility in its variable ordering. It should be noted however, that RC-Cache+ does
not have complete freedom in its variable ordering. It must still follow the inputed branch decomposition T . That is, the variable chosen must come from the set ~x ⊆ label (left(r)) ∩ label (right (r)).
This is in contrast with the DPLL based algorithms we present in the next section, which are always
free to choose any remaining unassigned variable as the next variable to assign.
Space-Time Tradeoff RC has the attractive feature that it can achieve a non-trivial space-time
tradeoff, taking less time if it caches its recursively computed values (RC-Cache), or taking less
space without caching (RC-Space). In fact, Darwiche and Allen (2002) show that there is a smooth
tradeoff that can be achieved, with RC-Space and RC-Cache at the two extremes.
The DPLL based algorithms presented here share a number of features with RC; they also reduce
and decompose the input problem by making instantiations, gain efficiency by caching, and achieve
a similar space-time tradeoff. However, our algorithms are based on the paradigm of backtracking,
rather than divide and conquer. In particular, they explore a single backtracking tree in which
the decomposed subproblems are not solved separately but rather can be solved in any interleaved
401

BACCHUS , DALMAO , & P ITASSI

fashion. As a result, they are not limited to following the decomposition scheme specified by a fixed
branch decomposition. As we will see, the limitation of a static decomposition scheme means that
RC-Space and RC-Cache must perform exponentially worse than our algorithms on some instances.
2.4.3 AND/OR S EARCH :
In more recent work Dechter and Mateescu (2007) have shown that the notion of AND/OR search
spaces (Nilsson, 1980) can be applied to formalize the divide and conquer approach to S UM P ROD
problems utilized by RC. In this formulation the structure that guides the AND/OR search algorithm
is a pseudo tree. (Choosing the pseudo tree is the nondeterministic part of the computation.)
Definition 5 (Primal Graph) The primal graph of a hypergraph H is an undirected graph G that
has the same vertices as H and has an edge connecting two vertices if and only if those two vertices
appear together in some hyperedge of H.
Definition 6 (Pseudo Tree) Given an undirected graph G with vertices and edges (V, EG ), a pseudo
tree for G is a directed rooted tree T with vertices and edges (V, ET ) (i.e., the same set of vertices as
G), such that any edge e that is in G but not in T must connect a vertex in T to one of its ancestors.
That is, e = (v1 , v2 ) ∧ e ∈ EG ∧ e 6∈ ET implies that either v1 is an ancestor of v2 in T or v2 is an
ancestor of v1 in T .
This implies that there is no edge of G connecting vertices lying in different subtrees of T .
Given a S UM P ROD problem S = hV, F, ⊕, ⊗i with underlying hypergraph H, we can form G, the
primal graph of H. The vertices of G are the variables of the problem V and any pair of variables
that appear together in some function of F will be connected by an edge in G. A pseudo tree T for
G will then have the property that two vertices of T (variables of S) can only appear in functions of
F with their ancestors or their descendants, they cannot appear in functions with their siblings nor
with their ancestor’s siblings nor with the descendants of such siblings.
This implies that once a variable v and all of its ancestors in T have been instantiated, the variables contained in its children subtrees become disconnected. That is, the variables in these subtrees
no longer appear in functions together, and the resulting subproblems can be solved independently.
The AND/OR search algorithm utilizes this fact to solve these subproblems independently, just like
recursive conditioning.
Example 5 Given the hypergraph H = (V, E) where V = {1, 2, 3, 4, 5} and E = {{1, 2, 3},
{1, 4},{2, 5}, {3, 5}}, the primal graph of H is G = (V, EG ) where EG = {(1, 2), (1, 3), (2, 3),
(1, 4), (2, 5), (3, 5)}. H, its primal graph G, and a pseudo tree for G are shown in Figure 4. The
dotted lines shown on the pseudo tree are the edges of G that are not in the pseudo tree. As can be
seen from the diagram these edges connect nodes only with their ancestors.
The space efficient version of the AND/OR-Space search algorithm (Dechter & Mateescu,
2007) is shown in Algorithm 3. It solves the S UM P ROD instance S = hV, F, ⊕, ⊗i, taking as
input a pseudo tree for the problem T (i.e., the hypergraph for S is converted to a primal graph
G, and T is a pseudo tree for G), and an initially empty set of instantiated variables ρ. The algorithm solves a sub-problem of the original instance S reduced by the instantiations ρ, S|ρ . The
sub-problem being solved is defined by the functions of S|ρ that are over the variables contained in
the passed sub-tree T . Initially, with ρ being empty and T being the original pseudo tree containing
all variables, the algorithm solves the original problem S.
402

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

1
1

2

3

1

2

3
4

4

5

Hypergraph

4

5

2
3
5

Primal Graph
Pseudo Tree

Figure 4: The hypergraph, primal graph, and a pseudo tree for Example 5.

The nodes of the pseudo tree T are variables of the problem S, and we also attach to each node
n of T a set of functions fns(n). A function f of F is in fns(n) if and only if (a) n is in the scope of
f and (b) all other variables in the scope of f are ancestors of n in T . This means that f will have a
fully instantiated set of arguments when AND/OR search instantiates the node (variable) n.
Algorithm 3: AND/OR-Space—Linear Space AND/OR search
1
2
3
4
5
6
7
8
9
10

AND/OR-Space (T, ρ)
begin
p = 0; r = root (T )
STr = set of subtrees below r
forall d ∈ {instantiations of r} do
Q
α = f ∈fns(r) LOOKUP(value of f on ρ ∪ {r = d})
Q
p = p + α × T ′ ∈STr AND/OR-Space (T ′ , ρ ∪ {r = d})
end
return p
end

The algorithm operates on the variable r that is the root of the pseudo tree T . For each instantiation of r the algorithm computes α, the product of the functions in F that have now become fully
instantiated by the assignment to r, i.e., those in fns(r). It then invokes a separate recursion for each
child of r passing the subtree rooted by that child to the recursive call. AND/OR search exploits
decomposition through these separate recursions. If r has only one child, then the problem is not
decomposed—there is only the single reduced subproblem that has resulted from instantiating r.
Like RC, AND/OR search can be made more time efficient at the expense of using more space.
Algorithm 4 shows the caching version AND/OR-Cache (called AND/OR graph search by Dechter
and Mateescu (2007)). Let label (n) for any node n in the pseudo tree T be the set of ancestors
of n that appear in some function with n or with some descendant of n in T . It is only the instantiations to label (n) that can affect the functions over the variables in the subtree rooted by n.
Hence, label (n) plays the same role as the root label of the passed branch decomposition in RCCache: only instantiations to these variables can affect the subproblem currently being computed.
403

BACCHUS , DALMAO , & P ITASSI

Hence, like RC-Cache, AND/OR-Cache can use the instantiations in the subset, y, of ρ intersecting
label (root(T )) along with T to index a cache.
Finally, as with RC-Cache+ , propagation can be used to decrease the number of branches that
AND/OR search needs to explore. For example, the recursive calls over the children of r can be
terminated when one of these calls returns the value zero.
Algorithm 4: AND/OR-Cache—AND/OR search with caching
1
2
3
4
5
6
7
8
9
10
11
12
13

AND/OR-Cache (T, ρ)
begin
p = 0; r = root (T )
y = ρ ∩ label (root (T ))
if InCache(T, y) then
return GetValue(T, y)
STr = set of subtrees below r
forall d ∈ {instantiations of r} do
Q
α = f ∈fns(r) LOOKUP(value of f on ρ ∪ {r = d})
Q
p = p + α × T ′ ∈STr AND/OR-Cache (T ′ , ρ ∪ {r = d})
end
return p
end

AND/OR-Cache+ Some variable order dynamism can be employed during AND/OR search. In
particular, the variables along any chain in the pseudo tree T can be reordered without affecting
the decompositions specified by T . A chain is a sub-path of T such that none of its nodes, except
perhaps the last, have more than one child. In Figure 4 nodes 2, 3, and 5 form a chain. The resultant
extension, AND/OR-Cache+ , can dynamically chose to next instantiate any of the variables in
the chain that starts at the root of its passed pseudo tree T . (Marinescu and Dechter (2006) refer
to AND/OR-Cache+ as “AND/OR with partial variable ordering”. However they did not utilize
caching in their version of the algorithm.)
It will then pass the rest of the chain (and the nodes below) to its next recursive call, or if the
chosen variable was the last in the chain it will invoke a separate recursive call for each child. Like
RC-Cache+ , AND/OR-Cache+ does not have complete freedom in its choice of variable—it must
chose a variable from the top most chain. Furthermore, AND/OR-Cache+ can only use its caching
scheme at the bottom of each chain (i.e., after all variables in the chain have been instantiated) since
its cache requires that the same set of variables be instantiated. This makes AND/OR-Cache+ very
similar to RC-Cache+ .
2.4.4 OTHER E XACT A LGORITHMS
The algorithm most commonly used for BAYES is the join tree algorithm (Lauritzen & Spiegelhalter, 1988), which can also be adapted to solve other kinds of S UM P ROD problems. The join-tree
algorithm first organizes the primal graph of the S UM P ROD problem into a tree by clustering the
variables, and it then performs message passing on the tree where the messages are computed by a
variable elimination process. In the context of BAYES the main advantage of join-tree algorithms is
404

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

that they compute all marginals. That is they compute the posterior probability of all of the variables
given some evidence.
In contrast, the default version of variable elimination computes only the posterior distribution
for a single variable. However, Kask et al. (2005) show how the join-tree algorithm can be reduced
to a version of VE that remembers some of its intermediate results and runs in the same time and
space as VE. Hence, all of the results we state here comparing VE with our new backtracking based
algorithms also hold for the join tree algorithm.
Computing all Marginals All of the algorithms described above, i.e., VE, RC, and AND/OR
search, can be modified to compute all marginals when solving BAYES without any change to their
worst case complexity. In particular, besides the results of Kask et al. (2005), Darwiche (2001) has
shown that RC can compute all marginals on BAYES problems with an extra bottom up traversal
of its search tree—at most doubling its run time. The same technique can be applied to AND/OR
search algorithms. For the DPLL algorithms we present here, Sang et al. (2005b) have given an
even simpler scheme for modifying them so that they can computing all marginals. Sang et al.’s
scheme involves maintaining some extra information during search and does not require an extra
traversal of the search tree.
Another algorithm that has now been mostly superseded is cut-set conditioning (Pearl, 1988).
Here the idea is to identify a subset of variables which when set reduce the underlying hypergraph
of the S UM P ROD into a tree. The reduced S UM P ROD can then be easily solved. However, the
approach requires trying all possible instantiations of the cut-set yielding a runtime that is usually
worse than RC-Cache. Nevertheless, cutset conditioning can potentially be applied in conjunction
with other exact algorithms (Mateescu & Dechter, 2005).
Finally, an important early algorithm called DDP was presented by Bayardo and Pehoushek
(2000). This was a version of DPLL that utilized dynamic decomposition for solving #S AT. In
terms of the algorithms discussed above, AND/OR-Space can be viewed as being an version of
DDP that utilizes a pseudo tree to guide its variable ordering. In the original presentation of DDP,
any variable ordering could be used including dynamic variable orderings. The search continued
until the problem was decomposed into independent components (tested for during search) at which
point a separate recursion was used to solve each component. Hence, the DDP explored an AND/OR
search tree, however this tree need not correspond to any pseudo tree over the original problem.
(The DVO and DSO AND/OR search schemes presented by Mateescu and Dechter (2005) are also
versions of DDP run with particular variable ordering heuristics). In comparison with the algorithms
we present in the next section, Bayardo and Pehoushek (2000) did not provide a complexity analysis
of DDP, DDP did not use caching to enhance its performance, and DDP still has less flexibility in its
variable ordering. In particular, once the problem has been split into independent components the
search must solve these components sequentially in separate recursions. Inside each recursion the
search can only branch on the variables of the current component. That is, DDP cannot interleave
the solution of these components like the DPLL algorithms we present here.
2.5 Complexity Analysis
All known algorithms for BAYES, #S AT and S UM P ROD run in exponential-time in the worst case.
However, when the branch width of the underlying hypergraph of the instance, w, is small, the
some of the above algorithms are much more efficient. It can be shown that the algorithms VE, RCCache and AND/OR-Cache discussed above run in time and space nO(1) 2O(w) . We note that the
405

BACCHUS , DALMAO , & P ITASSI

complexity of these algorithms is usually given in terms of tree width or elimination width, and not
branch width. However, by Lemmas 1, 2, and 3, these concepts are equivalent to within a factor of
2, and therefore the asymptotic complexity can equivalently be stated in terms of any of these three
notions of width (tree width, branch width, or elimination width). For analyzing our backtracking
algorithms, branch width is be somewhat more natural, and for this reason we have chosen to state
all complexity results in terms of branch width.
The runtime of the variable elimination algorithm is easily seen to be at most nO(1) 2O(w) . To
see this, notice that the algorithm proceeds in n stages, removing one variable at each stage. Suppose that the algorithm is run on some variable ordering that has elimination width v. The algorithm
removes the ith variable during the ith stage. At the ith stage, all functions involving this variable
are merged to obtain a new function. As indicated in Section 2.4.1, computing the new function
involves iterating overall possible instantiations of its variables. The runtime of this stage is therefore exponential in the number of underlying variables of the new function, which is bounded by v.
Thus, the runtime of the algorithm is bounded by nO(1) 2O(v) . Now by Lemmas 1 and 2 and 3, if the
elimination width is v, then the branch width is at most v + 1, and therefore the overall runtime is
as claimed. It can also be noted that since the new function must be stored, the space complexity of
variable elimination is the same as its time complexity, i.e., nO(1) 2O(w) .
It has also been shown that the run times of RC-Cache and RC-Cache+ are bounded by nO(1) 2O(w)
(Darwiche, 2001). Further, there is a nice time-space tradeoff. That is, the space-efficient implementation of RC, RC-Space, runs in time 2O(w log n) but needs only space linear in the size of the
input, where as RC-Cache has space complexity equal to its time complexity, nO(1) 2O(w) . We will
present proofs showing that our DPLL based algorithms can achieve the same time and time/space
bounds; our proofs give the bounds for RC-Space, RC-Cache, and RC-Cache+ as special cases.
Finally, it has been shown that AND/OR-Space runs in time 2O(w log n) (Dechter & Mateescu,
2007). Specifically, Dechter and Mateescu show that AND/OR-Space runs in time exponential in the
height of its inputed pseudo tree, and Bayardo and Miranker (1995) show that this height is bounded
w log n. Lemma 1 then shows that the bound also holds for branch width. Similarly, Dechter and
Mateescu (2007) show that AND/OR-Cache runs in time and space bounded by nO(1) 2O(w) by
exploiting the very close relationship between pseudo trees and elimination orders.
Making the algorithms deterministic. As stated above, all of these algorithms are in fact nondeterministic algorithms each requiring a different nondeterministically determined input. Hence, the
stated complexity bounds mean that there exists some choice of nondeterministic input (i.e., some
variable ordering for VE, some branch decomposition for RC, and some pseudo tree for AND/OR
search) with which the algorithm can achieve the stated complexity bound.
However, to achieve this runtime in practice, we will need to be able to find such a good branch
decomposition (variable ordering, pseudo tree) efficiently. Unfortunately, the general problem of
computing an optimal branch decomposition (i.e., one that has width equal to the branch width of
H) is NP-complete. However, Robertson and Seymour (1995) present an algorithm for computing
a branch decomposition with branch width that is within a factor of 2 of optimal and that runs in
time nO(1) 2O(w) , where w is the branch width of H. By first running this deterministic algorithm
to compute a good branch decomposition, one can obtain deterministic versions of RC-Cache and
RC-Cache+ that run in time and space nO(1) 2O(w) , as well as a deterministic version of RC-Space
that runs in linear space and time 2O(w log n) . These deterministic versions no longer require access
to a nondeterministically determined choice to achieve their stated runtimes.
406

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

Algorithm 5: DPLL for SAT
1
2
3
4
5
6
7
8
9
10

DPLL (φ)
begin
if φ has no clauses then
return TRUE
else if φ contains an empty clause then
return FALSE
else
choose a variable x that appears in φ
return (DPLL(φ|x=0 ) ∨ DPLL(φ|x=1 ))
end

Similarly with a nearly optimal branch decomposition, we can use Lemmas 1-3 to find a nearly
optimal elimination ordering, and thus can obtain a deterministic version of the variable elimination
algorithm that runs in time and space nO(1) 2O(w) . And finally, from that nearly optimal elimination
ordering the bucket-tree construction of Dechter and Mateescu (2007) can be used to construct a
nearly optimal pseudo tree, and thus we can obtain a deterministic version of AND/OR-Space that
runs in linear space and time 2O(w log n) , and a deterministic version of AND/OR-Cache that runs in
time and space nO(1) 2O(w) .

3. Using DPLL for #S AT and S UM P ROD
Now we present our methods for augmenting backtracking search with different caching schemes
so that it can solve S UM P ROD with time and space guarantees at least as good as the other exact
algorithm for S UM P ROD. For ease in presentation we present DPLL-based algorithms for solving
#S AT, and derive complexity results for these algorithms. Later we will discuss how the algorithms
and complexity results can be applied to other instances of S UM P ROD (like BAYES).
3.1 DPLL and #DPLL:
DPLL is a nondeterministic algorithm for S AT, that has also been used to solve various generalizations of S AT, including #S AT (Dubois, 1991; Zhang, 1996; Birnbaum & Lozinskii, 1999; Littman,
Majercik, & Pitassi, 2001). DPLL solves S AT by performing a depth-first search in the space of
partial instantiations (i.e., it is a standard backtracking search algorithm). The nondeterministic part
of the computation is lies in the choice of which variable to query (i.e., instantiate) next during its
search. It operates on S AT problems encoded in clause form (CNF).
The standard DPLL algorithm for solving S AT is given in Algorithm 5. We use the notation
φ|x=0 or φ|x=1 to denote the new CNF formula obtained from reducing φ by setting the variable x
to 0 or 1. Reducing φ by x = 1 (x = 0) involves removing from φ all clauses containing x (¬x)
and removing the falsified ¬x (x) from all remaining clauses.
DPLL is a nondeterministic procedure that generates a decision tree representing the underlying
CNF formula. For solving S AT, the decision tree is traversed in a depth-first manner until either a
satisfying path is encountered, or until the whole tree is traversed (and all paths falsify the formula).
The nondeterminism of the algorithm occurs in the choice of variable on line 8. In practice this
407

BACCHUS , DALMAO , & P ITASSI

Algorithm 6: #DPLL for #S AT (no caching)
1
2
3
4
5
6
7
8
9
10

#DPLL (φ)
// Returns the probability of φ
begin
if φ has no clauses then
return 1
else if φ contains an empty clause then
return 0
else
choose a variable x that appears in φ
return ( 12 #DPLL(φ|x=0 ) + 21 #DPLL(φ|x=1 ))
end

nondeterminism is typically resolved via some heuristic choice. Also, the algorithm utilizes early
termination of the disjunctive test on line 9; i.e., if the first test returns TRUE the second recursive
call is not made. Thus, the algorithm stops on finding the first satisfying path.
Note that we do not require that DPLL perform unit propagation. In particular, unit propagation
can always be realized through the choice of variable at line 8. In particular, if we force DPLL to
always chose a variable that appears in a unit clause of φ whenever one exists, this will have the
same effect as forcing DPLL to perform unit propagation after every variable instantiation. That is,
after a variable is chosen, and instantiated to one of its values, the input CNF φ will be reduced. The
reduced formula, φ|x=0 or φ|x=1 , passed to the next recursive call may contain unit clauses. With
unit propagation, the variables in these clauses would be instantiated so as to satisfy the unit clauses.
If instead, we force one of these variable to be chosen next, one instantiation would immediately
fail due to the generation of an empty clause, while the other would instantiate the variable to the
same value as unit propagation. Hence, since we analyze DPLL as a nondeterministic algorithm,
this includes those deterministic realizations that perform unit propagation.
A simple modification of DPLL allows it to count all satisfying assignments. Algorithm 6 gives
the #DPLL algorithm for counting. The algorithm actually computes the probability of the set of
satisfying assignments under the uniform distribution. Hence, the number of satisfying assignments
can be obtained by multiplying this probability by 2n , where n is the number of variables in φ. The
alternative would be to return 2 raised to the number of unset variables whenever φ has no clauses
(line 4) and not multiply the recursively computed counts by 21 (line 9).
Known exponential worst-case time bounds for DPLL also apply to #DPLL: for unsatisfiable
formulas, both algorithms have to traverse an entire decision tree before terminating. Although this
decision tree can be small (e.g., when an immediate contradiction is detected), for some families of
formulas the decision tree must be large. In particular, it is implicit in the results of Haken (1985)
that any decision tree for the formulas encoding the (negation of the) propositional pigeonhole
principle has exponential size, and thus DPLL and #DPLL must take exponential-time on these
examples. This lower bound does not, however, help us discriminate between algorithms since
all known algorithms for #S AT and BAYES take exponential-time in the worst-case. Nevertheless,
#DPLL requires exponential time even on instances that can be efficiently solved by competing
algorithms for S UM P ROD. To see this, consider a 3CNF formula over 3n variables consisting of
408

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

n clauses that share no variables. Any complete decision tree has exponential size, and therefore
#DPLL will require exponential time. In contrast, since this formula has low tree width it can be
solved in polynomial time by VE, RC, or AND/OR search.
3.2 DPLL with Caching:
Given that the obvious application of DPLL to solve S UM P ROD can give exponentially worse
performance than the standard algorithms, we now examine ways of modifying DPLL so that it
can solve #S AT (and thus BAYES and S UM P ROD) more efficiently. To understand the source of
#DPLL’s inefficiency consider the following example.
Example 6 The following diagram shows a run of #DPLL on φ = {(w ∨ x)(y ∨ z)}. Each node
shows the variable to be branched on, and the current formula #DPLL is working on. The left hand
branches correspond to setting the branch variable to FALSE, while on the right the variable is set
to TRUE. The empty formula is indicated by {}, while a formula containing the empty clause is
indicated by {()}. The diagram shows that #DPLL encounters and solves the subproblem {(y ∨ z)}
twice: once along the path (w = 0, x = 1) and again along the path (w = 1). Note that in this
example unit propagation is realized by the choice of variable ordering—after w is set to FALSE,
#DPLL chooses to instantiate the variable x since that variable appears in a unit clause.
w:{(x ∨ w))(y ∨ z)}
H
 HH

HH

HH


H

x:{(x)(y ∨ z)}

y:{(y ∨ z)}

H
 HH

HH


H
 HH

0:{()}

y:{(y ∨ z)}
H

H

H

z:{(z)}

z:{(z)}

1:{}

HH

0:{()}

1:{}

1:{}

HH

0:{()}

1:{}

If one considers the above example of applying #DPLL to disjoint sets of clauses, it becomes
clear that in some formulas #DPLL can encounter the same subproblem an exponential number of
times.
3.2.1 DPLL

WITH

S IMPLE C ACHING (#DPLL-S IMPLE C ACHE )

One way to prevent this duplication is to apply memoization. As indicated in Example 6, associated
with every node in the DPLL tree is a formula f such that the subtree rooted at this node is trying
to compute the number of satisfying assignments to f . When performing a depth-first search of
the tree, we can keep a cache that contains all formulas f that have already been solved, and upon
reaching a new node of the tree we can avoid traversing its subtree if the value of its corresponding
formula is already stored in the cache.
In Example 6 we would cache {(y ∨ z)}, when we solve it along the path (w = 0, x = 1)
thereby avoid traversing the subtree below (w = 1).
409

BACCHUS , DALMAO , & P ITASSI

Algorithm 7: #DPLL algorithm with simple caching (#DPLL-SimpleCache)
1
2
3

4
5
6
7
8
9
10

#DPLL-SimpleCache (φ)
// Returns the probability of φ
begin
if InCache(φ) then
// Also detects obvious formulas.
return GetValue(φ)
else
choose a variable x that appears in φ
val = 12 #DPLL-SimpleCache (φ|x=0 ) + 21 #DPLL-SimpleCache (φ|x=1 )
AddToCache(φ,val )
return val
end

The above form of caching, which we will call simple caching (#DPLL-SimpleCache) can be
easily implemented as shown in Algorithm 7.4 As with #DPLL, #DPLL-SimpleCache returns the
probability of its input formula φ; multiplying this by 2n gives the number of satisfying assignments.
In addition to formulas stored in the cache there are also the following obvious formulas whose
value is easy to compute. (1) The empty formula {} containing no clauses has value 1. (2) Any
formula containing the empty clause has value 0. Obvious formulas can be treated as if they are
implicitly stored in the cache (they need not be explicitly stored in the cache, rather their values can
be computed as required).
The following (low complexity) subroutines are used to access the cache. (1) AddToCache(φ, r):
adds to the cache the fact that formula φ has value r. (2) InCache(φ): takes as input a formula φ
and returns true if φ is in the cache. (3) GetValue(φ): takes as input a formula φ known to be in the
cache and returns its stored value. There are various ways of computing a cache key from φ. For
example, φ can be maintained as a sorted set of sorted clauses, and then cached as if it was a text
string. Such a caching scheme has nO(1) complexity.
Surprisingly, simple caching, does reasonably well. The following theorem shows that simple
caching achieves runtime bounded by 2O(w log n) , where w is the underlying branch width. As with
our complexity analysis of earlier algorithms presented in Section 2.5, the simple caching algorithm
can also be made deterministic by first computing a branch decomposition that is within a factor
of 2 of optimal (using the Robertson-Seymour algorithm), and then running #DPLL-SimpleCache
with a variable ordering determined by this branch decomposition.
Theorem 1 For solving #S AT with n variables, there is an execution of #DPLL-SimpleCache that
runs in time bounded by 2O(w log n) where w is the underlying branch width of the instance. Furthermore, the algorithm can be made deterministic with the same time guarantees.
Although the theorem shows that #DPLL-SimpleCache does fairly well, its performance is not
quite as good as the best S UM P ROD algorithms (which run in time nO(1) 2O(w) ).
4. Simple caching has been utilized before (Majercik & Littman, 1998), but without theoretical analysis.

410

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

Algorithm 8: #DPLL algorithm with component caching (#DPLL-Cache)
1
2
3

4
5
6
7
8
9
10
11
12
13
14
15
16

#DPLL-Cache (Φ)
// Returns the probability of the set of disjoint formulas Φ
begin
if InCache(Φ) then
// Also detects obvious formulas.
return GetValue(Φ)
else
Ψ = RemoveCachedComponents(Φ)
choose a variable x that appears in some component φ ∈ Ψ
Ψ− = ToComponents(φ|v=0 )
#DPLL-Cache (Ψ − {φ} ∪ Ψ− )
Ψ+ = ToComponents(φ|v=1 )
#DPLL-Cache (Ψ − {φ} ∪ Ψ+ )
AddToCache(φ, 21 GetValue(Ψ− ) + 12 GetValue(Ψ+ ))
if #DPLL-Space then
RemoveFromCache(Ψ− ∪ Ψ+ )
return GetValue(Φ)
end

3.2.2 DPLL

WITH

C OMPONENT C ACHING (#DPLL-C ACHE )

Now we show that a more sophisticated caching scheme allows #DPLL to perform as well as the
best known algorithms. We call the new algorithm #DPLL-Cache, and its implementation is given
in Algorithm 8.
In the algorithm we generalize the cache to deal with sets of formulas. First, we say that a
(single) formula φ is known if its value is stored in the cache or it is an obvious formula (and its
value is implicitly stored in the cache). Given a set of formulas Φ we say that the set is known if
either every φ ∈ Φ is known, or there is some φ ∈ Φ whose value is known to be zero. In both cases
we say that Φ’s value is equal to the product of the values of the φ ∈ Φ.
Now we generalize some of the cache access subroutines. (1) InCache(Φ) is generalized so that
it can take as input a set of formulas Φ. It returns true if Φ is known as just defined. (2) Similarly
GetValue(Φ) is generalized to take sets of formulas as input. It returns the product of the cached
values of the formulas φ ∈ Φ.
The intuition behind #DPLL-Cache is to recognize that as variables are set the input formula
may become broken up into disjoint components, i.e., sets of clauses that share no variables with
each other. Since these components share no variables we can compute the number of solutions to
each component and multiply the answers to obtain the total solution count. Thus, it is intended
that GetValue be called with a set of disjoint components Φ. In that case it will correctly return the
solution count for Φ—i.e., the product of the solution counts for each φ ∈ Φ.
The algorithm creates a standard DPLL tree, however it caches component formulas as their
values are computed. It keeps its input in decomposed form as a set of disjoint components, and
if any of these components are already in the cache (and thus their value is known) it can remove
411

BACCHUS , DALMAO , & P ITASSI

these parts of the input—reducing the size of the problem it still has to solve and avoiding having
to resolve these components.
The new algorithm uses the previously defined cache access subroutines along with two additional (low complexity) subroutines. (1) ToComponents(φ): takes as input a formula φ, breaks it
up into a set of minimal sized disjoint components, and returns this set. (2) RemoveCachedComponents(Φ): returns the input set of formulas Φ with all known formulas removed. The input to
#DPLL-Cache is always set of disjoint formulas. Hence, to run #DPLL-Cache on the input formula
φ we initially make the call #DPLL-Cache (ToComponents(φ)).
ToComponents simply computes the connected components of the primal graph generated by
φ. That is, in this graph all of the variables of φ are nodes, and two nodes are connected if and only
if the corresponding variables appear together (in any polarity) in a clause of φ. Each connected
component of this primal graph (which can be computed with a simple depth-first traversal of the
graph Cormen, Leiserson, Rivest, & Stein, 2001), defines a set of variables whose clauses form an
independent component of φ.
Each call of #DPLL-Cache completes with the solution of the unknown components from the
set of inputed components Φ. If all components of Φ are known the product of the values of these
components will be returned at line 4. Otherwise the input set of components is reduced by removing all known components (line 6), which must leave at least one unknown component and
potentially reduces the size of the remaining problem to be solved. Then a variable from some unsolved component is chosen and is branched on. Since the variable only appears in the component
φ its assignment can only affect φ. In particular, its assignment might break φ into smaller components (line 8 and 11). The recursive call will solve all components it is passed, so after the two
recursive calls the value of φ can be computed and cached (line 12). Finally, since all components
in the inputed set Φ are now solved its value can be retrieved from the cache and returned.
Example 7 Figure 5 illustrates the behavior of #DPLL-Cache on the formula φ = {(a, b, c, x),
(¬a, b, c), (a, ¬b, c), (d, e, f, x), (¬d, e, f ), (d, ¬e, f )}. Although the problem could be solved
with a simpler search tree, we use a variable ordering that generates a more interesting behavior.
Each node shows the variable to be branched on, and the current set of components #DPLLCache is working on. The known components (i.e., those already in the cache) are marked with an
asterisk (∗ ). The branch variables are set to FALSE on the left branch and TRUE on the right branch.
The empty formula is indicated by {}, while a formula containing the empty clause is indicated
by {()}. To simply the diagram we use unit propagation to simplify the formula after the branch
variable is set. This avoids the insertion into the diagram of nodes where unit clause variables are
branched on. Finally, note that known formulas are removed before a recursive call is made, as per
line 6 of Algorithm 8).
At the root, once x has been set to false, φ is broken up into two components φa,b,c = {(a, b, c),
(¬a, b, c), (a, ¬b, c)}, and φd,e,f = {(d, e, f ), (¬d, e, f ), (d, ¬e, f )}. The search tree demonstrates
that it does not matter how the search interleaves branching on variables from different components,
the components will still be solved independently. We see that the leftmost node in the tree that
branches on f succeeds in solving the component {(e, f ), (¬e, f )}. This component is then added
to the cache. Similarly, the parent node that branches on b solves the component {(b, c), (¬b, c)}.
(The subcomponents Ψ− and Ψ+ generated by setting b, lines 8 and 11 of Algorithm 8, and performing unit propagation are equal to the empty formula, {}, and thus are known). On backtrack
to d, the alternate value for d does not affect the component {(b, c), (¬b, c)}, so its value can be
retrieved from the cache leaving only the component {(e, f )} to be solved. Branching on e solves
412

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

x:{φ}
HH
HH

HH

H
  
..
φa,b,c ,
a:
.
φd,e,f
H
 HH

HH


HH


HH



 H


{(b, c)},
{(b, c), (¬b, c)},
b:
d:
φd,e,f
φd,e,f ∗
H
H
n 
o∗ nH o∗
 HH

H
{}
{}

H
HH

HH





{(b, c), (¬b, c)}∗ ,
{(b, c), (¬b, c)},
e:
b:
{(e, f )}
{(e, f ), (¬e, f )}
HH
HH

HH





 

∗
∗
{}
{}
f:
{(e, f ), (¬e, f )}∗
{(e, f ), (¬e, f )}
H

H
n  o nH o
{}∗
{()}∗

H
n 
o∗ nH o∗
{}
{}

Figure 5: Search Space of #DPLL-Cache

this component. Backtracking to d we have that both {(e, f )} and {(e, f ), (¬e, f )} are solved, so
φd,e,f ’s value can be computed and placed in the cache. On backtracking to a, the alternate value
for a does not affect the component φd,e,f , so its value can be retrieved from the cache leaving
only the component {(b, c)} to be solved. Branching on b solves this component, after which both
{(b, c)} and {(b, c), (¬b, c)} are solved so φa,b,c ’s value can be computed and placed in the cache.
The search can then backtrack to try setting x to TRUE.
We can obtain the following upper bound on the runtime of #DPLL-Cache.
Theorem 2 For solving #S AT on n variables, there exists an execution of #DPLL-Cache that runs in
time bounded by nO(1) 2O(w) where w is the underlying branch width of the instance. Furthermore,
the algorithm can be made deterministic with the same time guarantees (as discussed in Section 2.5).
So we see that #DPLL-Cache can achieve the same level of performance as the best S UM P ROD
algorithms.
Finally, there is a third variant of #DPLL with caching, #DPLL-Space , that achieves a nontrivial time-space tradeoff. This algorithm is the natural variant of #DPLL-Cache, modified to remove
cached values so that only linear space is consumed. The algorithm utilizes one additional subroutine. (6) RemoveFromCache(Φ): takes as input a set of formulas (a set of components) and removes
all of them from the cache. After splitting a component with a variable instantiation and computing
the value of each part, #DPLL-Space cleans up the cache by removing all of these sub-components,
so that only the value of the whole component is retained. Specifically, #DPLL-Space is exactly like
#DPLL-Cache, except that it calls RemoveFromCache(Ψ− ∪ Ψ+ ) just before returning (line 14).
413

BACCHUS , DALMAO , & P ITASSI

Theorem 3 For solving #S AT on n variables, there is an execution of #DPLL-Space that uses only
space linear in the instance size and runs in time bounded by 2O(w log n) where w is the underlying
branch width of the instance. Furthermore, the algorithm can be made deterministic with the same
time and space guarantees.
The proofs of Theorems 1–3 are given in the appendix.
3.3 Using DPLL Algorithms for Other Instances of S UM P ROD:
The DPLL algorithms described in this section can be easily modified to solve other instances of
S UM P ROD. However, since #S AT is #P complete many instances of S UM P ROD can also be solved
by simply encoding them in #S AT. For example, this approach is readily applicable to BAYES and
has proved to be empirically successful (Sang et al., 2005b). Furthermore, the encoding provided
by Sang et al. (2005b) achieves the same complexity guarantees as standard algorithms for BAYES.
(That is, the CNF encoding has tree width no greater than the original Bayes Net). Note that this
encoding assigns non-uniform probabilities to values of the variables. That is, for variable x the
probability of x = 0 might not be equal to the probability of x = 1. This is easily accommodated
in our algorithms: instead of multiplying the value returned by each recursive call by 21 we simply
multiply it by the probability of the corresponding variable value (i.e., by Pr (x = 0) or Pr (x = 1)).
On the other hand, if conversion to #S AT is inapplicable or undesirable the algorithms can
be modified to solve other instances of S UM P ROD directly. For S UM P ROD, we want to compute
L Nm
L
j=1 fj (Ej ). DPLL chooses a variable, Xi , and for each value d of Xi it recursively
Xn
X1 . . .
solves the reduced problem F|Xi =d . (Hence, instead of a binary decision tree it builds a k-ary tree).
The reduced problem F|Xi =d is to compute
M
X1

...

M M

Xi−1 Xi+1

...

m
MO

fj (Ej )|Xi =d ,

Xm j=1

where fj (Ej )|Xi =d is fj reduced by setting Xi = d. #DPLL-SimpleCache caches the solution to
the reduced problem to avoid recomputing it. For example, it can remember the reduced problem by
remembering which of the original functions in F remain (i.e., have not been reduced to a constant
value) and the set of assignments that reduced these remaining functions. #DPLL-Cache caches
the solution to components of the reduced problem. For example, it can remember a component by
remembering the set of original functions that form the component along with the set of assignments
that reduced these functions. It can compute the current components by finding the connected
components of the primal graph generated from the hypergraph of the S UM P ROD instance with
all instantiated variables removed. It is a straightforward adaptation to show that the above three
theorems continue to hold for #DPLL, #DPLL-Cache, and #DPLL-Space so modified to solve S UM P ROD.
Algorithm 9 shows how #DPLL-Cache, for example, can be modified to solve general S UM P ROD problems. The algorithm takes as input a set of components Φ, just like #DPLL-Cache,
initially containing the components of the original problem. In the algorithm fns(x) denotes the set
of functions of the original problem that (a) contain x in their scope, and (b) are fully instantiated
by the instantiation of x.
414

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

Algorithm 9: S UM P ROD-DPLL-Cache algorithm for arbitrary S UM P ROD problems
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

S UM P ROD-DPLL-Cache (Φ)
begin
if InCache(Φ) then
return GetValue(Φ)
else
Ψ = RemoveCachedComponents(Φ)
choose a variable x that appears in some component φ ∈ Ψ
p=0
foreach d ∈ domain of x do
Φd = ToComponents(φ|x=d )
Q
α = f ∈fns(x) LOOKUP(value of f on ρ ∪ {x = d})
p = p + α × S UM P ROD-DPLL-Cache(Φ − {φ} ∪ Φd )
end
AddToCache(φ, p)
return GetValue(Φ)
end

4. Comparing Algorithms for BAYES and #S AT
In this section, we will prove that our DPLL based algorithms are at least as powerful as the standard
complete algorithms for solving #S AT, and that they are provable more powerful than many of
them on some instances. This last feature is important as it means that solving S UM P ROD using
DPLL augmented with caching can in some cases solve problems that are beyond the reach of many
standard complete algorithms.
As mentioned earlier, the algorithms for S UM P ROD as well as our new DPLL-based algorithms,
are actually nondeterministic algorithms that require some nondeterministically chosen input. (This
input can be viewed as being a sequence of bits). For VE, the nondeterministic bits encode an elimination ordering; for RC, the nondeterministic bits encode a branch decomposition; for AND/OR
search the nondeterministic bits encode a pseudo tree; and for our DPLL based algorithms, the
nondeterministic bits encode the underlying decision tree indicating which variable will be queried
next in the backtracking process. Thus when comparing the “power” of these algorithms we must
be careful about how the nondeterminism is resolved. For example, VE operating with a very bad
elimination ordering cannot be expected to run as efficiently as #DPLL-Cache operating with a
very good branching strategy. First we present some definitions which allow us to state our results
precisely.
Definition 7 Let f be a CNF formula. Define Time[VE](f ) to be the minimal runtime of any variable elimination algorithm for solving #S AT for f , over all choices of elimination orderings for f .
Similarly define Time[A](f ), for A equal to RC-Cache, RC-Space, RC-Cache+ , AND/OR-Space,
AND/OR-Cache, AND/OR-Cache+ , #DPLL-Cache, and #DPLL-Space. (For example, Time[RCCache](f ) is the minimal runtime of the RC-Cache algorithm solving #S AT for f , over all possible
branch decompositions of f .)

415

BACCHUS , DALMAO , & P ITASSI

Definition 8 Let A and B be two nondeterministic algorithms for #S AT. Then we will say that A
polynomial-time simulates B if there is a fixed polynomial p such that for every CNF formula f
Time[A](f ) ≤ p(Time[B](f )).
The following theorem shows that RC-Cache and RC-Cache+ polynomially simulate VE. The
proof of this theorem is implicit in the results of Darwiche (2001).
Theorem 4 Both RC-Cache and RC-Cache+ polynomially simulate VE.
Now we prove that DPLL with caching is as powerful as previous algorithms.
Theorem 5 #DPLL-Cache polynomially simulates RC-Cache, RC-Cache+ , AND/OR-Cache,
AND/OR-Cache+ , and VE. #DPLL-Space polynomially simulates RC-Space, AND/OR-Space and
DDP.5
The proof of this theorem is given in the appendix. It should be noted that the proof also
implies that there is a deterministic version of #DPLL-Cache that has time (and space) complexity that is at least as good as any deterministic realization of RC-Cache, RC-Cache+ , AND/ORCache, AND/OR-Cache+ , or VE. Similarly, there is a deterministic version of #DPLL-Space that
has time (and space) complexity that is at least as good as any deterministic realization of RC-Space,
AND/OR-Space and DDP.
Now we prove that DPLL with caching can in some cases run super-polynomially faster than
previous algorithms. The proof is given in the appendix.
Theorem 6 None of RC-Space, RC-Cache, AND/OR-Cache, AND/OR-Space or VE can polynomially simulate #DPLL-Cache, #DPLL-Space, or #DPLL.
This theorem shows that #DPLL-Cache/Space has a basic advantage over the other standard
algorithms for S UM P ROD. That is, on some problems RC, AND/OR search, and VE will all require
time super-polynomially greater than #DPLL-Cache no matter what branch decomposition, pseudo
tree, or variable ordering they are supplied with, even when caching is utilized. The proof of this
theorem shows that the advantage of #DPLL-Cache arises from its ability to utilize dynamic variable orderings, where each branch can order the variables differently. The flexibility of a dynamic
variable ordering for these instances gives rise to increased opportunities for contradictions thereby
significantly decreasing the overall runtime.
We note that Theorem 6 does not cover those algorithms that have more flexibility in their
variable ordering, i.e., AND/OR-Cache+ , RC-Cache+ , and DDP. It is an open problem whether or
not #DPLL-Cache is superpolynomially faster than these algorithms on some instances, although
we conjecture that Theorem 6 is also true for these algorithms.
In particular, note that #DPLL-Cache still has greater flexibility in its variable ordering than any
of these algorithms. None of these algorithms have complete flexibility in their variable ordering.
AND/OR-Cache+ must select an uninstantiated variable from the chain that starts at the root of its
passed pseudo tree; RC-Cache+ must select an uninstantiated variable from the intersection of the
labels of the left and right children of the root of its passed branch decomposition; and DDP must
select an uninstantiated variable from the component it is currently solving. In contrast #DPLLCache can select any uninstantiated variable.
5. DDP is the algorithm presented by Bayardo and Pehoushek (2000).

416

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

The difficulty with proving Theorem 6 for these other algorithms is that all of them can tradeoff flexibility in their variable ordering with their ability to decompose the problem. The clearest
example of this occurs with AND/OR-Cache+ . If AND/OR-Cache+ is passed a pseudo tree that is
simply a single chain of variables, it will have complete flexibility in its variable ordering, but at the
same time it will never decompose the problem. Similarly, if RC-Cache+ is provided with a branch
decomposition that has large labels it will have more flexibility in its variable ordering, but will be
less effective in decomposing the problem. For the family of problems used to prove Theorem 6
only flexibility in the variable ordering is needed to achieve a superpolynomial speedup, and thus
for example AND/OR-Cache+ can achieve this speedup by completely sacrificing decomposition.
#DPLL-Cache can manage the tradeoff between flexibility in variable ordering and decomposing the problem in more sophisticated ways. For example, it has the ability to use a variable
ordering that encourages decomposition in some parts of its search tree while using a different variable orderings in other parts of its search tree. For instance, the Cachet system, which is based on
#DPLL-Cache, employs a heuristic that dynamically trades off a variable’s ability to decompose the
problem with its ability to refute the current subtree (Sang et al., 2005a). (It employs a weighted
average of the number of clauses the variable will satisfy and the variable’s VSID score Moskewicz,
Madigan, Zhao, Zhang, & Malik, 2001). #DPLL-Cache also has the ability to interleave the solving
of its current set of components by successively choosing variables from different components. To
extend Theorem 6 to cover AND/OR-Cache+ , RC-Cache+ and DDP a family of problems exploiting these features of #DPLL-Cache would have to be developed.

5. Impact on Practice
Some of the results of this paper were first presented in a conference paper (Bacchus, Dalmao, &
Pitassi, 2003), and since that time a number of works have been influenced by the algorithmic ideas
presented here.
The Cachet system (Sang et al., 2004, 2005a) is a state of the art #S AT solver directly based
on the results presented here. Cachet like our #DPLL-Cache algorithm, is based on the ideas of
dynamic decomposition into components and caching of component solutions. It was an advance
over previous #S AT solvers in its use of caching to remember previously solved components and
in its integration of clause learning. The previous best #S AT solver, the DDP solver (Bayardo
& Pehoushek, 2000), also performed dynamic component detection but had neither component
caching nor clause learning. Our results highlighted the importance of component caching and the
possibility of basing a #S AT solver on a standard DPLL implementation thus making the integration
of clause learning feasible.
Cachet resolved a number of issues in making the algorithms we presented here practical. This
included practical ways of implementing the caching of components including a method for efficiently computing a key that could be used for cache lookup. (This method was subsequently
improved by Thurley, 2006). The Cachet system has also been used to solve BAYES, most probable
explanation (MPE), and weighted MAX-SAT problems by encoding these problems as weighted
#S AT problems (Sang et al., 2005b, 2007). This approach has proved to be very successful, especially for BAYES where it is often much superior to standard BAYES algorithms. The applications
of #S AT and the Cachet system for BAYES has been further advanced by Li et al. (2006, 2008).
It should also be noted that practical #S AT solving and its applications to other problems like
BAYES has also been advanced during this period by work on the RC algorithm and its application
417

BACCHUS , DALMAO , & P ITASSI

to compiling CNF into representations on which model counting is tractable, e.g., (Darwiche, 2004;
Chavira & Darwiche, 2006, 2008). This work has also illustrated the value of converting various
problems into weighted #S AT instances, and the utilization of techniques like clause learning (in
this case integrated into a RC style algorithm). There has also been considerable work advancing AND/OR search, e.g., (Dechter & Mateescu, 2004; Marinescu & Dechter, 2006; Dechter &
Mateescu, 2007).
One difference between the Cachet system and the RC and AND/OR search based systems mentioned above is that Cachet utilized a dynamic decomposition scheme. In particular, Cachet used a
dynamic variable ordering heuristic that attempts to trade off a variable’s ability to decompose the
problem with its ability to refute the current subtree. Because the variable ordering was dynamically
determined during search, Cachet cannot predict what components will be generated during search.
Hence it has to examine the current component (i.e., the component containing the variable just
instantiated) to discover the new components generated. Thus Cachet utilized an approach like that
specified in Algorithm 8 where a function like ToComponents is invoked on newly reduced component (see line 8). ToComponents must do a linear computation to find the new components (e.g., a
depth-first search or a union-find algorithm). In addition, for each component it must examine the
clauses contained in the component to compute a cache key.
In contrast, RC and AND/OR search take as input a static or precomputed decomposition
scheme (i.e., a branch decomposition or a pseudo tree). Hence, they are able to find components
without doing any extra work during search, and are able to more efficiently compute cache keys for
these components. For example, with AND/OR search, the algorithm simply follows the supplied
pseudo tree. When the variable V along with all variables on the path from the root to V have
been instantiated, AND/OR search knows that the variables in each subtree rooted by a child of V
forms an independent component. Hence, it can “detect” these components during search in constant time. Similarly, it need not examine the clauses over the variables in these new components to
compute a cache key. Instead it can compute a cache key from the node of the pseudo tree that roots
the component and the set of instantiations of the parents of that root that appear in clauses with
the variables of the component. Note that, the set of parents whose instantiations are relevant can
be computed before search so that all that has to be done during search is to look up their current
values.
Thus, by using a static decomposition scheme RC and AND/OR search can gain efficiency
over Cachet. However, these statically computed decompositions are not always as effective as the
dynamic scheme employed by Cachet. First, it can be useful to override the precomputed decomposition scheme so as to drive the search towards contradictions. This is the gist of Theorem 6 which
shows that more dynamic flexibility in variable ordering can provide superpolynomial reductions in
the size of the explored search tree by better exploiting such contradictions. Second, static decompositions cannot account for the different values of the variables. That is, the formula that arises
after instantiating a variable V to 0 can be quite different from the formula that arises after instantiating V to 1. This difference can negatively affect the performance of RC and AND/OR search
in at least a couple of ways: components might be generated that are not predicted by the static
decomposition scheme and thus the static scheme might not fully exploit decomposition; and due to
the specific changes to the formula generated by particular instantiations, the static decomposition
scheme might be inappropriate for much of the search space.
In practice, Cachet displays a performance that is at least as good as systems built using the
RC algorithm, and in some cases its performance is superior (see the empirical results presented
418

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

by Sang et al., 2004, 2005a). It should also be noted that #DPLL-Cache can easily utilize a static
decomposition scheme and gain all of the efficiencies of such schemes. For example, if provided
with a pseudo tree #DPLL-Cache can follow any ordering of the variables in that pseudo tree under
which parents are always instantiated before their children. Like AND/OR search it will know that
the children of any node in the pseudo tree each root an independent component, so it also will be
able to detect these components in constant time. Furthermore, it would be able to utilize the more
efficient caching scheme of AND/OR search. In this case its advantage over AND/OR search would
be that it would have the freedom to interleave the solving of its components.6
In more recent work our algorithms have also been applied to optimization problems (Kitching
& Bacchus, 2008). This work involved adding branch and bound techniques to the decomposition
and component caching described in #DPLL-Cache. During branch and bound dynamic variable
ordering can be very effective. In particular, one wants to branch on variables that will drive the
value of the current path towards a better value as this can generate a global bound that can be more
effective in pruning the rest of the search space. The empirical results of Kitching and Bacchus
(2008) show that the added flexibility of #DPLL-Cache can sometimes yield significant performance improvements over AND/OR search even when the extra flexibility of AND/OR-Cache+ is
exploited.

6. Final Remarks
In this paper we have studied DPLL with caching, analyzing the performance of various types
of caching for #S AT. Our results apply immediately to a number of instances of the S UM P ROD
problem including BAYES, since #S AT is complete for the class #P. However, our proofs can also
be modified without much difficulty so that our complexity results apply directly to any problem in
S UM P ROD.
More sophisticated caching methods have also been explored for solving S AT by Beame et al.
(2003) who showed that some of these methods can considerably increase the power of DPLL.
However, these more sophisticated caching methods are currently not practical due to their large
overheads. In other related work, one of the results of Aleknovich and Razborov (2002) showed that
SAT could be solved in time nO(1) 2O(w) . Our results extend this to any problem in S UM P ROD—as
shown in Section 2.1 SAT is an instance of S UM P ROD.
We have proved that from a theoretical point of view, #DPLL-Cache is just as efficient in terms
of time and space as other state-of-the-art exact algorithms for S UM P ROD. Moreover, we have
shown that on specific instances, #DPLL-Cache substantially outperforms the basic versions of
these other algorithms. The empirical results presented in the works described in Section 5 indicate
that these advantages can often be realized in practice and that on some problems our DPLL based
algorithms can yield significant performance improvements.
There are a number of reasons why our DPLL based algorithms can outperform traditional
algorithms for S UM P ROD. Algorithms like VE and the join tree algorithm (which is used in many
BAYES inference systems), take advantage of the global structure of interconnections between the
functions as characterized by the tree width or branch width of the instance. Our DPLL algorithms
however, can also naturally exploit the internal or local structure within the functions. This is
accomplished by instantiating variables and reducing the functions accordingly. This can lead to
6. Marinescu and Dechter (2007) present a method for searching an AND/OR tree in a best-first manner. This method
can also interleave the solving of components, but in general best-first search has exponential space overheads.

419

BACCHUS , DALMAO , & P ITASSI

improvements especially when the functions are encoded in a way to expose more of the function’s
internal structure, such as an encoding by sets of clauses (e.g., see Li et al., 2008). There are two
prominent examples of structure that can be exploited by DPLL.
First, some of the subproblems might contain zero valued functions. In this case our algorithms
need not recurse further—the reduced subproblem must have value 0.7 In VE the corresponding
situation occurs when one of the intermediate functions, Fi , produced by summing out some of the
variables, has value 0 for some setting of its inputs. In VE there is no obvious way of fully exploiting
this situation. VE can achieve some gains by ignoring those parts of Fi ’s domain that map to 0
when Fi appears in a product with other functions. However, it can still expend considerable effort
computing some other intermediate function Fj many of whose non-zero values might in fact be
irrelevant because they will eventually be multiplied by zero values from Fi .
Second, it can be that some of the input functions become constant prior to all of their variables
being set (e.g., a clause might become equivalent to TRUE because one of its literals has become
true), or they might become independent of some of their remaining variables. This means the subproblems f |xi =1 and f |xi =0 might have quite different underlying hypergraphs. Our DPLL-based
algorithms can take advantage of this fact, since they work on these reduced problems separately.
For example, our algorithms are free to use dynamic variable orderings, where a different variable
ordering is used solving each subproblem. VE, on the other hand, does not decompose the problem
in this way, and hence cannot take advantage of this structure.
In BAYES this situation corresponds to context-specific independence where the random variable X might be dependent on the set of variables W, Y, Z when considering all possible assignments to these variables (so f (X, W, Y, Z) is one of the input functions), but when W = True it
might be that X becomes independent of Y (i.e., f (X, W, Y, Z)|W =1 might be a function F (X, Z)
rather than F (X, Y, Z)). Previously only ad-hoc methods have been proposed (Boutilier, Friedman,
Goldszmidt, & Koller, 1996) to take advantage of this kind of structure.
It should be noted however, that when the problem’s functions have little or internal structure
VE can be significantly more efficient than any of the other algorithms (RC, AND/OR search and
our DPLL algorithms). VE only uses simple multiplication and summation operations and does
have any of the overheads involved with instantiating variables and exploring an AND/OR search
tree or backtracking tree.
RC and AND/OR search share some of the same advantages over VE. However, they do not have
as much flexibility as our DPLL algorithms. We have shown in Theorem 6 that fully exploiting the
zero valued functions can in some instances require dynamic variable orderings that lie outside of
the range of the basic versions of RC and AND/OR search. Although our proof does not cover the
enhanced versions of RC and AND/OR (RC-Cache+ and AND/OR-Cache+ ), we have pointed out
that even these versions do not have the same flexibility as our DPLL algorithms. In practice, the
empirical evidence provided by the Cachet system (Sang et al., 2004, 2005a) and by the branch
and bound system described by Kitching and Bacchus (2008) support our belief that this added
flexibility can be important in practice.
The exploitation of context-specific independence also poses some problems for RC and AND/OR
search algorithms. In particular, the static decomposition schemes they employ are incapable of
fully exploiting this structure—as pointed out above the underlying hypergraphs of the subproblems arising from different instantiations can be radically different. However, although our DPLL
7. For #S AT this corresponds to the situation where a clause becomes empty.

420

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

algorithms are in principle able to exploit such structure, it remains an open problem to find practical
ways to accomplishing this. Specifically, when a decomposition scheme is computed prior to search
sophisticated (and computationally complex) algorithms can be utilized. It is difficult to overcome
the overhead of such methods when they are used dynamically during search (although see Li and
van Beek 2004 for some work in this direction). The development of methods that are light weight
enough to use during search and are still effective for selecting decomposition promoting variables
remains an open problem.
Finally, as shown in the proof of Theorem 5, RC and AND/OR search possess no intrinsic
advantages over our DPLL algorithms except perhaps conceptually simplicity. The proof shows
that our DPLL algorithms can simulate RC and AND/OR search in such a way that no additional
computation is required. Furthermore, as pointed out in the Section 5 our algorithms are also able
to utilize static decomposition schemes obtaining the same efficiency gains as RC and AND/OR
search.
Recently, several papers (Sanner & McAllester, 2005; Mateescu & Dechter, 2007) have made
significant progress on developing more compact representations for functions (rather than tabular
form), thereby potentially enhancing all of the algorithms discussed in this paper (VE, RC, etc.) by
allowing them to exploit additional local structure within the functions. An interesting future step
would be to combine the unique dynamic features of #DPLL-Cache with one of these promising
compact function representations to try to further improve S UM P ROD algorithms.
Acknowledgments This research funded by governments of Ontario and Canada through their
NSERC and PREA programs. Some of the results of this paper were presented in an earlier conference paper (Bacchus et al., 2003). We thank Michael Littman for valuable conversations.

Appendix A. Proofs
A.1 Lemmas Relating Branch Width, Tree Width, and Elimination Width
Lemma 2 Let H = (V, E) be a hypergraph with a tree-decomposition of width w. Then there is
an ordering π of the vertices V such that the induced width of H under π is at most w.
Proof: Let H = (V, E) be a hypergraph of tree width w and let Ttd be a tree decomposition
that achieves width w. That is, the maximum sized label of Ttd is of size w + 1. We can assume
without loss of generality that the labels of the leaves of Ttd are in a one-to-one correspondence
with the edges of H. For an arbitrary node m in Ttd , let label (m) be the set of vertices in the label
of m, Am be the tree rooted at m, vertices(m) be the union of the labels of the leaf nodes in Am
(i.e., the hyperedges of H appearing below Am ), and depth(m) be the distance from m to the root.
Let x be any vertex of H, and let leaves(x) be the set of leaves of Ttd that contain x in their
label. We define node(x) to be the deepest common ancestor in Ttd of all the nodes in leaves(x),
and the depth of a vertex, depth(x), to be depth(node(x)). Note that x ∈ label (node(x)), since
the path from the left-most leaf in leaves(x) to the right-most leaf must pass through node(x); and
that x does not appear in the label of any node outside of the subtree rooted at node(x), since no
leaf outside of this subtree contains x.
Finally let π = x1 , . . . , xn be any ordering of the vertices such that if depth(y) < depth(x),
then y must precede x in the ordering. We use the notation y <π x to indicate that y precedes x in
421

BACCHUS , DALMAO , & P ITASSI

the ordering π (and thus y will be eliminated after x). We claim that the induced width of π is at
most the width of Ttd , i.e., w.
Consider Anode(x) , the subtree rooted at node(x), and vertices(node (x)), the union of the labels
of the leaves of Anode (x) . We make the following observations about these vertices.
1. If y ∈ vertices(node (x)) and y <π x, then y labels node(x) and node(y) must be ancestor
of node(x) (or equal). y <π x implies that depth(y) ≤ depth(x). There must be a path from
the leaf in Anode(x) containing y to node(y), and since node (y) is at least as high as node(x)
the path must go through node(x) (or we must have node(x) = node(y)). In either case
y ∈ label (node(x)).
2. If y ∈ vertices(node(x)) and y >π x then node(y) must lie inside Anode(x) and node(y)
must be a descendant of node(x) (or equal). y >π x implies that depth(y) ≥ depth(x).
There must be a path from the leaf in A containing y to node(y), and since node(y) is at
least as deep at node(x) there must either be a further path from node(y) to node(x), or
node(y) = node(x).
Note further that condition 2 implies that if y >π x and y appears in the subtree below node(x),
then all hyperedges in the original hypergraph H containing y must also be in the subtree below
node(x).
We claim that the hyperedge produced at stage i in the elimination process when xi is eliminated
is contained in label (node(xi )). Since the size of this set is bounded by w + 1, we thus verify that
the induced width of π is bounded by w (note that the hyperedge produced in elimination does not
contain xi where as label (node(xi )) does).
The base case is when x1 is eliminated. All hyperedges containing x1 are contained in the subtree below node(x1 ), thus the hyperedge created when x1 is eliminated is contained in vertices(node (x1 )).
All other vertices in vertices(node (x1 )) follow x1 in the ordering so by the above they must label
node(x1 ) and vertices(node(x1 )) ⊆ label (node(x1 )).
When xi is eliminated there are two types of hyperedges that might be unioned together: (a)
those hyperedges containing xi that were part of the original hypergraph H, and (b) those hyperedges containing xi that were produced as x1 , . . . , xi−1 were eliminated. For the original hyperedges, all these are among the leaves below node(xi ), and thus are contained in vertices(node(xi )).
For a new hyperedge produced by eliminating one of the previous variables, say the variable y, the
hyperedge it produced is contained in label (node (y)) by induction, which in turn is contained in
vertices(node(y)). If y is in the subtree below node(x) we get that this hyperedge is contained in
vertices(node(x)) since this is a superset of vertices(node (y)). Otherwise, node(y) lies in another
part of the tree, and its label cannot contain x (no node outside the subtree below node(x) has x in
its label). Thus the hyperedge created when it is eliminated also cannot contain xi .
In sum the hyperedge created when xi is eliminated is contained in vertices(node(xi )), since
all of the hyperedges containing xi at this stage are in this set. Furthermore, all vertices x1 , . . . , xi−1
are removed from this hyperedge, thus it contains only variables following xi in the ordering. Hence,
by (1) above this hyperedge is contained in label (node(xi )). 2
Lemma 3 Let H be a hypergraph with elimination width at most w. Then H has a tree-decomposition
of tree width at most w.
422

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

Proof: (Proof of Lemma 3) Let π = x1 , . . . xn be an elimination ordering for H. Then we will
construct a tree decomposition for H using π as follows. Initially, we have |E| trees, each of size
1, one corresponding to each edge e ∈ E. We first merge the trees containing xn into a bigger tree,
Tn , leaving us with a new, smaller set of trees. Then we merge the trees containing xn−1 into a
bigger tree, Tn−1 . We continue in this way until we have formed a single tree, T . Now fill in the
labels for all intermediate vertices of T so that the tree is a tree-decomposition. That is, if m and
n are two leaves of T and they both contain some vertex v, then every node along the path from m
to n must also contain v in its label. It is not too hard to see that for each xi , the tree Ti (created
when merging the trees containing xi ) has the property that the label of its root (which connects it
with the rest of T ) is contained in ei ∪ xi , where ei is the hyperedge created when xi is eliminated.
Basically, all nodes with xj , j > i, are already contained in Ti so xi does not need to label Ti ’s root.
Furthermore, if xj j < i is contained in Ti ’s root label, then xj must have been in some original
hyperedge with a variable xk k ≥ i: thus xj would have appeared in the hyperedge ei generated
when xi was eliminated.
Hence the tree width of the final tree T can be no larger than the induced width of π. 2
A.2 Complexity Results for Caching Versions of DPLL
For the proof of theorems 1 and 2 we will need some common notation and definitions. Let f be
k-CNF formula with n variables and m clauses, let H be the underlying hypergraph associated with
f with branch width w. By the results of Darwiche (2001), there is a branch decomposition of
H of depth O(log m) and width O(w). Also by the results of Robertson and Seymour (1995), it
is possible to find a branch decomposition, Tbd , such that Tbd has branch width O(w) and depth
O(log m), in time nO(1) 2O(w) . Thus our main goal for each of the three theorems will be to prove
the stated time and space bounds for our DPLL-based procedures, when they are run on a static
ordering that is easily obtainable from Tbd .
Recall that the leaves of Tbd are in one-to-one correspondence with the clauses of f . We will
number the vertices of Tbd according to a depth-first preorder traversal of Tbd . For a vertex numbered
i, let fi denote the subformula of f consisting of the conjunction of all clauses corresponding to the
leaves of the tree rooted at i. Let Vars(fi ) be the set of variables in the (sub)formula fi . Recall
that in a branch decomposition the label of each vertex i, label (i), is the set of variables in the
intersection of Vars(fi ) and Vars(f −fi ). Each node i in Tbd partitions the clauses of f into three
sets of clauses: fi , fiL , and fiR , where fiL is the conjunction of clauses at the leaves of Tbd to the
left of fi , and fiR is the conjunction of clauses at the leaves to the right of fi .
All of our DPLL caching algorithms achieve the stated run time bounds by querying the variables in a specific, static variable ordering. That is, down any branch of the DPLL decision tree,
DT , the same variables are instantiated in the same order. (In contrast a dynamic variable ordering allows DPLL to decide which variable to query next based on the assignments that have been
made before. Thus different branches can query the variables in a different order.). The variable
ordering used in DT is determined by the depth-first pre-ordering of the vertices in the branch decomposition Tbd and by the labeling of these vertices. Let (i, 1), . . . , (i, ji ) denote the variables in
label (i) that do not appear in the label of an earlier vertex of Tbd . Note that since the width of Tbd
is w, ji ≤ w for all i. Let 1, . . . , z be the sequence of vertex numbers of Tbd . Then our DPLL algorithm will query the variables underlying f in the following static order: π = h(i1 , 1), (i1 , 2), . . . ,
(i1 , j1 ), (i2 , 1), . . . , (i2 , j2 ), . . . , (is , 1), . . . , (is , js )i i1 < i2 < . . . < is ≤ z, and j1 , . . . , js ≤ w.
423

BACCHUS , DALMAO , & P ITASSI

Note that for some vertices i of Tbd , nothing will be queried since all of the variables in its label may
have occurred in the labels of earlier vertices. Our notation allows for these vertices to be skipped.
The underlying complete decision tree, DT , created by our DPLL algorithms on input f is thus a
tree with j1 + j2 + . . . + js = n levels. The levels are grouped into s layers, with the ith layer
consisting of ji levels. Note that there are 2l nodes at level l in DT , and we will identify a particular
node at level l by (l, ρ) where ρ is a particular assignment to the first l variables in the ordering, or
by ((q, r), ρ), where (q, r) is the lth pair in the ordering π, and ρ is as before.
The DPLL algorithms carry out a depth-first traversal of DT , keeping formulas in the cache
that have already been solved along the way. (For #DPLL-SimpleCache, the formulas stored in
the cache are of the form f |ρ , and for #DPLL-Cache and #DPLL-Space, the formulas stored are
various components of ToComponents(f |ρ ).) If the algorithm ever hits a node where the formula
to be computed has already been solved, it can avoid that computation, and thus it does not do a
complete depth-first search of DT but rather it does a depth-first search of a pruned version of DT .
For our theorems, we want to get an upper bound on the size of the pruned tree actually searched by
the algorithm.
Theorem 1 For solving #S AT with n variables, there is an execution of #DPLL-SimpleCache that
runs in time bounded by 2O(w log n) where w is the underlying branch width of the instance. Furthermore, the algorithm can be made deterministic with the same time guarantees.
Proof: We want to show that the size of the subtree of DT searched by #DPLL-SimpleCache
is at most 2O(w log n) . When backtracking from a particular node (l, ρ) = ((q, r), ρ) at level l in DT ,
the formula put in the cache, if it is not already known, is of the form f |ρ . (Recall ρ is a setting to the
first l variables.) However, we will see that although there are 2l different ways to set ρ, the number
of distinct formulas of this form is actually much smaller than 2l . Consider a partial assignment, ρ,
where we have set all variables up to and including (q, r), for some q ≤ is and some r ≤ jq . The
number of variables set by ρ (the length of ρ) is j1 + j2 + . . . + jq−1 + r.
Let ρ− denote the partial assignment that is consistent with ρ where only the variables in ρ that
came from the labels of the vertices on the path from the root of Tbd up to and including vertex q
are set. The idea is that ρ− is a reduction of ρ, where ρ− has removed the assignments of ρ that are
irrelevant to fq and fqR .
Consider what happens when the DPLL algorithm reaches a particular node ((q, r), ρ) at level
l of DT . At that point the algorithm is solving the subproblem f |ρ , and thus, once we backtrack to
this node, f |ρ = fqL |ρ ∧ fq |ρ ∧ fqR |ρ is placed in the cache, if it is not already known. Note that all
variables in the subformula fqL are set by ρ, and thus either fqL |ρ = 0, in which case nothing new
is put in the cache, or fqL |ρ = 1 in which case f |ρ = fq |ρ ∧ fqR |ρ = fq |ρ− ∧ fqR |ρ− is put in the
cache. Thus, the set of distinct subformulas placed in the cache at level l = (q, r) is at most the
set of all subformulas of the form fq |ρ− ∧ fqR |ρ− , where ρ− is a setting to all variables in the labels
from the root to vertex q, plus the variables (q, 1), ..., (q, r). There are at most d · w such variables,
where q has depth d in Tbd (each label has at most w variables since this is the width of Tbd ). Hence
the total number of such ρ− ’s is at most 2(w·d) . This implies that the number of subtrees in DT at
level l + 1 that are actually traversed by #DPLL-SimpleCache is at most 2 · 2w·d = 2O(w·d) , where
d is the depth of node q in Tbd . Let t be the number of nodes in DT that are actually traversed by
#DPLL-SimpleCache. Then, t is at most n2O(w·log n) , since t is the sum of the number of nodes
visited at every level of DT and for each node q in Tbd d ∈ O(log m) = O(log n).
424

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

Accounting for the time to search the cache, the overall runtime of #DPLL-SimpleCache is at
most t2 , where again t is the number of nodes in DT that are traversed by the algorithm. Thus,
#DPLL-SimpleCache runs in time (n2O(w·log n) )2 = 2O(w·log n) . 2
Theorem 2 For solving #S AT on n variables, there exists an execution of #DPLL-Cache that runs in
time bounded by nO(1) 2O(w) where w is the underlying branch width of the instance. Furthermore,
the algorithm can be made deterministic with the same time guarantees.
Proof: We prove the theorem by placing a bound on the number of times #DPLL-Cache can
branch on any variable xl . Using the notation specified above, xl corresponds to some pair (q, r) in
the ordering π used by #DPLL-Cache. That is, xl is the r’th new variable in the label of vertex q of
the branch decomposition Tbd .
When #DPLL-Cache utilizes the static ordering π, it branches on, or queries, the variables
according to that order, always reducing the component containing the variable xi that is currently due to be queried. However, since previously cached components are always removed (by
RemoveCachedComponents in the algorithm), it can be that when it is variable xi ’s turn to be
queried, there is no component among the active components that contains xi . In this case, #DPLLCache simply moves on to the next variable in the ordering, continuing to advance until it finds
the first variable that does appear in some active component. It will then branch on that variable
reducing the component it appears in, leaving the other components unaltered.
This implies that at any time when #DPLL-Cache selects xl as the variable to next branch on it
must be the case that (1) xl appears in an active component. In particular the value of this component
is not already in the cache. And (2) no variable prior to xl in the ordering π appears in an active
component. All of these variables have either been assigned a particular value by previous recursive
invocations, or the component they appeared in has been removed because its value was already in
the cache.
In the branch decomposition Tbd let p be q’s parent (q must have a parent since the root has
an empty label). We claim that whenever #DPLL-Cache selects xl as the next variable to branch
on, the active component containing xl must be a component in the reduction of fp whose form is
determined solely by the settings of the variables in p and the r variables of q that have already been
set. If this is the case, then there can be at most 2(w+r) = 2O(w) different components that xl can
appear in, and hence #DPLL-Cache can branch on xl at most 2O(w) times as each time one more of
these components gets stored in the cache.
Now we prove the claim. The label of q consists of variables appearing in p’s label and variables
appearing in the label of q’s sibling. Since all of the variables in label (p) have been set, q and its
sibling must now have an identical set of unqueried variables in their labels. Hence, q must be the
left child of p as by the time the right child is visited in the ordering, xl will have already been
queried. Thus, at the time xl is queried, fp will have been affected only by the current setting of
label (p) (as these are the only variables it shares with the rest of the formula) and the first r queried
variables from label (q). That is, fp can be in at most 2(w+r) different configurations, and thus the
component containing xl can also be in at most this many different configurations.
Thus with n variables we obtain a bound on the number of branches in the decision tree explored
by #DPLL-Cache of n2O(w) . As in the proof of the previous theorem, the overall runtime is at most
quadratic in the number of branches traversed, to give the claimed bound of nO(1) 2O(w) . 2

425

BACCHUS , DALMAO , & P ITASSI

Theorem 3 For solving #S AT on n variables, there is an execution of #DPLL-Space that uses only
space linear in the instance size and runs in time bounded by 2O(w log n) where w is the underlying
branch width of the instance. Furthermore, the algorithm can be made deterministic with the same
time and space guarantees.
Proof: For this proof, it will be more natural to work with a tree decomposition rather than a
branch decomposition.
Let f be a k-CNF formula with n variables and m clauses and let H be the underlying hypergraph associated with f . We begin with a tree decomposition Ttd of depth O(log m) and width
O(w) (computable in time nO(1) 2O(w) ). We can assume without loss of generality that the leaves
of Ttd are in one-to-one correspondence with the clauses of f . Each node i in Ttd partitions f into
three disjoint sets of clauses: fi , the conjunction of clauses at the leaves of the subtree of Ttd rooted
at i, fiL , the conjunction of clauses of the leaves of Ttd to the left of fi , and fiR , the conjunction of
clauses of the leaves of Ttd to the right of fi . #DPLL-Space will query the variables associated with
the labels of Ttd according to the depth-first preorder traversal. Let the variables in label (i) not appearing in an earlier label on the path from the root to node i be denoted by S(i) = (i, 1), . . . , (i, ji ).
If i is a non-leaf node with j and k being its left and right children, then the variables in S(i) are
exactly the variables that occur in both fj and fk but that do not occur outside of fi . If we let c
be the total number of nodes in Ttd , then #DPLL-Space will query the variables underlying f in
the following static order: S(1), S(2), . . . , S(c), where some S(i) may be empty. The underlying
decision tree, DT , created by #DPLL-Space is a complete tree with n levels. As before we will
identify a particular node s at level l of DT by s = (l, ρ) where ρ is a particular assignment to the
first l variables in the ordering, or by s = ((q, r), ρ) (the r th variable in S(q)).
#DPLL-Space carries out a depth-first traversal of DT , storing the components of formulas in
the cache as they are solved. However, now components of formulas are also popped from the cache
so that the total space ever utilized is linear. If the algorithm hits a node where all of the components
of the formula to be computed are known, it can avoid traversing the subtree rooted at that node.
Thus it searches a pruned version of DT .
During the (pruned) depth-first traversal of DT , each edge that is traversed is traversed twice,
once in each direction. At a given time t in the traversal, let E = E1 ∪ E2 be the set of edges that
have been traversed, where E1 are the edges that have only been traversed in the forward direction,
and E2 are the edges that have been traversed in both directions. The edges in E1 constitute a partial
path p starting at the root of DT . Each edge in p is labeled by either 0 or 1. Let p1 , . . . , pk be the
set of all subpaths of p (beginning at the root) that end in a 1-edge. Let ρ1 , . . . , ρk be subrestrictions
corresponding to p1 , . . . , pk except that the last variable that was originally assigned a 1 is now
assigned a 0. For example, if p is (x1 = 0, x3 = 1, x4 = 0, x5 = 1, x6 = 0, x2 = 0), then
ρ1 = (x1 = 0, x3 = 0), and ρ2 = (x1 = 0, x3 = 1, x4 = 0, x5 = 0). Then the information that is
in the cache at time t contains ToComponents(f |ρi ), i ≤ k.
For a node q of Ttd and corresponding subformula fq , the context of fq is a set of variables
defined as follows. Let (q1 , . . . , qd ) denote the vertices in Ttd on the path from the root to q (excluding q itself). Then the context of fq is the set Context (fq ) = S(q1 ) ∪ S(q2 ) ∪ . . . ∪ S(qd ).
Intuitively, the context of fq is the set of all variables that are queried at nodes that lie along the path
to q. Note that when we reach level l = (q, 1) in DT , where the first variable of S(q) is queried,
we have already queried many variables, including all the variables in Context(fq ). Thus the set
of all variables queried up to level l = (q, 1) can be partitioned into two groups relative to fq : the
426

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

irrelevant variables, and the set Context(fq ) of relevant variables. We claim that at an arbitrary
level l = (q, r) in DT , the only nodes at level l that are actually traversed are those nodes ((q, r), ρ)
where all irrelevant variables in ρ (with respect to fq ) are set to 0. The total number of such nodes
at level l = (q, r) is at most 2|Context(fq )|+r which is at most 2w log n . Since this will be true for all
levels, the total number of nodes in DT that are traversed is bounded by n2w log n . Thus, all that
remains is to prove our claim.
Consider some node s = ((q, r), α) in DT . That is, α = α1 α2 . . . αq−1 b1 . . . br−1 , where for
each i, αi is an assignment to the variables in S(i), and b1 . . . br−1 is an assignment to the first r − 1
variables in S(q). Let the context of fq be S(q1 ) ∪ . . . ∪ S(qd ), d ≤ log n. Now suppose that α
assigns a 1 to some non-context (irrelevant) variable, and say the first such assignment occurs at αut ,
the tth variable in αu , u ≤ q − 1. We want to show that the algorithm never traverses s.
Associated with α is a partial path in DT ; we will also call this partial path α. Consider the
subpath/subassignment p of α up to and including αut = 1. If α is traversed, then we start by
traversing p. Since the last bit of p is 1 (i.e., αut = 1) when we get to this point, we have stored
in the cache ToComponents(f |ρ ) where ρ is exactly like p except that the last bit, αut , is zero. Let
j be the first node in q1 , q2 , . . . qd with the property that the set of variables S(j) are not queried
in p. (On the path to q in Ttd , j is the first node along this path such that the variables in S(j) are
not queried in p.) Then ToComponents(f |ρ ) consists of three parts: (a) ToComponents(fjL |ρ ), (b)
ToComponents(fj |ρ ), and (c) ToComponents(fjR |ρ ).
Now consider the path p′ that extends p on the way to s in DT , where p′ is the shortest subpath of
α where all of the variables S(i) for i < j have been queried. The restriction corresponding to p′ is a
refinement of p where all variables in S(1)∪S(2)∪. . . S(j−1) are set. Since we have already set everything that occurs before j, we will only go beyond p′ if some component of ToComponents(f |p′ )
is not already in the cache. ToComponents(f |p′ ) consists of three parts: (a) ToComponents(fjL |p′ ),
(b) ToComponents(fj |p′ ), and (c) ToComponents(fjR |p′ ). Because we have set everything that occurs before j, all formulas in (a) will be known. Since p′ and ρ agree on all variables that are relevant
to fj , ToComponents(fj |p′ ) = ToComponents(fj |ρ ) and hence these formulas in (b) in the cache.
Similarly all formulas in (c) are in the cache since ToComponents(fjR |p′ ) = ToComponents(fjR |ρ ).
Thus all components of ToComponents(f |p′ ) are in the cache, and hence we have shown that we
never traverse beyond p′ and hence never traverse s. Therefore the total number of nodes traversed
at any level l = (q, r) is at most 2wd , where d is the depth of q in Ttd , as desired. This yields an
overall runtime of 2O(w log n) .
It is left to argue that the space used is linear in the instance size. The total number of formulas
that are ever stored in the cache simultaneously is linear in the depth of the tree decomposition,
which is O(log m). Since we store each restricted formula f |ρ by storing the associated restriction
ρ, the total space ever used is O(n log m), which is linear in the input size. 2
A.3 Comparing Algorithms for BAYES and #S AT
Before proving the next theorem, we first discuss in more detail the structure of the search space
explored by various versions of RC, AND/OR search and DDP. All of these algorithms operate
in the same way. They instantiate variables and when the problem decomposes into independent
components they solve these components in separate recursions. Hence, when solving any CNF
formula f they all generate some AND/OR search tree (Dechter & Mateescu, 2007).
427

BACCHUS , DALMAO , & P ITASSI

The AND/OR search tree AO generated when one of the above algorithms solves the #S AT
instance f (a CNF formula), is a rooted tree. Each node n of AO is labeled by a formula n.f and
the subtree below n is generated when solving n.f . The root of A0 is labeled by the original formula
f . There are four different types of nodes in AO:
Query nodes. Each query node q has an associated variable q.var and two children corresponding
to the two possible instantiations of q.var . That is, its children are labeled by the formulas
q.f |q.var =0 and q.f |q.var =1 . A query node q is generated by the search algorithm whenever
it chooses to instantiate q.var and then executes recursive calls on the two resultant reduced
formulas.
AND nodes. Each AND node, a, has a query node as its parent, and has one or more children
all of which are query nodes. An AND node is generated by the search algorithm when it
decomposes the current formula into two or more independent components following the instantiation of the parent query node’s variable. Each of these components will then be solved
in one of the subtrees rooted by the AND node’s children. If a.f splits into the components
V
fi , i = 1, . . . , k, then a.f = i fi , and the i’th child of a is labeled by fi . Note that the fi
share no variables. Hence, the set of query node variables that appear in the subtree below
the i-th child of a are disjoint from the set of query node variables appearing below the j-th
child of a for all j 6= i.
Failure nodes. These are leaf nodes of the tree that are labeled with a formula containing the empty
clause. If caching is being used, failure nodes might also be labeled by a formula in the cache
that has already been shown to be unsatisfiable.
Satisfying nodes. These are leaf nodes of the tree that are labeled with a formula containing no
clauses. If caching is being used, satisfying nodes might also be labeled by a satisfiable
formula in the cache whose model count is already know.
Figure 6 shows an example AND/OR search tree.
Each node n of the AO also has a value, n.value, computed by the algorithm that generates it.
Here we only need to distinguish between zero values n.value = 0, and non-zero values denoted
by n.value = 1. Every satisfying node has value 1, and every failure node has value 0. A query
node has value 1 if and only if at least one of its children has value 1, and an AND node has value
1 if and only if all of its children have value 1. For example, in Figure 6 AND node D has value
0, while query node 2 has value 1. Note that all of the children of an AND node in AO must have
value 1 except possibly the right most child. The algorithms generating AO all terminate the search
below an AND node as soon as they discover a value 0 child—this implies that the AND node has
value 0. It can be seen that n.value = 0 if n.f is unsatisfiable and n.value = 1 if n.f is satisfiable.
Given any node n of AO, let AO(n) be the AND/OR subtree of AO rooted by n. Each satisfying
assignment ρ of n’s formula n.f defines a solution subtree S(n) of AO(n). In particular, S(n) is
a connected subtree of AO(n) rooted by n such that (1) if q is a query node in S(n) then S(n)
also contains the child of q corresponding to the assignment made by ρ (i.e., if ρ[q.var ] is the value
assigned to q.var in ρ, then S(n) will contain the child labeled by the formula q.f |q.var =ρ[q.var] ),
(2) if a is an AND node in S(n) then S(n) contains all children of a, and (3) S contains no failure
nodes. For example, a solution subtree of the AND/OR tree shown in Figure 6 (i.e., a solution
subtree of the root node) is formed by the leaf nodes b, c, f, and l; the query nodes 1, 2, 3, 4, 5, 6,
428

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

1

11

2

A

16

D

n
3

8

6

12

13

!

E

u
7

B

!

C

e
4

5

c

d

o

17

!

!

i

j

19

p
q

h

15

18

g

!

b

m

10

9

f
a

14

l

r

s

!

t

x

y

z

!

v

k

w

F il
Failure
node
d
!

Satisfying node
Query node
AND node
d

Figure 6: An example AND/OR search tree with query nodes numbered 1–19, leaf nodes (both
failure and satisfying) numbered a–z, and AND nodes labeled A–E.

7, and 8; and the AND nodes A, and B. In particular, the left value of query nodes 1, 2, 3, 5, 6, 7
and 8, along with the right value of query node 4 satisfy all clauses of the formula 1.f . A solution
subtree of AO(n) exists if and only if n.value = 1.
Finally, in an AND/OR search tree we say that a query node whose parent is an AND node is
a component root. We also classify the root node as a component root. In Figure 6 query nodes 1
(the root node), 3, 6, 8, 4, 5, 9, 10, 12, 13, 17, and 19 are component roots.
Theorem 5 #DPLL-Cache polynomially simulates RC-Cache, RC-Cache+ , AND/OR-Cache,
AND/OR-Cache+ , and VE. #DPLL-Space polynomially simulates RC-Space, AND/OR-Space and
DDP.
Proof: Since RC-Cache polynomially simulates VE we can ignore VE in our proof: showing that
#DPLL-Cache polynomially simulates RC-Cache also shows that it polynomially simulates VE.
Also we assume in our proof that if any of these algorithms use unit propagation, then so does
#DPLL-Cache/Space. As explained in Section 3.1, #DPLL-Cache/Space without unit propagation
can polynomially simulate versions of #DPLL-Cache/Space using unit propagation.
429

BACCHUS , DALMAO , & P ITASSI

Each of the stated algorithms will generate an AND/OR search tree when solving a CNF formula
f . To prove the theorem we first show how any AND/OR search tree solving f can be converted
into a partial DPLL decision tree, DT , that is no bigger. Then we show that our DPLL algorithms
can solve f using DT to guide its variable ordering. Thus, we obtain the result that the minimal
runtime for any of the stated algorithms, which must result in the generation of some AND/OR
search tree AOmin , can also be achieved by our DPLL algorithms. In particular, when run on the
partial decision tree constructed from AOmin , our DPLL algorithms will achieve a polynomially
similar runtime. (This suffices to prove the theorem, as we need only show the existence of an
execution of our DPLL algorithms achieving this run time.)
To make the distinction between the AND/OR search tree and the constructed partial decision
tree clear, we will use the suffixes ao and dt to indicate elements of the AND/OR tree and decision
tree respectively.
DPLL decision trees contain only query variables, satisfying nodes, and failure nodes, where
satisfying and failure nodes are both leaf nodes. We construct a partial decision tree DT from an
AND/OR tree AO by expanding the left most solution subtree S(nao ) below every node nao ∈ AO
with nao .value = 1 into a linear sequence of query variables in DT using a depth-first ordering
of the query variables in S(nao ). For nodes nao ∈ AO with nao .value = 0 the same expansion is
attempted, but in this case it will result in a sequence of query nodes that terminate at failure nodes.
Every node qdt in DT has a pointer, dt→ao(qdt ) to a node qao in AO, at the end of the construction these pointers establish a map between the nodes in DT and the nodes in AO. Initially, the root
of DT has a pointer to the root of AO. Then, for any node qdt ∈ DT :
1. If dt→ao(qdt ) is a query node qao in AO, then make qdt a query node and create a left and
right child, ldt and rdt , for qdt in DT . We make qdt query the same variable as qao (i.e.,
qdt .var = qao .var ), and set its children to point to the children of qao (i.e., dt→ao(ldt ) and
dt→ao(rdt ) are set to the left and right children of qao in AO).
2. If dt→ao(qdt ) is an AND node aao in AO, then we reset dt→ao(qdt ) to be the left most child
of aao in AO. We then apply the first rule above, and continue.
3. If dt→ao(qdt ) is a failure node in AO then we set qdt to be a failure node. In this case qdt has
no children.
4. If dt→ao(qdt ) is a satisfying node in AO then we examine the path ρao in AO from the root
to dt→ao(qdt ). Let rao be the last component root on ρao that has a right sibling.
(a) If such an rao exists, and no node on the path from rao to dt→ao(qdt ) in AO is the right
child of a query node whose left child has value 1, then we reset dt→ao(qdt ) to be the
leftmost right sibling of rao . This node is also a component root, and hence it is a query
node in AO. We then apply the first rule above, and continue.
(b) Otherwise (either rao does not exist or there is some node on the path from rao that is
the right child of a query node whose left child has value 1), we make qdt a satisfying
node. In this case qdt has no children.
Rule 4 of the construction is where we convert the leftmost solution subtree below each node nao
in AO into a sequence of query nodes in DT by performing a depth-first traversal of this solution
subtree. In particular, in this solution subtree the leftmost right sibling of the deepest component
430

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

1

11

2

3

16

12

n
4

!

13o

17

p

e
14

5b

u

15

18

a

x
6c

d
7

!

!

q

r

19v

s

t

w

9

y
!

8f

g

z

10i

h
!

l

m

j

k
Failure node
!

Satisfying node
Query node

Figure 7: The partial DPLL decision tree constructed from the AND/OR search tree of Figure 6.
Each query and leaf nodes n is numbered with the number of the corresponding node,
dt→ao(n), in the AND/OR search tree.

root is the depth-first successor of the satisfying leaf node. The condition that no node on route
to that sibling is the right child of a query node whose left child has value 1 ensures that we only
perform a depth-first traversal along the leftmost solution subtree and not along subsequent solution
subtrees. Figure 7 shows the partial decision tree that would be constructed from the AND/OR
search tree of Figure 6.
In the diagram, satisfying nodes whose pointers are reset to the next component root using
rule 4a, are numbered with the corresponding query node in the AND/OR tree followed by the
leaf label of the corresponding satisfying node. For example, node 5b in the Figure 7 represents
satisfying child b of node 4 in Figure 6 that has been redirected to its depth-first successor node 5
(the leftmost right sibling of the deepest component root 4).
As another example, in the AND/OR tree, the right child of node 6 is the AND node C. Hence, in
the decision tree, the right child of the corresponding query node 6, becomes query node 9 which is
the leftmost child of node C (rule 2). Furthermore, when we reach satisfying node j in the AND/OR
tree, we can proceed no further and hence the left child of query node 10 in the decision tree becomes
a terminal satisfying node (rule 3). In particular, although the path from the root to node 10 in the
AND/OR tree contains a component root with a right sibling, namely node 6, this path also contains
the node C that is the right child of a query node (node 6) whose left child (node 7) has value 1.
431

BACCHUS , DALMAO , & P ITASSI

There are two things to note. First, at any node ndt of DT all variables instantiated on the path
ρao in A0 from the root to dt→ao(n) have been instantiated to the same values on the path ρdt in
DT from the root to ndt . Since Rules 3 and 4b terminate paths, all nodes on ρdt are inserted only
by Rules 1, 2, and 4b. Rules 1 and 2 only insert nodes on ρdt whose parents are already on ρdt , and
Rule 1 ensures that the values assigned are the same as those in AO. Finally, Rule 4a only inserts a
node adt on ρdt if one of dt→ao(adt )’s siblings is already on ρdt , and hence that sibling’s (and a’s)
parent must already be on ρdt .
Second, no variable is queried twice along any path of DT . That is, no node ndt in DT has
an ancestor n′dt with ndt .var = n′dt .var . Again any path ρdt in DT is grown only by applications
of Rules 1, 2, and 4a. Since no path in AO queries the same variable twice, Rules 1 and 2 must
preserve this condition. Similarly Rule 4a moves to a new component root aao , and the set of query
variables at and below aao in AO is disjoint with the set of query variables already appearing in ρdt .
Using the above, from the AND/OR search tree AO generated by any of the algorithms RCSpace, AND/OR-Space or DDP when solving the formula f , we can construct a corresponding
partial decision tree DT . Now we show that #DPLL-Space can solve f by exploring a search tree
that is no larger than DT . Note that DT is itself no larger than AO, hence this will show that
#DPLL-Space can solve f with a polynomially similar run time, proving that it can polynomially
simulate RC-Space, AND/OR-Space and DDP. (Note that the run time of all of these algorithms is
polynomially related to the size of the search trees they explore.)
We execute #DPLL-Space using the variable ordering specified in DT . That is, starting at the
root rdt of DT , #DPLL-Space will always query the variable of the current node of DT , ndt .var ,
and then descend to ndt ’s left child. When it backtracks to ndt it will then descend to the right child.
Hence, we only need to show that #DPLL-Space must backtrack if it reaches a leaf of DT . That is,
it explores a search tree that is no larger than DT .
First, if #DPLL-Space reaches a failure node of DT it must detect an empty clause and backtrack. By Rule 3 of the construction any failure node fdt of DT must correspond to a failure node
dt→ao(fdt ) in AO. Since all variables instantiated on the path in AO from the root to dt→ao(fdt )
are instantiated to the same values on the path in DT from the root to fdt , we see that if an empty
clause was detected in AO at dt→ao(fdt ) then #DPLL-Space must also detect an empty clause
at fdt . (Note that if the algorithm that generated AO used unit propagation, then we assume that
#DPLL-Space does as well).
Second, if #DPLL-Space reaches a satisfying node sdt of DT it must detect that all of its current
set of components are solved and backtrack (line 4 of Algorithm 8). Let ρdt be the path in DT from
the root to sdt , ρao be the path in AO from dt→ao(sdt ) to the root, and crdt be a node on ρdt such
that dt→ao(crdt ) is a component root in AO (we say that crdt is a component root on ρdt ). We
claim that (a) if lao is a left sibling of dt→ao(crdt ) in AO, then there exists a node ldt on ρdt such
that dt→ao(ldt ) = lao , and lao .f is satisfied by ρdt ; (b) if rao is a right sibling of dt→ao(crdt ) in
AO then rao .f is in #DPLL-Space’s cache.
Given claim (a) the only clauses of the original formula not yet satisfied by ρdt are clauses from
rao .f for those nodes rao in AO that are right siblings of some component root crdt on ρdt (i.e.,
rao is a right sibling of component root dt→ao(crdt ) in AO). When #DPLL-Space arrived at crdt ,
prior to reaching sdt , all variables in AO on the path from the root to dt→ao(crdt ) have already
been instantiated to the same values on ρdt . Thus, if pao is dt→ao(crdt )’s parent in AO, #DPLLSpace would have recognized that rao .f was a separate component once it instantiated pao .var , and
it would have added rao .f to its list of components (at line 8 or 11 of Algorithm 8). Note that,
432

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

once solved rao .f would not be removed from #DPLL-Space’s cache until it backtracks to undo the
instantiation of pao .var . (At which point the solution all of pao ’s children would be combined to
yield a solution to pao .f ).
Furthermore, following the variable ordering specified in DT , #DPLL-Space would not instantiate any of the variables in r.f along the path ρdt . Hence, any component that is on #DPLL-Space’s
list of components when it reaches ns must be equal to rao .f for some right sibling rao of a component root on ρdt , and by claim (b) will be removed by the call to RemoveCachedComponents(Φ)
(line 6). This will leave #DPLL-Space with an empty list of components to solve, and hence it must
backtrack at sdt .
Now we prove the claims. For (a) we see that DT will always visit the children of an AND node
in AO in a left to right order. That is, before inserting a component root crdt on its path, it must
first visit all left siblings lao of dt→ao(crdt ). After inserting ldt on its path (with dt→ao(ldt ) = lao ),
it will instantiate ldt and then start to query the nodes under lao searching alternate instantiations
to these variables until it is able to traverse a leftmost solution subtree of AO(lao ). This traversal
results in the insertion into the path of a solution to lao .f , after which DT inserts crdt on its path
using Rule 4a.
For (b) we observe that sdt is a satisfying node in DT only through the application of Rule 4b.
Hence there are two possible cases. First, it can be that none of the component roots on ρdt have a
right sibling. In this case every clause of the original formula is satisfied and #DPLL-Space must
backtrack. For example, in Figure 7 this occurs at leaf nodes l and y.
Otherwise, let crdt be a component root on ρdt such that dt→ao(crdt ) has a right sibling in AO,
and let ndt be the first node on ρdt following crdt such that (i) ndt ’s successor on ρdt is its right
child, and (ii) dt→ao(ndt ) has a left child in AO with value 1. Such a node ndt must exist, else sdt
would not have been a leaf node of DT by Rule 4a. When #DPLL-Space arrived at node crdt it
would have rao .f on its list of components for all right siblings rao of dt→ao(crdt ). There might
also be other unsolved components on this list. All of these components, however, must be equal
to rao .f for some right sibling rao of a component root on ρdt preceding crdt , and must have been
placed on the list of components prior to #DPLL-Space reaching crdt . Then, when #DPLL-Space
arrived at ndt it would have taken the left branch first. Thus it would have previously been invoked
with all of these right sibling components on its component list.
When #DPLL-Space is invoked with a list of components it either solves every component,
placing them in its cache and keeping them there until it backtracks to the node where they were
first placed on its list, or it discovers that one of these components is unsatisfiable. If one of the
components is unsatisfiable, it will immediately backtrack to the point where that component was
first placed on its list. In particular, all recursive calls where the list of components contains a known
unsatisfiable component will return immediately since the call to InCache(Φ) will detect that the
list of components has product equal to zero.
Hence, on taking the left branch at ndt , #DPLL-Space, will have on its list of components,
components of the form rao .f for right siblings of component roots above ndt on ρdt , and also lao .f
where lao is the left child of dt→ao(ndt ) in AO. Since lao has value 1, lao .f is satisfiable, and either
#DPLL-Space will solve all its components, placing their value in its cache, or it will discover that
one of the components rao .f is unsatisfiable and will backtrack without visiting sdt . Therefore, if it
does visit sdt it would have solved all components that could potentially be on its list of components,
and these components would still be in is cache since they were placed on the list before arriving at
sdt .
433

BACCHUS , DALMAO , & P ITASSI

This shows that #DPLL-Space polynomially simulates RC-Space, AND/OR-Space and DDP.
RC-Cache and AND/OR-Cache gain over RC-Space and AND/OR-Space by not having to solve
some components more than once. That is, when they arrive at a node nao in their generated
AND/OR tree AO, if nao .f has been solved before they can immediately backtrack.
#DPLL-Cache gains the same efficiency over #DPLL-Space. In particular, it need never solve
the same component more than once. Using caching to removing previously solved components
from its list of components gives rise to the same savings that are realized by adding caching to
AND/OR or RC. Formally, the same construction of a partial decision tree DT can be used. In
AO we mark all nodes where search is terminated by a cache hit as a satisfying node (if the cached
formula is satisfiable) or as a failure node (if the cached formula is unsatisfiable). Now, for example,
AND nodes can have satisfying or failure nodes as children when those components have been
solved before. Applying our construction to AO gives rise to a partial decision tree DT , and it
can then be shown that #DPLL-Cache using DT to guide its variable choices will explore a search
tree that is about the same size as DT . This proves that #DPLL-Cache polynomially simulates
RC-Cache and AND/OR-Cache.
The only subtle point is that #DPLL-Cache might not solve a component at the same point
in its search. In particular, if a component φ first appears on #DPLL-Cache’s list of components
with a previously added unsatisfiable component, #DPLL-Cache will backtrack without solving φ.
Following DT , #DPLL-Cache will only do enough work to find φ’s first solution, after which it
will proceed to the other components on its list. During its search for φ’s first solution, it will cache
all unsatisfiable reductions of φ found during this search. Thus, the next time it encounters φ it can
follow the same variable ordering and not do any extra work: the cached unsatisfiable reductions
will immediately prune all paths leading to failure and it can proceed directly to the first solution
to φ. If the other components on its list are all satisfiable, it will eventually backtrack to this first
solution and then continue to solve φ. Hence, although #DPLL-Cache might encounter φ many
times before solving it, each such encounter, except for the first, require adding to its search tree
only a number of nodes linear in the number of variables in φ. The number of nodes added by
the first encounter, where the φ’s first solution is found, and the encounter where it finally solves
φ, together equal the number of nodes required in AO to solve φ. Hence, the “encounters without
solving” do not increase the size of #DPLL-Cache’s search tree by more than a polynomial.
Finally, we note that the construction given accommodates the use of dynamic variable orderings
where the order of variables varies from branch to branch in the AND/OR search tree. (Varying
the value assigned along the left and right branch of each query variable is also accommodated).
That is, the proof also shows that #DPLL-Cache polynomially simulates AND/OR-Cache+ and
RC-Cache+ . 2
Theorem 6 None of RC-Space, RC-Cache, AND/OR-Cache, AND/OR-Space or VE can polynomially simulate #DPLL-Cache, #DPLL-Space, or #DPLL.
To prove this theorem we first observe that from a result of Johannsen (Johannsen, 2001),
#DPLL-Cache, #DPLL-Space, and #DPLL can all solve the negation of the propositional stringof-pearls principle (Bonet, Esteban, Galesi, & Johannsen, 1998) in time nO(log n) , when run with a
dynamic variable ordering. Then we prove (in Theorem 7) that all of the other algorithms require
time exponential in n on this problem. Hence, none of these algorithms can polynomially simulate
#DPLL (or the stronger #DPLL-Space or #DPLL-Cache).
434

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

The string-of-pearls principle, introduced in a different form by Clote and Setzer (1998) and
explicitly by Bonet et al. (1998) is as follows. From a bag of m pearls, which are colored red and
blue, n pearls are chosen and placed on a string. The string-of-pearls principle says that if the first
pearl in the string is red and the last one is blue, then there must be a red-blue or blue-red pair of
pearls side-by-side somewhere on the string. The negation of the principle, Sm,n , is expressed with
variables pi,j and pj for i ∈ [n] and j ∈ [m] where pi,j represents whether pearl j is mapped to
vertex i on the string, and pj represents whether pearl j is colored blue (pj = 0) or red (pj = 1).
The clauses of SPm,n are as follows.
(1) Each hole gets at least one pearl: ∨m
j=1 pi,j , i ∈ [n].
(2) Each hole gets at most one pearl: (¬pi,j ∨ ¬pi,j ′ ), i ∈ [n] j ∈ [m] ,j ′ ∈ [m], j 6= j ′ .
(3) A pearl goes to at most one hole: (¬pi,j ∨ ¬pi′ ,j ), i ∈ [n], i′ ∈ [n], i 6= i′ , j ∈ [m].
(4) The leftmost hole gets assigned a red pearl and the rightmost hole gets assigned a blue pearl:
(¬p1,j ∨ pj ) and (¬pn,j ∨ ¬pj ), j ∈ [m].
(5) Any two adjacent holes get assigned pearls of the same color: (¬pi,j ∨ ¬pi+1,j ′ ∨ ¬pj ∨ pj ′ ),
1 ≤ i < n, j ∈ [m], j ′ ∈ [m], j 6= j ′ , and (¬pi,j ∨ ¬pi+1,j ′ ∨ pj ∨ ¬pj ′ ), 1 ≤ i < n, j ∈ [m],
j ′ ∈ [m], j 6= j ′ .
Johannsen (Johannsen, 2001) shows that SPn,n has quasipolynomial size tree resolution proofs.
It follows that #DPLL, #DPLL-Space and #DPLL-Cache can solve SPn,n in quasipolynomial time.
Lemma 4 (Johannsen, 2001) SPn,n can be solved in time nO(log n) by #DPLL, #DPLL-Space, and
#DPLL-Cache.
Theorem 7 Let ǫ = 1/5. Any of the algorithms RC-Space, RC-Cache, AND/OR-Cache, AND/ORǫ
Space, VE, or #DPLL-Cache using a static variable ordering, require time 2n to solve SPn,n .
Proof: It can be seen from the proof of Theorem 5 that #DPLL-Cache using a static variable
ordering can polynomially simulate all of the stated algorithms.
ǫ
Hence, it suffices to prove that #DPLL-Cache under any static ordering requires time 2n for
SPm,n , m = n. By a static ordering, we mean that the variables are queried according to this
ordering as long as they are mentioned in the current formula. That is, we allow a variable to be
skipped over if it is irrelevant to the formula currently under consideration. We will visualize SPn,n
as a bipartite graph, with n vertices on the left, and n pearls on the right. There is a pearl variable
pj corresponding to each of the n pearls, and an edge variable pi,j for every vertex-pearl pair. (Note
that there are no variables corresponding to the vertices but we will still refer to them.)
Fix a particular total ordering of the underlying n2 + n variables, θ1 , θ2 , . . . , θl . For a pearl j,
let fanin t (j) equal the number of edge variables pk,j incident with pearl j that are one of the first
t variables queried. Similarly, for a vertex i, let fanin t (i) equal the number of edge variables pi,k
incident with vertex i that are one of the first t variables queried. For a set of pearls S, let fanin t (S)
equal the number of edge variables pk,j incident with some pearl j ∈ S that are one of the first t
variables queried. Similarly for a set of vertices S, fanin t (S) equals the number of edge variables
pi,k incident with some vertex i ∈ S that are one of the first t variables queried. Let edgest (j) and
435

BACCHUS , DALMAO , & P ITASSI

edgest (S) be defined similarly although now it is the set of such edges rather than the number of
such edges. It should be clear from the context whether the domain objects are pearls or vertices.
We use a simple procedure, based on the particular ordering of the variables, for marking each
pearl with either a C or with an F as follows. In this procedure, a pearl may at some point be marked
with a C and then later overwritten with an F; however, once a pearl is marked with an F, it remains
an F for the duration of the procedure. If a pearl j is marked with a C at some particular point in
time, t, this means that at this point, the color of the pearl has already been queried, and fanin t (j)
is less than nδ , δ = 2/5. If a pearl j is marked with an F at some particular point in time t, it means
that at this point fanin t (j) is at least nδ . (The color of j may or may not have been queried.) If a
pearl j is unmarked at time t, this means that its color has not yet been queried, and fanin t (j) is
less than nδ .
For l from 1 to n2 +n, we do the following. If the lth variable queried is a pearl variable (θl = pj
for some j), and less than nδ edges pi,j incident to j have been queried so far, then mark pj with
a C. Otherwise, if the lth variable queried is an edge variable (θl = pi,j ) and fanin l (j) ≥ nδ , then
mark pearl j with an F (if not already marked with an F). Otherwise, leave pearl j unmarked.
Eventually every pearl will become marked F. Consider the first time t∗ where we have either
a lot of C’s, or a lot of F’s. More precisely, let t∗ be the first time where either there are exactly
nǫ C’s (and less than this many F’s) or where there are exactly nǫ F’s (and less than this many
C’s.) If exactly nǫ C’s occurs first, then we will call this case (a). Extend t∗ to t∗a as follows.
Let θt∗ +1 , . . . , θt∗ +c be the largest segment of variables that are all pearl variables pj such that j
is already marked with an F. Then t∗a = t∗ + c. Notice that the query immediately following θt∗a
is either a pearl variable pj that is currently unmarked, or an edge variable. On the other hand, if
exactly nǫ F’s occurs first, then we will call this case (b). Again, extend t∗ to t∗b to ensure that the
query immediately following θt∗b is either a pearl variable pj that is currently unmarked, or is an
edge variable.
The intuition is that in case (a) (a lot of C’s), a lot of pearls are colored prematurely–that is,
before we know what position they are mapped to–and hence a lot of queries must be asked. For
case (b) (a lot of F’s), a lot of edge variables are queried thus again a lot of queries will be asked.
We now proceed to prove this formally.
We begin with some notation and definitions. Let f = SPn,n , and let Vars(f ) denote the
set of all variables underlying f . A restriction ρ is a partial assignment of some of the variables
underlying f to either 0 or to 1. If a variable x is unassigned by ρ, we denote this by ρ(x) = ∗. Let
T be the DPLL tree based on the variable ordering θ. That is, T is a decision tree where variable θi
is queried at level i of T . Recall that corresponding to each node v of T is a formula f |ρ where ρ is
the restriction corresponding to the partial path from the root of T to v. The tree T is traversed by a
depth-first search. For each vertex v with corresponding path p that is traversed, we check to see if
f |p is already in the cache. If it is, then there is no need to traverse the subtree rooted below v. If
it is not yet in the cache, then we traverse the left subtree of v, followed by the right subtree of v.
After both subtrees have been traversed, we then pop back up to v, and store f |p in the cache. This
induces an ordering on the vertices (and corresponding paths) of T that are traversed—whenever
we pop back up to a vertex v (and thus, we can store its value in the cache), we put v (p) at the end
of the current order.
Lemma 5 Let f be SPn,n and let π be a static ordering of the variables. Let ρ be a partial restriction
of the variables. Then the runtime of #DPLL-Cache on (f, ρ) is not less than the runtime of #DPLLCache on (f |ρ , π ′ ), where π ′ is the ordering of the unassigned variables consistent with π.
436

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

Lemma 6 For any restriction ρ, if f |ρ 6= 0 and ρ(pi,j ) = ∗, then pi,j occurs in f |ρ .
Proof: Consider the clause Ci = (pi,1 ∨ . . . ∨ pi,m ) in f . Since pi,j is in this clause, if pi,j
does not occur in f |ρ , then Ci |ρ must equal 1. Thus there exists j ′ 6= j such that ρ(pi,j ′ ) = 1. But
then the clause (¬pi,j ∨ ¬pi,j ′ )|ρ = ¬pi,j and thus pi,j does not disappear from f |ρ . 2
Corollary 1 Let θ be a total ordering of Vars(f ). Let ρ, ρ′ be partial restrictions such that ρ sets
exactly θ1 , . . . , θq and ρ′ sets exactly θ1 , . . . , θq′ , q ′ < q. Suppose that there exists θk = pi,j such
that ρ sets θk but ρ′ (θk ) = ∗. Then either f |ρ = 0 or f |ρ′ = 0 or f |ρ 6= f |ρ′ .
Case (a). Let θ be a total ordering to Vars(f ) such that case (a) holds. Let P C denote the set of
exactly nǫ pearls that are marked C and let P F denote the set of less than nǫ pearls (disjoint from
P C ) that are marked F. Note that (the color of) all pearls in P C have been queried by time t∗a ; the
color of the pearls in P F may be queried by time t∗a , and the color of all pearls in P − P C − P F
have not been queried by time t∗a . Note further that the total number of edges pi,j that have been
queried is at most nǫ+δ + n1+ǫ ≤ 2n1+ǫ .
ǫ
We will define a partial restriction, Ma , to all but 2n of the variables in θ1 , . . . , θt∗a as follows.
For each j ∈ P F , fix a one-to-one mapping from P F to [n] such that range(j) ∈ edgest∗a (j) for
each j. For each j ∈ P C , for any variable pi,j queried in θ1 , . . . θt∗a , set pi,j to 0. For any vertex i
such that all variables pi,j have been queried in θ1 , . . . , θt∗a , map i to exactly one pearl j such that
pj ∈ P − P C − P F . There are at most 2nǫ such i. (This can be arbitrary as long as it is consistent
with the one-to-one mapping already defined on P F .) For all remaining pj ∈ P − P C − P F that
have not yet been mapped to, set all queried variables pi,j to 0. For all pearls pj in P F that have
been queried in θ1 , . . . , θt∗a , assign a fixed color to each such pearl (all Red or all Blue) so that the
smallest Red/Blue gap is as large as possible. Note that the gap will be of size at least n1−ǫ . Ma
sets all variables in θ1 , . . . θt∗a except for the variables pj , j ∈ P C . Since there are nǫ such variables,
ǫ
the number of restrictions ρ to θ1 , . . . , θt∗a consistent with Ma is exactly 2n . Let S denote this set
of restrictions.
Let f ′ = f |Ma and let θ ′ be be the ordering on the unassigned variables consistent with θ. (The
set of unassigned variables is: pj , for j ∈ P C , plus all variables in θk , k > t∗a .) Let T ′ be the DPLL
tree corresponding to θ ′ for solving f ′ . By Lemma 5, it suffices to show that #DPLL-Cache when
ǫ
run on inputs f ′ and T ′ , takes time at least 2n .
Note that the first nǫ variables queried in T ′ are the pearl variables in P C , and thus the set of all
ǫ
n
2 paths of height exactly nǫ in T ′ correspond to the set S of all possible settings to these variables.
ǫ
We want to show that for each vertex v of height nǫ in T ′ (corresponding to each of the 2n settings
of all variables in P C ), that v must be traversed by #DPLL-Cache, and thus the runtime is at least
ǫ
2n .
Fix such a vertex v, and corresponding path ρ ∈ S. If v is not traversed, then there is some
ρ′ ⊆ ρ and some σ such that σ occurs before ρ′ in the ordering, and such that f ′ |σ = f ′ |ρ′ . We want
to show that this cannot happen. There are several cases to consider.
1a. Suppose that |σ| ≤ nǫ and σ 6= ρ′ . Then both ρ′ and σ are partial assignments to some of the
variables in P C that are inconsistent with one another. It is easy to check that in this case,
f ′ |ρ′ 6= f ′ |σ .
2a. Suppose that |σ| > nǫ , and the (nǫ + 1)st variable set by σ is an edge variable pi,j . Because
|ρ′ | ≤ nǫ , ρ′ (pi,j ) = ∗. By Corollary 1, it follows that f ′ |ρ′ 6= f ′ |σ .
437

BACCHUS , DALMAO , & P ITASSI

3a. Suppose that |σ| > nǫ and the (nǫ + 1)st variable set by σ is a pearl variable pj . (Again, we
know that pj is unset by ρ′ .) Since this is case (a), we can assume that pj ∈ P − P C − P F .
Call a vertex i bad if P − P F − P C ⊂ edgest∗a (i). If i is bad, then fanin t∗a (i) is greater
than n − 2nǫ ≥ n/2. Since the total number of edges queried is at most 2n1+ǫ , if follows
that the number of bad vertices is at most 4nǫ . This implies that we can find a pair i, i + 1 of
vertices and a pearl j ′ such that: (1) pi,j is not queried in θ1 , . . . , θt∗a ; (2) pi+1,j ′ is not queried
in θ1 , . . . , θt∗a ; (3) pj ′ is in P − P C − P F and thus pj ′ is also not queried. Thus the clause
(¬pi,j ∨ ¬pj ∨ ¬pi+1,j ′ ∨ pj ′ )|ρ′ does not disappear or shrink in f ′ |ρ′ , and thus f ′ |ρ′ 6= f ′ |σ .
Case (b). Let θ be a total ordering to Vars(f ) such that case (b) holds. Now let P C denote the set
of less than nǫ pearls marked C and let P F denote the set of exactly nǫ pearls marked F.
ǫ
We define a partial restriction Mb to all but 2n of the variables in θ1 , . . . , θt∗ as follows. Call a
vertex i full if all variables pi,j have been queried in θ1 , . . . , θt∗b . There are at most nǫ full vertices.
For each j ∈ P F , we will fix a pair of vertices Fj = (ij , i′j ) in [n]. Let the union of all nǫ sets Fj be
denoted by F . F has the following properties. (1) For each j, no element of Fj is full; (2) For each
j ∈ P F , Fj ∈ edgest∗b (j); and (3) every two distinct elements in F are at least distance 4 apart.
Since f anint∗b (j) ≥ nδ , and δ = 2/5 > ǫ, it is possible to find such sets Fj satisfying these criteria.
For each pi,j queried in θ1 , . . . θt∗b , where j ∈ P F and i 6∈ Fj , Mb will set pi,j to 0. For each
j ∈ P C , and for any variable pi,j queried in θ1 , . . . θt∗b , set pi,j to 0. For any full vertex i , map i
to exactly one pearl j such that pj ∈ P − P C − P F . (Again this can be arbitrary as long as it is
consistent with a one-to-one mapping.) For the remaining pj ∈ P − P C − P F that have not yet
been mapped to, set all queried variables pi,j to 0. For all pearls pj in P C , color them Red. For all
pearls pj in P F that have been queried, assign a fixed color to each pearl.
The only variables that were queried in θ1 , . . . θt∗b and that are not set by Mb are the edge
ǫ
variables, pi,j , where j ∈ P F , and i ∈ Fj . Let S denote the set of all 2n settings of these edge
variables such that each j ∈ P F is mapped to exactly one element in Fj . Let f ′ = f |Mb and let
T ′ be the DPLL tree corresponding to θ ′ for solving f ′ , where θ ′ is the ordering on the unassigned
variables consistent with θ. By Lemma 5, it suffices to show that #DPLL-Cache on f ′ and T ′ takes
ǫ
time at least 2n .
Note that the first 2nǫ variables queried in T ′ are the variables Pij ,j , Pi′j ,j , j ∈ P F . The
only nontrivial paths of height 2nǫ in T ′ are those were each j ∈ P F is mapped to exactly one
vertex in Fj , since otherwise the formula f ′ is set to 0. Thus, the nontrivial paths in T ′ of height
2nǫ correspond to S. We want to show that for each such nontrivial vertex v of height 2nǫ in T ′
(corresponding to each of the restrictions in S), that v must be traversed by #DPLL-Cache, and thus
ǫ
the runtime is at least 2n .
Fix a vertex v and corresponding path ρ ∈ S. Again we want to show that for any ρ′ ⊆ ρ, and
σ where σ occurs before ρ′ in the ordering, that f ′ |ρ′ 6= f ′ |σ . There are three cases to consider.
1b. Suppose that |σ| ≤ 2nǫ . If σ is nontrivial, then both ρ′ and σ are partial mappings of the pearls
j in P F to Fj , that are inconsistent with one another. It is easy to check that in this case
f ′ |σ 6= f ′ |ρ′ .
2b. Suppose that |σ| > 2nǫ and the (2nǫ + 1)st variable set by σ is an edge variable pi,j . Because
|ρ′ | ≤ 2nǫ , ρ′ (pi,j ) = ∗. By Corollary 1, it follows that f ′ |σ 6= f ′ |ρ′ .
438

BACKTRACKING S EARCH

FOR

#SAT AND BAYES

3b. Suppose that |σ| > 2nǫ and the (2nǫ + 1)st variable set by σ is a pearl variable pj . By the
definition of t∗b , we can assume that pj ∈ P − P C − P F . By reasoning similar to case 3a, can
find vertices i, i+1, and pearl j ′ ∈ P −P C −P F such that none of the variable pi,j , pi+1,j , pj ′
are queried in θ1 , . . . , θt∗b . Thus the clause (¬pi,j ∨ ¬pj ∨ ¬pi+1,j ′ ∨ pj ′ )|ρ′ does not disappear
to shrink in f ′ |ρ′ 1, and therefore f ′ |ρ′ 6= f ′ |σ .
ǫ

Thus for each of the two cases, #DPLL-Cache on f ′ and T ′ takes time at least 2n and thus
ǫ
#DPLL-Cache on f and T takes time at least 2n . 2


