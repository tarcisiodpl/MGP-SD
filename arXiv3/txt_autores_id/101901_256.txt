

When using Bayesian networks for modelling
the behavior of man-made machinery, it usu­
ally happens that a large part of the model
is deterministic. For such Bayesian networks
the deterministic part of the model can be
represented as a Boolean function, and a cen­
tral part of belief updating reduces to the
task of calculating the number of satisfying
configurations in a Boolean function. In this
paper we explore how advances in the calcu­
lation of Boolean functions can be adopted
for belief updating, in particular within the
context of troubleshooting. We present ex­
perimental results indicating a substantial
speed-up compared to traditional junction
tree propagation.
1

INTRODUCTION

When building a Bayesian network model it frequently
happens that a large part of the model is determinis­
tic. This happens particularly when modelling the be­
havior of man-made machinery. Then the situation is
that we have a deterministic kernel with surrounding
chance variables, and it seems excessive to use stan­
dard junction tree algorithms for belief updating. First
of all, the calculations in the deterministic kernel are
integer calculations and double precision calculations
are unnecessary complex. However, there may be room
for further improvements. If the deterministic part of
the model is represented as a Boolean function, we
may exploit contemporary advances in calculation of
Boolean functions.
A major advance in Boolean calculation is Binary
Decision Diagrams, particularly Reduced Ordered Bi­
nary Decision Diagrams, ROBDDs[Bryant, 1986]. An
ROBDD is a DAG representation of a Boolean func­
tion. The representation is tailored for fast calculation

Uffe Kjcerulff

of values, but the representation can also be used for
fast calculation of the number of satisfying configura­
tions given an instantiation of a subset of the variables.
To be more precise: let B(X) be a Boolean function
over the Boolean variables X, and let Y � X with
X\Y. Define Cards (1)) on a configuration iJ
Z
of y as the number of configurations z over Z such
that B (1j, z)
true( 1). It turns out that given iJ an
ROBDD representation of B can be constructed such
that Cards can be calculated in time linear in the num­
ber of nodes in the ROBDD. However, the number of
nodes in an ROBDD may be exponential in the num­
ber of variables in the domain of the Boolean function.
=

=

In this paper we exploit the ROBDD representation for
propagation through a Boolean kernel in a Bayesian
network, and we illustrate that a central part of this
propagation is to calculate Cards (i)). We use the tech­
nique on models for troubleshooting. These models
are particularly well suited for ROBDD calculation as
the size of the ROBDD is quadratic in the size of the
domain.
In section 2 we illustrate the use of Cards for prob­
·
ability updating in Bayesian networks. Section 3 is
a brief introduction to ROBDDs and in section 4 we
show how to calculate Cards in an ROBDD. Section
5 introduces the troubleshooting task and the type of
Bayesian network models used. In section 6 the de­
terministic kernel of these models is represented as an
ROBDD and it is shown that the size of this represen­
tation is quadratic in the number of Boolean variables.
In section 7 we outline the propagation algorithms for
various troubleshooting tasks, and in section 8 we re­
port on empirical results indicating a substantial speed
up compared to traditional junction tree propagation.
2

TWO MOTIVATING EXAMPLES

To illustrate the special considerations in connection
with Boolean kernels we shall treat a couple of exam-

427

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

3

ples. First consider the situation in Figure 1.

BOOLEAN FUNCTIONS AND
ROBDDS

This section is a survey of classical logic in the context
of binary decision diagrams.
Figure 1: The Boolean variable A has a parent net­
work of proper chance variables and a child network
representing a Boolean function B.
For the situation in Figure 1 we have (U
P (U)

=

=

WUVU{A}):

Q (W,A)I.t B (V,A),

where 1-l = 1/ Lvu{A} B (V,A) is a normalization con­
stant. Assume we have evidence e ew u ev , where
ev is a configuration y of the variables Y � V, then:
=

P(A,e)

1-l

1-l

L
L
w
w

Q(W,ew,A) L B(Z,y,A)
z

If we extend the example s.t. a Boolean variable C E V
has a child network R (T, C) of proper chance variables,
we get ( the normalization constant is omitted) :
P (U) = Q (W,A)B (V,A)R (T,C)
Assume we have evidence e = ew u ev u er, where ev
is a configuration y of the variables Y � V. If er is
empty then R does not contribute, and the calculations
are as for Figure 1. If not, we have:

L
L (L
w

Q(W,ew,A)

·

B (Z,y, A, C)

Z

C

L

)

T

w

(L
L

B (Z,y,A,C

=

y)

Z

+

T

B (Z,y,A, C =n )

Z
=

L Q(W,ew,A)

R (T,e7,C =n )

)

T
·

w

(

CardB(y,A, C = y)

+

Card8(Y,A, C = n

� R (T,er, C
) � R (T,er, C

=

y)

=n )

All operators in propositional logic can be expressed
using only this operator and this can be done s.t. tests
are only performed on unnegated variables.
Definition 1. An If-then-else Normal Form (INF) is
a Boolean function built entirely from the if-then-else
operator and the constants 0 and 1 s.t. all tests are
performed only on variables.

B

R (T,er,C = y )

L
L

Let X --7 Y1, Y2 denote the if-then-else operator. Then
X --7 Y1 , Y2 is true if either X and Y, are true or X is
false and Y2 is true; the variable X is said to be the
test expression. More formally we have:

Consider the Boolean function B and let B[X H 0]
denote the Boolean function produced by substituting
0 for X in B. The Shannon expansion of B w.r.t. X is
defined as:

R (T,e7, C)

L Q (W,ew,A)·

=

A truth assignment to a Boolean function B is the
same as fixing a set of variables in the domain of B,
i.e., if X is a Boolean variable in the domain of B, then
X can be assigned either 0 or 1 ( denoted [X H 0] and
[X H 1], respectively) .
A Boolean function is said to be a tautology if it yields
true for all truth assignments, and it is satisfiable if it
yields true for at least one truth assignment.

Q(W,ew,A)CardB (y,A),

As the example illustrates, an efficient procedure for
calculating CardB is central for probability updating.

P(A,e) =

The classical calculus for dealing with truth assign­
ments consists of Boolean variables, the constants true
(1) and false (0) and the operators 1\ (conjunction),
V (disjunction), -. (negation), =} (implication) and {:::}
(bi-implication). A combination of these entities form
a Boolean function and the set of all Boolean functions
is known as propositional logic.

)

Again, calculation of Card8 is part of belief updating.

=:

X -t B [(<

H

1], B [X H 0]

From the Shannon expansion we get that any Boolean
function can be expressed in INF by iteratively using
the above substitution scheme on B.
By applying the Shannon expansion to a Boolean func­
tion B w.r.t. an ordering of all the variables in the do­
main of B we get a set of if-then-else expressions which
can be represented as a binary decision tree. The de­
cision tree may contain identical substructures and by
"collapsing" such substructures we get a binary deci­
sion diagram ( BDD) which is a directed acyclic graph.
The ordering of the variables, corresponding to the
order in which the Shannon expansion is performed,

428

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

is encoded in the BDD hence, we say that the BDD
is an ordered binary decision diagram (OBDD); the
variables occur in the same order on all paths from the
root. If all redundant tests are removed in an OBDD
it is said to be reduced and we have a reduced ordered
binary decision diagram (ROBDD).
Definition 2. A reduced ordered binary decision di­
agram {ROBDD) is a rooted, directed acyclic graph
with
•

•

•

•

ROBDD. The algorithm basically propagates a num­
ber (2 n , where n is the number of distinct variables
in the corresponding Boolean function) from the root
of the ROBDD to the terminal node. The value sent
from a node (including the root) to one of its children
is the value associated with that node divided by 2.
The value associated with a node (except the root) is
the sum of the values sent from its parents (see Fig­
ure 2).

one or two terminal nodes labeled 0 and 1 respec­
tively.
a set of non-terminal nodes of out-degree two with
one outgoing arc labeled 0 and the other 1.
a variable name attached to each non-terminal
node s.t. on all paths from the root to the ter­
minal nodes the variables respect a given linear
ordering.
no two nodes have isomorphic subgraphs.

We will use Eo to denote the set of 0-arcs (drawn as
dashed arcs) and £ 1 to denote the set of l-ares (drawn
as solid arcs).
Theorem 1 ([Bryant, 1986]). For any Boolean
function f : {0,l}n --7 {0,1} there is exactly one
ROBDD B with variables X, < X2 <
< Xn
s.t. B[X, H b1 ,X2 H b2, ... ,Xn H bnJ
f (b1,b2,... ,bn), \i (b1,b2,... ,bn) E {0,l}n.
·

Figure 2: There are 3 satisfying configurations for
the Boolean function "Exactly one variable among
A 1, A 2, A 3 is true" represented by this ROBDD.
Definition 3. Let B = (U,£) be an ROBDD. Propa­
gation in B is the computation of v (u), where u E U
and v : U --7 lR is defined as:

· ·

From Theorem 1 we have that in order to calculate
the number of satisfying configurations in a Boolean
function B we can produce an ROBDD equivalent to
B and then count in this structure.
In the remainder of this paper we assume that an
ROBDD has exactly one terminal node labeled 1, as we
are only interested in the number of satisfying configu­
rations; in this situation we allow non-terminal nodes
with out-degree one. Additionally, we will use the term
"nodes" in the context of ROBDDs and "variables"
when referring to a Boolean function or a Bayesian
network(BN); nodes and variables will be denoted with
lower case letters and upper case letters, respectively
(the nodes representing a variable Xi will each be de­
noted Xi if this does not cause any confusion).
4

2000

CALCULATION OF CARDB
USING ROBDDS

Given an ROBDD representation of a Boolean func­
tion B, the number of satisfying configurations can be
calculated in time linear in the number of nodes in the

•

•

v (r) 2 n , where r is the root in B and n is the
number of distinct variables in B.
LpEnd' v(p) , where nu repre\iu E U\{r}: v (u)
=

=

sents the set of parents for u in B.

So, in order to determine Cards for some Boolean
function B (U) we only need one propagation in the
corresponding ROBDD since Cards
v (l). In case
evidence y has been received on the variables Y � U
we simply modify the algorithm s.t. configurations, in­
consistent with y, does not contribute to the propaga­
tion, i.e., given a configuration y the function v (u)y is
defined as:
=

\iu E U\{r}.. v (u)y

_
-

LvEna v(p)y ,
2

where nR = {p E nul[p (j. Yl or [y(p) = i and (p, u) E
£i]}; y(p) is the state ofp E Y under y and v (r) 2n,
n being the number of distinct variables in B including
those on which evidence has been received. In partic­
ular we have that Cards (i:i) = v (1)y. Notice, that the
structure of the ROBDD is not changed when evidence
is received.
=

The size of the ROBDD has a significant impact on
the performance of the algorithm and the problem of

429

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

identifying a minimal sized ROBDD is NP-complete.
Thus, in the remainder of this paper we shall mainly
focus on troubleshooting models as it turns out that
the structure of such a model ensures that the size of
the corresponding ROBDD is at most quadratic in the
size of the domain.
5

TROUBLESHOOTING

Definition 4. A troubleshooting model is a con­
nected BN T5 =((U =Us U Uc U UA,£), P), where:
•

•

The set Us contains a distinct variable 5 with no
successors, and for each 51 E Us\{5} there exists
a directed path from 51 to 5.
For each variable C EUc there exists an 51 EUs
s.t. C Ens, and nc 0.
=

When troubleshooting a device which is not working
properly we wish to determine the cause of the problem
or find an action sequence repairing the device. At any
time during the process there may be numerous differ­
ent operations that can be performed e.g. a component
can be repaired/replaced or the status of a component
can be investigated. Because such operations can be
expensive and may not result in a functioning device,
it is expedient to determine a sequence of operations
that minimizes the expected cost and (eventually) re­
pairs the device.
[Breese and Heckerman, 1996] presents a method to
myopicly determine such a sequence. The method as­
sumes a BN representing the device in question, and
the BN is assumed to satisfy the following properties.
1. There is only one problem defining variable in the
BN and this variable represents the functional sta­
tus of the device.

2. The device is initially faulty.
3. Exactly one component is malfunctioning causing

the device to be faulty (single fault).
A central task of troubleshooting, within the frame­
work of [Breese and Heckerman, 1996], is the calcula­
tion of Pi =P( C i =faulty!e) which denotes the prob­
ability that component ci is the cause of the problem
given evidence e. So we are looking for a way to exploit
the logical structure of the model when calculating the
probabilities Pi· As such a scheme is strongly depen­
dent on the structure of the troubleshooting model we
give a syntactical definition of this concept. The def­
inition is based on BNs: a BN consists of a directed
acyclic graph G =(U, £) and a joint probability dis­
tribution P(U), where U is a set of variables and £ is
a set of edges connecting the variables in U; we use
sp(X) to denote the state space for a variable X E U.
The joint probability distribution P(U) factorizes over
U s.t. :

P(U)

=

•

•

•

For each variable A EUA there does not exist an
X EU s.t. A E nx.
sp(X) ={ok, �ok}, VX EUs U Uc.
For each X E Us: P(xly) =1 or P(xly)
sp(X) and Vy E sp(nx).

=

0, Vx E

The variable 5 is termed the problem defining variable
and the variables Us are termed system variables. The
variables Uc (termed cause variables) represent the set
of components which can be repaired, and the vari­
ables in UA (termed action variables) represent user
performable operations such as observations and sys­
tem repairing actions; notice that UA is not part of
the actual system specification. In the remainder of
this paper we shall extend the single fault assump­
tion to include the system variables also. That is, if
a system variable 5i is faulty, then there exists ex­
actly one variable X E ns, which is faulty also (see
[Skaanning et al., 1999] for further discussion of this
extension and how the single fault assumption can be
enforced using so-called "constraint variables" ).
Figure 3 depicts a troubleshooting model, where A is
an action variable and 5 represents the problem defin­
ing variable. The variables 51 ,5z,53 and 54 repre­
sent subsystems, which should be read as: the sys­
tem 5 can be decomposed into two subsystems 51 and
5z, and subsystem 51 can be decomposed into 53 and
54. Component C 1 can cause either 53 or 54 to fail,
whereas C z can cause either 5z or 54 to fail (neither
C 1 nor C z can cause two subsystems to fail simultane­
ously). Notice that A is not part of the actual system
model.

IJ P(XInx),

XEU

where nx is the parents of X in G. The set of con­
ditional probability distributions factorizing P(U) ac­
cording to G is denoted P.

Figure 3: A troubleshooting model with five system
variables, two cause variables and one action variable.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

430

From assumption (2) and (3) we have that:

P(C1 =y, e)
=P(C1 =1J, Cz =n, . . . , Cm =n, e)
m
= P(C1 =y)
P(Ci n)
i=2
�-tB(C1 = y, Cz n, .. . , Cm =n, Us, e)

II

=

.L_

=

Us

II P(Ci

=

i=2

n ) �-tCard s (C1 = y, e),
•

where B(Us, Uc) is a Boolean function (specified in the
following section) and J.t is a normalization constant.
Now, P(C11el = P(C1, e)/P(e) and P(e) is given by:

m

.[_ IJ P(Cd!-!B(C1, ··· , Cm, S, S, . . . , Sn, el
u

i=1

In the remainder of this paper we omit the normaliza­
tion constant.
6

•

m

= P(C1 =y)

P(e) =

(and only one) of its subsystems (Sc) is faulty (if a
cause is not present we can not say anything about its
subsystems). M says that there can be either zero or
at most one cause present (consistent with the system
state). B(U) is the Boolean function representing the
system as a whole. Note that:

ROBDDS AS
TROUBLESHOOTING MODELS

•

The Boolean function is a list of expressions for
local constraints and it can therefore be built in
an incremental fashion.
The Boolean function can easily be modified to
represent any logical relation between the compo­
nents.
The expression ensures the single fault assump­
tion based on the structure of the model, i.e., it is
not necessary to introduce "constraint variables" .

Example 1. The Boolean function representing the
troubleshooting model depicted in Figure 3 is specified
by B:

((SA(S1 l3l Sz)) V (�SA--,51--,Sz))
B1A((S1A(S3 l3l S4)) V (�S1A�s3�S4))

In what follows we shall assume single fault and use
the truth values 1 and 0 to denote the state of a com­
ponent/subsystem (1 indicates a fault).

B3A(Cz:::} (Sz!Zl S4))
84A((SA(C1 IZl Cz)) V (�SA�C1A�Cz))

Now, let nsi be the subsystems which immediately
compose Si E Us and let Sc � Us be the subsys­
tems that component C E Uc can cause to fail; Sc
is the immediate successors of C. The Boolean func­
tion representing the logical kernel of a troubleshoot­
ing model TS =((Us UUc UUA, £), P) is then given by
B(U' =Us UUc):

Given the ordering S, S1, Sz, S3, S4, C1, C2, the
ROBDD corresponding to B is depicted in Figure 4.
Note that all paths from the root S to the terminal
node are consistent with the ordering above.1
D

F(T)

G(C)

(

®

TA

) (

s' v �rA

S'EnT

Q9 T

C=}

1\

�s'

S'EnT

TESc

M

( ® ) (
( f\ ) ( f\
c

sA

v

�sA

CEUc

B(U')

F(T) A

TEUs

1\
CEUc

�c

)

)

)

G(C) AM,

CEUc

where ® �=1 xi denotes an exclusive-or between the
variables {X1, . . . , Xn}. F(T) specifies that if the sys­
tem Tis malfunctioning then one (and only) of its sub­
systems is faulty, and if the system is functioning prop­
erly then all of its subsystems are functioning properly
also. G(C) states that if a cause is present then one

Figure 4: An ROBDD representation of the trou­
bleshooting model depicted in Figure 3.
1The ROBDD was generated by the software tool
http:/ fwww.cs.auc.dk/�behrmann/iben/.

iben,

431

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Now, as indicated in Section 3, the size of the ROBDD
is dependent on the ordering of the variables. So we
are looking for a general "rule of ordering" producing
ROBDDs of "small" size.
Consider an ordering of the variables where each sys­
tem variable occurs before all the variables repre­
senting its subsystems, and where all the cause vari­
ables occur last in the ordering. By constructing the
ROBDD according to this ordering we get the node
representing the problem defining variable as root and
the nodes representing the cause variables at the bot­
tom (see Figure 4). Moreover, we get an upper bound
on the size of the ROBDD as stated in the following
theorem; note that the action variables are not part of
the logical kernel.
Theorem 2. Let TS=((U =UAUUsUUc,£), P) be a
troubleshooting model. Then the size of the ROBDD,
representing the Boolean function B(Us U Uc), is
O(IUsi2+1Ucl2), if the ordering a: UsUUc H IUsUUcl
satisfies:
•

•

VX E Us: a(X) < a(Y) for each Y E nx.
VZ E Uc there does not exist an X
a(Z) < a(X).

E

Us s.t.

Proof. Assume an indexing of the layers in the
ROBDD s.t. the layers containing the root node and
the terminal node have index 1 and IUs U Ucl + 1, re­
spectively; a layer is the set of nodes representing a
distinct variable.
Now, consider the layers consisting of system nodes
but no cause nodes. The number of nodes in the i'th
layer either equals the number of nodes in the i'th - 1
layer or it has exactly one more node than the i'th- 1
layer. This is the same as saying that at most one
node in the i'th- 1 layer branches in two; if two differ­
ent nodes in a layer branched into two we would have
two distinct paths from a node at a higher level to
these nodes however, this contradicts the single-fault
assumption due to the ordering of the nodes. Thus,
the number of nodes in the layers containing system
1
nodes is at most L. �Z::11 i= IUs l{l�s I+ 1.
For the cause nodes, there can be at most one distinct
path for each of their possible configurations. This
means that the number of nodes in the layers contain­
ing cause nodes is at most IUcl('�cl) = 1Ucl2. Hence,
D
the size of the ROBDD is O(IUsl2 + IUcl2).
In the ROBDDs, we have an all-false path from the
root to the terminal node. Indeed the Boolean function
is true when the model has no fault. However, we can
force S to be true (faulty) to avoid this path.

7

PROPAG ATION USING ROBDDS

For our context, we need to compute the number of
satisfying configurations for each instantiation of the
cause variables (see Section 5). Now, if we order the
variables as described in Theorem 2 we get an ROBDD
where the nodes representing the cause variables are
the nodes closest to the terminal node. This means
that after one propagation we can determine all the
values needed, i.e., the number of configurations con­
sistent with ci=1J and evidence e is given by:

CardB(C=y,e)=

c· )
" v(#\ie'
­

L

CtECt

2

where Ci is the set of nodes Ci with an outgoing 1-arc
and # li is the number of arcs on the path li from the
Ci in question to the terminal node; the single-fault
assumption ensures that there exist exactly one path
from each Ci to the terminal node which include the
1-arc emanating from Ci.
However, this scheme does not take user performable
operations (i.e. UA) into account, and in the follow­
ing section we extend the algorithm to include such
scenarios.
7.1

Inserting evidence

After an action has been performed we may gain new
knowledge about the system. This knowledge is incor­
porated into the model by instantiating the appropri­
ate variable. If either a system variable or a cause vari­
able is instantiated we can use the method described in
Section 4. So, let A E UA be a binary variable associ­
ated with a proper conditional probability distribution
P(AISi) and assume that A= y is observed. In order
to take the state of A into consideration we get:

P(C1 =y, A=y)
=P(Cl =1J,Cz=n, ... ,Cm=n, A=y)
=P(C1 =y) IT P(Ci=n) L (P(A=yiSd
Us
B(C1 =y, Cz=n, ... , Cm =n,Us))
By expanding the sum in the above equation we get:

L P(A=yiSi)B(Cl =y, Cz=n, ... , Cm= n,Us)
Us
=P(A=yiSi=y)CardB(C1 =y, Si=y)
+P(A=yiSi=n)CardB(Cl =1J,Si=n)
(1)

Thus, with one piece of evidence we can retrieve the
probabilities with two propagations. However, if we
have a set of actions u;,._ � UA with parents u;, �Us

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

432

we need to count the number of satisfying configura­
tions consistent with each configuration of U$. So, by
using the above approach, the number of times we need
to count is exponential in the number of variables on
which evidence has been received.
In what follows we will consider a different algorithm,
where all values can be found after one propagation.
Initially we assume that evidence has been received on
exactly one variable, but the algorithm can easily be
generalized to any number of variables.
In order to prove the soundness of the algorithm
we will use the following notation. If B is an
ROBDD with root r, then l;_
{ v i r = v1, . . . , V;_ =
vis a directed path in B} is termed the i'th layer of
B; the layers l1 and ln+ 1 contain the root node and
the terminal node, respectively. So, given a Boolean
function over the variables U = {X1 , Xz, . . . , X n} (or­
dered by index), the corresponding ROBDD can be
1
specified as B =(Us = U��1 lk, £ = £1 U £o); assum­
ing that the variable X;_ is represented by the layer
l_; . Now, let f : sp(W) -t IR be a function where
W ={Xi, . . . , Xi} � U, and assume that the variables
are ordered by index. We define the following parti­
1
tioning of B =(Us = u�;;1 lk, £) w.r.t. f:
=

•

•

•

The root part of B w.r.t. f is given by BT =
1
(Uf3, £f3), where Uf3 = u�: 1lk.
The conditioning part of B w.r.t. f is given by
lk.
Be =(U� , £�), where U� =

ul=i

The terminal part of B w.r.t. f is given by Bt =
1
(U�, £�), where U� = u�,:i +1lk.

For ease of exposition, we shall in the remainder of this
section assume that no evidence has been received on
any variable in Us UUc; the results presented can easily
be generalized to this situation also.
Algorithm 1. Let B =(U = U�11li, £ £1 U £o) be
an ROBDD corresponding to a Boolean function over
the variables U = {X1 , Xz, . . . , Xn}, and assume that
the variables are ordered by index. Let f : sp(W) -t IR
be a function with W � U and let Q =W\{Xj}, where
X i E W is the variable with highest index.
=

i) Propagate from the root to the terminal nodes in
the root part of B.
ii) Use the values obtained in step (i) to perform a
propagation in the conditioning part of B, i. e. , for
each q E sp( Q) :
a) Propagate to layer li.
b) If there exists an arc (p, u) E Ci from a node
p E lj to a node u E li +1 add the value
to the value ofu.

c(ii.X;=;i)v(p)q)

iii) Use the values obtained in step (ii) to propagate
in st.

Note, that the number of variables in the domain off
determines the number of iterations performed by the
algorithm. In particular, if IWI = 1 we only need one
iteration.
Theorem 3. Let B = (U = U �1 li, £ = £1 U£o) be
an ROBDD and let f : sp(W) -t IR be a function where
W � U. If Algorithm 1 is invoked on B, then:

1

v(l)

=

L f(w)Cards(w)

wEsp(W)

Proof. Let Q =W\{Xj}, where X i E W is the variable
i
with highest index. Let q E sp( Q) and let n�q, ) =
{p E nuiP f/. W or (p, u) E £;_}. Then 'v'u E li+ we
have:

1

LqEsp(Q)

v(u)

LqEsp(Q)

(L.bE{0,1},pEn�<i.b l v(p)qf(q, bl)
2

(L.bE{O,l},pEn�'l.bl v(p) (q,b f(q, bl)
)

2
LwEsp(W) (L.vEn� v(p)wf(wl)
2
f
)
(
LwEsp{Wl w LvEn� v(p)w
2
L f(w)v(u)w
wEsp(W)

=
=

t 1 :
Let u E lt, for l > j + 1. Suppose that 'v'p E lv(p) = LwEsp(W) f(w)v(P lw · Then:
v(p) LvEnu LwEsp(W) f(w)v(p)w
=
v(u) LvEnu
2
2
=

In particular we have that for l =n + 1:

v( 1)

LwEsp(W) f(w) LvEnu v(p)w
2

L f(w)Card8(w)
wEsp(W)
Thereby completing the proof.

D

By performing induction in the number of operations
the algorithm can easily be extended to handle multi­
ple functions, assuming that the variables in the do­
main of the functions do not overlap; the variables in
the domain of two functions f and g are said to over­
lap w.r.t. the ordering a if a(Xd < a(X k ) < a(Xj ),
where xk is a variable in the domain of g, and xi and
Xi are the variables in the domain of f with lowest
and highest index, respectively. If the variables of two
functions overlap we can multiply these functions and
consider the resulting function.

433

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Example 2. Consider the troubleshooting model de­
picted in Figure 3, and assume that action A E UA
is associated with the conditional probability distribu­
tion specified in Table 1; nA = {5z, 54}.

B'

B'

C0

(0.

'•

...

,,

Table 1:
The conditional probability function
P(AI5z, 54).
Sz
Sz

=
=

0.6
0.4

0.3
0.2

1
0

�&
0.4. 2

The ROBDD corresponding to this specification is de­
picted in Figure 4. In the naive approach, if A = y is
observed, we perform three propagations to the termi­
nal node (one propagation for each configuration of 5z
and 54 except for (5z
1 , 54
1 ) due to the single
fault assumption). The resulting counts are weighted
with the appropriate values and then added (see equa­
tion 1): 0 0.3 + 4 0.2 + 1 0.4 + 1 0.6 = 1.8.
=

·

·

=

·

·

When using algorithm 1 we start off by propagating to
the layer l3 (the nodes representing 5z); after propa­
gation, each node in l3 is associated with 25. We then
perform two propagations to the layer ls (the nodes
representing 54); each propagation is conditioned on
the state of 5z, i.e., 1 and 0, respectively. After each
propagation, the resulting value is multiplied with the
appropriate value from the conditional probability ta­
ble and then added to the value associated with its
child. So, the final value can be found with less than
two full propagations (see Figure 5); note that we only
D
perform one propagation in W and in Bt.
Step (ii) of Algorithm 1 can be optimized by start­
ing the iteration with the variable with highest in­
dex, and then iterate in reverse order of their in­
dex. That is, when iterating over the variables
{X1, Xz, . . . , Xt-1, Xt} we can start off by propagat­
ing to the layer containing Xt, for some configuration
of {X1, Xz, . . . , Xt-1}. The values associated with the
nodes Xt-1 can then be used when propagating from
the nodes Xt, for each instance of Xt. The same ap­
plies when considering variables of lower index, i.e.,
we can reuse previous computations. For instance, in
Figure 5 we can use the value from the first iteration
when computing the value 0.2 22 associated with c1
(consistent with (5z = 0, 54 1 ) ).
·

=

8

RESULTS

We have measured the performance of the ROBDD
algorithm by comparing it to the Shafer-Shenoy algo­
rithm [Shafer and Shenoy, 1990] and the Hugin algo­
rithm [Jensen et al., 1990] w.r.t. the number of opera­
tions performed during inference; the number of opera-

2

�

.. ()_
B'

:· .
.

0 4• 2

2

�

_':_.. __

0

·

,.···

- --�r
{a)

8

'Q_......':... �.::. ..... 0..
.

..

.

0 6•2 2

··8

i

.8"

•..··

(bl

Figure 5: Figure (a) depicts the ROBDD after propa­
gation w.r.t. the configuration (5z = 0, 54 = 0). Fig­
ure (b) depicts the ROBDD after the full propagation;
no propagation is performed w.r.t. (Sz
1, 54
1)
due to the single fault assumption.
=

=

tions refers to the number of additions, multiplications
and divisions.
The tests were performed on 225 randomly generated
troubleshooting models (see Definition 4) which dif­
fered in the number of system variables, cause vari­
ables and action variables; the total number of vari­
ables varied from 21 to 322 and for a fixed set of vari­
ables 15 different troubleshooting models were gener­
ated. As the single fault assumption is not ensured in
the troubleshooting models we augmented these mod­
els with constraint variables when using the Hugin al­
gorithm and the Shafer-Shenoy algorithm (the single
fault assumption is naturally ensured in the ROBDD
architecture). Finally, evidence were inserted on the
problem defining variable and on the constraint vari­
ables.
Figure 6 show plots of the number of operations per­
formed as a function of the number of variables in the
models. Note that we use a logarithmic scale on the
y-axis and that the numbers on the x-axis do not rep­
resent the actual number of variables in the models.
The plots show that, w.r.t. the number of operations,
propagation using ROBDDs is considerably more ef­
ficient than both Shafer-Shenoy and Hugin propaga­
tion. Moreover, as indicated in Section 6, the tradi­
tional tradeoff between time and space is less apparent
in the ROBDD architecture, as the space complexity
is O(IUcl2 + IUsl2 ).
It should be noted that the tests were designed to

434

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

""�

20000

I

�
..

ll�

I

•

I

I

I

I

I

1

I

I

1

�

K

10000

I

1000

�

100

•
20

100

I
..
Variable�

1<0

Variable•
Sluorer-Sheuoy

Figure 6: A plot of the number of operations per­
formed by Hugin, Shafer-Shenoy and ROBDD prop­
agation as a function of the number of variables in
randomly generated troubleshooting models (logarith­
mic scale).
compare ROBDD propagation with Shafer-Shenoy and
Hugin propagation, and they should not be seen as a
comparison of Shafer-Shenoy propagation and Hugin
propagation. In particular, we have only considered
troubleshooting models and not Bayesian networks in
general.
The efficiency of the ROBDD architecture is partly
based on the single fault assumption. However, this
assumption can also be exploited in certain trou­
bleshooting models by compiling the original model
TS=((Us UUc UUA,£), P) into a secondary Bayesian
network BN ((UA U{CU
} {S,} £'),P'), where Cis a
variable having a state for each cause variable in the
original model together with a state representing the
situation where no fault is present. S is a problem
defining variable having Cas parent, and UA is the set
of action variables in the original model each having C
as parent. We have compared the ROBDD architec­
ture with this approach using the randomly generated
troubleshooting models from the previous tests (see
Figure 7).
=

By using this secondary representation the speed-up
is less apparent. However, if we allow multiple faults
then this representation can not be used. Moreover,
a troubleshooting model allowing multiple faults will
in general not be simpler than a model with no con­
straints on the number of faults. In the case of ROB­
DDs, assume that the single fault assumption still ap­
plies to the system variables and consider the case
where exactly m components can fail simultaneously;
m is generally "small" . In this situation the number
of nodes in the layers containing system nodes does
not change but the number of nodes in the layers con­
taining cause nodes do: there can be a distinct path
for each configuration of the cause nodes so the num-

Figure 7: A plot of the number of operations per­
formed by ROBDD propagation and Hugin propaga­
tion with a single cause node.
ber of nodes in the layers containing cause nodes is
at most IUcl(l�1). Hence the size of the ROBBD is
O(IUsl2 + IUcl(l�1)); note that in an ROBDD there
does not exist two nodes having isomorphic subgraphs
so the size of the ROBDD is usually much smaller.
Now, as the complexity of propagation in an ROBDD
is linear in its size, the maximum number of operations
performed for m = 2 increases by a factor of n2l ; with
m faults the maximum number of operations increases
i. This corresponds to adding a
by a factor of
constant value to the ROBDD plots in Figure 6 since
we use a logarithmic scale on the y-axis.

n��!=

Furthermore, if we redefine the m-faults assump­
tion to cover at most m faults then the number of
nodes in the layers containing cause nodes is at most
IUcl L�l euicl). Again, it should be noticed that the
actual number of nodes is usually significantly smaller
as isomorphic subgraphs are collapsed.
In case m-faults is extended to include system vari­
ables also, it can be shown that the variables can be
ordered s.t. the number of nodes in the layers contain­
ing system nodes is exponential in m but quadratic in
the number of system variables if m � maxsEUs Ins/
(see Figure 8).
Finally, as the single fault assumption no longer ap­
plies, the number of configurations consistent with
Ct =y and evidence y is given by:
Cards(Ct =y , y)

=

v(c· )-

L 2#\iy,
Ci ECi li E.Ci
�
L

where Ct is the set of nodes Ct with an outgoing 1-arc,
Lt is the set of distinct paths from the Ct in question
to the terminal node and # lt is the number of arcs on
such a path.
Having multiple faults also supports other frame-

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

435

be ordered s.t. the size of the ROBDD is quadratic in
the size of the domain.

20

40
.so
60
70
Numb:rof•y..emvuiable•inlhed.omaoll

Figure 8: The number of nodes in the layers containing
system nodes as a function of the number of system
variables.
works
like
[de Kleer and Williams, 1987]
and
[Williams and Nayak, 1996]. For instance, in cir­
cuit diagnosis [de Kleer and Williams, 1987] uses a
logical model of the system to be diagnosed and
determines the next action based on expected Shan­
non entropy. To calculate the expected Shannon
entropy they require the conditional probability of
a set of failed components (termed a candidate in
[de Kleer and Williams, 1987]) given some observa­
tion. As their framework does not yield an easy way to
obtain this probability they use an approximation. In
our framework the logical circuits can be represented
as ROBDDs which makes the necessary probabilities
easily available.
So far we have not established a practical upper
bound on the size of ROBDDs with m faults, but
all the examples we have worked with until now have
been of a "small" size. Moreover, several heuristic
methods have been devised for finding a good order­
ing of the variables (see e.g. [Malik et al., 1988] and
(Fujita et al., 1988]).
9

CONCLUSION

When modelling the behavior of man-made machinery
using Bayesian networks it frequently happens that a
large part of the model is deterministic. In this pa­
per we have reduced the task of belief updating in
the deterministic part of such models to the task of
calculating the number of configurations satisfying a
Boolean function. In particular, we have exploited
that a Boolean function can be represented by an
ROBDD, and in this particular framework the number
of satisfying configurations can be calculated in time
linear in the size of the ROBDD.
The use of ROBDDs for belief updating was exempli­
fied in the context of troubleshooting, which is partic­
ular well-suited as it was shown that the variables can

The performance of ROBDD propagation was com­
pared with Shafer-Shenoy and Hugin propagation us­
ing randomly generated troubleshooting models. The
results showed a substantial speed-up and it was
argued that the single-fault assumption, underlying
troubleshooting models, can be weakened without sig­
nificantly affecting the performance of the algorithm
in case the number of faults is "small" .



A computational scheme for reasoning about
dynamic systems using (causal) probabilistic
networks is presented. The scheme is based
on the framework of Lauritzen and Spiegel­
halter {1988), and may be viewed as a gen­
eralization of the inference methods of clas­
sical time-series analysis in the sense that
it allows description of non-linear, multi­
variate dynamic systems with complex con­
ditional independence structures. Further ,
the scheme provides a met hod for efficient
backward smoothing and possibilities for effi­
cient, approximate forecasting methods. The
scheme has been implemented on top of the
HUGIN shell.
1

INTRODUCTION

The application of probabilistic graphical models
(belief nets, influence diagrams, etc.) for model­
ing domains with inherent uncertainties has become
widespread. A common trait of the domains, where
such applications turn out most successfully, is their
static nature. That is, each observable quantity is ob­
served once and for all, and confidence in the observa­
tions remaining true is not questioned. However, do­
mains involving repeated observations of a collection
of random quantities arise in many fields of science
(e.g. medical, economic, biological). For such domains
a static model is not very useful: the estimation of
probability distributions of domain variables based on
appropriate prior knowledge and observation of other
domain variables is reliable only for a limited period
of time, and further, upon arrival of new observations,
both these and the old observations must be taken into
account in the reasoning process. Thus, to cope with
such dynamic systems using probabilistic networks we
need to interconnect multiple instances of static net­
works. Obviously, as time evolves, new 'slices' must
be added to the model and old ones cut off. This
introduces the notion dynamic probabilistic networks
(DPNs).

In general, a dynamic model may be defined as a se­
quence of submodels each representing the state of a
dynamic system at a particular point or i nte rval in
time; henceforth, such a time instance will be referred
to as a times/ice. Hence, a DPN consists of a series of,
most often structurally identical, subnetworks inter­
connected by temporal relations. To make es t i mates of
variables of a dynamic system in a way that makes full
use of the information about past observations of t.he
system, requires a compact representation of this in­
formation. The creation of this representation is part
of the process of reducing the dynamic model. This
reduction process includes elimination of parts of the
model representing past time slices, and should have
no effect on future estimates, that is, the information
conveyed by the eliminated part of the model should
be completely represented in the remaining part. The
complementary process of expanding the model must
be carried out whenever new time slices have to be
included in the model.
In classical time-series analysis (see e.g. Box and .Jenk­
ins (1976) or West and Harrison (1989)) the emphasis
is on model assessment, i.e. estimation of model pa­
rameters given a time series of observations of some
stochastic process. The model thereby selected is then
used for making predictions about future behaviour
of the time series. Although the classical time-series
analysis techniques have been quite successful, their
ability to cope with such important issues as complex
independence structures and non-linear relationships
of have appeared to be rather modest. By formu­
lating the analysis in terms of DPNs both of these
limitations vanish. Attempts to integrate methods of
classical time-series analysis with network representa­
tion and inference techniques have been presented hy
Dagum, Galper and Horvitz {1992). This paper, how­
ever, does not address the issue of model assessment,
but merely problems related to making inferences (in­
cluding prediction and backward smoothing, in classi­
cal time-series analysis terms). That is, the dynamic
model is assumed to be given.
Among research activities applying DPNs, as defined
above, are a model for glucose prediction and insulin
dose adjustment by Andreassen, Hovorka, Benn, Ole-

122

Kjcerulff

sen and Carson (1991), an approach to building plan­
ning and control systems by Dean, Basye and Lejter
(1990), a model for making judgements concerning
persistence of propositions by Dean and Kanazawa
(1989), and a model for sensor validation by Nicholson
and Brady (1992). However, none of these activities
have dealt with the issues of reasoning in DPNs.
In Section 2 we briefly review some relevant graph the­
oretic concepts as well as some fundamental charac­
teristics of conventional (static) probabilistic networks
and some of the DPNs introduced. The processes of re­
ducing and expanding DPNs are described in detail in
Section 3 as well as the processes of backward smooth­
ing and forecasting. Section 4 briefly summarizes the
presented scheme and provides a list of some of the yet
unresolved issues.

2

TERMINOLOGY

Commonly used graphtheoretic terms like 'directed
graph', 'undirected graph', 'triangulated graph', 'par­
ent', 'children', 'cliques', 'paths', 'cycles', etc. shall be
used without formal definitions; see e.g. Lauritzen and
Spiegelhalter (1988) for details on relevant the termi­
nology. We shall use the following abbreviations: the
set of parents, children, ancestors, and neighbours of
a vertex a are denoted by, respectively, pa(a), ch(a),
an ( a), and adj(a). In the sequel the symbol® denotes
the binary operator producing the set of all unordered
pairs of distinct elements of its arguments. In the fol­
lowing two paragraphs we review some less common
graphtheoretic notation.
For a directed graph g = (V, E), gm denotes its moral
graph obtained by adding edges between pairs of ver­
tices with common children and dropping the direc­
tions of the edges. A decomposition of an undirected
graph g = (V, E) is a triple (A, B, C) of non-empty
and disjoint subsets of V such that V = AU BU C,
C separates A from B, and C is a complete subset of
V (i.e. each pair of vertices in C are neighbours). A
decomposition (A, B, C) decomposes g into subgraphs
OAuC and 9Buc (i.e. subgraphs induced by AUG and
BUG, respectively). g is decomposable (triangulated)
if and only if (A, B, C) decomposes g and both 9AuC
and fJBuC are decomposable.
When a vertex a E V and the edges incident to o: are
removed from g = (V, E), o: is said to be deleted, but
when adj(a) are made a complete subset by adding
the necessary edges (if any) to the graph before o: and
the edges incident to a are removed, then o: is said
to be eliminated. Note that connectivity of a graph is
invariant under elimination, but not necessarily under
deletion. The set, say T, of edges added by eliminating
all vertices in V in any order is called a triangulation
of g as (V, E U T) is triangulated. The edges of T
are called fill edges or fill-ins. An elimination order is
a bijection # : V ...... {1, ... , lVI}. g# is an ordered
graph. The triangulation T((i#) is the set of edges

produced by eliminating the vertices of g in order #.
An elimination order # is perfect if T(g#) = 0.

A probabilistic network, as used in this paper, is built
on a directed, acyclic graph (DAG) g = (V, E), where
each vertex a E V corresponds to a discrete random
variable X01 with finite state space X01• For A � V,
XA denotes the vector of variables indexed by A. Sim­
ilarly, XA denotes an element of the joint state spA.ce
XA = XaeAXa. Each random variable X01 of a. proba­
bilistic network is described in terms of a conditional
probability distribution p(xa I Xpa(a)) over X01, where
p(x01 I Xpa(a)) reduces to an unconditional distribution
if pa(a) = 0. In (i, the conditioning variables of Xa are
represented by pa(a). The joint probability, p = p11,
over Xv is the product of all conditional and uncon­
ditional probabilities. (i is called the independence
graph of p, since for each non-adjacent pair a, {3 E V,
all /31 r if and only if any path between a: and f3 in
Am contains at least one member of r � V, wher e A
is the subgraph of (i induced by { o:, /3}U an( a)Uan ({J)
(Lauritzen, Dawid, Larsen and Leimer 1990). Let V be
a set of non-empty subsets of V. Then p has potent.ial
representation if

p(x) =

z-1 1/;( :c )

= z-1

IT 1/!A(xA),

AEV

where 1/!A are called potentials and z is called the nor­
malization constant. In particular, the product of all
p(x01 I Xpa(a)), a E V, is a potential representation wit.h
normalization constant 1.
By exploiting the conditional independence relations
represented by g, the joint probability space, Xv, may
be decomposed into a set of subspaces {Xc }cec, where
C is the set of cliques of (V, EU T(O#)) (Spiegelhal­
ter 1986, Lauritzen and Spiegelhalter 1988), such t.ha.t
computation of marginal distributions can be done
in a junction tree T = (C, £) (Jensen 1988, Jensen,
Lauritzen and Olesen 1990) with nodes C and arcs
£ � C 0 C representing clique intersections, where for
each path {C = C1, ... , Ck =D) in T, CnD c CinCi
for all 1 $ i -:/: j $ k. The existence of a potential rep­
resentation is guaranteed in a junction tree, and the
tree is said to be calibrated if f/lc(xcnD) = T/JD (xcnD)
for all xcnD E XcnD and all C, D E C, where
C n D -:/: 0. Two junction trees T1 = (Ct, £1) and
T2 = (C2,£2) with non-empty and complete intersec­
tion S = Ct n C2, where Ct E C1 and C2 E C2, are
said to be jointly calibrated if both T1 and T 2 are
calibrated and 1/!c1(:cs) = 1/!c�(xs) for all xs E Xs.
Calculation of marginal distributions in a junction tree
is done in a two-stage process involving collection and
distribution of marginal potentials between all neigh­
bours in the tree. These two operations performed
in sequence are jointly referred to as propagation (or
fusion and propagation).

A DPN represents a finite (though possi b l y varying)
number, say n, of time slices. Thus, the vertices V
of the graph g = (V, E) of the network consists of
disjoint subsets each representing the random variables

A

X(t) of a particular time slice
appropriately chosen t
V = V(t- n + 1) U

Computational Scheme for Reasoning in Dynamic Probabilistic Networks

t. That is, for some
· ·

·

U

The subset int(t) r; V(t) is called the
is defined as
int(t) ={a E

V(t).

The time slices of a DPN are assumed to be chosen
such that the DPN obeys the Markov property: the
future is conditionally independent of the past given
the present. Formally this may be written as

interface of time

slice t and

V(t) 1.8

E

V(t- 1), {a,j3}

E

Eint(t)}.

The moralized graph of the sample DPN DAG in Fig­
ure 1 appears in Figure 31 where the interfaces are
indicated by filled circles ( note that int(O) = 0).

X(O), . .. ,X(t- 1} ll X(t + 1), ..., X(t + k) I X(t)

for all t > 0 and k
time slice.

>

0. Time slice 0 is called the initial

The set of directed edges

{(a, ,8) I a E V(t- 1), ,8 E V(t)} r; E

is called the temporal edges (or temporal relations )
of time slice t and express conditional independence
assumptions between slices t-1 and t. Thus, temporal
edges are those between vertices of adjacent time slices
( see Figure 1).

1

0

Figure

1:

n-1

� "future" slices

Figure 2: Time slices of a DPN.
of the corresponding graph

V

=

V(t -1r) u . .

and the edges by

·

u

g = (V, E) is given by

V(t + r/J),

E= E(t- 1r) U E*(t- 1r + 1)U ..

where

At any point in time, there is a series P,, ... , PN
of distinct but strongly related models, where each
Pn1 1 ::5 n ::5 N, is specified by the quadruple
(p, (in,t,..(n)1t.p(n)), where t,..(n) < t.p(n) is the old­
est and t.p(n) the newest time slice represented by P,.,
and where gn = (Vn 1 En) is the independence graph of
the probability p. At any time , PN refers to the most
recent model called the current model.

(Vn, En)=

B···�···G
V

Figure 3: Sample initial DPN moral graph.

Sample initial DPN DAG.

"past" slices

,.. � 0, r/J � 0.

·

U

(1)

E*(t + r/J), ( 2)

E(t) r; V(t)® V(t),
E*(t) = E(t)U Eint(t)

,
E int(t) r; V(t- 1)® V(t),

Obviously, the set of temporal edges of time slice
a subset of Eint(t).

n-1

By the series P1, ... 1 PN we understand the following.
For any 1 ::5 n ::5 N the graph 9n of Pn is given by

At time slice t > 0, a DPN represents 7r "past" slices
and rjJ ''future" slices (see Figure 2) . Thus, the vertices
'If

0

( V, E )

(V U in�{t' ) ,
E U Emt(t'))

if n=

N

if n <

N

(:'1)

where t' = t,..(n+1), and VandE are given by (1) and
( 2) , respectively, with t- 1r = t ..(n) < t + ¢ = t.p(n).
Although Pn, n < N, contains variables of Pn+l we
define tr/>(n) = t..(n + 1} -1. Thus t4>(n) represents the
latest time slice about which Pn is guaranteed to be
capable of containing complete information. For any
1 ::5 n < N, t,..(n) and t.p(n) are fixed. Also t.,..(N) =
t.p(N- 1) + 1 is fixed, but t.p(N) is a non-decreasing
number meaning that the expanded model generated
by including new time slices to PN is still referrecl t.o

asPN.

Finally, by

gN

Q CQ, Q )

Vn,
En
gn =
n l
l
composite graph of g,, ... , 9 N.

=

we denote the

3

REASONING IN DPNs

The time slices

t is

{

t:or(N), ... , t.p(N) of the current moclel,

PN, are divided into two groups: the first w slices .con­
stitute a group referred to as the window of time slices

123

124

Kj<l"rulff

(or simply the window ) and the remaining time slices
comprising t.- (N) + w, ..., tq,(N) are referred to as the
forecasting slices; see Figure 4. Similarly, the time
Forecasting

Backward smoolhillg

·-----------------·�h·----------··-··-·-----------------------------------�-···-·--------------------...

l �.(N)+�--·a!
:
s...�::
:3����
J
........
.. .. :: .. .. ..... ..
i
p
:... --------------'!:'--·

w-l

i\i
Window
-----_; �·::: ·.-:: ·:::.-: ..·.-::.-·::.-.-:.-: ·:.-:
:

:

Figure 4: The current model,
of w time slices.

PN,

---...!'!

-·.- -- .. �---- ---- ---- --�

includes a window

slices 0, .. . , tq,(N- 1) are referred to as the backward
smoothing slices. Note that the term forecasting slices
is slightly imprecise since all inference concerning vari­
ables of time slices for which no observations have been
entered, actually are forecasts, even if such time slices
belong to the window. For similar reasons the term
backward smoothing slices is also slightly imprecise.
For the purpose of making inferences, the window is
assumed to consist of a triangulated version of the
composite graph of the time slices involved. Hence a
junction tree is associated with the window such that
inferences in it are carried out as in a conventional
static network. Inferences involving backward smooth­
ing and forecasting are described in Sections 3.4 and

3.5.

The process of moving the window forward involves the
two more or less separate processes of model expansion
and model reduction discussed in detail in Sections 3.2
and 3.3. Since the window is represented by a junction
tree, these processes roughly amount to, respectively,
adding a new subtree to the junction tree and cutting
off a part of the tree.
Model expansion by, say, k new time slices consists
of (a) adding k new slices (conditional representation)
to the current model (i.e. t¢(N) :::;: tq,(N) + k), (b)
moralizing the hybrid composite graph of the trian­
gulated graph of the window and the DAGs of the k
consecutive time slices starting at t.-(N) + w, (c) tri­
angulating that graph and identifying the new clique
set, (d) constructing the new (expanded) junction tree,
and (e) calibrating the new clique potentials with ap­
propriate consideration of the old ones. As discussed
in Section 3.2, the last step is optional. Expanding the
current model by k new time slices causes the width
of the window to b e increased by k, while the number
of forecasting slices remains unchanged.
Model reduction by k time slices involves elim­
ination of all variables pertaining to time slices
t..-(N), ... , t..-(N) + k- 1. Recall that elimination of a
vertex a (variable Xcr) forms a complete subset of the
vertices adj( a: ) unless the y already constitute a com­
plete set. The end-product of the elimination process
is a potential involving the variables int(t.-(N) + k).
This potential, represented in one of the cliques of the
reduced junction tree,
say, represents all informa-

TN

tion about the past necessary for the reduced m o del to
take full account of the knowledge about t he history
of the system. Reducing the current model by k time
slices causes the number of backward smoothing slices
to be increased by k and the width of the wi nd o w t.o he
decreased by k, while the number of forecasting slicr.s
remains unchanged.
Two issues are of major importance here: (a) if back­
ward smoothing is to be performed, the cliques of t.he
triangulated graph resulting from the reduction pro­
cess must be linked together in a new junction t r ee ,
say, such that backward smoothing can be per­
formed by passing messages from 1 N to 1 N _1 via the
potential involving variables int(t.-(N) + k), and (b)
since both the expansion and the reduction process
performs a triangulation (i.e. finds an elimination or­
der) of {basically) the same model, these two processes
should be coordinated such that the same elimination
order is employed.

TN-1

The triangulation carried out as a subtask of the ex­
pansion process is unconstrained in the sense that the
search space of elimination orders consists of all per­
mutations of the set V of vertices of the (expanded)
window, whereas the reduction process may be per­
ceived as a constrained triangulation, where the ver­
tices eliminated define the prefix of orders compris­

ing all vertices in V. Then obviously it might be ad­
vantageous to make a constrained decomposition in
the first place, rendering the reduction process triv­
ial, provided it is carried out in the fundamental way
described above (i.e. assuming the reduction concerns
k lumps of 'PN, where each lump inclu des all vertices
of a particular time slice). This introduces the notion
of a constrained elimination order which is discut=:sed
further in Section 3.1.

3.1

CONSTRAINED ELIMINATION
ORDERS

A constrained elimination order is defined

as

follows.

gN

= Lj�=I gn = (V, E) be a com­
posite graph and let # : V ...... {1, ... , lVI} define nn
elimination order. This order is said to be constrained
if #(a:) < #(/3) for all 1 ::; i < j ::; N, a: E V; and
{3 E Vj. Similarly, T(g�) is said to be a constrained
triangulation of gN .

Definition 1 Let

Constrained elimination orders have a number of i m­
portant properties which shall be used in Sections 3.3
and 3.4.

First, we observe that the order in which the vertices
are eliminated does not affect the com­
plexity of PN. This fact follows from L emma 1 Lhe
proof of which has been made by Rose, Tarjan and
Lueker (1976).

Uo<n<N Vn

Lemma 1 (Rose et al. (1976)) Let g# = (V, E)
be an ordered graph. Then {a:, /3} E E U T(9#) if

A

Computational Scheme for Reasoning in Dynamic Probabilistic Networks

and only if there is a path (o: = 0:1, ... , O:k = {3} such
that #(o:i) < min{ #(o:), #(.B)} for al1 1 < i < k.

implicit as part of the operation of moving the window
k time slices forward.

This property implies that, under constrained elimina­
tion, an optimal elimination order for gN = U�=l fJn
is given by optimal orders for fJn, 1 :::; n :::; N.

A new time slice is added to the current model via con­
ditional probability relations such that the variables
added have parents among the variables of the current
model (relations in the opposite direction are not al­
lowed). The structure of the DAGs of the conditional
models of individual time slices will most often be iden­
tical. Note, however, that we make no structural or
logical restrictions as to the conditional networks and
temporal relations added. Thus, if an initial assump­
tion implying identical time slice models turn ou I. t.o be
inadequate or erroneous, the presented scheme poses
no obstacles to changing such assumptions.

Let 'P1 , . ., 'PN be a series of conditional
models with composite moral graph ((}N)m, and let
Pi, .. ., 'Piv be the corresponding constrainedly decom­
posable models with composite Jraph ((}*)N. Then for
any 1 :::; t :::; t4>(N), int(t) in ((} )m is a complete sep­
arator of (fJ*)N.
Lemma 2

.

Proof: From the definition of int(t) it follows that
int(t) is a separator of ((}N)m. Since 'Pi, ..., Piv are
constrainedly decomposable it follows from Lemma 1
that for all paths {o: = o:1, .. , o:;; = {3}, where o: E
V(t- 1) and {3 E V(t)\int(t), {o:1, ... , a;;}nint(t) # 0.
That is , int(t) is also a separator of((}*)N. Also due
to the constrained elimination order it follows from
Lemma 1 that int(t) induces a complete subgraph of
D
((}*')N.
.

Thus under constrained elimination the interface of
time slice t, 1 :::; t :::; t4>(N), is identical in the moral
and the corresponding decomposable graphs. This re­
sult is used in the following.
3 Let 'P1, ... , 'P be a series of constrainedly
N
decomposable models with composite graph

Lemma

gN

N

=

U fJn

=

In order to produce a junction tree for the expanded
window we perform the operations of moralization and
triangulation. The moralization step involves moral­
ization of the hybrid composite graph (of the triangu­
lated graph of the window and the DAGs of the k new
time slices) and implies that the conditional probabil­
ities of the k new time slices of the window are con­
ceived as potentials. These potentials are in turn at­
tached to appropriate cliques of the triangulated graph
resulting by employing the constrained triangulation
scheme to the moralized graph. A sample model ex­
pansion is shown in F igure 5, where the dashed lines
are the edges added by moralization. In this example,
the window is assumed to consist of a single time slice
(the initial one).

(V,E).

n=l

Then gN is constrainedly triangulated.
Proof: From Lemma 2 we have that for any 1 :::;
t :::; t4>(N), int(t) is a complete separator of gN and
hence (A, B, int(t)) is a decomposition of gN, where
A = V(l)U···UV(t -1) and B = V\(AUint(t)).
The graphs 9fuint(t) and 9f:uint(t) have complete sep­
arators int(l), ... , int(t) and int(t), ... , int(t4>(N)), re­
spectively. Continuing this argument we end up with
subgraphs (}1, .., 9N all of which are constrainedly
D
triangulated, and the result follows.
.

This shows that backward smoothing, at least in prin­
ciple, can be accomplished by constructing a junction
tree for gN and performing propagation in that tree.
However, a less space consuming technique exists as
described in Section 3.4.
3.2

MODEL EXPANSION

The operation of expanding the current model by, say,
k new time slices t4>(N) + 1, .. , t4>(N) + k is carried
out for the purpose of including k new time slices (not
necessarily tq,(N) + 1, ..., t.p(N) + k) into the window.
The wish to expand the window may be explicit or
.

·
� �-·---··"'·--- ·· .........................
·

rune

sliceO

Time slice 1

Figure 5: Sample model expansion.
Obviously, in finding an optimal elimination order, we
have to take into consideration the topology of the
graph as it appears after addition of the next time
slice. Since we want the model complexity in terms of
the state space size to be as low as possible to minimize
the complexity of inference, and since the state space
size varies heavily over the range of elimination orders,
a careful analysis must be conducted to establish an
appropriate order. To find an optimum elimination
order for an arbitrary graph is, however, an NP-hard
problem as proved by Wen (1990). Yet, in practice

125

126

Kj<erulff
it turns out that near optimum triangulations may be
found using simple heuristic ordering strategies (Rose
1973, Kjrerulff 1992). In Figure 5 the applied elimina­
tion order is b, e, /, c, g, d, a, h. (The original directed
and moral graphs are shown in Figures 1 and 3.)
Having found the cliques of the new expanded graph
on the basis of an appropriate elimination order, the
next step concerns construction of a junction tree for
those cliques. As much as possible of the junction
tree, l' = (C,£), in existence prior to the expansion
should be reused in order to minimize the amount of
work required to construct the expanded junction tree
l'' = (C',£'). Note that as a direct consequence of
the constrained decomposition scheme there is for each
'old' clique C E C a 'new' clique C' E C' such that
C � C'. For some cliques the containment might be
strict. The creation of l'' can be described as follows.
1. Identify the set C' of cliques of 1'.
2. Construct a 'skeleton' of l'':
(a) Create clique objects for all members of C'\C
and clique intersection objects for all mem­
bers of£'\ C.
(b) Initiate the potential tables of these new
clique and clique intersection objects to unity.
(The potentia.! tables of the cliques in C n C'
and of the clique intersections in en£' remain
unchanged.)
3. For each C E C\ C' and each E E£\£' (i.e. 'old'
cliques rendered redundant and their associated
intersections) attach (by multiplication) the asso­
ciated potential tables to the tables of appropriate
clique and clique intersection objects.
4. Attach the conditional probability tables of the
variables of the new time slices to appropriate new
cliques.
(The term 'appropriate' in points 3 and 4 refers to the
index set of the table to be attached being a subset of
the clique or clique intersection upon which it is at­
tached.) The expanded junction tree 1' has now been
created. That is, a potential representation for the
joint probability distribution for the expanded win­
dow has been established. In Figure 6 the cliques and
clique intersections remaining unchanged are shown in
bold and the attachment of potential tables of redun­
dant 'old' cliques and clique intersections are indicated
by dashed arrows. Note that the cliques has been
numbered according to the order of creation using the
above elimination order and that clique 5 in part a is
a proper subset of clique 5 in part b.

Now, if we have an immediate interest in the marginal
distributions of variables (or sets of variables) in the k
new time slices of the window, a propagation can be
performed; otherwise we might postpone the propaga­
tion step until e.g. new observations has been recorded.
If l' was calibrated immediately before the model ex­
pansion was executed, we only need to perform prop-

a

b

Figure 6: Sample junction tree expansion
agation
cliques.
3.3

m

the subtree induced by the set of

new

MODEL REDUCTION

Due to the constrained decomposition scheme em­
ployed by the model expansion process, model reduc­
tion becomes a relatively easy task as previously dis­
cussed. In developing a model reduction scheme it is
important to recognize the requirements for convenient
backward smoothing beyond time slice tr(N}. Below
we develop a reduction scheme which meets such re­
quirements and which is based on the results of the
following theorem.
Theorem 1 Let 'P1, ... , 'PN be a series of con­
strainedly decomposable models, where each 'P;, l �
i � N, is calibrated. Assume 'Pn-1 and Pn are jointly
uncalibrated for some 1 < n $ N. Complete informa­
tion required to calibrate 'Pn-I to 'Pn or vice versa is
represented by the marginal-rf;int(t .(n))• where there is
a. clique
of9n-1 and a clique c2 of9n such that
int(t .. (n)) c Ct and int(tr(n)) � C2.

cl

•

Proof: From Lemma 2 we have that int(tr(n)) is a
complete separator of Yn-1 U Yn and hence tPint(t.(n))
contains complete mutual information between 'Pn-1
and 'Pn. From the definition of Yi, 1 � i < N, (cf.
(3)) we have that int(tr(n)) C V (t cl> (n - 1)), and since
for each pair {a,P}, where a E V(tcl>(n- 1)) and
P E int(tr(n)), #(a) < #(P), int(tr(n)) induces a
complete subgraph of Yn-l· Hence there is a clique C1
of Yn-1 such that int(t .. (n)) C Ct. Since int(t .. (n.)) is
complete in Yn-1 it follows immediately that it is also
complete in 9n and hence there is a clique C2 in 9n
0
such that int(t ... (n)) � c2.

So far we have not been concerned with the pro­
cess of creating new models to be added to a series
P1. . . . , 'PN. However, the reduction process partition
'PN into two models, one representing the time slices
eliminated and the other the remaining time slices of
'PN (subsequently defining the new current model).
That is, whenever 'PN is subjected to reduction, the
number, N, of models is increased by one. Thus, r.on­
forming to (3), we define the reduction of 'PN by the

A

Computational Scheme for Reasoning in Dynamic Probabilistic Networks

k oldest time slices by sequentially executing the fol­
lowing steps.

1.

Let 1'' = (g' = (V', E'), to = t1r(N), tk
k), where 0 � k < t�(N)- t1r(N) and

V'

=

V(to) U

·

·

·

U

=

t,.(N) +

V(tk) u int(tk + 1),

E(to) U E*(t1) U U E*(tk)·
2. Let N := N + 1.
3. Let 'PN = (g N = (VN,EN),t'lr(N) = tk +
1, t�(N) = t¢(N- 1)), where
E'

=

VN

· ·

=

·

VN-I \ (V' \ int(t1r(N))),
EN= EN-I\ E'.

4. Let

'PN-l

=

1''.

In terms of operations on the junction tree of 'PN (ac­
tually the junction tree of the window) an equivalent
description of the reduction process may be formulated
as follows, where t = t1r(N) + k + 1 is the oldest time
slice of the window when the reduction has been com­
pleted.

1.

2.

3.

Prior to the reduction, let
tree for 'PN.
Let C'

=

{C

E

i = (C, £) be a junction

C I C n U������� V(i) ::f 0}

be the

cliques containing variables to be eliminated, and
C" = C \ C' the remaining cliques.

Let i' = iC' an d i" = ien be the junction trees
induced by C' and C", respectively (see Figure 7).

4. Let B = {C

E

C" I adj(C ) n C' ::f 0 in i}.

E B such that int(t) <;;;; C then add
int(t) to C" and let adj(int(t)) = B; otherwise add
B\ {C} to adj(C) .

5. If there is no C

6. Let

N

:=

N + 1.

First assume that the condition of the 'if' part of
Step 5 holds. Since the constrained decomposition
forces int(t) to induce a complete subgraph of g N and
since there is no clique in C" containing int(t), then
int( t) itself must be a clique of g N. The subset B <;;;; C",
where for each B E B there is a non-empty intersec­
tion between the adjacency set of B and C' in T, is
then made the adjacency set of int(t). Since the path
in 1 between any pair of elements of B includes ele­
ments of C' (i.e. C' separates the elements of B from
one another), this does not violate the tree structure
ofi•. Neither does it violate the property ofi• being
a junction tree, as the intersection of any pair (C', C'")
of cliques, where C' E C' and C" E C", is a subset of
int (t ) .

Next, assume the condition to fail (i.e. there is a clique
E C" such that int(t) <;;;C
; ) in which case B\ {C} is
made a subset of the adjacency set of C in 1". With
arguments similar to those above it is readily reali:r.ed
that the property of T* being a junction tree is not
violated.

C

3.4

BACKWARD SMOOTHING

Clearly, the arrival of external evidence (observations)
affects not only the estimates of (unobserved) variables
of the relevant time slice(s), but may also have sig­
nificant effect on estimates of variables of other time
slices. The process of re-estimating variables of past
slices in light of new evidence (retrospective assess­
ment) is often referred to as backward smoothing. If
the variables for which re-estimated probability distri­
butions are required, are all included in the current
model, 'PN, backward smoothing is an implicit part of
propagation in the window of time slices. However, if
we want to backward smooth from 'PN to 'PN-1 special
actions should be taken. Specifically, complete infor­
mation about observations pertaining to the window
should be transferred from 'PN to 'PN-l·
Given the model reduction strategy described in Sec­
tion 3.3 the process of propagating complete relevant
information backward from 'Pn to 'Pn-1 or forward
from 'Pn-1 to Pn becomes very simple. Consider t,he
example where 1'1,
, 'PN are calibrated, but jointly
uncalibrated. Let the inconsistency be caused by a
series E2,
, EN of sets of external evidence, such
that 'Pn, 1 � n � N, is uninformed of En+l• . . . , EN.
Now, 'Pn may become informed of En+l•···,EN by
the following calibration process (see also Figure 8).
For convenience we first define the concept of an inl.er­
fa.ce clique as follows.
•

T

.

.

• • •

Figure 7: Partitioning i into

i'

and 1".

After the execution of Steps 1-6, 'PN-1 is given by i'
and 'PN by 1• which is the result of modifying i" as
described in Step 5 above. It is easily verified that
i * is a junction tree for g N of Step 3 of the four-step
description of the reduction process.

Definition 2 Let 1'1, .. . , PN be a series of con­
strainedly decomposable models. Then for any 1 �
n � N let I;; denote the set of cliques of9n such that
for any IC;; E I;;, int(t1r(n)) <;;;; IC;;. Similarly, for
any 1 � n < N let I;t denote the set of cliques of 9n
such tha.t for any IC;t E I;t, int(t1r(n + 1)) C IC't.
IC;; a.nd 1c: are called interface cliques of'Pn.

127

128

Kjcerulff

1. Initially let i = N. Then repeat steps 2 and 3
sequentially while i > n.
2. Let ICi- E Ii-, /Ct_1 E It_1, and I= int(t,..(i)).
=

tP1c+

•-�

L:1c·v
?/!Ic-:'
'

L:w;_. \I t/JJet_,

where superscript "*" denotes the updated poten­
tial.
3. Calibrate Pi-t by propagation and decrement i
by one.

cases even larger than those required by exact meth­
ods, but of course with much less space requirements
since the sampling is performed in the DAG struc­
ture involving relatively low-dimensional probability
tables. Another important feature of sampling meth­
ods is that the time complexity grows only linearly in
the dimensionality of the tables involved, whereas it
grows exponentially for exact methods.
Another method that might be fruitful is based on the
fact that (a subset of) the conditional probabilities of a
probabilistic model quite often exhibits linearity in the
sense that they are (approximately) linear functions in
the variables upon which they are given. That is,

�---=-···--=--�
Figure 8: Backward smoothing from PN to Pn.
3.5

FORECASTING

In time-series analysis applications there is typically a
desire to make optimal forecasts of the random pro­
cess considered. Within the computational framework
presented above, forecasts which do not exceed the
extent of the window are an implicit part of propaga­
tion in the junction tree of the window; otherwise it
may be performed by expanding the window by the
required number time slices. If forecasts are wanted
for a large number of time slices ahead of the window,
the complexity of the resulting decomposable model
might, however, easily exceed the capacity of the avail­
able computing resources. Such cases may be solved
in a number of ways.
One is to move the window the required number of
steps, where propagation is performed in each step,
and subsequently, moving it back again. This might,
however, be a very time consuming operation, and fur­
thermore, a lot of unnecessary calculations will quite
often be carried out as we typically only want the fore­
casts for a limited number of variables. Therefore,
there is a demand for alternative forecasting methods
which either avoids the junction tree approach and/or
exploits the fact that forecasts are only required for a
limited number of variables.
Concerning non-junction-tree methods (i.e. no trian­
gulation), various Monte-Carlo sampling schemes may
be useful. A common trait of these schemes is the fact
that the variance of the resulting distributions can be
made arbitrarily small. In fact, some of the most fruit­
ful approaches to variance reduction is Monte-Carlo
sampling (Ripley 1987). Note, however, that a reduc­
tion of the standard error of an estimator by a factor
of k requires an increase in the sampling size, n, by
around a factor of k2 due to the ubiquitous 1j.,fii. law
of statistical variation. Thus, to get forecasts within a
small distance from the 'exact' values, we should ex­
pect the computing time to be relatively large; in some

p(xa) �

L

Xpa(<>)

p(xOI I Xpa(a))

rr

PEpa(a)

p(xp).

The method is then simply given by calculating n.ll
such approximate marginal probability distributions
in an appropriate order (i.e. the distributions of all
parents of a variable should be calculated before the
distribution of the variable itself). Given that the di­
vergence between such approximate distributions and
the 'exact' ones are below an acceptable upper bound
for the variables of interest, this is a very fast fore­
casting method. The interesting point concerning the
exactness of the method is that an upper bound on
the error can be computed in advance by application
of theorems of linear algebra.
4

SUMMARY

We have presented a computational scheme for rea­
soning in dynamic probabilistic networks featuring de­
scription of non-linear, multivariate dynamic systems
with complex conditional independence structures and
providing a mechanism for efficient backward smooth­
ing. As opposed to a static network representing a
finite and fixed number of time slices (i.e. capable of
reasoning only about a finite series of observations of a
dynamic system) the proposed scheme can handle infi­
nite series of observations. Further, in applying static
networks representing a fixed number of time slices as
models of dynamic systems, there is typically a desire
to include as many time slices as possible in the model.
Thus, inference easily becomes time consuming and in­
flexible (i.e. propagation involves all time slices in the
model even if updated distributions are wanted only
for a limited number of time slices). The proposed
scheme, on the other hand, provides a high degree of
flexibility in the reasoning process, since the widt.h of
the window of time slices can be changed dynamically
as well as the number of 'backward smoothing slices'
and the number of 'forecasting slices'. In addition, the
scheme provides selective inference in the sense t.hat
inference can be performed in (i) the window, as (ii)
backward smoothing, or as (iii) forecasting.
Since the presented model reduction scheme supports a
convenient and efficient backward smoothing method

A

Computational Scheme for Reasoning in Dynamic Probabilistic Networks

it also supports inclusion and modification of obser­
vations pertaining to time slices 'to the left of ' the
window. Delayed observations is a quite typical phe­
nomenon; for example, in a medical setting delays may
be caused by processing time in a laboratory (e.g. anal­
ysis of a blood sample).

Dean, T. and Kanazawa, K. ( 1 989). A model for rea­
soning about persistence and causation, Compu­

Although we have presented a scheme for reasoning in
dynamic networks, a range of issues still remain to be
dealt with. A couple of the most important issues are
the following.

Workshop on Innovative Approaches to Planning,
Scheduling, and Control, pp. 271-276.

Only preliminary studies has been carried out to inves­
tigate the applicabilities the various forecasting meth­
ods discussed in Section 3.5. Especially, a scheme for
establishing an upper bound on the forecast error by
applying the linear approximation algorithm is desir­
able. But also a study of the applicability of various
Monte-Carlo sampling schemes should be conducted.
Since many applications feature a large number of tem­
poral relations, the state space sizes of the interface
cliques of the time slices of the window and of the
'backward smoothing slices' may become unmanage­
ably large. In such cases there will be a need for
approximations. One obvious way of approximating
the inference is to exclude some of the edges required
between members of the interface set of a time slice.
An extreme approach could be assumption of indepen­
dence between all parents of interface variables (i.e. no
fill edges at all added between interface vertices). To
that end, studies on the upper bounds of the resulting
error and its attenuation as time evolves, should be
conducted.
An implementation of the computational scheme pre­
sented in this paper has been built on top of the
H U GIN shell.

Acknowledgements
I wish to thank Steffen L. Lauritzen for his valuable
comments on an earlier draft of this paper and other
members of the ODIN group at Aalborg University for
stimulating discussions.




To investigate the robustness of the output
probabilities of a Bayesian network, a sensi­
tivity analysis can be performed. A one-way
sensitivity analysis establishes, for each of the
probability parameters of a network, a func­
tion expressing a posterior marginal proba­
bility of interest in terms of the parameter.
Current methods for computing the coeffi­
cients in such a function rely on a large num­
ber of network evaluations. In this paper, we
present a method that requires just a single
outward propagation in a junction tree for es­
tablishing the coefficients in the functions for
all possible parameters; in addition, an in­
ward propagation is required for processing
evidence. Conversely, the method requires
a single outward propagation for computing
the coefficients in the functions expressing all
possible posterior marginals in terms of a sin­
gle parameter. We extend these results to an
n-way sensitivity analysis in which sets of pa­
rameters are studied.

1

INTRODUC TION

The robustness of the output probabilities of a
Bayesian network can be investigated by performing
a sensitivity analysis of the network. For mathemat­
ical models in general, sensitivity analysis serves to
identify the effects of the inaccuracies in a model's pa­
rameters on its output (Morgan & Henrion 1990). For
a Bayesian network, more specifically, performing a
sensitivity analysis yields insight in the relation be­
tween the probability parameters of the network and
its posterior marginals. The simplest type of sensi­
tivity analysis is a one-way analysis in which a single
parameter is studied; a more general n-way analysis
serves to investigate the joint effects of inaccuracies in

a set of parameters.
In the brute-force approach to performing a one-way
sensitivity analysis of a Bayesian network, each proba­
bility parameter of the network is varied systematically
and the effect on the output probabilities of the net­
work is investigated. Performing a sensitivity analysis
in this way requires thousands of network evaluations,
or (full) propagations, and is, therefore, much too time
consuming to be of any practical use.
Laskey (1995) has been the first to address the compu­
tational complexity of sensitivity analysis of Bayesian
networks. She has introduced a method for computing
the partial derivative of a posterior marginal proba­
bility with respect to a parameter under study. Her
method thus yields a first-order approximation of the
effect of varying a single probability parameter on a
posterior marginal. Compared to the brute-force ap­
proach, her method requires considerably less compu­
tational effort. The method, however, provides insight
only in the effect of small variations of parameters;
when larger variations are considered, the quality of
the approximation may rapidly break down.
The relation between a posterior marginal probability
of interest and a parameter under study can be ex­
pressed through a simple mathematical function. The
function expressing the posterior marginal is a quo­
tient of two linear functions in the parameter, as has
been shown by Castillo et al. (1996). Building upon
this property, it suffices to establish the coefficients
in this function to determine the effect of parameter
variation. Castillo et al. (1997) and Coupe & van der
Gaag (1998) have designed methods to this end. These
methods require a single network evaluation for each
coefficient to be established. Although these methods
currently are the most efficient available, they rely on
a large number of network evaluations and, as a con­
sequence, are infeasible for large realistic networks.
In this paper, we present a new method for sensitivity
analysis of Bayesian networks. Our method, like the

318

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

two methods mentioned above, exploits the property
that a posterior marginal probability relates by a sim­
ple mathematical function to a parameter under study.
It requires just a single outward propagation in a junc­
tion tree, however, to compute the coefficients in the
functions for all possible parameters; in addition, it re­
quires an inward propagation for processing evidence.
Conversely, the method requires a single outward prop­
agation for establishing the coefficients in the functions
expressing all possible posterior marginals in terms of
a single parameter. Our method can be readily ex­
tended to an n-way sensitivity analysis in which sets
of parameters are varied.
In addition to a sensitivity analysis, an uncertainty
analysis can be performed for investigating the robust­
ness of the output probabilities of a Bayesian network.
In an uncertainty analysis, all parameters are varied si­
multaneously through sampling; it therefore provides
little insight into the effects of variation of specific pa­
rameters. Experiments with uncertainty analysis have
led to the suggestion that Bayesian networks are in­
sensitive to inaccuracies in their parameters (Pradhan
et al. 1996, Henrion et al. 1996). In these experiments,
however, a measure of model robustness was obtained
by assuming a lognormal distribution for each parame­
ter and averaging over the probability of the true diag­
nosis for various diagnostic situations in a medical ap­
plication. Rather than in the average of the probabili­
ties of the true diagnosis, however, it is in the variation
of these probabilities that inaccuracies in parameters
are reflected. From these experimental results, there­
fore, no decisive conclusions can be drawn as to the
sensitivity of Bayesian networks. In fact, Coupe et al.
(1999) have reported high sensitivities in an emprical
study in the medical domain, involving real patient
data. We feel that these and emerging similar expe­
riences warrant further investigation into sensitivity
analysis of Bayesian networks.

2

THE BASIC P ROPERT Y

Sensitivity analysis of a Bayesian network basically
amounts to establishing, for each of the network's pa­
rameters, a function expressing an output probabil­
ity in terms of the parameter under study. For out­
put probabilities, we shall consider posterior marginal
probabilities of the form y = p(a I e), where a is a
value of a variable A and e denotes the evidence avail­
able. Each of the network's parameters is of the form
x = p(bi l1r), where b; is a value of a variable B and
1r is an arbitrary combination of values of the set of
parents II= pa(B) of B. We will write p(ale)(x) to
denote the function expressing the posterior marginal
p(a I e) in terms of the parameter x.
In the sequel, we will assume that in a sensitivity anal­
ysis, upon varying a parameter x p(b; l1r), each of
the other probabilitiesp(bj l1r) is co-varied accordingly,
by scaling by the ratio between the probability masses
left. More formally, let the variable B have for its do­
main dom(B)
{b1,...,bm}, m 2: 1. Note that the
parameters p(bj l1r), j =f. i , are functions of x. We now
assume for these functions that
=

=

if j= i
otherwise,
with p(b; l1r)

<

(1)

1.

With the assumption of co-variation as outlined above,
the function y(x) yielded by a sensitivity analysis is a
quotient of two linear functions in x. The following
theorem reviews this important property; the associ­
ated proof provides the basis for the algorithms pre­
sented in Sections 4 and 5.
Theorem 1 Let p be the probability function defined
by a Bayesian network over a set of variables V. Let
y = p(a I e) and x p(b; l1r) be as indicated above.
Then,
=

The paper is organised as follows. Section 2 reviews
the important basic property that a posterior marginal
probability can be expressed as a quotient of two linear
functions in a parameter under study. In Section 3, we
briefly describe currently available methods for sensi­
tivity analysis that build upon this property. In Sec­
tion 4, we present our method for computing the co­
efficients in the functions for all possible parameters,
using just one propagation in a junction tree. In Sec­
tion 5, we describe a similar method for computing
the coefficients in the sensitivity functions relating all
possible posterior marginals to a single probability pa­
rameter. These methods are generalised to an n-way
sensitivity analysis in Section 6. The paper ends with
some concluding remarks in Section 7.

y=
where a, (3,

"(,

p(a,e)(x)
p(e)(x)

ax+ f3
"(X+ 8'

(2)

and 8 are constants with respect to x.

Proof: The joint probability p(a,e) can be expressed

in terms of x as

p(a,e}(x)

�

(�:{V)) (x),

where Lv:a,...,dp(V) denotes summation over the variables V\ {A, . . . , D} with A, ... , DE V fixed at values
a, ... , d, respectively.
The sum Lv:a,ep(V) in the above equation can be
split into n+ 1 separate sums, such that the first sum

319

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

includes only terms with the value b1 for B and the
state 1r for II, the second sum includes only terms with
the value b2 for B and II in state 1r, and so on, and
the last sum includes the remaining terms. So,

(1990), as described by Castillo et a!. (1997). After
having identified the set of relevant parameters, the
sensitivity analysis can be restricted to this set.
Building upon the set of n relevant parameters,
x1, . . . , Xn, the algorithm of Castillo et a!. (1997) iden­
tifies sets of monomials for which the coefficients will
be zero in the linear function p(a, e)(x1, . . . , Xn ) · For
the resulting m monomials, the algorithm constructs
a system of m independent equations of the form
i
y
p(a, e)(xi, . . . , x�), where, for each j, xj denote
arbitrary values for parameter Xj. The corresponding
values yi, i
1, ... , m, are obtained through m net­
work evaluations. The coefficients in the function are
now determined by solving the set of equations thus
obtained. Coupe & van der Gaag independently de­
scribed a similar method, also based on the idea of
solving a system of independent equations. They fur­
ther argue that in a one-way sensitivity analysis three
network evaluations suffice per relevant parameter.
=

=

p( )

" p(V).
" l:v:a,e,b;,11' V +
L
L
1-p(b·l7r)
'
V:a,e,n'f11'
j#i
For the probability p(e) we derive a similar expression
by summing, in the above derivation, over all values of
the variable A instead of keeping it fixed at a. From
the resulting expressions p(a, e)(x) and p(e)(x), it is
readily seen that the output y p(a I e) can be written
as a quotient of two functions that are linear in x. D
=

From Theorem 1 we have that the function that ex­
presses a posterior marginal probability y in terms
of a single parameter x is characterised by at most
three coefficients. The theorem is easily extended to
n parameters. The function then includes the prod­
ucts of all possible combinations of parameters, termed
monomials, in both its numerator and its denomina­
tor. The numerator as well as the denominator are
characterised by 2n coefficients, many of which may
be zero.
3

CURRENT METHODS

The most efficient methods for sensitivity analysis of
Bayesian networks currently available exploit the basic
property reviewed in the previous section. We briefly
review these methods.
Not all parameters in a Bayesian network can influ­
ence a posterior marginal probability of interest. The
subset of parameters (possibly) influencing the poste­
rior marginal is dependent upon the evidence e. The
set of relevant parameters is easily identified using a
variation of the algorithm described by Geiger et a!.

The methods reviewed above have a computational
complexity that is considerably less than the brute­
force approach of systematic variation of parameters.
However, the methods can still be quite time consum­
ing: for a network of realistic size, it can easily require
several hundreds of network evaluations to perform a
one-way sensitivity analysis. An more general n-way
sensitivity analysis to study the joint effect of simulta­
neous variation of n parameters can in fact be so time
consuming that it is infeasible in practice.
4

ANALYSIS OF ONE OUTPUT
WRT. ALL PARAMETERS

The new methods for sensitivity analysis presented in
this paper have been tailored to Bayesian networks in
their junction-tree representation. The methods basi­
cally perform a single or a few outward propagations
in a junction tree and, as a result, are much less time
consuming than the methods reviewed in the previous
section.
In this section, we present our method for computing
the coefficients in the sensitivity functions expressing
a posterior marginal of interest y p(a I e) in terms of
all possible probability parameters x. Recall that these
functions are of the form presented in Theorem 1. Our
method now builds on the idea that, in a junction tree,
the expressions for p(a, e) and p(e) in terms of x can be
derived from the potential of a clique containing both
the variable and the parents to which the parameter x
pertains. The following theorem details the coefficients
to be computed.
=

Theorem 2 Let p be the probability function defined
by a Bayesian network and let T be a junction tree

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

320

for the network. Let y = p(aie) and x = p(b; l1r) be
as before. Suppose that, in T, an inward propagation
has been performed towards a clique containing the
variable of interest A; suppose that subsequently an
outward propagation from this clique has been per­
formed with the value a for A. Now, let K be a
clique in T containing both the variable B and its par­
ents II= pa(B); let ¢x = p(K,a,e) be the potential
of clique K after the abovementioned propagations.
Then, p(a,e)(x)= ax+ (3 with

a=

LK:b; ,,. ¢x ""'LK:b; ,,. ¢x
p(b;l1r) - �1-p(b;l7r)'

(3)

J -r- 1

(3 = L

#i

LK:b; ,,. ¢x
+ L ¢x
.
1-p(b; 17r) K:ll-=f-11"

(4)

Proof: The property follows directly from the proof
of Theorem 1 by observing that p(a,e)= 2:: K ¢ K. D

Building upon similar observations, we have the fol­
lowing corollary.
Corollary 1 Letp be the probability function defined
by a Bayesian netwerk and let T be a junction tree for
the network. Let x = p(b;l1r) and K be as before.
Suppose that the evidence e has been processed in T
by an inward and subsequent outward propagation.
Let ¢'K = p(K,e) be the potential of clique K after
the propagation. Then, p(e)(x)= "(X+ 6 with

(5)

6

=

'K
""' 'K.
""'LK:b;,,. ¢
¢
+
�
� 1- p(b1·i7r)
j-=f-i
K:0-=/-1r

4. Compute the coefficients a and (3, using the equa­
tions (3) and (6) from Theorem 2, for all relevant
parameters, locally per clique.
We would like to note that our method requires just
one inward and two outward propagations to estab­
lish all sensitivity functions for a specific posterior
marginal, whereas the methods reviewed in the pre­
vious section require three inward and outward prop­
agations per parameter.
The method described above outlines the basic idea.
The method, however, may be easier to implement in
the alternative form based upon Theorems 3 and 4.
Theorem 3 Let the junction tree T be as before.
Also, let y = p(aie) and x = p(b;l1r) be as before
and let K be a clique in T including both B and
II = pa(B). Now, let x1 be the initially specified
value for x and let x2 denote an arbitrary other value
for x. Suppose that, in T, an inward propagation has
been performed towards a clique containing the vari­
able of interest A; suppose that subsequently an out­
ward propagation has been performed with the value a
for A. Now, let ¢x =p(K,a,e) be the resulting clique
potential for clique K. Let

y1 =p(a,e)(x1)= L ¢x,
p'(Bi1r)
y2=p(a,e)(x2)= ""'
¢K
'
�
p(B i
7r)

1. Enter the evidence e into the junction tree and
perform an inward and an outward propagation
using an arbitrary root clique.
2. Compute the coefficients 'Y and 6, using the equa­
tions (5) and (6) from Corollary 1, for all relevant
parameters, locally per clique.
3. Perform an outward propagation from a clique
containing the variable of interest A, with the ad­
ditional evidence A= a.

(8)

where p(B l1r) and p'(B l1r) denote parameter vectors
with x = x1 and x = x2, respectively. Then, y =
ax+ (3 with

(6)

Theorem 2 and Corollary 1 provide the basis for our
method for computing the coefficients in the func­
tions expressing the posterior marginal of interest
y =p(a I e) in terms of all possible parameters x. The
method is composed of the following steps:

(7)

K

(9)
Since both variable B and its parents are in­
cluded in clique K, we can obtain from the parameter
vector

Proof:

p(Bi1r)

= (q1(x1 ), ... , Qi- (x1 ), x1, Qi+l (x1 ), ...,Qn(x1))
1

the parameter vector
p'(Bi1r)

(q� (x2),... 'q�- (x2),x2,q�+l (x2),...,q�(x2))
1

where q and q' are parameters co-varying according
to equation (1), by multiplication of the potential ¢x
by p'(Bl1r)jp(Bl1r). From Theorem 1, we have that
y= ax+ (3. The expressions for a and (3 now follow
D
from simple mathematical manipulation.

321

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Let the junction tree T be as before.
Also, let y = p(afe) and x = p(bi f1r) be as before.
Suppose that, in T, an inward propagation has been
performed towards a clique including the variable of
interest A. Then, p(e)(x)= "(X + 8 with

9. Compute the coefficients
(10) from Theorem 4.

Theorem 4

"(= eta + a�a and 8 =f3a + f3�a1

(10)

where aa,f3a and a�a,f3�a are as in equation (9), ob­
tained from outward propagations with the evidence
A = a and A -::/:- a, respectively.
Proof:
We begin by observing that p(e)(x) =
p(a,e)(x) + p(•a,e)(x). By entering the evidence
A = a in a clique H containing the variable A
and propagating outwards, we obtain the potential
p(K, a, e) for clique K. From this potential,
¢K
p(a, e)
Y1.
LK cPK is readily computed, as de­
scribed in equations (7) from Theorem 3. Similarly,
by entering the evidence that A does not have the
value a (that is, by multiplying the clique potential
for H with a vector over dom(A), in which the en­
try corresponding to state a is zero and all other en­
tries equal 1) and propagating outwards, we obtain
the potential ¢�
p(K, •a, e). From this poten­
tial, Y�a = p(•a,e) = LK ¢� is readily computed.
Using equation (8) from Theorem 3, we get y� and
Y�a· Now, using equation (9), we find eta, a�a, f3a,
and f3�a· Inserting these coefficients into the expres­
sion p(e)(x)= p(a, e)(x) + p(•a, e)(x) yields the result
D
stated in the theorem.
=

=

=

=

Our alternative method, building upon Theorems 3
and 4, is composed of the following steps:
1. Enter the evidence e into the junction tree and
perform an inward propagation towards a clique
H containing the variable of interest A.
2. Perform an outward propagation from H with the
additional evidence A a.
=

3. Compute y;,_ and y�, using the equations (7)
and (8) from Theorem 3.
4. Compute the coefficients a = aa and f3 = f3a,
using (9), for all relevant parameters, locally per
clique.
5. Retract the evidence A
the evidence e.

=

a without retracting

6. Perform an outward propagation from H with the
additional evidence A -::/:- a.
7. Compute Y�a and Y�a' using the equations (7)
and (8) from Theorem 3.

and 8, using equation

To allow for retracting the evidence A = a in Step 5 of
our method without retracting e, fast-retraction prop­
agation (Cowell & Dawid 1992) is used in Step 2.
Comparing the computational costs of the two al­
ternative methods, we note that they both require
one inward and two outward propagations. Consider­
ing the first method, we observe that Steps 2 and 4
are equally costly. The computation of the coeffi­
cients a and 'Y costs 2 ·Jdom(K)J/m operations, where
m = fdom(pa(B))J; the computation of j3 and 8 costs
2fdom(K) I arithmetic operations. Thus, the addi­
tional cost of the first method is roughly in the or­
der of 2 to 3 times Jdom(K)f. The additional costly
steps in the second method are Steps 3 and 7, both
costing approximately 3 ·fdom(K)J operations. Thus,
the additional cost of the second method is roughly
6 fdom(K)f arithmetic operations. This rough com­
parison of the computational costs of the two methods
only addresses the number of arithmetic operations in­
volved. The first method, however, has a much larger
overhead in terms of computing indices in performing
the various summations. Thus, depending on the im­
plementation, the two methods might very well have
comparable performance.
·

5

ANALYSIS OF ALL OUTPUTS
WRT. ONE PARAMETER

Having identified a parameter x to which an output
probability of a Bayesian network is particularly sensi­
tive, one might be interested in establishing sensitivity
functions for all possible posterior marginals in terms
of this parameter. Such an analysis amounts to com­
puting the coefficients in these functions. Note that
while the method described in Section 4 provides for
evaluating the overall robustness of a Bayesian net­
work, the method described in this section is provides
for getting insight in the spread of influence from sep­
arate parameters.
The method of Castillo et a!. (1997) and the method of
Coupe & van der Gaag (1998) can be exploited for es­
tablishing the coefficients in the sensitivity functions
for all possible output probabilities, requiring three
propagations per posterior marginal. Building upon
the ideas put forward in the previous section, how­
ever, a more efficient method is obtained. Theorem 5
provides the basis for our method.
Let the junction tree T be as before.
Also, let y
p(afe) and x
p(bl1r) be as be­
fore. Suppose that, in T, an inward propagation has
Theorem 5

=

8. Compute a�a and f3�a, using (9).

'Y

=

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

322

been performed towards a clique containing the vari­
able B to which the parameter x pertains. Then,
p(e)(x) ="(X+ J with

'Y = O:a + O:�a and J =f3a + f3�a,

(11)

where O:a,f3a and a.�a,f3�a are as in equation (9), ob­
tained from two outward propagations with two dis­
tinct values for x.
Proof: Let K be a clique containing both the variable
B and its set of parents II = pa(B). Let x1 and x2
denote two different values of the parameter x. From
the outward propagation using x1 from clique K, we
obtain the probability vector p(A,e)(x1) = (y�, Y�a)
through marginalization of the clique potential for a
clique H containing A. Similarly, from the outward
propagation with x2, we find the vector p'(A,e)(x2) =

(y�, Y�a)·

From

6

n-WAY SENSITIVITY ANALYSIS

So far, we have addressed one-way sensitivity analy­
ses only, in which the effects of separate parameters
are studied. In this section, we turn our attention to
more general n-way analyses in which the effects of
simultaneous variation of n parameters are studied.
One can regardn-way sensitivity analysis as involving
analyses of joint effects for all subsets of size n or less
of all, say m, relevant parameters. Using the method
of Castillo et al. (1997), this would involve L:::�=l

( 7)

separate analyses and I::�=I ri
probability propa­
gations to compute the 2n coefficients, assuming r-ary
variables.

( 7)

Provided the n parameters all belong to the same
clique, Theorem 6 below states that we only need
one propagation to compute the 2n coefficients, but
I::�= I
local computations involving marginaliza­
tions of clique potentials.

( 7)

=

we get the result stated in the theorem.

0

Theorem 5 provides the basis for our method for com­
puting the coefficients in the functions expressing all
possible output probabilities y = p(a I e) in a single pa­
rameter x p(blrr). The method is composed of the
following steps:
=

1. Enter the evidence e into the junction tree and
perform an inward and an outward propagation
using an arbitrary root clique.
2. Compute the probability vector p(A,e )
(y�, Y�a) through marginalization of the clique po­
tential for a clique H containing A, for all vari­
ables of interest.
3. Change the value of parameter x and perform
an outward propagation from a clique containing
both the variable B and its parents.
4. Compute the probability vector p'(A,e) =
(y�, Y�a) through marginalization of the potential
for clique H, for all variables of interest.

In essence, Theorem 1 states that the mathematical
expression for a probability, p(e)(x), of a vector of in­
stantiations, e, as a function of a probability parame­
ter, x, takes the form of a linear function of x. The­
orem 6 generalizes this statement to the case with n
parameters, x1,. . . , Xn, and states that the resulting
function is a multilinear function in x1, ... , Xn.
To simplify the exposition, we shall assume that the
parameters are independent; that is, for each pair of
parameters, x; = p(bx; lrrx;) and Xj = p(bx; lrrx;), with
the associated variables Bx;, IIx;, Bx;, and IIx;, it
holds true that rrx; # rrx;, Bx; (j. IIx;, and Bx; (j.
IIx;. To generalize the theorem to cover the case of
dependent parameters is fairly straightforward.
Let p be the probability function for a
Bayesian network, where evidence e has been prop­
agated in a junction tree, T, for the network. Let
X
{ x1, . . . , Xn} be a set of parameters of the net­
work, where, for each i = 1, . . . ,n,

Theorem 6

=

X;= p(bx; lrrx;),
with the associated variables, Bx; and IIx;, being
members of a clique, C, in T. Then

p(e)(X) =

5. Compute a.a, a.�a, f3a, and f3�a, using the equation
(9) from Theorem 3.
6. Compute

'Y

and J, using the equation (11).

We would like to note that our method requires just
one inward and two outward propagations to estab­
lish all sensitivity functions for a specific probability
parameter.

L 'Yx(Z) II

z�x

z

+

zEZ

L

1/Jc,

C:ll#1r

where II= {IIx,,...,IIxn }, rr= (rrx,,...,rrxn ), and

'Yx(Z) = (-1)1X\ZI
where

fx(Y)

(-1)1X\YI

L !x(Y),

y�z

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

323

If, instead of summing over subsets S � X\Z, we sum
over the subsets of Z and takes care that the signs of
the terms are preserved, we get the desired result. D
where¢>c = p(C,e), W = X\Y
{w1, ,wk },
k = ! W I , by= (byp···,byiY1), b� = (b:n ,,... ,b: n1w1),
and Px, denotes the initial value of Xi.
=

•

•

•

Proof: Using the same procedure as in the proof of
Theorem 1 we get

p(e)(X)

(� )
(F � c,o�,�·�. /
L L
(X)

¢>c

=

c+

=

·

b'zt

�/c) (X)

c

·· P(b�1 !1l"x,)(xr) · · · p(b�n !1l"xn )(xn)
b'Zn

Lc.·b'zl ,... ,b':l!n ,1f ¢>c
p(b�,17l"x,) ...p(b�n 11l"xJ

+

L

C:0¥71"

¢>c.

L:: li

L:: ...

z

p(b�, !1ru,)(ur)

· .

flzEZ p(bz !1rz ) fluEU p(b� i1ru )
C:07"'7r

+

p(b�, l1ru,)(ur) · p(b�1u1 l1ru1u1)(uiUI)

=

uEU
=

II

· ·

p(b� 17ru)
p(b� 17ru)
-U
1-p(bul1ru)
1 -p(bu l1ru)

�;

(b� 1l"u
1 p( u I u)
uEU

�

)

II x (13)
(-1)ISI
.
xES
sr::;u

(L

)

Inserting (13) in (12) and rearranging terms yields
p(e)(X)

=

L IT L
z

Z<;;X zEZ S<;;U

n

,

p(e)(X)=

(-1)ISI

IT x

xES

L r(Z) II

Z<;;X

z

+

zEZ

J.

(15)

Through one-way analysis involving the parameter x
we obtain the constants ax and f3x in
=

axx + f3x·

The 2n constants in (15) are related to ax and f3x
in the way that ax equals the sum of all coefficients,
r (Z), for which xE Z, and f3x equals the sum of the
remaining coefficients. That is,
ax=

f3x

(12)

¢>c,

=

(

.

and

where U = X\ Z
{u1,... ,uiU I}. Now, expand­
ing the terms p(b� 17ru)(u), uE U, using (1), an easy
calculation yields

II

•

· p(b�1u111l"uiui)(uiUI)

Lc:bz,b;_,,11" ¢>c

L

•

p(e)(x)

The multiple sum can be grouped into sums over the
subsets Z � X such that b� = bz for each zE Z:
p(e)(X)

Note that, since the computation of /X (X) ranges over
all subsets of X, rx(Z) can be computed, for each
Z C X, as a sum over a subset of the terms involved
in the computation of 'Yx(X)
.
The result presented in Theorem 6 is limited in the
sense that all parameters under investigation must
belong to the same clique in the junction tree. We
shall now present a more general method which uti­
lizes the results obtained by lower-order analyses. Let
X n } be the
parameters under investi­
X { x1 ,
gation and write

=

L
L

r (Z)

(16)

r (Z) + J.

(17)

Z<;;X:xEZ

Z<;;X:x{lZ

... , n,

2

Thus, for each one-way analysis of a parameter Xi,
i
1,
we obtain equations of the form (16)
and (17). In addition, we obtain the equation
=

p(e)

=

L

Z<;;X

r (Z)

II Zo + J,

(18)

zEZ

2n

where z0 denote the value specified for parameter z in
the Bayesian network. This gives us a total of
+ 1
equations. However, we need (at least) 2n equations
to compute the 2n coefficients.
Now, if for each parameter, z E Z, we assign a new
value and perform a full propagation, then we obtain
an additional
+ 1 equations. Thus, we can obtain
at least 2n equations by performing

2n

2
l2n : 1J

full propagations with different parameter values, in
addition to the initial propagation performed for the

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

324

one-way analyses. For example, to perform 4-way
analyses, a total of two full propagations is sufficient.
This result can be generalized very easily, since each
m-way analysis gives rise to 2m equations of the form
(16) and (17). Thus, to perform n-way analyses, where
n > m, we need at most

( )

additional propagations, as there are ;:, relevant m­
way analyses. So, for example, if we have performed
2-way analyses and want to perform 5-way analyses,
no further propagations are needed.
7

CONC LUD ING REMARKS

We have presented methods for sensitivity analysis
of Bayesian networks which are significantly more ef­
ficient than current methods. In the case of one­
way analysis, the number of probability propagations
of current methods grows linearly in the number of
relevant parameters, whereas the methods presented
above only requires one inward and one or two out­
ward propagations, no matter the number of relevant
parameters.
To substantiate the importance of this difference, we
have investigated three real-world networks to get an
idea of the typical number of relevant parameters in
a realistic scenario. All three networks are from the
medical domain: a subnetwork of Munin (Andreassen
et a!. 1989) containing 1003 variables, a network mod­
elling the pathophysiology of ventricular septal defect
(Coupe et al. 1999) containing 38 variables, and a net­
work related to disorders in the oesophagus containing
70 variables. The investigation were conducted using
real patient data involving, respectively, 15, 5, and 3
patients. The average number of relevant parameters
were found to be 16313, 738, and 992, respectively.
(Since no censoring of parameters representing func­
tional relationships were performed on parameters for
the Munin network, the figure 16313 is probably some­
what overestimated.)
Efficient methods for sensitivity analysis play an im­
portant role in both the knowledge acquisition and the
validation phases for manually constructed Bayesian­
network models.
Coupe et al. (1999) reports on an empirical study using
sensitivity analysis to focus attention on the most in­
fluential parameters in the knowledge acquisition pro­
cess, thereby considerably reducing the time required
to acquire the parameter values.
The validation phase involves two aspects: fine-tuning
and robustness analysis. The fine-tuning aspect in-

volves adjustment of the parameter values to make the
network respond correctly to a number of test cases.
A gradient descent approach is useful for that purpose
(cf. neural network type training), where the gradi­
ent of a posterior marginal with respect to a subset of
parameters can easily be computed through a minor
modification of the algorithms for sensitivity analy­
sis. Based on the work described in the present paper,
Jensen (1999) has suggested a method for gradient de­
scent training of Bayesian networks.
Once a network has been fine-tuned, and thus responds
correctly on a selection of test cases, the robustness
of the network may be investigated. This involves,
in essence, determining lower and upper bounds for
parameter values for which the output of the network
still agrees with the test cases. A parameter value
close to one of the bounds indicate a possible lack of
robustness. Given analytic expressions for the outputs
in terms of the parameters, derived by e.g. methods
described in the present paper, these bounds are easily
determined.
The time complexity ofn-way sensitivity analysis may
be fairly high for large n, even with the methods pre­
sented in this paper. Also, our method assumes that
the variables and the parents associated with then pa­
rameters reside in the same clique in a junction tree.
The method of Coupe et al. (2000) for n-way sensitiv­
ity analysis is based on propagation of tables of coeffi­
cients in a junction tree, and, therefore, has a (poten­
tially very much) larger space requirement. However,
their method is general in the sense that it does not
put any restrictions on the location of the parameters.
During the initial phase of the
work, the authors received valuable comments from
Finn V. Jensen. Also, he suggested the basis for the
general method for n-way analysis, described at the
end of Section 6.
Acknowledgements

The research has been partly funded by the Danish
National Centre for IT research, Project no. 87.2.



in-depth presentations, see above references. We shall
assume that all variables of a Bayesian network are

The efficiency of inference in both the Hugin
and, most notably, the Shafer-Shenoy archi­
tectures can be improved by exploiting the
independence relations induced by the incom­
ing messages of a clique.

That is, the mes­

sage to be sent from a clique can be com­

discrete.
A Bayesian network consists of an independence graph,

G

=

(V,E),

(which is an acyclic, directed graph or,

more generally, a chain graph) and a probability func­
tion,

p,

which factorizes according to G. T hat is,

puted via a factorization of the clique poten­

Pv

tial in the form of a junction tree. In this pa­
per we show that by exploiting such nested

=

IT p(v I pa(v)),

vEV

junction trees in the computation of messages

where pa(v) denotes the parents of v (i.e., the set of

both space and time costs of the conventional

vertices of G from which there are directed links to

propagation methods may be reduced. The

The junction tree corresponding to G is constructed

paper presents a structured way of exploit­

v).

via the operations of moralization and triangulation

ing the nested junction trees technique to

such that the nodes of the junction tree correspond

achieve such reductions.

to the cliques (i.e., maximal complete subgraphs) of
the triangulated graph. To each clique, C, and each

The usefulness of

the method is emphasized through a thor­
ough empirical evaluation involving ten large

separator, S, (i.e., link between a pair of neighbouring

real-world Bayesian networks and the Hugin

cliques of the junction tree) is associated potential ta­
bles </>c and 1/Js, respectively, by which, at any time,

inference algorithm.

we shall denote the current potentials associated with

1

C and S.

INTRODUCTION

Now define

'1/Jc

Inference in Bayesian networks can be formulated as

=II 1/Jv;,

message passing in a junction tree corresponding to the
network (Jensen, Lauritzen & Olesen
Shenoy

1990).

1990,

Shafer &

More precisely, a posterior probability

distribution for a particular variable can be computed
by sending messages inward from the leaves of the tree
toward the clique (root) containing the variable of in­
terest. If a subsequent outward propagation of mes­

where ¢v,
p(viiV. \{vi}) and pa(v;) = V; \ {v;}.
That is, for each clique, C, is associated a subset of
=

the conditional probabilities specified for the Bayesian
network, and the function

'1/Jc

tree is given as

sages from the root toward the leaves is performed, all
cliques will then contain the correct posterior distribu­
tions (at least up to a normalizing constant). In many
situations, however, we are only interested in the pos­
terior distribution(s) for one or a few variables, which
makes the outward pass redundant.

represents the product

over this subset. Initially the potentials of the junction

1/Jc ='lj!c

and

¢s

=

1

for each clique, C, and each separator, S.
Propagation is based on the operation of absorption.
Assume that clique C is absorbing from neighbouring

The Hugin and the Shafer-Shenoy propagation meth­

cliques C1, . . . , Cn via separators St, . . . , Sn.

ods will be reviewed briefly in the following; for more

two architectures, this is done as indicated in Table 1.

In the

Nested Junction Trees
Hugin
1.

2.

4>8,
4>c

L

=

C;\S•
=

4>c

IT

3.

</>s, := 4>8,,

4.

¢c := 4>(;

Shafer-Shenoy

¢c.,

i=l

i

i

=

1, ... ,n

1,

1.

4>8,

L

=

¢c.,

C,\S;

i

=

1,

... , n

n

4>8
,
4>s,
=

295

2.

4>(:

=

1/Jc IJ 4>5,
i=l

. . . , n

3.

</>s,

:=

¢5,,

i

=

1, . .

. , n

Table 1: Absorption in the Hugin and the Shafer-Shenoy architectures.

{X1, X2, X3, X4},

that C receives

Propagation can be defined as a sequence of inward

contains variables

message absorptions followed by a sequence of outward
message absorptions, where inward means from leaf

messages ¢>{x1 ,x2} and ¢{x2,x3}, and that the poten­
tial ¢{x3,x4} was initially associated with C. Themes­

cliques of the junction tree towards a root clique, and

sage,

outward means from the root clique towards the leaves.

¢{X1,X4},

to be sent to Dis thus

Note that the ¢5 's are called messages. In the inward
,
pass, since then ¢s = 1 for all separators, the only
difference between the two architectures is step 4 of
the Hugin procedure (see Table 1).

In the outward pass, on the other hand, the differ­
ence between the two architectures becomes more pro­
nounced. Consider clique C which, in the inward pass,
has absorbed messages from C2, . . . , Cn and sent a
message to C1. Now, having received a message from

"L:

¢{x •.x2.x3,x4}

L

¢{x.,x2}4>{x2,x3}¢{x3,x4}{1)

{X2,X3}

{X2,X3}

However, since ¢{X!,X2} does not depend on x3, we
can compute ¢{x1,x4} as

cP{x •. x4}

cl in the outward pass, it is going to send messages

=

L¢{xl.x2} L¢{x-..x3}¢{xJ,x4},
X3

X2

(2)

to 02, . . . , Cn. In the two architectures, this is done as

which reduces both space and time complexities: as­

indicated in Table

suming all binary variables, Eq.

2.

Note that in the Hugin architecture, when a clique C
absorbs a message

¢8, ,

<Ps,

it is always true that
=

L ¢c.

C\S;

This fact is exploited in the Hugin architecture to
avoid performing repeated multiplications. Hence, in
the outward pass of the Hugin algorithm a clique C can
compute the product of all messages from its neigh­
bours simply by 'substituting' one term of ¢c using
division. Thus, the main difference between the Hugin
and the Shafer-Shenoy architectures is the use of di­
vision. As we shall see later, avoiding division is ad­
vantageous when we use nested junction trees for in­
ference.

1

implies a space cost

of 16 and a time cost (i.e., number of arithmetic op­
erations) of 64

(3

x

16 for the multiplications and 16

for the marginalization), whereas Eq.

2 implies a space

cost of 8 and a time cost of 48.
Basically, the trick in Eq. 2 is all there is to inference
in Bayesian networks. In fact, the first general infer­
ence methods for probabilistic networks developed by
Cannings, Thompson & Skolnick (1976) used exactly

that method. Their method is referred

to as "peel­

ing", since the variables are peeled off one by one un­
til the desired marginal has been computed. In most
inference methods for Bayesian networks, finding the
peeling order (elimination order) is done

as

an off-line,

one-off process. That is, the acyclic, directed graph of
the Bayesian network is moralized and triangulated
(Lauritzen & Spiegelhalter 1988), and a secondary

The computation of messages is carried out as indi­

structure referred to as a junction tree (Jensen 1988)

cated in Tables 1 and

is constructed once and for all. The junction tree is

2,

namely by multiplying all

<Pv, 's and ¢$ 's together and marginalizing from that
)

product. However, often ¢5 can be computed via a series of marginalizations over smaller tables , which can

greatly reduce both space and time complexities.
As a small illustrative example, assume that clique C

then used as an efficient and versatile computational
device.
Now, since Eq.

2

expresses nothing but inference in a

probabilistic network consisting of four variables and
three probability potentials, the computation of the

Kj�rulff

296

Hugin

1.

- 1>
1>*cc

2.

For j

=

¢8j

Table

message,

2:

Clique

¢{x,,x4},

C

r/>s,

4.

r/>c

:=
:=

1>8,
r/>s,

1.

· -

2

to

n

For j

2 to n
·

do

II

i=l, ... ,j - l ,j+ l , ... ,n

1>8,

1>*Si-- 2::: 1>6

2::: rt>'C

=

=

1>6 = 1/Jc

do

C\Si

C\S1

¢8,

r/>s,

2.

:=

1>8,

¢'C

absorbs a message from clique

C1

and sends messages to its remaining neighbours.

can be formulated as inference in

a junction tree with cliques

{X3, X4}.

3.

Shafer-Shenoy

{X1,X2}, {X2,X3}, and

Thus, we have a junction tree in the clique

of another junction tree!

For slightly more compli­

cated examples the nesting level might even be larger
than two as shall be exemplified in Section

2,

where

we describe the construction of nested junction trees.
Section

3

describes the space and time costs associ­

ated with computation in nested junction trees, and
Section

4 briefly explains how the space and time costs

of an inward probability propagation can be computed
through propagation of costs. Section

5

presents the

results of an empirical study of the usefulness of nested
junction trees. Finally, in Section

6,

we conclude the

work by discussing the benefits and as well as the lim­
itations of nested junction trees.

2

CONSTRUCTING NESTED
JUNCTION TREES

s1

=

s2

=

s3

=

v1

=

{22, 26, 83, 84,
94,95, 168}
{83, 84, 97,
164, 168}
{94, 95, 97}
{22, 26, 97}

To illustrate the process of constructing nested junc­
tion trees, we shall consider the situation where clique

c16

is going to send a message to clique

the junction tree of a subnet, here called
of the

Munin

c13 in
Munin1,

network (Andreassen, Jensen, Ander­

sen, Falck, Kjrerulff, Woldbye, S�rensen, Rosenfalck
& Jensen

1 989).

Clique

C16

and its neighbours are

shown in F igure 1. For simplicity, the variables of C16,

{22, 26, 83, 84, 94, 95, 97, 164, 168},

are named corre­

sponding to their node identifiers in the network, and
they have

4, 5, 5, 5, 5, 5, 5, 7, and 6

states, respec­

tively.
The set of probability potentials for a Bayesian net­
work defines the cliques of the moral graph derived
from the acyclic, directed graph associated with the
network (notice that the directed graph is also de­
That is, each potential r/>v
(e.g., given by a conditional probability table) induces

fined by the potentials).

F igure 1:
and

r/>s3

Clique

C16

receives messages

from cliques c19, c26' and

c63'

¢s11 r/>s2,

respectively.

Based on these messages and the probability potential,
r/>v,

=

P(97l22, 26),
cl3·

sent to clique

a message must be generated and

Nested Junction Trees

297

a complete subgraph. A junction tree is then con­
structed through triangulation of the moral graph.
Thus, in our example, the undirected graph induced
by the potentials ¢sll c/Js2, ¢s3, and ¢v1 may be
depicted as in Figure 2. At first sight this graph
looks quite messy, and it might be hard to be­
lieve that its triangulated graph will be anything
but a complete graph. However, a closer exam­
ination reveals that the graph is already triangu­
lated and that its cliques are {83, 84, 97, 164, 168} and
{22,26,83,84,94,95,97,168}.

Figure 3: The undirected graph induced by potentials
¢s1, c/Js3, and ¢ v,.

down. The two potentials induce the graph shown in
Figure 4, hence a further break-down is possible as the
graph is triangulated and contains two cliques.

Figure 2: The undirected graph induced by potentials
¢sl, ¢s2, ¢s3, and ¢v,.
So, the original 9-clique (i.e., clique containing nine
variables) with a table of size 2,625,000 has been re­
duced to a junction tree with a 5-clique and an 8-clique
with tables of total size 381,000 (including a separator
table of size 750).
Thus encouraged we shall try to continue our clique
break-down! In the two-clique junction tree, the 5clique has associated with it only potential ¢s2, so
it cannot be further broken down. The 8-clique, on
the other hand, has got the remaining three potentials
associated with it. These potentials (i.e., ¢s1, ¢s3, and
¢v,) induce the graph shown in Figure 3.
This graph also appears to be triangulated and con­
tains the 5-clique {22, 26, 94,95,97} and the 7-clique
{22, 26, 83, 84, 94,95,168} with tables of total size
78,000 (including a separator table of size 500). The
reduced space cost is 375,000- 78,000 297,000.
=

In this junction tree, the 7-clique cannot be further
broken down since it contains only one potential. The
5-clique, however, contains two potentials, ¢s3 and
¢v1 , and can therefore possibly be further broken

Figure 4: The undirected graph induced by potentials
¢s3 and ¢v ,.
Now, no further break-down is possible. The result­
ing nested junction tree, shown in Figure 5, has a
total space cost of 81,730, which is significantly less
than the original 2, 625,000. Carrying out the nesting
to this depth, however, have a big time cost, since,
for example, 500 message passings is needed through
the separator {83,84, 97,168} in order to generate the
message from C16 to C13. A proper balance between
space and time costs will most often be of interest. We
shall address that issue in the next section.
3

SPACE AND TIME COSTS

As already discussed in Section 2, the smallest space
cost of sending a message from a clique C equals the
accumulated size of the clique and separator tables of
the nested junction tree(s) induced by the potentials
of C (i.e., messages sent to C and potentials initially

298

Kj�rulff

872, 750, 000

t

525,000

22,26, 83,84,94,95,164,168
2, 625,000

5, 250 (7x)

Figure 5: The nested junction tree for clique C16 in

Munin 1.

Only the connection to neighbour C13 is shown. The

small figures on top of the cliques and separators indicate table sizes, assuming no nesting. The labels attached
to the arrows indicate

(1) the

time cost of sending a single message, and

compute the separator marginal one nesting level up.

(2) the

number of messages required to

299

Nested Junction Trees

associated with it). For example, sending a message

from clique Cb (see Figure 5) has a smallest space cost
of

75, 000 + 500 + (100 + 5 + 125)

=

75, 730.

Note that

this smallest space cost results from choosing clique

Cc

as root. Choosing clique Cd as root instead would

make the inner-most junction tree (cliques

Ce

and C1)

collapse to a single clique with a table of size
resulting in an overall space cost of

2, 500,

77, 500. A

sim­

case be

2, 625,000

and 5

x

2, 625, 000

=

13, 125,000,

respectively. The similar costs in the four-level nest­
ing case are

81, 730 and 872,750,000, which are 32
67 times larger, respectively, than
conventional costs. A more satisfying result is ob­

times smaller and
the

tained by avoiding the two inner-most nestings (i.e.,
collapsing cliques Cc, Cd, Cd, and c,, which happens

if instead of Ca w e let Cb be root) , in which case we get

ilar analysis shows that, depending on which cliques

costs

are selected as roots, the space cost of generating the

only slightly larger than in the conventional case, but

message for clique cl3 varies from 81,730 to

with a 7 times reduction in the space cost.

381,000.

381,000 and 14,211, 750 with the time cost being

Let us consider the time cost of sending a single mes­
sage from clique

Cb

to clique Ca, and assume that

4

PROPAGATION OF COSTS

clique Cc is selected as root. To generate a message
from Cb,

Cc

must receive five messages from Cd, corre­

sponding to the number of states of variable

97 which

The calculation of the costs of performing inward prob­
ability propagation toward a root clique

C

can be for­

is a member of the (Ca, Cb)-separator but not a mem­

mulated elegantly through propagation of costs in the

ber of Cc. The time cost of each message from Cd is

junction tree. Let, namely, each clique send a cost mes­

17, 000 as

shall be explained shortly.

sage (consisting of a space cost and a time cost) being

Given this information we can now do our calculations.
Generating a message involves the operations of multi­
plication (of the relevant potentials) and marginaliza­
tion. The cost of multiplying the two potentials onto
the clique table is 2 times the table size. The cost of
marginalization equals the table size. Therefore, the
time cost of sending a message from clique Cb is
5 X

(17,000 + 3

X

75, 000)

=

1, 210,000.

Notice that the cost of marginalization equals the size
of the larger of the clique table and the separator ta­

the sum of the costs of sending a message (i.e, a proba­
bility potential) and the sum of the cost messages from
its remaining neighbours. Then a cost message states
the cost of letting the sender be root in the subtre e
containing the sender and the subtrees from which it
received its messages. Thus, when C has received cost
messages from all of its neighbours, the overall cost
of an inward probability propagation is given by the

sum of its cost messages plus the co st of computing
the C-marginal potential.
Now, if we perform an outward propagation of costs

C,

ble. For example, whenever Ca has received a message

from

from Cb, it must basically run through the

cost of an inward probability propagation to any other

separator table.

(Cl6,Cl3)­

Smart indexing procedures might,

we will subsequently be able to compute the

clique, just as we did for clique C!

however, reduce that cost dramatically. So, our cost
considerations given here are worst-case.

5

EXPERIMENTS

The time cost of 17, 000 for sending a single message
from

Cd

to Cc is found when selecting c, as root; se­

lecting Ce as root instead would have a cost of

20, 625.

Clique C., must send 20 messages for C1 to be able
to generate a single message to Cc. Notice, again,
that this could be done much more efficiently, since
for each message variable

97

is fixed, effectively split­

ting the inner-most junction tree into two. However,

To investigate the practical relevance of nested junc­
tion trees, the cost propagation scheme described
above has been implemented as an extension to the
Hugin algorithm.

In order to find a proper balance

between space and time costs, the algorithm makes a
junction tree representation of a clique only if

to keep the exposition as clear and general as possible,

space_cost + --y time_cost,
·

we shall refrain from introducing smart special-case
procedures. Now using the same line of reasoning as
above, we get the time cost of
20

X

(100 + 2

X 125 +

500)

=

17, 000,

where each marginalization has a cost of

500 since the

table of C 1 is smaller than the (Cc, Cd)-separator ta­
ble.

is smaller than it is using conventional representation.
The time factor, /, is chosen by the user.
Cost measurements have been made on the following
ten large real-world networks. The KK network is an
early prototype model for growing barley.

The Link

network is a version of the LQT pedigree by Professor
Brian Suarez extended for linkage analysis (Jensen
Kong

nested) message generation would in the C16-to-C13

agnosing lymph node diseases (Heckerman, Horvitz

1996).

&

The Pathfinder network is a tool for di­

The space and time costs of conventional (i.e., non­

&

Kj�rulff

300

Nathwani

1992).

The

Pignet

network is a small subnet

of a pedigree of breeding pigs. The Diabetes network is

method for inward probability propagation, as indi­
cated in Section 1. However, in the example shown in
the peeling method is not able to exploit e.g.

a time-sliced network for determining optimal insulin

Figure

dose adjustments (Andreassen, Hovorka, Benn, Olesen

the conditional independence of variable

& Carson 1991). The Munin1-4 networks are different

ables

subnets of the

168.

The

Water

Munin

sy stem (Andreassen et al.

1989).

network is a time-sliced model of the bio­

logical processes of a water treatment plant (Jensen,
Kjrerulff, Olesen & Pedersen

1989).

ward probability propagation is measured for each of
obtained.

22, 26, 94,

and

95

given variables

164 of vari­
83 , 84, 97, and

So, the technique presented in this paper is much

more general than peeling.
Note that if the triangulated version of the graph in­
duced by the separators of a clique is not complete (i.e.,

The average space and time costs of performing an in­
these ten networks.

5

Table 5 summarizes the results

All space/time figures should be read

as

contains more than one clique), then one

fill-in links

of

that clique are

more of the

or

redundant; that is, the

clique can be split into two or more cliques. Therefore,
assuming triangulations without redundant

fill-ins, the

The first pair of space/time columns lists

nested junction trees technique cannot be exploited in

the costs associated with conventional junction tree

the outward pass of the Hugin algorithm, since mes­

millions.

propagation. The remaining three pairs of space/time

sages have been received from all neighbours (includ­

columns show, respectively,

least possible space

ing the recipient of the message). In the Shafer-Shenoy

cost with its associated time cost, the costs corre­

algorithm, on the hand, there is no difference between

sponding to the highest average relative saving, and

the inward and the outward passes, which makes the

the

the least possible time cost with its associated space

nested junction trees technique well-suited for that al­

cost. The percentages in parentheses indicate the rela­

gorithm. A detailed comparison study should be con­

tive savings calculated from the exact costs. The high­

ducted to establish the relative efficiency of the nested

est average relative savings were found by running the

junction trees technique in the two architectures.

algorithm with various 1-values for each network. The
optimal value, 1*, varied from

0.25

to

0.45.

Table 3 shows that the time costs associated with min­
imum space costs are much larger than the time costs
of conventional (inward) propagation. Thus, although
maximum nesting yields minimum space cost, it is not
recommended in general, since the associated time cost
may be unacceptably large.

Acknowledgements

I

wish to thank Steffen L. Lauritzen for suggesting the

cost propagation scheme, Claus S. Jensen for provid­
ing the

Pignet networks, David Beckerman
Pathfinder network, Kristian G. Ole­
sen for providing the Munin networks, and Steen An­
dreassen for providing the Diabetes network.
Link

and

for providing the

However, as the 1 = 1* columns show, a moderate
increase in the space costs tremendously reduces the
time costs.

(The example in Figure 5 demonstrates

the dramatic effect on the time cost as the degree of
nesting is varied.)

In fact, the time costs of conven­

tional and nested computation are roughly identical

for

1

=

1*, while space costs are

still

significantly re­

duced for most of the networks.




The paper presents a method for reducingthe
computational complexity of Bayesian net­
works through identification and removal of
weak dependences (removal of links from the
(moralized) independence graph). The re­
moval of a small number of links may re­
duce the computational complexity dramat­
ically, since several fill-ins and moral links
may be rendered superfluous by the removal.
The method is described in terms of im­
pact on the independence graph, the junc­
tion tree, and the potential functions associ­
ated wit h these. An empirical evaluation of
the method using large real-world networks
demonstrates the applicability ofthe method.
Further, the method, which has been imple­
mented in Hugin, complements the approxi­
mation method suggested by Jensen & An­
dersen (1990).
1

INTRODUCTION

bronchitis and lung cancer (shorthand: b Jl l), and
between coughing and lung cancer (c Jl l). It might,
however, be quite sensible to replace c Jl l with
c Jl ll (b, d); that is,
conditional independence be­
tween coughing and lung cancer given bronchitis and
dyspnoea. The independence graph of this alterna­
tive model could be achieved through replacement of
the directed link from coughing to dyspnoea with an
undirected one, whereby the chain graph of Figure 1b
emerges. Semantically, this implies that the relation­
ship between coughing and dyspnoea is non-causal.
(Note that an independence graph equivalent to that
of Figure 1b might be obtained by simple reversal of
the directed link from coughing to dyspnoea.)

(a)

(b)

Figure 1: Removal of the moral link between coughing
and lung cancer in part (a) results in the 'less demand­
ing' independence graph of part (b).

Decision making in domains wit h inherent uncertainty
using Bayesian (belief) networks and exact computa­
tions often involve very high dimensional probability
tables. Hence, for many practical problems, exact
computations are prohibitive. Therefore, approximate
solutions are often the best that can be hoped for.
Such solutions can be provided through simulation or
model simplification. We shall address the latter, al­
though methods of the former type shall play an im­
portant role in our approach, which involves enforce­
ment of additional conditional independence assump­
tions through removal of links from the moralized in­
dependence graph.

Specification of conditional probabilities for model (a)
involves a four-dimensional table for (d I b, c, l) and a
two-dimensional one for (c I b), whereas for model (b)
it suffices to specify two three-dimensional tables, one
for (c, d Ib) and one for (d I b, l). If, for example, each of
the four variables is described in terms of five discrete
states, this meansthat model (a) requires specification
of 4 53 + 4 5 = 520 conditional probabilities, whereas
model (b) requires 'only' 24 5 + 4 52
220.

To illustrate the approach, consider the following toy
example. Assume that dyspnoea (shortness of breath)
(d) can be caused by one or more of the 'diseases'
coughing (c) , bronchitis (b), and lung cancer (l), and
further that bronchitis causes coughing (Figure la).
This model suggests marginal independence between

Briefly, the method provides a systematic way of per­
forming model transformations as illustrated in Fig­
ure 1 such that one additional conditional indepen­
dence assumption is explicitly being enforced (and pos­
sibly some implicit ones, which follow naturally) and
such that an (sub)optimal balance between reduction

·

·

·

·

=

Usingthe suggested approximation method, model (b)
can be obtained from model (a) by removal of the
moral link between coughing and lung cancer.

Link Removal in Bayesian Networks

of computational complexity and approximation er­
ror is achieved. A candidate new (explicit) assump­
tion takes the form a ll /31 C \ {a, /3}, where C is a
clique in a junction tree corresponding to an indepen­
dence graph Q, such that a and f3 are connected in
the moral graph corresponding to Q and such that C
is the unique clique containing both a and /3. That
is, the method aims at splitting (large) cliques into
smaller ones while keeping a small 'distance' between
the exact and the approximate distributions. This
distance is computed using either exact or simulated
clique potentials of a (imaginary) junction tree, where
the storage requirements of simulated potentials, ob­
tained through Monte-Carlo sampling, depends only
linearly on both the clique size and the sample size.
The rest of the paper is organized as follows. Section 2
reviews the key features of graphical chain models and
junction trees necessary for the presentation. Section 3
presents the method, including descriptions of its im­
pact on the junction tree, the independence graph, and
the potential functions associated with these. Please
note that the results are stated without proofs; the in­
terested reader is referred to Kjrerulff (1993). Section 4
demonstrates the applicability of the method by pre­
senting some results of applying it on large real-world
networks. Section 5 summarizes the features of the
presented approach and argues that it complements
the approach of Jensen & Andersen (1990).
For a discussion of the choice of criterion for selecting
the optimal link to remove and a presentation of the
implications of link removal in terms of correctness of
inference, the reader is referred to Kjrerulff (1993).
2

GRAPHICAL CHAIN MODELS
AND JUNCTION TREES

The term Bayesian networks has traditionally been
used as a synonym for recursive graphical models
(Wermuth & Lauritzen 1983) for which the indepen­
dence structure is encoded by directed acyclic graphs.
In the present paper we shall, however, use 'Bayesian
networks' as a synonym for the more general class
of models denoted graphical chain models (Lauritzen
& Wermuth 1984, Lauritzen & Wermuth 1989) for
which the independence structure is encoded by chain
graphs. Notice that the class of graphical chain models
also contains the subclass of graphical models (Dar­
roch, Lauritzen & Speed 1980) with independence
structure encoded by undirected graphs.
2.1

CHAIN GRAPHS

In the following the notion of chain graphs shall be
reviewed briefly and fairly informally. For a more
thorough treatment of the subject see e.g. Frydenberg
( 1989).
Let Q
(V, E = Ed U Eu) be a graph with nodes
(vertices) V and links (edges) E � V x V, where Ed
=

=

375

{(a, {3) E E I (/3, o:) ¢ E} is the subset of directed
links and Eu
{(a, {3) E E I (/3, a) E E} the subset
of undirected links. If there is a link between o: and
{3, denoted a,...., {3, they are said to be connected. A
directed link between a and f3 is denoted a ---t f3 or
a+- {3, and an undirected link is denoted a - {3. We
shall use a "' {3, etc. to denote either 'a and f3 are
connected' or 'the link between a and /3' depending
on the context.
=

A path (a
a1, ... , O:k
/3) from o: to {3 in Q is an
ordered sequence of distinct nodes such that ai "'O:i+1
for each i = 1, ... , k - 1. The path is undirected if
1, . . . , k - 1. The path is directed if
a- {3 for each i
either a-{3 or ai ---t ai+l for each i = 1, .. . , k - 1 and
the path includes at least one directed link. A cycle is
a path 1r = ( o: = a1, ... , ak = /3) with the exception
that o: = {3.
=

=

=

For A, B, C � V, C is said to separate A from B if
for all paths (a = a1,
, ak = /3), where a E A and
f3 E B, {at, ... ,ak} n C =10. A graph Q is connected
if there is a path between each pair of nodes of Q.
Unless otherwise stated, connectivity shall henceforth
be assumed.
•

•

.

Now, Q is a chain graph if it contains no directed cycles. If Q is a chain graph, then { K1, ... , Kn} are
called the chain components of Q if {K1, ... , Kn} is
the set of connected components of (V, Eu). There
are two important special classes of chain graphs. If
n = lVI (i.e., one node per chain component), Q is
called a directed acyclic graph (DAG). If n = 1, Q is
called an undirected graph.
A subset A � V induces a subgraph QA = (A, EA) of
Q, where EA = En (A x A). (Note that any subgraph
of a chain graph is a chain graph.) A graph is complete
if all nodes are pairwise connected. A subset A c;::; V
is complete if it induces a complete subgraph, and if
A is maximal (i.e., there is no complete subset B c;::; V
such that A C B), then it is called a clique.
The parents of A� V is the subset pa(A) � V \ A such
that for each f3 E pa(A) there is an a E A for which
f3 -t a. The set of children of A, denoted ch(A), is
defined analogously. The neighbours of A is the subset
nb(A) � V \A such that for each f3 E nb(A) there is
an a E A for which a-{3. The ancestral set of A c;::; V
is the subset An( A) � V such that for each f3 E An( A)
either f3 E A or there is a directed or undirected path
from f3 to at least one o: E A.
The moral graph gm of a chain graph Q is obtained
by first adding undirected links between each pair of
unconnected nodes in pa(K) for each chain component
K, and then replacing all directed links by undirected
ones.
An undirected graph Q = (V, E) is triangulated (also:
decomposable or chordal) if each cycle of length greater
than 3 has a chord (i.e., a link between two non­
consecutive nodes of the cycle).

376

2.2

Kj.-erulff

2.3

GRAPHICAL CHAIN MODELS

For a chain graph Q = (V, E) we consider a collection
of discrete random variables (Xa)aEV taking values
in probability spaces Sp(Xa)- For brevity we shall
interchangeably refer to a E V as both a node and a
variable. Thus we shall write e.g. a instead of Xa. For
a subset A s;;; V we let Sp(A) = XaEASp(a) (i.e., the
Cartesian product of the state spaces of the variables
in A).
A probability function p = pv is said to factorize ac­
cording to a chain graph Q = (V, E) if there exist non­
negative functions ¢>A defined on Sp(A) such that
p ex

II ¢>A,

(1)

AEA

where A is the set of cliques of g=. The functions ¢>A
shall be called component potentials of p. For Q being
a DAG this simplifies to
p=

IJ

(2)
p(v I pa(v)).
vEV
A similar factorization exists in the general case. Let
namely
p(K I pa(K))

=

( II ) I (L IT )
AEAK

¢>A

K AEAK

¢>A

(3)

where K is a chain component of Q and AK = {A
AI As;;; K U pa(K),A n K =/:- 0}. Then
P=

II p(KI pa(K)),

KEIC

E

(4)

where JC is the set of chain components of Q.
If p factorizes according to Q, then Q is said to be an in­
dependence graph of p, and p is a graphical chain model
(a probability function of a Bayesian network with Q
as underlying graph). (The phrase 'p is Markov with
respect to Q' is a synonym for 'p factorizes according
to Q'.)
In the special case of Q = (V, E) being a DAG all
conditional independence statements captured by Q
can be found using the d-separation criterion of Pearl
(1988) or the equivalent criterion of Lauritzen, Dawid,
Larsen & Leimer (1990). But in the general case
the Markov properties (i.e., conditional independence
properties) captured by g are expressed by the follow­
ing theorem (Frydenberg 1989).
Theorem 1

Let p factorize according to a chain
graph, Q = (V, E). Then A ll BI C with respect to
p for any subsets A, B, C � V whenever C separates
A from B in ( Q An(AuBuo})m.

Note that the formulation of this theorem, describing
the global chain Markov property, is identical to the
theorem of Lauritzen et al. (1990) describing the di­
rected global Markov property for recursive graphical
models (i.e., where Q is a DAG).

JUNCTION TREES

By exploiting the conditional independence relations
among the variables of a Bayesian network, the under­
lying joint probability space may be decomposed into a
set of subspaces corresponding to a decomposable (hy­
pergraph) cover of the moralized graph such that exact
inference can be performed by simple message passing
in a maximal spanning tree of the cover (Lauritzen &
Spiegelhalter 1988, Jensen 1988, Jensen, Lauritzen &
Olesen 1990). Technically, a decomposable cover of
a Bayesian network with underlying chain graph 9 is
created by triangulating gm (i.e., adding undirected
links, so-called fill-ins, to gm to make it triangulated).
That is, the set of cliques of the triangulated graph is
a decomposable cover of the network.
Jensen (1988) has shown that any maximal spanning
tree of a decomposable cover, C, can be used as the
basis for a simple inward/outward message-passing
scheme for propagation of evidence (belief updating)
in Bayesian networks, where maximality is defined in
terms of the sum of cardinalities of the intersections
between adjacent nodes (cliques) of the tree. Jensen
named these trees junction trees. The intersections be­
tween neighbouring cliques of a junction tree are called
separators (Jensen et al. 1990).
We shall henceforth refer to a junction tree by the pair
(C, S) of cliques and separators. It can be shown that
for each path (C C1, . , Ck D) in a junction tree,
c n D s;;; ci for all 1 :::; i :::; k, implying that A ll B I s
for each S E S, where A and Bare the sets of variables
of the two subtrees (except S) induced by the removal
of the link corresponding to S {Jensen 1988).
=

.

.

=

To each clique and each separator is associated a belief
potential, ¢>A. The joint probability distribution, Pv,
of a Bayesian network with a junction tree (C, S) is
proportional to the joint (system) belief ¢v given by

IToEC 1>c .
(5)
ITSES ¢>S
A belief potential tPA is normalized if :EA tPA = 1. If
all belief potentials of a junction tree are normalized,
then ¢>v is normalized (i.e., Pv = ¢>v).
ex

Pv

A junction tree Y

=

¢>v

==

(C, S) is said to be consistent if

L <Pc L <Pn
ex

0\D

D\C

for all C,D

E

C

(i.e., the marginal potentials for C n D with respect
to ¢>c and <Pv are proportional). Consistency of T
shall interchangeably be referred to as consistency of
its associated joint belief, ¢>v.
3

ENFORCING INDEPENDENCE
ASSUMPTIONS

The computational complexity imposed by a particu­
lar junction tree (C, S) is roughly determined by the

Link Removal in Bayesian Networks

377

clique, C E C, with the largest state space. Thus by
splitting C into smaller cliques a significant reduction
of the computational complexity might be obtained.
If { o:, ,8} � C such that there is no other clique in C
containing {o:, ,8}, then adding o: ll ,81 C \ { o:, ,8} to
the set of independence statements amounts to split­
C \ {o:}, which
ting C into Ca. = C \ {,8} and Cf3
might or might not become new cliques of the modified
junction tree (see the examples of Figure 2).

c

enforce
dl (a, e)

remove red.

ll

fill-in

=

o:-,8

1/1
0

"(

o:-,8

IX
"(-0

o:-,8
0

"(

(c)

(b)

(a)

I

I

Figure 2: Removal of o:- ,8: (a): both {o:,"f} and
{,8,"/} become new cliques; (b): {,8,"(} become a new
clique, but { o:, "'} does not; (c): neither {o:} nor {,8}
become new cliques.
The requirement that C must be the only clique con­
taining {o:, ,8} ensures that o: ll ,81 C\{o:,,B} or, equiv­
alently, that the graph obtained by removing o:-,8 in
the triangulated graph corresponding to (C, S) is tri­
angulated; see Kjrerulff (1993) for details.
3.1

AN EXAMPLE

To understand the main issues of the proposed ap­
proximation method we shall present a small example.
Consider the sample chain graph of Figure 3a with
corresponding moral graph of Figure 3b (solid links).
The dashed link is a fill-in added to make the graph
triangulated. The junction tree corresponding to the
triangulated graph of Figure 3b is shown in Figure 4a.
a-b

c-d

�
a-b--e-d

��/

(a)

(c)

(b)

Figure 4: (a) Junction tree corresponding to Figure 3b.
(b) Removal of c- d causes clique {a, c, d, e} to dis­
appear. (c) The fill-in a - c is rendered redundant,
splitting clique {a, b, c, e} into two smaller ones.
induce several new independence statements (c ll d,
d ll e, a ll dIe, etc.) which do not follow as natural
consequences of c ll d I (a, e). The set of independence
statements displayed by each chain graph of Figure 5
is a subset of I*
I U { c ll d I (a, e)}. This follows
from the fact that the three moral graphs are identical
to the moral graph of Figure 3b with c-d removed.
Thus, each graph of Figure 5 is a correct representation
of I*, but none of the graphs are perfect representa­
tions, since they fail to represent e.g. the statements
d ll e I c and a ll e I b.
=

a

w �;7
d

(a)

"

d

(b)

(c)

Figure 5: Competing independence graphs obtained
by adding c Jl d I (a, e).

Figure 3: (a) Sample independence graph. (b) Corre­
sponding moral graph (solid links) and a triangulated
graph (all links).

Notice that the moral graph corresponding to the
graphs of Figure 5 is triangulated. This eliminates
the need for the fill-in between a and c, allowing
clique {a, b, c, e} to be split into the two smaller cliques
{a, b, e} and {b, c, e} (Figure 4c). In general, a possi­
bly large number of fill-ins and moral links might be
rendered redundant by the removal of a single link. If,
for example, b-+ e is removed in Figure 3a, the moral
link b-e disappears.

Reduction of the computational complexity of the
junction tree could be accomplished by extending the
set of conditional independence statements displayed
by the tree. Adding e.g. the statement c ll d I (a, e)
(i.e., removal of c- d from the triangulated graph)
causes clique {a, c, d, e} to split into the sets {a, d, e}
and {a, c, e} neither of which appear to be cliques of
the reduced graph (Figure 4b).

Enforcement of the conditional independence state­
ment c ll d I (a, e) thus provided a reduction of com­
plexity in terms of sizes of cliques from three 4-cliques
(i.e., cliques of four variables) to one 4-clique and two
3-cliques. This corresponds to at least a 37% reduction
of space requirements (binary variables) even though
the resulting independence graph(s) at first glance
seems more 'complicated'.

�J/
(a)

f

(b)

Since we wish to add just one statement to the set I
of independence statements displayed by the original
independence graph of Figure 3a, the revised indepen­
dence graph is, in general, not obtained through sim­
ple link removal. Removal of c-d in Figure 3a would

3.2

OUTLINE OF METHOD

The above example provided insight into some of the
issues related to the approximation method. Before

378

Kj�rulff

presenting the technicalities of the method, let us sum­
marize the underlying philosophy and list the issues to
be dealt with in more detail.

'1/Js = ¢s. That is, the potentials of the (possible) new
cliques are ¢C\{a} and cPC\{f3}• and the potentials of
the cliques in

W hen attempts to compile a Bayesian network into a
junction tree fails on account of excessive memory re­
quirements, the problems are often caused by a small
number of cliques. The proposed method is based on
the idea of splitting these cliques into smaller ones (i.e.,
extending the set of independence statements). There­
fore, the first step is to create a junction tree with exact
or simulated clique potentials. (Although exact clique
potentials can be created, there might still be a wish to
reduce the space requirements if this can be done with­
out attaining an unacceptable level of imprecision.)
Clique potentials (whether exact or simulated) must
be provided such that the deviation between these 'cor­
rect' potentials and the approximate ones can be com­
puted. These measures of deviation (or distance) must
then be used as the basis of a criterion for selecting the
link to be removed.
Simulated clique potentials can be provided through
various kinds of Monte-Carlo simulation like Gibbs
sampling and 'forward sampling' which have complex­
ities proportional to the moral graph. We shall not
discuss this issue any further, even though there are
some interesting points concerning optimal choice of
simulation method, especially when the underlying in­
dependence graph is not a DAG.

3.4

Let

C \ {C}

remain unaltered.

JUNCTION TREE

cl

I

• • •

I

ck

be the neighbours of c in a junc­

tion tree T = (C, S) and S1 , ... , S1c the associated
separators, where C is the unique clique containing
{a, f)}. As demonstrated in F igure 2, the removal of
the link between a and f) produces two, one, or zero
new cliques. That is, (a) both Get = C \ {f)} and
Cf3 = C \ {a} are cliques in the revised junction tree
T', (b) Co: (Cf3) is a clique in T' and c13 (G.:,) is not,
or (c) neither Co nor C13 are cliques in T'. It is easy
to see that T' is constructed from T as indicated in
Figure 6, where the dashed parts illustrate the cliques,
separators, and links to be added toT (with C and its
incident links removed) and the dotted parts the sep­
arators and links to be removed; see Kj<Erulff

(1993)

for details.
Note that in all three cases we have S = C \
meaning that S separates T' into two subtrees

{a, f)}
T� =

(C�,S�) and T� = (Ck,Sk), where A and B are the
corresponding sets of variables such that a E A and
t3 E B. From the discussion in Section 3.3 it follows
trivially that

A Bayesian network with underlying probability model

p may be exhaustively described in terms of four com­

ponents: (1) a potential representation of p based on
component potentials (cf. Equation (1)), (2) an inde­
pendence graph, 9, of p, (3) a junction tree (decom­
posable hypergraph cover of gm), and (4) a poten­
tial representation of p based on belief potentials (cf.

Equation (5)). Notice that it suffices to include one
of the potential representations for an exhaustive de­
scription of a Bayesian network. We shall, however,
include them both as a matter of convenience.
We shall now detail the impacts on these four compo­
nents when removing a"'/3 from the moral graph.
3.3

Let T = (C, S) be a junction tree, C E C the unique
clique containing {a, /3}, and¢ a consistent joint belief
for T. Let further

L

c
<> <Pc L/3 ¢
L<>,f3 ¢c

with respect to

'1/J.

{:}

Since

L <Pc = L'I/Jc,
-r

a Jl /31 C \ {a, /3}

-r

{a,/3},

and Cis the unique clique containing {a, t3} it follows
that for each separator, S, between C and its neigh­
bours in T either S � C \ {a} or S � C \ {/3} implying

¢>Bus. Therefore,

=

cPAuScPBuS
cPs

(6)
·

The reduction, 0' ( o:, f)), of the computational complex­
ity achieved by the removal of a-f) can be expressed
as (a) IICII-CliO., II+ IIC11II + II SII) , (b) IICII-(IIColl+
II SII)+II Skll, or (c) IICII-II SII+liSt II+II S�cll, where
II II = ISp(·)l; cf. Figure 6. This can be expressed
·

compactly

as

=

where 1-r

=

(1- lall/311-l- 11311all-1) -IIS II +
(7)
(1- lo) II Stll + (1- lfj)II Skll,
IICII

1 ('Y

E

{a, /3})

if

C-r

is a clique and

0

otherwise. Note that -IISII :::; 0':::; IICII-II SII + II S 1II +
II S�cll, where 0' reaches its lower bound when llall =
11/311 = 2 and 1o: = 1{3 =
and its upper bound when

lex

=

3.5
'Y E

=

.!.
'+'

O'(o:,/3)

BELIEF POTENTIALS

'1/Jc =

and similarly 'If;Bu S

!13

=

1,

0.

INDEPENDENCE GRAPH

Since 'If;A = 'If;A us
¢Aus, the independence relations
among the variables of the set A remain unaltered by
=

the removal of o:- /3, where A, B, S, ¢, and 'If; are
given in Section 3.4. The same applies to B. That is,
the marginal independence graphs for A, B and S are

Link Removal in Bayesian Networks

379

(b)

(a)

Figure 6: Removal of the link between a and j3 results in a junction tree with a new separator S == C \ {a, /3}
separating the tree into a subtree containing a but not j3 and a subtree containing j3 but not a. In parts (b)
and (c) we assume, without loss of generality, that Ca C Ct (part (c) only) and C13 C Ck (i.e., Ca
S1 and
cf3 = sk)·
=

identical for ¢ and 1/J. Therefore, the problem of de­
termining the independence graph of 'ljJ may be formu­
lated as the problem of combining marginal indepen­
dence graphs such that the independence statements
expressed by these are not violated and such that the
combined graph represents the fact that A Jl B I S (or
A \ S Jl B \ S I S to be exact).
Given an independence graph of a probability function
(belief potential), p = pv, the following theorem pro­
vides a way of establishing an independence graph of
any marginal PA, A � V.

E)

Let the chain graph Q = (V,
be an
independence graph of p = pv and a E V. Then
is an independence graph
gt\{a} = ( V \{a},

Theorem 2

Eh{a})

ofPV\{a} = LaPv, where Qt\{a} is constructed from

g by rendering nb(a) complete by adding undirected

links if necessary, adding /3-+ 1 for each /3 E pa(a)
and 1 E nb(a) U ch( a), unless /3 '""1, adding /3-+ 1 for
each /3 E nb(a) and 1 E ch(a) , unless /3 ""'1, rendering
ch(a) complete in such a way that no directed paths
are introduced, and removing a and the links incident
t o it.

In proving Theorem 2, it is profitable to note that
correctness of Q' = 9t\{a} follows if separation of A
and B by C in (Q�n(AuBuC))m implies separation in
<fhn(AuBuC))m as well, and that perfectness of 9t\{a}
follows if separation in (9An(AuBuc))m implies separa­
tion in (Q�n(AuBuC))m provided Q is perfect.
It should be noticed that perfectness of Q does not

gt\{a}.

necessarily imply perfectness of
The following
example illustrates this point. Let V = {a, /3, '")', 6, c:}
and let the DAG of Figure 7a be an independence
graph of p. Since j3 -¥- 6 I{!, c:} with respect to p
(and PV\ {a}), j3 and 6 must be connected in an inde­
pendence graph of PV\{a} = Lap, and, since 1 Jl c:
and 1 -¥- e I /3, a candidate independence graph of
PV\{a} could be the one of Figure 7b. However, since
1 Jl e IS w ith respect top (and PV\{"'}), this graph is
not perfect, but it is correct, since it does not repre­
sent non-existing independence statements. Thus, all

marginalize
w.r.t.

a

(a )

(b)

Figure 7: 1 Jl e 16 with respect to top (and LaP)
which is Markov with respect to the DAG in part (a).
However, La p is not Markov with respect to the
graph in part (b), since according to that 1-¥- e 16.
the independence properties of PV\{a} cannot be rep­
resented by a single chain graph. If we want a perfect
representation, a more sophisticated language must be
adopted. One such language may be given by the class
of annotated graphs (Geva & Paz 1992). However, in
the present paper we shall refrain from pursuing this
any further.
Theorem 2 provides a method for constructing an in­
dependence graph of the marginal distribution PV \{a}·
However, the construction of an independence graph of
the approximate joint belief 'ljJ = ¢Aus¢B 1 s involves
combination of a marginal independence graph and a
conditional (marginal) independence graph.1 The in­
dependence graph of the conditional distribution Pv 1 a
is obtained simply by moralizing the subgraph induced
by An(a) and removing a and the links incident to it.
Let the chain graph Q = (V, E) be an in­
dependence graph of p = Pv and let
be the links
of t9An(A))m. Then Q' = (V, E U EA.';,( A)) is a chain
graph and a conditional independence graph of Pv 1 A.

Theorem 3

EAn(A)

By the methods of Theorem 2 and Theorem 3 we can
construct any marginal independence graph (possibly
conditional on a set of variables) by successive removal
of the relevant variables.
Note that the presence of the set A and the links
incident to A in the independence graph of Pv 1 A is
unnecessary for a correct interpretation of the condi­
tional independence relations among variables in V \A
1 For brevity we shall refer to an independence graph of
a marginal distribution as a marginal independence graph,

and similarly for the conditional case.

380

Kjrerulff

given A. However, when combining a conditional and
a marginal independence graph to obtain a joint in­
dependence graph, A and some links connecting A to
V \ A are needed. In fact, when constructing a condi­
tional independence graph we shall proceed as follows.
Let p and 9' be given as in Theorem 3.
The graph obtained by (i) removing all links between
nodes in S and (ii} making all links between S and
nb( S) undirected is a conditional independence graph
of Pv I A·

a-b

Ei)

=

E�)

=

,

Returning to the example in Section 3.1, we iden­
tify the sets A = {a, b, c,e}, B = {a,d,e, !}, and
S = {a,e}. Following the above results we deter­
mine a marginal independence graph and a conditional
one, and then combine these into a new joint inde­
pendence graph. This combination can involve one of
three principally different sets of marginal and con­
ditional graphs: (1) marginal graph for AU S plus
conditional one for B IS, (2) marginal graph forB US
plus conditional one for A IS, or (3) conditional graphs
for A I S and B I S plus marginal one for S, reflecting
the factorizations 7/J = rPAuSrPB 1 s, 7/J = rPA 1 srPBus,
and 7/J
¢A 1 s¢B 1 st/Js, respectively. The relevant
marginal and conditional graphs are a-+e for S and
the ones of Figure 8. Forming the independence graph
of 7/J through graph union, we find the three possi­
ble solutions displayed in Figure 5a-c corresponding,
respectively, to combination alternatives (1)-(3) with
the modifications that a-d (solutions (a) and (c))
and a-b (solutions (b) and (c)) have been replaced
with a-+ d and a-+b to avoid directed cycles. (Note
that these modifications do not alter the represented
independence statements.) Since we shall prefer so­
lutions representing the largest sets of independence
statements, there is a clear preference order among
the three alternatives (solution (a) is preferable to so­
lution (b) which is preferable to solution (c)).

a

AUS
a

Theorem 4 below states that a joint independence
graph can be formed by simple graph union of a con­
ditional and a marginal independence graph.
Let the chain graph 9ius
(A,
be a marginal independence graph of PAus and the
chain graph 9t 1 5 = (B,
a conditional indepen­
dence graph of pB 1 s complying with Corollary 1, where
AUBUS = V such that AnB =Sand All B IS with
respect to p pv. If 9' = 9ius u 9t 1 5 is not a chain
graph (i.e., it contains directed cycle(s)}, replace links
'Y-6 with 1-+6, where 1 E Sand 6 E nb(S) n B until
9' becomes a chain graph. Then 9' is an independence
graph of p. Further, 9' is perfect if both 9ius and
9"1 1 5 are perfect.

c

d

��7
f

Corollary 1

Theorem 4

"-...e./

BUS
d

�r/

a-b -- c

""/
e

f

BIS

AIS

Figure 8: Marginal and conditional independence
graphs of the graph of Figure 3a with A = {a, b, c,e},
B = {a, d, e,!}, and S = {a, e}.
3.6

COMPONENT POTENTIALS

Given a joint belief, 7/J, and a chain graph, 9, obtained
through enforcement of one or more conditional inde­
pendence assumptions, we wish to determine an asso­
ciated set of component potentials. Furthermore, we
have available a set of belief potentials associated with
a junction tree corresponding to 9.
Notice that if 7/J and 9 are produced as described in
Sections 3.3-3.5, 7/J is guaranteed to factorize according
to 9. That is, there exist component potentials �A such
that 7/J ex nA �A (cf. Equation (1)).
Following Equation (3) the problem can be divided
into n subproblems, where n is the number of chain
components of 9.
More specifically, since 'ljJ =
n 7/J(K I pa(K)), we must determine potentials �A for
each chain component K such that

=

A similar analysis can be performed for the dyspnoea
example in the Introduction. Again there appears to
be a clear preference order among the solutions, with
the optimal solution displayed in Figure 1b.

(cf. Equation (4)), where K+ = K U pa(K) and AK
is the set of cliques in (9K+ ) m containing at least one
node in K. Notice that, since belief potentials are
available, 7/J K+ can be computed.
The potentials �A can be found via Mobius inversion
when 7/JK+ is positive; see e.g. Lauritzen & Wermuth
(1989). Unfortunately, this is rarely the case. How­
ever, it seems plausible that an extended version of
the Mobius inversion exists when 7/JK+ is known to
factorize according to 9.
Lauritzen & Wermuth (1984) has shown that for any
decomposable graphical chain model there exists an
equivalent recursive model; that is, if (9K+ )m is tri­
angulated for each K. Thus, if 9 is decompos­
able, we may generate the equivalent DAG and com­
pute conditional probabilities (component potentials)
7/J (v I pa(v)) (cf. Equation (2)). If 9 is not decompos­
able, we may triangulate each subgraph (9K+ ) m by

Link Removal in Bayesian Networks

inserting fill-ins and then generate a DAG, 9*, from
the resulting graph. In the latter case the resulting
recursive model will be suboptimal in two ways. First,
9* fails to represent all the independence statements
represented by 9. Second, the computational complex­
ity imposed by the optimal triangulation of (9*)m is
at least as large as the computational complexity im­
posed by the optimal triangulation of gm, since the
triangulation of each (9K+ ) m 'constrains' the triangu­
lation.
4

EXPERIMENTS

Since, from a theoretical point of view, not much can
said about the practical importance of link removal,
we shall now report on some results of an empirical
study conducted on a number of real-world networks.
The networks are Pathfinder (Heckerman, Horvitz &
Nathwani 1992) (including 109 nodes) for diagnosing
lymph node pathology, two subnetworks of MUNIN
(Andreassen, Woldbye, Falck & Andersen 1987) (in­
cluding about 190 nodes each) for diagnosing disor­
ders in the peripheral nervous system, and a time­
sliced network model of the biological processes of a
water treatment plant including 32 process variables
(Jensen, Kjcerulff, Olesen & Pedersen 1989).
The criterion applied for selecting a link a""' (3 to be re­
moved from a clique C is based on the reduction of the
total state space and the 'distance' between the exact,
¢c , and the approximate, 1/Jc , clique potentials. The
distance, D(¢c,1/Jc ), is measured as the conditional
mutual information between a and (3 given C \ {a, (3}
(also: the Kullback-Leibler divergence between ¢c and
1/Jc) given as
I ( a,(JI C \ {a,(J}) = D(¢c ,'l/Jc)

=

Elog(¢c Nc )

with expectation taken with respect to ¢, and where
'1/Jc = 0. A useful relationship
= 0 when ¢c
between the absolute divergence and the Kullback­
Leibler divergence (see e.g. Kullback (1967)) states
that
(8)
I¢A- '1/JAI :S j�D(¢c,'I/Jc )
I

=

for any A � C.
In the experiments links with lower mutual informa­
tion were preferred, and savings (reduction of state
space) were used only to break ties. Further, links
were removed until a total divergence of at most 0.001
was reached (the total divergence after a series of re­
movals equals the sum of the individual divergences
(Kjcerulff 1993)). Using Inequality (8) the theoreti­
cally upper bound on the absolute error is found to
be 2%.
Table 1 displays the results. The 'size' of a net­
work equals the sum of the sizes of the state spaces
of the cliques after sensible triangulation. For all
networks except MUNIN2 the Kullback-Leibler diver­
gences were computed using exact clique potentials.

381

For the MUNIN2 network simulated potentials based
on 10, 000 iterations of forward sampling were used.
Links removed Reduction
Network (size)
Pathfinder (187, 244)
36.4%
26
MUNIN1 (2, 302, 119)
145
34.3%
MUNIN2 (183, 549, 219)
190
96.0%
Water (9, 443, 571)
97.2%
126
Table 1: Empirical results of applying link removal to
real-world networks.
The savings obtained for the Pathfinder and the
MUNINl networks are relatively modest, whereas sig­
nificant savings are obtained for the MUNIN2 and the
Water networks. The reduction from 183.5 M to 7.3 M
for the MUNIN2 network makes it possible to perform
exact computations using the junction-tree methodol­
ogy. The large savings for the MUNIN2 and the Water
networks are due partly to the fact that a number of
the orphan nodes are instantiated to their 'normal'
state.
5

DISCUSSION

An important feature of a clique-potential approxi­
mation is the attenuation of its impact with increas­
ing distance from the target clique (Kjcerulff 1993).
This feature is especially important in connection with
time-sliced Bayesian networks. An additional property
of the method, is the property of errors remaining lo­
calized in absence of posterior evidence and, under cer­
tain conditions, even in presence of posterior evidence
(Kjcerulff 1993).
The presented approximation method has been com­
pared with the method suggested by Jensen & Ander­
sen (1990). Briefly, their method is based on anni­
hilation of small probabilities by setting the k small­
est probabilities to zero for each clique potential of a
junction tree, where k is chosen such that the sum
of the k smallest probabilities is less than a predeter­
mined threshold. After annihilation, the belief tables
are compressed in order to take advantage of the in­
troduced zeros.
The comparison (reported in Kjcerulff (1993)) demon­
strates that link removal in some cases is significantly
better than annihilation. In other cases, however, a
comparison turns out to the disadvantage of link re­
moval. Intuitively, this seems absolutely reasonable,
since a model including links representing weak de­
pendences will be almost equivalent to a model which
lacks these links, but it might be quite different from a
model obtained by uniformly removing a correspond­
ing amount of probability mass from the belief tables.
On the other hand, link removal is unsuited in cases
where there are no 'weak links'. Thus, to approxi­
mate a given network using these two methods, link
removal should be tried first and when all 'weak links'
have been removed, annihilation should take over.

382

Kja!rulff

Application of link removal does not require the con­
struction of exact clique potentials (as opposed to an­
nihilation). Further, the creation of simulated clique
potentials (through e.g. forward sampling) and possi­
ble subsequent link removal provides a way of estab­
lishing an annihilated and compressed junction tree
representation of a network without first creating ex­
act potentials.
Inequality (8) is essential, since the key indicator asso­
ciated with an approximation is most often the max­
imum absolute error. However, under arrival of pos­
terior evidence, the inequality can only be used as a
rough guideline. Thus, among directions for future re­
search, an important one is assessment of a good upper
bound on the error given evidence.
Acknowledgements

I am indebted to Steffen L. Lauritzen for providing
many valuable comments on earlier drafts. The re­
search has been funded partly by the Danish Research
Councils through the PIFT programme.

