
Most causal discovery algorithms in the literature exploit an assumption usually referred to as the Causal Faithfulness or Stability Condition. In this paper, we highlight two components of the condition used
in constraint-based algorithms, which we call
“Adjacency-Faithfulness” and “OrientationFaithfulness.” We point out that assuming Adjacency-Faithfulness is true, it is possible to test the validity of OrientationFaithfulness. Motivated by this observation, we explore the consequence of making
only the Adjacency-Faithfulness assumption.
We show that the familiar PC algorithm
has to be modified to be correct under the
weaker, Adjacency-Faithfulness assumption.
The modified algorithm, called Conservative PC (CPC), checks whether OrientationFaithfulness holds in the orientation phase,
and if not, avoids drawing certain causal conclusions the PC algorithm would draw. However, if the stronger, standard causal Faithfulness condition actually obtains, the CPC
algorithm outputs the same pattern as the
PC algorithm does in the large sample limit.
We also present a simulation study showing that the CPC algorithm runs almost as
fast as the PC algorithm, and outputs significantly fewer false causal arrowheads than the
PC algorithm does on realistic sample sizes.

1

MOTIVATION: FAITHFULNESS
DECOMPOSED

Directed acyclic graphs (DAGs) can be interpreted
both probabilistically and causally. Under the causal
interpretation, a DAG G represents a causal structure
such that A is a direct cause of B just in case there

Jiji Zhang
Division of Humanities and Social Sciences
California Institute of Technology
Pasadena, CA 91125
jiji@hss.caltech.edu

is a directed edge from A to B in G. Under the probabilistic interpretation, a DAG G, also referred to as
a Bayesian network, represents a probability distribution P that satisfies the Markov Property: each variable in G is independent of its non-descendants conditional on its parents. The Causal Markov Condition
is a bridge principle linking the causal interpretation
of a DAG to the probabilistic interpretation.1
Causal Markov Condition: Given a set of variables
whose causal structure can be represented by a DAG
G, every variable is probabilistically independent of its
non-effects (non-descendants in G) conditional on its
direct causes (parents in G).
The assumption that the causal structure can be represented by a DAG entails that there is no causal feedback, and that no common cause of any pair of variables in the DAG is left out. All DAG-based causal
discovery algorithms assume the causal Markov condition, and most of them (e.g., those discussed in Pearl
2000, Spirtes et al. 2000, Heckerman et al. 1999)
also assume, if only implicitly, the converse principle,
known as the Causal Faithfulness or Stability Condition:
Causal Faithfulness Condition: Given a set of variables whose causal structure can be represented by a
DAG, no conditional independence holds unless entailed by the Causal Markov Condition.
Conditional independence relations entailed by the
Markov condition are captured exactly by a graphical criterion called d-separation (Neapolitan 2004),
defined as follows. Given a path p in a DAG, a nonendpoint vertex V on p is called a collider if the two
edges incident to V on p are both into V (→ V ←),
otherwise V is called a non-collider on p.
Definition 1 (d-separation). In a DAG, a path p
between vertices A and B is active (d-connecting)
relative to a set of vertices C (A, B ∈
/ C) if
1

For a more formal presentation of the notions mentioned in this section, see Spirtes et al. (2000).

i. every non-collider on p is not a member of C;
ii. every collider on p is an ancestor of some member
of C.
Two sets of variables A and B are said to be dseparated by C if there is no active path between any
member of A and any member of B relative to C.
A well-known important result is that for any three
disjoint sets of variables A, B and C in a DAG G,
A and B are entailed (by the Markov condition) to
be independent conditional on C if and only if they
are d-separated by C in G. So the causal Faithfulness
condition can be rephrased as saying that for every
three disjoint sets of variables A, B and C, if A and
B are not d-separated by C in the causal DAG, then
A and B are not independent conditional on C.
Two simple facts about d-separation are particularly
relevant to our purpose (see e.g. Neapolitan 2004, pp.
89 for proofs):
Proposition 1. Two variables are adjacent in a DAG
if and only if they are not d-separated by any subset of
other variables in the DAG.
Call a triple of variables hX, Y, Zi in a DAG an unshielded triple if X and Z are both adjacent to Y but
are not adjacent to each other.
Proposition 2. In a DAG, any unshielded triple
hX, Y, Zi is a collider if and only if all sets that dseparate X from Z do not contain Y ; it is a noncollider if and only if all sets that d-separate X from
Z contain Y .
Below we focus on two implications of the Causal
Faithfulness Condition, easily derivable given Propositions 1 and 2. We call them Adjacency-Faithfulness
and Orientation-Faithfulness, respectively.
Implication 1 (Adjacency-Faithfulness). Given a
set of variables V whose causal structure can be represented by a DAG G, if two variables X, Y are adjacent
in G, then they are dependent conditional on any subset of V\{X, Y }.

resented by a DAG G, let hX, Y, Zi be any unshielded
triple in G.
(O1) if X → Y ← Z, then X and Z are dependent
given any subset of V\{X, Z} that contains Y ;
(O2) otherwise, X and Z are dependent conditional on
any subset of V\{X, Z} that does not contain Y .
Orientation-Faithfulness obviously serves to justify the
step of identifying unshielded colliders (and unshielded
non-colliders). For any unshielded triple hX, Y, Zi resulting from the adjacency step, a conditioning set
that renders X and Z independent must have been
found. The Orientation-Faithfulness condition then
implies that the triple is an unshielded collider if
and only if the conditioning set does not contain
Y . This is in fact what the familiar PC algorithm
checks. The rest of our paper is motivated by the following simple observation: assuming the AdjacencyFaithfulness condition is true, we can in principle test
whether Orientation-Faithfulness fails of a particular
unshielded triple. Suppose we have a perfect oracle of
conditional independence relations, which is in principle available for many parametric families in the large
sample limit by performing statistical tests. Since the
Adjacency-Faithfulness is by assumption true, out of
the oracle one can construct correct adjacencies and
non-adjacencies, and thus correct unshielded triples in
the causal graph. For such an unshielded triple, say,
hX, Y, Zi, if there is a subset of V\{X, Z} containing
Y that renders X and Z independent and a subset
not containing Y that renders X and Z independent,
then Orientation-Faithfulness fails on this triple. This
failing condition can of course be verified by the oracle.
Note that this simple test of Orientation-Faithfulness
does not rely on knowing what the true causal DAG
is. The reason why this test works is that a distribution that satisfies the Adjacency-Faithfulness with
respect to the true causal DAG but fails the above test
is not Orientation-Faithful to any DAG, and hence not
Orientation-Faithful to the true causal DAG.

We call this condition Adjacency-Faithfulness for the
obvious reason that this is the part of the Faithfulness
condition that is used to justify the step of recovering adjacencies in constraint-based algorithms. Generically, this step proceeds by searching for a conditioning set that renders two variables independent, and by
the causal Markov and Adjacency-Faithfulness conditions, the two variables are not adjacent if and only if
such a conditioning set is found.

This suggests that theoretically we can relax the standard causal Faithfulness assumption and still have
provably correct and informative causal discovery procedures. In fact, one main result we will establish in
this paper is that the PC algorithm, though incorrect
under the weaker, Adjacency-Faithfulness condition,
can be revised in such a way that the modified version – that we call CPC (conservative PC) – is correct
given the Adjacency-Faithfulness condition, and is as
informative as the standard PC algorithm if the Causal
Faithfulness Condition actually obtains.

Implication 2 (Orientation-Faithfulness). Given
a set of variables V whose causal structure can be rep-

In addition to the theoretical demonstration, we will
present a simulation study comparing the CPC algo-

rithm and the PC algorithm. The results show that
the CPC algorithm runs almost as fast as the PC algorithm, which is known for its computational feasibility. More importantly, even when the standard
Causal Faithfulness Condition holds, the CPC algorithm turns out to be more accurate on realistic sample
sizes than the PC algorithm in that it outputs significantly fewer false causal arrowheads and (almost) as
many true causal arrowheads.

2

HOW PC ALGORITHM ERRS

Before we present our modification of the PC algorithm, it is helpful to explain how the PC algorithm can make mistakes under the causal Markov and
Adjacency-Faithfulness conditions. The relevant details of the PC algorithm are reproduced below, where
we use ADJ(G, X) to denote the set of nodes adjacent
to X in a graph G:
PC Algorithm
S1 Form the complete undirected graph U on the set
of variables V;
S2 n = 0
repeat
For each pair of variables X and Y that
are adjacent in (the current) U such that
ADJ(U, X)\{Y } or ADJ(U, Y )\{X} has at
least n elements, check through the subsets of ADJ(U, X)\{Y } and the subsets of
ADJ(U, Y )\{X} that have exactly n variables. If a subset S is found conditional on
which X and Y are independent, remove the
edge between X and Y in U , and record S as
Sepset(X, Y );
n = n + 1;
until for each ordered pair of adjacent variables X
and Y , ADJ(U, X)\{Y } has less than n elements.
S3 Let P be the graph resulting from step S2. For
each unshielded triple hA, B, Ci in P , orient it as
A → B ← C iff B is not in Sepset(A, C).
S4 Execute the orientation rules given in Meek 1995.2
If the input to the PC algorithm is a sample from a
population distribution that is faithful to some DAG,
then in the large sample limit, the output of the PC
algorithm can be interpreted as a set of DAGs, all of
2
Details of the Meek orientation rules do not matter for
the purposes of this paper. The rules are also described in
Neapolitan 2004, pp. 542.

which are d-separation equivalent (that is, they imply exactly the same d-separation relations). The dseparation equivalence class of DAGs output by the
PC algorithm (and the score-based GES algorithm as
well) is represented by a graphical object called a pattern, or a PDAG (Chickering 2002). A pattern is a
mixture of directed and undirected edges. A DAG is
represented by a pattern if it contains the same adjacencies as the pattern, every directed edge A → B in
the pattern is oriented as A → B in the DAG, and if
the DAG contains an unshielded collider, then so does
the pattern. The output of the PC algorithm is correct
given the Causal Markov and Faithfulness Conditions
and a perfect conditional independence oracle (such as
statistical tests in the large sample limit) in the sense
that the true causal DAG is among the DAGs represented by the output pattern. The output of the PC
algorithm is complete in the sense that if an edge A
→ B occurs in every DAG in the d-separation equivalence class represented by the output pattern, then it
is oriented as A → B in the output pattern. (Meek
1995, Spirtes et al. 2000).
Two specific features of PC are worth noting. First, in
S2, the adjacency step, the PC algorithm essentially
searches for a conditioning set for each pair of variables
that renders them independent, which we henceforth
call a screen-off conditioning set. But it does this with
two additional tricks: (1) it starts with the conditioning set of size 0 (i.e., the empty set) and gradually
increases the size of the conditioning set; and (2) it
confines the search of a screen-off conditioning set for
two variables to within the potential parents – i.e., the
currently adjacent nodes – of the two variables, and
thus systematically narrows down the space of possible screen-off sets as the search goes on. These two
tricks increase both computational and statistical efficiency in most real cases, and we will keep this step
intact in our modification.
Secondly, in S3 the PC algorithm uses a very simple
rule to identify unshielded colliders or non-colliders.
For any unshielded triple hX, Y, Zi, it simply checks
whether or not Y is contained in the screen-off set
for X and Z found in the adjacency stage. Now
if we assume the causal Markov and AdjacencyFaithfulness conditions are true, the adjacencies (and
non-adjacencies) resulting from the adjacency stage
are asymptotically correct.
However, these two
conditions do not imply the truth of OrientationFaithfulness, and when the latter fails, the PC algorithm will err even in the large sample limit.
Consider the simplest example A → B → C where
A⊥
⊥C and A⊥
⊥C|B. This is the case when, for example, causation fails to be transitive, an issue of great
interest to philosophers of causality. In this situation

the causal Markov and Adjacency-Faithfulness conditions are both satisfied, but Orientation-Faithfulness
is not true of the triple hA, B, Ci. Now, given the
correct conditional independence oracle, the PC algorithm would remove the edge between A and C in
S2 because A⊥
⊥C, and later in S3 orient the triple as
A → B ← C because B is not in the screen-off set
found in S2, i.e., the empty set. Simple as it is, the
example suffices to establish that the PC algorithm is
not asymptotically correct3 under the causal Markov
and Adjacency-Faithfulness assumptions.

3

CONSERVATIVE PC

It is not hard, however, to modify the PC algorithm to
retain correctness under the weaker assumption. Indeed a predecessor of the PC algorithm, called the
SGS algorithm (Spirtes et al. 2000), is almost correct.
The SGS algorithm decides whether an unshielded
triple hX, Y, Zi is a collider or a non-collider by literally checking whether (O1) or (O2) in the statement
of Orientation-Faithfulness is true. Theoretically all
it lacks is a clause that acknowledges the failure of
Orientation-Faithfulness when neither (O1) nor (O2)
passes the check. Practically, however, the SGS algorithm is a terribly inefficient algorithm. Computationally, it is best case exponential because it has to
check dependence between X and Z conditional on
every subset of V\{X, Z}. Statistically, tests of independence conditional on large sets of variables have
very low power, and are likely to lead to errors. In addition,the sheer number of conditional independence
tests makes it exceedingly likely that some of them
will err, and we suspect that almost every unshielded
triple will be marked as unfaithful if we run the SGS
algorithms on more than a few variables.
Fortunately, the main idea of the PC algorithm comes
to the rescue. A correct algorithm does not have
to check every subset of V\{X, Z} in order to test
whether hX, Y, Zi is a collider, a non-collider, or an
unfaithful triple. It only needs to check subsets of the
variables that are potential parents of X and Z. This
trick, as we shall show shortly, is theoretically valid,
and turns out to work well in simulations.
The CPC algorithm replaces S3 in PC with the following S3’, and otherwise remains the same.
S3’ Let P be the graph resulting from step 1. For each
unshielded triple hA, B, Ci, check all subsets of
A’s potential parents and of C’s potential parents:
3

By ”asymptotically correct” we mean the probability
of the output containing an error converges to zero in the
large sample limit, no matter what the true probability
distribution is.

(a) If B is NOT in any such set conditional on
which A and C are independent, orient A −
B −−C as A → B ← C;
(b) if B is in all such sets conditional on which
A and C are independent, leave A −−B −−C
as it is, i.e., a non-collider;
(c) otherwise, mark the triple as “unfaithful” by
underlining the triple, A −−B−− C.
(In S4, The orientation rules that are applied to unshielded non-colliders in the PC algorithm are, of
course, applied only to unshielded non-colliders in the
CPC algorithm; in particular they are not applied to
triples that are marked as unfaithful.)
The output of the CPC algorithm can also be interpreted as a set of DAGs. If the input to the CPC
algorithm is a sample from a distribution that satisfies
the Markov and Adjacency-Faithfulness Assumptions,
in the large sample limit, the output is an extended
pattern, or e-pattern for short. An e-pattern contains
a mixture of undirected and directed edges, as well
as underlinings for unshielded triples that are unfaithful. A DAG is represented by an e-pattern if it has
the same adjacencies as the e-pattern, every directed
edge A → B in the e-pattern is oriented as A → B in
the DAG, and every unshielded collider in the DAG
is either an unshielded collider or a marked unfaithful
triple in the e-pattern. These rules allow that an unfaithful triple in the e-pattern can be oriented as either
a collider or a non-collider in a DAG represented by
the e-pattern.
The set of DAGs represented by an e-pattern may
not be d-separation equivalent, if the e-pattern contains an unfaithful triple. For example, if A causes
B, and B causes C, but the causation is not transitive (i.e. I(A, C|B) and I(A, C)), the resulting epattern is A−
−B−−C, because it is an unfaithful triple.
The set of DAGs represented by A −−B−− C contains
A → B → C, A ← B → C, A ← B ← C, and
A → B ← C. The latter DAG is not d-separation
equivalent to the first three DAGs. Note that in this
case the true distribution lies in the intersection of
sets of distributions represented by non- d-separation
equivalent DAGs. The intersection would be ruled out
as impossible by the standard Faithfulness assumption.
At this point it should be clear why the modified PC
algorithm is labeled “conservative”: it is more cautious than the PC algorithm in drawing unambiguous
conclusions about causal orientations. A typical output of the CPC algorithm is shown in Figure 1. The
conservativeness is of course what is needed to make
the algorithm correct under the causal Markov and
Adjacency-Faithfulness assumptions.

Figure 1: A typical output for CPC. Underlining
(which in the figure looks like “crossing”) indicates unfaithful unshielded triples discovered by the algorithm.

6
4
2
0

Time (seconds)

Elapsed Time

0

20 40 60 80
Dimension

Theorem 1 (Correctness of CPC). Under the
causal Markov and Adjacency-Faithfulness assumptions, the CPC algorithm is correct in the sense that
given a perfect conditional independence oracle, the algorithm returns an e-pattern that represents the true
causal DAG.
Proof. Suppose the true causal graph is G, and all
conditional independence judgments are correct. The
Markov and Adjacency-Faithfulness assumptions imply that the undirected graph P resulting from step
S2 has the same adjacencies as G does (Spirtes et al.
2000). Now consider step S30 . If S30 (a) obtains, then
A → B ← C must be a subgraph of G, because otherwise by the Markov assumption, either A’s parents
or C’s parents d-separate A and C, which means that
there is a subset S of either A’s potential parents or
C’s potential parents containing B such that A⊥
⊥C|S,
contradicting the antecedent in S30 (a). If S30 (b) obtains, then A → B ← C cannot be a subgraph of
G (and hence the triple must be an unshielded noncollider), because otherwise by the Markov assumption, there must be a subset S of either A’s potential
parents or C’s potential parents not containing B such
that A⊥
⊥C|S, contradicting the antecedent in S30 (b).
So neither S30 (a) nor S30 (b) will introduce an orientation error. It follows that every unshielded collider in
G is either an unshielded collider or a marked triple
in P . Trivially S30 (c) does not produce an orientation error, and it has been proven (in e.g., Meek 1995)
that S4 will not produce any, which implies that every
directed edge in P is also in G.
The theorem entails that the output e-pattern (1) has
the same adjacencies as the true causal DAG; and (2)
all arrowheads and unshielded non-colliders in the epattern are also in the true causal DAG. Theorem 1,
together with the consistency of statistical tests of in-

Figure 2: Average elapsed time

dependence, entails that the probability of the output
containing an error approaches zero as the sample size
approaches infinity.
Note that a triple A→ B ←C or A −−B →C may occur in cases where the triple was initially marked as
unfaithful, but all later orientation rules provided further consistent orientation information. In those cases,
the underlining serves no purpose (as ambiguity concerning collider vs. non-collider is already dissolved)
and can be removed. The remaining triples marked
unfaithful by the CPC algorithm in the large sample
limit are truly ambiguous in that either a collider or
a non-collider is compatible with the conditional independence judgments. We conjecture but cannot yet
prove that the CPC algorithm is complete in the sense
that for every undirected edge in an e-pattern output
by the CPC algorithm, there is a DAG represented
by the e-pattern that orients the edge in one direction and another DAG represented by the e-pattern
that orients the edge in the other direction. Finally,
it is obvious that asymptotically the CPC algorithm
and the PC algorithm produce the same output if the
standard Faithfulness assumption actually obtains.

4

SIMULATION RESULTS

The theoretical superiority of the CPC algorithm over
the PC algorithm may not necessarily cash out in practice if the situations where the Adjacency-Faithfulness
but not the Orientation-Faithfulness holds do not arise
often. We will not try to make an argument to the
contrary here, even though we believe such an argu-

The simulations illustrate that the extra independence
checks invoked in the CPC algorithms do not render CPC significantly slower than PC and that CPC
is more accurate than PC in terms of arrow orientations. The explanation for the first point is that the
main computational expense of the PC algorithm occurs in the adjacency stage; the number of independence checks added in CPC for orientation is small by
comparison. The second point suggests that the PC
algorithm too often infers that unshielded triples are
colliders, and the CPC algorithm provides the right
antidote to this by means of the extra checks it performs. Again, we expect that the CPC algorithm will
do particularly better than the PC algorithm when the
distribution generated is close-to-unfaithful to the true
graph – a situation pointed out by several authors as
a major obstacle to reliable causal inference (Robins
et al. 2003, Zhang and Spirtes 2003).
To illustrate these points, the following simulations
were performed on linear Gaussian models, with variations for sparser and denser graphs, with dimensions
(numbers of variables) ranging from 5 to 100 variables.
For the sparser case, for each dimension d from 5 to 100
in increments of 5, five random graphs were selected
uniformly from the space of DAGs with at most d edges
and with a maximum degree of 10. For each such
graph, a random structural equation model was constructed by selecting edge coefficients randomly 0.95
of the time uniformly from [−1.5, −0.5] ∪ [0.5, 1.5] and
0.05 of the time uniformly from [−0.001, 0.001]. (Selection from the range [−0.001, 0.001] guarantees the
presence of weak edges, which in turn often lead to al4

Intuitively,
almost violations of OrientationFaithfulness refer to situations where two variables,
though entailed to be dependent conditional on some
variables by the Orientation-Faithfulness condition, are
close to conditionally independent. How to quantify the
“closeness” and just how close is close enough to cause
trouble depend on distributional assumptions and sample
sizes.

0

40

80 120

Arrows Added

Count

ment can be made. Instead we wish to show that
the CPC algorithm in practice performs better than
the PC algorithm, regardless of whether OrientationFaithfulness holds or not. That is, even when the data
are generated from a distribution Markov and Faithful
to the true causal graph, it pays to be conservative on
realistic sample sizes. One possible rationale for this
is that even though PC is correct in the large sample limit if Orientation-Faithfulness is not violated,
it is very liable to error on realistic sample sizes if
Orientation-Unfaithfulness is almost violated. Almost
violations of Orientation-Faithfulness can arise in several ways – for example, when a triple chain is almost non-transitive, or more generally, when one of
the edges in an unshielded triple is very weak4 .

0

20 40 60 80
Dimension

Figure 3: Average count of arrow false positives
most violations of faithfulness.) For each such model,
a random data set of 1000 cases was simulated, to
which PC and CPC were applied with significance level
α = 0.05 for each hypothesis test of conditional independence, tested using Fisher’s Z transformation of
partial correlation. The output was compared to the
pattern of the true DAG (the true pattern), not the
true DAG itself. Performance statistics were recorded,
including elapsed time and false positive and negative counts for arrows, unshielded colliders, unshielded
non-colliders, and adjacencies. For each number of
variables, each performance statistic was averaged over
the five random models constructed at that dimension,
for PC and for CPC. This procedure was repeated
for denser models with DAGs randomly selected uniformly from the set of DAGs with at most 2d edges
and a maximum degree of 10.
Counting orientation errors when there are differences
in adjacencies as well raises some subtle issues that
we have chosen to resolve in the following way. An
arrowhead removal error (false negative) occurs when
the true pattern P1 contains A → B, but the output
P2 either does not contain an edge between A and B
or does contain an edge between A and B but there
is no arrowhead on this edge at B. An analogous rule
is used to count arrowhead addition errors (false positive). This has the consequence that if A and B are
not adjacent in P1 , but A − B is in P2 , this is counted
as an adjacency addition error, but not an arrowhead
addition or removal error. In contrast, if A → B is in
P2 , this is counted as an adjacency addition error and
an arrowhead addition error, because of the arrowhead
at B. The justification for this is that the A − B er-

0

20 40 60 80

60
0 20

Count

40 80

Noncolliders Added

0

Count

Arrows Removed

0

Dimension

20 40 60 80
Dimension

Figure 4: Average count of arrow false negatives

Figure 5: Average count of non-collider false positive

ror leaves open whether there is an arrowhead at B,
and does not lead to any errors in predicting the effects of manipulations (the effects of manipulation are
unknown because the orientation of the edge is unknown). In contrast, the A → B error does definitively
state that there is an arrowhead at B, and does lead
to errors in predicting the effects of manipulations.

false negative unshielded non-colliders also matches
that of PC, as shown in Figures 5 and 6. In a word, in
no respect is PC noticeably better than CPC, whereas
CPC is significantly better than PC in avoiding false
positive causal arrowheads, the arguably most consequential type of errors.

There is an unshielded non-collider addition error for
the triple hX, Y, Zi if they form an unshielded noncollider in P2 , but P1 either has different adjacencies
among X, Y , and Z, or the same adjacencies but is a
collider. An unfaithful triple in G2 does not count as
an unshielded non-collider or collider addition error,
regardless of what is in G1 . Unshielded non-collider
removal errors are handled in an analogous fashion.
In all the figures, PC statistics are represented by triangles and CPC statistics are represented by circles;
sparser models use filled symbols, and denser models
used unfilled symbols. The horizontal axis is the number of variables in the true DAG.
Figure 2 shows that for both sparser and denser models, CPC is only slightly slower than PC.
Figure 3 shows that for both sparser and denser models, the number of extra arrows introduced is far better
controlled by CPC than by PC. For sparser models, the
number is particularly well-controlled. Figure 4 shows
that for both sparser and denser models, the number
of arrowhead removal errors committed by CPC is almost indistinguishable from the number of arrowhead
removal errors committed by PC.
The performance of CPC regarding false positive and

Figure 7 plots the percentage of unfaithful triples
among the total number of unshielded triples output
by CPC. For sparser models, the percentage of unfaithful triples is around 30 percent; for denser models, it
rises to around 40 percent. This confirms our expectation that CPC is more conservative the denser the
true graph.
Similar simulations were carried out parameterizing
random graphs using discrete Bayes nets with 2 to
4 categories per variable, but otherwise with identical setup to the sparser continuous simulations above,
with similar results.

5

CONCLUSION

The CPC algorithm we proposed in this paper is
provably correct under the causal Markov assumption
plus a weaker-than-standard Faithfulness assumption,
the Adjacency-Faithfulness assumption. It can be regarded as a conservative generalization of the PC algorithm in that it theoretically gives the same answer
as the PC does under the standard assumptions. Perhaps more importantly, simulation results suggest that
the CPC algorithm works much better than the PC algorithm in terms of avoiding false causal arrowheads,
and achieves this without costing significantly more

0

40

80

Percent Unfaithful Triples

Percent

400
200
0

Count

Noncolliders Removed

0

20 40 60 80

0

Dimension

20 40 60 80
Dimension

Figure 6: Average count of non-collider false negatives

Figure 7: Percent of unfaithful triples among all unshielded triples

in running time or missing positive information. We
do not claim that the evidence is conclusive, and we
think it would be interesting to compare CPC and PC
on real data sets.



The presence of latent variables can greatly
complicate inferences about causal relations
between measured variables from statistical data.
In many cases, the presence of latent variables
makes it impossible to determine for two
measured variables A and B, whether A causes
B, B causes A, or there is some common cause.
In this paper I present several theorems that
state conditions under which it is possible to
reliably infer the causal relation between two
measured variables, regardless of whether latent
variables are acting or not.

1

Introduction

The problem of inferring causal relations from statistical
data in the absence of experiments arises repeatedly in
many scientific disciplines, including sociology,
economics, epidemiology, and psychology. In addition,
the building of expert systems could be expedited if
background knowledge elicited from experts could be
supplemented with automated techniques. Recently,
efficient algorithms for determining causal structure (in
the form of Bayesian networks) from statistical data when
there are no unmeasured or "latent" variables have been
proposed. (See Spirtes, Glymour and Scheines 1990,
Spirtes and Glymour 199 1, Spines, Glymour, and
Scheines forthcoming, Verma and Pearl 1990, and Pearl
and Verma 199 1.)
Inferring causal relations when unmeasured variables are
also acting is a much more difficult problem. In many
cases it is impossible to infer the structure among the
latent variables from statistical relations among the
measured variables. But the presence of latent variables
can also make it difficult to infer the causal relations
among the measured variables themselves. One important
question for many policy decisions is "Does A cause B?"
Statistics about A and B alone do not suffice to answer
these questions. When only two variables, A and B, have
been measured, and there is a correlation between the two,
this does not suffice to establish whether A caused B, B
caused A, or there is a third variable causing both A and

B. Nevertheless, when more variables are measured, more
knowledge about the causal relations between A and B is
possible. Drawing upon recent results of Verma and Pearl
(Verma and Pearl 1990, and Pearl and Verma 1991), I will
prove in Theorem 2 that there are some circumstances in
which it is possible to establish that A caused B, rather
than that B caused A, or that a third variable caused both
A and B; I will prove in Theorem 3 that there are other
circumstances in which the possibility that A caused B
can be eliminated. The proofs are given in the Appendix.
I will also demonstrate that a recent proposal derived from
Pearl and Verma (1991) to establish more general
conditions for causal pathways in the presence of
unmeasured causes is incorrect.

2

Results

Causal processes between a set of random variables V are
represented by a directed acyclic graph over V, where there
is an edge from A to B if and only if A is an immediate
cause of B relative to V. (For a discussion of the
meaning of immediate causation, see Spirtes, Glymour
and Scheines 1991.) If there is a directed path from A to
B in the causal graph, I will say that A is a (possibly
indirect) cause of B.
If a distribution is placed over the exogenous variables in
a causal process (variables of zero indegree in the causal
graph), which in turn affect the values of other random
variables, the result is a joint distribution over all of the
random variables. In that case, I will say that the causal
process generated the joint distribution. Following Pearl
( 1988) I assume that the distribution generated by a causal
process satisfies the Minimality and Markov conditions
for the causal graph of that process.
In Pearl's
terminology (Pearl 1988) the causal graph is a Bayes
network of any distribution that it generates. (In what
follows, I will capitalize random variables, and boldface
any sets of variables.)

Markov Condition: Let Descendant(V) be the set of
descendants of V in a graph G, and Parents(V) be the set
of parents of V in G. A graph G and a probability
distribution P on the vertices V of G satisfy the Markov
condition if and only if for every V in V, and every subset

Detecting Causal Relations in the Presence

X of V, V and X (
\ V} u Descendants(V) are independent
conditional on Parents(V).
Minimality Condition: A graph G and a probability

distribution P satisfies the minimality condition if and
only if G and P satisfy the Markov condition and for every
graph H obtained by deleting an edge from G, H and P do
not satisfy the Markov condition.
If P satisfies the Markov condition for graph G, and every
conditional independence true of the distribution P is
entailed by the Markov condition, then we say that P is
faithful to G.
In a directed graph G, I will write X ->Y if there is an
edge from X toY in G. In an undirected graph U, I will
write X -Y if there is an undirected edge between X and
Y. X and Y are adjacent in a directed graph G if
and only if either X ->Y orY ->X in G. Two edges are
adjacent in an undirected graph U if and only if X Y in U. In a directed acyclic graph G, an undirected
path U from X to Y is a sequence of vertices starting
with X and ending with Y such that for every pair of
variables A and B that are adjacent to each other in the
path, A and B are adjacent in G, and no vertex occurs more
than once in U. In a directed acyclic graph G, a directed
path P from X to Y is a sequence of vertices starting
with X and ending with Y such that for every pair of
variables A and B that are adjacent to each other in the
path in that order, the edge A -> B occurs in G, and no
vertex occurs more than once in P. An edge between
X and Y occurs in a p ath P (directed or undirected) if
and only if X and Y are adjacent in P. If an undirected
path U contains an edge between X and Y, and an edge
between Y and Z, the two edges collide at Z if and only
if X ->Y and Z ->Y in G. On an undirected path U, Z is
an unshielded collider if and only if there exist edges
X ->Y and Z ->Y in U, and Z and X are not adjacent in
G. X is an ancestor of Y and Y is a descendant of X
if and only if there is a directed path from X toY.
Pearl and Verma have shown how to calculate the
conditional independence relations that are entailed by
distributions satisfying the Markov condition for a graph
G using the d-separability relation. In graph G, variables
X and Y are d-separated by a set of vertices S not
containing X orY if and only if there exists no undirected
path U between X andY, such that (i) every collider on U
has a descendent in S and (ii) no other vertex on U is in S.
Disjoint sets of variables X and Y are d-separated by S in
G if and only if every member of X is d-separated from
every member of Y by S in G. If distribution P satisfies
the Markov condition for graph G, then the Markov
condition entails that X is independent of Y conditional
on S if and only if X is d-separated from Y by S in G
(Pearl l988).
The following algorithm (Spirtes 1990, Spirtes 199 1)
reconstructs the set of all graphs of causal processes that
could have generated a given probability distribution,
under the assumptions that no latent variables are present
(i.e. every cause of a pair of measured variables is itself
measured) and that the distribution is faithful to the graph
of the causal process that generated it. The d-separability

of Unmeasured Variables

relations of the graph can be determined either by
performing tests of conditional independence on the
generated distribution, or in the linear case, tests of zero
partial correlations.
Let Ac(A,B) denote the set of vertices adjacent to A or to
B in graph C, except for A and B themselves. Let
Uc(A,B) denote the set of vertices in graph C on (acyclic)
undirected paths between A and B, except for A and B
themselves. (Since the algorithm is continually updating
C, Ac(A,B) and Uc(A,B) are constantly changing as the
algorithm progresses.)
PC Algorithm
A.) Form the complete undirected graph C on the vertex
set V.
B.)

n= 0.

repeat
For each pair of variables A, B adjacent in C, if
Ac(A,B) n Uc(A,B) has cardinality greater than
or equal to n and A, B are d-separated by any
subset of Ac(A,B) n Uc(A,B) of cardinality n,
delete A - B from C.
n=n+l.
until for each pair of vertices A, B that are adjacent in
C, Ac(A,B) n Uc(A,B) is of cardinality less than n.
C.) Let F be the graph resulting from step B. For each
triple of vertices A, B, C such that the pair A, B and the
pair B, C are each adjacent in F but the pair A, C are not
adjacent in F, orient A B - C as A ->B <- C if and only
if A and C are not d-separated by any subset of Ap(A,C)
n UF(A, C) containing B.
-

D. repeat
If there is a directed edge A -> B, and an
undirected edge B - C, and no edge of either
kind connecting A and C, then orient B - C
as B ->C. If there is a directed path from A
to B, and an undirected edge A - B, orient A B as A ->B.
until no more arrowheads can be added.
We have run this algorithm on as many as 90 variables in
randomly generated sparse graphs. In Monte Carlo
simulations on linear models, for large sample sizes (on
the order of several thousand) on sparse graphs, the
percentage of errors of omission or commission for
adjacencies is below 2%. Under the same conditions, the
percentage of arrowheads that are erroneously omitted is
less than 2 %, and the percentage of arrowheads that are
erroneously added is about 20%. (See Spirtes, Glymour,
and Scheines forthcoming for the details of this Monte
Carlo study.)
Following Pearl's terminology, the output of the
algorithm when there are no unmeasured common causes
is a mixture of directed and undirected edges called a

393

394

Spirtes

pattern. A pattern TI represents a set of directed acyclic
graphs. A graph G is in the set of graphs represented by
TI if and only if:

1. G has the same adjacency relations as TI.
2. If the edge between A and B is oriented A -> B in
TI, then it is oriented A-> B in G.
3. There are no unshielded colliders in G that are not
also unshielded colliders in TI.
A hybrid graph may contain contain bidirected edges
edges. Such graphs may be used to represented the
marginal structure on a set of measured variables when
unmeasured common causes have edges that collide (in
Verma and Pearl's terminology, are "head to head") with
edges between measured variables. When there are
unmeasured common causes, the output of the PC
algorithm can include such bi-directed edges. X - Y
denotes that there is an undirected edge between X andY
in a pattern TI, X o->Y denotes !hat X andY are adjacent
and there is at least an arrowhead intoY, X ->Y denotes
X o-> Y and not Y o-> X, and X <-> Y denotes X o->
Y and Y o-> X. Two vertices are adjacent in a hybrid
graph if X-Y, X -> Y, or X <-> Y. Two edges collide
atY if and only if each edge has an arrowhead directed into
Y. The definitions of directed path, undirected path, and d­
separability for hybrid graphs are the same as for directed
graphs.
Let

D be a directed acyclic graph with vertex set U of

which the subset 0 are observable. The pattern TI of
D restricted to 0 is the hybrid graph that is the result
of applying the PC algorithm to the d-separability
relations of D that involve just variables in 0. From the
marginals over 0 of distributions fairhful to D, the PC
algorithm can be used to construct the pattern of D. (The
method that Verma and Pearl use to construct the pattern
of D is based upon an earlier algorithm described in
Spirtes 1990 for constructing graphs when no latent
variables are present given fairhful input. It is equivalent
in output to the PC algorithm, but is too slow to be used
on large numbers of variables, and is less reliable in
practice because it requires testing high order conditional
independence relations.)

I will show the following;

Theorem 2: Let G be a graph over a set of vertices U,

and 0 be a subset of U containing X and Y, and TI the
pattern of G over 0. If there exists a directed path A from
X toY in G then TI contains a semi-directed path B from
X toY.
Theorem 2 states that if the probability distribution on the
set of measured variables is not compatible with a semi­
directed path B from X toY then if that distribution is the
marginal of a distribution perfectly represented by a graph
on a larger vertex set, the larger graph also contains no
directed path from X to Y. In that case, it is possible to
know that A is not a cause of B by examining the
marginal over the observed variables, even if latent
variables are present.
The following theorem states under what conditions it is
possible to infer from a pattern the existence of a directed
path in a graph G In a directed acyclic graph or a pattern,
X ,Y, and Z form a triangle if and only if X is adjacent
toY and Z, and Z is adjacent toY.

3: Let 0 be a subset of vertices of G
containing X and Z, and let the pattern of G for 0 contain
a directed edge X -> Z, no triangle containing X and Z,

Theorem

and a variable C such that C o-> X . Then in G there is
a directed path from X to Z.
Furthermore:

Corollary 1: Let 0 be a subset of vertices of G
containing X and Z, and let the pattern of G for 0 contain
a vertex C such that C o-> X , and a directed path A from
X to Z such that for no adjacent pair U, W on A is there a
triangle in TI containing both U and W. Then in G there
is a directed path from X to Z.
Theorem 2 and Corollary 1 state conditions under which it
is possible to know that A is a (possibly indirect) cause of

B simply by examining the marginal over the observed
variables, even if latent variables are present.

3

Can Theorem 3 Be Strengthened?

Theorem 1: Given a graph G over a set of variables U,

Verma and Pearl have recently published a claim
(Corollary 1 in Pearl 1991) that entails a stronger version
of Theorem 3. A consequence of their Corollary 1,
translated into the language of this paper isl:

a distribution P that satisfies the Markov condition for G,
and some subset 0 of U , for X , Y , and C that are
disjoint subsets of 0, the Markov condition entails that X
is independent of Y conditional on C if and only if X and
Y are d-separated by C in the pattern of G over 0.

Let 0 be a subset of vertices of G containing X and Z,
and let the pattern TI of G for 0 contain a directed path P
from X to Z, such that for every edge A -> B in P there is
a variable C such that C o-> A. Then in G there is a
directed path from X to Z.

The following theorem is a corollary of a theorem proved
by Verma and Pearl:

In a pattern TI , a semi-directed path P from X to Y
is an undirected path from X to Y such that if A occurs
before B in P, then in TI the edge from B to A docs not
have an arrowhead into A. In other words, a semi-directed
path can contain both undirected edges and directed edges
pointing in the direction from X to Y, but it cannot
contain any bidirected edges or edges pointing in the
direction fromY to X.

This claim is false. It is stronger than Theorem 3 because
it does not require that each edge in P not be part of a
triangle in P order to infer the existence of a directed palh
l
The statement of their Corollary

1 is somewhat ambiguous, so it is

not immediately obvious that the correct interpretation entails the
consequence stated here.

However Pearl (personal communication)

has confinned that the interpretation presented here is the intended
one.

Detecting Causal Relations in the Presence of Unmeasured Variables

from X to Z in G. The graph depicted in Figure I
( suggested by the proof of Theorem 3) provides a
counterexample. In G, there is no directed path fromX to
Z; however in the pattern of G over the set of variables
that does not include T, there is a uni-directed edge fromX
to Z, and an edge D ->X.

path PJ (X,Y) in G from X to Y that induces an edge
betweenX andY in IT, then eitherX -> Y orX -Y in IT.

Proof. The proof is a reductio. Assume that there is a
directed path P in G from X to Y that induces and edge
betweenX andY in IT, but neitherX -> Y nor X -Y in
IT. It follows thatY o->X in IT. By lemma 1, there is a
vertex Z in 0 such that either Z is adjacent to X and not
toY in IT, and both of the edges betweenX andY andX
and Z are induced by paths pointing at X, or Z o-> X in
IT, andX is a descendant ofY in G. X is not a descendant
ofY in G, becauseY is a descendant ofX in G, and G is

•

E

Graph G

acyclic. Suppose then that there is a vertex Z in 0 such
that Z is adjacent toX and not toY in IT, and both of the
edges betweenX andY andX and Z are induced by paths
pointing atX.
Let P( Z,X) be an undirected path between X and Z that
points intoX and induces an edge betweenX and Z in IT,
and P2(XY
, ) be an undirected path betweenX andY that
points intoX and induces an edge betweenX andY in IT.
Let R be the first point of intersection of P(X,Z) with
P2(X,Y), P( Z,R) be the subpath of P( Z,X) from Z to R,
P( R,Y ) be the subpath of P2(X,Y ) from R to Y, and
P( Z,Y ) be the concatenation of P( Z,R) and P( R,Y).
P( Z,Y) is an undirected path because P( Z,R) and P( R,Y)
by construction intersect only at R, and hence P( Z, R) and
P( R,Y) contain any given vertex at most once.

Pattern of G over (C,D,X,Z,E}

Figure 1

4

Appendix

Lemma 1 ( Verma and Pearl): Let 0 be a subset of U, D

a directed acyclic graph over U, and IT the pattern of D
restricted to 0. Two variables A, B in 0 are adjacent in
IT if and only if there exists a path P between A andB in
D satisfying the following two conditions:

Every vertex on P( Z,Y ) that is in 0 is a collider on
P( Z,Y). By lemma 1, every vertex in 0 on P( Z, R) except
for Z and R are collidcrs on P( Z, R), and every vertex in 0
on P2( R,Y) except for R andY are colliders on P2( R,Y)
( because they are subpaths of paths that induce edges
between Z andX, andX andY respectively.) This shows
that each vertex in 0 with the possible exception of R on
P( Z,Y) is a collider on P( Z,Y). I will now show that if R
is in 0, it is also a collider on P( Z,Y). If R is equal to
X, thenX is a collider on P( Z,Y) because both P( Z,X) and
P(XY)
,
are intoX. If R is not equal to X, and R is in 0,
then R is a collider on both P( Z,X) and P(X,Y); hence R
is a collider on P( Z,Y). It follows that every vertex on
P( Z,Y ) that is in 0 is a collider on P( Z,Y).

(!) every observable node on P ( except the endpoints) is a
collider along P; and

Suppose first that every collider on P( Z,Y ) is a shieldable
ancestor of either Z orY. By lemma 1, P( Z,Y) induces an
edge between Z andY in IT. It follows from lemma 2

( 2) every collider along P is a shieldable ancestor of either
A orB.

orientation. This contradicts the assumption.

where an ancestor S of A is shieldable if and only if every
directed path from S to A contains an observable other
than A.

Lemma 2 ( Verma and Pearl): For any pattern IT of D
over 0, A o-> B if and only if there is a node C such that
either (1) C is adjacent to B and not A (in IT) and both
edges A -B andB - C were induced by paths ( of D) which
ended pointing at B, or ( 2) C o-> A in IT and B is a
descendent of A in D.

Lemma 3: Let G be a directed acyclic graph over a set of
vertices U, 0 be a subset of U containing X andY, and
IT be the pattern of G restricted to 0. If there is a directed

that P( Z,X) and P2(X,Y ) do not induce a Y o-> X

Suppose next that there is a collider on P( Z,Y) that is not
a shieldable ancestor of either Z or Y, and let W be the
first such collidcr after Z. R is the only vertex on P( Z,Y)
that may be a collider on P( Z Y
, ) but not a collider in
either P( Z,X ) or P2(XY
, ). Hence W is either equal to R
or a collider on P( Z,X) or P2(XY
, ).
In either caseY is a descendant of W. Suppose first that
W is a collider on P( Z,X) or P2(XY
, ). Because W is not
a shieldable ancestor of either Z orY, by lemma 1, W is a
shieldable ancestor ofX. X is an ancestor ofY, and W is
an ancestor ofX, so W is an ancestor ofY. Suppose next
that W is equal to R. In this case R is not equal to X,
becauseX is an ancestor ofY andX is in 0, and henceX

395

396

Spirtes

is a shieldable ancestor ofY. Either X is a descendant of
, )
R, Y is a descendant of R, or some collider along P2(XY
is a descendant of R. If X is a descendant of R, thenY is
a descendant of R, because Y is a descendant of X. If
some collider along P2(X,Y) is a descendant of R, thenY
is a descendant of R because each collidcr on P2(X,Y) is a
shieldable ancestor of either X orY, andY is a descendant
of X. In any case, then, Y is a descendant of W.
W is an ancestor ofY, but not a shicldable ancestor ofY,
so there is a directed path P(WY
, ) from W to Y that
contains no vertices in 0 other thanY. Let S be the first
point of intersection of P(ZY
, ) with P(WY
, ), P(Z, S) the
subpath of P(Z,Y) from Z to S, P(S,Y) the subpath of
P(WY
, ) from S toY, and P2(ZY
, ) the concatenation of
P(Z,S) and P(S,Y). S is not a collidcr on P2 ( Z , Y)
because the first edge P(SY
, ) is not directed into S. Hence
every collider on P(Z, S) is a collidcr on P2(Z, W). W is
the first collider on P(Z,Y) that is not a shieldable
ancestor of Z or Y, S either equals W or is before W on
P(Z,Y), S is not a collidcr on P2(Z,Y ), and P(S,Y)
contains no colliders; hence every collidcr on P2(Z,Y) is a
shieldable ancestor of either Z orY.
Every vertex V on P2 (Z,Y) that is in 0 is on P(Z, S). V
is not equal to S because S is not in 0. Ir V is not equal
to S, then V is a collidcr on P(Z, S), and hence a collider
on P2(Z,Y).
By lemma 1, P2(Z,Y) induces an edge between Z andY in
P. It follows from lemma 2 that P(Z,X) and P2(X,Y) do
not induce a Y o-> X orientation. This contradicts the
assumption.
Theorem 2: Let G be a graph over a set of vertices U,

and 0 be a subset of U containing X andY, and TI the
pattern of G over 0. If there exists a directed path A from
X toY in G then TI contains a semi-directed path B from
X toY.
Proof. Break the path A in G into a series of subpaths

such that only the endpoints of the subpaths are in 0.
Let U be the source and V be the sink of some such
arbitrary subpath. There is an edge between U and Vin TI
by lemma 3. U is prior to V on B. The concatenation of
the edges induced by the subpaths arc an undirected path B
from X toY inTI. By Lemma 1, it is not the case that
V o-> U. By definition of semi-directed path, B is a semi­
directed path from X toY.
3: Let 0 be a subset of vertices of G
containing X and Z, and let the pattern TI of G for 0
contain a directed edge X -> Z, no triangle containing X
and Z, and a variable D such that D o-> X. Then in G
there is a directed path from X to Z.

Theorem

Proof. Since X and Z are adjacent in TI there is a path A
in G between X and Z such that every observable node on
A is a collider and every collider on A is a shieldable
ancestor of X or Z.

If X -> Z in pattern TI arises because of clause (2 ) of
lemma 2, we are done because Z is a descendant of X in
G. So suppose X -> Z is oriented by condition (1) of

lemma 2. Then there is a path A in G that induces X -> Z
and A is into Z in G. If A contains no colliders and is not
into X, then A is a directed path from X to Z, and we are
done. Otherwise there are two cases: A contains a
collider, or A is into X.
First, we consider the case where A is into X. Then there
is a path between X and Z that induces an edge in TI, and
is into X. By assumption there is a vertex D in TI such
that D o-> X. By lemma 2, either there is a vertex C in
n such that C is adjacent to X and not D, and both edges
C-X and D-X are induced by paths of G which point at X,
or C o-> D in TI and X is a descendant of D in G.
Suppose that the first disjunct is true. In that case, either
C is adjacent to Z inTI or it isn't. If it is adjacent to Z,
then there is a triangle in IT containing X and Z. If C is
not adjacent to Z, then by clause (2) of lemma 2, Z o->
X, contrary to our assumption.
Suppose now that the second disjunct is true. Because X
is a descendant of D in G, there is a directed path in G
from some variable E in 0 to X that does not contain any
variables in 0 other than X and E. This path induces an
edge between E and X in IT. If E is adjacent to X in TI,
then TI contains a triangle containing X and Z; if E is not
adjacent to X inTI, then by clause (2) of lemma 2, Z o->
X, contrary to our assumption.
We now consider the case where A is not into X, but there
is a collider on A. Let K be the first collider on A after
X. Because A is not into X, then there is a directed path
from X to K, and hence no directed path from K to X.
This implies that K is not an ancestor, and hence not a
shieldable ancestor of X. So by clause (2) of lemma 1, K
is a shieldable ancestor of Z. The concatenation of the
paths from X to K and from K to Z is a directed path from
X to Z.
1: Let 0 be a subset of vertices of G
containing X and Z, and let the pattern TI of G for 0
contain a vertex C such that C o-> X, and a directed path
P from X to Z such that for no adjacent pair U, W on P is
there a triangle in n containing both U and W. Then in G
there is a directed path from X to Z.
Corollary

Proof. Since there is a directed path from X to Z in TI ,

there i s a sequence of edges X -> A -> B ... -> Z i n TI.
By Theorem 3, there is a directed path from X to A in G.
Since X -> A in TI, Theorem 3 can next be applied to A
-> B, to show that there is a directed path from A to B in
G. Repeating this process in turn for each edge on P
implies that there is a directed path from X to Z in G.

Acknowledgements

I wish to thank Clark Glymour for a number of useful
discussions and suggestions on the topic of causal
inference when latent variables are present. This research
was supported in part by a graph with the Office of Naval
Research, and the Naval Manpower Research and
Development Center under Contract number N00114-89-J1964. Theorem 3 and a weaker version of Theorem 2
were reported in "Causal Structure among Measured

Detecting Causal Relations in the Presence of Unmeasured Variables

Variables Preserved with Unmeasured Variables", by Peter
Spirtes and Clark Glymour, Laboratory for Computational
Linguistics Technical Report No. CMU-LCL-90-5,
August, 1990.




gorithm, an efficient search procedure over equivalence
classes of DAGs (Meek 1996, Chickering 2002).

Different directed acyclic graphs (DAGs)
may be Markov equivalent in the sense that
they entail the same conditional independence relations among the observed variables. Chickering (1995) provided a transformational characterization of Markov equivalence for DAGs (with no latent variables),
which is useful in deriving properties shared
by Markov equivalent DAGs, and, with certain generalization, is needed to prove the asymptotic correctness of a search procedure
over Markov equivalence classes, known as
the GES algorithm.

In many situations, however, we need also to consider
DAGs with latent variables. Indeed there are cases
where no DAGs can perfectly explain the observed conditional independence relations unless latent variables
are introduced (see, e.g., Figure 2). But it is often
undesirable to work with latent variable DAG models,
especially with respect to model search. For example,
given a set of observed variables, there are infinitely
many latent variable DAG models to search over. Besides, to fit and score a DAG with latent variables is
usually difficult due to statistical issues such as identifiability. Fortunately, such latent variable DAG models can be represented by ancestral graphical models
(Richardson and Spirtes 2002), in that for any DAG
with latent variables, there is a (maximal) ancestral
graph that captures the exact observable conditional
independence relations as well as some of the causal relations entailed by that DAG. Since ancestral graphs
do not explicitly include latent variables, they are more
amenable to search (Spirtes et al. 1997).

For DAG models with latent variables, maximal ancestral graphs (MAGs) provide a
neat representation that facilitates model
search. However, no transformational characterization — analogous to Chickering’s —
of Markov equivalent MAGs is yet available.
This paper establishes such a characterization for directed MAGs, which we expect will
have similar uses as it does for DAGs.

1

Peter Spirtes
Department of Philosophy
Carnegie Mellon University
Institute for Human and Machine Cognition
University of West Florida
ps7z@andrew.cmu.edu

INTRODUCTION

Markov equivalence between directed acyclic graphs
(DAGs) has been characterized in several ways (e.g.,
Verma and Pearl 1990, Chickering 1995, Andersson et
al. 1997). All of them have been found useful for various purposes. In particular, the transformational characterization provided by Chickering (1995) — that two
Markov equivalent DAGs can be transformed to each
other by a sequence of single edge reversals that preserve Markov equivalence — is useful in deriving properties shared by Markov equivalent DAGs. Moreover,
when generalized, the transformational characterization implies the asymptotic correctness of the GES al-

Markov equivalence for ancestral graphs has been characterized in ways analogous to the one given by Verma
and Pearl (1990) for DAGs (Spirtes and Richardson
1996, Ali et al. 2004). However, no result is yet available that is analogous to Chickering’s transformational
characterization. In this paper we establish one for directed ancestral graphs. Specifically we show that two
directed maximal ancestral graphs are Markov equivalent if and only if one can be transformed to the other
by a sequence of single mark changes — adding or
dropping an arrowhead — that preserve Markov equivalence. This characterization we expect will have similar uses as Chickering’s does for DAGs. In particular,
it is a step towards justifying the application of the
GES algorithm to MAGs, and hence to latent variable
DAG models.
The paper is organized as follows. The remainder of
this section introduces the relevant definitions and no-

tations. We then present the main result in section
2, drawing on some facts proved in Zhang and Spirtes
(2005) and Ali et al. (2005). We conclude the paper in
section 3 with a discussion of the potential application,
limitation and generalization of our result.
1.1

DIRECTED ANCESTRAL GRAPHS

In full generality, an ancestral graph can contain three
kinds of edges: directed edge (→), bi-directed edge
(↔) and undirected edge (−−). In this paper, however, we will confine ourselves to directed ancestral
graphs — which do not contain undirected edges —
until section 3, where we explain why our result does
not hold for general ancestral graphs. The class of
directed ancestral graphs, due to its inclusion of bidirected edges, is suitable for representing observed
conditional independence structures in the presence of
latent confounders (see Figure 2). Without undirected
edges, however, ancestral graphs cannot represent the
presence of latent selection variables.
By a directed mixed graph we denote an arbitrary
graph that can have two kinds of edges: directed and
bi-directed. The two ends of an edge we call marks or
orientations. So the two marks of a bi-directed edge
are both arrowheads (>), while a directed edge has
one arrowhead and one tail (−) as its marks. Sometimes we say an edge is into (or out of) a vertex if
the mark of the edge at the vertex is an arrowhead
(or a tail). The meaning of the standard graph theoretical concepts, such as parent/child, (directed)
path, ancestor/descendant, etc., remains the same
in mixed graphs. Furthermore, if there is a bi-directed
edge between two vertices A and B (A ↔ B), then A
is called a spouse of B and B a spouse of A.
Definition 1 (ancestral). A directed mixed graph is
ancestral if
(a1) there is no directed cycle; and
(a2) for any two vertices A and B, if A is a spouse of
B (i.e., A ↔ B), then A is not an ancestor of B.
Clearly DAGs are a special case of directed ancestral
graphs (with no bi-directed edges). Condition (a1) is
just the familiar one for DAGs. Condition (a2), together with (a1), defines a nice feature of arrowheads
— that is, an arrowhead implies non-ancestorship.
This motivates the term ”ancestral” and induces a natural causal interpretation of ancestral graphs.
Mixed graphs encode conditional independence relations by essentially the same graphical criterion as
the well-known d-separation for DAGs, except that in
mixed graphs colliders can arise in more edge configurations than they do in DAGs. Given a path u in a

mixed graph, a non-endpoint vertex V on u is called a
collider if the two edges incident to V on u are both
into V , otherwise V is called a non-collider.
Definition 2 (m-separation). In a mixed graph,
a path u between vertices A and B is active (mconnecting) relative to a set of vertices Z (A, B ∈
/ Z)
if
i. every non-collider on u is not a member of Z;
ii. every collider on u is an ancestor of some member
of Z.
A and B are said to be m-separated by Z if there is
no active path between A and B relative to Z.
The following property is true of DAGs: if two vertices
are not adjacent, then there is a set of some other
vertices that m-separates (d-separates) the two. This,
however, is not true of directed ancestral graphs in
general. For example, Figure 1(a) is an ancestral graph
that fails this condition: C and D are not adjacent, but
no subset of {A, B} m-separates them.

A

B

A

B

C

D

C

D

(a)

(b)

Figure 1: (a) an ancestral graph that is not maximal;
(b) a maximal ancestral graph
This motivates the following definition:
Definition 3 (maximality). A directed ancestral
graph is said to be maximal if for any two nonadjacent vertices, there is a set of vertices that mseparates them.
It is shown in Richardson and Spirtes (2002) that
every non-maximal ancestral graph can be easily transformed to a unique supergraph that is ancestral and
maximal by adding bi-directed edges. This justifies
considering only those ancestral graphs that are maximal (MAGs). From now on, we focus on directed
maximal ancestral graphs, which we will refer to as
DMAGs. A notion closely related to maximality is
that of inducing path:
Definition 4 (inducing path). In an ancestral
graph, a path u between A and B is called an inducing
path if every non-endpoint vertex on u is a collider
and is an ancestor of either A or B.

For example, in Figure 1(a), the path hC, A, B, Di is
an inducing path between C and D. Richardson and
Spirtes (2002) proved that the presence of an inducing
path is necessary and sufficient for two vertices not to
be m-separated by any set. So, to show that a graph
is maximal, it suffices to demonstrate that there is no
inducing path between any two non-adjacent vertices
in the graph.
Given any DAG with (or without) latent variables,
the conditional independence relations as well as the
causal relations among the observed variables can be
represented by a DMAG that includes only the observed variables. The DMAG is constructed as follows: for every pair of observed variables, Oi and Oj ,
put an edge between them if and only if they are not
d-separated by any set of other observed variables in
the given DAG, and mark an arrowhead at Oi (Oj ) on
the edge if it is not an ancestor of Oj (Oi ) in the given
DAG.
For example, Figure 2(a) is a DAG with latent variables {L1, L2, L3}. Figure 2(b) depicts the DMAG
(G1) resulting from the above construction. The mseparation relations in G1 correspond exactly to the
d-separation relations over {X1, X2, X3, X4, X5} in
Figure 2(a). By contrast, no DAG without extra latent
variables has the exact same d-separation relations.
Furthermore, the orientations in G1 accurately represent the ancestor relationships — which, upon natural
interpretations, are causal relationships — among the
observed variables in 2(a). (This, however, is not the
case with G2.)
X1

X2

1.2

MARKOV EQUIVALENCE

A DMAG represents the set of joint distributions that
satisfy its global Markov property, i.e., the set of distributions of which the conditional independence relations entailed by m-separations in the DMAG hold.
Hence, if two DMAGs share the same m-separation
structures, then they represent the same set of distributions.
Definition 5 (Markov equivalence). Two DMAGs
G1 , G2 (with the same set of vertices) are Markov
equivalent if for any three disjoint sets of vertices
X, Y, Z, X and Y are m-separated by Z in G1 if and
only if X and Y are m-separated by Z in G2 .
Figure 2(c), for example, is a DMAG Markov equivalent to 2(b). It is well known that two DAGs are
Markov equivalent if and only if they have the same
adjacencies and the same unshielded colliders (Verma
and Pearl 1990). (A triple hA, B, Ci is said to be unshielded if A, B are adjacent, B, C are adjacent but
A, C are not adjacent.) The conditions are still necessary for Markov equivalence between DMAGs, but are
not sufficient. For two DMAGs to be equivalent, some
shielded colliders have to be present in both or neither
of the graphs. The next definition is related to this.
Definition 6 (discriminating path). In a DMAG,
a path between X and Y , u = hX, · · · , W, V, Y i, is a
discriminating path for V if
i. u includes at least three edges (i.e., at least four
vertices as specified);

L1

ii. V is adjacent to an endpoint Y on u; and
L2

X3

(a)

X1
X2

X5

L3

X4

X3

iii. X is not adjacent to Y , and every vertex between
X and V is a collider on u and is a parent of Y .

Y

X1
X2

X3

X4

X5

X
X4

X5

(b) G1

(c) G2

Figure 2: (a): A DAG with latent variables; (b): A
DMAG that captures both the conditional independence and causal relations among the observed variables represented by (a); (c): A DMAG that entails
the right conditional independence relations but not
the right causal relations in (a).

W

V

Figure 3: A discriminating path for V : the triple
hW, V, Y i is ”discriminated” to be a collider here.
Discriminating paths behave similarly to unshielded
triples in that if u = hX, · · · , W, V, Y i is discriminating for V , then hW, V, Y i is a (shielded) collider (See
Figure 3) if and only if every set that m-separates X
and Y excludes V ; it is a non-collider if and only if
every set that m-separates X and Y contains V . The

following proposition is proved in Spirtes and Richardson (1996)1 .

(2004) explored the statistical significance of this fact
for fitting bi-directed graphs.

Proposition 1. Two DMAGs over the same set of
vertices are Markov equivalent if and only if

Another feature which will be particularly relevant to
our argument is that between a DMAG and any of its
LEGs, only one kind of difference is possible, namely,
some bi-directed edges in the DMAG are oriented as
directed edges in its LEG. For a simple illustration,
compare the graphs in Figure 4, where H1 is a LEG of
G1, and H2 is a LEG of G2. This feature is important
because it will be the condition for Theorem 1 in 2.3.

(e1) They have the same adjacencies;
(e2) They have the same unshielded colliders;
(e3) If a path u is a discriminating path for a vertex
B in both graphs, then B is a collider on the path
in one graph if and only if it is a collider on the
path in the other.

2

TRANSFORMATION BETWEEN
EQUIVALENT DMAGS

X1

X1
X2

X3

X4

X5

X2

X4

2.1

LOYAL EQUIVALENT GRAPH

Given a MAG G, a mark (or edge) in G is invariant if
it is present in all MAGs Markov equivalent to G. Invariant marks are particularly important for causal inference because data alone usually cannot distinguish
between members of a Markov equivalence class. An
algorithm for detecting all invariant arrowheads in a
MAG is given by Ali et al. (2005), and one for further detecting all invariant tails is presented in Zhang
and Spirtes (2005). The following is a special case of
Corollary 18 in Zhang and Spirtes (2005).
Proposition 2. Given any DMAG G, there exists a
DMAG H Markov equivalent to G such that all bidirected edges in H are invariant, and every directed
edge in G is also in H.
We will call H in Proposition 2 a Loyal Equivalent
Graph (LEG) of G. In general a DMAG could have
multiple LEGs. A distinctive feature of the LEGs
is that they have the fewest bi-directed edges among
Markov equivalent DMAGs2 . Drton and Richardson
1
The conditions are also valid for maximal ancestral
graphs that contain undirected edges.
2
For general MAGs, Corollary 18 in Zhang and Spirtes
(2005) also asserts that the LEGs have the fewest undirected edges as well.

X5

H1

G1

We present the main result of the paper in this section, namely Markov equivalent DMAGs can be transformed to each other by a sequence of single mark
changes that preserve Markov equivalence. We first
describe in section 2.1 two corollaries from Zhang and
Spirtes (2005) and Ali et al. (2005) which our arguments will rely upon. Section 2.2 establishes sufficient
and necessary conditions for a single mark change to
preserve Markov equivalence. The theorems are then
presented in section 2.3.

X3

X1

X1
X2

X3

X2

X3

X4

X5

X4

X5

G2

H2

Figure 4: A LEG of G1 (H1) and a LEG of G2 (H2)
A directed edge in a DMAG is called reversible if
there is another Markov equivalent DMAG in which
the direction of the edge is reversed. To prove Theorem
2 in 2.3, we also need a fact that immediately follows
from Corollary 4.1 in Ali et al. (2005).
Proposition 3. Let A → B be any reversible edge in
a DMAG G. For any vertex C (distinct from A and
B), there is an invariant bi-directed edge between C
and A if and only if there is an invariant bi-directed
edge between C and B.
In particular, if H is a LEG of a DMAG, then A → B
being reversible implies that A and B have the same
spouses, as every bi-directed edge in H is invariant.
2.2

LEGITIMATE MARK CHANGE

Eventually we will show that any two Markov equivalent DMAGs can be connected by a sequence of
equivalence-preserving mark changes. It is thus desirable to give some relatively simple graphical conditions under which a single mark change would preserve
equivalence. Lemma 1 below gives necessary and sufficient conditions under which adding an arrowhead
to a directed edge (i.e., changing the directed edge to
a bi-directed one) preserves Markov equivalence. By
symmetry, they are also the conditions for dropping

an arrowhead from a bi-directed edge while preserving
Markov equivalence.
Lemma 1. Let G be an arbitrary DMAG, and A → B
an arbitrary directed edge in G. Let G 0 be the graph
identical to G except that the edge between A and B
is A ↔ B. (In other words, G 0 is the result of simply
changing A → B into A ↔ B in G.) G 0 is a DMAG
and Markov equivalent to G if and only if
(t1) there is no directed path from A to B other than
A → B in G;
(t2) For any C → A in G, C → B is also in G; and
for any D ↔ A in G, either D → B or D ↔ B is
in G;
(t3) there is no discriminating path for A on which B
is the endpoint adjacent to A in G.
Proof Sketch: 3 We skip the demonstration of necessity because it is relatively easy and will not be used
later. To prove sufficiency, suppose (t1)-(t3) are met,
and we show that they guarantee G 0 is a DMAG and
is Markov equivalent to G. To see that G 0 is ancestral,
note that it only differs from G, an ancestral graph,
regarding the edge between A and B. So the only way
for G 0 to violate the definition of ancestral graph is for
A to be an ancestor of B in G 0 , which contradicts (t1).
To show that G 0 is maximal, we need to show that
there is no inducing path (Definition 4) between any
two non-adjacent vertices. Suppose for contradiction
that there is an inducing path u in G 0 between two nonadjacent vertices, D and E. Then u includes A ↔ B
and A is not an endpoint of u, for otherwise u would
also be an inducing path in G, contradicting the fact
that G is maximal. Also note that D is not a parent
of B, otherwise D is an ancestor of E by definition 4,
which easily leads to a contradiction given that G 0 has
been shown to be ancestral.
Suppose, without loss of generality, that D is the
endpoint closer to A on u than it is to B. Let
u(D, A) = hD = V0 , ..., Vn , Ai be the subpath of u
between D and A. Then some Vi is B’s spouse (in
G), for otherwise we can show by induction (starting
from Vn ) that every Vi , and in particular V0 = D, is a
parent of B, which is a contradiction.
Let Vj be a spouse of B on u(D, A). Replacing
u(Vj , B) on u with Vj ↔ B yields an inducing path
between D and E that does not include A ↔ B,
and hence an inducing path between D and E in G,
a contradiction. So the initial supposition of nonmaximality is false. G 0 is also maximal.
3
The full version of the paper can be found at
www.andrew.cmu.edu/user/jiji/transformation.pdf.

Lastly, we verify that G and G 0 satisfy the conditions
for Markov equivalence in Proposition 1. Obviously
they have the same adjacencies, and share the same
colliders except possibly A. But A will not be a collider in an unshielded triple, for condition (t2) requires
that any vertex that is incident to an edge into A is
also adjacent to B. So the only worry is that a triple
hC, A, Bi might be discriminated by a path, but (t3)
guarantees that there is no such path.
We say a mark change is legitimate when the conditions in Lemma 1 are satisfied. Recall that for DAGs
the basic unit of equivalence-preserving transformation is (covered) edge reversal (Chickering 1995). In
the current paper we treat an edge reversal as simply a
special case of two consecutive mark changes. That is,
a reversal of A → B is simply to first add an arrowhead
at A (to form A ↔ B), and then to drop the arrowhead
at B (to form A ← B). An edge reversal is said to be
legitimate if both of the two consecutive mark changes
are legitimate. Given Lemma 1, it is straightforward
to check the validity of the following conditions for legitimate edge reversal. (We use PaG /SpG to denote
the set of parents/spouses of a vertex in G.)
Lemma 2. Let G be an arbitrary DMAG, and A → B
an arbitrary directed edge in G. The reversal of A → B
is legitimate if and only if PaG (B) = PaG (A) ∪ {A}
and SpG (B) = SpG (A).
When there is no bi-directed edge in G, that is, when
G is a DAG, the condition in Lemma 2 is reduced to
the familiar definition for covered edge, i.e., PaG (B) =
PaG (A) ∪ {A} (Chickering 1995). The condition given
by Drton and Richardson (2004) for an edge in a bidirected graph to be ”orientable” in either direction
(SpG (B) = SpG (A)) can be viewed as another special
case of the above lemma.
2.3

THE MAIN RESULT

We first state two intermediate theorems crucial for the
main result we are heading for. The first one says if the
differences between two Markov equivalent DMAGs G
and G 0 are all of the following sort: a directed edge is
in G while the corresponding edge is bi-directed in G 0 ,
then there is a sequence of legitimate mark changes
that transforms one to the other. The second one says
that if every bi-directed edge in G and every bi-directed
edge in G 0 are invariant, then there is a sequence of
legitimate mark changes (edge reversals) that transforms one to the other. The proofs follow the strategy
of Chickering’s proof for DAGs.
Theorem 1. Let G and G 0 be two Markov equivalent
DMAGs. If the differences between G and G 0 are all
of the following sort: a directed edge is in G while the

corresponding edge is bi-directed in G 0 , then there is
a sequence of legitimate mark changes that transforms
one to the other.
Proof Sketch: We prove that there is a sequence of
transformation from G to G 0 , the reverse of which will
be a transformation from G 0 to G. Specifically we show
that as long as G and G 0 are different, there is always a
legitimate mark change that can eliminate a difference
between them. The theorem then follows from a simple
induction on the number of differences.
Let
Diff = {y| there is a x such that x → y is in G and
x ↔ y is in G 0 }
By the antecedent condition, Diff exhausts all the differences there are between G and G 0 . So the two graphs
are identical if and only if Diff = Ø. We claim that if
Diff is not empty, there is a legitimate mark change
that eliminates a difference. Choose B ∈ Diff such
that no proper ancestor of B in G is in Diff . Let
Diff B = {x|x → B is in G and x ↔ B is in G 0 }
Since B ∈ Diff , Diff B is not empty. Choose A ∈
Diff B such that no proper descendant of A in G is in
Diff B . The claim is that changing A → B to A ↔ B
in G is a legitimate mark change — that is, it satisfies
the conditions stated in Lemma 1.
The verifications of conditions (t1) and (t2) in Lemma
1 take advantage of the specific way by which we
choose A and B. For example, if condition (t1) were
violated, i.e., if there were a directed path d from A to
B other than A → B, then in order for G 0 to be ancestral, d would not be directed in G 0 , which implies that
some edge on d would be bi-directed in G 0 . It is then
easy to derive a contradiction to our choice of A or B
in the first place. The verification of (t2) is similarly
easy (which uses the fact that G and G 0 have the same
unshielded colliders).
To show that (t3) also holds, suppose for contradiction that there is a discriminating path u =
hD, · · · , C, A, Bi for A in G. By Definition 6, C is a
parent of B. It follows that the edge between A and
C is not A → C, for otherwise A → C → B would be
a directed path from A to B, which has been shown
to be absent. Hence the edge between C and A is bidirected, C ↔ A (because C, Definition 6, is a collider
on u). Then the antecedent of the theorem implies
that C ↔ A is also in G 0 . Moreover, the antecedent
implies that every arrowhead in G is also in G 0 , which
entails that in G 0 every vertex between D and A is

still a collider on u. It is then easy to prove by induction that every vertex between D and A on u is also
a parent of B in G 0 (using the fact that G 0 is Markov
equivalent to G), and hence u is also discriminating for
A in G 0 . But A is a collider on u in G 0 but not in G,
which contradicts (e3) in Proposition 1.
Obviously a DMAG and any of its LEGs satisfy the antecedent of Theorem 1, so they can be transformed to
each other by a sequence of legitimate mark changes.
Steps 0-2, in Figure 5, for example, portraits a stepwise transformation from G1 to H1.
Theorem 2. Let G and G 0 be two Markov equivalent
MAGs. If every bi-directed edge in G is invariant and
every bi-directed edge in G 0 is also invariant, then there
is a sequence of legitimate mark changes that transforms one to the other.
Proof Sketch: Without loss of generality, we prove
that there is a transformation from G to G 0 . Let
Diff = {y| there is a x such that x → y is in G and
x ← y is in G 0 }
By the antecedent, Diff exhausts all the differences
there are between G and G 0 . If Diff is not empty,
we can choose an edge A → B by exactly the same
procedure as that in the proof of Theorem 1. The
claim is that reversing A → B is a legitimate edge
reversal (that is, a couple of legitimate mark changes),
i.e., satisfies the conditions in Lemma 2.
The verification is fairly easy. Note that A → B, by
our choice, is a reversible edge in G (for A ← B is
in G 0 , which is Markov equivalent to G). It follows
directly from Proposition 3 (and the assumption about
bi-directed edges in G) that SpG (B) = SpG (A). The
argument for PaG (B) = PaG (A) ∪ {A} is virtually the
same as Chickering’s proof for DAGs (Lemma 2, in
particular, in Chickering 1995).
Note that after an edge reversal, no new bi-directed
edge is introduced, so it is still true of the new graph
that every bi-directed edge is invariant. Hence we can
always identify a legitimate edge reversal to eliminate
a difference in direction as long as the current graph
and G 0 are still different. An induction on the number
of differences between G and G 0 would complete the
argument.
Since a LEG (of any MAG) only contains invariant bidirected edges, two LEGs can always be transformed
to each other via a sequence of legitimate mark changes
according to the above theorem. For example, steps
2-4 in Figure 5 constitute a transformation from H1 (a

LEG of G1) to H2 (a LEG of G2). Note that Chickering’s result for DAGs is a special case of Theorem 2,
where bi-directed edges are absent.

X1

X1
X2

X3

X2

X3

We are ready to prove the main result of this paper.
Theorem 3. Two DMAGs G and G 0 are Markov
equivalent if and only if there exists a sequence of single mark changes in G such that
1. after each mark change, the resulting graph is also
a DMAG and is Markov equivalent to G;
2. after all the mark changes, the resulting graph is
G0.

X4

X5

X4

step 0 (G1)

X1

X1
X2

X3

X4

X5

X2

As a simple illustration, Figure 5 gives the steps in
transforming G1 to G2 according to Theorem 3. That
is, G1 is first transformed to one of its LEGs, H1; H1
is then transformed to H2, a LEG of G2. Lastly, H2
is transformed to G2.
Theorems 1 and 2, as they are currently stated, are
special cases of Theorem 3, but their proofs actually
achieve a little more than they claim. The transformations constructed in the proofs of Theorems 1 and
2 are efficient in the sense that every mark change in
the transformation eliminates a difference between the
current DMAG and the target. So the transformations
consist of as many mark changes as the number of differences at the beginning. By contrast, the transformation constructed in Theorem 3 may take some ”detours”, in that some mark changes in the way actually
increase rather than decrease the difference between G
and G 0 . (This is not the case in Figure 5, but if, for
example, we chose different LEGs for G1 or G2, there
would be detours.) We believe that no such detour is
really necessary, that is, there is always a transformation from G to G 0 consisting of as many mark changes
as the number of differences between them. But we
are yet unable to prove this conjecture.

X3

X4

X5

step 3

step 2 (H1)
X1

Proof: The ”if” part is trivial – since every mark
change preserves the equivalence, the end is of course
Markov equivalent to the beginning. Now suppose G
and G 0 are equivalent. We show that there exists such
a sequence of transformation. By Proposition 2, there
is a LEG H for G and a LEG H0 for G 0 . By Theorem 1, there is a sequence of legitimate mark changes
s1 that transforms G to H, and there is a sequence of
legitimate mark changes s3 that transforms H0 to G 0 .
By Theorem 2, there is a sequence of legitimate mark
changes s2 that transforms H to H0 . Concatenating s1 ,
s2 and s3 yields a sequence of legitimate mark changes
that transforms G to G 0 .

X5

step 1

X1

X2

X3

X2

X3

X4

X5

X4

X5

step 4 (H2)

step 5 (G2)

Figure 5: A transformation from G1 to G2

3

Conclusion

In this paper we established a transformational property for Markov equivalent directed MAGs, which is
a generalization of the transformational characterization of Markov equivalent DAGs given by Chickering
(1995). It implies that no matter how different two
Markov equivalent graphs are, there is a sequence of
Markov equivalent graphs in between such that the
adjacent graphs differ in only one edge. It could thus
simplify derivations of invariance properties across a
Markov equivalence class — in order to show two arbitrary Markov equivalent DMAGs share something in
common, we only need to consider two Markov equivalent DMAGs with the minimal difference. Indeed,
Chickering (1995) used his characterization to derive
that Markov equivalent DAGs have the same number
of parameters under the standard CPT parameterization (and hence would receive the same score under the
typical penalized-likelihood type metrics). The discrete parameterization of DMAGs is currently under
development4 . We think our result will prove useful to
show similar facts once the discrete parameterization
is available.
4

Drton and Richardson (2005) provide a parameterization for bi-directed graphs with binary variables, for which
the problem of parameter equivalence does not arise because no two different bi-directed graphs are Markov equivalent.

The property, however, does not hold exactly for general MAGs, which may also contain undirected edges5 .
A simple counterexample is given in Figure 6. When
we include undirected edges, the requirement of ancestral graphs is that the endpoints of undirected edges
are of zero in-degree — that is, if a vertex is an endpoint of an undirected edge, then no edge is into that
vertex (see Richardson and Spirtes (2002) for details).
So, although the two graphs in Figure 6 are Markov
equivalent MAGs, M1 cannot be transformed to M2
by a sequence of single legitimate mark changes, as
adding any single arrowhead to M1 would make it nonancestral. Therefore, for general MAGs, the transformation may have to include a stage of changing the
undirected subgraph to a directed one in a wholesale
manner.

A

A



In a causal graphical model, an instrument
for a variable

X

and its effect Y is a ran­

dom variable that is a cause of

X

and in­

dependent of all the causes of Y except
(Pearl 1995).

X

h

For continuous variables, in­

�

strumental variables can be used to estimate

r

how the distribution of an effect will respond
to a manipulation of its causes, even in the
presence of unmeasured common causes (con­
founders).

f

In typical instrumental variable

1

s

z----.x ---•Y

estimation, instruments are chosen based on
domain knowledge.

fy

There is currently no

statistical test for validating a continuous
variable as an instrument.

In this paper,

we introduce the concept of semi-instrument,
which generalizes the concept of instrument:

Figure 1: One Instrumental Variable

each instrument is a semi-instrument, but the
converse does not hold.

We show that in

the framework of additive models, under cer­
tain conditions, we can test whether a vari­
able is semi-instrumental. Moreover, adding
some distribution assumptions, we can test
whether two semi-instruments are instrumen­

tal.

We give algorithms to test whether a.

variable is semi-instrumental, and whether
two semi-instruments are both instrumental.
These algorithms can be used to test the ex­
perts' choice of instruments, or to identify in­
struments automatically.

random variable
on

X

X

is the cause of Y, then intervening

will change Y, but intervening on Y should not

change X. 1 By regressing Y on
effect of the intervention of

X

X,

we can estimate the

on Y, up to a constant,

if no unmeasured common cause of

X

and Y exists.

However, this advantage becomes problematic if we
know, or even suspect, that there are some unmea­
sured common causes of

X

and Y, in which case we

cannot estimate the direct effect of

X

on Y.

Consider the the causal structure illustrated in figure
1. Suppose that Z is not observable for the moment.

Key Words: Causality, Instrumental Variables

Among

X,

Y, Ex, and fy, only

X andY

are observed

variables. The functional relations among them are:

1

INTRODUCTION

One of the major advantages of knowing the causal
relations among the variables we are interested in is
that we can use the causal structure to predict the
effects of interventions. For example, if we know that a

1 Here by intervention we mean the manipulation of the
value of one random variable, with the assumption that
this manipulation can affect other variables only through
the manipulated variable. We also assume no feedback,
i.e., no directed cycles, and no reversible causal relations,
in a causal graph. See Spirtes et al (2001)

CHU ET AL.

84

2

UAI2001

PRIOR WORK

There has been intensive study of the use of instru­
ments in the econometric literature. Much work has
focused on the study of the efficiency of an instrument

h

(Nelson et al 1990, Shea 1997, and Zivot et al 1998).

�

...._

instrument

l

s

X

z

strument for

ey

r

f

Z1 is an in­
X and Y, provided we already have an
Z2 for X and Y (Wu 1973, Newey 1985,

There are studies of whether a variable

and Magdalinos 1994), but how to find the first in­
strument is still an open problem.

There is no test

available to find out whether a continuous variable is

y
___.;t(

an instrument, without knowing that some other vari­
4

able is an instrument.

In practice, people usually resort to domain knowledge

g

to determine whether a variable is an instrument. As
some studies show, moreover, it is often very difficult
to find instruments, and a wrong choice may signifi­
cantly affect the final result (Bartels 1991, Heckman

Figure 2: One Common Cause

1995, and Bound et al 1995). Therefore, even in the
case where we do have some domain knowledge to help
us identify instruments, some kind of testing procedure
is still useful in that it can serve as a double check. It

X
Y
E[t:Yitx]
Note that

ey

X

and

(1)
(2)

f(Z) +ex
s(X) +ey
h(Ex)

=

(3)
E[EyiX]
>.(X). The

are dependent, that is,

will be a non-constant function of

X,

say,

regression of Y on X will give:

E[YIXJ

=

s( X)

+

E[EyjX]

=

s(X),

X- E[XIZ].

X

on

Z

3

APPROACH

W ithin the framework of linear models, the fact that a
variable is an instrument imposes no constraint on the
Consider the causal structure illustrated in figure 1,

even up to a constant.

However, with the help of variable
Ex by regressing

variable is an instrument.

joint marginal distribution of the observed variables.

s( X) +>.( X)

Because we have no way to estimate >.(X), we also
have no way to identify

is the goal of this paper to find out whether, under
certain reasonable conditions, we can test whether a

Z, we can estimate

where only

Z, X, and Y

to get an estimate of ex =

Then, we can regress Y on X and EX to

=

= s(X)

s(X) + E[EY!Ex]

+ E[t:yiEx,X]
=

s(X) + h(Ex). 2

where f and

An additive regression method will give an estimate of
and h(Ex) simultaneously. 3

s(X)
Here

Z

is called an instrumental variable for

Y, by which we mean that
and

X.

Z

Z

X

and

is a direct cause of

is independent of all other causes of

Y

X,

except

(Note that X must be a direct cause of Y.)
2Here we use the fact that X and

conditional on
figure 1.

Ex,

In

particular, we assume that:

get:

E[YIX,t:x]

are observable. Assume for

now that all the functional relations are linear.

Ey

are independent

which is implied by the causal graph in

3For the proof of the identifiability of
constant, see Newey et a! (1999).

s(X),

up to a

s are

X

=

fZ+cx

Y

=

sX +t:y

(4)
(5)

non-zero constants.

Now we construct another model based on the causal

2. Let
g'
t:x. Let Z'

structure illustrated in figure

81

=

s-

g'
7' and ey

=

Ey+

y

g'
=

=

c

=/:- 0,

z, X1 =X,

4Pearl (1995) shows that, for discrete data, Z being an
instrument for X and Y imposes some constraint, which is
called by him as instrumental inequality, on the joint dis­
tribution of Z, X, andY. This inequality is not sufficient
for Z to be an instrument, and, as pointed in this paper, it
cannot be extended to the continuous models. This paper
does not give an instrument testing procedure for discrete
data.

Y1
gf Z' + s1 X1 + f�. Clearly Y1
Y, hence
(Z,X,Y) has the same distribution as (Z1,X1,Y1).
However, we notice that in the first case, Z is an in­
strument for X and Y, while in the second case, Z1 is
not an instrument, but a common cause of X' and Y1•
Actually, we can set g' to be any value, and adjust s1
X and Y1 Y. This
and E� accordingly so that X'
and

=

=

=

=

means that, a joint distribution implied by a model

where

85

CHU ET AL.

UAI2001

Z

is an instrument could also be implied by

uncountably many other models where

Z

is not an in­

strument. Based on the joint distribution, we cannot
tell whether

Z

figure represents the correct model.
Note that a linear model is a special case of the ad­
ditive model. Thus, given that we cannot determine
whether a random variable is an instrument in a linear
model, we could not, in general, determine whether it
is an instrument in an additive model. Therefore, we
should look for some other conditions, preferably a lit­
tle bit weaker than those required by an instrument, so
that a random variable that satisfies these conditions
can be identified in an additive model.
One such candidate set of conditions is what we call

is an instrument.

The above analysis suggests two possible approaches
for instrument testing. First, we may extend the space
of linear structural models to a larger space. The idea
is that the space of linear structural models already im­
p oses strong constraints on each variable so that being
an instrument does not impose any extra constraints.

However, i n a larger space, it is possible that being
an instrument does imply some conditions not neces­
sarily satisfied by all models in that space. Another
approach is to get more candidates for instrumental
variables, and see whether the instrument assumption
imposes some constraint on the joint distribution of
these candidate variables.
In this section we shall try a combination of both ap­

the semi-instrumental conditions. A random variable
is a

Z

semi-instrument

for

X

2. Z is an exogenous variable in M;
3. Z is the cause of X, and X is a cause of Y in M;

4. If Z is also a cause of Y, then the direct effect of
Z on Y is a linear function of the direct effect of
Z on X.
Note that if a random variable

ment variables. In the first stage of the test, we shall
exclude some, but not necessarily all, non-instruments.

g(Z),

In the second stage of the test, we shall determine

X, say,

whether all the remaining ones are instruments, pro­

if the following

1. The joint distribution of Z, X, andY can be rep­
resented by an additive model M consisting only
of Z, X, and Y as the observed variables;

for X and

vided we still have at least two candidates left.

Y

conditions are satisfied:

proaches. That is, we shall propose a two-stage proce­
dure. Suppose we are given a set of candidate instru­

and

Y,

Z is

a semi-instrument

then the direct effect of

Z

on

Y,

say,

is a linear function of the direct effect of Z on

f(Z). That is, there is a pair of real numbers
that g(Z) = af( Z ) +b. We shall call a the
linear coefficient of the semi-instrument Z.
(a, b) such

It is easy to see that the semi-instrumental assump­
3.1

SEMI-INSTRUMENTAL VARIABLES

tion is weaker than the instrumental assumption: All

FOR THE ADDITIVE

instruments are semi-instruments (with a linear coeffi­

NONPARAMETRIC MODELS

cient 0), but not all semi-instruments are instruments.
Moreover, in general, in a linear model, an exogenous

Here by an additive nonparametric model we mean

Z that is a common cause of X

that, for each endogenous variable, its expectation

be an instrument, is a semi-instrument for X andY,

and Y, which could not

given its parents is a linear combination of univari­

because both its effect on X, i.e., f, and its effect on

ate functions of its parents, plus some error term with

Y,

unknown distribution. We further assume that all ex­

ing the space of linear models to the space of additive

i.e., g, are linear functions. Therefore, by extend­

ogenous variables are independent, and that they are

models, we find that the instrument assumption does

independent of all the error terms. We do allow de­

impose some constraints on the possible kinds of ef­

pendence between the error terms of

X

and

Y.

Later

fects of

Y

Z

on Y: Only models where the effect of

Z

on

nonparametric model interchangeably.

compatible with the distribution implied by a model

Figure 1 and figure

2

give two very simple additive

models. We can take them

as

two alternative hypothe­

ses about the underlying model that generates a sam­
ple with observable variables
lem of testing whether

Z

Z,

X, and Y. The prob­

is an instrument for X and

Y is equivalent to the problem of determining which

where

is a linear function of the effect of

Z

in this paper we shall use additive model and additive

Z

on X are

is an instrument.

To test whether a random variable

Z

is a semi­

instrument for X and Y, theoretically we should check
whether all the four conditions are satisfied.

How­

ever, it turns out that not all these four conditions are
testable.

For example, the second condition, i.e., Z

CHU ET AL.

86

UAI2001

is an exogenous variable, cannot be tested if we only
have the joint distribution of

X, Y,

and

Z.

From the

joint distribution only, there is no way to tell whether

Z

and the error term associated with

X,

say,

t:x,

dependent.
On the other hand, we do have many cases where it is
reasonable to assume that the first three conditions are
satisfied. For example, it is typical that when testing
whether

Z

is an instrument for

X and

Ey

r

are

Y, we are deal­

1

s

---•Y

ing with an additive model described by conditions 1
to 4, and our question is whether

Z

is also a direct

cause ofY.
Assuming that the first three conditions are satisfied,
under certain smoothness conditions, to test whether
the fourth condition is also satisfied is equivalent to
testing whether

E[YjX, t:x] is a linear combination of
X and a univariate function

Figure

a univariate function of

3:

Two Common Causes

of t:x:
Theorem 1

Consider the additive model given by fig­
ure 2. Suppose that (X, t:x) has a joint density, and
that all the functions, i.e., J, g, s, and h, are differen­
tiable. 5 Then E[YIX, t:x] is a linear combination of
a univariate function of X and a univariate function
oft:x, and Var(YjZ,t:x) = Var(YIE[XIZ],t:x), if and
only if g(Z) = af(Z) + b for some pair of real numbers
(a, b).

4. If

The second step is the measurability test for the null
hypothesis that Var ( Y j Z, t:x)
1. Regress

X

2. Regress

Y on Z and t:x

The above theorem suggests an algorithm, which we

semi-instrument testing algorithm,

will call the

Z

3.

to test

test has two steps, the first step is the additivity test

4.

2. Regress

to estimate

Yon X and Z

t:x =X� E[XjZ]

with a surface smoother,

and score the fit of this model with some score
function that penalizes model complexity.

3.

Regress

Y

on

X

and

t:x,

with a surface smoother,

t:x

with an additive

smoother, and score the fit of this model with
some score function that penalizes model com­
plexity.
s
These two conditions are much stronger than what we
need. Actually, we only need to assume the boundary of
the support of the (X, t:x ) has no positive probability, (see
proof of Theorem 2.3 in Newey et a! (1999)), and that all
the functions are absolutely continuous. We choose these
two conditions because they are more intuitive.

Regress Y on

E(XIZJ

and

t:x

with a surface

If the regression of Y on

Z

and fX has a smaller

hypothesis. Otherwise, accept it. 6

3.2

X on Z

and

sum of residuals, i.e., RA < RN, reject the null

X and a univariate

t:x:

Regress

E[XjZ]

to estimate

uals.

for the null hypothesis thatE [Yj X, t:x] is a linear com­
bination of a univariate function of

1.

Z

smoother, and let RN be the sum of the resid­

is a semi-instrument for a sample S gener­

ated from an additive model given by figure 2. This

function of

on

= Var(YIE[XIZJ, t:x):

and let RA be the sum of the residuals.

For the proof, see the appendix.

whether

the additive model has a better score, accept

the null hypothesis. Otherwise, reject it.

TWO SEMI-INSTRUMENTS

If the test for whether a random variable, say, zl' is a
semi-instrument for

X andY

gives a negative result,

there is not much left to do with

Z1.

However, if the

test says that zl is a semi-instrument, we will face
another problem: Is

Z1

an instrument?

We have pointed out that this question cannot be an­
swered if we only have the joint distribution of

Z1, X,

andY. However, from the Bayesian point of view, with
some further assumption, if there is a second semi­
instrument, say, z2' we might be able to determine
6
Here we might also want to do a bootstrap estimation
of the distribution of RN- RA. If we adopt this approach,
we could generate bootstrap samples by adding permuta­
tions of the residuals obtained by regressing Y on E[XID]
and ex to the fitted values ofY obtained from the same
regression.

87

CHU ET AL.

UAI2001

whether Z1 and Z2 are both instruments. (If not both
of them are instrument, we would not be able to tell
whether both are non-instrumental, or only one is non­
instrumental. ) The following theorem gives the condi­
tion when two semi-instruments are both instruments
almost surely.
Let Z1 and Z2 be two independent ran­
dom variables that are both semi-instruments for X
and Y. Let a1 and a2 be the linear coefficients of Z1
and Z2 respectively. Suppose a1 and az are indepen­
dent, and each has a distribution function that has one
and only one point of discontinuity, 0. 7 If Z1 and Z2
have the same linear coefficients, then with probability
1, Z1 and Z2 are both instruments.

Theorem 2

For the proof, see the appendix.
Assume that the sample S was generated from the
causal structure illustrated in figure 3, and that both
Z1 and Z2 are semi-instruments for X andY. We can
use the following algorithm, called the double instru­
ments testing algorithm, to test whether Z1 and Z2
have the same linear coefficient: 8
1. Create a new variable Z ft (Z1) + h(Z2), where
ft(Zt) == E[XIZ1], !2(Z2) = E[XIZ2].
=

2. Test whether Z is a semi-instrument.
This algorithm is based on the following observation:
Assume that g1 and gz are differentiable, and that Z1
and Z2 have a joint density. Then g1(ZI) + 92(Z2) =
a(ft(Z1) + h (Z2)) + b for some (a,b) iff g1(Z1) =
aft (Z1) + b1 and g2(Z2) = af2(Z2) + b2 for some
b1 + b2 =b. 9

testing and double instrument testing. It turns out
that, in order for the semi-instrument testing to
work, we need to find a better additive regression
method that can handle dependent predictors, which
seems currently not available. However, the double­
instruments testing algorithm does work for a subset
of additive models: the models where the influence of
X on Y is linear. This subset includes a very impor­
tant class of models: the linear models. 10
4.1

SEMI-INSTRUMENT TESTING

The semi-instrument testing algorithm requires a sur­
face smoother and an additive smoother. We use the
Splus functions loess as the surface smoother, and
gam as the additive smoother. The gam function im­
plements the back-fitting algorithm proposed in Hastie
et al (1990). The loess function is an implementation
of the local polynomial regression. We use BIC score
function to score the fitted models returned by gam
and loess respectively.
We generate 6 samples from the following 2 models:

X = Z2 +t:x
2
Y; = X + ciZ3
where E[t:ylt:x]

=

�:i-, c1

1: gam

and

SIMULATION

size
200
1000
5000
200
1000
5000

We have done some simulation studies to estimate the
performance of the two algorithms for semi-instrument
7
Note that here by imposing a distribution on a1 and
a2, which are actually parameters of our models, we have
adopted a Bayesian perspective. Also, the conditions for
the distribution are stronger than required. What we really

need is to ensure that the P(at == a2) = P(at = a2 = 0) >
0. That is, we want to assume that it is possible that
a1 = a2, and if a1 = a2, it is almost sure that a1 = a2 = 0,
which means that both Zt and Z2 are instruments.
8
Note that from the classical point of view, this algo­
rithm can be used to reject the null hypothesis that Zt and
Z2 are both instruments in the cases where Zt and Z2 have
different linear coefficients. However, to make it a testing
algorithm for double instruments, a certain Bayesian as­
sumption about the prior distributions of the linear coeffi­
cients of Zt and Z2, like the one proposed in Theorem 2,
is required.
9
The proof of this observation is similar to that of The­
orem 1.

=o:

0, and c2 = 1.

These two models share the same Z, t:x, X, and fy,
differ in the effect of Z on Y, and hence differ in Y.
In the first model, Z is an instrument, hence a semi­
instrument. In the second model, Z is not a semi­
11 For each model, we generated 3 sam­
instrument.
ples with sizes 200, 1000, and 5000 respectively.
Table

4

+ Ey

model
1
1
1
2
2
2

loe ss
gam

models comparison

BIC
486.8
2530.3
12438.3
350. 9
1404. 6
6670.7

loess

BIC

239.4
1041.8
5053.4
238.8
1041.8
5053. 4

From the above data, we can see that the gam model is
10

The second algorithm works for the models where the
inftuence of X on Y is linear because in this case we can
modify the algorithm so that we do not need to apply ad­
ditive regression method to models with dependent predic­
tors. For a detailed discussion, see section 4.2. ·
11
The distribution of these variables are: Z is uniform

between 0 and 5. There is also a latent variable T that is
uniform between 0 and 2. EX is the sum ofT and a normal
noise with standard deviation 0.5, Ey is the sum of T2 and
a normal noise with standard deviation 0.5.

CHU ET AL.

88

always much worse than the loess model, no matter
whether

Z

to get

is a semi-instrument or not. This implies

2. Let Z

that no matter whether the null hypothesis is true,

Z

i.e.,

The most plausible explanation of

tx

and

X,

the performance of gam is signif­
4.

with some further conditions, we could still make the
double instruments testing algorithm work. Consider

Zr
Z2 are semi-instruments, we further assume that
s(X) = eX + d, i.e., the direct effect of X on Y is
a linear function in X, then we will be able to test
whether Z1 and Z2 have the same linear coefficients.
the rnodel given by figure 3. If besides assuming

and

a2h(Z2) + b2,

and do

=eX+d + ty

+

Y

Z1, Z2,

where

Z1, Z2

Y

=

Let Z

=

!t(Zt) + h(Z2),

Var(YIZr , Z2)

=

� Var ( ctx + ty)

+

are

ar

ctx

+

models:

E[tyJtx]

+ Ey
=

t�, and Ct

=

0, e2

=

0.2, and C3

=

1.

€y'

Z2, tx, X, and
z 1 and z2'
model, Z1 and Z2

but differ in the linear coefficients of

and hence differ in

Y.

In the first

have the same linear coefficients. In the second model,
there is a small difference in the linear coefficients. In
13

For

Table 2: Values of BICa -BlOt for the 12 samples
sample size

Ct =

50

11.35

0

C2 =

0.2

C3

=

1

8.79

-95.57

100

14.23

1.60

-333.81

200

13.07

-2.53

-419.26

500

19.10

-35.56

-1233.05

The entries of the above table are the values of BIC1BIGa for each sample.

Var (ar !t (Zt) + a2!2(Z2)jZ )

A positive

value means that the

null hypothesis is accepted for that sample. From the
table, we can see that to detect the significant differ­

a1

=

ence between the linear coefficients of

a2.

double instruments testing algorithm,

Zt

and

Z2,

a

sample of size 50 is sufficient. But when the difference

The following algorithm, which is called the

linear

is small, we need a sample size of 200.

compares the

mean square error of regressing Y on

Z1

Z2,
Z =

and

with the mean square error of regressing Y on

DISCUSSION AND FUTURE

5

WORK

It can be used to test whether the two

semi-instruments Z1 and Z2 have the same linear coef­

ficients, assuming the direct effect of
in

3

it is easy to see that:

with the equality holds only when

E [X IZt , Z2].

have the same linear coefficients if

These three models share the same Z r,

ty + do.

Var(YjZ)

=

where

=

Var(etx + ty)
+

Z2

Yi = X + CiZi

we further have:

(c + ai)[ft(Zr) + h(Z2)]

and

X= z; + Zi +Ex

r:y.

If Z1 and Z2 have the same linear coefficient, i.e.,

a2, 12

using additive nonpara­

200, and 500 respectively.

independent of each other, and jointly independent of
r:x an

Z2

each model, we generated 4 samples with sizes 50, 100,

9r(Z l) + 9 2(Z2)

is additive in

and

the third model, the difference is significant.

= (c + at)ft(ZI) + (c + a2}h(Z2) + ctx + ty + do.
That is,

Z1

from the following

Despite the lack of good additive regression method,

Y

Z1

on

To test the above algorithm, we generated 12 samples

DOUBLE INSTRUMENTS TESTING

=

Y

BIC1 < BICa.

and in some cases fails to converge.

92(Zr) = atfr(Zr) + br, g2(Z2)
= br + b2 + d we have:

Regress Yon Z using linear

BICa.

back-fitting algorithm often gets trapped in a plateau,

Let

E[XJZr, Z2].

metric regression, and compute the BIC score

icantly worse than that of loess. It seems that the

4.2

=

3. Regress

this phenomenon is that because of the dependence
between

E[XJZr, Z2].

regression, and compute the BIC score BIC1.

is a semi-instrument, the test procedure will

always reject it!

UAI2001

X

on Y is linear

X:

5.1

THREE ASSUMPTIONS

The semi-instrument testing algorithm assumes that
the first three semi-instrumental conditions are sat-

1.

Use additive regression to regress X on Zt and

Z2

2
1 Note that by Theorem 2, under certain distribution
assumptions, a1 = a2 implies that a1 = a2 = 0 w.p.l., i.e.,

both zl and z2 are instruments.

13The distribution of these variables are: Z1 and Z2 both

are uniform between 0 and 4. There is also a latent variable

T that is uniform between 0 and 2.

€X

is the sum ofT and

a normal noise with standard deviation 0.5, €y is the sum
of T2 and a normal noise with standard deviation 0.5.

UAI 2001

CHU ET AL.

isfied. While in general we cannot test whether
a random variable satisfies all the first three semi­
instrumental conditions, it is interesting to know
whether we can test for one of them, especially the
second semi-instrumental condition: Z is an exogenous
variable. The answer is: in principle, this assumption
can be tested by the method of instrument, if we have
an instrument for Z and X. However, it is easy to see
that this will lead to an infinite regression.
Another assumption key to the double instruments
testing algorithm is: The prior probability that Z is
instrumental is positive, while the prior probability is
0 for a semi-instrument to have linear coefficient a if
a -::/::- 0.
This raises a question: Why does the value
0 have a special status in the range of possible values
of the linear coefficients of a semi-instrument? Here
we want to give an argument for the plausibility of
this assumption: If we take the set of possible causal
structures among X, Y, Z1 and Z2 as a discrete sample
space, it is reasonable to assign a positive prior prob­
ability to one element in the space, i.e., the structure
where both Z1 and Z2 are instruments, which means
that both Z1 and Z2 have linear coefficients 0. On the
other hand, if a semi-instrument is not an instrument,
there is no specific reason to believe that its linear co­
efficient should take any specific non-zero value.
We make a third assumption in an effort to modify the
double instruments testing algorithm so that it has suf­
ficient power: We assume that the direct effect of X
on Y is a linear function of X. We notice that this is a
rather strong assumption for an additive model. More­
over, because currently we do not have a suitable addi­
tive regression method for the semi-instrument testing,
we also have to assume, without any testing, that Z1
and Zz are semi-instruments. Nevertheless, the mod­
ified double instruments testing algorithm is general
enough to provide a double instruments test for linear
models.
5.2

-;

FUTURE WORK

To make the semi-instrument testing powerful, we will
continue to look for some additive regression method
that is suitable for the case where the predictors are
dependent. 14 Alternatively, we may also try to find
some new ways of testing semi-instruments where the
problem of the dependence of the predictors will not
significantly affect the test results.
14
Tom Minka suggested that by letting the back-fitting
algorithm run for a sufficient number of iterations, it will
eventually return a good fitted model, or we can use least

squares to the the best fitted model directly ( without iter­
ations ) . We have yet to test whether this will work.

89

Acknowledgments

This paper was supported by NSF grant DMS9873442.
Reference

[1] Bartels, L. (1991), "Instrumental and 'Quasi­
Instrumental' Variables," in American Journal of Po­
litical Science, 35, 777-800.
[2] Bound, J., Jaeger, D., Baker, R. (1995), "Prob­
lems with Instrumental Variables Estimation When
the Correlation Between the Instruments and the En­
dogenous Explanatory Variable is Weak," in Journal
of the American Statistical Ass oc iat ion, Vol. 90, 443450.
[3] Fennessey, J., d'Amico, R. (1980), "Collinearity,
Ridge Regression, and Investigator Judgment," in So­
ciological Methods and Research 8, 309-340.
[4] Gozalo, P., Linton, 0. (2001) "Testing Additiv­
ity in Generalized Nonparametric Regression Models
with Estimated Parameters," forthcoming in Journal
of Econometrics.

[5] Hastie, T., Tibshirani, R. (1990), Generalized Ad­
ditive Models. New York : Chapman and Hall.
[6] Heckman, J. (1995), "Instrumental Variables: A
Cautionary Tale," NBER Technical Working Paper
No. 185.

[7] Magdalinos, M. (1994), "Testing Instrument Ad­
missibility: Some Refined Asymptotic Results", in
Econometrica Vol. 62, 373-403
[8] Nelson, C., Startz, R. (1990), "The Distribution of
the Instrumental Variables Estimator and Its t-Ratio
when the Instrument is a Poor One," in Journal of
Business, vol. 63, S125-S140.
[9] Newey, W. (1985), "Generalized Method of Mo­
ments Specification Testing," in Journal of Economet­
rics, Vol. 29, 229-256.
[10] Newey, W., Powell, J., Vella, F . (1999), "Nonpara­
metric Estimation of Triangular Simultaneous Equa­
tions Models," in Econometrica Vol. 67, 565-603.

[11] Pearl, J. (1995) "On the Testability of Causal
Models with Latent and Instrumental Variables," in
P. Besnard and S. Hanks (Eds.), Uncertainty in Ar­
tificial Intelligence 11, 435-443. San Francisco, CA:
Morgan Kaufmann
[12] Shea, J. (1997), "Instrument Relevance in Multi­
variate Linear Models: A Simple Measure," in Review
of Economics and Statistics Vol. 79, 348-352.
(13] Spirtes, P., Glymour, C. and Scheines, R. (2001).

CHU ET AL.

90

Causation, Prediction, and Search,

2nd ed. New York,

N.Y.: MIT Press.

[14] Wu, D. (1973), "Alternative Tests oflndependence
Between Stochastic Regressors and Disturbances," in
Econometrica Vol. 41, 733-750.
[15] Zivot, E., Startz, R., Nelson, C., (1998), "Valid
Confidence Intervals and Inference in the Presence of
Weak Instruments," in International Economic Re­
view, Vol. 39, 1119-44.
Appendix
Proof of Theorem 1 .

Because (X, Ex) has a joint density, and j, g, s , and h
are differentiable, it immediately follows that E[Y]X =
x, Ex = u] is differentiable with respect to X and t:x
with probability one.
Also note that E[Y]X

=

x, Ex =u]

=E[s(X) +g(Z)) +t:y]X =x, EX = u]
=

s(x) +E[g(Z)]j(Z)

=

x

�

u]

+

h(u),

because conditional on f(Z) =X� Ex =x
is independent of X =x and t:x =u

�

u,

g(Z)

It is easy to see that if g(Z) aj(Z) +b, i.e., the direct
effect of Z on Y is a linear function of the direct effect
of Z on X, then with probability one, E[Y]X, t:x] =
s(X) +aX + h(Ex) � at:x + b, i.e., E[Y]X, t:x] is a
linear combination of a univariate function of X and
a univariate function of t:x. Moreover, we have:
=

Var(Y]E[X]Z], Ex)
=Var(s(j(Z) +EX) +g(Z) +Ey lf(Z), Ex )
=

=

Var(t:ylf(Z), ex)

=

Var(t:yjt:x)

Var(Ey]Z,t:x) = Var(Y]Z,t:x)

To show the converse, suppose:
E[Y]X

=

x,t:x =u]

Let g0(x� u)

=

=

s1 (x) +h1(u).

E[g(Z)]f(Z)

= x

�

] we have:

u ,

8go(x u)
1
d
= 9o(x� u)
d (sl ( x) � s(x))
x
ax
:::::::> nb ( x� u) is constant in u
�

=

:::::::>

ub is a constant

:::::::>

go is a linear function

Note that the assumption that Var(YjE[XjZ], t:x) =
Var(Y]Z,ex) implies that Var(g(Z)]j(Z)) = 0, which
again implies that g(Z) E[g(Z)]j(Z)] w.p.l. There­
fore, we have:
=

g(Z)

=

af(Z) +b, where a, bare constants.

UAI2001

Proof of Theorem 2 .

Let L1 be the linear coefficient of Z1 , L2 the linear
coefficient of Z2, and f-lL, the distribution of £1. Then:

P(L t
P(Lt

=

=

£2

=

O]Lt

=

L2, L1 =/: 0)

£ 2)

=

=

P(Lt L2 0)
= 1, for:
P(L1 L2)
=

fn�.\{O} P(L2

=

=

=

lt)df-lL1 (lt)

=

0




An important task in data analysis is the
discovery of causal relationships between observed variables. For continuous-valued data,
linear acyclic causal models are commonly
used to model the data-generating process,
and the inference of such models is a wellstudied problem. However, existing methods
have significant limitations. Methods based
on conditional independencies (Spirtes et al.
1993; Pearl 2000) cannot distinguish between
independence-equivalent models, whereas approaches purely based on Independent Component Analysis (Shimizu et al. 2006) are
inapplicable to data which is partially Gaussian. In this paper, we generalize and combine the two approaches, to yield a method
able to learn the model structure in many
cases for which the previous methods provide answers that are either incorrect or are
not as informative as possible. We give exact graphical conditions for when two distinct
models represent the same family of distributions, and empirically demonstrate the power
of our method through thorough simulations.

1

INTRODUCTION

In much of science, the primary focus is on the discovery of causal relationships between quantities of interest. The randomized controlled experiment is geared
specifically to inferring such relationships. Unfortunately, in many studies it is unethical, technically extremely difficult, or simply too expensive to conduct
such experiments. In such cases causal discovery must

Gustavo Lacerda
Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA, USA
Shohei Shimizu
Osaka University
Japan

be based on uncontrolled, purely observational data
combined with prior information and reasonable assumptions.
In cases in which the observed data is continuousvalued, linear acyclic models (also known as recursive
Structural Equation Models) have been widely used
in a variety of fields such as econometrics, psychology,
sociology, and biology; for some examples, see (Bollen
1989). In much of this work, the structure of the models has been assumed to be known or, at most, only
a few different models have been compared. During
the past 20 years, however, a number of methods have
been developed to learn the model structure in an unsupervised way (Spirtes et al. 1993; Pearl 2000; Geiger
and Heckerman 1994; Shimizu et al. 2006). Nevertheless, all approaches so far presented have either required distributional assumptions or have been overly
restricted in the amount of structure they can infer
from the data. In this contribution we show how to
combine the strenghts of existing approaches, yielding
a method capable of inferring the model structure in
many cases where previous methods give incorrect or
uninformative answers.
The paper is structured as follows: Section 2 precisely
defines the models under study, and Section 3 discusses existing methods for causal discovery of such
models. In Section 4 we formalize the discovery problem and give exact theoretical results on identifiability.
Then, in Section 5 we introduce and analyze a method
termed PClingam that combines the strenghts of existing methods and overcomes some of their weaknesses,
and is, in the limit, able to estimate all identifiable
aspects of the underlying model. Section 6 provides
empirical demonstrations of the power of our method.
Finally, Section 7 maps out future work and Section 8
provides a summary of the main points of the paper.

2

a

LINEAR MODELS

In this paper, we assume that the observed data has
been generated by the following process:
1. The observed variables xi , i = {1 . . . n} can be
arranged in a causal order, such that no later variable causes any earlier variable. We denote such
a causal order by k(i). That is, the generating
process is recursive (Bollen 1989), meaning it can
be represented graphically by a directed acyclic
graph (DAG) (Pearl 2000; Spirtes et al. 1993).
2. The value assigned to each variable xi is a linear
function of the values already assigned to the earlier variables, plus a ‘disturbance’ (noise) term ei ,
and plus an optional constant term ci , that is
X
bij xj + ei + ci ,
(1)
xi =
k(j)<k(i)

where we only include non-zero coefficients bij in
the equation.
3. The disturbances ei are all continuous random
variables with arbitrary densities pi (ei ), and the ei
are
Q independent of each other, i.e. p(e1 , . . . , en ) =
i pi (ei ).
This formulation neither requires the disturbances to
be normally distributed nor does it require them to
have non-Gaussian (non-normal) densities. In general,
some of the distributions can be Gaussian and some
not, and we do not a priori know which are which.
We assume that we are able to observe a large number of data vectors x (which contain the variables xi ),
and each data vector is generated according to the
above described process, with the same causal order
k(i), same coefficients bij , same constants ci , and the
disturbances ei sampled independently from the same
distributions. Note that the independence of the disturbances implies that there are no unobserved confounders (Pearl 2000). Spirtes et al. (1993) call this
the causally sufficient case.
Finally, we assume that the observed distribution is
faithful to the generating graph (Spirtes et al. 1993),
i.e. the model is stable in the terminology of Pearl
(2000). If the model parameters are in some sense
randomly generated, this is not a strong assumption,
as violations of faithfulness have Lebesgue measure 0
in the space of the linear coefficients.
An example of such a model is given in Figure 1a. Note
that the full model consists of a directed acyclic graph
over the variables, the connection strenghts bij , the
constants ci , and the densities pi (ei ). In this example
we have chosen ci = 0 for all i, so these are not shown.

b
x

c

d

x

x

x

x

x

y

y

y

y

y

z

z

z

z

z

3

y
-2

z

Figure 1: An example case used to illustrate the concepts described in Sections 2–4. (a) A linear, acyclic
causal model for x, y and z. The data is generated as
x := ex , y := 3x + ey , and z := −2y + ez , with ex and
ey drawn from Gaussian distributions and ez from a
non-Gaussian distribution, and ex , ey and ez are all
mutually independent. Note that we show variables
with Gaussian disturbances using circles whereas variables with non-Gaussian disturbances are marked by
squares. (b) The three directed acyclic graphs over
x, y and z which all entail the same conditional independence relationships as the generating model. (c)
The three DAGs in (b) succintly represented as a dseparation-equivalence pattern. (d) The distributionequivalence pattern of the original model.

3

EXISTING METHODS

Given our set of data vectors x, to what extent can
we estimate the data generating process? Obviously,
if the number N of data vectors is small, estimation
may be quite unreliable. Therefore we will here mainly
focus on the theoretical question: To what extent (and
with what methods) can we identify the true model in
the limit as N → ∞?
The most well-known approach to inference of this
type of causal networks is based on (conditional) independencies between the variables (Spirtes et al. 1993;
Pearl 2000). When, as in our case, there are assumed
to be no hidden confounding variables and no selection
bias, one can in the large-sample limit identify the set
of networks which represent the same independencies
as the true data generating model. To illustrate, in
Figure 1b we show all three DAGs which imply the
set of independencies produced by the true model.
This set is known as the d-separation-equivalence class,
and is often represented in the form of a d-separationequivalence pattern: a partly directed graph in which
undirected edges represent edges for which both directions are present in the equivalence class (Spirtes et al.
1993), as illustrated in Figure 1c. We want to emphasize that, using conditional independence information
alone, it is impossible to distinguish between members
inside a d-separation-equivalence class because these
(by definition) represent the same set of conditional

independencies between the observed variables.
Fortunately, in many cases there is additional information available that can be used to further distinguish
between different DAGs. In particular, it can be shown
(Shimizu et al. 2006) that if all (or all but one) distributions of the error variables are non-Gaussian, it is
in fact possible to identify the complete causal model,
including all the parameters. This is possible using
a method based on Independent Component Analysis (ICA) (Hyvärinen et al. 2001). Unfortunately,
however, when two or more disturbances are Gaussian the standard method based on ICA will fail. As
an extreme example, when all disturbances are Gaussian, standard ICA-based methods return nonsense
and are not even able to find the correct d-separationequivalence class.
These considerations raise the question of whether it
is possible to combine the methods so as to obtain
robustness with respect to Gaussian distributions but
not forgo the possibility of identifying the full model in
favourable circumstances. Indeed, such a combination
is possible and is presented in Section 5. Here, we simply note that the naı̈ve solution of first running some
test and then selecting one of the two methods, will not
be optimal. Consider, for instance, our example model
in Figure 1a. Because there is more than one Gaussian error variable the standard ICA-based method
(Shimizu et al. 2006) is not applicable, and hence one
would have to settle for the d-separation-equivalence
class (Figure 1c) given by independence-based methods. However, as we show in the next section, in this
example we can actually reject one of the DAGs in
the equivalence class and hence obtain a smaller set of
possible generating models.

4

DISTRIBUTION-EQUIVALENCE

In general, an ngDAG D is instantiated by many different models M which differ in their connection
strengths bij as well as in their distributions pi (ei ).
Next, we define the important concept of distributionequivalence between ngDAGs, which defines to what extent it is possible to infer the ngDAG which represents
the true data generating causal model, from observational data alone.
Definition 3 Two ngDAGs D1 and D2 are
distribution-equivalent if and only if for any linear acyclic causal model M1 which instantiates D1
there exists an instantiation M2 of D2 which yields
the same joint observed distribution as M1 , and vice
versa.
Distribution-equivalence partitions the set of ngDAGs
into distribution-equivalence classes, and these may be
represented using simplified graphs:
Definition 4 An ngDAG pattern representing an
ngDAG D is a mixed graph (consisting of potentially
both directed und undirected edges), obtained in the following way:
1. Derive the d-separation-equivalence pattern corresponding to the DAG in D
2. Orient any unoriented edges which originate from,
or terminate in, a node positively marked in ng of
D, in the orientation given by the DAG in D
3. Finally, orient any edges which follow from the
orientations given in the previous step and dseparation-equivalence, according to the rules derived by Meek (1995).
We say that a mixed graph is an ngDAG pattern if it
represents some ngDAG.

First, we need to extend a DAG object to include information on the non-Gaussianity of associated disturbance variables.

An ngDAG pattern is similar in many respects to dseparation-equivalence patterns. For example, we have
the following result:

Definition 1 An ngDAG is a pair (G, ng) where G is a
directed acyclic graph over a set of variables V and ng
is a binary vector of length |V |, each element of which
is associated with one of the variables of V .

Lemma 1 An ngDAG pattern is a chain graph.

Definition 2 We say that a linear acyclic causal
model M instantiates an ngDAG D (alternatively, D
represents M ) if and only if the directed acyclic graph
associated with M is equal to that specified in D, and
further if the set of variables with non-Gaussian disturbance variables in M is equal to the set of positive
entries in the binary vector specified in D.

The proof is given in the Appendix.
Our main result connects ngDAG patterns with
distribution-equivalence in mixed Gaussian and nonGaussian models in the same way that d-separationequivalence patterns are associated with distributionequivalence in purely Gaussian models:
Theorem 1 Two ngDAGs are distribution-equivalent
if and only if they are represented by the same ngDAG
pattern.

The proof of this theorem is provided in the Appendix.
The important point is that we now know exactly
which models are indistinguishable from each other on
the basis of observational data alone.

(c) Calculate the corresponding ICA objective
function
X
2
Uf =
(E{f (ei )} − k)
(2)

As a simple illustration, in Figure 1d we show the
ngDAG pattern representing the ngDAG corresponding
to the generating model of Figure 1a. Note that
the ngDAG pattern is more informative than the dseparation-equivalence pattern of Figure 1c. Nevertheless, there are still two ngDAGs (leftmost two in
Figure 1b) which cannot be distinguished based on
non-experimental data.

where k is the expected value of f applied to
a zero-mean, unit variance Gaussian variable,
i.e. k = E{f (g)}, g ∼ N (0, 1). In the ICA
literature, many different choices of f have
been utilized; here we suggest simply taking
the absolute value function f (ei ) = |ei |, giving
2
X
p
(3)
U=
E{|ei |} − 2/π

Henceforth in the paper we shall use the terms
ngDAG pattern and distribution-equivalence pattern interchangably.

5

PC-LINGAM

Although an important goal in this study was to look
at the theoretical aspects of identifying DAGs in mixed
Gaussian / non-Gaussian acyclic linear causal models, an equally significant objective is to give a practical method with which to infer models from a finite data set. Although there are a number of possible approaches, we here give a simple combination
of independence-based techniques and the ICA-based
method. The method, termed PClingam, consists of
three steps:
1. Use methods based on conditional independence
tests to estimate the d-separation-equivalence
class within which the generating model lies. In
particular, we advocate using the PC algorithm
(Spirtes et al. 1993) which is computationally efficient even for a large number of variables. Note
that, for linear models, to obtain the d-separationequivalence class it is sufficient to identify the zero
partial correlations in the data, as these depend
only on the linear coefficients and the variances
of the disturbances (and not on non-Gaussianity
aspects of the distributions). However, since the
data may well be signficantly non-Gaussian, nonparametric tests should optimally be used to find
the zero partial correlations.
2. For each DAG G in the estimated d-separationequivalence class:
(a) Estimate the coefficients bij using ordinary
least-squares regression. (Note that this provides consistent estimates regardless of nonGaussianity of the variables.)
(b) Calculate the corresponding residuals ei and
rescale them to zero mean and unit variance
for each i

i

i

Of course, since we only have samples we
have to take the sample mean rather than
the expectation.
3. Select the highest-scoring DAG Gopt from Step
2 and apply a statistical test for normality for
each of the corresponding residuals ei . Using Definition 4, compute and return the ngDAG pattern
representing the ngDAG (Gopt , ng) where ng is the
vector indicating those residuals whose normality
was rejected by the normality tests.
The objective function U is commonly used in ICA as
a measure of the non-Gaussianity of a random variable, and it can be shown to give a consistent estimator for finding independent components under weak
conditions (Hyvärinen et al. 2001). ICA estimation
is closely related to choosing the right DAG because
statistical independence of the estimated residuals is a
necessary condition for the correct model: Any DAG
for which the estimated residuals are not independent
violates the assumptions of the model (see Section 2)
and hence cannot be the data-generating DAG. On
the other hand, any DAG which results in statistically
independent residuals represents one valid model that
could have generated the data.
Note that if we could disregard sampling effects,
distribution-equivalent models would attain exactly
the same value of U . However, in the practical case of
a finite sample this is not the case, thus Step 3 in the
PClingam algorithm is required to identify the correct
distribution-equivalence class.
The method as presented above has at least a couple of
shortcomings. One is that, for any given function f (ei )
used, there always exist distributions which are nonGaussian yet are not distinguished from the Gaussian
by this measure. This is a well-known issue in ICA
which fortunately tends to have little practical significance since few such distributions are encountered in
practice. If needed, non-parametric Gaussianity measures could be used to remedy this potential problem.

a

b

2
5

-1.3

3

1.1

2

0.8

2

5

-1.4
0.5 6

c

d

2

5

3

6

e

2

5

3

6

5

3

6

3

6

0.6

4

1

4

1

4

1

4

1

4

1

Figure 2: One of the networks used in the simulations. Variables with non-Gaussian disturbances are shown in
squares, while those with Gaussian disturbances are plotted as circles. (a) True data-generating model. (b)
True d-separation-equivalence pattern. (c) True distribution-equivalence pattern. (d) Estimated DAG Gopt .
(e) Estimated distribution-equivalence pattern. See main text for details.
Naturally, in some cases many of the disturbances may
be slightly non-Gaussian yet sufficiently close to Gaussian that the available samples may not be sufficient
to distinguish the two and utilize the information for
determining causal directions in the model. Of course,
this is not a shortcoming of this particular method but
is a more general phenomenon.
Another important limitation is that the ICA objective function given above will only provide a proper
comparison of different DAGs for which the residuals
ei are linearly uncorrelated. This is guaranteed to be
the case when the search is in the correct d-separationequivalence class, but if in Step 1 of the procedure
we select a too simple model (i.e. containing too few
edges) then the estimated disturbances may be linearly correlated and the objective function misleading.
Thus, it might be wise to include a term penalizing linear correlations such as is used in maximum likelihood
estimation of ICA (Hyvärinen et al. 2001). However,
to keep our method as simple as possible, we have
omitted such a penalty term in this paper.

6

SIMULATIONS

In this section we report on simulations used to test
the performance of the PClingam method. First,
we tested the ability of the non-Gaussianity objective function (3) of Step 2 and the normality tests
of Step 3 of PClingam to identify the correct ngDAG
pattern (distribution-equivalence class) when the true
d-separation-equivalence pattern was known. In other
words, we tested how well the algorithm would function if Step 1 of the method worked flawlessly. Subsequently, we experimented with the full method incorporating the necessary estimation of the d-separationequivalence class (Step 1).
Figure 2a displays one of the models used to test the
procedure. The disturbance distributions of variables
X1 and X5 were a standard Gaussian the values of
which were squared (but keeping the original sign)

while the disturbance of X4 was produced in a similar
way but instead raising the values to the third power.
The disturbances of X2 , X3 , and X6 were Gaussian.
The disturbance variables were scaled such that their
variances ranged from 1.0 to 3.0. A sample of 1000
data vectors was generated from the model.
Figure 2b shows the true d-separation-equivalence pattern of the model in (a). The equivalence class consists
of 12 different DAGs. However, the non-Gaussianity
of the disturbances of X1 , X4 , and X5 means that
there are actually only 2 DAGs which are distributionequivalent; these are represented by the distributionequivalence pattern of Figure 2c. Figure 2d shows the
DAG Gopt found by Step 2 of PClingam from the data,
when the true d-separation-equivalence class was given
to the algorithm. An Anderson-Darling test for normality (Anderson and Darling 1954) gave the p-values
0.000, 0.3145, 0.2181, 0.000, 0.000, and 0.0197 for the
corresponding residuals e1 to e6 . Inferring a residual to be non-Gaussian when p < 0.01 in Step 3 of
the method produced the ngDAG pattern of Figure 2e,
which turns out identical to the true ngDAG pattern in
(c).
This basic procedure was repeated 20 times, with the
results summarized in Table 1a. In each simulation,
we randomly generated a linear acyclic causal model
over 6 variables, with each variable randomly chosen to have either a Gaussian or a non-Gaussian disturbance. The non-Gaussian distributions used were
those mentioned above as well as a Student’s t (2 degrees of freedom), a bimodal Mixture of Gaussians
(0.5 N (−2, 1) + 0.5 N (2, 1)), a log-normal distribution
(exponentiated standard normal) and a uniform distribution. The true d-separation-equivalence pattern was
input to the algorithm, to test the functioning of the
PClingam method when the correct pattern is selected
in Step 1. The panel shows how often a specific type of
true edge (in the true distribution-equivalence pattern)
gave rise to a specific type of estimated edge (in the estimated distribution-equivalence pattern). Rows cor-

Table 1: Summary of the simulations employing various methods for inferring the d-separation-equivalence
class in Step 1 of PClingam. Each table is a confusion
matrix of arcs in the true distribution-equivalence patterns vs arcs in the estimated distribution-equivalence
pattern. See main text for details.

a

Using the true
d-sep-equiv pattern

b

PC

185

0

0

0

183

0

0

2

0

12

2

0

0

12

2

0

0

0

61

0

9

1

43

8

0

0

0

40

3

2

6

29

c

d

CPC

GES

183

0

1

1

173

0

8

4

0

12

2

0

0

10

2

2

9

0

50

2

2

0

51

8

3

1

2

34

1

0

1

38

7

While the theoretical aspects of identifiability are
solved, at least a couple of important issues regarding the estimation of the model from finite samples
remain.
First and foremost, non-parametric methods for identifying zero partial correlations in non-Gaussian settings should be used so as to obtain better estimates of
the appropriate d-separation-equivalence class within
which to search. Although the methods developed for
Gaussian variables seem to work relatively well in our
partly non-Gaussian setting, it is likely they will be
outperformed by methods that take into account the
possibility of non-Gaussian distributions.
Another important question is how to make the procedure scalable to data involving many (tens or even
hundreds of) variables. Although the current approach
relies on a brute-force enumeration of all DAGs in the
d-separation-equivalence class, it would not be difficult to adapt the method to do a local search among
DAGs in an equivalence class. The extent to which
such a method would be hampered by local maxima is
unknown.

8

respond to the true edges, columns to estimated ones.
Optimally all off-diagonal elements would be zero. It
can be seen that the results are close to perfect; the
method misclassifies two undirected edges as directed,
but correctly estimates all others.
These simulations confirm that the PClingam method
works well at least when the d-separation-equivalence
class can reliably be estimated. But in practice, with
finite datasets, there may be significant errors in inferring the d-separation-equivalence class. The degree to
which this affects the algorithm is an important practical issue.
Thus, in further simulations, we applied several different methods for learning d-separation-equivalence
patterns from the simulated data, as Step 1 in the
PClingam method. The methods we compared were
the PC algorithm (Spirtes et al. 1993), the Conservative PC algorithm (Ramsey et al. 2006), and the GES
algorithm (Chickering 2002). Panels b-d of Table 1
summarize the results. Although all of the methods
assumed Gaussianity when learning the d-separationequivalence pattern, the results are still quite encouraging, and a clear majority of edges were correctly
estimated.

FUTURE WORK

SUMMARY

The discovery of linear acyclic causal models is a topic
which has been thoroughly investigated in the last
two decades. Both the Gaussian and the fully nonGaussian special cases are well understood, but the
general mixed case has not been previously discussed.
In this paper we have provided a complete characterization of distribution-equivalence and a practical estimation method in this setting.
Acknowledgements
The authors wish to thank Clark Glymour for helpful
and stimulating discussions. P.O.H. was funded by a
postdoctoral researcher grant from the University of
Helsinki.


Causal discovery from observational data in
the presence of unobserved variables is challenging. Identification of so-called Y substructures is a sufficient condition for ascertaining some causal relations in the large
sample limit, without the assumption of no
hidden common causes. An example of a Y
substructure is A → C, B → C, C → D.
This paper describes the first asymptotically
reliable and computationally feasible scorebased search for discrete Y structures that
does not assume that there are no unobserved
common causes. For any parameterization
of a directed acyclic graph (DAG) that has
scores with the property that any DAG that
can represent the distribution beats any DAG
that can’t, and for two DAGs that represent
the distribution, if one has fewer parameters than the other, the one with the fewest
parameter wins. In this framework there is
no need to assign scores to causal structures
with unobserved common causes. The paper
also describes how the existence of a Y structure shows the presence of an unconfounded
causal relation, without assuming that there
are no hidden common causes.

1

Introduction

Discovering causal relationships from observational
data is challenging due to the presence of observed confounders1 , particularly, hidden (latent) confounders.
∗
Currently at the Department of Biomedical Informatics, Vanderbilt University, Nashville, TN 37232-8340.
1
A node W is said to be a confounder of nodes A and B
if there is a directed path from W to A and a directed path
from W to B that does not traverse A. If W is observed,
it is said to be a measured confounder, otherwise it is a
hidden confounder.

Gregory F. Cooper
gfc@cbmi.pitt.edu
Center for Biomedical Informatics
University of Pittsburgh
Pittsburgh, PA 15213

Furthermore, it is known that members of an independence (Markov) equivalence class of causal Bayesian
network (CBN) models are indistinguishable using
only probabilistic dependence and independence relationships among the observed variables.
There are several algorithms for reliably identifying
(some) causal effects in the large sample limit, given
the assumptions of acyclicity, and the Causal Markov
and Causal Faithfulness assumptions (explained below). Due to space limitations, we mention only two
representative algorithms here. The Greedy Equivalence Search (Chickering, 2002) is a score-based search
that reliably identifies some causal effects in the large
sample limit, but only under the additional assumption of no unobserved common causes. The FCI algorithm (Spirtes et al., 1999) reliably identifies some
causal effects in the large sample limit, but it is a
constraint-based search, and such methods have several important disadvantages relative to score-based
searches (Heckerman et al., 1999)—(1) the inability
to include prior belief in the form of structure probabilities, (2) the output is based on the significance
level used for the independence tests, and (3) there
is no quantification of the strength of the hypothesis
that is output. A constraint based search performs a
sequence of conditional independence tests. A single
conditional independence test may force the removal
of an edge, or a particular orientation, even if that
makes the rest of the model a poor fit. In contrast, a
score gives a kind of overall evaluation of how all the
conditional independence constraints are met by a directed acyclic graph (DAG), and would not decrease
the overall fit to save one conditional independence.
A score based search over models that explicitly include hidden variables faces several difficult problems:
there are a potentially infinite number of such models, and there are both theoretical and computational
difficulties in calculating scores for models with hidden variables (Richardson & Spirtes, 2002). Another
approach is to use graphical models (mixed ances-

tral graphs) that represent the marginal distributions
of hidden variable models over the observed variables, but do not explicitly contain hidden variables
(Richardson & Spirtes, 2002). Mixed ancestral graph
models, unlike hidden variable models, are finite in
number, and in the linear case, there are no theoretical
or computational problems in scoring mixed ancestral
graphs. However, there is no currently known way to
score discrete mixed ancestral graphs, and there is no
known algorithm for efficiently searching the space of
mixed ancestral graphs. Finally, hidden variable models are known to entail non-conditional independence
constraints upon the marginal distribution; these constraints can be used to guide searches for hidden variable models that entail the constraints (Spirtes et al.,
1993; Tian & Pearl, 2002). However, no such computationally feasible search is known to be guaranteed
correct.

per case and random variables by upper case letters
italicized. Graphs are denoted by upper case letters
such as G or M or by calligraphic letters such as G
or M. Lower case letters such as x or z denote an
assignment to variables X or Z.

The search proposed here is the first computationally
feasible score-based search to reliably identify some
causal effects in the large sample limit for both discrete and linear models, without assuming that there
are no unobserved common causes, and without making any assumptions about the true causal structure
(other than acyclicity). There is also no need to assign
scores explicitly to causal structures with unobserved
common causes in this framework. We identify a class
of structures (Y structures) as “sufficient” for assigning causality, and provide the necessary theorems and
proofs for the claim. The proofs presented in this paper depend only upon two features of the score—(1)
a DAG that cannot represent a distribution loses in
the limit to one that can; and (2) if two DAGs with
different numbers of parameters can represent the distribution, the one with more parameters loses to the
DAG with fewer parameters.

Viral infection in
@ABC
GFED
X1
early pregnancy
:
::

:

::



Low birth @ABC
GFED
@ABC
GFED
X
X3 A Congenital
2
weight
disease
::
Aheart

AA
::

AA
::

A
 
Infant @ABC
Heart
GFED
@ABC
GFED
X4
X5
mortality

The remainder of the paper is organized as follows.
Section 2 introduces the causal Bayesian network
framework and presents the basic assumptions related
to causal discovery. Section 3 describes the V and
Y structures. Section 4 introduces the relevant theorems and proofs. Additional information on search
and the score function is provided in Appendix A and
Appendix B.

2

Background

In this section we first introduce the CBN framework.
Subsequently, we define d-separation and
d-connectivity, independence equivalence, the causal
sufficiency assumption, the causal Markov condition,
and the causal Faithfulness condition. The following
notational convention will be used throughout the paper. Sets of variables are represented in bold and up-

2.1

Causal Bayesian network

A causal Bayesian network (CBN) is a directed acyclic
graph (DAG) with the vertices representing observed
variables in a domain and each arc is interpreted as a
direct causal influence between a parent node2 (variable) and a child node (Pearl, 1991). For example, if
there is a directed edge from A to B (A → B), node
A is said to exert a causal influence on node B. Figure 1 illustrates the structure of a hypothetical causal
Bayesian network structure, which contains five nodes.
The states of the nodes and the probabilities that are
associated with this structure are not shown.

murmur

Figure 1: A hypothetical causal Bayesian network
structure
The causal Bayesian network structure in Figure 1 indicates, for example, that a Viral infection in early
pregnancy can causally influence whether Congenital
heart disease is present in the newborn, which in turn
can causally influence Infant mortality and presence of
a Heart murmur.
2.2

d-separation and d-connectivity (Pearl,
1991)

d-separation is a graphical condition. Assume that A
and B are vertices in a DAG G and C is a set of vertices
in G such that A 6∈ C and B 6∈ C. The d-separation
theorem states that if a distribution P satisfies the
Markov condition (each vertex is independent of its
non-descendants conditional on its parents) for a DAG
G, and A is d-separated from B conditional on C in
G, then A is independent of B conditional on C in G.
Consider the DAG G in Figure 1. A and B are said
to be d-separated given C iff the following property
2
If there is an arc from node A to node B in a CBN, A
is said to be the parent of B, and B, the child of A.

holds: there exists no undirected path3 U between X
and Y s.t.

Bayesian network:
In a causal DAG for a causally sufficient set of
variables, each variable is independent of its
non-descendants (i.e., non-effects) given just
its parents (i.e., its direct causes).

1. every collider4 on U is in C or has a descendant
in C.
2. no other vertex on U is in C.
Likewise, if A and B are not in C, then A and B are
d-connected given C iff they are not d-separated given
C.
In Figure 1 the nodes X1 and X5 are d-separated by
X3 . The nodes X2 and X5 are d-connected given {}.
Two disjoint sets of variables A and B are d-separated
conditional on C if and only if every vertex in A is dseparated from every vertex in B conditional on C;
otherwise they are d-connected. See (Pearl, 1991) for
more details on d-separation and d-connectivity.
2.3

Independence equivalence

Two Bayesian network structures S and S∗ over a set
of observed variables V are independence equivalent
(Heckerman, 1995) iff S and S∗ have the same set of
d-separation and d-connectivity relationships between
A and B conditional on C where A ∈ V, B ∈ V,
and C ⊂ V and A 6∈ C and B 6∈ C. Independence
equivalence is also referred to as Markov equivalence.
For Gaussian or discrete distributions, it is also the
case that two DAGs are independence equivalent if and
only if the set of distributions that satisfy the Markov
condition for one equals the set of distributions that
satisfy the Markov condition for the other (i.e. they
represent the same set of distributions).
2.4

The causal sufficiency assumption

The CMC represents the “locality” of causality. This
implies that indirect (distant) causes become irrelevant
when the direct (near) causes are known. The CMC is
the fundamental principle relating causal relations to
probability distributions, and is explicitly or implicitly
assumed by all causal graph search algorithms.
2.6

The causal Faithfulness condition

While the causal Markov condition specifies independence relationships among variables, the causal
Faithfulness condition (CFC) specifies dependence
relationships:
In a causal DAG, two disjoint sets of variables
A and B are dependent conditional on a third
disjoint set of variables C unless the Causal
Markov Condition entails that A and B are
independent conditional on C.
The CFC is related to the notion that causal events are
typically correlated in observational data. The CFC
relates causal structure to probabilistic dependence.
The CFC can fail for certain parameter values, but for
linear or discrete parameters, the Lebesgue measure
of the set of parameters for which it fails is 0. It is at
least implicitly assumed by both constraint based and
score based causal DAG discovery algorithms.

A set of variables S is causally sufficient if no variable
that is a direct cause5 of two variables in S is not in
S. In Figure 1, S = {X2 , X3 } is not causally sufficient
because X1 is a direct cause of X2 and X3 , but is not
in S. However, note that for the causal discovery approach based on Y structures introduced in this paper,
we do not assume causal sufficiency.

Based on the causal Markov condition each vertex of
a CBN is independent of its non-descendants given its
parents. The independence map or I-map of a CBN
is the set of all independencies that follow from the
causal Markov condition. The dependence map or Dmap of a CBN is the set of all dependencies that follow
from the causal Faithfulness condition.

2.5

The I-map and D-map of the causal network
W1 → X ← W2 is as follows:

The causal Markov condition

The causal Markov condition (CMC) gives the independence relationships that are specified by a causal
3

An undirected path between two vertices A and B in
a graph G is a sequence of vertices starting with A and
ending with B and for every pair of vertices X and Y in
the sequence that are adjacent there is an edge between
them (X → Y or X ← Y ) (Spirtes et al., 2000, page 8)
4
A node with a head to head configuration. C is a
collider in A → C ← B.
5
The direct causation is relative to S.

• W1 ⊥⊥W2 ; W1⊥
6 ⊥X; W1⊥
6 ⊥X|W2
• W2⊥
6 ⊥X; W2⊥
6 ⊥X|W1 ; W1⊥
6 ⊥W2 |X
2.7

Markov Blanket

The Markov blanket (MB) of a node X in a causal
Bayesian network G is the union of the set of parents
of X, the children of X, and the parents of the children

of X (Pearl, 1991). Note that the MB is the minimal
set of nodes when conditioned on (instantiated) that
makes a node X independent of all the other nodes in
the CBN.

3

V and Y structures

In this section we first define the V and Y structures
and discuss their potential role in causal discovery.
3.1

3.2

Y structure

@ABC
GFED
@ABC
@ABC
GFED
@ABC
@ABC
GFED
@ABC
GFED
GFED
GFED
@ABC
W2 GFED
W1
W2 GFED
W1
W2
W2 @ABC
W1
W1
11
11
X11
F
11
11
11
 
 
?>=<
89:;
89:;
?>=<
89:;
?>=<
89:;
?>=<
X
X
X
X
O
O

89:;
?>=<
89:;
?>=<
89:;
?>=<
89:;
?>=<
Z
Z
Z
Z
G1

V structure

A “V” structure over variables W1 , W2 and X is shown
in DAG M1 in Figure 2. There is a directed edge from
W1 to X and another directed edge from W2 to X.
There is no edge between W1 and W2 . A “V” structure contains a “collider” and the node X is a collider
in Figure 2, M1 . Since there is no arc between W1 and
W2 , X is also termed as an unshielded collider. Figure 2, M2 is a model in which there is an arc between
W1 and W2 , and thus X is a shielded collider in this
example.
@ABC
GFED
@ABC
GFED
W1 C
W2
CC
{
{
C!
{
{}
?>=<
89:;
X

@ABC
GFED
@ABC
/ GFED
W2
W1 C
CC
{
{
C!
{
{}
89:;
?>=<
X

M1

M2

Figure 2: X is an unshielded collider in M1 and a
shielded collider in M2 . M1 is also referred to as a “V”
structure.
@ABC
GFED
@ABC
GFED
W1
W2
C
8[ 8

?>=<
89:;
89:;
?>=<
H KK
sH
KKK
s
s
%
sy s
89:;
?>=<
X
Figure 3: A model that has the same dependence/independence relationships over W1 , W2 and X
as Figure 2, M1 ; H denotes a hidden variable.
The V structure is not sufficient for discovering that
some variable W1 or W2 causes X if we do not make
the assumption that M1 is causally sufficient. On the
other hand, even allowing for confounders (hidden and
measured), we can conclude that X does not cause
W1 or W2 (Spirtes et al., 2000). Figure 3 shows a
confounder for the pair (W1 , X) and the pair (W2 , X).
In other words we can make an acausal discovery but
not a causal one using the V structure.

G2

G3

G4

Figure 4: Several CBN models that contain four nodes.
G1 is a Y structure.
We now introduce the concept of a Y structure. Let
W1 → X ← W2 be a V structure. Note that X is an
unshielded collider in this V structure since there is no
arc between W1 and W2 . If there is a node Z such that
there is an arc from X to Z, and there are no arcs from
W1 to Z and W2 to Z, then the nodes W1 , W2 , X and
Z form a Y structure. A Y structure has interesting
dependence and independence properties.
If W1 , W2 , X, Z form a Y structure over a set of four
variables V and the Y structure is represented by G1
(see Figure 4), there is no DAG that contains a superset of the variables in V, entails the same conditional
independence relations over V, and in which there is a
(measured or unmeasured) confounder of X and Z, or
in which X is not an ancestor of Z (Robins et al., 2003;
Spirtes et al., 2000, page 187) that is in the same independence equivalence class as G1 . In other words, if a
Y structure is learned from data, the arc from X to Z
represents an unconfounded causal relationship. Since
G1 also has the same set of independence/dependence
relationships over the observed variables W1 , W2 , and
X as Figure 3), the arcs W1 → X and W2 → X cannot
be interpreted as causal relationships.

4

Y structure theorems

Definition 1 (Complete-table Bayesian network). A complete-table Bayesian network is one that
contains all discrete variables and for which the probabilities that define the Bayesian network are described
by contingency tables with no missing values.
Definition 2 (Perfect map). A Bayesian network
structure S is a perfect map of a distribution θ if A
and B are independent conditional on C in θ iff A and
B are d-separated conditional on C in S.
Suppose Bayesian network B defines a joint distribution θ over all the variables in B. Let S be the structure

of B. If the Markov and Faithfulness conditions hold
for B, then S is a perfect map of θ.
In the results of this section, we will only be considering complete-table Bayesian networks that satisfy the
Markov and Faithfulness conditions.
Definition 3 (Y structure Bayesian network). A
Y structure Bayesian network is a Bayesian network
containing four variables that has the structure shown
in Figure 5, where the node labels are arbitrary.
@ABC
GFED
@ABC
GFED
W2
W1 G
GG
ww
GG
w
G#
{www
?>=<
89:;
X

?>=<
89:;
Z
G1
Figure 5: A Y structure.

We will use the following notation in regard to an
arbitrary complete-table Bayesian network that satisfies the Markov and Faithfulness conditions and has
a Y structure: By denotes the network, Sy denotes its
structure, Vy denotes the four variables in the structure, Qy denotes its complete table parameters, and θy
denotes the correspondingly defined joint distribution
over the four variables.
Lemma 1. There is no other Bayesian network structure on the variables in Vy that is independence equivalent to Sy .
Proof. The proof of Lemma 1 follows from Theorem 1
given below.
Theorem 1. Two network structures Bs1 and Bs2 are
independence equivalent iff they satisfy the following
conditions (Verma & Pearl, 1991):
1. Bs1 and Bs2 have the same set of vertices.
2. Bs1 and Bs2 have the same set of adjacencies.
3. If there is a configuration such as A → C ← B
where A and B are not adjacent (“V” structure)
in Bs1 , the same pattern is present in Bs2 , and
vice-versa.

Lemma 2. Let B be a Bayesian network that contains
the fewest number of parameters that can represent the
population distribution. Let B∗ be a Bayesian network
that either cannot represent the population distribution, or can but does not contain the fewest number of
parameters. Let S and S∗ be the network structures of

B and B∗ , respectively. Let m denote the number of iid
cases in D that have been sampled from the population
distribution defined by B.
P (S∗ , D)
< 1.
m→∞ P (S, D)

Then lim

(1)

Proof. The proof of Lemma 2 follows from the results
in (Chickering, 2002), which in turn uses results in
(Haughton, 1988).
Theorem 2. Let B = (S,Q) be a complete-table
Bayesian network that contains n measured variables,
where S and Q are the structure and parameters of B,
respectively. Suppose that B defines a distribution θ on
the n variables, such that S is a perfect map of θ. Let
D be a database containing m complete cases on the
n variables in B, for which the cases are iid samples
from distribution θ. Let B∗ be a Bayesian network over
the same n variables with structure S∗ that is not independence equivalent to B. Suppose that P (S, D) and
P (S∗ , D) are computed using the BDe score with nonzero parameter and structure priors. (The BDe score
assigns the same score to members of the same Markov
equivalent class when equal structural priors are assigned; see (Heckerman et al., 1995).)
P (S∗ , D)
< 1.
m→∞ P (S, D)

Then lim

(2)

Proof. If B∗ cannot represent the generative distribution θ then according to Lemma 2 the current theorem
holds. Suppose B∗ can represent the generative distribution. Since by assumption B∗ is not independence
equivalent to B, B∗ must contain all the dependence relationships in B, plus additional dependence relationships. Therefore B∗ contains more parameters than
B (Chickering, 2002, Proposition 8). Thus it follows
from Lemma 2 that the theorem holds.
Theorem 3. Assume the notation and conditions in
Theorem 2 and suppose the number of variables is four
(n = 4). If S is the data generating structure on the
four variables and S is a Y structure, then in the large
sample limit P (S, D) > P (S∗ , D) for all S∗ 6= S (where
S∗ contains just the same 4 variables). Conversely, if
S is the data generating structure and S is not equal
to some Y structure, S∗ , then in the large sample limit
P (S, D) > P (S∗ , D).
Proof. The proof follows from Theorem 2 and
Lemma 1.
Theorem 3 shows (under the conditions assumed) that
in the large sample limit a Y structure will have the
highest BDe score if and only if it is the structure
of the data generating Bayesian network. Note that

Theorem 3 assumes that some DAG over the measured
variables is a perfect map of the observed conditional
independence and dependence relations; this is not in
general true if there are unmeasured common causes.
Lemma 2 can be strengthened using results in (Nishii,
1988, Theorem 4) to show that in the large sample limit, with probability 1 the ratio approaches 0,
rather than merely approaching some positive number less than 1.6 Correspondingly, Theorem 3 can be
strengthened to state that the data generating structure has posterior probability 1 and all other structures
have probability 0 in the large sample limit7 . This
strengthened version of Theorem 3 implies that in the
large sample limit, model averaging using Equation 3
in the appendix will derive an arc X → Z as causal
and unconfounded with probability 1, if and only if
it (X → Z) is a causal and unconfounded arc within
a Y structure of the data generating causal Bayesian
network.
The proof of sufficiency of the Y structure for ascertaining causality in the possible presence of hidden
confounders requires an understanding of the common
properties of DAGs in the same Markov equivalence
class in the possible presence of such hidden variables.
A class of structures that can represent the common
properties of DAGs in the same Markov equivalence
class are called partial ancestral graphs (PAGs). We
next describe the theorems that make use of PAGs.
A PAG has a richer representation compared to a directed acyclic graph (DAG) and makes use of the following types of edges: →, ↔, ◦→, ◦—◦.
Partial ancestral graphs
A Markov equivalence class of DAGs over a set of observed variables O is the set of all DAGs that contain
at least the variables in O and that have the same
set of d-separation relations among the variables in
O (i.e., G1 and G2 are in the same Markov equivalence class over O if for all disjoint X, Y, Z ⊆ O, X
is d-separated from Y conditional on Z in G1 iff X is
d-separated from Y conditional on Z in G2 ). A PAG
P over O is a graphical object with vertices O that
represents the Markov equivalence class of DAGs M
over O in two distinct ways:
1. A PAG represents the d-separation relations over
O in M.
6

The results in (Nishii, 1988) are based on “almost
surely” convergent proofs, which guarantee that in the
large sample limit the data will with probability 1 support
the stated convergence.
7
If there are several Bayesian networks that contain the
fewest number of parameters that can represent the data
generating distribution, then the result states that the sum
of their posterior probabilities is equal to 1.

2. A PAG represents the ancestor and non-ancestor
relations among variables in O common to every
DAG in M.
More specifically, it is possible to extend the concept of
d-separation in a natural way to PAGs so that if PAG
P represents the Markov equivalence class M over O,
then for all disjoint X, Y, Z ⊆ O, X is d-separated
from Y conditional on Z in P iff X is d-separated
from Y conditional on Z in every DAG in M. A PAG
is formally defined as stated below.
Definition 4 (PAG). The PAG P that represents a
Markov equivalence class M over O can be formed in
the following way:
1. A and B are adjacent in P iff A and B
are d-connected conditional on every subset of
O\{A, B}.
2. If A and B are adjacent in P, there is an “−”
(arrowtail) at the A end of the edge iff A is an
ancestor of B in every member of M.
3. If A and B are adjacent in P, there is an “>”
(arrowhead) at the B end of the edge iff B is not
an ancestor of A in every member of M.
4. If A and B are adjacent in P, an “o” at the A
end of the edge entails that in some DAG in M,
A is an ancestor of B and in some other DAG in
M, A is not an ancestor of B. (In (Richardson &
Spirtes, 2002), this is called a maximally oriented
PAG.)
@ABC
GFED
@ABC
GFED
W2
W1 
GG
w
GG
w
w
G#
{ww
89:;
?>=<
X

89:;
?>=<
Z
G1P
Figure 6: A Y PAG.
For example, suppose M is the Markov equivalence
class of the Y structure. It can be shown that the PAG
that represents a Y structure is in Figure 6, indicating
that for every DAG in M, the following conditions
hold:
• X is not an ancestor of W1 or W2 .
• Z is not an ancestor of X.
• X is an ancestor of Z.
• W1 and W2 are ancestors of X in some members
of M, and not ancestors of X in other members
of M.

Definition 5 (DAG PAG). For a PAG P, if there
is an assignment of arrowheads and arrowtails to the
“o” endpoints in P such that the resulting graph is a
DAG that has the same d-separation relations as P,
then P is a DAG PAG.
For example, a Y PAG is a DAG PAG because the
DAG in Figure 5 (which we will call the Y DAG) has
the same d-separation relations as the PAG in Figure 6. A DAG PAG can be parameterized in the same
way as a corresponding DAG. Every DAG PAG has
the same d-separations over the measured variables as
the DAGs that it represents. Every DAG PAG can be
assigned a score equal to the score of any of the DAGs
that it represents. The reader is referred to (Spirtes
et al., 1999) and (Spirtes et al., 2000, pages 299–301)
for additional details about PAGs. The FCI algorithm
is a constraint-based algorithm that generates a PAG
from data faithful to a DAG represented by the PAG.
A listing of the PAGs over four variables that includes
the Y PAG with a discussion of their causal implications is presented in (Richardson & Spirtes, 2003).
Definition 6 (Embedded pure Y structure). Let
B be a causal Bayesian network with structure S.
We say that B contains an embedded pure Y structure (EPYS) involving the variables W1 , W2 , X and Z,
iff all and only the following d-separation conditions
hold among the variables W1 , W2 , X and Z (A><B|C
means that A and B are d-separated conditioned on
C):
• W1 ><W2 |{}; W1 ><Z|{X}
• W1 ><Z|{X, W2 }; W2 ><Z|{X}
• W2 ><Z|{X, W1 }
Context 1. Let B be a complete-table Bayesian network involving the variables W1 , W2 , X and Z. Furthermore, let B be the data generating model for data
on just W1 , W2 , X and Z. In general, B may contain other variables, which we consider as hidden with
regard to the data being measured on these four variables.
Let θw1w2xz be the data generating distribution on the
variables W1 , W2 , X and Z that is given by a marginal
distribution of B. Suppose that every d-separation condition among W1 , W2 , X and Z in B implies a corresponding independence according to θw1w2xz (call
this the marginal Markov condition)8 . Suppose also
that every d-connection condition among W1 , W2 , X
and Z implies a corresponding dependence according
8

The marginal Markov condition is entailed by the
Markov condition, and is strictly weaker than the Markov
condition.

to θw1w2xz (call this the marginal Faithfulness condition)9 .
To summarize:
1. Let Sy denote the Y structure in Figure 5.
2. Let VSy denote the variables in Sy .
3. Let B be the Bayesian network generating the
data.
4. Let VB denote the variables in B.
5. In general VSy ⊆ VB .
6. Assume the marginal Markov and Faithfulness
conditions hold for B with respect to θw1w2xz .
@ABC
GFED
@ABC
GFED
W1 C
W2
CC
{=
{
C!
{{
89:;
?>=<
X

@ABC
GFED
@ABC
GFED
W1 aC
= W2
CC
{
{
C {{
89:;
?>=<
X

Figure 7: X is a non-collider in both the models
Definition 7 (Non-collider). A variable X is said
to be a non-collider on a path if it does not have two
incoming arcs (arrowheads). See Figure 7 for examples.
Lemma 3. If B contains an EPYS, then in the large
sample limit, the Y PAG receives a higher score than
any other DAG PAG over the same variables, with
probability 1.
Proof. If B contains an EPYS, then by the Marginal
Faithfulness condition, the marginal population distribution is faithful to the Y PAG, and hence to the Y
DAG. By Theorem 3, the Y PAG receives a higher
score than any other DAG over the same variables,
and hence any other DAG PAG.
Let PAG(B) be the PAG for Bayesian network(B) over
the set of measured variables O.
Lemma 4. If B does not contain an EPYS, and the
population distribution is faithful to PAG(B), then in
the large sample limit, with probability 1 there is a
DAG PAG P over O that receives a higher score than
the Y PAG.
Proof. Suppose that B does not contain an EPYS.
There are three cases.
Suppose first that PAG(B) contains any adjacency between some pair of observed vertices A and C that
are not adjacent in the Y PAG. It follows that A
9

The marginal Faithfulness condition is entailed by the
Faithfulness condition, and is strictly weaker than the
Faithfulness condition.

and C are d-separated conditional on some subset of
O in the Y PAG but not in PAG(B). Hence, by the
Marginal Faithfulness condition, in the population distribution A and C are dependent conditional on every
subset of O. Hence the Y PAG cannot represent the
marginal population distribution. Some DAG can represent the marginal population distribution, since a
DAG in which every pair of vertices are adjacent can
represent any distribution. Hence there is a DAG G
with the fewest number of parameters that can represent the marginal population distribution. By Lemma
2, in the large sample limit with probability 1, the Y
PAG receives a lower score than G.
Suppose next that PAG(B) contains the same adjacencies as the Y PAG, but different orientations. It is
easy to see by exhaustively considering all of the possible PAGs with the same adjacencies as the Y PAG
that each of them is a DAG PAG. Since the two different PAGs have the same d-separation relations as
some pair of different DAGs over O, we can reason
about their d-separation relations using two DAGs.
Given that they have the same adjacencies, the Y DAG
and the DAG G with the same d-separation relations
as PAG(B) are not equivalent iff they have different
unshielded colliders. Suppose that there is some unshielded collider A → D ← C in the Y DAG, but not in
G. It follows that A and C are d-separated conditional
on some subset of variables in the Y DAG, and that
every set that d-separates A and C in the Y DAG does
not contain D. In G, A and C are d-separated conditional only on subsets of variables that do contain D.
Hence there is a d-separation relation in the Y PAG
that is not in G. The case where A → D ← C is an unshielded collider in G, but not in the Y DAG, is analogous. Hence, by the marginal Faithfulness assumption
the Y PAG cannot represent the marginal population
distribution. By Lemma 2, in the large sample limit
with probability 1, the Y PAG receives a lower score
than some other DAG with the fewest parameters that
does represent the population distribution.
Finally, suppose that PAG(B) contains a proper subset of the adjacencies in the Y PAG. Inspection shows
that all of the PAGs with subsets of adjacencies of
the Y PAG are DAG PAGs. Hence, by the Marginal
Faithfulness condition, the population distribution is
faithful to some such PAG, and hence faithful to a
corresponding DAG. By Theorem 3 the DAG that the
population distribution is faithful to receives a higher
score than the Y PAG with probability 1 in the large
sample limit.

Let us consider the following example. Assume that
B contains the substructure shown in Figure 8. We
refer to this structure as Near-Y DAG or N-Y DAG

GFED
@ABC
@ABC
GFED
W1 G
W2
55 GG
w
w
w
55 GGG
{www
55 #?>=<
X
55 89:;
55
 
89:;
?>=<
Z
M3
Figure 8: A Near-Y structure.
@ABC
GFED
GFED
@ABC
W2
W1 
55GG
w
G
w
55 GG
w
{ww
55 #?>=<
89:;
55 X
55
 
89:;
?>=<
Z
M3P
Figure 9: A Near-Y PAG.

for short. The N-Y DAG has an additional arc from
W1 to Z when compared to the Y DAG shown in Figure 5.10 Note that for the N-Y DAG the d-separation
W1 ><Z|{X} does not hold and hence if B contains an
N-Y DAG, it will not be an embedded pure Y structure. According to Lemma 4 then with probability 1
there is a DAG PAG P over O that receives a higher
score than the Y PAG. Such a DAG PAG P is shown
in Figure 9.
The N-Y DAG also has interesting independence/dependence properties. It is the only member
of its independence class over the observed variables
W1 , W2 , X, and Z. A causal claim can be made for
the arc from X to Z in the N-Y DAG and it can be
estimated from P(x | z, w). We plan to generate a formal proof of causality for the N-Y DAG as part of our
future work.
Theorem 4. Assume that Context 1 holds. In the
large sample limit, in scoring DAGs on VSy , Sy is assigned the highest score, iff B contains a corresponding
EPYS, and if B contains such an EPYS, then X is an
ancestor of Z in B, and there is no unmeasured common cause of X and Z.
Proof. By Lemmas 3 and 4, Sy is assigned the highest
score, iff B contains a corresponding EPYS. In (Spirtes
et al., 2000) it is shown that if there is a directed edge
from X to Z in a PAG (as in the Y PAG), then X
is an ancestor of Z in every DAG represented by the
10
The N-Y DAG can have either the extra arc from W1
to Z or the arc from W2 to Z, but not both.

PAG, and there is no hidden common cause of X and
Z in any such DAG.
Theorem 4 indicates that local Bayesian causal discovery using Y structures is possible (under assumptions),
even when the data generating process is assumed to
be a causal Bayesian network with hidden variables.

5

Discussion

The local Bayesian causal discovery based on Y structures may have practical applications on large data
sets such as gene expression data sets with thousands
of variables or large population-based data sets with
hundreds of thousands of records. As the Y structure
represents an unconfounded causal influence of a variable X on variable Z, it can be used for performing
planned interventions leading to desirable outcomes.
When experimental studies are contra-indicated, due
to ethical, logistical, or cost considerations, causal discovery from observational data remains the only feasible approach. Moreover, in resource limited settings
such methods can be initially used to generate plausible causal hypothesis that can then be tested using
experimental methods resulting in better utilization
of available resources. As part of our future work, we
plan to relax some of our causal discovery assumptions
(for example, the acyclicity assumption) and extend
the proofs to more complex Y structures such as those
with measured confounders.

A

Appendix: Search

The proofs presented in this paper for the EPYS do not
depend on a particular search heuristic for the identification of tetrasets (sets of four variables) for Y structure scoring. An obvious search method is to search
for EPYS in all possible tetrasets of a domain. For different finite sample sizes the BLCD search method has
been shown to be effective in simulation studies. The
BLCD search first estimates the Markov blanket of a
node X, and creates sets of four variables by choosing three nodes from the MB of X in addition to X.
The tetrasets are scored using the Score function described in Appendix B. A preliminary version of the
BLCD algorithm was published in (Mani & Cooper,
2004). The reader is referred to (Mani, 2005) for additional details, including results of an extensive set of
simulation experiments.

B

Appendix: Scoring measure

The Score function assigns a score to a model that
represents the probability of the model given data

and prior knowledge. For scoring the DAGs, we use
the Bayesian likelihood equivalent (BDe) scoring measure (Heckerman et al., 1995). We use uniform, noninformative priors in order to test the ability of the
algorithm to discover causal relationships from data,
rather than from a combination of data and prior
knowledge. The latter introduces two sources of experimental variation. Note that for a Y structure,
the causal claim is valid for only the arc from X to
Z. We represent the Y structure using the notation
X ⇒ Z. In the large sample limit under the causal
Markov and causal Faithfulness assumptions (see Section 2), P(X ⇒ Z|D) (D denotes the dataset) can be
estimated using Equation 3:
Score(G1 |D)
P543
i=1 Score(Gi |D)

(3)

where Gi represents one of the 543 CBNs over V =
{W1 , W2 , X, Z}.
Note that P(X ⇒ Z|D) is a heuristic approximation
to what would be obtained if we were to explicitly
perform a full Bayesian scoring that includes hidden
variables. The score for each of the 543 measured
structures represents a score for both the measured
structure itself, as well as the score for an infinite number of structures with hidden variables. Interpreting
the score as a probability is a heuristic approximation
borne out by simulation studies (Mani, 2005), but the
theorems in the paper do not depend upon that approximation. The only probabilities being considered
are the probability of being a Y structure and the probability of not being a Y structure. The theorems in the
paper show the conditions under which P(X ⇒ Z|D)
converges to the correct value in the large sample limit.
In the large sample limit the posterior probability of
G1 will be greater relative to the other 542 models if
indeed (1) X causally influences Z in an unconfounded
manner, (2) X is an unshielded collider of W1 and W2
in the distribution of the causal process generating the
data, and (3) the Markov and Faithfulness conditions
hold.

Acknowledgements
We thank the anonymous reviewers for helpful suggestions. We also thank Thomas Richardson for comments on an earlier version of this paper. This research was supported in part by grant IIS-0325581
from the National Science Foundation and by grant
R01-LM008374 from the National Library of Medicine
awarded to Greg Cooper.




We generalize Shimizu et al’s (2006)
ICA-based approach for discovering linear
non-Gaussian acyclic (LiNGAM) Structural
Equation Models (SEMs) from causally sufficient, continuous-valued observational data.
By relaxing the assumption that the generating SEM’s graph is acyclic, we solve the
more general problem of linear non-Gaussian
(LiNG) SEM discovery. LiNG discovery algorithms output the distribution equivalence
class of SEMs which, in the large sample
limit, represents the population distribution.
We apply a LiNG discovery algorithm to simulated data. Finally, we give sufficient conditions under which only one of the SEMs in
the output class is “stable”.

1.1

Patrik O. Hoyer
Dept. of Computer Science
University of Helsinki
Helsinki, Finland

The model, with an illustration

Let x be the random vector of substantive variables, e
be the vector of error terms, and B be the matrix of
linear coefficients for the substantive variables. Then
the following equation describes the linear SEM model:
x = Bx + e

(1)

For example, consider the model defined by:
x1 = e1
x2 = 1.2x1 − 0.3x4 + e2
x3 = 2x2 + e3

(2)

x4 = −x3 + e4
x5 = 3x2 + e5
Note that the coefficient of each variable on the lefthand-side of the equation is 1.

1

Linear SEMs

Linear structural equation models (SEMs) are statistical causal models widely used in the natural and social
sciences (including econometrics, political science, sociology, and biology) [1].
The variables in a linear SEM can be divided into two
sets, the error terms (typically unobserved), and the
substantive variables. For each substantive variable
xi , there is a linear equation with xi on the left-handside, and the direct causes of xi plus the corresponding
error term on the right-hand-side.
Each SEM with jointly independent error terms can
be associated with a directed graph (abbreviated as
DG) that represents the causal structure of the model
and the form of the linear equations. The vertices of
the graph are the substantive variables, and there is a
directed edge from xi to xj just when the coefficient
of xi in the structural equation for xj is non-zero. 1
1

Traditionally, SEMs with acyclic graphs are called “re-

Fig. 1: Example 1
x can also be expressed directly as a linear combination of the error terms, as long as I − B is invertible.
Solving for x in Eq. 1 gives x = (I − B)−1 e. If we
let A = (I − B)−1 , then x = Ae. A is called the reduced form matrix (in the terminology of Independent
cursive”, and SEMs with cyclic graphs “non-recursive”[15].
We avoid this usage, and use “acyclic” or “cyclic” instead.

Components Analysis (see Section 3.1), it is called the
“mixing matrix”).
The distributions over the error terms in a SEM, together with the linear equations, entail a joint distribution over the substantive variables. This joint distribution can be interpreted in terms of physical processes,
as shown next.
1.2

Interpretating linear SEMs

These equations (contained in matrix equation (1))
can be given several different interpretations. Under
one class of interpretations, they are a set of equations satisfied by a set of variables x in equilibrium.
With some further assumptions, the B matrix in the simultaneous equations (a.k.a. “equilibrium equations”)
also represents the coefficients in a set of dynamical
equations describing a deterministic dynamical system.
Fisher [5] gave one such interpretation as follows:
There is a relatively long observation period of length
1, and a much shorter “reaction” lag of length ∆θ =
1/n. The observed variable is the vector x̄[t], defined
as the average of x over the observation period starting
at t:
n
1X
x[t + k∆θ]
(3)
x̄[t] ≡
n

that do not contain any “self-loops” (edges from a vertex to itself) 2 , i.e. the B matrices output by our
LiNG discovery algorithms have all zeros in the diagonal. This is because it is impossible to determine the
values of the diagonal entries of the B matrix from
equilibrium data alone.
In the underlying dynamical equations, it may be that
for some index a, xa [t + (k − 1)∆θ] affects xa [t + k∆θ]
(i.e. ba,a 6= 0). Our goal is to recover the coefficients
that both represent the distribution of x̄ and correctly
predict the effects of manipulations. A manipulation of
a variable xi to a distribution P is modeled by replacing the dynamical equation for xi by a new dynamical
equation xi [t + k∆θ] = e0 i , where e0 i has distribution
P [10].
For these purposes, the following argument sketches
why the underdetermination of the diagonal of Bequil
by the equilibrium data is not a problem, as long as
ba,a 6= 1 in the underlying dynamical equations.
The equation for x̄a has the form:
x̄a = ba,a x̄a +

n
X

ba,k x̄k + ea

(6)

k6=a,k=1

If ba,a 6= 1, it is possible to rewrite this as:

k=1

x̄a − ba,a x̄a =

Suppose that the underlying dynamical equations are:

n
X

ba,k x̄k + ea

(7)

k6=a,k=1

x[t + k∆θ] = Bdyn x[t + (k − 1)∆θ] + e

(4)

where e is constant over the observation period (but
may differ for different units in the population, e.g.
different observation periods).
Fisher showed that, in the limit as ∆θ approaches 0,
there is a Bequil = Bdyn such that:
x̄[t] = Bequil x̄[t] + e

(5)

if and only if the modulus of each eigenvalue of Bdyn
is less than or equal to 1, and no eigenvalue is equal
to 1.
The assumptions underlying this model are fairly
strong, but commonly made in econometrics, and defended by Fisher [5].
A simpler, but similar interpretation with similar consequences is the one in which the observed value x is
the state in which the dynamical system converged,
rather than its average over an observation period.
1.2.1

Dealing with self-loops

The LiNG discovery algorithms presented in this paper
(described in section 4) output a set of directed graphs


n
X
1

x̄a =
1 − ba,a

k6=a,k=1


ba,k x̄k + ea  =

n
X

b0a,k x̄k +e0a

k=1

(8)
b0a,a

= 0. The modified system of equations conwhere
taining Equation 8 is represented by a graph that has
no self-loops, and has a different underlying dynamical
equation in which the coefficient for xa [t + (k − 1)∆θ]
in the equation for xa [t + k∆θ] is zero.
Note that in the second equation, the error term ea has
been rescaled by 1/(1 − ba,a ) to form a new error term
e0a and when (I − B)−1 is taken to form the reduced
form coefficients, the coefficients corresponding to ea
in the first set of equations will be multiplied by (1 −
ba,a ), and the two changes cancel each other out.
Now, if we consider the original dynamical system and
the one that results from setting the diagonal of B to
zero (as above), it is sometimes the case that one dynamical system satisfies the conditions for the dynamical equations to approach the simultaneous equations
2
Fisher argues that self-loops are not realistic, but these
arguments are not entirely convincing.

in the limit, while the other one does not, because the
magnitude of the coefficients in the equation for xa [t]
are different. If both forms satisfy Fisher’s conditions,
then the act of manipulating any variable to a fixed
distribution for all t makes the two sets of dynamical
equations have equivalent limiting simultaneous equations.
1.2.2

Self-loops with coefficient 1

Unfortunately, the case where ba,a = 1 cannot be handled in the same way, since 1/(1 − ba,a ) is infinite. If
ba,a = 1, then there may be no equivalent form without a self-loop (or more precisely, the corresponding
equations without a self-loop may require setting the
variance of some error terms to zero). The case where
ba,a = 1 is a genuine problem that we do not currently
have a solution for. For the purposes of this paper, we
assume that no self-loops have a coefficient of 1.
As Dash has pointed out [4], there are cases where the
simultaneous equations have a different graph than the
underlying dynamical equations, and hence the graph
that represents the simultaneous equations cannot be
used to predict the effects of a manipulation of the
underlying dynamical system. In [4], Dash presents
two such examples. In both of them, in effect, Bdyn
has a 1 in the diagonal.

2
2.1

The problem and its history
The problem of DG causal discovery

Using the interpretations from 1.2, we can frame the
problem as follows: given samples of the equilibrium
distribution of a LiNG process whose observed variables form a causally sufficient set 3 , find the set of
SEMs that describe this distribution, under the assumption that it is non-empty.

the linear coefficients, and features common to those
directed graphs (such as ancestor relations). The algorithm performs a series of statistical tests of zero partial correlations to construct the PAG. The set of zero
partial correlations that is entailed by a linear SEM
with uncorrelated errors depends only upon the linear
coefficients, and not upon the distribution of the error
terms. Under some assumptions4 , in the large sample
limit, CCD outputs a PAG that represents the true
graph.
There are a number of limitations to this algorithm.
First, the set of DGs contained in a PAG can be large,
and while they all entail the same zero partial correlations (viz., those judged to hold in the population),
they need not entail the same joint distribution or even
the same covariances. Hence in some cases, the set represented by the PAG will include cyclic graphs that do
not fit the data well. Therefore, even assuming that
the errors are all Gaussian, it is possible to reduce the
size of the set of graphs output by CCD, although in
practice this can be intractable. For details on the
algorithm, see [11].

3

Shimizu et al’s approach for
discovering LiNGAM SEMs

The “LiNGAM algorithm”[12], which uses Independent Components Analysis (ICA), reliably discovers a
unique correct LiNGAM SEM, under the following assumptions about the data: the structural equations
of the generating process are linear and can be represented by an acyclic graph; the error terms have
non-zero variance; the samples are independent and
identically distributed; no more than one error term
is Gaussian; and the error terms are jointly independent.5
3.1

2.2

Richardson’s Cyclic Causal Discovery
(CCD) Algorithm

While many algorithms have been suggested for
discovering (equivalence classes of) directed acyclic
graphs (DAGs) from data, for general linear directed
graphs (DGs) only one provably correct algorithm was
known (until now), namely Richardson’s Cyclic Causal
Discovery (CCD) algorithm.
CCD outputs a “partial ancestral graph” (PAG) that
represents both a set of directed graphs that entail the
same set of zero partial correlations for all values of
3
A set V of variables is causally sufficient for a population if and only if in the population every common direct
cause of any two or more variables in V is in V . (For
subtleties regarding this definition, see [13]).

Independent Components Analysis (ICA)

Independent components analysis [3, 8] is a statistical technique used for estimating the mixing matrix
A in equations of the form x = Ae (e is often called
“sources” and written s), where x is observed and e
and A are not.
ICA algorithms find the invertible linear transforma4

The assumptions are: the samples are independent and
identically distributed, no error term has zero variance, the
statistical tests for zero partial correlations are consistent,
linearity of the equations, the existence of a unique reduced
form, faithfulness (i.e. there are no zero partial correlations
in the population that are not entailed for all values of the
free parameters of the true graph), and that the error terms
are uncorrelated.
5
The error terms are typically not jointly independent
if the set of variables is not causally sufficient.

tion W = A−1 of the data X that makes the error distributions corresponding to the implied samples E of
e maximally non-Gaussian (and thus, maximally independent). The matrix A can be identified up to scaling
and permutation as long as the observed distribution
is a linear, invertible mixture of independent components, at most one of which is Gaussian [3]. There are
computationally efficient algorithms for estimating A
[8].
3.2

The LiNGAM discovery algorithm

If we run an ICA algorithm on data generated by a
linear SEM, the matrix WICA obtained will be a rowscaled, row-permuted version of I − B, where B is the
coefficient matrix of the true model (this is a consequence of the derivation in Section 1.1). We are now
left with the problem of finding the proper permutation and scale for the W matrix so that it equals I −B.

Fig. 2: After removing the edges whose coefficients are
statistically indistinguishable from zero: (a) the raw WICA
matrix output by ICA on a SEM whose graph is x2 →
f matrix, obtained by
x1 ← x3 (b) the corresponding W
permuting the error terms in WICA

Since the order of the error terms given by ICA is arbitrary, the algorithm needs to correctly match each
error term ei to its respective substantive variable xi .
This means finding the correct permutation of the rows
of WICA . We know that the row-permutation of WICA
corresponding to the correct model cannot have a zero
in the diagonal (we call such permutations “inadmissible”) because W = I − B, and the diagonal of B is
zero.
Since, by assumption, the data was generated by a
DAG, there is exactly one row-permutation of WICA
that is admissible [12]. To visualize this, this constraint says that there is exactly one way to reorder
the error terms so that every ei is the target of a vertical arrow.6
In this example, swapping the first and second error
terms is the only permutation that produces an admissible matrix, as seen in Fig. 2(b).

After the algorithm finds the correct permutation, it
finds the correct scaling, i.e. “normalizing” W by dividing each row by its diagonal element, so that the
diagonal of the output matrix is all 1s (i.e. the coefficient of each error term is 1, as specified in Section
1).
Bringing it all together, the algorithm computes B by
f ), W
f=
using B = I − W 0 , where W 0 = normalize(W
RowP ermute(WICA ) and WICA = ICA(X).
Besides the fact that it determines the direction of every causal arrow, another advantage of LiNGAM over
conditional-independence-based methods [13] is that
the correctness of the algorithm does not require the
faithfulness assumption.
For more details on the LiNGAM approach, see [12].

4

Discovering LiNG SEMs

The assumptions of the family of LiNG discovery algorithms described below (abbreviated as “LiNG-D”) are
the same as the LiNGAM assumptions, replacing the
assumption that the SEM is acyclic with the weaker
assumption that the diagonal of Bdyn contains no 1s.
In this more general case, as in the acyclic case, candidate models are generated by finding all admissible matches of the error terms (ei ’s) to the observed
variables (xi ’s). In other words, each candidate corresponds to a row-permutation of the WICA matrix that
has a zeroless diagonal.
As in LiNGAM, the output is the set of admissible
models. In LiNGAM, this set is guaranteed to contain
a single model, thanks to the acyclicity assumption.
If the true model has cycles, however, more than one
model will be admissible.
The remainder of this section addresses the problem
of finding the admissible models, given that ICA has
finite data to work with.
4.1

Prune and solve Constrained n-Rooks

These algorithms generate candidate models by testing which entries of WICA are zero (i.e. pruning),
and finding all admissible permutations based on that
(i.e. solving Constrained n-Rooks, see Section 4.1.2).
We call an algorithm “local” if, for each entry wi,j of
WICA , it makes a decision about whether wi,j is zero
using only wi,j .
4.1.1

Deciding which entries are zero

6

Another consequence of acyclicity is that there will be
no right-pointing arrows in this representation, provided
that the xs are topologically sorted w.r.t. the DAG.

There are several methods for deciding which entries
of WICA to set to zero:

• Thresholding: the simplest method for estimating which entries of WICA are zero is to simply choose a threshold value, and set every entry
of WICA smaller than the threshold (in absolute
value) to zero. This method fails to account for
the fact that different coefficients may have different spreads, and will miss all coefficients smaller
than the threshold.
• Test the non-zero hypothesis by bootstrap
sampling: another method for estimating which
entries of WICA are actually zero is to do bootstrap sampling. Bootstrap samples are created
by resampling with replacement from the original
data. Then ICA is run on each bootstrap sample, and each coefficient wi,j is calculated for each
bootstrap sample. This leads to a real-valued distribution for each coefficient.7 Then, for each one,
a non-parametric quantile test is performed in order to decide whether 0 is an outlier. If it isn’t,
the coefficient is set to 0 (i.e. the corresponding
edge is pruned.)8
• Use sparse ICA: Use an ICA algorithm that
returns a sparse (i.e. pre-pruned) mixture, such
as the one presented by Zhang and Chan [16].
Unlike the other methods above, this is not a local
algorithm.
4.1.2

Constrained n-Rooks: the problem and
an algorithm

Once it is decided which entries are zero, the algorithm
searches for every row-permutation of WICA that has
a zeroless diagonal. Each such row-permutation corresponds to a placement of n rooks onto the non-zero
entries on an n × n chessboard such that no two rooks
threaten each other. Then the rows are permuted so
that all the rooks end up on the diagonal, thus ensuring that the diagonal has no zeros.
To solve this problem, we use a simple depth-first
search that prunes search paths that have nowhere to
place the next rook. In the worst case, every permutation is admissible, and the search must take O(n!).
7

One needs to be careful when doing this, since each run
of ICA may return a WICA in a different row-permutation.
This means that we first need to row-permute each bootstrap WICA to match with the original WICA .
8
One could object that, instead of a quantile test, the
correct procedure would be to simulate under the null hypothesis (i.e.: edge is absent) using the estimated error
terms, and then compare the obtained distribution of the
ICA statistics with their distribution for the bootstrap.
However, this raises issues and complexities that are tangential to the current paper.

4.2

A non-local algorithm

Local algorithms work under the assumption that the
estimates of the wi,j are independent of each other –
which is in general false when estimating with finite
samples. This motivates the use of non-local methods.
In the LiNGAM (acyclic) approach [12], a non-local
algorithm is presented for finding the single best rowpermutation of WICA , which minimizes a loss function
that heavily penalizes entries in the diagonal that are
close to zero (such as x → |1/x|). This is written as a
linear assignment problem (i.e. finding the best match
between the ei s and xi s), which can be solved using
the Hungarian algorithm [9] or others.
For general LiNG discovery, however, algorithms that
find the best linear assignment do not suffice, since
there may be multiple admissible permutations.
One idea is to use a k-th best assignment algorithm
[2] (i.e. the k-th permutation with the least penalty
on the diagonal), for increasing k. With enough data,
all permutations corresponding to inadmissible models
will score poorly, and there should be a clear separation between admissible and inadmissible models.
The non-local method presented above, like the thresholding method, fails to account for differences in spread
among estimates of the entries of WICA . It would be
straightforward to fix this by modifying the loss function to penalize diagonal entries for which the test fails
to reject the null hypothesis (as described in the part
about bootstrap sampling in Section 4.1.1), instead of
penalizing them for merely being close to zero.
4.3

Sample run

We generated 15000 sample points using the SEM in
Example 1 and error terms distributed according to a
symmetric Gaussian-squared distribution9 .
Fig. 3 shows the output of the local thresholding algorithm with the cut-off set to 0.05.
For the sake of reproducibility, our code with instructions is available from: www.phil.cmu.edu/~tetrad/
cd2008.html .

5

Theory

5.1

Notions of DG equivalence

There are a number of different senses in which the
directed graphs associated with SEMs can be “equivalent” or “indistinguishable” given observational data,
9

The distribution was created by sampling from the
standard Gaussian(0,1) and squaring it. If the value sampled was negative, it was made negative again.

the error terms are assumed to be Gaussian, then distribution equivalence entails (but is not entailed by)
covariance equivalence, which entails (but is not entailed by) d-separation equivalence.

Fig. 3: The output of LiNG-D: Candidate #1 and Candidate #2

assuming linearity and no dependence between error
terms:
• DGs G1 and G2 are zero partial correlation equivalent if and only if the set of zero partial correlations entailed for all values of the free parameters
(non-zero linear coefficients, distribution of the error terms) of a linear SEM with DG G1 is the same
as the set of zero partial correlations entailed for
all values of the free parameters of a linear SEM
with G2 . For linear models, this is the same as
d-separation equivalence. [13]
• DGs G1 and G2 are covariance equivalent if and
only if for every set of parameter values for the free
parameters of a linear SEM with DG G1 , there is
a set of parameter values for the free parameters
of a linear SEM with DG G2 such that the two
SEMs entail the same covariance matrix over the
substantive variables, and vice-versa.
• DGs G1 and G2 are distribution equivalent if and
only if for every set of parameter values for the free
parameters of a linear SEM with DG G1 , there is a
set of parameter values for the free parameters of
a linear SEM with DG G2 such that the two SEMs
entail the same distribution over the substantive
variables, and vice-versa. Do not confuse this with
the notion of distribution-entailment equivalence
between SEMs: two SEMs with fixed parameters
are distribution-entailment equivalent iff they entail the same distribution.
It follows from well-known theorems about the Gaussian case [13], and some trivial consequences of known
results about the non-Gaussian case [12], that the following relationships exist among the different senses of
equivalence for acyclic graphs: If all of the error terms
are assumed to be Gaussian, distribution equivalence
is equivalent to covariance equivalence, which in turn
is equivalent to d-separation equivalence. If not all of

So for example, given Gaussian error terms, A ← B
and A → B are zero partial correlation equivalent, covariance equivalent, and distribution equivalent. But
given non-Gaussian error terms, A ← B and A → B
are zero-partial-correlation equivalent and covariance
equivalent, but not distribution equivalent. So for
Gaussian errors and this pair of DGs, no algorithm
that relies only on observational data can reliably select a unique acyclic graph that fits the population distribution as the correct causal graph without making
further assumptions; but for all (or all except one) nonGaussian errors there will always be a unique acyclic
graph that fits the population distribution.
While there are theorems about the case of cyclic
graphs and Gaussian errors, we are not aware of any
such theorems about cyclic graphs with non-Gaussian
errors with respect to distribution equivalence. In
the case of cyclic graphs with all Gaussian errors,
distribution equivalence is equivalent to covariance
equivalence, which entails (but is not entailed by) dseparation equivalence [14]. In the case of cyclic graphs
in which at most one error term is non-Gaussian, distribution equivalence entails (but is not entailed by)
covariance equivalence, which in turn entails (but is
not entailed by) d-separation equivalence. However,
given at most one Gaussian error term, the important
difference between acyclic graphs and cyclic graphs is
that no two different acyclic graphs are distribution
equivalent, but there are different cyclic graphs that
are distribution equivalent.
Hence, no algorithm that relies only on observational
data can reliably select a unique cyclic graph that fits
the data as the correct causal graph without making further assumptions. For example, the two cyclic
graphs in Fig. 3 are distribution equivalent.
5.2

The output of LiNG-D is correct and as
fine as possible

Theorem 1 The output of LiNG-D is a set of SEMs
that comprise a distribution-entailment equivalence
class.
Proof: First, we show that any two SEMs in the output of LiNG-D entail the same distribution.
The weight matrix output by ICA is determined only
up to scaling and row permutation. Intuitively, then,
permuting the error terms does not change the mixture. Now, more formally:

Let M1 and M2 be candidate models output by LiNGD. Then W1 and W2 are row-permutations of WICA :
W1 = P1 WICA , W2 = P2 WICA
Likewise, for the error terms: E1 = P1 E, E2 = P2 E
Then the list of samples X implied by M1 is A1 E1 =
−1
(W1 )−1 E1 = (P1 WICA )−1 (P1 E) = WICA
P1 −1 P1 E =
−1
WICA E.
By the same argument, the list of samples X implied
−1
by M2 is also WICA
E. Therefore, any two SEM models output by LiNG-D entail the same distribution.
Now, it remains to be shown that if LiNG-D outputs
one SEM that entails a distribution P , it outputs all
SEMs that entail P .
Suppose that there is a SEM S that represents the
same distribution as some T , which is output by
LiNG-D. Then the reduced-form coefficient matrices
for S and T , AS and AT , are the same up to columnpermutation and scaling. Hence, I − BS and I − BT
are also the same up to scaling and row-permutation
(by I − B = A−1 ). By the assumption that there are
no self-loops with coefficient 1, neither I − BT nor
I − BS has zeros on the diagonal. Since I − BT is a
scaled row-permutation of WICA that has no zeros on
the diagonal, so is I − BS . Thus S is also output by
LiNG-D. 
Theorem 2 If the simultaneous equations are linear
and can be represented by a directed graph; the error
terms have non-zero variance; the samples are independently and identically distributed; no more than one
error term is Gaussian; and the error terms are jointly
independent, then in the large sample limit, LiNG-D
outputs all SEMs that entail the population distribution.
Proof: ICA gives pointwise consistent estimates of A
and W under the assumptions listed [3]. This entails
that there are pointwise consistent tests of whether an
entry in the W matrix is zero, and hence (by definition) in the large sample limit, the limit of both type I
and type II errors of tests of zero coefficients are zero.
Given the correct zeroes in the W matrix, the output
of the local version of the LiNG-D algorithm is correct
in the sense that the simultaneous equation describes
the population distribution. 
In general, each candidate model B 0 = I − W 0 has
the structure of a row-permutation of WICA . The
structures can be generated by analyzing what happens when we permute the rows of W 0 . Remember
that edges in B 0 (and thus W 0 ) are read column-torow. Thus, row-permutations of W 0 change the positions of the arrow-heads (targets), but not the arrow-

tails (sources). Richardson proved that the operation
of reversing a cycle preserves the set of entailed zero
partial correlations, but did not consider distribution
equivalence [11].
5.3

Adding the assumption of stability

In dynamical systems, “stable” models are ones in
which the effects of one-time noise dissipate. For example, a model that has a single cycle whose cycleproduct (product of coefficients of edges in the cycle) is ≥ 1 is unstable, while one that has a single
cycle whose cycle-product is between -1 and 1 is stable. On the other hand, if a positive feedback loop of
cycle-product 2 is counteracted by a negative loop with
cycle-product −1.5, then the model is stable, because
the effective cycle-product is 0.5.
A general way to express stability is lim B t = 0,
t→∞
which is mathematically equivalent to: for all eigenvalues e of B, |e| < 1, in which |z| means the modulus
of z. This eigenvalues criterion is easy to compute.
Given only the coefficients between different variables,
it is impossible to measure the stability of a SEM without assuming something about the self-loops. Therefore, in this section, it is assumed that the true model
has no self-loops.
It is often the case that many of the SEMs output
by LiNG-D are unstable. Since in many situations,
the variables are assumed to be in equilibrium, we are
often allowed to rule out unstable models.
In the remainder of this section, we will prove that if
the SEM generating the population distribution has a
graph in which the cycles are disjoint, then among the
candidate SEMs output by LiNG-D, at most one will
be stable.
Theorem 3 SEMs in the form of a simple cycle with
a cycle-product π such that |π| ≥ 1 are unstable.
Proof: Let k be the length of the cycle. Then B k =
πI. Then for all integers i, B ik = π i I. So if |π| ≥ 1,
the entries of B ik do not get smaller than the entries
of B as i increases. Thus, B t will not converge to 0 as
t → ∞. 
Corollary 1: For SEMs in the form of a simple cycle,
having a cycle-product ≥ 1 is equivalent to having an
eigenvalue ≥ 1 (in modulus), which is equivalent to
being unstable.
Theorem 4 Suppose that there is a SEM M with disjoint cycles with coefficient matrix B and graph G that
entails a distribution Q, and a SEM M0 6= M with
graph G0 , coefficient matrix B0 , which is an admissible permutation of M and also entails Q. Then G0

also contains disjoint cycles, at least one of which is a
reversal of a cycle C in G, whose cycle-product is the
inverse of the cycle-product of C.
Proof: Due to space limitations, the proof is just
sketched here. Every permutation can be represented
as a product of disjoint cyclic subpermutations of the
form a → b → . . . m → n → a, where a → b
means a gets mapped onto b. (Some cyclic subpermutations may be trivial, i.e. contain a single object mapped onto itself). Hence it suffices to prove
the theorem for a single admissible cyclic row permutation of B. It can be shown that if a cyclic
row permutation of B, a → b → . . . m → n → a
is admissible, then G contains the cycle C equal to
a ← b ← . . . m ← n ← a, and G0 contains the reversed
cycle C equal to a → b → . . . m → n → a. Moreover,
if G0 contains two cycles that touch, so does G.
Consider BC , the submatrix of B that contains the
coefficients of the edges in cycle C.


0
...
0 bk,1


 b1,2
0
...
0 




BC = 

.
..

 0
b
0
2,3




..
.
0
0
0
Qk−1
Note that the cycle-product πC = bk,1 i=0 bi,i+1 .
WC = I − BC .
The “reversal” is the row-permutation in which the
first row gets “rotated” into the bottom:

−b1,2
1
...
0




..

 0
.
−b
0
2,3


RowP ermute(WC ) = 



..

 0
.
0
1


1
0
. . . −bk,1
Normalizing the diagonal to be all 1s, we get WC 0 .
Computing BC 0 = IQ− WC 0 , one can see that the cyclek−1
1
1
product πC 0 = bk,1
i=0 bi,i+1 = 1/πC . 
We will now show that for SEMs in which the cycles are
disjoint, their stability only depends on the stability of
the cycles.
Theorem 5 A SEM in which the cycles are disjoint
is stable if and only if it has no unstable cycles.
Proof: Let be G be a SEM whose cycles are disjoint.
Then BG can be written as a block-triangular matrix
where each diagonal block is a cycle. The set of eigenvalues of a block-triangular matrix is the union of the
sets of eigenvalues of the blocks in the diagonal (in this

case, the eigenvalues of the cycles). Suppose a cycle
of G is unstable. Then it has an eigenvalue ≥ 1 (in
modulus). But since this is also an eigenvalue of BG ,
it follows that G is unstable. The other direction goes
similarly. 
Theorem 6 If the true SEM is stable and has a graph
in which the cycles are disjoint, then no other SEMs
in the output of LiNG-D will be stable.
Proof: Suppose the true SEM is stable and has a
graph in which the cycles are disjoint. Call it G. Since,
by Theorem 2, the output of LiNG-D are the admissible distribution-entailment equivalent alternatives to
the true SEM, it suffices to show that all other admissible candidates are unstable.
By Theorem 5, all cycles in G are stable. Let H be
an admissible alternative to G, such that H 6= G. By
Theorem 4, H will have at least one cycle C reversed
relative to G and this reversed cycle will have a cycle product that is the inverse of the cycle product of
C. By Corollary 1, the reversed matrix is not stable.
Thus, by Theorem 5, H is unstable.
Therefore, the only stable admissible alternative to G
is G itself. 
It follows that if the true model’s cycles are disjoint,
then under the assumption that the true model is stable, we can fully identify it using a LiNG discovery algorithm (at most one SEM in the output of the LiNG
discovery algorithm will be stable).
For example, consider the two candidate models shown
in Fig. 3. By assuming that the true model is stable,
one would select candidate #2. Since our simulation
used a stable model, this is indeed the correct answer
(see Fig. 1).
In general, however, there may be multiple stable models, and one cannot reliably select the correct one.
When the cycles are not disjoint, it is easy to find examples for which there are multiple stable candidates.
The condition of disjoint cycles is sufficient, but not
necessary: it is easy to come up with SEMs where
we have exactly one stable SEM in the distributionentailment equivalence class, despite intersecting cycles.

6

Discussion

We have presented Shimizu’s approach for discovering
LiNGAM SEMs, and generalized it to a method that
discovers general LiNG SEMs. This improves upon the
state-of-the-art on cyclic linear SEM discovery by outputting only the distribution-entailment equivalence
class of SEMs, instead of the entire d-separation equiv-

alence class; and by relaxing the faithfulness assumption. We have also shown that stability can be a powerful constraint, sometimes narrowing the candidates
to a single SEM.

[4] D. Dash (2005) - Restructuring Dynamic Causal
Systems in Equilibrium. Proceedings of the Tenth
International Workshop on Artificial Intelligence
and Statistics (AIStats 2005)

There are a number of questions that remain open for
future research:

[5] F. Fisher (1970) - A correspondence principle
for simultaneous equation models. Econometrica,
38(1):73-92.

• The LiNG-D algorithm generates all admissible
permutations. The worst-case time-complexity of
n-Rooks is high, but can we do better than depthfirst search for random instances? Is there an algorithm to efficiently search for the stable models,
without going through all candidates? In the case
where the cycles are disjoint, it is possible to just
find the correct permutation for each cycle independently, but no such trick is known in general.
• How can prior information be incorporated into
the algorithm?
• How can the algorithm be modified to allow the
assumption of causal sufficiency to be relaxed?
For the acyclic case, see [7].
• How can the algorithm be modified to allow for
mixtures of non-Gaussian and Gaussian (or almost Gaussian) error terms? Hoyer et al [6] address this problem for the acyclic case.
• How could we integrate this method into mainstream dynamical systems research? Can the algorithm handle noisy dynamics and noisy observations? Could it be made to handle non-linear
dynamics? What about self-loops of coefficient 1?
How could one integrate this with methods that
use non-equilibrium time-series data?
Acknowledgements
The authors wish to thank Anupam Gupta, Michael
Dinitz and Cosma Shalizi. GL was partially supported
by NSF Award No. REC-0537198.


