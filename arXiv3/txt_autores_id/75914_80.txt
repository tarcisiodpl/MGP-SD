
We define the notion of compiling a Bayesian
network with evidence and provide a specific approach for evidence–based compilation, which makes use of logical processing.
The approach is practical and advantageous
in a number of application areas—including
maximum likelihood estimation, sensitivity
analysis, and MAP computations—and we
provide specific empirical results in the domain of genetic linkage analysis. We also
show that the approach is applicable for networks that do not contain determinism, and
show that it empirically subsumes the performance of the quickscore algorithm when
applied to noisy–or networks.

1

INTRODUCTION

It is well–known that exploiting evidence can make inference in a Bayesian network more tractable. Two of
the most common techniques are removing leaf nodes
that are not part of the evidence or query (Shachter,
1986) and removing edges outgoing from observed
nodes (Shachter, 1990). These preprocessing steps,
which we call classical pruning, can significantly reduce the connectivity of the network, making accessible many queries that would be inaccessible otherwise.
Although classical pruning can be very effective, one
can identify situations where it does not exploit the
full power of evidence, especially when the network
contains local structure. The investigation in this paper serves to spotlight the power of evidence by discussing the extent to which it can be exploited computationally, and by introducing the notion of compiling
a Bayesian network with evidence.
Traditionally, one incurs a compilation cost to prepare
for answering a large number of queries over different
evidence, amortizing the cost over the queries. But

when evidence is fixed, this benefit may seem illusory
at first. We will show, however, that compiling with
evidence is often more tractable than compiling without evidence and that it can be very practical. First,
the evidence may be fixed on only a subset of the variables, leaving room for posing queries with respect to
other variables (this happens in MAP computations).
Second, one may be interested in estimating the value
of network parameters which will maximize the probability of given evidence (this happens, for example, in
genetic linkage analysis). In this case, one may want
to use iterative algorithms such as EM or gradient ascent (Dempster et al. , 1977), which pose many network queries with respect to the given evidence but
different network parameter values. A similar application appears in sensitivity analysis, where the goal is
to search for some network parameters that satisfy a
given constraint.
Our approach to compiling with evidence is based on
an approach to compiling a network without evidence
into an arithmetic circuit (Darwiche, 2002; Darwiche,
2003); see Figure 1. The inputs to the circuit correspond to evidence indicators (for recording evidence)
and network parameters and the output to the probability of recorded evidence under the given values of
parameters. Given evidence, we will then compile an
arithmetic circuit that is hardwired for that evidence
and, hence, will only be good for computing queries
with respect to that evidence. The particular compilation approach we adopt reduces the problem to one
of logical inference, which we argue is a natural setting for exploiting the interaction between evidence
and network local structure.
We apply the compilation approach to genetic linkage
analysis, where we provide experimental results showing order of magnitude improvement over state of the
art systems for certain benchmarks. We also show
that the proposed approach subsumes empirically the
quickscore algorithm (Heckerman, 1989).
This paper is organized as follows. Section 2 defines

+

A

B

*
Compile

θa

+

*
λa

1

θa

1

λa

2

+

2

C

*

*

+

*

θc |a b
2 1 1

*

+

*
*

θc |a b
θc |a b
1 1 1
2 1 2
θc |a b
3 1 1

θb

1

λb

1

*

*
λc

θc |a b
1 1 2

*

*
+

*
θb

*
λc

2
θc |a b
3 1 2

1

2

λb

+

*

2
λc

*

*

*

θc |a b
3 2 1

3
θc |a b
2 2 1

*

*
θc |a b
3 2 2

θc |a b
1 2 1

θc |a b
1 2 2
θc |a b
2 2 2

Figure 1: A Bayesian network and a corresponding AC.
a semantics for compiling with evidence and describes
areas where it applies. Section 3 describes our specific
approach to compiling with evidence. We illustrate
some of the reasons the approach works and apply it
to genetic linkage analysis in Section 4. In Section 5,
we show that the approach empirically subsumes the
quickscore algorithm and applies to networks without
determinism. Finally, Section 6 presents some concluding remarks.

2

COMPILING WITH EVIDENCE

This section defines a semantics for compiling a
Bayesian network with evidence and explains some areas where such compilation can provide significant advantage.
2.1

Semantics

We begin by reviewing compilation without evidence
as described in (Darwiche, 2003) and (Darwiche,
2002). We view each network as a multi–linear function (MLF), which contains two types of variables: an
evidence indicator λx for each value x of each variable X and a parameter variable θx|u for each network
parameter. The MLF contains a term for each instantiation of the network variables, and the term is
the product of all indicators and parameters that are
consistent with the instantiation. For example, consider the network in Figure 1, where A and B have
two values, and C has three values. The corresponding MLF involves twenty–three variables and contains
twelve terms as follows,
λa1 λb1 λc1 θa1 θb1 θc1 |a1 ,b1
λa1 λb1 λc3 θa1 θb1 θc3 |a1 ,b1
λa1 λb2 λc2 θa1 θb2 θc2 |a1 ,b2
λa2 λb1 λc1 θa2 θb1 θc1 |a2 ,b1
λa2 λb1 λc3 θa2 θb1 θc3 |a2 ,b1
λa2 λb2 λc2 θa2 θb2 θc2 |a2 ,b2

+ λa1 λb1 λc2 θa1 θb1 θc2 |a1 ,b1 +
+ λa1 λb2 λc1 θa1 θb2 θc1 |a1 ,b2 +
+ λa1 λb2 λc3 θa1 θb2 θc3 |a1 ,b2 +
+ λa2 λb1 λc2 θa2 θb1 θc2 |a2 ,b1 +
+ λa2 λb2 λc1 θa2 θb2 θc1 |a2 ,b2 +
+ λa2 λb2 λc3 θa2 θb2 θc3 |a2 ,b2
(1)

To compute the probability of evidence e, we evaluate
the MLF after setting indicators that contradict e to
0 and other indicators to 1. For example, to compute
Pr(a2 , b1 ), we evaluate the above MLF after setting
λa1 , λb2 to 0 and λa2 , λb1 , λc1 , λc2 , λc3 to 1. Setting indicators has the effect of excluding those terms
that are incompatible with the evidence. Computing
answers to other probabilistic queries, such as posterior marginals on network variables or families, can be
obtained from the partial derivatives of the MLF; see
(Darwiche, 2003) for details.
As is clear from the above description, the MLF has an
exponential size. Yet, one may be able to factor this
function and represent it more compactly. An arithmetic circuit (AC) is a rooted DAG, where each leaf
represents a real–valued variable or constant and each
internal node represents the product or sum of its children. Figure 1 depicts an AC. If we can factor the network MLF efficiently using an arithmetic circuit, then
inference can be done in time linear in the size of the
circuit, since the value and (first) partial derivatives
of an arithmetic circuit can all be computed simultaneously in time linear in the circuit size (Darwiche,
2003). In an AC representing a network MLF, each
leaf represents an indicator or parameter. An effective
method of producing an AC is given in (Darwiche,
2002), and (Park & Darwiche, 2003b) shows that the
AC is a generalization of the jointree.1
The MLF above and its corresponding AC are capable of answering queries with respect to any evidence.
However, if we are willing to commit to specific evidence e, then we can instead work with a much simpler MLF. For evidence e = {a2 , b1 }, the above MLF
can be reduced as follows,
λc1 θa2 θb1 θc1 |a2 ,b1 + λc2 θa2 θb1 θc2 |a2 ,b1 +
λc3 θa2 θb1 θc3 |a2 ,b1

(2)

1
The AC is a generalization of Jointree in the sense that
a Jointree embeds an AC with extra syntactic restrictions.
Moreover, the two passes in jointree inference correspond
to circuit evaluation and differentiation.

*
θa

2

θb

1

*
θc |a b
3 2 1

+

*
λc

3

θc |a b
2 2 1

λc

*
2

θc |a b
1 2 1

λc

1

Figure 2: An AC that incorporates evidence.
In general, one obtains the instantiated MLF for a
given network and evidence by removing each term
from the MLF that contradicts the evidence. An instantiated MLF, and hence its corresponding AC, is
therefore capable of answering only queries where e
is given, such as Pr(e), Pr(X|e) for network variable
X, and Pr(e, m) for additional evidence m. Figure 2
depicts the instantiated AC, which contrasts with the
AC in Figure 1.
Although we have lost the ability to apply arbitrary
evidence, compiling with evidence can be much more
efficient than compiling without. Moreover, the instantiated AC still captures information that is critical
to many inference tasks and does so in a way that provides significant advantage to an approach not based
on compilation. We provide some examples next.
2.2

Applications

Genetic linkage analysis can model genetic information for a population of related individuals (a pedigree) using a Bayesian network (Fishelson & Geiger,
2002). Some network parameters θ1 , . . . , θn represent
recombination factors, and the goal is to search for the
recombination factors which maximize the probability of given evidence e: argmax θ1 ,...,θn Pr(e|θ1 , . . . , θn ).
The procedure amounts to ordering genes on a
chromosome and determining the distance between
them. Typically, one solves this problem by posing
Pr(e|θ1 , . . . , θn ) using particular values of recombination factors (parameters), and then repeating multiple
times for different values. Our results demonstrate
that, on some benchmarks, compilation can significantly improve on superlink 1.4,2 which is a state–
of–the–art system for the task.
Sensitivity analysis involves searching for parameter changes that satisfy certain constraints. For example, an expert may decide that Pr(A = a1 |e) must be
greater than Pr(A = a2 |e) for some specific evidence e.
Our goal is to identify minimal parameter changes that
satisfy this constraint. The problem can be solved relatively efficiently for a single parameter change, or multiple parameter changes within the same CPT (Chan
& Darwiche, 2004). For multiple parameters spread
over multiple CPTs, the solution involves numerical
2

http://bioinfo.cs.technion.ac.il/superlink/

methods that pose multiple probabilistic queries under evidence e, but with different values for network
parameters. In this case, compiling the network with
given evidence is quite practical, as the work done during compilation can be amortized over the many different queries.
MAP is the problem of computing the most likely
instantiation of a set of variables M given evidence
e. Computing MAP can utilize compilation with evidence in a way that is similar to that of genetic linkage and sensitivity analysis, but instead of adjusting
parameters, we adjust indicators. Both exact and approximate algorithms for computing MAP involve obtaining initial evidence e and then repeatedly computing Pr(e, m) for different instantiations m of a subset
of the MAP variables (Park & Darwiche, 2003a). We
can therefore compile an AC with evidence e and then
evaluate it for different values associated with indicators of variables M.

3

IMPLEMENTATION

We now describe the technique used in the experimental results to compile a network with evidence into an
AC. The approach for compiling a network without
evidence into an AC has been described in (Darwiche,
2002), and is based on encoding the network MLF into
a set of logical clauses (CNF), factoring the CNF, and
then extracting an AC from the factored CNF. The details of factoring the CNF and extracting the AC are
not critical here, so we will refer the reader to (Darwiche, 2002) for details. We will however review how
a real–valued MLF can be encoded semantically into a
propositional theory, and show how the network MLF
can be encoded using a CNF. This is needed for explaining how evidence is exploited during compilation.
To illustrate the encoding scheme, consider the MLF
f = a + ad + abd + abcd over real–valued variables
a, b, c, d. The basic idea is to specify this MLF using
a propositional theory that has exactly four models,
one for each term in f . Specifically, the propositional
theory ∆f = Va ∧ (Vb ⇒ Vd ) ∧ (Vc ⇒ Vb ) over Boolean
variables Va , Vb , Vc , Vd has exactly four models and encodes f as follows,
Model
σ1
σ2
σ3
σ4

Va
true
true
true
true

Vb
false
false
true
true

Vc
false
false
false
true

Vd
false
true
true
true

encoded term
a
ad
abd
abcd

That is, model σ encodes term t since σ(Vj ) = true iff
term t contains the real–valued variable j.
The encoding described above is semantic; that is, it
describes the theory ∆f which encodes a multi–linear

function by describing its models. We specify these
theories using a CNF that has one Boolean variable
Vλ for each indicator variable λ, and one Boolean variable Vθ for each parameter variable θ. For brevity
though, we will abuse notation and simply write λ and
θ instead of Vλ and Vθ . CNF clauses fall into three
sets. First, for each network variable X with domain
x1 , x2 , . . . , xn , we have,
Indicator clauses : λx1 ∨ λx2 ∨ . . . ∨ λxn
¬λxi ∨ ¬λxj , for i < j
These clauses ensure that exactly one of X’s indicator
variables appears in each term of the MLF. The second
two sets of clauses correspond to network parameters.
In particular, for each parameter θxn |x1 ,x2 ,...,xn−1 , we
have,
IP clause : λx1 ∧ λx2 ∧ . . . ∧ λxn ⇒ θxn |x1 ,x2 ,...,xn−1
PI clauses : θxn |x1 ,x2 ,...,xn−1 ⇒ λxi , for each i
The models of this CNF are in one–to–one correspondence with the terms of the MLF. In particular, each
model of the CNF will correspond to a unique network
variable instantiation, and will set to true only those
indicator and parameter variables which are compatible with that instantiation. The encoding we use in
our experiments is a bit more sophisticated than described above (Chavira & Darwiche, 2005), but the
above encoding will suffice to make our points below.
The encoding as discussed does not include information about evidence. Recall that to incorporate evidence e, we need to exclude MLF terms that contradict
e. It is quite easy to do so in the current framework.
Consider the network in Figure 1, its MLF (1), and the
evidence {a2 , b1 }. Assume that we have generated the
CNF ∆ for this network. Our goal becomes excluding
from ∆ models corresponding to terms that contradict the evidence. We can easily do so by adding the
following unit clauses to the CNF: λa2 and λb1 . In general, to incorporate evidence x1 , x2 , . . ., xn , we add
unit clauses λx1 , λx2 , . . ., λxn . Moreover, it is easy
to incorporate more general types of evidence. For example, we can incorporate the assertion “c1 or (a1 and
b2 )” by including the clauses λc1 ∨ λa1 and λc1 ∨ λb2 .
In our implementation, we simplify the constructed
CNF together with evidence by running unit resolution and then removing subsumed clauses. We then
invoke our compiler which factors the CNF using a
version of the recursive conditioning algorithm (Darwiche, 2004). This algorithm makes repeated use of
conditioning to decompose the CNF into disconnected
CNFs that are compiled independently. Moreover, the
algorithm runs unit resolution after each conditioning
to simplify the CNF further. This process of decomposition becomes much more effective given the initial

evidence injected into the CNF, which helps to simplify
the CNF considerably. Some of the benefit is obtained
immediately from the initial preprocessing of the CNF.
Other benefits, however, are obtained during the compilation process itself since conditioning sets the value
of variables, which together with the injected evidence
can lead to even more simplification of the CNFs and,
hence, better decomposition. We will see examples of
this behavior in the following section.

4

THE POWER OF EVIDENCE

Consider the “Original Evidence” portion of Table 1,
which contains a set of Bayesian networks corresponding to pedigrees in the domain of genetic linkage analysis. Each network has been classically pruned for
specific evidence, yet they still have very connected
topologies, as shown by the cluster sizes obtained using a minfill variable ordering heuristic.3 None of these
networks could be compiled without evidence, yet the
table lists data on successful compilations for most of
these networks once evidence is introduced, despite the
large cluster sizes.4 In particular, the table shows the
offline time (which includes preprocessing and compiling), size of AC, and online inference time for computing Pr(e). Note that online inference may be repeated
for new recombination values, without re–incurring the
offline cost.
Our current implementation uses only unit resolution
and removal of subsumed clauses during its simplification of the CNF before compiling. However, based on
the amount of determinism in these networks, more advanced logical techniques can be utilized. We therefore
augmented the given evidence with some additional evidence learned by the domain specific Lange and Goradia algorithm (Lange & Goradia, 1987). It should be
noted that this additional evidence can be inferred by
standard logical techniques applied to the initial evidence and network determinism, and could therefore
be made domain independent. By using this additional (inferred) evidence, we can see in the “Learned
Evidence” portion of Table 1 that all these networks
compile in a reasonable amount of time, and that online inference is faster. Since the additional learned
evidence may apply to internal (non-leaf) nodes, one
may use this evidence to empower classical pruning.
Indeed the table lists the adjusted cluster sizes for
these networks after having applied classical pruning
using the additional evidence. The learned evidence
makes many of these networks accessible to classical
3

We are reporting here normalized cluster sizes (log 2
of the number of instantiations of a given cluster).
4
Experiments in this paper ran on a 1.6GHz Pentium
M processor with 2GB of memory.

Table 1: EA results.
NET
ea1
ea2
ea3
ea4
ea5
ea6
ea7
ea8
ea9
ea10
ea11

MAX
CLUST
31.6
38.6
40.6
46.0
60.9
70.9
82.9
106.9
200.5
204.1
235.1

Original Evidence
OFFLINE
AC
SEC
EDGES
2.67
98,613
3.65
144,181
10.45
272,503
8.02
322,063
38.48
992,917
125.62
3,557,015
4,591.65
26,934,471
1,732.10
24,375,244
n/a
n/a
n/a
n/a
n/a
n/a

Pr(e)
SEC
0.01
0.01
0.01
0.03
0.03
0.11
3.35
6.05
n/a
n/a
n/a

MAX
CLUST
13.0
13.0
13.0
13.0
13.0
14.9
15.6
16.0
28.3
31.6
30.6

Learned Evidence
OFFLINE
AC
SEC
EDGES
1.57
24,055
1.57
28,390
1.68
31,575
1.89
34,126
2.79
54,703
7.75
120,700
73.20
997,652
9.35
180,100
1,226.43
3,597,965
1,665.57
13,758,985
2,586.64
12,298,513

Table 2: EE results with full preprocessing.
NET
ee33
ee37
ee30
ee23
ee18

MAX
CLUST
20.2
29.6
35.9
38.0
41.5

OFFLINE
SEC
25.33
61.29
376.78
89.47
283.96

AC
EDGES
2,070,707
1,855,410
27,997,686
3,986,816
23,632,200

Pr(e)
SEC
0.59
0.39
8.37
1.08
6.63

superlink
SEC
1046.72
1381.61
815.33
502.02
248.11

inference algorithms, but three of the networks still
pose problems for classical techniques.
It is worth putting the results in perspective by comparing to state-of-the-art results in genetic linkage
analysis obtained with superlink. This system uses
a combination of variable elimination and conditioning, along with many domain specific preprocessing
rules and a sophisticated search for a variable ordering. All superlink timings we report include preprocessing and computing answers to two Pr(e) queries,
where on difficult networks, the majority of the time
is spent doing inference. Until the latest release, the
networks in Table 1 were considered very challenging,
with EA11 taking over 10 hours. The newest version
of superlink, 1.4, includes enhancements that preprocess and perform the two Pr(e) queries in 7 seconds on even the most difficult of these networks. If
we allow ourselves to use further simplification techniques, which include some simplifications from superlink 1.4 and also some rules to detect variable
equivalence, we obtain the results shown in the “Full
Preprocessing” portion of Table 1. Here, offline time
takes about 10 seconds on the hardest network and
online inference is very fast.
More dramatic are the results reported in Table 2 on
five networks from the challenging superlink 1.4 data
sets. On these networks, the compilation approach
was able to improve on superlink’s performance as
reported in (Fishelson et al. , 2004). On four of these
networks, offline time is shorter than the superlink
time. Once compiled, the generated ACs can repeatedly compute Pr(e) extremely efficiently compared to
superlink. Because one of the main tasks of genetic
linkage analysis is to do maximum likelihood estima-

Pr(e)
SEC
0.00
0.01
0.01
0.01
0.01
0.01
0.03
0.01
1.81
1.42
5.81

MAX
CLUST
11.3
11.3
11.3
11.3
11.3
12.3
12.3
12.3
12.3
12.3
12.3

Full Preprocessing
OFFLINE
AC
SEC
EDGES
1.30
20,230
1.35
21,218
1.34
20,489
1.42
19,455
1.64
22,963
1.96
31,146
2.34
39,957
3.24
41,249
7.50
82,297
8.61
95,417
10.04
92,274

Pr(e)
SEC
0.01
0.01
0.01
0.01
0.01
0.01
0.02
0.01
0.03
0.03
0.03

tion over many iterations, the ability to perform online
inference quickly is critical.
Note that we can differentiate these circuits and,
hence, obtain marginals over families in about 2 − 3
times as long as it takes to evaluate Pr(e). This allows
us to run the EM algorithm, which requires marginals
over families to perform each iteration. When comparing these timings with the time it would take to
re-run superlink for the same purpose, one sees the
significant benefit of compiling with evidence. Suppose for example that we have 10 parameters we want
to estimate and that EM or gradient ascent takes 20
iterations to converge. For network ee33, we would
perform 200 queries in about 350 seconds using AC,
whereas running superlink to compute those values
would require days.
4.1

Examples

We now demonstrate how combining evidence with local structure can make the inference process more efficient. These gains cannot be obtained using classical
pruning, although some can be obtained using more sophisticated schemes (e.g., (Larkin & Dechter, 2003)).
The first example uses the network in Figure 1, where
A and B have two states and C has three states. Let
the CPT for the variable C contain all zeros except for
the four lines below.
A
a1
a1
a2
a2

B
b1
b2
b1
b2

C
c1
c2
c2
c3

P r(C|A, B)
1.0
1.0
1.0
1.0

Suppose we know that C = c1 . From this information,
we can logically infer A = a1 and B = b1 . In fact,
this information can be obtained by preprocessing the
CNF encoding of the network using unit resolution.
The learned evidence can be added to the CNF, or it
could be used to empower classical pruning.
Now assume that we have evidence {c2 }. Because A
and B are binary, there would normally be four possible configurations of A and B. However, given the

CPT parameterization, we can rule out both {a1 , b1 }
and {a2 , b2 }, leaving only two configurations. This
conclusion can again be obtained by applying unit resolution to our CNF encoding. However, in this case,
the inferred information cannot be expressed in terms
of classical pruning. Furthermore, it cannot be expressed using a more advanced form of simplification,
where variable states known to never occur are removed form the variable’s domain, since every state
of A and B is valid. The learned “multi-variable”
evidence is, however, easily written in the language
of CNF, and can be utilized in further simplifications
during the compilation process.
One question is how often situations like the above
occur in real networks. The examples actually derive
from real networks in the domain of genetic linkage
analysis, where variables A and B represent a person’s
genotype (for example a person’s blood type) and C
represents the phenotype (the observed portion of the
genotype). The example then shows one way the genotype can be mapped to the phenotype. Take the simplified case where there are only two blood types, 1
and 2. Then the four possible genotype combinations
are 1/1, 1/2, 2/1, 2/2, although frequently 1/2 and 2/1
cannot be differentiated, so there are only three phenotypes. The example models this situation by mapping
two configurations of A and B to the same value for
C. Furthermore, in this domain, evidence is typically
on phenotype variables, which translates to evidence
on C in our model.
The third example from genetic linkage analysis involves four variables: child C with parents A, B, and
S. The variable C in this case is not the phenotype,
but the genotype in a child which is inherited from one
of the parent’s genes, A/B, based on the value of S.
We assume that all four variables are binary and that
the portion of the table with S = s1 is as follows,5
S
s1
s1
s1
s1

A
a1
a1
a2
a2

B
b1
b2
b1
b2

C
c1
c1
c2
c2

P r(C|A, B)
1.0
1.0
1.0
1.0

The point of this example is to illustrate how compilation can utilize evidence even when preprocessing cannot. This type of gain is one of the factors that allows
us to successfully compile a network whose treewidth
remains high after preprocessing. Compiling repeatedly conditions to decompose the CNF. Let us consider
the case where we are given evidence {c1 }, and during compilation, we condition on S = s1 . Assuming a
proper encoding of the network into CNF, combining
the evidence with the value for S allows us to infer a1 ,
5
In general, the variables are multi–valued, and this discussion also applies in this case.

Table 3: Friends and Smokers Results.
DOM
SIZE
1
4
7
10
13
16
19
22
25
28
29

MAX
CLUST
3
13
36
70
118
172
244
316
412
528
560

OFFLINE
SEC
0.02
0.03
0.08
0.34
1.07
3.21
9.04
23.56
48.32
105.74
130.00

AC
EDGES
18
293
1,295
3,512
7,430
13,535
22,313
34,250
49,832
69,545
77,118

ONLINE
SEC
0.00
0.01
0.01
0.02
0.03
0.04
0.05
0.07
0.09
0.13
0.14

which unit resolution can use to achieve further gains.
Conditioning on S = s2 yields a similar conclusion for
b1 . In this case, the full power of evidence on C is
realized only when combined with conditioning, which
takes place during the compilation step.
We close this section by quickly examining one more
set of networks. (Richardson & Domingos, 2004) discusses a relational probabilistic model involving an
imaginary domain of people and relations on the people including which smoke, which have cancer, and
who are friends of whom. There are also logical constraints on the model, such as the constraint that if a
person’s friend smokes, then the person also smokes.
We worked with a slight variation on this model, and
each network in Table 3 represents an instance for a
different number of people. For a given network, some
nodes represent ground relations and others represent
logical constraints. The key point is that, in the absence of evidence, we could only compile the first two
networks listed. However, when we commit to evidence asserting that the logical constraints are true,
the networks become relatively easy, the hardest requiring 130 seconds to compile and 0.14 seconds for
online inference. Online inference involves asserting
evidence e on some of the relations and computing
Pr(e) and marginals on all remaining relations.

5

THE QUICKSCORE
ALGORITHM

We illustrate two points in this section. First, our compiling approach empirically subsumes the quickscore
algorithm, a dedicated algorithm for two–level noisy–
or networks. Second, networks which do not contain
determinism, and hence may not look amenable to exploiting evidence as described earlier, can be transformed to make them amenable to these techniques.
We start by considering two–level noisy–or networks
as given in Figure 3(a). Here, each di represents a dis-

d1

d2

f1

...

d3

...

f2

dn

fm

(a)
d1

d2

...

d3

dn

c1,1

c1,2

c2,1

c2,2

c3,2

...

c3,m

cn,m

a1,1

a1,2

a2,1

a2,2

a3,2

...

a3,m

an,m

f1

...

f2

fm

(b)

Figure 3: (a) A disease/feature network; (b) the network with determinism introduced.

ease, which occurs in a patient with probability pi , and
each fj represents a feature (e.g., test result), which we
may observe to be negative or positive in the patient.
We assume a noisy–or relationship between a specific
feature fj and the diseases di that may causes it. That
is, if di is not present, then di will not cause fj . Otherwise, di will cause fj with probability pi,j . We wish
to compute a marginal for each disease given evidence
on features. Standard inference has a worst case time
complexity that is exponential in the number n of diseases. However, (Heckerman, 1989) showed that computing such marginals can be done in time exponential
only in the size m+ of the set F + of features known to
be positive. The argument makes several appeals to
the independence relationships created by the noisy–or
assumptions and by the network structure. It culminates with the definition of the quickscore algorithm,
which iterates over the power set of F + and computes
+
a marginal on a single disease in time Θ(nm− 2m ),
where m− is the number of negative findings.
It is well–known that the semantics of the noisy–or relationships allows us to transform the network in Figure 3(a) into the network in Figure 3(b). Here, each
edge from di to fj in the original network is replaced
with two nodes, ci,j and ai,j , and three edges. Each introduced ci,j is a binary root such that Pr(ci,j ) = pi,j ;
and each introduced ai,j represents whether disease
di causes feature fj . Therefore, ai,j represents a conjunction of di and ci,j , and each feature is a disjunction of its parents. This disjunction can be represented very compactly in CNF, even when there are
a large number of parents. Although the network in
Figure 3(a) typically does not possess determinism,
the transformed network possesses an abundance in

the form of introduced conjunctions and disjunctions,
leading one to wonder whether combining this determinism with evidence in the manner proposed would
duplicate quickscore’s performance. To test this hypothesis, we chose different values for m+ , and for
each, we constructed ten experiments, each designed
to be similar to the experiments on the proprietary
network used to demonstrate quickscore. For each experiment, we generated a random network containing
600 diseases and 4100 features. For each feature, we
chose 11 possible causes uniformly from the set of diseases. We then chose each pi and each pi,j uniformly
from the open interval (0, 1). In addition, we generated evidence by setting to positive m+ features chosen uniformly from the set of features and setting the
remaining features to negative.6 In this way, the experiment utilizes its own randomly generated network
and its own randomly generated evidence. Finally, we
compiled and evaluated the network with the evidence,
yielding a marginal over each disease.
Each of the experiments produced a network for which
minfill computed a maximum cluster size between 586
and 589. Because the set of evidence variables is the
same as the set of leaves in the network, classical pruning would have no effect on this cluster size.
For each value of m+ , Table 4 shows results, averaged
over the ten experiments. The most important observation is that the approach to compiling with evidence
empirically subsumes quickscore. Indeed, quickscore
is exponential in m+ even in the best case, whereas
compiling was sometimes fast, even for large m+ . For
example, the minimum compile times for m+ = 28
and m+ = 29 were 41s and 135s, respectively. Furthermore, quickscore computes a marginal only for a
single disease, whereas the described method computes
marginals over all 600 diseases simultaneously.
The transformation to introduce determinism applies
not only to the types of networks on which quickscore
runs, but to any network involving noisy–or relationships. There are similar transformations for other
types of local structure, including noisy–or with leak,
noisy–max (Dı́ez & Galán, 2003), and context–specific
independence (Boutilier et al. , 1996). Consider a final
example involving a family containing binary variable
C with binary parents A and B. Suppose that given
A = a1 , C becomes independent of B; yet this is not
true for A = a2 . In this case, we introduce auxiliary
variable S with three states between A/B and C. S’s
CPT is deterministic and sets S to s1 when A = a1 , to
s2 for parent state {a2 , b1 }, and to s3 for parent state
{a2 , b2 }. Moreover, the CPT for C becomes,
We could have left some features F 0 out of the evidence. In this case, classical pruning would suffice to remove nodes F 0 from the network.
6

Table 4: Averaged diagnosis results.
TRUE
FEATURES
0
3
6
9
12
15
18
21
23
25
27
28
29

S
s1
s2
s3

OFFLINE
SEC
23.73
23.86
23.81
23.82
24.19
23.60
24.95
30.95
42.81
155.12
469.70
728.52
1,046.93

AC
EDGES
48,100
52,830
57,638
62,547
67,632
73,321
81,629
109,335
145,333
434,445
1,141,674
1,691,833
2,352,820

ONLINE
SEC
0.05
0.05
0.05
0.05
0.05
0.04
0.05
0.05
0.06
0.08
0.17
0.23
0.30

P r(c1 |S)
P r(c1 |a1 , b1 ) = P r(c1 |a1 , b2 )
P r(c1 |a2 , b1 )
P r(c1 |a2 , b2 )

Given evidence A = a1 , our logic–based strategy can
infer both the value of S and the independence of
C from B. This technique allows for more efficient
decomposition during the compilation process, even
though the original network contains no determinism.

6

CONCLUSION

We discussed the exploitation of evidence in probabilistic inference and highlighted the extent to which
it can render inference tractable. We proposed a particular notion and approach for compiling networks
with evidence, and discussed a number of practical
applications to maximum likelihood estimation, sensitivity analysis and MAP computations. We presented
several empirical results illustrating the power of proposed approach, and showed in particular how it empirically appears to subsume the performance of the
quickscore algorithm.



We formulate in this paper the mini-bucket
algorithm for approximate inference in terms
of exact inference on an approximate model
produced by splitting nodes in a Bayesian
network. The new formulation leads to
a number of theoretical and practical implications. First, we show that branchand-bound search algorithms that use minibucket bounds may operate in a drastically
reduced search space. Second, we show that
the proposed formulation inspires new minibucket heuristics and allows us to analyze existing heuristics from a new perspective. Finally, we show that this new formulation allows mini-bucket approximations to benefit
from recent advances in exact inference, allowing one to significantly increase the reach
of these approximations.

1

INTRODUCTION

Probabilistic reasoning tasks in Bayesian networks are
typically NP–hard, and approximation algorithms are
often sought to address this apparent intractability.
One approach to approximate inference is based on
mini-buckets, a scheme that has been successfully employed by branch-and-bound algorithms for computing
MPEs (Most Probable Explanations) (Dechter & Rish,
2003; Marinescu, Kask, & Dechter, 2003). Roughly
speaking, mini-buckets is a greedy approach to approximate inference that applies the variable elimination algorithm to a problem, but only as long as computational resources allow it. When time and space
constraints keep us from progressing, a mini-buckets
approach will heuristically ignore certain problem dependencies, permitting the process of variable elimination to continue (Zhang & Poole, 1996; Dechter,
1996). Mini-buckets will therefore give rise to a family

of approximations that, in particular, are guaranteed
to produce upper bounds on the value we seek, and
further whose quality depends on the heuristic used to
ignore dependencies.
In this paper, we make explicit in the most fundamental terms the dependencies that mini-bucket approximations ignore. In particular, we reformulate the
mini-bucket approximation using exact inference on an
approximate model, produced by removing dependencies from the original model. We refer to this process
of removing dependencies as node splitting, and show
that any mini-bucket heuristic can be formulated as a
node splitting heuristic.
This perspective on mini-buckets has a number of
implications, both theoretical and practical. First,
it shows how one can significantly reduce the search
space of brand-and-bound algorithms that make use
of mini-bucket approximations for generating upper
bounds. Second, it provides a new basis for designing
mini-bucket heuristics, a process which is now reduced
to specifying an approximate model that results from
node splitting. We will indeed propose a new heuristic and compare it to an existing heuristic, which we
reformulate in terms of node splitting. Third, it allows one to embed the mini-bucket approximation in
the context of any exact inference algorithm—for example, ones that exploits local structure (Chavira &
Darwiche, 2006)—which could speed up the process of
generating mini-bucket bounds, without affecting the
quality of the approximation. We will illustrate this
ability in some of the experiments we present later.
This paper is organized as follows. In Section 2, we
review the MPE task, as well as algorithms for finding
MPEs. In Section 3, we define node splitting operations for Bayesian networks, and show in Section 4
how mini-bucket elimination is subsumed by splitting
nodes. In Section 5, we examine mini-buckets as a
node splitting strategy, and introduce a new strategy
based on jointrees. In Section 6, we consider branchand-bound search for finding MPEs, and show how

58

CHOI ET AL.

we can exploit node splitting to improve the efficiency
of search. In Section 7, we provide empirical support
for the claims in Section 6, and conclude in Section 8.
Proofs and other results appear in the Appendix.

2

MOST PROBABLE
EXPLANATION

We will ground our discussions in this paper using the
problem of computing MPEs, which we define formally
next. Let N be a Bayesian network with variables X,
inducing distribution Pr . The most probable explanation (MPE) for evidence e is then defined as:
MPE (N, e)

def

=

arg max Pr (x),
x∼e

where x ∼ e means that instantiations x and e are
compatible: they agree on every common variable.
Note that the MPE solution may not be unique, in
which case MPE (N, e) denotes a set of MPEs. One
can also define the MPE probability:
MPE p (N, e)

def

=

max Pr (x).
x∼e

A number of approaches have been proposed to tackle
the MPE problem, when a Bayesian network has a
high treewidth. These include methods based on local search (Park, 2002; Hutter, Hoos, & Stützle, 2005)
and max-product belief propagation (e.g., Pearl, 1988;
Weiss, 2000), including generalizations (e.g., Yedidia,
Freeman, & Weiss, 2005; Dechter, Kask, & Mateescu,
2002) and related methods (Wainwright, Jaakkola, &
Willsky, 2005; Kolmogorov & Wainwright., 2005). Although these approaches have been successful themselves, and can provide high-quality approximations,
they are in general non-optimal.
An approach based on systematic search can be used to
identify provably optimal MPE solutions, although the
efficiency of a search depends heavily on the problem
formulation as well as the accompanying heuristics.
In particular, it is quite common also to use branchand-bound search algorithms for computing MPEs and
their probability (e.g., Marinescu et al., 2003; Marinescu & Dechter, 2005). The use of these search algorithms, however, requires the computation of an upper
bound on the MPE probability to help in pruning the
search space. The mini-buckets method is the state of
the art for computing such bounds (Dechter & Rish,
2003). In fact, the success of mini-buckets is most apparent in this context of computing MPEs, which is
the reason we will use this application to drive our
theoretical analysis and empirical results.

X

X

Figure 1: When we split a variable X (left), we create
a clone X̂ that inherits some of the children (right).

3

SPLITTING NODES

We will define in this section a method for approximating Bayesian networks by splitting nodes: An operation that creates a clone X̂ of some node X, where the
clone inherits some of the children of X; see Figure 1.
Definition 1 Let X be a node in a Bayesian network N with children Y. We say that node X is
split according to children Z ⊆ Y when it results in
a network that is obtained from N as follows:
• The edges outgoing from node X to its children Z
are removed.
• A new root node X̂ with a uniform prior is added
to the network with nodes Z as its children.
A special case of node splitting is edge deletion, where
a node is split according to a single child (i.e., splitting also generalizes edge deletion as defined in Choi
& Darwiche, 2006a, 2006b).
Definition 2 Let X → Y be an edge in a Bayesian
network N . We say that node X is split along an edge
X → Y when the node X is split according to child Y .
The following case of node splitting will be the basis of
a splitting strategy that yields a special class of minibucket approximations with implications in search.
Definition 3 Let X be a node in a Bayesian network
N . We say that node X is fully split when X is split
along every outgoing edge X → Y .
Thus, when we fully split a node X, we create one clone
for each of its outgoing edges. Figure 2 illustrates an
example of a network where two nodes have been split.
Node C has been split according to children {D, E},
and Node A has been split along the edge A → D.
A network N 0 which results from splitting nodes in
network N has some interesting properties. To explicate these properties, however, we need to introduce
a function which, given an instantiation x of variables
in network N , gives us an instantiation of clones in N 0
that agrees with the values given to variables in x.

CHOI ET AL.

59

Algorithm 1 ve(N, e): returns MPE p (N, e).
1: i ← 0
2: S ← {f e | f e is a CPT (incorporating e) of N }
3: while S contains variables do
4:
i←i+1
5:
X ← a variable appearing in S
6:
Si ← all factors
Y in S that contain X
7:
fi ← max
f
X

Figure 2: A Bayesian network N (left) and an approximation N 0 (right) found by splitting C according to
{D, E}, and splitting A according to D.
Definition 4 Let N be a Bayesian network, and let
N 0 be the result of splitting nodes in N . If x is an
→
instantiation of variables in N , then let −
x be the compatible instantiation of the corresponding clones in N 0 .
For example, in the split network in Figure 2, an instantiation x = {A = a1 , B = b1 , C = c2 , D = d3 , E = e1 }
→
is compatible with instantiation −
x = {Â = a1 , Ĉ = c2 }.
Moreover, x is not compatible with {Â = a1 , Ĉ = c1 }.
To see the effect that splitting a node can have on a
network, consider a simple two-node network A → B
with binary variables, where θa1 = .2, θb1 |a1 = .1, and
θb1 |a2 = .7. After splitting A according to B, we have:
x
a1 b 1
a1 b 2
a2 b 1
a2 b 2

0

Pr (x)
0.02
0.18
0.56
0.24

x
a1 â1 b1
a1 â1 b2
a1 â2 b1
a1 â2 b2
a2 â1 b1
a2 â1 b2
a2 â2 b1
a2 â2 b2

0

0

0

Pr (x )
0.01
0.09
0.07
0.03
0.04
0.36
0.28
0.12

x
a1 â1 b1
a1 â1 b2
a1 â2 b1
a1 â2 b2
a2 â1 b1
a2 â1 b2
a2 â2 b1
a2 â2 b2

0

0

βPr (x )
0.02
0.18
0.14
0.06
0.08
0.72
0.56
0.24

where β = |A1 | = 2. We see that whenever A1 and its
clone Â1 are set to the same value, we can recover the
original probabilities Pr (x) after splitting, by using
βPr 0 (x0 ). This includes the value of the MPE in N ,
which may no longer be the largest value of βPr 0 (x0 ).
This intuition yields the key property of split networks.
Theorem 1 Let N be a Bayesian network, and let N 0
be the result of splitting nodes in N . We then have
→
MPE (N, e) ≤ βMPE (N 0 , e, −
e ).
Q

Here, β =
network N 0 .

p

C∈C

p

|C|, where C is the set of clones in

That is, the MPE probability with respect to a split
network provides an upper bound on the MPE probability with respect to the original network. We note
that the probability of evidence is also upper bounded
in the split network; see Theorem 3 in the Appendix.

f ∈Si

8:
S ← S − Si ∪ {fi }
9: return product of factors in S

Algorithm 2 mbe(N, e): returns an upper bound on
MPE p (N, e).
{ Identical to Algorithm 1, except for Line 6: }
6:
Si ← some factors in S that contain X

The following corollary shows that splitting degrades
the quality of approximations monotonically.
Corollary 1 Let network N2 be obtained by splitting
nodes in network N1 , which is obtained by splitting
nodes in network N0 . We then have
MPE p (N0 , e)

→
≤ β1 MPE p (N1 , e, −
e1 )
−
→
≤ β MPE (N , e, e ),
2

p

2

2

−
→
where β1 , β2 and →
e1 , −
e2 are as defined by Theorem 1.

4

MINI-BUCKET ELIMINATION

We discuss in this section the relationship between the
approximations returned by split networks and those
computed by the mini-buckets algorithms (Dechter &
Rish, 2003). In particular, we show that every minibuckets heuristic corresponds precisely to a node splitting strategy, where exact inference on the resulting
split network yields the approximations computed by
mini-buckets. Our discussion here will be restricted to
computing MPEs, yet the correspondence extends to
probability of evidence as well.
We start first by a review of the mini-buckets method,
which is a relaxed version of the variable elimination
method given in Algorithm 1 (Zhang & Poole, 1996;
Dechter, 1996). According to this algorithm, variable
elimination starts with a set of factors corresponding
to the CPTs of a given Bayesian network. It then
iterates over the variables appearing in factors, eliminating them one at a time. In particular, to eliminate a variable X, the method multiplies all factors
that contain X and then max-out X from the result.
The bottleneck of this algorithm is the step where the
factors containing X are multiplied, as the resulting

60

CHOI ET AL.
Θ Θ

Θ

Θ Θ

Θ
Θ

Θ
Θ
Θ

∅

∅ ∅

Figure 3: An execution trace of ve on N (left) and
mbe on N (right). The network is defined in Figure 2.

factor may be too big for the computational resources
available. The mini-bucket method deals with this difficulty by making a simple change to the variable elimination algorithm (also known as the bucket elimination
algorithm).1 This change concerns Line 6 in which all
factors containing variable X are selected. In minibuckets, given in Algorithm 2 (Dechter & Rish, 2003),
one chooses only a subset of these factors in order to
control the size of their product. Which particular set
of factors is chosen depends on the specific heuristic
used. Yet, regardless of the heuristic used, the answer
obtained by the mini-buckets method is guaranteed to
be an upper bound on the correct answer.2 One should
note here that the simple change from all to some on
Line 6 implies the following. The number of iterations
performed by Algorithm 1 is exactly the number of
network variables, since each iteration will eliminate
a network variable. However, Algorithm 2 may only
partially eliminate a variable in a given iteration, and
may take multiple iterations to eliminate it completely.
To help us visualize the computations performed by
Algorithms 1 and 2, consider their execution trace.
Definition 5 Given an instance of ve or mbe run
on a given network N , we define its execution trace T
as a labeled DAG which adds, for each iteration i,
• a node i, labeled by the factor set Si , and
• directed edges j → i, for all factors fj ∈ Si , each
labeled by the corresponding factor fj .
1

More precisely, bucket elimination is a particular implementation of variable elimination in which one uses a list
of buckets to manage the set of factors during the elimination process. Although the use of such buckets is important
for the complexity of the algorithm, we ignore them here
as the use of buckets is orthogonal to our discussion.
2
This is also true for versions of the algorithm that compute the probability of evidence.

Figure 3 depicts traces of both algorithms on the network in Figure 2 (left). Variable elimination, whose
trace is shown on the left, eliminates variables from A
to E, and performs five iterations corresponding to the
network variables. Mini-buckets, however, performs
seven iterations in this case, as it takes two iterations
to eliminate variable A and two iterations to eliminate
variable C. Note that an execution trace becomes a
rooted tree after reversing the direction of all edges.
Given an execution trace T , we can visually identify
all of the network CPTs used to construct any factor
in Algorithms 1 and 2. For mini-buckets, we also want
to identify a subtrace of T , but one that covers only
those network CPTs that are relevant to a particular
attempt at eliminating variable X at iteration i. A
CPT is not relevant to iteration i if X is eliminated
from it in a later iteration, or if X has already been
eliminated from it in some previous iteration.
Given a trace T , we thus define the subtrace Ti relevant
to an iteration i as the nodes and edges of T that are
reachable from node i (including itself), but only by
walking up edges j → i, and only those edges labeled
with factors fj mentioning variable X. For example,
in Figure 3 (right), the subtrace Ti for iteration i = 7 is
the chain 4 → 6 → 7. In the same trace, the subtrace
Ti for iteration i = 5 is the chain 1 → 3 → 5.
Given a subtrace Ti , we can identify only those CPTs
that are relevant to a partial elimination of X, but
further, the set of variables those CPTs belong to.
Definition 6 Let i be an iteration of mbe where we
eliminate variable X, and let Ti be the subtrace of T
that is relevant to iteration i. The basis B of an iteration i is a set of variables where Y ∈ B iff:
• ΘY |U ∈ Sj for some node j of Ti , and
• X ∈ {Y } ∪ U,
where ΘY |U are CPTs in N .
For example, in Figure 3 (right), the basis of iteration
i = 4 is {D, E}, since C is eliminated from the CPTs
of D and E at iteration 4.
Given this notion, we can show how to construct a network with split nodes, that corresponds to a particular
execution of the mini-bucket method. In particular,
exact variable elimination in N 0 will be able to mimic
mini-bucket elimination in N , with the same computational complexity. This is given in Algorithm 3 which
returns both a network N 0 and an ordering π 0 of the
variables in N 0 (this includes the variables in original
network N and their clones in N 0 ). Figure 4 shows a
trace corresponding to a split network, and the associated variable order.

CHOI ET AL.
Algorithm 3 split-mbe(N, e): returns a split network N 0 and variable ordering π 0 , corresponding to a
run of mbe(N, e).
1: N 0 ← N
2: for each iteration i of mbe(N, e) do
3:
X ← as chosen on Line 5 of mbe
4:
Si ← as chosen on Line 6 of mbe
5:
B ← basis of iteration i
6:
if X ∈ B then
7:
π 0 (i) ← X
8:
else
9:
split node X in N 0 according to children B
10:
π 0 (i) ← clone X̂ of X resulting from split
11: return network N 0 and ordering π 0
Θ Θ

Θ Θ
Θ

Θ

Θ
Θ

5

61

NODE-SPLITTING STRATEGIES

Given the correspondences in the previous section,
every mini-bucket heuristic can now be interpreted as
a node splitting strategy. Consider for example the
mini-bucket heuristic given in (Dechter & Rish, 2003),
which is a greedy strategy for bounding the size of
the factors created by mbe. This heuristic works as
follows, given a bound on the size of the largest factor:
• A particular variable order is chosen and followed
by the heuristic.
• When processing variable X, the heuristic will
pick a maximal set of factors Si whose product
will be a factor of size within the given bound.
• The above process is repeated in consecutive iterations and for the same variable X until variable
X is eliminated from all factors.
• Once X is completely eliminated, the heuristic
picks up the next variable in the order and the
process continues.

∅ ∅

∅ ∅

Figure 4: An execution trace of mbe on N (left) and
ve on N 0 (right). For simplicity, we ignore the priors of
clone variables in N 0 . Networks are defined in Figure 2.

We now have our basic correspondence between minibuckets and node splitting.
Theorem 2 Let N be a Bayesian network, e be
some evidence, and let N 0 and π 0 be the results of
split-mbe(N, e). We then have:
→
mbe(N, e) = βMPE (N 0 , e, −
e ),
Q

p

where β = C∈C |C| and C are the clone variables
in N 0 . Moreover, variable elimination on network N 0
using the variable order π 0 has the same time and space
complexity of the corresponding run mbe(N, e).
Note that the ordering π 0 returned by Algorithm 3 may
not be the most efficient ordering to use when running
exact variable elimination in a split network: there
→
may be another variable order where ve(N 0 , e, −
e ) produces smaller intermediate factors than mbe(N, e). Indeed, we need not restrict ourselves to variable elimination when performing inference on the split network,
as any exact algorithm suffices for this purpose. This
property can have significant practical implications, a
point we highlight in Section 7 where we exploit recent
advances in exact inference algorithms.

This heuristic tries then to minimize the number of
instances where a proper subset of factors is selected
in Line 6 of Algorithm 2, and can be interpreted as a
heuristic to minimize the number of clones introduced
into an approximation N 0 . In particular, the heuristic
does not try to minimize the number of split variables.
We now introduce a new node splitting strategy based
on fully splitting nodes, where a variable is split along
every outgoing edge. The strategy is also a greedy algorithm, which attempts to fully split the variable that
contributes most to the difficulty of running a jointree algorithm in the approximate network N 0 . This
process is repeated until the network is sufficiently simplified. In particular, the method starts by building a
jointree of the original network. It then picks a variable whose removal from the jointree will introduce the
largest reduction in the sizes of the cluster and separator tables. Once a variable is chosen, it is fully split.
One can obtain a jointree for the split network by simply modifying the existing jointree, which can then be
used to choose the next variable to split on.3 In our
empirical evaluation, we go further and construct a
new jointree for the simpler network, and choose the
next variable to split from it. This process is repeated
until the largest jointree cluster is within our bound.
We now have two strategies for splitting nodes in a network. The first is based on the classical mini-bucket
heuristic that tries to minimize the number of clones,
3
In particular, one can simply adjust the separators and
clusters without changing the structure of the jointree.

62

CHOI ET AL.

Algorithm 4 split-bnb: z and q ? are global variables.
→
1: q ← βMPE p (N 0 , z, −
z)
?
2: if q > q then
3:
if z is a complete instantiation then
4:
q? ← q
5:
else
6:
pick some X ∈
/Z
7:
for each value x of variable X do
8:
z ← z ∪ {X = x}
9:
split-bnb()
10:
z ← z − {X = x}

and the second one is based on reducing the size of
jointree tables and tries to minimize the number of
split variables. Recall that Corollary 1 tells us that
the quality of the MPE bound given by a split network degrades monotonically with further splits. As
we shall see in Section 6, and empirically in Section 7,
it may sometimes be more important to minimize the
number of split variables, rather than the number of
clones, in the context of branch-and-bound search.

6

SEARCHING FOR MPE’S

When computing the MPE is too difficult for traditional inference algorithms, we can employ systematic
search methods to identify provably optimal solutions.
Suppose now that we are given network N and evidence e, and that we want to compute MPE p (N, e)
using depth-first branch-and-bound search. We want
then to select some network N 0 using a node-splitting
heuristic from the previous section to allow for exact
inference in N 0 (say, by the jointree algorithm). Theorem 1 gives us the upper bound
→
MPE p (N, e) ≤ βMPE p (N 0 , e, −
e ).
Moreover, one can easily show that if z is a complete
variable instantiation x of N , we then have
→
MPE p (N, x) = βMPE p (N 0 , x, −
x );
see Lemma 1. These two properties form the basis of
our proposed search algorithm, split-bnb, which is
summarized in Algorithm 4.
Throughout the search, we keep track of two global
variables. First, z is a partial assignment of variables
in the original network that may be extended to produce an MPE solution in MPE (N, e). Second, q ? is a
lower bound on the MPE probability that is the largest
probability of a complete instantiation so far encountered. The search is initiated after setting z to e and
q ? to 0.0: we use evidence e as the base instantiation,

and 0.0 as a trivial lower bound. Upon completion
of the search, we have the optimal MPE probability
q ? = MPE p (N, e).
At each search node, we compute a bound on the best
completion of z by performing exact inference in the
approximate network N 0 . If the resulting upper bound
q is greater than the current lower bound q ? , then we
must continue the search, since it is possible that z
can provide us with a better solution than what we
have already found. In this case, if z is already a complete instantiation, it is easy to show that q is equal
to Pr (z) (by Lemma 1, in the Appendix) and that we
have found a new best candidate solution q ? . If z is
not a complete instantiation, we select some variable
X that has not been instantiated. For each value x
of X, we add the assignment {X = x} to z and call
split-bnb recursively with the new value of z and our
candidate solution q ? . Upon returning from the recursive call, we retract the assignment {X = x}, and
continue to the next value of X.
6.1

REDUCING THE SEARCH SPACE

Consider now the following critical observation.
Proposition 1 Let N be a Bayesian network, and let
N 0 be the result of splitting nodes in N . If Z contains
all variables that were split in N to produce N 0 , then
→
MPE p (N, z) = βMPE p (N 0 , z, −
z ),
where β =

Q

C∈C

|C| and C are all the clones in N 0 .

According to this proposition, once we have instantiated in z all variables that were cloned, the resulting
approximation is exact. This tells us that during our
search, we need not instantiate every one of our network variables X. We need only instantiate variables
in a smaller set of variables Z ⊆ X containing precisely
the variables that were split in N to produce N 0 . Once
the bound on the MPE probability becomes exact, we
know that we will not find a better solution by instantiating further variables, so we can stop and backtrack.
This observation allows us to work in a reduced search
space: rather than searching in a space whose size is
exponential in the number of network variables X, we
search in a space whose size is exponential only in the
number of split variables!
Moreover, if our variable splitting strategy seeks to
minimize the number of split variables, rather than
the number of clones introduced, we can potentially
realize dramatic reductions in the size of the resulting
search space. As we shall see in the following section,
this can have a drastic effect on the efficiency of search.

CHOI ET AL.

In our experiments, we compared the splitting strategy based on a jointree (JT) with the strategy based
on a greedy mini-bucket elimination (MB), both described in Section 5. In particular, we asserted limits
on the maximum cluster size for JT, and equivalently,
the size of the largest factor for MB. We then compared the two strategies across a range of cluster and
factor size limits from 0 to 12, where 0 corresponds to
a fully disconnected network and 12 corresponds to exact inference (no splits). In all of our experiments, to
emphasize the difference between splitting strategies,
we make neutral decisions in the choice of a search seed
(we use a trivial seed, 0.0), variable ordering (random)
and value ordering (as defined by the model).
First, consider Figure 5, which compares the effectiveness of node splitting strategies in minimizing the
number of variables split and the number of clones.
Recall that the heuristic based on jointrees (JT) seeks
to minimize the number of split variables, while the
greedy mini-bucket (MB) strategy would seek to minimize the number of clones. We see that in Figure 5, on
4

In particular, each network is associated with its own
piece of evidence corresponding to a codeword received via
transmission through a (simulated) noisy Gaussian channel, with standard deviations ranging from σ = 0.2 to
σ = 0.8 in steps of 0.1.

JT
MB

10
5
0
0

80

JT
MB

60
40
20
0
0

5
10
log2(max cluster size)

5
10
log2(max cluster size)

Figure 5: Comparing splitting heuristics.

10

10

10

JT
MB

4

2

0

0

5
10
log (max cluster size)
2

search nodes

We begin with experiments on networks for decoding
error-correcting codes (see, e.g., Frey & MacKay, 1997;
Rish, Kask, & Dechter, 1998). We first consider simpler networks, that correspond to codes containing 16
information bits and 24 redundant bits. Each of our
plot points is an average of 42 randomly generated networks: 6 networks for each of 7 levels of noise.4 Here,
an MPE solution would recover the most likely word
encoded prior to transmission. Our method for exact
inference in the approximate model is based on compiling Bayesian networks (Chavira & Darwiche, 2007),
an approach that has already been demonstrated to be
effective in branch-and-bound search for MAP explanations (Huang, Chavira, & Darwiche, 2006).

# of vars split

We present empirical results in this section to highlight the trade-offs in the efficiency of search based on
the quality of the bound resulting from different node
splitting strategies, and the size of the resulting search
space. We further illustrate how our framework allows
for significant practical gains with relatively little effort, by employing state-of-the-art algorithms for exact inference in the approximate, node-split network.
Thus, our goal here is, not to evaluate a completely
specified system for MPE search, but to illustrate the
benefits that our node-splitting perspective can bring
to existing systems.

15

# of clones created

EMPIRICAL OBSERVATIONS

search space size

7

63

10

10

5

0

0

5
10
log (max cluster size)
2

Figure 6: Evaluating the efficiency of search. On the
right, the top pair searches in the full space, and the
bottom pair searches in the reduced space.

the left, our jointree (JT) method can split nearly half
of the variables that the mini-bucket (MB) strategy
splits. On the other hand, we see that on the right, the
mini-bucket (MB) strategy is introducing fewer clones.
Note that on both extremes (no splits and all split),
MB and JT are identical.
To see the impact that reducing the number of split
variables has on the efficiency of search, consider Figure 6. On the left, we see that JT can get an order
of magnitude savings over MB in the size of the reduced search space, which is exponential only in the
number of split variables (see again Figure 5). Consider now, on the right, the number of nodes visited
while performing split-bnb search. The top pair plots
the efficiency of search using the full search space (JTF and MB-F), while the bottom pair plots the efficiency of using the reduced search space (JT-R and
MB-R). We see that both JT-R and MB-R experience
several orders of magnitude improvement when using
the reduced-search space versus the full search space.
When we compare JT-F and MB-F (top pair), we see
that MB-F is in fact more efficient in terms of the
number of nodes visited. In this setting, where both
methods are searching in the same space, we see that
the number of clones introduced appears to be the
dominant factor in the efficiency of search. This is
expected, as we expect that the upper bounds on the
MPE probability should be tighter when fewer clones
are introduced. When we now compare JT-R and
MB-R (bottom pair), we see that the situation has

64

CHOI ET AL.
JT
MB

# of clones created

# of vars split

30
20
10

0

10
20
log (max cluster size)

JT
MB

80
60
40
20
0

10
20
log (max cluster size)
2

2

JT
MB

search nodes

search space size

Figure 7: Comparing splitting heuristics.

5

10

0

4

10

2

10

10
20
log2(max cluster size)

0

10
20
log2(max cluster size)

Figure 8: Evaluating the efficiency of search.

Table 1: Compilation versus Variable Elimination
Network
90-20-1
90-20-2
90-20-3
90-20-4
90-20-5
90-20-6
90-20-7
90-20-8
90-20-9
90-20-10

Search
Nodes
14985
137783
3065
4545
29343
5065
2987
6213
5121
8419

AC
Time (s)
18
111
4
3
38
3
2
6
5
10

VE
Time (s)
2417
15953
1271
988
6579
630
1155
812
2367
2343

Imp.
135
144
334
355
173
227
485
146
480
235

reversed, and that JT-R is now outperforming MB-R.
Here, each method is performing search in their own
reduced search spaces. A strategy based on reducing
the number of split variables reduces the size of the
search space, and this reduction now dominates the
quality of the bound.
Figures 7 and 8 depict similar results but for larger
coding networks, in which we have a rate 21 code with
32 information bits and 32 redundant bits. Note that
only the reduced space was used for search here.
Our approach based on node splitting has another major advantage, which we have only briefly mentioned
thus far. By formulating mini-buckets as exact inference in an approximate network, the evaluation of the
mini-bucket approximation need not rely on any specific exact inference algorithm. We mention here that

the arithmetic circuit (AC) approach we have been
using to compute the bound indeed has a key advantage over mainstream algorithms, in that it is able
to effectively exploit certain types of local structure
(Chavira & Darwiche, 2006). To highlight the extent
to which using a different algorithm can be significant,
we constructed another set of experiments. In each, we
used a different grid network, first introduced in (Sang,
Beame, & Kautz, 2005), and constructed a single MPE
query. Each grid network has treewidth in the low
thirties, just out of reach for traditional algorithms for
exact inference. We ran our search twice, each time using a different algorithm to compute the mini-bucket
bound: the first using AC and the second using standard variable elimination (that does not exploit local
structure). Table 1 shows the results for each network,
including the number of search nodes visited and, for
each algorithm, the total search time. For each network, we performed two identical searches for each algorithm: the only difference being in how the bound
was computed. Consequently, the dramatic differences
we observe reflect the ability of the AC approach to
exploit local structure, showing how advances in exact
inference can be easily utilized to extend the reach of
mini-bucket approximations.

8

CONCLUSION

We presented in this paper a new perspective on minibucket approximations, formulating it in terms of exact inference in an approximate network, but one
found by splitting nodes. This perspective has led to a
number of theoretical and practical insights. For one,
it becomes apparent that a branch-and-bound search
using a mini-bucket bound may operate in a drastically reduced search space. This suggests a heuristic for identifying a mini-bucket approximation that
is explicitly based on minimizing this search space,
rather than the quality of the resulting bound. Empirically, we observe that a reduced search space can have
more impact than a better bound, in terms of the efficiency of branch-and-bound search. Moreover, as our
approach is independent of the algorithm used for exact inference in the resulting approximate network, we
can effortlessly employ state-of-the-art algorithms for
exact inference, including those that can exploit compilation and local structure.

A

PROOFS

Lemma 1 Let N be a Bayesian network, and let N 0
be the result of splitting nodes in N . We then have
→
Pr (x) = βPr 0 (x, −
x ).

CHOI ET AL.
Q
Here β = C∈C |C|, where C is the set of clones in
network N 0 .
→
Proof of Lemma 1 Note first that −
x is an instantiation of only root variables, and that all clones have
uniform priors, i.e., θc = |C|−1 . We then have that
Y
Y
→
Pr 0 (−
x) =
θc =
|C|−1 = β −1 .
−
c∼→
x

C∈C

→
Since instantiation x is compatible with −
x , where a
variable and its clones are set to the same value, we
→
find in Pr 0 (x | −
x ) that clone variables act as selectors
for the CPT values composing Pr (x). Thus
→
→
→
Pr (x, −
x ) = Pr 0 (x | −
x )Pr 0 (−
x ) = Pr (x)β −1
0

→
and we have Pr (x) = βPr 0 (x, −
x ), as desired.



Proof of Theorem 1 Suppose for contradiction that
there exists an instantiation z ∈ MPE (N, e) such that
→
Pr (z) > βMPE p (N 0 , e, −
e ). By Lemma 1, the instan→
−
tiation z gives us
→
→
Pr (z) = βPr 0 (z, −
z ) > βMPE p (N 0 , e, −
e ),
→
contradicting the optimality of MPE p (N 0 , e, −
e ).



Proposition 1 is in fact a generalization of Lemma 1
from a complete instantiation x to a partial instantiation z where Z contains all nodes that have been split
in N 0 . Note that splitting a node X when the value
of X has already been fixed corresponds to a common preprocessing rule for Bayesian networks given
evidence. In particular, when a given piece of evidence
z fixes the value of variable Z, any edge Z → Y can be
pruned and a selector node Ẑ can be made a parent of
Y . Node Ẑ is then set to the value that instantiation
z assigns to Z. This pruning process yields a simpler
network which corresponds exactly to the original network for any query of the form {α, z}.
Proof of Proposition 1 From the correspondence
to pruning edges outgoing instantiated variables, we
know that queries of the form {α, z}, including complete instantiations {x, z}, are equivalent in N condi→
tioned on z and N 0 conditioned on {z, −
z }. Thus the
MPEs of each network must also be the same.

Proof of Theorem 2 Given the trace of an instance
of mbe(N, e), algorithm split-mbe(N, e) returns a
network N 0 and an ordering π 0 of variables in N 0 .
We show, by induction, that each iteration of ve
on N 0 mimics each iteration of mbe on N . We can
then conclude that the product of factors returned by
both must be the same, and further, that they are of
the same time and space complexity. In particular,

65

→
we show how ve(N, e, −
e ) mimics mbe(N, e) first on
Line 2, and then Lines 5, 6 and 7, in Algorithms 1 and
2. For simplicity, we ignore the constant factor β that
the clone CPTs contribute to the MPE value of N 0 .
On Line 2 (iteration i = 0), by construction, the CPTs
in N are the same as the CPTs in N 0 , after relabeling.
For iterations i > 0, assume for induction that the
factors available to both ve and mbe are the same.
On Line 5, if mbe picked variable X on Line 5, then
algorithm ve picks variable X 0 = π 0 (i), which is either
X or a clone X̂, by construction (Lines 7 and 10 of
Algorithm 3).
On Line 6, each factor in the set Si is either 1) a CPT
mentioning X, or 2) a factor that is composed of a
CPT mentioning X. The variables that these CPTs
belong to are the variable set B, the basis of iteration
i. Algorithm 3 decides to split (or not split), so that
each variable in B will have a CPT in N 0 that mentions
X 0 = π 0 (i). We know by induction, that all factors f
selected by mbe are available for selection by ve in N 0 .
Since Algorithm 3 ensures that each of these factors
f now mention X 0 , and since ve picks all factors
mentioning X 0 , we know ve picks the same factors
mbe picked.
On Line 7, consider any variable Z mentioned in Si .
Let j ≥ i be the iteration where Z is eliminated in
mbe. The relevant CPTs mentioning Z at iteration i
are among the relevant CPTs of the basis at iteration
j. Thus, Algorithm 3 ensures that they all mention the
same instance of Z in N 0 . Thus, the resulting product
of factors fi must be the same after relabeling.

A node-split network also upper bounds Pr (e). The
following theorem corresponds to a mini-bucket bound
on the probability of evidence (Dechter & Rish, 2003).
Theorem 3 Let N be a Bayesian network, and let N 0
be the result of splitting nodes in N . We then have
→
Pr (e) ≤ βPr 0 (e, −
e ).
Q
Here, β = C∈C |C|, where C is the set of clones in
network N 0 .
Proof of Theorem 3 By Lemma 1, we know that
→
Pr (x) = βPr 0 (x, −
x ). Therefore:
X
X
→
Pr (e) =
Pr (x) = β
Pr 0 (x, −
x)
x∼e

≤β

X

x∼e

→
Pr (x ) = βPr 0 (e, −
e)
0

0

−
x0 ∼e,→
e

where x0 is an instantiation of variables in N 0 , but
where the values of the original network variables are
not necessarily compatible with the values of the clone
→
variables (as they are in x and −
x ).


66

CHOI ET AL.

B

LOOP-CUTSET CONDITIONING

The loop-cutset conditioning algorithm and split-bnb
search are closely related when our splitting strategy
performs only full splits (see Definition 3). This correspondence reveals the difficulty of answering the following decision problem:
D-FS: Given k and ω, does there exist a set
Z of size ≤ k such that fully splitting nodes
Z in network N results in an approximate
network N 0 with treewidth ≤ ω?
We now state the following negative result.
Theorem 4 Decision problem D-FS is NP–complete.
Hardness can be shown by reduction from the loopcutset problem, which is NP–complete (Suermondt
& Cooper, 1990). In particular, when we fully split
enough variables Z to render N 0 a polytree, then Z
also constitutes a loop-cutset of N .
0

If N is rendered a polytree, and we ignore the bound
during split-bnb search and further employ the reduced search space over split variables Z, then splitbnb reduces to loop-cutset conditioning. More generally, when we split enough variables Z so that network
N 0 has treewidth ω, split-bnb reduces to ω–cutset
conditioning (Bidyuk & Dechter 2004).
0

Assuming that for exact inference in N , we use an
algorithm that is exponential in the treewidth ω of
N 0 , this correspondence tells us that the worst-case
time and space complexity of split-bnb search is precisely that of ω–cutset conditioning. In particular, say
that n is the number of variables in N , value m is the
number of variables cloned in N 0 , and value ω is the
treewidth of network N 0 . The worst-case time complexity of split-bnb search is thus
O(n exp{ω} · exp{m}) = O(n exp{ω + m}),
since we spend O(n exp{ω}) time at each of at most
exp{m} search nodes. Note that the space complexity
of split-bnb search is only O(n exp{ω} + m).

Choi, A., & Darwiche, A. (2006b). A variational approach
for approximating Bayesian networks by edge deletion. In UAI, pp. 80–89.
Dechter, R. (1996). Bucket elimination: A unifying framework for probabilistic inference. In UAI, pp. 211–219.
Dechter, R., Kask, K., & Mateescu, R. (2002). Iterative
join-graph propagation. In UAI, pp. 128–136.
Dechter, R., & Rish, I. (2003). Mini-buckets: A general
scheme for bounded inference. J. ACM, 50 (2), 107–
153.
Frey, B. J., & MacKay, D. J. C. (1997). A revolution:
Belief propagation in graphs with cycles. In NIPS,
pp. 479–485.
Huang, J., Chavira, M., & Darwiche, A. (2006). Solving
map exactly by searching on compiled arithmetic circuits. In AAAI, pp. 143–148.
Hutter, F., Hoos, H. H., & Stützle, T. (2005). Efficient
stochastic local search for MPE solving. In IJCAI,
pp. 169–174.
Kolmogorov, V., & Wainwright., M. J. (2005). On the
optimality of tree-reweighted max-product message
passing. In UAI.
Marinescu, R., & Dechter, R. (2005). AND/OR branchand-bound for graphical models. In IJCAI, pp. 224–
229.
Marinescu, R., Kask, K., & Dechter, R. (2003). Systematic
vs. non-systematic algorithms for solving the MPE
task. In UAI, pp. 394–402.
Park, J. D. (2002). Using weighted max-sat engines to solve
MPE. In AAAI/IAAI, pp. 682–687.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers, Inc., San Mateo, California.
Rish, I., Kask, K., & Dechter, R. (1998). Empirical evaluation of approximation algorithms for probabilistic
decoding. In UAI, pp. 455–463.
Sang, T., Beame, P., & Kautz, H. (2005). Solving Bayesian
networks by weighted model counting. In AAAI,
Vol. 1, pp. 475–482. AAAI Press.
Suermondt, H. J., & Cooper, G. F. (1990). Probabilistic
inference in multiply connected networks using loop
cutsets. IJAR, 4, 283–306.
Wainwright, M. J., Jaakkola, T., & Willsky, A. S. (2005).
Map estimation via agreement on trees: messagepassing and linear programming. IEEE Transactions
on Information Theory, 51 (11), 3697–3717.


