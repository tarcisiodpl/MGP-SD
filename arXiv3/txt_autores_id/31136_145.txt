
Bayesian priors offer a compact yet general
means of incorporating domain knowledge
into many learning tasks. The correctness of
the Bayesian analysis and inference, however,
largely depends on accuracy and correctness
of these priors. PAC-Bayesian methods overcome this problem by providing bounds that
hold regardless of the correctness of the prior
distribution. This paper introduces the first
PAC-Bayesian bound for the batch reinforcement learning problem with function approximation. We show how this bound can be
used to perform model-selection in a transfer learning scenario. Our empirical results
confirm that PAC-Bayesian policy evaluation
is able to leverage prior distributions when
they are informative and, unlike standard
Bayesian RL approaches, ignore them when
they are misleading.

1

Introduction

Prior distribution along with Bayesian inference have
been used in multiple areas of machine learning to
incorporate domain knowledge and impose general
variance-reducing constraints, such as sparsity and
smoothness, within the learning process. These methods, although elegant and concrete, have often been
criticized not only for their computational cost, but
also for their strong assumptions on the correctness of
the prior distribution. Bayesian guarantees often fail
to hold when the inference is performed with priors
that are different from the distribution of the underlying true model.
Frequentist methods such as Probably Approximately
Correct (PAC) learning, on the other hand, provide distribution-free convergence guarantees (Valiant,

Csaba Szepesvári
Department of Computing Science
University of Alberta
Edmonton, Canada
szepesva@ualberta.ca

1984). These bounds, however, are often loose and impractical, reflecting the inherent difficulty of the learning problem when no assumptions are made on the
distribution of the data.
Both Bayesian and PAC methods have been proposed separately for reinforcement learning (Kearns
and Singh, 2002; Brafman and Tennenholtz, 2003;
Strehl and Littman, 2005; Kakade, 2003; Duff, 2002;
Wang et al., 2005; Poupart et al., 2006; Kolter and
Ng, 2009), where an agent is learning to interact with
an environment to maximize some objective function.
These methods are mostly focused on the so-called
exploration–exploitation problem, where one aims to
balance the amount of time spent on gathering information about the dynamics of the environment and the
time spent acting optimally according to the current
estimates. PAC methods are much more conservative
and spend more time exploring the system and collecting information. Bayesian methods, on the other
hand, are greedier and only solve the problem over a
limited planning horizon.
The PAC-Bayesian approach (McAllester, 1999;
Shawe-Taylor and Williamson, 1997), takes the best
of both worlds by combining the distribution-free correctness of PAC theorems with the data-efficiency of
Bayesian inference. PAC-Bayesian bounds do not require the Bayesian assumption to hold. They instead
measure the consistency of the prior over the training data, and leverage the prior only when it seems
informative. The empirical results of model selection
algorithms for classification tasks using these bounds
are comparable to some of the most popular learning algorithms, such as AdaBoost and Support Vector
Machines (Germain et al., 2009).
Fard and Pineau (2010) introduced the idea of PACBayesian model-selection in reinforcement learning
(RL) for finite state spaces. They provided PACBayesian bounds on the approximation error in the
value function of stochastic policies when a prior distribution is available either on the space of possible

models, or on the space of value functions. Model selection based on these bounds provides a robust use of
Bayesian priors outside the Bayesian inference framework. Their work, however, is limited to small and
discrete domains, and is mostly useful when sample
transitions are drawn uniformly across the state space.
This is problematic as most RL domains are relativity
large, and require function approximation over continuous state spaces.
This paper provides the first PAC-Bayesian bound
for value function approximation on continuous state
spaces. We use results by Samson (2000) to handle
non i.i.d. data that are collected on Markovian processes, and use this, along with general PAC-Bayesian
inequalities, to get a bound on the approximation error
of a value function sampled from any distribution over
measurable functions. We empirically evaluate these
bounds for model selection on two different continuous RL domains in a knowledge-transfer setting. Our
results show that a PAC-Bayesian approach in this setting is indeed able to use the prior distribution when
it is informative and matches the data, and ignore it
when it is misleading.

2

the transition and reward models are not known, one
can use a finite sample set of transitions to learn an
approximate value function. Least-squares temporal
difference learning (LSTD) and its derivations (Boyan,
2002; Lagoudakis and Parr, 2003) are among the methods used to learn a value function based on a finite
sample.

3

A general PAC-Bayes bound

We begin by first stating a general PAC-Bayes bound.
In the next section, we use this result to derive our
main bound for the approximation error in an RL setting.
Let F be a class of real-valued functions over a wellbehaved domain X (i.e., X could be a bounded measurable subset of a Euclidean space). For ease of
presentation, we assume that F has countably many
functions. For a measure ρ over F, and a functional,
def R
R : F → R, we define ρR = R(f )dρ(f ).
Theorem 1. Let R be a random functional over F
with a bounded range. Assume that for some C > 0,
c > 1, for any 0 < δ < 1 and f ∈ F, w.p. 1 − δ,

Background and Notation
r

A Markov Decision Process (MDP) M = (X , A, T, R)
is defined by a (possibly infinite) set of states X ,
a set of actions A, a transition probability kernel
T : X × A → M(X ), where T (.|x, a) defines the distribution of next state given that action a is taken in
state x, and a (possibly stochastic) reward function
R : X × A → M([0, Rmax ]). Throughout the paper,
we focus on finite-action, continuous state, discountedreward MDPs, with the discount factor denoted by
γ ∈ [0, 1). At discrete time steps, the reinforcement
learning agent chooses an action and receives a reward.
The environment then changes to a new state according to the transition kernel.
A policy is a (possibly stochastic) function from states
to actions. The value of a state x for policy π, denoted
the expected value of the discounted sum
by V π (x), isP
of rewards ( t γ t rt ) if the agent starts in state x and
acts according to policy π. The value function satisfies
the Bellman equation:
Z
π
V (x) = R(x, π(x)) + γ V π (y)T (dy|x, π(x)). (1)
There are many methods developed to find the value of
a policy (policy evaluation) when the transition and reward functions are known. Among these there are dynamic programming methods in which one iteratively
applies the Bellman operator (Sutton and Barto, 1998)
to an initial guess of the optimal value function. When

R(f ) ≤

log(C/δ)
.
c

(2)

Then, for any measure ρ0 over F, w.p. 1 − δ, for all
measures ρ over F:
s
ρR ≤

log( 1+C(c−1)
) + K(ρ, ρ0 )
δ
,
c−1

(3)

where K(ρ, ρ0 ) denotes the Kullback-Leibler divergence
between ρ and ρ0 .
The proof (included in the appendix) is a straightforward generalization of the proof presented by
Boucheron et al. (2005).

4

Application to RL

Consider some MDP, with state space X , and a policy
π whose stationary distribution ρπ exists. Let Dn =
0
((Xi , Ri , Xi+1
)ni=1 ) be a random sample of size n such
π
0
that Xi ∼ ρ , (Xi+1
, Ri ) ∼ P π (·|Xi ), where P π is the
Markov kernel underlying policy π: The ith datum
0
(Xi , Ri , Xi+1
) is an elementary transition from state
0
Xi to state Xi+1
while policy π is followed and the
reward associated with the transition is Ri . Further, to
simplify the exposition, let (X, R, X 0 ) be a transition
whose joint distribution is the same as the common
0
joint of (Xi , Ri , Xi+1
).

Define the functionals R, Rn over the space of realvalued, bounded measurable functions over X as follows: Let V : X → R be such a function. Then
R(V )
Rn (V )

= E
=

n
o2 
R + γV (X 0 ) − V (X)
,

2

(Antos et al., 2008)).1 Thus, for ε2π (V ) = kV − V π kρπ ,
ε2π (V ) ≤

1
[R(V ) − Γπ (V )] .
(1 − γ)2

(7)

Combining this inequality with (5), we get the following result:

n
o2
1 Xn
Ri + γV (Xi0 ) − V (Xi ) .
n i=1

The functional R is called the squared sample Bellman error, while Rn is the empirical squared sample
Bellman error. Clearly, E [Rn (V )] = R(V ) holds. The
following lemma (proved in the appendix) is a concentration bound connecting R and Rn .
Lemma 2. Under proper mixing conditions for the
sample, and assuming that the random rewards are
sub-Gaussian, there exists constants c1 > 0, c2 ≥ 1
which depend only on P π such that for any Vmax > 0,
for any measurable function V bounded by Vmax , and
any 0 < δ < 1, w.p. 1 − δ,

Theorem 3. Fix a countable set F of real-valued,
measurable functions with domain X , which are
bounded by Vmax . Assume that the conditions of
Lemma 2 hold and let c1 , c2 be as in this lemma. Fix
any measure µ0 over these functions. Assume that
2
n > Vmax
c1 . Then, for all 0 < δ < 1, with probability
1 − δ, for all measures µ over F:
v


u
(
u log
c2 n
+ K(µ, µ0 )
2
u
c
V
δ
1
1 max
t
µR
+
µε2π ≤
n
n
(1 − γ)2
2
Vmax
c1 − 1
)
− µ E [Γπ ]

.
2

r
R(V ) − Rn (V ) ≤

c 
2 c
Vmax
1
2
log
.
n
δ

(4)

Hence, by Theorem 1, for any countable set F of functions V bounded by Vmax , for any distribution µ0 over
2
c1 , then for all 0 < δ < 1,
these functions, if n > Vmax
w.p. 1 − δ, for all measures µ over F:
v


u
u log
c2 n
2
u
c1 Vmax
δ + K(µ, µ0 )
µ(R − Rn ) ≤ t
.
n
V 2 c1 − 1

(5)

Further, the same bound holds for V̄µ − V π ρπ , where
R
V̄µ = V dµ(V ) is the µ-average of value functions
from F.
Proof. The first statement follows from (7) combined
with (5), as noted earlier. To see this just replace R(V ) in (7) with R(V ) − Rn (V ) + Rn (V ).
Then, integrate both sides with respect to µ and apply (5) to bound µ(R − Rn ). The second part follows from the first part, Fubini’s theorem and Jensen’s
R π
2
2
(V − V )dµ(V ) ρπ ≤
inequality: V̄µ − V π ρπ =
R
2
kV π − V kρπ dµ(V ) = µε2π .

max

Now, we show how this bound can be used to derive a
PAC-Bayes bound on the error of a value function V
that is drawn from an arbitrary distribution over measurable functions. For a distribution ρ over the state
L2 norm: kV k2ρ =
Rspace X2, let k · kρ be the weighted
π
[V (x)] dρ(x). Further, let B be the Bellman operator underlying π: B π V (x) = E [R + γV (X 0 )|X = x].
Fix some V . Since B π is a γ-contraction w.r.t. the
norm k · kρπ , a standard argument shows that (Bertsekas and Tsitsiklis, 1996):

kV − V π kρπ ≤

kB π V − V kρπ
1−γ

.

(6)

Now,
variance decomposition Var [U ] =
 using the
2
E U 2 −E [U ] , we get R(V ) = kB π V −V k2ρπ +Γπ (V ),
where Γπ (V ) = E [Var [R + γV (X 0 )|X]] (see, e.g.,

The theorem bounds the expected error of approximating V π with a value function drawn randomly from
some distribution µ. Note that in this theorem, µ0
must be a fixed distribution, chosen a priori (i.e. prior
distribution), but µ can be chosen in a data dependent
manner, i.e., it can be a “posterior” distribution.
Notice that there are three elements to the above
bound (right hand side). The first term is the empirical component of the bound, which enforces the
selection of solutions with smaller empirical Bellman
residuals. The second term is the Bayesian component of the bound, which penalizes distributions that
are far from the prior. The third term corrects for the
variance in the return at each state.
1



Here, Var [U |V ] = E (U − E [U |V ])2 |V is the conditional variance of U as usual. We shall also use the similarly
defined conditional covariance, Cov [(U1 , U2 )|V ] =

E (U1 − E [U1 |V ])(U2 − E [U2 |V ])> |V . When U1 = U2 ,
def

we will also use Cov [U1 |V ] = Cov [(U1 , U2 )|V ].

If we can empirically estimate the right hand side of
the above inequality, then we can use the bound in
an algorithm. For example, we can derive a PACBayesian model-selection algorithm that searches in
the space of posteriors µ so as to minimize the upper
bound.
4.1

Linearly parametrized classes of
functions

Theorem 3 is presented for any countable families of
functions. One can extend this result to sufficiently
regular classes of functions (which can carry measures)
without any problems.2 Here we consider the case
where F is the class of linearly parametrized functions
with bounded parameters,

FC = θ> φ : kθk ≤ C
where φ : X → Rd is some measurable function such
def
that Fmax = supx∈X kφ(x)k2 < ∞. In this case, the
measures can be put on the ball { θ : kθk ≤ C }.
Let us now turn to the estimation of the variance term.
Assuming that the reward for each transition is independent of the next state, one gets
Var [R + γV (X 0 )|X]

= Var [R|X]
+γ 2 Var [V (X 0 )|X] .

Now, if Vθ = φ> θ, then:
Var [V (X 0 )|X]

= θ> Cov [φ(X 0 )|X] θ.

Assuming homoscedastic variance for the rewards, and
2
= Var [R] and Σφ = E [Cov [φ(X 0 )|X]], we
defining σR
get:
2
Var [R + γV (X 0 )|X] = σR
+ γ 2 θ > Σφ θ .

4.2

Estimating the constants

2
In some cases the terms σR
and Σφ are known (e.g.,
2
σR = 0 when the rewards are a deterministic function
of the start state and action, and Σφ = 0 when the
dynamics is deterministic). An alternative is to estimate these terms empirically. This can be done by,
e.g., double sampling of next states (assuming one has
access to a generative model, or if one can reset the
state).3 If such estimates are generated based on finite
sample sets, then we might need to add extra deviation terms to the bound of Theorem 3. For simplicity,
2

The extension presents only technical challenges, but
leaves the result intact and hence is omitted.
3
Alternately, one can co-estimate the mean and variance terms (Sutton et al., 2009), keeping a current guess of
them and updating both estimates as new transitions are
observed.

we assume that these terms are either known or can be
estimated on a separate dataset of many transitions.
Examples of such cases are studied in the empirical
results.
The constant c2 , which depends on the mixing condition of the process, can also be estimated if we have
access to a generative model. There are upper bounds
for c2 when the sample is a collection of independent
trajectories of length less than h.

5

Empirical Results

In this section, we investigate how the bound of Theorem 3 can be used in a model selection mechanism for
transfer learning in the RL setting. One experiment
is presented on the well-known mountain car problem,
the other focuses on a generative model of epileptic
seizures built from real-world data.
5.1

Case Study: Mountain Car

We design a transfer learning experiment on the Mountain Car domain (Sutton and Barto, 1998), where the
goal is to drive an underpowered car beyond a certain altitude up a mountain. We refer the reader to
the reference for details of the domain. We learn the
optimal policy (name it π) on the original Mountain
Car problem (γ = 0.9, reward = 1 passed the goal
threshold and 0 otherwise). Note that the reward and
2
= 0 and
the dynamics are deterministic, therefore σR
Σφ = 0. The task is to learn the value function on
the original domain, and use that knowledge in similar (though not identical) environments to accelerate
the learning process in those new environments. (The
other environments will be described later.)
We estimate the value of π on the original domain with
tile coding (4 tiles of size 8 × 8). Let θ0 be the LSTD
solution on a very large sample set in the original domain. To transfer the domain knowledge from this
problem, we construct a prior distribution µ0 : product of Gaussians with mean θ0 and variance σ02 = 0.01.
In a new environment, we collect a set of trajectories
(100 trajectories of length 5), and search in the space of
λ-parametrized posterior measures, defined as follows:
measure µλ is the product of Gaussians with mean
. 


−1

λθ0
θ̂
λ
1
λ
1
+
+
and
variance
+
,
2
2
2
2
2
2
σ̂
σ̂
σ̂
σ
σ
σ
0

0

0

where θ̂ is the LSTD solution based on the sample set
on the new environment, and σ̂ 2 (variance of the empirical estimate) is set to 0.01. The search for the best
λ-parameterized posterior is driven by our proposed
PAC-Bayes upper bound on the approximation error.
When λ = 0, µλ will be a purely empirical estimate,
whereas when λ = 1, we get the Bayesian posterior for

We test this model-selection method on two new environments. The first is a mountain domain very similar
to the original problem, where we double the effect
of the acceleration of the car. The true value function of this domain is close the original domain, and
so we expect the prior to be informative (and thus λ
to be close to 1). In the second domain, we change
the reward function such that it decreases, inversely
proportional to the car’s altitude: r(x) = 1 − h(x),
where h(x) ∈ [0, 1] is the normalized altitude at state
x. The value function of π under this reward function
is largely different from that of the original one, which
means that the prior distribution is misleading, and
the empirical estimate should be more reliable (and λ
close to 0).
Table 1 reports the average true error of approximating V π using different methods over 100 runs (purely
empirical method is when λ = 0, Bayesian is when
λ = 1). This corresponds to the left hand side of Theorem 3 for these methods. For the similar environment, the PAC-Bayes bound is minimized consistently
with λ = 1, indicating that the method is fully using the Bayesian prior. The error is thus decreased to
less than a half of that of the empirical estimate. For
the environment with largely different reward function,
standard Bayesian inference results in poor approximation, whereas the PAC-Bayes method is selecting
small values of λ and is mostly ignoring the prior.
Table
1: Error in the estimated value function V π
R
2
( kV − V π kρπ dµ(V )) on the Mountain Car domain.
The last row shows the value of the λ parameter selected by the PAC-Bayesian method.
Purely empirical
Bayesian
PAC-Bayes
λPAC−Bayes

Similar Env

Different Env

2.35 ± 0.12
1.03 ± 0.09
1.03 ± 0.09
1

0.03 ± 0.01
2.38 ± 0.05
0.07 ± 0.01
0.06 ± 0.01

To further investigate how the value function estimate
changes with these different methods, we consider an
estimate of the value for the state when the car is at
the bottom of the hill. This point estimate is constructed from the PAC-Bayes estimate using the value
function obtained by using only the mean of µλ . To

Figure 5.1(right) compares the distribution of the estimated values for the highly different environment.
We can see that, as expected, the Bayesian estimate
is heavily biased due to the use of a misleading prior.
The PAC-Bayes estimate is only slightly biased away
from the empirical one with the same variance on the
value. Again, this confirms that PAC-Bayes modelselection is largely ignoring the prior when the prior is
misleading.
35
40
30

Emp

30

Bayes

PacBayes

25

Emp

frequency

Note that because the Mountain Car is a deterministic
domain, the variance term of Theorem 3 is 0. As we
use trajectories with known length, we can also bound
the other constants in the bound and evaluate the
bound completely empirically based on the observed
sample set.

get a sense of the dependence of this estimate on the
randomness of the sample the estimate is constructed
over 100 runs. We also obtain these estimates using
a Bayes estimate and purely empirical estimate. Figure 5.1(left) shows a normal fit to the histogram of the
resulting estimates, for the purely empirical and the
PAC-Bayes estimates. As it can be seen, the distribution of PAC-Bayes estimates (which coincides with
the Bayesian posterior as the best λ is consistently 1
in this case) is centered around the correct value, but
is more peaked than the empirical distribution. This
shows that the method is using the prior to converge
faster to the correct value.

frequency

the mean of a Gaussian with known variance (standard
Bayesian inference with empirical priors).

20

PacBayes

20
15
10

10

5
0
−4

−2

0

2

4

6

0

0.5

value

1
value

1.5

Figure 1: Distribution of the estimated value function
on similar (left) and different (right) environments
5.2

Case Study: Epilepsy Domain

We also evaluate our method on a more complex domain. The goal of the RL agent here is to apply
direct electrical neurostimulation such as to suppress
epileptiform behavior. We use a generative model constructed from real-world data collected on slices of rat
brain tissues (Bush et al., 2009); the model is available
in the RL-Glue framework. Observations are generated over a 4-dimensional real-valued state space. The
action choice corresponds to selecting the frequency at
which neurostimulation is applied. The reward is −1
for steps when a seizure is occurring, −1/40 for each
stimulation pulse, and 0 otherwise.
We first apply the best clinical fixed rate policy (stimulation is applied at a consistent 1Hz) to collect a large
sample set (Bush et al., 2009). We then use LSTD to
learn a linear value function over the original feature
space. Similar to the experiment described above, we
construct a prior (with a similar mean and variance
structure), and use it for knowledge transfer in two

new cases. This time, we keep the dynamics and reward function intact and instead change the policy.
The first modified policy we consider applies stimulation at a fixed rate of 2Hz; this is expected to have
a similar value function as the original (1Hz) policy.
The other policy we consider applies no stimulation;
this is expected to have a very different value function
as the seizures are not suppressed.
Table
2: The error of value-function estimates
R
2
( kV − V π kρπ dµ(V )) on the Epilepsy domain.
Empirical
Bayesian
PAC-Bayes
λPAC−Bayes

2 Hz Stimulation

No Stimulation

0.0044 ± 0.0007
0.0013 ± 0.0001
0.0022 ± 0.0004
0.62 ± 0.05

0.54 ± 0.06
0.86 ± 0.07
0.69 ± 0.08
0.30 ± 0.05

We sample 10,000 on-policy trajectories of length 1
and use them with the PAC-Bayes model-selection
mechanism described previously (with similar λparametrized posterior family on the θ parameters,
γ = 0.8) to get estimates of the value function. Table 2 summarizes the performance of different methods
on the evaluation of the new policies (averaged over
50 runs). The results are not as polarized as those
of the Mountain Car experiment, partly because the
domain is noisier, and because the prior is neither exclusively informative or misleading. Nonetheless, we
observe that the PAC-Bayes method is using the prior
more (λ averaging around 0.62) in the case of the 2Hz
policy, which is consistent with clinical evidence showing that 1Hz and 2Hz have similar effect (Bush et al.,
2009), whereas the prior is considered less (λ averaging
around 0.30) in the case of the 0Hz policy which has
substantially (though not entirely) different effects.

6

Discussion

This paper introduces the first PAC-Bayesian bound
for policy evaluation with function approximation and
general state spaces. We demonstrate how such
bounds can be used for value function estimation based
on finite sample sets. Our empirical results show that
PAC-Bayesian model-selection uses prior distributions
when they are informative, and ignores them when
they are misleading. Our results thus far focus on the
policy evaluation case. This approach can be used in
a number of applications, including transfer learning,
as explored above.
Model-selection based on error bounds has been studied previously with regularization techniques (Farahmand et al., 2009). These bounds are generally tighter
for point estimates, as compared to the distributions

used in this work. However, our method is more general as it could incorporate arbitrary domain knowledge into the learning algorithm with any type of prior
distribution. It can of course use sparsity or smoothness priors, which correspond to well-known regularization methods.
An alternative is to derive margin bounds, similar
to those of large-margin classifiers, using PAC-Bayes
techniques. This was recently done by Fard and
Pineau (2010) in the discrete case. The extension to
continuous domains with general function approximation is an interesting future work.
This work does not address the application of PACBayes bounds to derive exploration strategies for the
RL problem. Seldin et al. (2011b,a) have studied
the exploration problem for multiarmed bandits and
have provided algorithms based on PAC-Bayes analysis of martingales. Extensions to contextual bandits
and more general RL settings remain interesting open
problems.

Acknowledgements
This work was supported in part by AICML, AITF
(formerly iCore and AIF), the PASCAL2 Network of
Excellence under EC (grant no. 216886), the NSERC
Discovery Grant program and the National Institutes
of Health (grant R21 DA019800).



Online learning with delayed feedback has received increasing attention recently due to its
several applications in distributed, web-based
learning problems. In this paper we provide a
systematic study of the topic, and analyze the
effect of delay on the regret of online learning
algorithms. Somewhat surprisingly, it turns
out that delay increases the regret in a multiplicative way in adversarial problems, and
in an additive way in stochastic problems.
We give meta-algorithms that transform, in
a black-box fashion, algorithms developed for
the non-delayed case into ones that can handle the presence of delays in the feedback
loop. Modifications of the well-known UCB
algorithm are also developed for the bandit
problem with delayed feedback, with the advantage over the meta-algorithms that they
can be implemented with lower complexity.

1. Introduction
In this paper we study sequential learning when the
feedback about the predictions made by the forecaster are delayed. This is the case, for example, in
web advertisement, where the information whether a
user has clicked on a certain ad may come back to
the engine in a delayed fashion: after an ad is selected, while waiting for the information if the user
clicks or not, the engine has to provide ads to other
users. Also, the click information may be aggregated
and then periodically sent to the module that decides
about the ads, resulting in further delays. (Li et al.,
2010; Dudik et al., 2011). Another example is parallel,
distributed learning, where propagating information
among nodes causes delays (Agarwal & Duchi, 2011).
Proceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR:
W&CP volume 28. Copyright 2013 by the author(s).

While online learning has proved to be successful in
many machine learning problems and is applied in
practice in situations where the feedback is delayed,
the theoretical results for the non-delayed setup are
not applicable when delays are present. Previous work
concerning the delayed setting focussed on specific online learning settings and delay models (mostly with
constant delays). Thus, a comprehensive understanding of the effects of delays is missing. In this paper, we
provide a systematic study of online learning problems
with delayed feedback. We consider the partial monitoring setting, which covers all settings previously considered in the literature, extending, unifying, and often
improving upon existing results. In particular, we give
general meta-algorithms that transform, in a blackbox fashion, algorithms developed for the non-delayed
case into algorithms that can handle delays efficiently.
We analyze how the delay effects the regret of the algorithms. One interesting, perhaps somewhat surprising,
result is that the delay inflates the regret in a multiplicative way in adversarial problems, while this effect
is only additive in stochastic problems. While our general meta-algorithms are useful, their time- and spacecomplexity may be unnecessarily large. To resolve this
problem, we work out modifications of variants of the
UCB algorithm (Auer et al., 2002) for stochastic bandit problems with delayed feedback that have much
smaller complexity than the black-box algorithms.
The rest of the paper is organized as follows. The problem of online learning with delayed feedback is defined
in Section 2. The adversarial and stochastic problems
are analyzed in Sections 3.1 and 3.2, while the modification of the UCB algorithm is given in Section 4.
Some proofs, as well as results about the KL-UCB algorithm (Garivier & Cappé, 2011) under delayed feedback, are provided in the appendix.

2. The delayed feedback model
We consider a general model of online learning, which
we call the partial monitoring problem with side in-

Online Learning under Delayed Feedback

Parameters: Forecaster’s prediction set A, set of outcomes B, side information set X , reward function r :
X × A × B → R, feedback function h : X × A × B → H,
time horizon n (optional).
At each time instant t = 1, 2, . . . , n:
1. The environment chooses some side information
xt ∈ X and an outcome bt ∈ B.
2. The side information xt is presented to the forecaster, who makes a prediction at ∈ A, which
results in the reward r(xt , at , bt ) (unknown to the
forecaster).
3. The feedback ht = h(xt , at , bt ) is scheduled to be
revealed after τt time instants.
4. The agent observes Ht = {(t′ , ht′ ) : t′ ≤ t, t′ +
τt′ = t}, i.e., all the feedback values scheduled
to be revealed at time step t, together with their
timestamps.
Figure 1: Partial monitoring under delayed, timestamped feedback.

formation. In this model, the forecaster (decision
maker) has to make a sequence of predictions (actions), possibly based on some side information, and
for each prediction it receives some reward and feedback, where the feedback is delayed. More formally,
given a set of possible side information values X , a
set of possible predictions A, a set of reward functions
R ⊂ {r : X × A → R}, and a set of possible feedback
values H, at each time instant t = 1, 2, . . ., the forecaster receives some side information xt ∈ X ; then,
possibly based on the side information, the forecaster
predicts some value at ∈ A while the environment simultaneously chooses a reward function rt ∈ R; finally, the forecaster receives reward rt (xt , at ) and some
time-stamped feedback set Ht ⊂ N × H. In particular,
each element of Ht is a pair of time index and a feedback value, the time index indicating the time instant
whose decision the associated feedback corresponds to.
Note that the forecaster may or may not receive any direct information about the rewards it receives (i.e., the
rewards may be hidden). In standard online learning,
the feedback-set Ht is a singleton and the feedback in
this set depends on rt , at . In the delayed model, however, the feedback that concerns the decision at time t
is received at the end of the time period t+τt , after the
prediction is made, i.e., it is delayed by τt time steps.
Note that τt ≡ 0 corresponds to the non-delayed case.
Due to the delays multiple feedbacks may arrive at the
same time, hence the definition of Ht .

The goal of P
the forecaster is to maximize its cumulative reward nt=1 rt (xt , at ) (n ≥ 1). The performance
of the forecaster is measured relative to the best static
strategy selected from some set F ⊂ {f | f : X → A}
in hindsight. In particular, the forecaster’s performance is measured through the regret, defined by
Rn = sup

n
X

a∈F t=1

rt (xt , a(xt )) −

n
X

rt (xt , at ).

t=1

A forecaster is consistent if it achieves, asymptotically,
the average reward of the best static strategy, that is
E [Rn ] /n → 0, and we are interested in how fast the
average regret can be made to converge to 0.
The above general problem formulation includes most
scenarios considered in online learning. In the full
information case, the feedback is the reward function itself, that is, H = R and Ht = {(t, rt )}) (in
the non-delayed case). In the bandit case, the forecaster only learns the rewards of its own prediction,
i.e., H = R and Ht = {(t, rt (xt , at ))}. In the partial monitoring case, the forecaster is given a reward
function r : X × A × B → R and a feedback function h : X × A × B → H, where B is a set of choices
(outcomes) of the environment. Then, for each time
instant the environment picks an outcome bt ∈ B,
and the reward becomes rt (xt , at ) = r(xt , at , bt ), while
Ht = {(t, h(xt , at , bt ))}. This interaction protocol is
shown in Figure 1 in the delayed case. Note that
the bandit and full information problems can also be
treated as special partial monitoring problems. Therefore, we will use this last formulation of the problem.
When no stochastic assumption is made on how the
sequence bt is generated, we talk about the adversarial
model. In the stochastic setting we will consider the
case when bt is a sequence of independent, identically
distributed (i.i.d.) random variables. Side information may or may not be present in a real problem; in
its absence X is a singleton set.
Finally, we may have different assumptions on the delays. Most often, we will assume that (τt )t≥1 is an i.i.d.
sequence, which is independent of the past predictions
(as )s≤t of the forecaster. In the stochastic setting, we
also allow the distribution of τt to depend on at .
Note that the delays may change the order of observing the feedbacks, with the feedback of a more recent
prediction being observed before the feedback of an
earlier one.
2.1. Related work
The effect of delayed feedback has been studied in the
recent years under different online learning scenarios

Online Learning under Delayed Feedback

Stochastic Feedback

Full Info

No
Side
Info

 
R(n) ≤ R′ (n) + O(E τt2 )
(Agarwal & Duchi, 2011)
L

Side Info

Bandit
Feedback

No Side
Info
Side Info

Partial

No
Side Info

Monitoring

Side Info

R(n) ≤ R′ (n) + O(D∗ )
(Mesterharm, 2007)
R(n) ≤ C1 R′ (n) + C2 τmax log(τmax )
(Desautels et al., 2012)
√
R(n) ≤ R′ (n) + O(τconst log n)
(Dudik et al., 2011)
Rn ≤ R′ (n) + O(G∗n )

General (Adversarial) Feedback
L
R(n) ≤ O(τconst ) × R′ (n/τconst )
(Weinberger & Ordentlich, 2002)
(Langford et al., 2009)
(Agarwal & Duchi, 2011)
L
R(n) ≤ O(D̄) × R′ (n/D̄)
(Mesterharm, 2007)
R(n) ≤ O(τconst ) × R(n/τconst )
(Neu et al., 2010)


n
∗
 1 + E [Gn ] 
n
Rn ≤ (1 + E [G∗n ]) × R′
1 + E [G∗n ]
Rn ≤ (1 + E [G∗n ]) × R′



Table 1. Summary of work on online learning under delayed feedback. R(n) shows the (expected) regret in the delayed
setting, while R′ (n) shows the (upper bound on) the (expected) regret in the non-delayed setting. L denotes a matching
lower bound. D∗ and D̄ indicate the maximum and average gap, respectively, where a gap is a number of consecutive
time steps the agent does not get any feedback (in the adversarial delay formulation used by Mesterharm (2005; 2007)).
The term τconst indicates that the results are for constant delays only. For the work of (Desautels et al., 2012), C1 and
C2 are positive constants, with C1 > 1, and τmax denotes the maximum delay. The results presented in this paper are
shown in boldface, where G∗t is the maximum number of outstanding feedbacks during
t time-steps. In particular,

 the firstp
∗
∗
Gn ≤ τmax when the delays have an upper bound τmax , and we show that Gn = O E [τt ] + E [τt ] log n + log n when
the delays τt are i.i.d. The new bounds for the partial monitoring problem are automatically applicable in the other,
spacial, cases, and give improved results in most cases.

and different assumptions on the delay. A concise summary, together with the contributions of this paper, is
given in Table 1.
To
the
best
of
our
knowledge,
Weinberger & Ordentlich (2002) were the first to
analyze the delayed feedback problem; they considered the adversarial full information setting with
a fixed, known delay τconst . They showed that
the minimax optimal solution is to run τconst + 1
independent optimal predictors on the subsampled
reward sequences: τconst + 1 prediction strategies
are used such that the ith predictor is used at time
instants t with (t mod (τconst + 1)) + 1 = i. This
approach forms the basis of our method devised for
the adversarial case (see Section 3.1). Langford et al.
(2009) showed that under the usual conditions, a
sufficiently slowed-down version of the mirror descent
algorithm achieves optimal decay rate of the average
regret. Mesterharm (2005; 2007) considered another
variant of the full information setting, using an
adversarial model on the delays in the label prediction setting, where the forecaster has to predict
the label corresponding to a side information vector

xt . While in the full information online prediction
problem Weinberger & Ordentlich (2002) showed
that the regret increases by a multiplicative factor of
τconst , in the work of Mesterharm (2005; 2007) the
important quantity becomes the maximum/average
gap defined as the length of the largest time interval
the forecaster does not receive feedback. Mesterharm
(2005; 2007) also shows that the minimax regret in
the adversarial case increases multiplicatively by the
average gap, while it increases only in an additive
fashion in the stochastic case, by the maximum gap.
Agarwal & Duchi (2011) considered the problem of
online stochastic optimization and showed that, for
i.i.d. random delays, the
increases with an
 regret

additive factor of order E τ 2 .

Qualitatively similar results were obtained in the
bandit setting. Considering a fixed and known delay τconst
√, Dudik et al. (2011) showed an additive
O(τconst log n) penalty in the regret for the stochastic setting (with side information), while (Neu et al.,
2010) showed a multiplicative regret for the adversarial
bandit case. The problem of delayed feedback has also
been studied for Gaussian process bandit optimization

Online Learning under Delayed Feedback

(Desautels et al., 2012), resulting in a multiplicative
increase in the regret that is independent of the delay and an additive term depending on the maximum
delay.
In the rest of the paper we generalize the above results
to the partial monitoring setting, extending, unifying,
and often improving existing results.

3. Black-Box Algorithms for Delayed
Feedback
In this section we provide black-box algorithms for the
delayed feedback problem. We assume that there exists a base algorithm Base for solving the prediction
problem without delay. We often do not specify the
assumptions underlying the regret bounds of these algorithms, and assume that the problem we consider
only differs from the original problem because of the
delays. For example, in the adversarial setting, Base
may build on the assumption that the reward functions are selected in an oblivious or non-oblivious way
(i.e., independently or not of the predictions of the
forecaster). First we consider the adversarial case in
Section 3.1. Then in Section 3.2, we provide tighter
bounds for the stochastic case.
3.1. Adversarial setting
We say that a prediction algorithm enjoys a regret or expected regret bound f : [0, ∞) → R under the given assumptions in the non-delayed setting if (i) f is nondecreasing, concave, f (0) = 0;
and (ii) supb1 ,...,bn ∈B Rn ≤ f (n) or, respectively,
supb1 ,...,bn ∈B E [Rn ] ≤ f (n) for all n. The algorithm
of Weinberger & Ordentlich (2002) for the adversarial full information setting subsamples the reward sequence by the constant delay τconst +1, and runs a base
algorithm Base on each of the τconst + 1 subsampled
sequences. Weinberger & Ordentlich (2002) showed
that if Base enjoys a regret bound f then their algorithm in the fixed delay case enjoys a regret bound
(τconst + 1)f (n/(τconst + 1)). Furthermore, when Base
is minimax optimal in the non-delayed setting, the subsampling algorithm is also minimax optimal in the (full
information) delayed setting, as can be seen by constructing a reward sequence that changes only in every
τconst + 1 times. Note that Weinberger & Ordentlich
(2002) do not require condition (i) of f . However,
these conditions imply that yf (x/y) is a concave function of y for any fixed x (a fact which will turn out to
be useful in the analysis later), and are satisfied by all
regret bounds we are aware of (e.g., for multi-armed
bandits, contextual bandits, partial monitoring, etc.),

e α)
which all have a regret upper bound of the form O(n
for some 0 ≤ α ≤ 1, with, typically, α = 1/2 or 2/3.1 .

In this section we extend the algorithm of
Weinberger & Ordentlich (2002) to the case when the
delays are not constant, and to the partial monitoring
setting. The idea is that we run several instances of
a non-delayed algorithm Base as needed: an instance
is “free” if it has received the feedback corresponding
to its previous prediction – before this we say that
the instance is “busy”, waiting for the feedback.
When we need to make a prediction, we use one of
existing instances that is free, and is hence ready to
make another prediction. If no such instance exists,
we create a new one to be used (a new instance is
always “free”, as it is not waiting for the feedback of a
previous prediction). The resulting algorithm, which
we call Black-Box Online Learning under Delayed
feedback (BOLD) is shown below (note that when the
delays are constant, BOLD reduces to the algorithm
of Weinberger & Ordentlich (2002)):

Algorithm 1 Black-box Online Learning under Delayed feedback (BOLD)
for each time instant t = 1, 2, . . . , n do
Prediction:
Pick a free instance of Base (independently
of past predictions), or create a new instance if
all existing instances are busy. Feed the instance
picked with xt and use its prediction.
Update:
for each (s, hs ) ∈ Ht do
Update the instance used at time instant s with
the feedback hs .
end for
end for

Clearly, the performance of BOLD depends on how
many instances of Base we need to create, and how
many times each instance is used. Let Mt denote the
number of Base instances created by BOLD up to
and including time t. That is, M1 = 1, and we create a new instance at the beginning of any time instant whenP
all instances are waiting for their feedback.
t−1
Let Gt = s=1 I {s + τs ≥ t} be the total number of
outstanding (missing) feedbacks when the forecaster
is making a prediction at time instant t. Then we
have Gt algorithms waiting for their feedback, and so
Mt ≥ Gt + 1. Since we only introduce new instances
when it is necessary (and each time instant at most
1
e n ) means that there is a β ≥ 0 such that
un = O(v
limn→∞ un /(vn logβ n) = 0.

Online Learning under Delayed Feedback

one new instance is created), it is easy to see that
Mt = G∗t + 1

Now, using the fact that fBase is an (expected) regret
bound, we obtain
(1)

for any t, where G∗t = max1≤s≤t Gt .

E [Rn |τ1 , . . . , τn ] ≤

We can use the result above to transfer the regret guarantee of the non-delayed base algorithm Base to a
guarantee on the regret of BOLD.
Theorem 1. Suppose that the non-delayed algorithm
Base used in BOLD enjoys an (expected) regret bound
fBase . Assume, furthermore, that the delays τt are independent of the forecaster’s prediction at . Then the
expected regret of BOLD after n time steps satisfies



n
E [Rn ] ≤ E (G∗n + 1)fBase
G∗ + 1

 n
n
.
≤ (E [G∗n ] + 1)fBase
E [G∗n ] + 1
Proof. As the second inequality follows from the concavity of y 7→ yfBase (x/y) (x, y > 0), it remains to
prove the first one.
For any 1 ≤ j ≤ Mn , let Lj denote the list of time
instants in which BOLD has used the prediction chosen by instance j, and let nj = |Lj | be the number
of time instants this happens. Furthermore, let Rnj j
denote the regret incurred during the time instants t
with t ∈ Lj :
X

Rnj j = sup

a∈F t∈L
j

rt (xt , a(xt )) −

X

rt (xt , at ),

t∈Lj

where at is the prediction made by BOLD (and instance j) at time instant t. By construction, instance
j does not experience any delays. Hence, Rnj j is its regret in a non-delayed online learning problem. 2 Then,
Rn = sup

n
X

a∈F t=1

= sup

rt (xt , a(xt )) −

Mn X
X

a∈F j=1
t∈Lj

≤
=

Mn
X
j=1

Mn
X



 sup

n
X

rt (xt , at )

t=1

rt (xt , a(xt )) −

X

a∈F t∈L
j

Mn X
X

j=1 t∈Lj

rt (xt , a(xt )) −

X

t∈Lj

rt (xt , at )


rt (xt , at )

Rnj j .

j=1

2

Note that Lj is a function of the delay sequence and
is not a function of the predictions (at )t≥1 . Hence, the
reward sequence that instance j is evaluated on is chosen
obliviously whenever the adversary of BOLD is oblivious.

Mn
X
j=1

i
h
E Rnj j |τ1 , . . . , τn

Mn
X
1
fBase (nj )
Mn
j=1
j=1




Mn
X
1
n
≤ Mn fBase 
,
nj  = Mn fBase
Mn
Mn
j=1

≤

Mn
X

fBase (nj ) = Mn

where the first inequality follows since Mn is a deterministic function of the delays, while the last inequality follows from Jensen’s inequality and the concavity
of fBase . Substituting Mn from (1) and taking the
expectation concludes the proof.
Now, we need to bound G∗n to make the theorem meaningful. When all delays are the same constants, for
n > τconst we get G∗n = τt = τconst , and we get back
the regret bound


n
E [Rn ] ≤ (τconst + 1)fBase
τconst + 1
of Weinberger & Ordentlich (2002), thus generalizing their result to partial monitoring.
We do
not know whether this bound is tight even when
Base is minimax optimal, as the argument of
Weinberger & Ordentlich (2002) for the lower bound
does not work in the partial information setting (the
forecaster can gain extra information in each block
with the same reward functions).
Assuming the delays are i.i.d., we can give an interesting bound on G∗n . The result is based on the fact that
although Gt can be as large as t, both its expectation
and variance are upper bounded by E [τ1 ].
Lemma 2. Assume τ1 , . . . , τn is a sequence of i.i.d.
random variables with√finite expected value, and let
B(n, t) = t + 2 log n + 4t log n. Then
E [G∗n ] ≤ B(n, E [τ1 ]) + 1.
Proof. First consider the expectation and the variance
of Gt . For any t,
" t−1
# t−1
X
X
E [Gt ] = E
I {s + τs ≥ t} =
P {s + τs ≥ t}
s=1

=

t−2
X
s=0

P {τ1 > s} ≤ E [τ1 ] ,

s=1

Online Learning under Delayed Feedback

and, similarly
σ 2 [Gt ] =

t−1
X
s=1

σ 2 [I {s + τs ≥ t}] ≤

t−1
X
s=1

P {s + τs ≥ t} ,

so σ 2 [Gt ] ≤ E [τ1 ] in the same way as above. By
Bernstein’s inequality (Cesa-Bianchi & Lugosi, 2006,
Corollary A.3), for any 0 < δ < 1 and any t we have,
with probability at least 1 − δ,
q
Gt − E [Gt ] ≤ log δ1 + 2σ 2 [Gt ] log 1δ .
Applying the union bound for δ = 1/n2 , and our previous bounds on the variance and expectation of Gt ,
we obtain that with probability at least 1 − 1/n,
p
max Gt ≤ E [τ1 ] + 2 log n + 4E [τ1 ] log n.
1≤t≤n

Taking into account that max1≤t≤n Gt ≤ n, we get the
statement of the lemma.
Corollary 3. Under the conditions of Theorem 1, if
the sequence of delays is i.i.d, then


n
.
E [Rn ] ≤ (B(n, E [τ1 ]) + 2)fBase
B(n, E [τ1 ]) + 2

Note that although the delays can be arbitrarily large,
whenever the expected value is finite, the bound only
increases by a log n factor.
3.2. Finite stochastic setting
In this section, we consider the case when the prediction set A of the forecaster is finite; without loss of
generality we assume A = {1, 2, . . . , K}. We also assume that there is no side information (that is, xt is
a constant for all t, and, hence, will be omitted; the
results can be extended easily to the case of a finite
side information set, where we can repeat the procedures described below for each value of the side information separately). The main assumption in this
section is that the outcomes (bt )t≥1 form an i.i.d. sequence, which is also independent of the predictions
of the forecaster. When B is finite, this leads to the
standard i.i.d. partial monitoring (IPM) setting, while
the conventional multi-armed bandit (MAB) setting is
recovered when the feedback is the reward of the last
prediction, that is, ht = rt (at , bt ). As in the previous section, we will assume that the feedback delays
are independent of the outcomes of the environment.
The main result of this section shows that under these
assumptions, the penalty in the regret grows in an additive fashion due to the delays, as opposed to the multiplicative penalty that we have seen in the adversarial
case.

By the independence assumption on the outcomes, the
.
sequences of potential rewards rt (i) = r(i, bt ) and feed.
backs ht (i) = h(i, bt ) are i.i.d., respectively, for the
same prediction i ∈ A. In this setting we also assume that the feedback and reward sequences of different predictions are independent of each other. Let
µi = E [rt (i)] denote the expected reward of predicting i, µ∗ = maxi∈A µi the optimal reward and i∗
= µ∗ the optimal prediction. Moreover, let
with µi∗ P
Ti (n) = nt=1 I {at = i} denote the number of times i
is predicted by the end of time instant n. Then, defining the “gaps” ∆i = µ∗ − µi for all i ∈ A, the expected
regret of the forecaster becomes
E [Rn ] =

n
X
t=1

µ∗ − µat =

K
X

∆i E [Ti (n)] .

(2)

i=1

Similarly to the adversarial setting, we build on a base
algorithm Base for the non-delayed case. The advantage in the IPM setting (and that we consider expected
regret) is that here Base can consider a permuted order of rewards and feedbacks, and so we do not have
to wait for the actual feedback; it is enough to receive
a feedback for the same prediction. This is the idea at
the core of our algorithm, Queued Partial Monitoring
with Delayed Feedback (QPM-D):
Algorithm 2 Queued Partial Monitoring with Delays
(QPM-D)
Create an empty FIFO buffer Q[i] for each i ∈ A.
Let I be the first prediction of Base.
for each time instant t = 1, 2, . . . , n do
Predict:
while Q[I] is not empty do
Update Base with a feedback from Q[I].
Let I be the next prediction of Base.
end while
There are no buffered feedbacks for I, so predict
at = I at time instant t to get a feedback.
Update:
for each (s, hs ) ∈ Ht do
Add the feedback hs to the buffer Q[as ].
end for
end for
Here we have a Base partial monitoring algorithm
for the non-delayed case, which is run inside the algorithm. The feedback information coming from the
environment is stored in separate queues for each prediction value. The outer algorithm constantly queries
Base: while feedbacks for the predictions made are
available in the queues, only the inner algorithm Base
runs (that is, this happens within a single time instant

Online Learning under Delayed Feedback

in the real prediction problem). When no feedback is
available, the outer algorithm keeps sending the same
prediction to the real environment until a feedback for
that prediction arrives. In this way Base is run in a
simulated non-delayed environment. The next lemma
implies that the inner algorithm Base actually runs in
a non-delayed version of the problem, as it experiences
the same distributions:
Lemma 4. Consider a delayed stochastic IPM problem as defined above. For any prediction i, for any
s ∈ N let h′i,s denote the sth feedback QPM-D receives
for predicting i. Then the sequence (h′i,s )s∈N is an i.i.d.
sequence with the same distribution as the sequence of
feedbacks (ht,i )t∈N for prediction i.
To relate the non-delayed performance of Base and
the regret of QPM-D, we need a few definitions. For
any t, let Si (t) denote the number of feedbacks for
prediction i that are received by the end of time instant t. Then the number of missing feedbacks for i
when making a prediction at time instant t is Gi,t =
Ti (t − 1) − Si (t − 1). Let G∗i,n = max1≤t≤n Gi,t . Furthermore, for each i ∈ A, let Ti′ (t′ ) be the number
of times algorithm Base has predicted i while being
queried t′ times. Let n′ denote the number of steps
the inner algorithm Base makes in n steps of the real
IPM problem. Next we relate n and n′ , as well as the
number of times QPM-D and Base (in its simulated
environment) make a specific prediction.
Lemma 5. Suppose QPM-D is run for n ≥ 1 time
instants, and has queried Base n′ times. Then n′ ≤ n
and
0 ≤ Ti (n) − Ti′ (n′ ) ≤ G∗i,n .
(3)
Proof. Since Base can take at most one step for each
feedback that arrives, and QPM-D has to make at least
one step for each arriving feedback, n′ ≤ n.
Now, fix a prediction i ∈ A. If Base, and hence,
QPM-D, has not predicted i by time instant n, (3)
trivially holds. Otherwise, let tn,i denote the last time
instant (up to time n) when QPM-D predicts i. Then
Ti (n) = Ti (tn,i ) = Ti (tn,i − 1) + 1. Suppose Base
has been queried n′′ ≤ n times by time instant tn,i
(inclusive). At this time instant, the buffer Q[i] must
be empty and Base must be predicting i, otherwise
QPM-D would not predict i in the real environment.
This means that all the Si (tn,i −1) feedbacks that have
arrived before this time instant have been fed to the
base algorithm, which has also made an extra step,
that is, Ti′ (n′ ) ≥ Ti′ (n′′ ) = Si (tn,i − 1) + 1. Therefore,
Ti (n) − Ti′ (n′ ) ≤ Ti (tn,i − 1) + 1 − (Si (tn,i − 1) + 1)
≤ Gi,tn,i ≤ G∗i,n .

We can now give an upper bound on the expected regret of Algorithm 2.
Theorem 6. Suppose the non-delayed Base algorithm is used in QPM-D in a delayed stochastic IPM
environment. Then the expected regret of QPM-D is
upper-bounded by
K


 X

∆i E G∗i,n ,
E [Rn ] ≤ E RnBase +

(4)

i=1



where E RnBase is the expected regret of Base when
run in the same environment without delays.
When the delay τt is bounded by τmax
 forall t, we also
have G∗i,n ≤ τmax , and E [Rn ] ≤ E RnBase + O(τmax ).
When the sequence of delays for each prediction is
i.i.d. with a finite expected value but unbounded support, we can use Lemma 2 to bound G∗i,n , and obtain
p


a bound E RnBase + O(E [τ1 ] + E [τ1 ] log n + log n).

Proof. Assume that QPM-D is run longer so that
Base is queried for n times (i.e., it is queried n − n′
more times). Then, since n′ ≤ n, the number of times
i is predicted by the base algorithm, namely Ti′ (n), can
only increase, that is, Ti′ (n′ ) ≤ Ti′ (n). Combining this
with the expectation of (3) gives


E [Ti (n)] ≤ E [Ti′ (n)] + E G∗i,n ,
which in turn gives,
K
X
i=1

∆i E [Ti (n)] ≤

K
X
i=1

∆i E [Ti′ (n)] +

K
X
i=1



∆i E G∗i,n .

(5)

As shown in Lemma 4, the reordered rewards and feedbacks h′i,1 , h′i,2 , . . . , h′i,T ′ (n′ ) , . . . h′i,Ti (n) are i.i.d. with
i
the same distribution as the original feedback sequence
(ht,i )t∈N . The base algorithm Base has worked on the
first Ti′ (n) of these feedbacks for each i (in its extended
run), and has therefore operated for n steps in a simulated environment with the same reward and feedback
distributions, but without delay. Hence, the first

 summation in the right hand side of (5) is in fact E RnBase ,
the expected regret of the base algorithm in a nondelayed environment. This concludes the proof.

4. UCB for the Multi-Armed Bandit
Problem with Delayed Feedback
While the algorithms in the previous section provide
an easy way to convert algorithms devised for the nondelayed case to ones that can handle delays in the feedback, improvements can be achieved if one makes modifications inside the existing non-delayed algorithms

Online Learning under Delayed Feedback

while retaining their theoretical guarantees. This can
be viewed as a ”white-box” approach to extending online learning algorithms to the delayed setting, and
enables us to escape the high memory requirements
of black-box algorithms that arises for both of our
methods in the previous section when the delays are
large. We consider the stochastic multi-armed bandit
problem, and extend the UCB family of algorithms
(Auer et al., 2002; Garivier & Cappé, 2011) to the delayed setting. The modification proposed is quite natural, and the common characteristics of UCB-type algorithms enable a unified way of extending their performance guarantees to the delayed setting (up to an
additive penalty due to delays).

2002) uses UCBs of the form Bi,s,t = µ̂i,s +
p
Ps
2 log(t)/s, where µ̂i,s = 1s t=1 h′i,t is the average
of the first s observed rewards. Using this UCB in
our decision rule (6), we can bound the regret of the
resulting algorithm (called Delayed-UCB1) in the delayed setting:

Recall that in the stochastic MAB setting, which is a
special case of the stochastic IPM problem of Section
3.2, the feedback at time instant t is ht = r(at , bt ), and
there is a distribution νi from which the rewards of
each prediction i are drawn in an i.i.d. manner. Here
we assume that the rewards of different predictions are
independent of each other. We use the same notation
as in Section 3.2.

Note that the last term in the bound is the additive penalty, and, under different assumptions, it can
be bounded in the same way as after Theorem 6.
The proof of this theorem, as well as a similar regret bound for the delayed version of the KL-UCB
algorithm (Garivier & Cappé, 2011) can be found in
Appendix B.

Several algorithms devised for the non-delayed
stochastic MAB problem are based on upper confidence bounds (UCBs), which are optimistic estimates
of the expected reward of different predictions. Different UCB-type algorithms use different upper confidence bounds, and choose, at each time instant, a
prediction with the largest UCB. Let Bi,s,t denote the
UCB for prediction i at time instant t, where s is the
number of reward samples used in computing the estimate. In a non-delayed setting, the prediction of
a UCB-type algorithm at time instant t is given by
at = argmaxi∈A Bi,Ti (t−1),t . In the presence of delays,
one can simply use the same upper confidence bounds
only with the rewards that are observed, and predict

5. Conclusion and future work

at = argmaxi∈A Bi,Si (t−1),t

(6)

at time instant t (recall that Si (t − 1) is the number
of rewards that can be observed for prediction i before
time instant t). Note that if the delays are zero, this
algorithm reduces to the corresponding non-delayed
version of the algorithm.
The algorithms defined by (6) can easily be shown to
enjoy the same regret guarantees compared to their
non-delayed versions, up to an additive penalty depending on the delays. This is because the analyses of
the regrets of UCB algorithms follow the same pattern
of upper bounding the number of trials of a suboptimal
prediction using concentration inequalities suitable for
the specific form of UCBs they use.
As an example, the UCB1 algorithm (Auer et al.,

Theorem 7. For any n ≥ 1, the expected regret of the
Delayed-UCB1 algorithm is bounded by
E [Rn ] ≤

 X
K
X  8 log n


∆i E G∗i,n .
+ 3.5∆i +
∆i
i=1

i:∆i >0

We analyzed the effect of feedback delays in online
learning problems. We examined the partial monitoring case (which also covers the full information and the
bandit settings), and provided general algorithms that
transform forecasters devised for the non-delayed case
into ones that handle delayed feedback. It turns out
that the price of delay is a multiplicative increase in the
regret in adversarial problems, and only an additive increase in stochastic problems. While we believe that
these findings are qualitatively correct, we do not have
lower bounds to prove this (matching lower bounds are
available for the full information case only).
It also turns out that the most important quantity
that determines the performance of our algorithms is
G∗n , the maximum number of missing rewards. It is
interesting to note that G∗n is the maximum number
of servers used in a multi-server queuing system with
infinitely many servers and deterministic arrival times.
It is also the maximum deviation of a certain type of
Markov chain. While we have not found any immediately applicable results in these fields, we think that
applying techniques from these areas could lead to an
improved understanding of G∗n , and hence an improved
analysis of online learning under delayed feedback.

6. Acknowledgements
This work was supported by the Alberta Innovates
Technology Futures and NSERC.

Online Learning under Delayed Feedback



We study the problem of learning Markov decision processes with finite state and action
spaces when the transition probability distributions and loss functions are chosen adversarially and are allowed to change with time. We introduce an algorithm whose regret with
respect to any policy in a comparison class grows as the square root of the number of rounds
of the game, provided the transition probabilities satisfy a uniform mixing condition. Our
approach is efficient as long as the comparison class is polynomial and we can compute
expectations over sample paths for each policy. Designing an efficient algorithm with small
regret for the general case remains an open problem.

1

Notation

Let X be a finite state space and A be a finite action space. Let ∆S be the space of probability
distributions over set S. Define a policy π as a mapping from the state space to ∆A , π : X → ∆A .
We use π(a|x) to denote the probability of choosing action a in state x under policy π. A random
action under policy π is denoted by π(x). A transition probability kernel (or transition model) m
is a mapping from the direct product of the state and action spaces to ∆X : m : X × A → ∆X .
Let P (π, m) be the transition probability matrix of policy π under transition model m. A loss
function is a bounded real-valued
function over state and action spaces, ℓ : X × A → R. For a
P
vector v, define kvk1 = i |vi |. For a real-valued function f defined over X × A, define kf k∞,1 =
P
maxx∈X a∈A |f (x, a)|. The inner product between two vectors v and w is denoted by hv, wi.

2

Introduction

Consider the following game between a learner and an adversary: at round t, the learner chooses a
policy πt from a policy class Π. In response, the adversary chooses a transition model mt from a set
of models M and a loss function ℓt . The learner takes action at ∼ πt (.|xt ), moves to state xt+1 ∼
mt (.|xt , at ) and suffers loss ℓt (xt , at ). To simplify the discussion, we assume that the adversary is
oblivious, i.e. its choices do not depend on the previous choices of the learner. We assume that
ℓt ∈ [0, 1]. In this paper, we study the full-information version of the game, where the learner
observes the transition model mt and the loss function ℓt at the end of round t. The game is shown
in Figure 1. The objective of the learner is to suffer low loss over a period of T rounds, while the
performance of the learner is measured using its regret with respect to the total loss he would have
achieved had he followed the stationary policy in the comparison class Π minimizing the total loss.
Even-Dar et al. (2004) prove a hardness result for MDP problems with adversarially chosen
transition models. Their proof, however, seems to have gaps as it assumes that the learner chooses a
deterministic policy before observing the state at each round. Note that an online learning algorithm
only needs to choose an action at the current state and does not need to construct a complete
deterministic policy at each round. Their hardness result applies to deterministic transition models,
while we make a mixing assumption in our analysis. Thus, it is still an open problem whether it is
possible to obtain a computationally efficient algorithm with a sublinear regret.
Yu and Mannor (2009a,b) study the same setting, but obtain only a regret bound that scales
with the amount of variation in the transition models. This regret bound can grow linearly with
time.

Initial state: x0
for t := 1, 2, . . . do
Learner chooses policy πt
Adversary chooses model mt and loss function ℓt
Learner takes action at ∼ πt (.|xt )
Learner suffers loss ℓt (xt , at )
Update state xt+1 ∼ mt (.|xt , at )
Learner observes mt and ℓt
end for
Figure 1: Online Markov Decision Processes

Even-Dar et al. (2009) prove regret bounds for MDP problems with a fixed and known transition
model and adversarially chosen loss functions. In this paper, we prove regret bounds for MDP
problems with adversarially chosen transition models and loss functions. We are not aware of any
earlier regret bound for this setting. Our approach is efficient as long as the comparison class is
polynomial and we can compute expectations over sample paths for each policy.
MDPs with changing transition kernels are good models for a wide range of problems, including
dialogue systems, clinical trials, portfolio optimization, two player games such as poker, etc.

3

Online MDP Problems

Let A be an online learning algorithm that generates a policy πt at round t. Let xA
t be the state
at round t if we have followed the policies generated by algorithm A. Similarly, xπt denotes the state
if we have chosen the same policy π up to time t. Let ℓ(x, π) = ℓ(x, π(x)). The regret of algorithm
A up to round T with respect to any policy π ∈ Π is defined by
RT (A, π) =

T
X
t=1

πt (xA
t ).

ℓt (xA
t , at ) −

T
X

ℓt (xπt , π) ,

t=1

where at =
Note that the regret with respect to π is defined in terms of the sequence of
states xπt that would have been visited under policy π. Our objective is to design an algorithm that
achieves low regret with respect to any policy π.
In the absence of state variables, the problem reduces to a full information online learning
problem (Cesa-Bianchi and Lugosi, 2006). The difficulty with MDP problems is that, unlike the full
information online learning problems, the choice of policy at each round changes the future states
and losses. The main idea behind the design and the analysis of our algorithm is the following regret
decomposition:
RT (A, π) =

T
X
t=1

Let

ℓt (xA
t , at ) −

T
X

BT (A) =

T
X

t=1

t=1

CT (A, π) =

ℓt (xπt t , πt ) +

T
X
t=1

T
X
t=1

ℓt (xA
t , at ) −

T
X

ℓt (xπt t , πt ) −

ℓt (xπt t , πt ) −

T
X

ℓt (xπt , π) .

(1)

t=1

ℓt (xπt t , πt ) ,

t=1

T
X

ℓt (xπt , π) .

t=1

Notice that the choice of policies has no influence over future losses in CT (A, π). Thus, CT (A, π)
can be bounded by a specific reduction to full information online learning algorithms (to be specified
later). Also, notice that the competitor policy π does not appear in BT (A). In fact, BT (A) depends
only on the algorithm A. We will show that if algorithm A and the class of models satisfy the
following two “smoothness” assumptions, then BT (A) can be bounded by a sublinear term.
Assumption A1 Rarely Changing Policies Let αt be the probability that algorithm A changes
its policy at round t. There exists a constant D such
√ that for any 1 ≤ t ≤ T , any sequence of models
m1 , . . . , mt and loss functions ℓ1 , . . . , ℓt , αt ≤ D/ t.
2

N : number of experts, T : number of rounds.
Initialize wi,0 = 1 for each expert i.
W0 = N .
for t := 1, 2, . . . do
For any i, pi,t = wi,t−1 /Wt−1 .
Draw It such that for any i, P (It = i) = pi,t .
Choose the action suggested by expert It .
The adversary chooses loss function ct .
The learner suffers loss ct (It ).
For expert i, wi,t = wi,t−1 e−ηct (i) .
P
Wt = N
i=1 wi,t .
end for
Figure 2: The EWA Algorithm
N : number
p of experts, T : number of rounds.
η = min{ log N/T , 1/2}.
Initialize wi,0 = 1 for each expert i.
W0 = N .
for t := 1, 2, . . . do
For any i, pi,t = wi,t−1 /Wt−1 .
With probability βt = wIt−1 ,t−1 /wIt−1 ,t−2 choose the previously selected
expert, It = It−1 and with probability 1 − βt , choose It based on the
distribution qt = (p1,t , . . . , pN,t ).
Learner takes the action suggested by expert It .
The adversary chooses loss function ct .
The learner suffers loss ct (It ).
For all experts i, wi,t = wi,t−1 (1 − η)ct (i) .
PN
Wt = i=1 wi,t .
end for
Figure 3: The Shrinking Dartboard Algorithm

Assumption A2 Uniform Mixing There exists a constant τ > 0 such that for all distributions
d and d′ over the state space, any deterministic policy π, and any model m ∈ M ,
kdP (π, m) − d′ P (π, m)k1 ≤ e−1/τ kd − d′ k1 .
As discussed by Neu et al. (2010), if Assumption A2 holds for deterministic policies, then it holds
for all policies.
3.1

Full Information Algorithms
We would like to have a full information online learning algorithm that rarely changes its policy.
The first candidate that we consider is the well-known Exponentially Weighted Average (EWA)
algorithm (Vovk, 1990, Littlestone and Warmuth, 1994) shown in Figure 2. In our MDP problem,
the EWA algorithm chooses a policy π ∈ Π according to distribution
!
t−1
X
E [ℓs (xπs , π)] , λ > 0 ,
(2)
qt (π) ∝ exp −λ
s=1

The policies that this EWA algorithm generates most likely are different in consecutive rounds and
thus, the EWA algorithm might change its policy frequently. However, a variant of EWA, called
Shrinking Dartboard (SD) (Geulen et al., 2010) and shown in Figure 3, satisfies Assumption A1.
Our algorithm, called SD-MDP, is based on the SD algorithm and is shown in Figure 4. Notice
that the algorithm needs to know the number of rounds, T , in advance.
3

T : number
p of rounds.
η = min{ log |Π| /T , 1/2}.
For all policies π ∈ {1, . . . , |Π|}, wπ,0 = 1.
for t := 1, 2, . . . do
For any π, pπ,t = wπ,t−1 /Wt−1 .
With probability βt = wπt−1 ,t−1 /wπt−1 ,t−2 choose the previous policy, πt =
πt−1 , while with probability 1 − βt , choose πt based on the distribution
qt = (p1,t , . . . , p|Π|,t ).
Learner takes the action at ∼ πt (.|xt )
Adversary chooses transition model mt and loss function ℓt .
Learner suffers loss ℓt (xt , at ).
Learner observes mt and ℓt .
Update state: xt+1 ∼ mt (.|xt , at ).
π
For allP
policies π, wπ,t = wπ,t−1 (1 − η)E[ℓt (xt ,π)] .
Wt = π∈Π wπ,t .
end for
Figure 4: SD-MDP: The Shrinking Dartboard Algorithm for Markov Decision Processes

Consider a basic full information problem with N experts. Let RT (SD, i) be the regret of the SD
algorithm with respect to expert i up to time T . We have the following results for the SD algorithm.
Theorem 1. For any expert i ∈ {1, . . . , N },
p
RT (SD, i) ≤ 4 T log N + log N ,

and also for any 1 ≤ t ≤ T ,

P (Switch at time t) ≤

r

log N
.
T

Proof. The proof of the regret bound can be found in (Geulen et al., 2010, Theorem 3). The proof
of the bound on the probability of switch is similar to the proof of Lemma 2 in (Geulen et al., 2010)
and is as follows: As shown in (Geulen et al., 2010, Lemma 2), the probability of switch at time t is
αt =

Wt−1 − Wt
.
Wt−1

Thus, Wt = (1 − αt )Wt−1 . Because the loss function is bounded in [0, 1], we have that
Wt =

N
X
i=1

wi,t =

N
X
i=1

wi,t−1 (1 − η)ct (i) ≥

Thus, 1 − αt ≥ 1 − η, and thus,

αt ≤ η ≤

r

N
X
i=1

wi,t−1 (1 − η) = (1 − η)Wt−1 .

log N
.
T

3.2 Analysis of the SD-MDP Algorithm
The main result of this section is the following regret bound for the SD-MDP algorithm.
Theorem 2. Let the loss functions selected by the adversary be bounded in [0, 1], and the transition
models selected by the adversary satisfy Assumption A2. Then, for any policy π ∈ Π,
p
E [RT (SD-MDP, π)] ≤ (4 + 2τ 2 ) T log |Π| + log |Π| .

In the rest of this section, we write A to denote the SD-MDP algorithm. For the proof we use
the regret decomposition (1):
RT (A, π) = BT (A) + CT (A, π) .
4

3.2.1 Bounding E [CT (A, π)]
Lemma 3. For any policy π ∈ Π,
" T
#
T
X
X
p
πt
π
E [CT (A, π)] = E
ℓt (xt , πt ) −
ℓt (xt , π) ≤ 4 T log |Π| + log |Π| .
t=1

t=1

Proof. Consider the following imaginary game between a learner and an adversary: we have a set
of experts (policies) Π = {π 1 , . . . , π |Π| }. At round t, the adversary chooses a loss vector ct ∈ [0, 1]Π ,
whose ith element determines the loss of expert π i at this round. The learner chooses a distribution
over experts qt (defined by the SD algorithm), from which it draws an expert πt . Next, the learner
observes the loss function ct . From the regret bound for the SD algorithm (Theorem 1), it is
guaranteed that for any expert π,
T
X
t=1

hct , qt i −

T
X
t=1

p
ct (π) ≤ 4 T log |Π| + log |Π| .

Next, we determine how the adversary
h chooses
ithe loss vector. At time t, the adversary chooses a
i
πi
i
loss function ℓt and sets ct (π ) = E ℓt (xt , π ) . Noting that hct , qt i = E [ℓt (xπt t , πt )] and ct (π) =

E [ℓt (xπt , π)] finishes the proof.

3.2.2 Bounding E [BT (A)]
First, we prove the following two lemmas.
Lemma 4. For any state distribution d, any transition model m, and any policies π and π ′ ,
kdP (π, m) − dP (π ′ , m)k1 ≤ kπ − π ′ k∞,1 .
Proof. Proof is easy and can be found in (Even-Dar et al., 2009), Lemma 5.1.
p
Lemma 5. Let αt be the probability of a policy switch at time t. Then, αt ≤ log |Π|/T .

Proof. Proof is identical to the proof of Theorem 1.
Lemma 6. We have that
E [BT (A)] = E

" T
X
t=1

ℓt (xA
t , at ) −

T
X
t=1

#

ℓt (xπt t , πt ) ≤ 2τ 2

p
log |Π|T .

Proof. Let Ft = σ(π1 , . . . , πt ). Notice that the choice of policies are independent of the state
variables. We can write
" T
#
T
X
X
πt
A
E [BT (A)] = E
ℓt (xt , at ) −
ℓt (xt , πt )
t=1

=E
=E

"

"

=E

"

≤E

"

=E

"

≤E

"

T
X

t=1

X

t=1 x∈X

T X
X

t=1 x∈X

T X
X

t=1 x∈X

T
X
t=1

T
X
t=1

T
X
t=1

#



I{xA
− I{xπt t =x} ℓt (x, πt (x))
t =x}

E

h



I{xA
− I{xπt t =x} ℓt (x, πt (x)) FT
t =x}

#
i

i

h
πt
F
ℓt (x, πt (x))E I{xA
−
I
T
{xt =x}
t =x}

kℓt k∞ E

h

I{xA
− I{xπt t =x}
t =x}

kℓt k∞ kut − vt,t k1
#

kut − vt,t k1 ,
5

#



FT

i

1

#

#

(3)

i
h


A
πt
where us = E I{xA
is the
is
the
distribution
of
x
for
s
≤
t
and
v
=
E
I
F
F
s,t
T
T
=x}
s
{x
=x}
s
s
distribution of xπs t for s ≤ t.1 Let Et be the event of a policy switch at time t. From inequality
kπt−k − πt k∞,1 ≤ kπt−k − πt−k+1 k∞,1 + · · · + kπt−1 − πt k∞,1 ≤ 2

t
X

I{Es } ,

s=t−k+1

and Lemma 5, we get that
h

i

E kπt−k − πt k∞,1 ≤ 2

r

log |Π|
k.
T

(4)

Let Ptπ = P (π, mt ). We have that




πt−1
πt
E kut − vt,t k1 = E ut−1 Pt−1
− vt−1,t Pt−1
1


πt−1
πt
πt
πt
= E ut−1 Pt−1
− ut−1 Pt−1
+ ut−1 Pt−1
− vt−1,t Pt−1
1


πt−1
πt
πt
πt
+ ut−1 Pt−1
− vt−1,t Pt−1
≤ E ut−1 Pt−1
− ut−1 Pt−1
1
1
h
i
≤ E kπt−1 − πt k∞,1 + e−1/τ kut−1 − vt−1,t k1
h
πt−2
πt
≤ E kπt−1 − πt k∞,1 + e−1/τ ( ut−2 Pt−2
− ut−2 Pt−2
1
i
πt
πt
)
+ ut−2 Pt−2
− vt−2,t Pt−2
1
h
i
≤ E kπt−1 − πt k∞,1 + e−1/τ kπt−2 − πt k∞,1 + e−2/τ kut−2 − vt−2,t k1
≤ ...
≤
≤

t
X

k=0
t
X

k=0

≤2

r

h
i
e−k/τ E kπt−k − πt k∞,1 + e−t/τ ku0 − v0,t k1
2e

−k/τ

r

log |Π|
k+0
T

By (4)

log |Π| 2
τ ,
T

(5)

where we have used the fact that ku0 − v0,t k1 = 0, because the initial distributions are identical. By
(5) and (3), we get that
r
T
X
p
log |Π|
2
E [BT (A)] ≤ 2τ
= 2τ 2 log |Π|T .
T
t=1

What makes the analysis possible is the fact that all policies mix no matter what transition
model is played by the adversary.
Proof of Theorem 2. The result is obvious by Lemmas 3 and 6.
The next corollary extends the result of Theorem 2 to continuous policy spaces.
Corollary 7. Let Π be an arbitrary policy space, N (ǫ) be the ǫ-covering number of space (Π, k.k∞,1 ),
and C(ǫ) be an ǫ-cover. Assume that we run the SD-MDP algorithm on C(ǫ). Then, under the same
assumptions as in Theorem 2, for any policy π ∈ Π,
p
E [RT (SD-MDP, π)] ≤ (4 + 2τ 2 ) T log N (ǫ) + log N (ǫ) + τ T ǫ .
1

Notice that FT contains only policies, which are independent of the state variables.

6

hP
i
T
π
Proof. Let LT (π) = E
ℓ
(x
,
π)
be the value of policy π. Let uπ,t (x) = P (xπt = x). First,
t
t
t=1
we prove that the value function is Lipschitz with Lipschitz constant τ T . The argument is similar
to the argument in the proof of Lemma 6. For any π1 and π2 ,
" T
#
T
X
X
π1
π2
|LT (π1 ) − LT (π2 )| = E
ℓt (xt , π1 ) −
ℓt (xt , π2 )
t=1

≤2
≤2

T
X
t=1

T
X
t=1

t=1

kuπ1 ,t − uπ2 ,t k1 kℓt k∞
kuπ1 ,t − uπ2 ,t k1 .

With an argument similar to the one in the proof of Lemma 6, we can show that
kuπ1 ,t − uπ2 ,t k1 ≤ τ kπ1 − π2 k∞,1 .
Thus,
|LT (π1 ) − LT (π2 )| ≤ τ T kπ1 − π2 k∞,1 .

Given this and the fact that for any policy π ∈ Π, there is a policy π ′ ∈ C(ǫ) such that kπ − π ′ k∞,1 ≤
ǫ, we get that
p
E [RT (SD-MDP, π)] ≤ (4 + 2τ 2 ) T log N (ǫ) + log N (ǫ) + τ T ǫ .
In particular if Π is the space of all policies, N (ǫ) ≤ (|A|/ǫ)|A||X |, so regret is no more than
r
|A|
|A|
2
+ |A||X | log
+ τT ǫ .
E [RT (SD-MDP, π)] ≤ (4 + 2τ ) T |A||X | log
ǫ
ǫ
p
By the choice of ǫ = T1 , we get that E [RT (SD-MDP, π)] = O(τ 2 T |A| |X | log(|A|T )).



We consider the problem of simultaneously learning to linearly combine a very large
number of kernels and learn a good predictor based on the learnt kernel. When the
number of kernels d to be combined is very large, multiple kernel learning methods whose
computational cost scales linearly in d are intractable. We propose a randomized version
of the mirror descent algorithm to overcome this issue, under the objective of minimizing
the group p-norm penalized empirical risk. The key to achieve the required exponential
speed-up is the computationally efficient construction of low-variance estimates of the
gradient. We propose importance sampling based estimates, and find that the ideal
distribution samples a coordinate with a probability proportional to the magnitude of
the corresponding gradient. We show the surprising result that in the case of learning
the coefficients of a polynomial kernel, the combinatorial structure of the base kernels
to be combined allows the implementation of sampling from this distribution to run
in O(log(d)) time, making the total computational cost of the method to achieve an optimal solution to be O(log(d)/2 ), thereby allowing our method to operate for very large
values of d. Experiments with simulated and real data confirm that the new algorithm is
computationally more efficient than its state-of-the-art alternatives.

1

Introduction

We look into the computational challenge of finding a good predictor in a multiple kernel
learning (MKL) setting where the number of kernels is very large. In particular, we are
interested in cases where the base kernels come from a space with combinatorial structure
and thus their number d could be exponentially large. Just like some previous works (e.g.
Rakotomamonjy et al., 2008; Xu et al., 2008; Nath et al., 2009) we start with the approach
that views the MKL problem as a nested, large scale convex optimization problem, where
the first layer optimizes the weights of the kernels to be combined. More specifically, as the
1

objective we minimize the group p-norm penalized empirical risk. However, as opposed to
these works whose underlying iterative methods have a complexity of Ω(d) for just any one
iteration, following (Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richtárik and
Takáĉ, 2011) we use a randomized coordinate descent method, which was effectively used in
these works to decrease the per iteration complexity to O(1). The role of randomization in our
method is to use it to build an unbiased estimate of the gradient at the most recent iteration.
The issue then is how the variance (and so the number of iterations required) scales with d.
As opposed to the above mentioned works, in this paper we propose to make the distribution
over the updated coordinate dependent on the history. We will argue that sampling from a
distribution that is proportional to the magnitude of the gradient vector is desirable to keep
the variance (actually, second moment) low and in fact we will show that there are interesting
cases of MKL (in particular, the case of combining kernels coming from a polynomial family
of kernels) when efficient sampling (i.e., sampling at a cost of O(log d)) is feasible from this
distribution. Then, the variance is controlled by the a priori weights put on the kernels,
making it potentially independent of d. Under these favorable conditions (and in particular,
for the polynomial kernel set with some specific prior weights), the complexity of the method
as a function of d becomes logarithmic, which makes our MKL algorithm feasible even for
large scale problems. This is to be contrasted to the approach of Nesterov (2010, 2012) where
a fixed distribution is used and where the a priori bounds on the method’s convergence rate,
and, hence, its computational cost to achieve a prescribed precision, will depend linearly on d
(note that we are comparing upper bounds here, so the actual complexity could be smaller).
Our algorithm is based on the mirror descent (or mirror descent) algorithm (similar to the
work of Richtárik and Takáĉ (2011) who uses uniform distributions).
It is important to mention that there are algorithms designed to handle the case of
infinitely many kernels, for example, the algorithms by Argyriou et al. (2005, 2006); Gehler
and Nowozin (2008). However, these methods lack convergence rate guarantees, and, for
example, the consistency for the method of Gehler and Nowozin (2008) works only for “small”
d. The algorithm of Bach (2008), though practically very efficient, suffers from the same
deficiency. A very interesting proposal by Cortes et al. (2009) considers learning to combine
a large number of kernels and comes with guarantees, though their algorithm restricts the
family of kernels in a specific way.
The rest of the paper is organized as follows. The problem is defined formally in Section 2.
Our new algorithm is presented and analyzed in Section 3, while its specialized version
for learning polynomial kernels is given in Section 4. Finally, experiments are provided in
Section 5.

2

Preliminaries

In this section we give the formal definition of our problem. Let I denote a finite index set,
indexing the predictors (features)
the set of predictors considered
 to be combined, and define
P
over the input space X as F = fw : X → R : fw (x) = i∈I hwi , φi (x)i , x ∈ X . Here Wi
is a Hilbert space over the reals, φi : X → Wi is a feature-map, hx, yi is the inner product
.
over the Hilbert space that x, y belong to and w = (wi )i∈I ∈ W = ×i∈I Wi (as an example,
Wi may just be a finite dimensional Euclidean space). The problem we consider is to solve

2

the optimization problem
minimize Ln (fw ) + Pen(fw )

subject to w ∈ W ,
and Ln (fw ) = n1
convex losses `t

(1)
Pn

where Pen(fw ) is a penalty that will be specified later,
t=1 `t (fw (xt ))is the
empirical risk of predictor fw , defined in terms of the
: R → R (1 ≤ t ≤ n)
and inputs xt ∈ X (1 ≤ t ≤ n). The solution w∗ of the above penalized empirical risk
minimization problem is known to have favorable generalization properties under various
conditions, see, e.g., Hastie et al. (2009). In supervised learning problems `t (y) = `(yt , y)
for some loss function ` : R × R → R, such as the squared-loss, `(yt , y) = 21 (y − yt )2 , or the
hinge-loss, `t (yt , y) = max(1 − yyt , 0), where in the former case yt ∈ R, while in the latter
case yt ∈ {−1, +1}. We note in passing that for the sake of simplicity, we shall sometimes
abuse notation and write Ln (w) for Ln (fw ) and even drop the index n when the sample-size
is unimportant.
As mentioned above, in this paper we consider the special case in (1) when the penalty
is a so-called group p-norm penalty with 1 ≤ p ≤ 2, a case considered earlier, e.g., by Kloft
et al. (2011). Thus our goal is to solve
!2
p
1 X p
p
minimize Ln (w) +
ρi kwi k2
,
(2)
w∈W
2
i∈I

where the scaling factors ρi > 0, i ∈ I, are assumed to be given. We introduce the notation
u = (ui ) ∈ RI to denote the column vector obtained from the values ui .
The rationale of using the squared weighted p-norm is that for 1 ≤ p < 2 it is expected to
encourage sparsity at the group level which should allow one to handle cases when I is very
large (and the case p = 2 comes for free from the same analysis). The actual form, however,
is also chosen for reasons of computational convenience. In fact, the reason to use the 2-norm
of the weights is to allow the algorithm to work even with infinite-dimensional feature vectors
(and thus weights) by resorting to the kernel trick. To see how this works, just notice that
the penalty in (2) can also be written as
!2
(
)
p
2 kw k2
X p
X
ρ
i
2
i
ρi kwi kp2
= inf
: θ∈∆ p
,
2−p
θi
i∈I

i∈I

where for ν ≥ 1, ∆ν = {θ ∈ [0, 1]|I| : kθkν ≤ 1} is the positive quadrant of the |I|-dimensional
`ν -ball (see, e.g., Micchelli and Pontil, 2005, Lemma 26). Hence, defining
1 X ρ2i kwi k22
J(w, θ) = L(w) +
2
θi
i∈I

for any w ∈ W, θ ∈ [0, 1]|I| , an equivalent form of (2) is
minimize J(w, θ)

w∈W,θ∈∆ν

(3)

where ν = p/(2 − p) ∈ [1, ∞) and we define 0/0 = 0 and u/0 = ∞ for u > 0, which implies
that wi = 0 if θi = 0. That this minimization problem is indeed equivalent to our original
task (2) for the chosen value of ν follows from the fact that J(w, θ) is jointly convex in (w, θ).1
1

Here and in what follows by equivalence we mean that the set of optimums in terms of w (the primary
optimization variable) is the same in the two problems.

3

Let κi : X × X → R be the reproducing kernel underlying φi : κi (x, x0 ) = hφi (x), φi (x0 )i
(x, x0 ∈ X ) and let Hi = Hκi the corresponding reproducing kernel Hilbert space (RKHS).
Then, for any given fixed value of θ, the above problem becomes an instance
P of a standard
penalized learning problem in the RKHS Hθ underlying the kernel κθ = i∈I θi ρ−2
i κi . In
particular, by the theorem on page 353 in Aronszajn (1950), the problem of finding w ∈ W
for fixed θ can be seen to be equivalent to minimizef ∈Hθ L(f ) + 21 kf k2Hθ , and thus (2) is
seen to be equivalent to minimizef ∈Hθ ,θ∈∆ν L(f ) + 21 kf k2Hθ . Thus, we see that the method
can be thought of as finding the weights of a kernel κθ and a predictor minimizing the Hθ norm penalized empirical risk. This shows that our problem is an instance of multiple kernel
learning (for an exhaustive survey of MKL, see, e.g., Gönen and Alpaydın, 2011 and the
references therein).

3

The new approach

When I is small, or moderate in size, the joint-convexity of J allows one to use off-the-shelf
solvers to find the joint minimum of J. However, when I is large, off-the-shelf solvers might
be slow or they may run out of memory. Targeting this situation we propose the following
approach: Exploiting again that J(w, θ) is jointly convex in (w, θ), find the optimal weights
by finding the minimizer of
.
J(θ) = inf J(w, θ),
w

.
or, alternatively, J(θ) = J(w∗ (θ), θ), where w∗ (θ) = arg minw J(w, θ) (here we have slightly
abused notation by reusing the symbol J). Note that J(θ) is convex by the joint convexity of
J(w, θ). Also, note that w∗ (θ) exists and is well-defined as the minimizer of J(·, θ) is unique
for any θ ∈ ∆ν (see also Proposition 3.2 below). Again, exploiting the joint convexity of
J(w, θ), we find that if θ∗ is the minimizer of J(θ), then w∗ (θ∗ ) will be an optimal solution
to the original problem (2). To optimize J(θ) we propose to use stochastic gradient descent
with artificially injected randomness to avoid the need to fully evaluate the gradient of J.
More precisely, our proposed algorithm is an instance of a randomized version of the mirror
descent algorithm (Rockafellar, 1976; Martinet, 1978; Nemirovski and Yudin, 1998), where in
each time step only one coordinate of the gradient is sampled.

3.1

A randomized mirror descent algorithm

Before giving the algorithm, we need a few definitions. Let d = |I|, A ⊂ Rd be nonempty
with a convex interior A◦ . We call the function Ψ : A → R a Legendre (or barrier) potential
if it is strictly convex, its partial derivatives exist and are continuous, and for every sequence
{xk } ⊂ A approaching the boundary of A, limk→∞ k∇Ψ(xk )k = ∞. Here ∇ is the gradient
∂
operator: ∇Ψ(x) = ( ∂x
Ψ(x))> is the gradient of Ψ. When ∇ is applied to a non-smooth
0
convex function J (θ) (J may be such without additional assumptions) then ∇J 0 (θ) is defined
as any subgradient of J 0 at θ. The corresponding Bregman-divergence DΨ : A × A◦ → R
is defined as DΨ (θ, θ0 ) = Ψ(θ) − Ψ(θ0 ) − h∇Ψ(θ0 ), θ − θ0 i. The Bregman projection ΠΨ,K :
A◦ → K corresponding to the Legendre potential Ψ and a closed convex set K ⊂ Rd such
that K ∩ A 6= ∅ is defined, for all θ ∈ A◦ as ΠΨ,K (θ) = arg minθ0 ∈K∩A Dψ (θ0 , θ).
Algorithm 1 shows a randomized version of the standard mirror descent method with an
unbiased gradient estimate. By assumption, ηk > 0 is deterministic. Note that step 1 of the
4

Algorithm 1 Randomized mirror descent algorithm
1: Input: A, K ⊂ Rd , where K is closed and convex with K ∩ A 6= ∅, Ψ : A → R Legendre,
step sizes {ηk }, a subroutine, GradSampler, to sample the gradient of J at an arbitrary
vector θ ≥ 0
2: Initialization: θ (0) = arg minθ∈K∩A Ψ(θ), k = 0.
3: repeat
4:
k = k + 1.
(k−1) )
5:
Obtain ĝk = GradSampler(θ

(k)
6:
θ̃ = arg minθ∈A ηk−1 hĝk , θi + DΨ (θ, θ(k−1) ) .
7:
θ(k) = ΠΨ,K (θ̃(k) ).
8: until convergence.
algorithm is well-defined since θ̃(k) ∈ A◦ by the assumption that k∇Ψ(x)k tends to infinity
as x approaches the boundary of A. The performance of Algorithm 1 is bounded in the next
theorem. The analysis follows the standard proof technique of analyzing the mirror descent
algorithm (see, e.g., Beck and Teboulle, 2003), however, in a slightly more general form than
what we have found in the literature. In particular, compared to (Nemirovski et al., 2009a;
Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richtárik and Takáĉ, 2011), our
analysis allows for the conditional distribution of the noise in the gradient estimate to be
history dependent. The proof is included in Section A in the appendix.
Theorem 3.1. Assume that Ψ is α-strongly convex with respect to some norm k · k (with
dual norm k · k∗ ) for some α > 0, that is, for any θ ∈ A◦ , θ0 ∈ A
Ψ(θ0 ) − Ψ(θ) ≥ ∇Ψ(θ), θ0 − θ + α2 kθ0 − θk2 .

(4)

Suppose, furthermore, that Algorithm 1 is run for T time steps. For 0 ≤ k ≤ T − 1 let Fk
denote the σ-algebra generated by θ1 , . . . , θk . Assume that, for all 1 ≤ k ≤ T , ĝk ∈ Rd is an
unbiased estimate of ∇J(θ(k−1) ) given Fk−1 , that is,
E [ ĝk | Fk−1 ] = ∇J(θ(k−1) ).

(5)

Further, assume that there exists a deterministic constant B ≥ 0 such that for all 1 ≤ k ≤ T ,


E kĝk k2∗ Fk−1 ≤ B a.s.
(6)
Finally, assume that δ = supθ0 ∈K∩A Ψ(θ0 ) − Ψ(θ(0) ) is finite. Then, if ηk−1 =
k ≥ 1, it holds that
!#
"
r
T
1 X (k−1)
2Bδ
E J
θ
− inf J(θ) ≤
.
θ∈K∩A
T
αT

q

2αδ
BT

for all

(7)

k=1

Furthermore, if
kĝk k2∗ ≤ B 0

5

a.s.

(8)

B0

for some deterministic constant
and ηk−1 =
it holds with probability at least 1 −  that
J

T
1 X (k−1)
θ
T

q

2αδ
B0T

!

r
− inf J(θ) ≤

k=1

for all k ≥ 1 then, for any 0 <  < 1,

2B 0 δ
αT

θ∈K∩A

s
+4

B 0 δ log 1
.
αT

(9)

The convergence rate in the above theorem can be improved if stronger assumptions are
made on J, for example if J is assumed to be strongly convex, see, for example, (Hazan et al.,
2007; Hazan and Kale, 2011).
Efficient implementation of Algorithm 1 depends on efficient implementations of steps
1-1, namely, computing an estimate of the gradient, solving the minimization for θ̃(k) , and
projecting it into K. The first problem is related to the choice of gradient estimate we use,
which, in turn, depends on the structure of the feature space, while the last two problems
depend on the choice of the Legendre function. In the next subsections we examine how these
choices can be made to get a practical variant of the algorithm.

3.2

Application to multiple kernel learning

It remains to define the gradient estimates ĝk in Algorithm 1. We start by considering
importance sampling based estimates. First, however, let us first verify whether the gradient
exist. Along the way, we will also derive some explicit expressions which will help us later.
Closed-form expressions for the gradient. Let us first consider how w∗ (θ) can be
calculated for a fixed value of θ. As it will turn out, this calculation will be useful not
only when the procedure is stopped (to construct the predictor fw∗ (θ) but also during the
iterations when we will need to calculate the derivative of J with respect to θi . The following
proposition summarizes how w∗ (θ) can be obtained. Note that this type of result is standard
(see, e.g., Shawe-Taylor and Cristianini, 2004; Schölkopf and Smola, 2002), thus we include
it only for the sake of completeness (the proof is included in Section A in the appendix).
Proposition 3.2. For 1 ≤ t ≤ n, let `∗t : R → R denote the convex conjugate of `t : `∗t (v) =
0 )i, and let K =
supτ ∈R {vτ − `t (τ )}, v ∈ R. For i ∈ I, recall that κi (x, x0 ) = hφi (x), φi (xP
i
(κi (xt , xs ))1≤t,s≤n be the n × n kernel matrix underlying κi and let Kθ = i∈I ρθ2i Ki be the
i
P
θi
kernel matrix underlying κθ =
κ
.
Then,
for
any
fixed
θ,
the
minimizer
w∗ (θ) of
i
2
i∈I ρi
J(·, θ) satisfies
n
θi X ∗
wi∗ (θ) = 2
α (θ)φi (xt ), i ∈ I ,
(10)
ρi t=1 t
where

(
α∗ (θ) = arg min
α∈Rn

n

1 >
1X ∗
α Kθ α +
`t (−nαt )
2
n

)
.

(11)

t=1

Based on this proposition, we can compute the
Ppredictor fw∗ (θ) usingPthe kernels {κi }i∈I
and the dual variables (αt∗ (θ))1≤t≤n : fw∗ (θ) (x) = i∈I hwi∗ (θ), φi (x)i = nt=1 αt∗ (θ)κθ (xt , x) .

6

Let us now consider the differentiability of J = J(θ) and how to compute its derivatives.
Under proper conditions with standard calculations (e.g., Rakotomamonjy et al., 2008) we
find that J is differentiable over ∆ and its derivative can be written as2

 ∗ >
∂
α (θ) Ki α∗ (θ)
J(θ) = −
.
(12)
∂θ
ρ2i
i∈I
Importance sampling based estimates. Let d = |I| and let ei , i ∈ I denote the ith unit
vector of the standard basis of Rd , that is, the ith coordinate of ei is 1 while the others are
0. Introduce
D
E
gk,i = ∇J(θ(k−1) ), ei , i ∈ I
(13)
to denote the ith component of the gradient of J in iteration k (that is, gk,i can be computed
based on (12)). Let sk−1 ∈ [0, 1]I be a distribution over I, computed in some way based on
the information available up to the end of iteration k − 1 of the algorithm (formally, sk−1 is
Fk−1 -measurable). Define the importance sampling based gradient estimate to be
ĝk,i =

I{Ik =i}
gk,Ik ,
sk−1,Ik

i ∈ I, where Ik ∼ sk−1,· .

(14)

That is, the gradient estimate is obtained by first sampling an index from sk−1,· and then
setting the gradient estimate to be zero at all indices i ∈ I except when i = Ik in which
gk,I
case its value is set to be the ratio sk−1,Ik . It is easy to see that as long as sk−1,i > 0 holds
k

whenever gk,i 6= 0, then it holds that E [ ĝk | Fk−1 ] = ∇J(θ(k−1) ) a.s.
Let us now derive the conditions under which the second moment of the gradient estimate
stays bounded. Define Ck−1 = ∇J(θ(k−1) ) 1 . Given the expression for the gradient of J
shown in (12), we see that supk≥1 Ck−1 < ∞ will always hold provided that α∗ (θ) is continuous
since (θ(k−1) )k≥1 is guaranteed to belong to a compact set (the continuity of α∗ is discussed
in Section B in the appendix).
1
Define the probability distribution qk−1,· as follows: qk−1,i = Ck−1
|gk,i | , i ∈ I. Then
2
qk−1,I

2
2
keIk k2∗ = s2 k Ck−1
keIk k2∗ . Therefore, it also holds
it holds that kĝk k2∗ = s2 1 gk,I
k
k−1,Ik
k−1,Ik
2


P
qk−1,i
qk−1,i
2
2
2
2
that E kĝk k2∗ Fk−1 = Ck−1
i∈I sk−1,i kei k∗ ≤ Ck−1 maxi∈I sk−1,i kei k∗ . This shows that


q
supk≥1 E kĝk k2∗ Fk−1 < ∞ will hold as long as supk≥1 maxi∈I sk−1,i
< ∞ and supk≥1 Ck−1 <
k−1,i
∞. Note that when sk−1 = qk−1 , the gradient estimate becomes ĝk,i = Ck−1 I{It =i} . That is,
in this case we see that in order to be able to calculate ĝk,i , we need to be able to calculate
Ck−1 efficiently.

Choosing the potential Ψ. The efficient sampling of the gradient is not the only practical
issue, since the choice of the Legendre function
and the convex set K may also cause some
P
complications. For example, if Ψ(x) =
i∈I xi (ln xi − 1), then the resulting algorithm
is exponential weighting, and one needs to store and update |I| weights, which is clearly
infeasible if |I| is very large (or infinite). On the other hand, if Ψ(x) = 12 kxk22 and we project
2

For completeness, the calculations are given in Section B in the appendix.

7

Algorithm 2 Projected stochastic gradient algorithm.
1:
2:
3:
4:
5:
6:

(0)

Initialization: Ψ(x) = 21 kxk22 , θi = 0 for all i ∈ I, k = 0, step sizes {ηk }.
repeat
k = k + 1.
Sample a gradient estimate ĝk of g(θ(k−1 ) randomly according to (14).
θ(k) = ΠΨ,∆2 (θ(k−1) − ηk−1 ĝk ).
until convergence.

to K = ∆2 , the positive quadrant of the `2 -ball (with A = [0, ∞)I ), we obtain a stochastic
projected gradient method, shown in Algorithm 2. This is in fact the algorithm that we
use in the experiments. Note that in (2) this corresponds to using p = 4/3. The reason we
made this choice is because in this case projection is a simple scaling operation. Had we
chosen K = ∆1 , the `2 -projection would very often cancel many of the nonzero components,
resulting in an overall slow progress. Based on the above calculations and Theorem 3.1 we
obtain the following performance bound for our algorithm.
Corollary 3.3. Assume that α∗ (θ) is continuous on ∆2 . Then there exists a C > 0 such
q
∂
that k ∂θ
J(θ)k1 ≤ C for all θ ∈ ∆2 . Let B = 21 C 2 maxi∈I,1≤k≤T sk−1,i
. If Algorithm 2 is run
k−1,i
√
for T steps with ηk−1 = η = 1/ BT , k = 1, . . . , T , then, for all θ ∈ ∆2 ,
!#
"
r
T
1 X (k−1)
B
− J(θ) ≤
θ
.
E J
T
T
k=1

Note that to implement Algorithm 2 efficiently, one has to be able to sample from sk−1,·
and compute the importance sampling ratio gk,i /sk,i efficiently for any k and i.

4

Example: Learning polynomial kernels

In this section we show how our method can be applied in the context of multiple kernel learning. We provide an example when the kernels in I are tensor products of a set of base kernels
(this we shall call learning polynomial kernels). The importance of this example follows from
the observation of Gönen and Alpaydın (2011) that the non-linear kernel learning methods of
Cortes et al. (2009), which can be viewed as a restricted form of learning polynomial kernels,
are far the best MKL methods in practice and can significantly outperform state-of-the-art
SVM with a single kernel or with the uniform combination of kernels.
Assume that we are given a set of base kernels {κ1 , . . . , κr }. In this section we consider the
set KD of product kernels of degree at most D: Choose I = {(r1 , . . . , rd ) : 0 ≤ Q
d ≤ D, 1 ≤ ri ≤ r}
and the multi-index r1:d = (r1 , . . . , rd ) ∈ I defines the kernel κr1:d (x, x0 ) = di=1 κri (x, x0 ).
For d = 0 we define κr1:0 (x, x0 ) = 1. Note that indices that are the permutations of each
other define the same kernel. On the language of statistical modeling, κr1:d models interactions of order d between the features underlying the base kernels κ1 , . . . , κr . Also note that
|I| = Θ(rD ), that is, the cardinality of I grows exponentially fast in D.
We assume that ρr1:d depends only on d, the order of interactions in κr1:d . By abusing

8

Algorithm 3 Polynomial kernel sampling. The symbol
denotes the Hadamard product/power.
1: Input: α ∈ Rn , the solution to the dual problem; kernel matrices {K1 , . . . , Kr }; the
degree
of the polynomial kernel, the weights (ρ20 , . . . , ρ2D ).
PD
r
>
2: S ←
j=1 KDj , M ← αα
E
0
−2
3: δ(d0 ) ← ρd0 M, S d , d0 ∈ {0, . . . , D}
PD
4: Sample d from δ(·)/ d0 =0 δ(d0 )
5: for i = 1 to d do
tr(M S (d−i) K )
6:
π(j) ← tr(M S (d−i+1) j) , j ∈ {1, . . . , r}
7:
Sample zi from π(·)
8:
M ← M K zi
9: end for
10: return (z1 , . . . , zd )
notation, we will write ρd in the rest of this section to emphasize this.3 Our proposed
algorithm to sample from qk−1,· is shown in Algorithm 3. The algorithm is written to return
a multi-index (z1 , . P
. . , zd ) that isP
drawn from qk−1,· . The key idea underlying the algorithm
r
d
is to exploit that ( j=1 κj ) = r1:d ∈I κr1:d . The correctness of the algorithm is shown in
Section 4.1. In the description of the algorithm
denotes the matrix entrywise product
(a.k.a. Schur, or Hadamard product) and A s denotes A
.{z
. . A}, and we set the priority
|
s

of
A

to be higher than that of the ordinary matrix product (by definition, all the entries of
0 are 1).
Let us now discuss the complexity of Algorithm 3. For this, first note that computing all
0
the Hadamard products S d , d0 = 0, . . . , D requires O(Dn2 ) computations. Multiplication
with Mk−1 can be done in O(n2 ) steps. Finally, note that each iteration of the for loop takes
O(rn2 ) steps, which results in the overall worst-case complexity of O(rn2 D) if α∗ (θk−1 ) is
readily available. The computational complexity of determining α∗ (θk−1 ) depends on the
exact form of `t , and can be done efficiently in many situations: if, for example, `t is the
squared loss, then α∗ can be computed in O(n3 ) time. An obvious improvement to the
approach described here, however, would be to subsample the empirical loss Ln , which can
bring further computational improvements. However, the exploration of this is left for future
work.
Finally, note that despite the exponential cardinality of |I|, due to the strong algebraic
structure of the space of kernels, Ck−1 can be calculated
In fact, it is not hard to
P efficiently.
0 ). This also shows that if ρ
see that with the notation of the algorithm, Ck−1 = D
δ(d
0
d
d =0
decays “fast enough”, Ck−1 can be bounded independently of the cardinality of I.

4.1

Correctness of the sampling procedure

In this section we prove the correctness of Algorithm 3.
As said earlier, we assume that ρr1:d depends only on d, the order of interactions in κr1:d
3

Using importance sampling, more general weights can also be accommodated, too without effecting the
results as long as the range of weights (ρr1:d ) is kept under control for all d.

9

and, by abusing notation, we will write ρd to emphasize this. Let us
P how one can
Pnow consider
sample from qk−1,· . The implementation relies on the fact that ( rj=1 κj )d = r1:d ∈I κr1:d .
Remember that we denoted the kernel matrix underlying some kernel k by Kk , and recall
that Kk is an n × n matrix. For brevity, in the rest of this section for κ = κr1:d we will write
Kr1:d instead of Kκr1:d . Define Mk−1 = α∗ (θk−1 )α∗ (θk−1 )> . Thanks to (12) and the rotation
property of trace, we have
(15)
gk,r1:d = −ρ−2
d tr(Mk−1 Kr1:d ) .
P
The plan to sample from qk−1,· = |gk,· |/ r1:d ∈I |gk,r1:d | is as follows: We first draw the order
of interactions, 0 ≤ dˆ ≤ D. Given dˆ = d, we restrict the draw of the random multi-index
ˆ
R1:d to the set {r1:d ∈ I}. A multi-index will be sampled in a d-step
process: in each step
we will randomly choose an index from the indices of base kernels according to the following
distributions. Let S = K1 + . . . + Kr , let


ρ−2 tr(Mk−1 S d )
P dˆ = d|Fk−1 = PD d −2
d0 )
d0 =0 ρd0 tr(Mk−1 S
and, with a slight abuse of notation, for any 1 ≤ i ≤ d define


P Ri = ri |Fk−1 , dˆ = d, R1:i−1 = r1:i−1




i
(d−i)
tr Mk−1
K
S
j=1 rj




= P
r
i−1
(d−i)
0
tr
M
K
K
S
0
r
k−1
r
j
r =1
j=1
i
i

where we used the sequence notation (namely, s1:p denotes the sequence (s1 , . . . , sp )). We
have, by the linearity of trace and the definition of S that
r
X


tr Mk−1



i−1
j=1 Krj



Kri0

S

(d−i)



ri0 =1


= tr Mk−1



i−1
j=1 Krj



S

(d−i+1)



Thus, by telescoping,


P dˆ = d, R1:d = r1:d |Fk−1
=

ρ−2
. . . Krd−1 Krd )
d tr(Mk−1 Kr1
.
PD
−2
d0 )
d0 =0 ρd0 tr(Mk−1 S

as desired. An optimized implementation of drawing these random variables is shown as
Algorithm 3. The algorithm is written to return the multi-index R1:d .

5

Experiments

In this section we apply our method
P to the problem of multiple kernel learning in regression
with the squared loss: L(w) = 21 nt=1 (fw (xt ) − yt )2 , where (xt , yt ) ∈ Rr × R are the inputoutput pairs in the data. In these experiments our aim is to learn polynomial kernels (cf.
Section 4).
10

We compare our method against several kernel learning algorithms from the literature
on synthetic and real data. In all experiments we report mean squared error over test sets.
A constant feature is added to act as offset, and the inputs and output are normalized to
have zero mean and unit variance. Each experiment is performed with 10 runs in which we
randomly choose training, validation, and test sets. The results are averaged over these runs.

5.1

Convergence speed

In this experiment we examine the speed of convergence of our method and compare it against
one of the fastest standard multiple kernel learning algorithms, that is, the p-norm multiple
kernel learning algorithm of Kloft et al. (2011) with p = 2,4 and the uniform coordinate
descent algorithm that updates one coordinate per iteration uniformly at random (Nesterov,
2010, 2012; Shalev-Shwartz and Tewari, 2011; Richtárik and Takáĉ, 2011). We aim to learn
polynomial kernels of up to degree 3 with all algorithms. Our method uses Algorithm 3
for sampling with D = 3. The set of provided base kernels is the linear kernels built from
input variables, that is, κ(i) (x, x0 ) = x(i) x0(i) , where x(i) denotes the ith input variable. For
the other two algorithms the kernel set consists of product kernels from monomial terms for
D ∈ {0, 1, 2, 3} built from r base kernels, where r is the number of input variables. The
number of distinct product kernels is r+D
D . In this experiment for all algorithms we use
ridge regression with its regularization parameter set to 10−5 . Experiments with other values
of the regularization parameter achieved similar results.
We compare these methods in four datasets from the UCI machine learning repository
(Frank and Asuncion, 2010) and the Delve datasets5 . The specifications of these datasets are
shown in Table 1. We run all algorithms for a fixed amount of time and measure the value
Table 1: Specifications of datasets used in experiments.
Dataset
german
ionosphere
ringnorm
sonar
splice
waveform

# of variables
20
34
20
60
60
21

Training size
350
140
500
83
500
500

Validation size
150
36
1000
21
1000
1000

Test size
500
175
2000
104
1491
2000

of the objective function (1), that is, the sum of the empirical loss and the regularization
term. Figure 1 shows the performance of these algorithms. In this figure Stoch represents
our algorithms, Kloft represents the algorithm of Kloft et al. (2011), and UCD represents
the uniform coordinate descent algorithm. The results show that our method consistently
outperforms the other algorithms in convergence speed. Note that our stochastic method
updates one kernel coefficient per iteration, while Kloft updates r+D
kernel coefficients
D
per iteration. The difference between the two methods is analogous to the difference between stochastic gradient vs. full gradient algorithms. While UCD also updates one kernel
4

Note that p = 2 in Kloft et al. (2011) notation corresponds to p = 4/3 or ν = 2 in our notation, which
gives the same objective function that we minimize with Algorithm 2.
5
See, www.cs.toronto.edu/~delve/data/datasets.html

11

10

10

german

objective function

ionosphere

6

10
Kloft
Stoch
UCD

ringnorm

6

10

4

4

10

waveform

6

10

4

10

10

5

10

2

2

10

0

10

0

10

0

2

10

0

10

10

10

−2

−2

10
−5

10

0

−4

50
100
time (sec.)

150

10

0

−2

10

10

−4

5

10
15
time (sec.)

20

10

0

−4

100

200
300
time (sec.)

400

10

0

100
200
time (sec.)

300

Figure 1: Convergence comparison of our method and other algorithms.
coefficient per iteration its naive method of selecting coordinates results in a slower overall convergence compared to our algorithm. In the next section we compare our algorithm
against several representative methods from the MKL literature.

5.2

Synthetic data

In this experiment we examine the effect of the size of the kernel space on prediction accuracy
and training time of MKL algorithms. We generated data for a regression problem. Let r
denote the number of dimensions of the input space. The inputs are chosen uniformly at
random from [−1, 1]r . The output of each instance is the uniform combination of 10 monomial
terms of degree 3 or less. These terms are chosen uniformly at random among all possible
terms. The outputs are noise free. We generated data for r ∈ {5, 10, 20, . . . , 100}, with 500
training and 1000 test points. The regularization parameter of the ridge regression algorithm
was tuned from {10−8 , . . . , 102 } using a separate validation set with 1000 data points.
We compare our method (Stoch) against the algorithm of Kloft et al. (2011) (Kloft),
the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical
kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear
kernels built from the input variables.
P Recall that the method of Cortes et al. (2009) only
considers kernels of the form κθ = ( ri=1 θi κi )D , where D is a predetermined integer that
specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent
to adding polynomial kernels of degree less than D to the combination too. We provide all
possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011).
For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3.
The results are shown in Figure 2, the mean squared errors are on the left plot, while the
training times are on the right plot. In the training-time plot the numbers inside brackets
6
While several fast MKL algorithms are available in the literature, such as those of Sonnenburg et al. (2006);
Rakotomamonjy et al. (2008); Xu et al. (2010); Orabona and Luo (2011); Kloft et al. (2011), a comparison of
the reported experimental results shows that from among these algorithms the method of Kloft et al. (2011)
has the best performance overall. Hence, we decided to compare against only this algorithm. Also note
that the memory and computational cost of all these methods still scale linearly with the number of kernels,
making them unsuitable for the case we are most interested in. Furthermore, to keep the focus of the paper
we compare our algorithm to methods with sound theoretical guarantees. As such, it remains for future work
to compare with other methods, such as the infinite kernel learning of Gehler and Nowozin (2008), which lack
such guarantees but exhibit promising performance in practice.

12

1

250
Kloft
Stoch
Cortes
Bach
Uniform

0.9
0.8

200
training time (sec.)

0.7

MSE

0.6
0.5
0.4
0.3
0.2

150

100

50

0.1
0

20
40
60
80
number of dimensions of input space

0

100

20
[1,771]

40

60

80

100

[12,341]

[39,711]

[91,881]

[176,851]

Figure 2: Comparison of kernel learning methods in terms of test error (left) and training
time (right).
indicate the total number of distinct product kernels for each value of r. This is the number of
kernels fed to the Kloft algorithm. Since this method deals with a large number of kernels,
it was possible to precompute and keep the kernels in memory (8GB) for r ≤ 25. Therefore,
we ran this algorithm for r ≤ 25. For r > 25, we could use on-the-fly implementation of this
algorithm, however that further increases the training time. Note that the computational cost
of this method depends linearly on the number of kernels, which in this experiment, is cubic
in the number of input variables since D = 3. While the standard MKL algorithms, such
as Kloft, cannot handle such large kernel spaces, in terms of time and space complexity,
the other three algorithms can efficiently learn kernel combinations. However their predictive
accuracies are quite different. Note that the performance of the method of Cortes et al.
(2009) starts to degrade as r increases. This is due to the restricted family of kernels that
this method considers. The method of Bach (2008), which is well-suited to learn sparse
combination of product kernels, performs better than Cortes et al. (2009) for higher input
dimensions. Among all methods, our method performs best in predictive accuracy while its
computational cost is close to that of the other two competitors.

5.3

Real data

In this experiment we aim to compare several MKL methods in real datasets. We compare
our new algorithm (Stoch), the algorithm of Bach (2008) (Bach), and the algorithm of
Cortes et al. (2009) (Cortes). For each algorithm we consider learning polynomial kernels
of degree 2 and
P 3. We also include uniform combination of product kernels of degree D,
i.e. κD = ( ri=1 κi )D , for D ∈ {1, 2, 3} (Uniform). To find out if considering higherorder interaction of input variables results in improved performance we also included a MKL
algorithm to which we only feed linear kernels (D = 1). We use the MKL algorithm of Kloft
et al. (2011) with p ∈ {1, 2} (Kloft).
We compare these methods on six datasets from the UCI machine learning repository

13

ionosphere

german

ringnorm

0.78

Bach (d=2)
Bach (d=3)

MSE

Stoch (d=2)
Stoch (d=3)
Stoch (d=3, prior)

0.8

0.7

0.76
0.74

0.6

0.6
0.72
0.7

0.4

0.5

0.68
0.2

Cortes (d=2)
Cortes (d=3)
Kloft (p=1)
Kloft (p=2)
Uniform (d=1)
Uniform (d=2)
Uniform (d=3)

sonar

splice

waveform

0.9

0.8

0.6

0.5

0.55

0.45

0.5

0.4

0.7

0.45
0.4

0.35

0.6

0.35

0.3

Figure 3: Prediction error of different methods in the real data experiment
and Delve datasets. In these datasets the number of dimensions of the input space is 20
and above. The specifications of these datasets are shown in Table 1. The regularization
parameter is selected from the set {10−4 , . . . , 103 } for all methods using a validation set. The
results are shown in Figure 3.
Overall, we observe that methods that consider non-linear variable interactions (Stoch,
Bach, and Cortes) perform better than linear methods (Kloft). Among non-linear methods, Cortes performs worse than the other two. We believe that this is due to the restricted
kernel space considered by this method. The performance of Stoch and Bach methods is
similar overall.
We observe that our method overfits when it considers kernels of degree 3. However, one
can easily prevent overfitting by assigning larger ρ values to higher-degree kernels such that
the stochastic algorithm selects lower-degree kernels more often. For this purpose, we repeat
this experiment for D = 3 with a modified set of ρ values, where we use ρ2d = 1 for kernels of
degree 2 or less and ρ2d = 4 for kernels of degree 3. With the new ρ coefficients we observe an
improvement in algorithm’s performance. See Stoch (D = 3, prior) error values in Figure 3.

6

Conclusion

We introduced a new method for learning a predictor by combining exponentially many linear
predictors using a randomized mirror descent algorithm. We derived finite-time performance
bounds that show that the method efficiently optimizes our proposed criterion. Our proposed
method is a variant of a randomized stochastic coordinate descent algorithm, where the main
trick is the careful construction of an unbiased randomized estimate of the gradient vector
that keeps the variance of the method under control, and can be computed efficiently when
the base kernels have a certain special combinatorial structure. The efficiency of our method
14

was demonstrated for the practically important problem of learning polynomial kernels on a
variety of synthetic and real datasets comparing to a representative set of algorithms from
the literature. For this case, our method is able to compute an optimal solution in polynomial
time as a function of the logarithm of the number of base kernels. To our knowledge, ours is
the first method for learning kernel combinations that achieve such an exponential reduction
in complexity while satisfying strong performance guarantees, thus opening up the way to
apply it to extremely large number of kernels. Furthermore, we believe that our method is
applicable beyond the case studied in detail in our paper. For example, the method seems
extendible to the case when infinitely many kernels are combined, such as the case of learning
a combination of Gaussian kernels. However, the investigation of this important problem
remains subject to future work.
Acknowledgements
This work was supported by Alberta Innovates Technology Futures and NSERC.

A

Proofs

In this section we present the proofs of Theorem 3.1 and Proposition 3.2. The proof of
Theorem 3.1 is based on the standard proof of the convergence rate of the proximal point
algorithm, see, for example, (Beck and Teboulle, 2003), or the proof of Proposition 2.2 of
Nemirovski et al. (2009b), which carry over the same argument to solve very similar but less
general problems. We also provide some improvements and simplifications at the end. Before
giving the actual proof, we need the following standard lemma:
Lemma A.1 (Lemma 2.1 of Nemirovski et al. 2009b). Assume that Ψ is α-strongly convex
with respect to some norm k · k (i.e., (4) holds). Let θ1 ∈ K ∩ A◦ , θ ∈ K ∩ A, and g ∈ Rd .
Define θ2 = arg minθ0 ∈K∩A {hg, θ0 i + DΨ (θ0 , θ1 )}. Then
hg, θ1 − θi ≤ DΨ (θ, θ1 ) − DΨ (θ, θ2 ) +

kgk2∗
.
2α

We provide an alternate proof that is based on the so-called 3-DIV lemma. The 3-DIV
lemma (e.g., Lemma 11.1, Cesa-Bianchi and Lugosi, 2006) allows one to express the sum of
the divergences between the vectors u, v and v, w in terms of the divergence between u and
w and an additional “error term”, where u ∈ A, v, w ∈ A◦ :
DΨ (u, v) + DΨ (v, w) = DΨ (u, w) + h∇ψ(w) − ∇ψ(v), u − vi .
Proof. Note that θ2 ∈ A◦ due to behavior of Ψ at the boundary of A. Thus, Ψ is differentiable
at θ2 and
∇1 DΨ (θ2 , θ1 ) = ∇ψ(θ2 ) − ∇ψ(θ1 ) ,

(16)

where ∇1 denotes differentiation of DΨ w.r.t. its first variable. Let f (θ0 ) = hg, θ0 i+DΨ (θ0 , θ1 ).
By the optimality property of θ2 and since θ ∈ K ∩ A, we have
h∇f (θ2 ), θ2 − θi ≤ 0 .
15

Plugging in the definition of f together with the identity (16) gives
hg + ∇ψ(θ2 ) − ∇ψ(θ1 ), θ2 − θi ≤ 0 .

(17)

Now, by the 3-DIV Lemma,
DΨ (θ, θ2 ) + DΨ (θ2 , θ1 ) = DΨ (θ, θ1 ) + h∇Ψ(θ1 ) − ∇Ψ(θ2 ), θ − θ2 i
= DΨ (θ, θ1 ) + hg + ∇Ψ(θ2 ) − ∇Ψ(θ1 ), θ2 − θi + hg, θ − θ2 i .
Hence, by reordering and using the inequality (17) we get
DΨ (θ, θ2 ) − DΨ (θ, θ1 ) ≤ hg, θ − θ2 i − DΨ (θ2 , θ1 )
= hg, θ1 − θ2 i − DΨ (θ2 , θ1 ) + hg, θ − θ1 i
≤

kgk2∗
+ hg, θ − θ1 i ,
2α

where in the last line we used Young’s inequality7 and that due to the strong convexity of Ψ,

DΨ (θ2 , θ1 ) ≥ α2 kθ2 − θ1 k2 .
Theorem 3.1. Assume that Ψ is α-strongly convex with respect to some norm k · k (with
dual norm k · k∗ ) for some α > 0, that is, for any θ ∈ A◦ , θ0 ∈ A
Ψ(θ0 ) − Ψ(θ) ≥ ∇Ψ(θ), θ0 − θ + α2 kθ0 − θk2 .

(4)

Suppose, furthermore, that Algorithm 1 is run for T time steps. For 0 ≤ k ≤ T − 1 let Fk
denote the σ-algebra generated by θ1 , . . . , θk . Assume that, for all 1 ≤ k ≤ T , ĝk ∈ Rd is an
unbiased estimate of ∇J(θ(k−1) ) given Fk−1 , that is,
E [ ĝk | Fk−1 ] = ∇J(θ(k−1) ).

(5)

Further, assume that there exists a deterministic constant B ≥ 0 such that for all 1 ≤ k ≤ T ,


E kĝk k2∗ Fk−1 ≤ B a.s.
(6)
q
Finally, assume that δ = supθ0 ∈K∩A Ψ(θ0 ) − Ψ(θ(0) ) is finite. Then, if ηk−1 = 2αδ
BT for all
k ≥ 1, it holds that
"
!#
r
T
1 X (k−1)
2Bδ
E J
θ
− inf J(θ) ≤
.
(7)
θ∈K∩A
T
αT
k=1

Furthermore, if
kĝk k2∗ ≤ B 0 a.s.
(8)
q
2αδ
for some deterministic constant B 0 and ηk−1 = B
0 T for all k ≥ 1 then, for any 0 <  < 1,
it holds with probability at least 1 −  that
s
!
r
T
B 0 δ log 1
1 X (k−1)
2B 0 δ
J
θ
− inf J(θ) ≤
+4
.
(9)
θ∈K∩A
T
αT
αT
k=1

7

Young’s inequality states that for any x, y vectors and α > 0, hx, yi ≤ kxk∗ kyk ≤

16

1
2



kxk2
∗
α


+ αkyk2 .

P
(T )
Proof. Introduce the average learning rates η k = ηk / Tk=1 ηk−1 , k = 1, . . . , T , the averaged
parameter estimates
T
X
(T )
θ̄(T −1) =
η k−1 θ(k−1)
k=1

and choose some θ∗ ∈ K ∩ A. To prove the first part of the theorem,
it suffices to show that

(T
−1)
∗
(k−1)
the bound holds for J(θ̄
) − J(θ ). Define gk = ∇J θ
. By the convexity of J(θ),
we have
T


 


X
(T )
η k−1 J θ(k−1) − J(θ∗ )
J θ̄(T −1) − J(θ∗ ) ≤

≤

=

k=1
T
X
k=1
T
X

D
E
(T )
η k−1 gk , θ(k−1) − θ∗
T
E X
E
D
D
(T )
(T )
η k−1 ĝk , θ(k−1) − θ∗ +
η k−1 gk − ĝk , θ(k−1) − θ∗ (18)
k=1

k=1

Notice that the first term on the right hand side above is the sum of linearized losses appearing
in the standard analysis of the proximal point algorithm with loss functions ĝk and learning
(T )
rates η k−1 , and the second sum contains the term that depends on how well ĝk estimates
the gradient gk . Thus, in this way, it is separated how the proximal point algorithm and the
gradient estimate effect the convergence rate of the algorithm. The first sum can be bounded
by invoking the standard bound for the proximal point algorithm (we will give the very short
proof for completeness, based on Lemma A.1), while the second sum can be analyzed by
noticing that, by assumption (5), its elements form an {Fk }-adapted martingale-difference
sequence.
To bound the first sum, first note that the conditions of Lemma A.1 are satisfied for
(T )
θ1 = θ(k−1) , θ = θ∗ , g = η k−1 ĝk , since θ1 ∈ K ∩ A◦ (as mentioned beforehand, this follows
from the behavior of Ψ at the boundary of A). Further, note that due to the so-called
projection lemma (i.e., the DΨ -projection of the unconstrained optimizer is the same as the
optimizer of the constrained optimization problem),we can conclude that θ(k) = θ2 , where θ2
is defined in Lemma A.1. Thus, Lemma A.1 gives
D
E
η 2 kĝk k2∗
ηk−1 ĝk , θ(k−1) − θ∗ ≤ DΨ (θ∗ , θ(k−1) ) − DΨ (θ∗ , θ(k ) + k−1
.
2α
Summing the above inequality for k = 1, . . . , T , the divergence terms cancel each other,
yielding
!
T
T
D
E
X
X
1
1
(T )
2
η k−1 ĝk , θ(k−1) − θ∗ ≤ PT
ηk−1
kĝk k2∗ .
DΨ (θ∗ , θ(0) ) − DΨ (θ∗ , θ(T ) ) +
2α
η
k=1 k−1
k=1
k=1
(19)
Let us now turn to the second sum. We start with developing a bound on the expected
(T )
regret. For any 1 ≤ k ≤ T , by construction η k−1 and θ(k−1) are Fk−1 -measurable. This,
together with (5) gives
D
D
h
E
i
E
(T )
(T )
E η k−1 gk − ĝk , θ∗ − θ(k−1) Fk−1 = η k−1 gk − E [ ĝk | Fk−1 ] , θ∗ − θ(k−1) = 0 . (20)
17

Combining this result with (18) and (19) yields
h 

i
E J θ̄(T ) − J(θ∗ ) ≤
≤

1

∗

DΨ (θ , θ
η
k−1
k=1
1 PT
2
δ + 2α
k=1 ηk−1 B
,
PT
k=1 ηk−1
PT

(0)

∗

) − DΨ (θ , θ

(T )

!
T
 

1 X 2
2
)+
ηk−1 E E kĝk k∗ Fk−1
2α
k=1

(21)

where we used the tower rule to bring in the bound (6), the nonnegativity of Bregman
divergences, and DΨ (θ, θ(0) ) ≤ Ψ(θ) − Ψ(θ(0) ); the latter holds as ∇Ψ(θ(0) ), θ − θ(0) ≥ 0
q
since θ(0) minimizes Ψ on K. Substituting ηk−1 = η = 2αδ
BT , k = 1, . . . , T finishes the proof
of (7).

To prove the high probability result (9), notice that thanks to (5) ηk−1 gk − ĝk , θ∗ − θ(k−1)
is an {Fk }-adapted martingale-difference sequence (cf. (20)). By the strong convexity of Ψ
we have
α (k−1)
kθ
− θ∗ k2 ≤ Ψ(θ(k−1) ) − Ψ(θ∗ ) ≤ δ.
2
Furthermore, conditions
(5) and (8) imply that kgk k2∗ ≤ B 0 a.s., and so by (8) we have
√
kgk − ĝk k∗ ≤ 2 B 0 a.s. Then by Hölder’s inequality
r
E
D
2B 0 δ
∗
(k−1)
∗
(k−1)
gk − ĝk , θ − θ
≤ kgk − ĝk k∗ kθ − θ
k≤2
.
α
Thus, by the Hoeffding-Azuma inequality (see, e.g., Lemma A.7, Cesa-Bianchi and Lugosi,
2006), for any 0 <  < 1 we have, with probability at least 1 − ,
v
!
u
T
T
D
E
u B0δ X
X
4
1
(T )
∗
(k−1)
t
2
η k−1 gk − ĝk , θ − θ
≤ PT
ηk−1 ln .
(22)
α

η
k−1
k=1
k=1
k=1
Combining (19) with (8) implies an almost sure upper bound on the first sum on the right
hand side of (18) as in (21) with B 0 in place of B. This, together
q with (22) proves the required
high probability bound (9) when substituting ηk−1 = η 0 =

2αδ
B0T .


Proposition 3.2. For 1 ≤ t ≤ n, let `∗t : R → R denote the convex conjugate of `t : `∗t (v) =
0 )i, and let K =
supτ ∈R {vτ − `t (τ )}, v ∈ R. For i ∈ I, recall that κi (x, x0 ) = hφi (x), φi (xP
i
(κi (xt , xs ))1≤t,s≤n be the n × n kernel matrix underlying κi and let Kθ = i∈I ρθ2i Ki be the
i
P
θi
∗
kernel matrix underlying κθ =
i∈I ρ2i κi . Then, for any fixed θ, the minimizer w (θ) of
J(·, θ) satisfies
n
θi X ∗
wi∗ (θ) = 2
α (θ)φi (xt ), i ∈ I ,
(10)
ρi t=1 t
where

(
∗

α (θ) = arg min
α∈Rn

n

1 >
1X ∗
α Kθ α +
`t (−nαt )
2
n
t=1

18

)
.

(11)

Proof. By introducing the variables τ = (τt )1≤t≤n ∈ Rn and using the definition of L we can
write the optimization problem (3) as the constrained optimization problem
n

minimizen

w∈W,τ ∈R

1X
1 X ρ2i kwi k22
`t (τt ) +
n
2
θi
t=1

s.t. τt =

X

hwi , φi (xt )i ,

(23)

i∈I

i∈I

In what follows, we call this problem the primal problem. The Lagrangian of this problem is
(
)
n
n
X
1 X ρ2i kwi k22 X
. 1X
hwi , φi (xt )i ,
L(w, τ, α) =
`t (τt ) +
+
αt τt −
n
2
θi
t=1

t=1

i∈I

i∈I

Rn

where α = (αt )1≤t≤n ∈
is the vector of Lagrange multipliers (or dual variables) associated
.
with the n equality constraints. The Lagrange dual function, g(α) = inf w,τ L(w, τ, α), can be
readily seen to satisfy
!
n
1 >
1X ∗
g(α) = −
α Kθ α +
`t (−nαt ) .
2
n
t=1

Now, since the objective function of the primal problem is convex and the primal problem
involves only affine equality constraints and the primal problem is clearly feasible, by Slater’s
condition (p.226, Boyd and Vandenberghe, 2004), if α∗ (θ) is the maximizer of g(α) then
w∗ (θ) = arg min infn L(w, τ, α∗ (θ))
w∈W τ ∈R
(
)
n
X ρ2 kwi k2 X
2
i
= arg min
−
αt hwi , φi (xt )i .
2θi
w∈W
t=1

i∈I

The minimum of the last expression is readily seen to be equal to the expression given in (10),
thus finishing the proof.


B

Calculating the derivative of J(θ)

In this section we show that under mild conditions the derivative of J exist and we also give
explicit forms. These derivations are quite standard and a similar argument can be found in
the paper by (e.g.) Rakotomamonjy et al. (2008) specialized to the case when `t is the hinge
loss.
As it is well-known, thanks to the implicit function theorem (e.g., Brown and Page,
∂2
∂
1970, Theorem 7.5.6), provided that J = J(w, θ) is such that ∂θ∂w
J(w, θ) and ∂w
J(w, θ)
are continuous, the gradient of J(θ) can be computed by evaluating the partial derivative
∂
∂
∗
∂θ J(w, θ) of J(w, θ) with respect to θ at (w (θ), θ)), that is, ∂θ J(θ) = ∂θ J(w, θ)|w=w∗ (θ) .Note
that the derivative is well-defined only if θ > 0, that is, when no coordinates of θ is zero, in
which case
 2 ∗

ρi kwi (θ)k22
∂
∗
J(w (θ), θ) = −
.
(24)
∂θ
θi2
i∈I
If θi = 0 for some i ∈ I, we define the derivative in a continuous manner as
∂
J(θ) =
∂θ

lim
0

θ →θ

θ0 ∈∆,θ0 >0

19

∂
J(θ0 )
∂θ

(25)

assuming that the limit exists. From (10) we get, for any i ∈ I, kwi∗ (θ)k22 =
Combining with (24) we obtain

 ∗ >
∂
α (θ) Ki α∗ (θ)
∗
.
J(w (θ), θ) = −
∂θ
ρ2i
i∈I

θi2 ∗
α (θ)> Ki α∗ (θ).
ρ4i

Now, by (25) and the implicit function theorem, α∗ (θ) is a continuous function of θ provided
that the functions `∗t (1 ≤ t ≤ n) are twice continuously differentiable. This shows that under
the conditions listed so far, the limit in (25) exists. In the application we shall be concerned
with, these conditions can be readily verified.



Motivated by value function estimation in reinforcement learning, we study statistical linear inverse problems, i.e., problems where the
coefficients of a linear system to be solved
are observed in noise. We consider penalized estimators, where performance is evaluated using a matrix-weighted two-norm of
the defect of the estimator measured with
respect to the true, unknown coefficients.
Two objective functions are considered depending whether the error of the defect measured with respect to the noisy coefficients
is squared or unsquared. We propose simple, yet novel and theoretically well-founded
data-dependent choices for the regularization
parameters for both cases that avoid datasplitting. A distinguishing feature of our
analysis is that we derive deterministic error
bounds in terms of the error of the coefficients, thus allowing the complete separation
of the analysis of the stochastic properties of
these errors. We show that our results lead to
new insights and bounds for linear value function estimation in reinforcement learning.

1. Introduction
Let A be a real-valued m×d matrix, b be a real-valued
m-dimensional vector, M be an m × m positive semidefinite matrix, and consider the loss function LM :
Rd → R defined by
.
LM (θ) = kAθ − bkM ,
Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012.
Copyright 2012 by the author(s)/owner(s).

where k·kM denotes the M matrix-weighted two-norm.
We consider the problem of finding a minimizer of this
loss when instead of A, b, one has access only to their
respective “noisy” versions, Â, b̂. We call this problem
a statistical linear inverse problem.
Our main motivation to study this problem is to better understand the so-called least-squares approach to
value function estimation in reinforcement learning,
whose goal is to estimate the value function that corresponds to a Markov reward process.1 The leastsquares approach originates from the work of Bradtke
and Barto (1996), who proposed to find the parametervector θ̂ of a linear-in-the-parameters value function
by solving Âθ = b̂ where the “noisy” matrix-vector
pair, (Â, b̂), is computed based on a finite sample.
They have proven the almost sure convergence of θ̂
to θ∗ , the solution of Aθ = b, under appropriate conditions on the sample as the sample-size converges to
infinity. In particular, they assumed that the sample is generated from either an absorbing or an ergodic Markov chain. More recently, several studies appeared where the finite-sample performance of LSTDlike procedures were investigated (see, e.g., (Antos
et al., 2008; Ghavamzadeh et al., 2010; Lazaric et al.,
2010; Ghavamzadeh et al., 2011)). The nonparametric
variant has also received some attention (Farahmand
et al., 2009; Maillard, 2011).
One of the difficulties in the analysis of these procedures is that in these problems the sample is correlated, so the standard techniques of supervised learning that assume independence cannot be used. The
approach followed by the above-mentioned papers is
to extend the existing techniques on an individual basis to deal with correlated samples. However, this
1
For background on this problem the reader may consult, e.g., the books by Bertsekas and Tsitsiklis (1996);
Sutton and Barto (1998); Szepesvári (2010).

Linear statistical estimation

might be quite laborious, even only considering the relatively easier case of regression2 (e.g., Farahmand and
Szepesvári 2011). Thus, a more appealing approach
might be to first derive error bounds as a function of
the errors Â − A, b̂ − b. The advantage of this approach is that it allows one to decouple the technical
issue of studying the concentration of the errors Â−A,
b̂ − b from the error (or stability) analysis of the estimation procedures. This is the approach that we
advocate and follow in this paper. Consequently, our
results will always be applicable when one can prove
the concentration of the errors Â − A, b̂ − b, leading
to an overall elegant, modular approach to deriving
finite-sample bounds. In some way, our approach parallels the recent trend in learning theory where sharp
finite-sample bounds are obtained by first proving deterministic “regret bounds” (e.g., Cesa-Bianchi et al.,
2004).
A second unique feature of our approach is that we derive our results in the above-introduced framework of
general statistical linear inverse problems. This allows
us to concentrate on the high-level structure of the
problem and yields cleaner proofs and results. Furthermore, we think that the problem of linear estimation is interesting on its own due to its mathematical elegance and its applicability beyond value function estimation (a number of specific linear inverse
problems, ranging from computer tomography to time
series analysis, are discussed in the books by Kirsch
(2011) and Alquier et al. (2011)).
We will also place special emphasis in statistical linear inverse problems whose underlying system is inconsistent (i.e., when there is no solution to Aθ = b).
In value function estimation, such inconsistency may
arise in the so-called off-policy version of the problem.
Understanding the inconsistent case is important because results that apply to it may shed light on issues
arising when learning in badly conditioned systems.
1.1. Goals
In this paper, our goal will be to derive exact, uniform, fast, high-probability oracle inequalities for the
estimation procedures we study. That is, our goal is
to prove that for our choice of an estimator θ̂, for any
0 < δ < 1, with probability 1 − δ,
n
o
LM (θ̂) ≤ inf LM (θ) + cÂ,b̂ (θ, δ) ,
(1)
θ

where, for fixed values of θ, δ,
cÂ,b̂ (θ, δ) = O(max(kÂ − Ak, kb̂ − bk))

(2)

2
Regression is a special case of value function estimation
(Szepesvári, 2010).

for some appropriate norm k·k. The above is called an
oracle inequality since the performance of θ̂ (as measured with the loss) is compared to that of an “oracle”
that has access to the true loss function. The term
cÂ,b̂ (θ, δ) expresses the “regret” permitted due to the
lack of knowledge of the true loss function. The scaling of this term with θ (or a norm of it) and δ will also
be of interest.
Let us now explain the special attributes of the above
inequality. We call the “rate” in the above inequality
“fast” when (2) holds. Such a “fast rate” is possible
in simple settings (e.g., when d = 1, A = Â = 1),
hence it is natural to ask whether such rates are still
possible in more general settings. The oracle inequality above is called exact because the leading constant
(the constant multiplying LM (θ)) equals to 1. When
L∗ = inf θ LM (θ) is positive (implying that the system
is inconsistent), then only a leading constant of one
can guarantee the convergence of the loss to the minimal loss, i.e., the consistency of the estimator. We call
the above inequality uniform because it holds for any
value of δ. This should be contrasted with inequalities where the range of δ is lower-bounded and/or the
estimator uses its value as input, which may be useful in some cases but falls short of fully characterizing
the tail behavior of the loss of the resulting estimator.
With some abuse of terminology, an inequality of the
above form that holds for all small values of δ shall
be also called uniform. Uniform bounds seem to be
harder to prove than their non-uniform counterparts,
and we do not know of any uniform, high-probability
exact oracle inequality with fast rates, not even in the
case of linear regression. Unfortunately, we were also
unable to derive such results.
When deriving the estimators, we shall see that a major challenge is to control the magnitude of θ̂. Indeed,
it follows from our objective function that the size of
Aθ̂ must be controlled, and when A is unknown the
magnitude of θ̂ must be controlled. This might be difficult when following a naive approach of solving Âθ = b̂
to get θ̂, e.g., when Â is singular, or near-singular (as
might be the case frequently in practice). To cope
with this issue, in this paper we study procedures built
around penalized estimators where a penalty Pen(θ) is
combined with the empirical loss L̂M (θ) = kÂθ − b̂kM .
The penalty is assumed to be some norm of θ. We
study two procedures. In the first one, the loss is combined directly with the penalty, in an additive way to
get the objective function L̂M (θ) + λkθk, while in the
second one the square of the empirical loss is combined
with the penalty: L̂2M (θ) + λkθk. Note that both objective functions are convex. We note in passing that
the second objective function when kθk is the `1 -norm

Linear statistical estimation

gives a Lasso-like procedure, but we postpone further
discussion of these choices to later sections of the paper.
In the case of both objective functions the main issue
becomes selecting the regularization coefficient λ > 0.
In this paper we give novel procedures to this end and
show that these procedures have advantageous properties: we are able to derive oracle inequalities with
fast rates for our procedures, although the inequalities
will be either exact or uniform (but not both). To
the best of our knowledge our general approach, our
procedures, analytic tools and results are novel.
The organization of the paper is as follows: in the
next section, to motivate the general framework, we
briefly describe value function estimation and how it
can be put into our general framework. This is followed by a brief section that gives some necessary definitions. Section 3 contains our main results for the
two approaches mentioned above. Section 4 discusses
the results in the context of value function estimation.
The paper is concluded and future work is discussed
in Section 5.

2. Value-estimation in Markov Reward
Processes
The purpose of this section is to show how our
results can be applied in the context of valueestimation in Markov Reward Processes. Consider a
Markov Reward Process (MRP) (X0 , R1 , X1 , R2 , . . .)
over a (topological) state space X . By this we
mean that (X0 , R1 , X1 , R2 , . . .) is a stochastic process,
(Xt , Rt+1 ) ∈ X × R for t ≥ 0 and given the history
Ht = (X0 , R1 , X1 , R2 , . . . , Xt ) up to time t, the distribution of state Xt+1 is completely determined by
Xt , while the distribution of the reward Rt+1 is completely determined by Xt and Xt+1 given the history
Ht+1 . Denote by PM the distribution of (Rt+1 , Xt+1 )
given Xt . We shall call PM a transition kernel. Assume that support of the distribution of X0 covers the
whole state space
P∞X . Define the value of a state x ∈ X
by V (x) = E [ t=0 γ t Rt+1 |X0 = x], where 0 < γ < 1
is the so-called discount factor.One central problem in
reinforcement learning is to estimate the value function V given the trajectory (X0 , R1 , X1 , R2 , . . .) (Sutton and Barto, 1998). One popular method is to
exploit that the value function is the unique solution to the so-called Bellman equation, which takes
the form T W − W = 0, where W : X → R and
T : RX → RX is the so-called Bellman operator defined using (T W )(x) = E [Rt+1 + γW (Xt+1 )|Xt = x].
Note that T is affine linear.

Given a finite sample (X0 , R1 , X1 , R2 , . . . , Xn+1 ), the
LSTD algorithm of Bradtke and Barto (1996) finds
an approximate solution to the Bellman equation by
solving the linear system
n
X
(Rt+1 + γWθ (Xt+1 ) − Wθ (Xt ))φ(Xt ) = 0

(3)

t=1

in θ ∈ Rd . Here φ = (φ1 , . . . , φd )> is a vector of d basis
functions, φi : X → R, 1 ≤ i ≤ d, and Wθ : X → R
is defined using Wθ (x) = hθ, φ(x)i. Denoting by θ̂ the
solution to (3), Wθ̂ is the approximate value function
computed by LSTD. This method can be derived as an
instrumental variable method to find an approximate
fixed point of T (Bradtke and Barto, 1996) or as a
Bubnov-Galerkin method (Yu and Bertsekas, 2010).
In any case, the method can be viewed as solving a
“noisy” version of the linear system
Aθ = b .

(4)



st
st >
st
and
Here, A = E (φ(X
t ) − γφ(Xt+1 ))φ(Xt )

st
, where (X0st , R1st X1st , R2st , . . .) is
b = E φ(Xtst )Rt+1
a steady-state MRP with transition kernel PM .3 The
linear system (4) can be shown to be consistent (Bertsekas and Tsitsiklis, 1996).4 Note that (3) can also
be writtenPin the compact form Âθ = b̂, where
n
Â = 1/n t=1 (φ(Xt ) − γφ(Xt+1 ))φ(Xt )> and b̂ =
Pn
1/n t=1 Rt+1 φ(Xt ). By thinking of Â, b̂ as “noisy”
versions of A, b and observing that for any M  0 solutions to (4) coincide with the minimizers of LM (θ) =
kAθ − bkM we see that the least-squares approach to
value function estimation can be cast as an instance of
−1
statistical
 linear inverse
 problems. When M = C ,
>
C = E φ(Xt )φ(Xt ) , LM (·) becomes identical to
the so-called projected Bellman error loss which can
also be written as LM (θ) = kΠφ,µ (T Wθ − Wθ )kµ,2 ,
where µ is the steady-state distribution underlying
PM , k · kµ,2 is the weighted L2 (µ)-norm over X and
Π : L2 (X , µ) → L2 (X , µ) is the projection on the linear space spanned by φ with respect to the k·kµ,2 -norm
(Antos et al., 2008).
Note that under mild technical assumptions (to be
discussed later) one can show that (Ân , b̂n ) = (Â, b̂)
gets concentrated around (A, b) at the usual parametric rate as the sample size n diverges. Thus, we can
indeed view Â, b̂ as “noisy” approximations to (A, b).
3

The MRP is said to be in a steady-state if the distribution of Xt is independent of t.
4
For a discussion of how well Wθ∗ approximates V the
reader is directed to consult the paper by Scherrer (2010)
and the references therein. In this paper, we do not discuss this interesting problem but accept (4) as our starting
point.

Linear statistical estimation

One variation of this problem, the so-called off-policy
problem, gives further motivation to recast the problem in terms of a loss function LM (·) to be minimized.
In the off-policy problem the data comes in the form
of triplets, ((X0 , R̃1 , X̃1 ), (X1 , R̃2 , X̃2 ), . . .), where the
distribution of (R̃t+1 , X̃t+1 ) is again independent of
Ht = ((X0 , R̃1 , X̃1 ), (X1 , R̃2 , X̃2 ), . . . , (Xt−1 , R̃t , X̃t ))
given Xt and is equal to the transition kernel PM . Further, it is assumed that (Xt )t≥0 is a Markov process.
The previous setting (also called the on-policy case)
is replicated when X̃t = Xt , thus this new setting is
more general than the previous one. The straightforward generalization
h of the least-squares approach
i is
st
to define A = E (φ(Xtst ) − γφ(X̃t+1
))φ(Xtst )> and
h
i
st
b = E R̃t+1
φ(Xtst ) for the “steady-state” process
st
st
(Xtst , R̃t+1
, X̃t+1
)t≥0 . In this case, the linear system Aθ = b is not necessarily consistent but one
can still aim for minimizing (for example)
Pn the projected Bellman error. Using Â = 1/n t=1 (φ(Xt ) −
Pn
γφ(X̃t+1 ))φ(Xt )> and b̂ = 1/n t=1 R̃t+1 φ(Xt ) we
can again cast the problem as a statistical linear inverse problem.

3. Results
In this section we give our main results for statistical
linear inverse problems. We start with a few definitions. For real numbers a, b, we use a ∨ b to denote
max(a, b). The operator norm of a matrix S with respect to the Euclidean norm k · k2 is known to satisfy
kSk2 = νmax (S). In what follows, we fix a vector norm
k · k. Define the errors of Â and b̂ with the following
respective equations: let
1
1
.
.
∆A = kM 2 (A − Â)k2,∗ , ∆b = kM 2 (b − b̂)k2 , (5)

where kXk2,∗ denotes the operator norm of matrix X
with respect to the norms k · k2 and k · k, meaning that
kXk2,∗ = supv6=0 kXvk2 /kvk.
Although our main results are oracle inequalities, it
will also be interesting to name a minimizer of LM (θ)
to explain the structure of some bounds. For this,
we introduce θ∗ ∈ Rd as a vector such that θ∗ ∈
arg minθ∈Rd LM (θ) where if multiple minimizers exist
we choose one with the minimal norm k · k. 5

suitable high-probability bounds on ∆A and ∆b are
available:
Assumption 3.1. There exist known scaling constants sA , sb > 0 and known “tail” functions zA,δ , zb,δ ,
δ ∈ (0, 1] s.t. for any 0 < δ < 1, the following hold
simultaneously with probability (w.p.) at least 1 − δ:
∆A ≤ sA zA,δ ,

∆b ≤ sb zb,δ .

To fix the scales of these bounds, we restrict zA,δ , zb,δ
so that zA, 1e = zb, 1e = 1, where e is the base of natural
logarithm.
The reason to have two terms on the right-hand side
in the above inequalities as opposed to having a single
term only is because we wish to separate the terms
attributable to δ and the sample size. The intended
meaning of sa (and sb ) is to capture how the errors
behave as a function of the sample size n (typically, we
expect sA , sb = O(n−1/2 )), while the terms zA,δ , zb,δ
capture how the errors behave
p as a function δ (e.g.,
they are typically of size O( ln(1/δ))). In particular,
sA , sb should be independent of δ and zA,δ , zb,δ should
be independent of the sample size. This separation
will allow us to distinguish between uniform and nonuniform versions of our oracle inequalities.
3.1. Minimizing the unsquared penalized loss
In this section, we present the results for the unsquared
penalized loss. Choose k · k to be some norm of the
d-dimensional Euclidean space. For λ > 0, define
o

θ̂λ ∈ arg min L̂M (θ) + λkθk ,
(6)
θ∈Rd

where L̂M (θ) = kÂθ − b̂kM . Our first result gives an
oracle inequality for θ̂λ as a function of ∆A and ∆b .
Lemma 3.2. Consider θ̂λ as defined in (6). Then,
h
i

LM (θ̂λ ) ≤ 1 ∨ ∆λA inf LM (θ) + (∆A + λ)kθk
θ∈Rd


+ 2 ∨ 1 + ∆λA ∆b .

In general, ∆A , ∆b are unknown. As it will turn out, in
order to properly tune the penalized estimation methods we consider, we need at least upper bounds on
these quantities (in particular, on ∆A ). To stay independent of sampling assumptions, we assume that

The proof, which is attractively simple and thus elegant, is given in the appendix. The result suggests that
the ideal choice for λ is ∆A . Since ∆A is unknown,
we use its upper bound to choose λ. Depending on
whether we allow λ to depend on δ or not, we get a
non-uniform or uniform oracle inequality. In all cases,
the rate in the oracle inequality will be fast. We start
with the uniform version, non-exact version.

5
Since our loss function is convex one can always find
at least one minimizer.

Theorem 3.3. Let Assumption 3.1 hold and consider
θ̂λ as defined in (6) where λ = sA . Then, for any

Linear statistical estimation

0 < δ < 1, w.p. at least 1 − δ it holds that
h
i
LM (θ̂sA ) ≤ zA,δ · inf LM (θ) + sA (1 + zA,δ )kθk
θ∈Rd

+ sb (1 + zA,δ )zb,δ .
By allowing λ to depend on δ, we get an exact, nonuniform oracle inequality with a fast rate:
Theorem 3.4. Let Assumption 3.1 hold. Fix 0 <
δ < 1 arbitrarily and choose θ̂λ as defined in (6) with
λ = sA zA,δ . Then, w.p. at least 1 − δ it holds that
h
i
LM (θ̂sA zA,δ ) ≤ inf LM (θ) + 2sA zA,δ kθk + 2sb zb,δ .
θ∈Rd

Note that this bound is as tight as if we had first chosen
λ = ∆A and then applied the stochastic assumptions
to obtain a high probability (h.p.) bound.
When the linear system defined by (A, b) is consistent,
LM (θ∗ ) = 0. In this case one may prefer Theorem 3.3
to Theorem 3.4. Indeed, focusing on the behavior at
θ∗ we get from Theorem 3.3 the bound sA zA,δ (1 +
zA,δ )kθ∗ k + sb (1 + zA,δ )zb,δ that holds w.p. 1 − δ for
any value of δ, while from Theorem 3.4 we conclude the
bound 2sA zA,δ0 kθ∗ k + 2sb zb,δ0 , which however, holds
only for δ 0 ≥ δ.
3.2. Minimizing the squared penalized loss
A more “traditional” estimator uses the square of the
empirical loss function:
n
o
θ̂ρ = arg min L̂2M (θ) + ρkθk ,
ρ > 0.
(7)

We now have two parameters that need tuning. However, as we will see, the tuning of these parameters is
very similar to what we have seen in the previous section. The reason for this is that Λ is rich enough to
contain a value ρ that makes L̂M (θ̂ρ )+ρkθ̂ρ k comparable to (not much larger than) L̂M (θ) + λkθk no matter
what θ one selects. This is in fact the key to the proof
of the following lemma, which gives a deterministic
oracle inequality for θ̃λ,c :
Lemma 3.5. Let θ̃λ,c be as in (9). Then,
h
i

LM (θ̃λ,c ) ≤ 1 ∨ ∆λA inf LM (θ) + (∆A + 2λ)kθk
θ∈Rd



+ 2 ∨ 1 + ∆λA ∆b + 1 ∨ ∆λA c.
With the (unattainable) choice λ = ∆A , c = ∆b we get
h
i
LM (θ̃λ,c ) ≤ inf LM (θ) + 3∆A kθk + 3∆b .
θ∈Rd

These choices are impractical but, as it happened with
in the previous section, we can obtain uniform nonexact or non-uniform exact oracle inequalities with fast
rates. The non-exact uniform oracle inequality is formalized as follows:
Theorem 3.6. Let Assumption 3.1 hold and choose
θ̃λ,c be as in (9) with λ = sA and c = sb . Then, for
any 0 < δ < 1 w.p. at least 1 − δ it holds that
h
i
LM (θ̃λ,c ) ≤ {1 ∨ zA,δ } inf LM (θ) + sA (zA,δ + 2)kθk
θ∈Rd

+ {2 ∨ (1 + zA,δ )} sb zb,δ + {1 ∨ zA,δ } sb .

θ∈Rd

To be able to handle Lasso-like procedures, we decided
to avoid squaring the norm of θ. Moreover, not squaring this term is convenient for the proof techniques we
used. The extension of our results for other types of
penalties, in particular kθk2 , is left for future work.
Unlike the previous case where the loss function and
the norm were both unsquared, in this case the selection of the regularization parameter ρ will be more
involved. In practice, one often uses a hold-out estimate to choose the best value of ρ amongst a finite
number of candidates on an exponential grid. Here,
we propose a procedure that avoids splitting the data,
but uses the unsquared penalized loss with the same
data. The new procedure is defined as follows. For
some λ, c > 0 to be chosen later, let
n
o
ρ̂(λ, c) ∈ arg min
L̂M (θ̂ρ ) + λkθ̂ρ k ,
(8)
ρ∈Λ(λ,c)

. 
where Λ(λ, c) = 2k · 2cλ : k ∈ N
.
θ̃λ,c = θ̂ρ̂(λ,c) .

and define
(9)

The next theorem gives a non-uniform, exact oracle
inequality with fast rates.
Theorem 3.7. Let Assumption 3.1 hold. Fix 0 < δ <
1 and choose θ̃λ,c be as in (9) with λ = sA zA,δ and
c = sb zb,δ . Then, w.p. at least 1 − δ it holds that
h
i
LM (θ̃λ,c ) ≤ inf LM (θ) + 3sA zA,δ kθk + 3sb zb,δ .
θ∈Rd

The relative merits of the uniform and non-uniform
oracle inequalities are unchanged compared to what
we have seen in the previous section.

4. Value-estimation in Markov Reward
Processes: Results
Let us now return to value-estimation in Markov Reward Processes. We consider the projected Bellman
error objective, LM (θ) = kAθ − bkM , where M = C −1
(for the definitions see Section 2). Assume that ∆A ,
∆b are concentrated as in Assumption 3.1, with known

Linear statistical estimation

bounds. This can be arranged for example if the features φi (Xt ) and rewards Rt+1 are a.s. bounded, and
if we assume appropriate mixing, such as exponential β-mixing (Yu, 1994), or when the Markov chain
(Xt )t≥0 forgets its past sufficiently rapidly (Samson,
2000). Note that in these cases (Â, b̂) gets concentrated around
rate, i.e.,
p
p (A, b) at the usual parametric
sA , sb = O( 1/n) and zA,δ , zb,δ = O( ln(1/δ)).
For simplicity, assume first that C is given and consider the on-policy case. As mentioned previously, in
this case the system Aθ = b is guaranteed to have a
solution and therefore LM (θ∗ ) = 0. Consider the estimator that minimizes the unsquared penalized loss.
Then, Theorem 3.3 shows a uniform fast rate when
using λ = sA :
h
i
LM (θ̂sA ) ≤ (1 + zA,δ ) sA zA,δ kθ∗ k + sb zb,δ .
We get a similar inequality for the squared penalized
loss using the result Theorem 3.6 with a slightly larger
bound.
In the off-policy case, the linear system Aθ = b may
not have a solution. When it does, the previous bound
applies. However, when this linear system does not
have a solution, to get an exact oracle inequality we
are forced to choose λ (in the case of minimizing the
unsquared penalized loss) based on δ. In particular,
with the choice λ = sA zA,δ , Theorem 3.4 gives
h
i
LM (θ̂sA zA,δ ) ≤ inf LM (θ) + 2sA zA,δ kθk + 2sb zb,δ .
θ∈Rd

(10)
p
Again, this p
inequality gives fast, O( 1/n) rates when
sA , sb = O( 1/n). Similar results hold for the procedure defined for the squared penalized loss where the
bound is given by the inequality of Theorem 3.7.
When C is unknown, one may resort replacing it by
M  0. Then, a non-exact oracle inequality can be
derived using kxk2P ≤ νmax (Q−1/2 P Q−1/2 )kxk2Q . (For
a matrix S, we denote by νmax (S), νmax (S) its largest
and smallest singular values, respectively.) Consider
first the unsquared penalized loss. In this case, kAθ −
1/2
bkC −1 ≤ νmax (M −1/2 C −1 M −1/2 )kAθ − bkM . Assume
thathfor an estimator θ̂ iti holds that kAθ − bkM ≤

inf θ kAθ − bkM + cÂ,b̂ (θ) . Then, from kAθ − bkM ≤
1/2

νmax (C 1/2 M C 1/2 )kAθ − bkC −1 we get
h
i
kAθ̂ − bkC −1 ≤ inf κ1/2 kAθ − bkM + τ −1/2 cÂ,b̂ (θ) .
θ

where κ = νmax (C 1/2 M C 1/2 )/νmin (M 1/2 CM 1/2 ) is
the “conditioning number” of M 1/2 CM 1/2 and τ =

νmin (M 1/2 CM 1/2 ). In the on-policy case, for example, this gives bounds of the form
h
i
LM (θ̂sA ) ≤ τ −1/2 (1 + zA,δ ) sA zA,δ kθ∗ k + sb zb,δ .
The bound for the off-policy case derived from (10)
takes the form
LM (θ̂sA zA,δ ) ≤
h
i
inf κ1/2 LM (θ) + 2τ −1/2 sA zA,δ kθk + 2τ −1/2 sb zb,δ .
θ∈Rd

Similar inequalities can be derived for our procedures
that minimize the squared penalized loss.
Finally, let us discuss the dependence of our bounds
on the choice of the basis functions. This dependence
comes through Assumption 3.1. As an example, assume that φi : X → [−1, 1] and k · k = k · kp with
1 ≤ p ≤ 2. In this case, the bound on ∆A is expected
to scale linearly
√ with d, while ∆b is expected to scale
linearly with d. To see why ∆A is expected to scale
linearly with d note that ∆A ≤ kM 1/2 (Â − A)k2,2 =
kM 1/2 (Â − A)kF , where k · kF denotes the Frobenius
norm. Now, the Frobenius norm is the norm underlying the Hilbert-space of square matrices with the inner product hP, Qi = trace(P > Q) and thus an application of any concentration inequality for Hilbertspace valued random variables (e.g., (Steinwart and
Christmann, 2008)) gives a bound that scales with the
“range” of N = kM 1/2 (φ(Xt ) − γφ(X̃t+1 ))φ(Xt )> kF .
Using the rotation property of trace, we get that
N = kφ(Xt ) − γφ(X̃t+1 )kM kφ(Xt )k. The first term
can be bounded using the triangle inequality as a
function of kφ(Xt )kM and kφ(X̃t+1 )kM . Assuming
(e.g.,) that M is the identity matrix, we get that
both
√ kφ(Xt )kM = kφ(Xt )k and kφ(X̃t+1 )k are of size
O( d). Hence, their product scales linearly with d.
The above bound on ∆A is naive; we believe using
∆A ≤ νmax (Â − A) may yield a tighter dependency on
d. E.g., for d × d-matrices with i.i.d standard
normal
√
entries, the maximum eigenvalue is O( d) (Vershynin,
2010). Furthermore, note that if the basis functions
are correlated, or if they are sparse, the dimension
will not necessarily appear linearly in the bound either. For a discussion of when to expect a milder dependence of the norm of φ on d, the interested reader
may consult the paper by Maillard and Munos (2009).
4.1. Related work
Antos et al. (2008) proved a uniform high-probability
inequality both for the on-policy and the off-policy
cases for LSTD.
takes the form LM (θ̂) −
 Their bound
1 
1 4
∗
LM (θ ) = O d ln(d) n
, which is a slower rate

Linear statistical estimation

than the rate we are able to obtain. Further, with our
bounding method the ln d factor can be removed from
this bound.
There are more results available for the on-policy case.
As mentioned earlier, in this case the system Aθ = b
is consistent and thus our bound, under appropriate
mixing conditions, takes the form
!
r
d
(1 + R) ,
LM (θ̂) = O L
τn
1
1
.
where τ = νmin (M 2 CM 2 ), L is the worst-case norm
.
of features in the dual norm (L = √
supx∈X kφ(x)k∗ ;
as discussed previously, L may be O( d)) and R is a
worst-case bound on the norm of the parameter vector
(i.e., kθ∗ k ≤ R). In the next two results, the norm k · k
is the 2-norm. Lazaric et al. (2010) for their (unregularized) path-wise LSTD method obtain
!
r
d log d
(1 + R)
LM (θ̂) = O L
nτ

(cf. Theorem 3 in their work). Although this is a
fast rate, it also shares the undesirable dependence
on τ1 . Non-uniform, slow rates can be extracted from
the paper by Ghavamzadeh et al. (2010) for LSTD
with random projections. The result with our notation
would look like (cf. Theorem 2)
!
r
  14
1
log
d
LR
LM (θ̂) = O L2
.
R+ √
τ
n
n
More recently, for the so-called Lasso-TD method,
Ghavamzadeh
et al. (2011) showed non-uniform
 1 
1 4
O n
-rates, but only for the so-called in-sample
error, i.e., the empirical norm at the states used by the
algorithm. These rates depend on the `1 -norm of θ∗
and have no dependence on the minimum eigenvalue,
but they are slow in n. At the expense of additional
assumptions on the Gram matrix Ĉ (a sample estimate
of C), they have also derived fast rates.

5. Conclusion and future work
We have shown performance bounds for two estimators in linear inverse problems. Each of these minimizes one of LM (θ) and L2M (θ), plus a penalty λkθk.
The penalty weight λ can be chosen a priori without
the need for a separate validation data set, and the
bounds were presented in a general form that apply
to many different instances of statistical linear inverse
problems, requiring only that ∆A and ∆b concentrate
around zero. Our split analysis, into a deterministic

step and a stochastic step, allows us to decouple the
behavior of ∆A , ∆b from that of the estimators.
We have recovered `1 -penalized variations of LSTD
(Bradtke and Barto, 1996) for value function estimation in MRPs. We have shown fast, uniform rates,
which, in the on-policy case, are exact and competitive
with those existing in the literature. In the off-policy
case, the rates are non-exact, and the non-uniform
bound is also competitive with existing results.
Finally, we would like to point out interesting ways to
further develop our work.
`1 -penalties. The choice when the norm used in
the penalty is the `1 -norm has been extensively studied in the supervised learning literature (see, e.g.,
(Bickel et al., 2009; Koltchinskii, 2011; Bühlmann and
Geer, 2011) and the references therein), as well as
in the reinforcement learning setting (Kolter and Ng,
2009; Ghavamzadeh et al., 2010; 2011; Maillard, 2011),
mainly because it allows for non-trivial performance
bounds even when the dimension d of the parameter
vector is comparable to the sample size n (or even
larger than n) provided that the true parameter vector is sparse (i.e., there are many zeroes in it). In this
paper we decided not to specialize to this case but
rather to focus on the problem of proving fast, exact
and (possibly uniform) oracle inequalities. Our results,
when applied to the case of an `1 -penalty show that in
a way adding an `1 -penalty does not hurt performance
(as we expect that the oracle inequalities with the said
properties should hold for a decent method) even if the
conditions ideal for the `1 -penalty do not hold. We do
not know of performance bounds (ours included) for
`1 -penalized estimation have all of the characteristics
we are after in a bound (viz. bounds that are exact,
fast and uniform).
Linear regression. Our results are also worth investigating in the context of linear regression. It is easy to
cast regression as a statistical linear estimation problem whose underlying system is always consistent. If
we use k · k as the `1 -norm, we recover procedures similar to the square-root Lasso (Belloni et al., 2010) and
the Lasso (Tibshirani, 1996) for the estimators studied
in Sections 3.1 and 3.2, respectively. We believe that
confronting the bounds that can be derived from our
results with bounds for linear regression in the literature can be very instructive.
Connection to Inverse Problems. The theory of
Inverse Problems is very pertinent to this work, and
it is important to study our results under the light of
those shown in Chapter 2 of Kirsch (2011); Alquier
et al. (2011). The existing knowledge of inverse prob-

Linear statistical estimation

lems may help us better understand which choices of
k·k allow ∆A to concentrate around zero, and how fast
this concentration occurs. The idea of having learning problems as inverse problems is not new; Rosasco
(2006); Vito et al. (2006) study regression in Hilbert
spaces as an inverse problem.

Acknowledgements
This work was supported by AITF and NSERC.



In real supervised learning scenarios, it is not
uncommon that the training and test sample follow different probability distributions,
thus rendering the necessity to correct the
sampling bias. Focusing on a particular covariate shift problem, we derive high probability confidence bounds for the kernel mean
matching (KMM) estimator, whose convergence rate turns out to depend on some regularity measure of the regression function and
also on some capacity measure of the kernel. By comparing KMM with the natural
plug-in estimator, we establish the superiority of the former hence provide concrete evidence/understanding to the effectiveness of
KMM under covariate shift.

1. Introduction
In traditional supervised learning, the training and
test sample are usually assumed to be drawn from
the same probability distribution, however, in practice, this assumption can be easily violated for a variety of reasons, for instance, due to the sampling bias
or the nonstationarity of the environment. It is therefore highly desirable to devise algorithms that remain
effective under such distribution shifts.
Needless to say the problem is hopeless if the training
and test distribution share nothing in common. On the
other hand, if the two distributions are indeed related
in a nontrivial manner, then it is a quite remarkable
fact that effective adaptation is possible. Under reasonable assumptions, this problem has been attacked
by researchers from statistics (Heckman, 1979; Shimodaira, 2000) and more recently by many researchers
from machine learning, see for instance, Zadrozny
Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012.
Copyright 2012 by the author(s)/owner(s).

(2004); Huang et al. (2007); Bickel et al. (2009); BenDavid et al. (2007); Blitzer et al. (2008); Cortes et al.
(2008); Sugiyama et al. (2008); Kanamori et al. (2009).
We focus in this paper on the covariate shift assumption which was first formulated by Shimodaira (2000)
and has been followed by many others.
The assumption that the conditional probability distribution of the output variable given the input variable remains fixed in both the training and test set is
termed covariate shift, i.e. the shift happens only for
the marginal probability distributions of the covariates. It is well-known that under this setting, the key
to correct the sampling bias caused by covariate shift
is to estimate the Radon-Nikodym derivative (RND),
also called the importance weight or density ratio. A
number of methods have been proposed to estimate
the RND from finite samples, including kernel mean
matching (KMM) (Huang et al., 2007), logistic regression (Bickel et al., 2009), Kullback-Leibler importance estimation (Sugiyama et al., 2008), least-squares
(Kanamori et al., 2009), and possibly some others.
Despite of the many algorithms, our current understanding of covariate shift still seems to be limited.
From the analyses we are aware of, such as (Gretton
et al., 2009) on the confidence bound of the RND by
KMM, (Kanamori et al., 2012) on the convergence rate
of the least-squares estimate of the RND, and (Cortes
et al., 2008) on the distributional stability, they all
assume that certain functions lie in the reproducing
kernel Hilbert space (RKHS) induced by some user selected kernel. Since this assumption is impossible to
verify (even worse, almost certainly violated in practice), one naturally wonders if we can replace it with
something more reasonable. Such goal is pursued in
this paper and constitutes our main contribution.
We consider the following simple problem: Given the
tr
training sample {(Xitr , Yitr )}ni=1
and the test sample
te nte
{Xi }i=1 , how well can we estimate the expected value
EY te , provided covariate shift has happened? Note
that we do not observe the output Yite on the test
sample. This problem, at a first glance, ought to be

Analysis of Kernel Mean Matching under Covariate Shift

“easy”, after all we are humbly asking for estimating a scalar. Indeed, under usual assumptions, plus
the nearly impossible assumption that the regression
function lies in the RKHS, we prove a parametric rate,
−1
−1
that is O(ntr 2 + nte 2 ), for the KMM estimator in Theorem 1 below (to fix ideas, we focus exclusively on
KMM in this paper). For a more realistic assumption on the regression function that we borrow from
learning theory (Cucker & Zhou, 2007), the convergence rate, proved in Theorem 2, degrades gracefully
−

θ

−

θ

to O(ntr 2(θ+2) +nte 2(θ+2) ), where θ > 0 is a smoothness
parameter measuring certain regularity of the regression function (in terms of the kernel). Observe that in
the limit when θ → ∞, the regression function eventually lies in the RKHS and we recover the previous
parametric rate. In this regard our bound in Theorem 2 is asymptotically optimal. A very nice feature
we discovered for the KMM estimator is that it does
not require knowledge of the smoothness parameter θ,
thus, it is in some sense adaptive.
On the negative side, we show that, if the chosen kernel does not interact very well with the unknown regression function, the convergence rate of the
KMM estimator could be exceedingly slow, roughly
·nte
), where s > 0 again measures certain
O(log−s nntrtr+n
te
regularity of the regression function. This unfortunate
result should draw attention to the importance of selecting which kernel to be used in practice. A thorough
comparison between the KMM estimator and the natural plug-in estimator, conducted in Section 4.3, also
reveals the superiority of the former.
We point out that our results are far from giving a
complete picture even for the simple problem we consider here, for instance, it is unclear to us whether or
not the rate in Theorem 2 can be improved, eventually,
to the parametric rate in Theorem 1? Nevertheless, we
hope that our paper will convince others about the importance and possibility to work with more reasonable
assumptions under covariate shift, and as an example,
suggest relevant tools which can be used to achieve
that goal.

2. Preliminaries
In this section we formally state the covariate shift
problem under our consideration, followed by some relevant discussions.
2.1. Problem Setup
Consider the familiar supervised learning setting,
where we are given independent and identically disntr
tributed (i.i.d.) training samples {(Xitr , Yitr )}i=1
from

the joint (Borel) probability measure Ptr (dx, dy) on
the (topological) domain X × Y, and i.i.d. test samte
from the joint probability measure
ples {Xite }ni=1
Pte (dx, dy) on the same domain. Notice that we do
not observe the output Yite on the test sample, and
more importantly, we do not necessarily assume that
the training and test sample are drawn from the same
probability measure. The problem we consider in this
paper is to estimate the expected value EY te from the
tr
and the test sample
training sample {(Xitr , Yitr )}ni=1
te nte
{Xi }i=1 . In particular, we would like to determine
how fast, say, the 1 − δ confidence interval for our estimate shrinks to 0 when the sample sizes ntr and nte
increase to infinity.
This problem, in its full generality, cannot be solved
simply because the training probability measure can
be completely irrelevant to the test probability measure that we are interested in. However, if the two
probability measures are indeed related in a nontrivial way, our problem becomes solvable. One particular
example, which we focus on hereafter, is known in the
literature as covariate shift (Shimodaira, 2000):
Assumption 1 (Covariate shift assumption)
Ptr (dy|x) = Pte (dy|x).

(1)

We use the same notation for the joint, conditional and
marginal probability measures, which should cause no
confusion as the arguments would reveal which measure is being referred to. Note that the equality
P(dx, dy) = P(dy|x)·P(dx) holds from the definition of
the conditional probability measure, whose existence
can be confirmed under very mild assumptions.
Under the covariate shift assumption, the difficulty of
our problem, of course, lies entirely on the potential
mismatch between the marginal probability measures
Ptr (dx) and Pte (dx). But the Bayes rule already suggests a straightforward approach:
Pte (dx, dy) = Pte (dy|x)·Pte (dx) = Ptr (dx, dy)·

dPte
(x),
dPtr

where the three quantities on the right-hand side can
all be estimated from the given samples. However, in
order for the above equation to make sense, we need
Assumption 2 (Continuity assumption) The
te
Radon-Nikodym derivative β(x) := dP
dPtr (x) is welldefined and bounded from above by B < ∞.
Note
that B ≥ 1 due to the normalization constraint
R
β(x)P
tr (dx) = 1. The Radon-Nikodym derivative
X
(RND) is also called the importance weight or the
density ratio in the literature. Evidently, if β(x) is

Analysis of Kernel Mean Matching under Covariate Shift

not well-defined, i.e., there exists some measurable
set A such that Pte (A) > 0 and Ptr (A) = 0, then
in general we cannot infer Pte (dx, dy) from merely
Ptr (dx), Pte (dx) and Ptr (dy|x), even under the covariate shift assumption. The bounded from above assumption is more artificial. Recently, in a different
setting, (Cortes et al., 2010) managed to replace this
assumption with a bounded second moment assumption, at the expense of sacrificing the rate a bit. For us,
since the domain X will be assumed to be compact, the
bounded from above assumption is not too restrictive
(automatically holds when β(x) is, say, continuous).
Once we have the RND β(x), it becomes easy to correct the sampling bias caused by the mismatch between Ptr (dx) and Pte (dx), hence solving our problem.
Formally, let
Z
m(x) :=
y Pte (dy|x)
(2)
Y

be the regression function, then
Z
Z
EY te =
m(x) Pte (dx) =
m(x)β(x) Ptr (dx).
X

X

By the i.i.d. assumption, P
a reasonable estimator for
ntr
β(Xitr ) · Yitr . Hence,
EY te would then be n1tr i=1
similarly to most publications on covariate shift, our
problem boils down to estimating the RND β(x).

KMM tries to match the mean elements in a feature
space induced by a kernel k(·, ·) on the domain X × X :
)

(

ntr
nte
1 X
1 X
min L̂(β̂) :=
β̂i Φ(Xitr ) −
Φ(Xite )
ntr i=1
nte i=1
β̂i

s.t. 0 ≤ β̂i ≤ B,

H

(3)

where Φ : X 7→ H denotes the canonical feature map,
H is the reproducing kernel Hilbert space1 (RKHS) induced by the kernel k and k · kH stands for the norm in
H. To simplify later analysis, we have chosen to omit
Pntr
β̂i − 1 ≤ ,
the normalization constraint n1tr i=1
where  is a small positive number, mainly to reflect the fluctuation caused by random samples. It
is not hard to verify that (3) is in fact an instance
of quadratic programming, hence can be efficiently
solved. More details can be found in the paper of
Gretton et al. (2009).
A finite sample 1−δ confidence bound for L̂(β) (similar
as (10) below) is established in Gretton et al. (2009).
This bound is further transferred into a confidence
bound for the generalization error of some family of
loss minimization algorithms in Cortes et al. (2008),
under the notion of distributional stability. However,
neither results can provide a direct answer to our problem: a finite sample confidence bound on the estimate
of EY te .

2.2. A Naive Estimator?
2.4. Plug-in Estimator
An immediate solution for estimating β(x) is to estimate the two marginal measures from the training sample {Xitr } and the test sample {Xite }, respectively. For instance, if we know a third (Borel) measure
Q(dx) (usually the Lebesgue measure on Rd ) such that
dPtr
te
both dP
dQ (x) and dQ (x) exist, we can employ standard density estimators to estimate them and then set
dPtr
te
β̂(x) = dP
dQ (x)/ dQ (x). However, this naive approach
is known to be inferior since density estimation in high
dimensions is hard, and moreover, small estimation ertr
ror in dP
dQ (x) could change β̂(x) significantly. To our
knowledge, there is little theoretical analysis on this
seemingly naive approach.
2.3. A Better Estimator?
It seems more appealing to directly estimate the RND
β(x). Indeed, a large body of work has been devoted to this line of research (Zadrozny, 2004; Huang
et al., 2007; Sugiyama et al., 2008; Cortes et al., 2008;
Bickel et al., 2009; Kanamori et al., 2009). From the
many references, we single out the kernel mean matching (KMM) algorithm, first proposed by Huang et al.
(2007) and is also the basis of this paper.

Another natural approach is to estimate the regression
function from the training sample and then plug into
the test set. We postpone the discussion and comparison with respect to this estimator until section 4.3.

3. Motivation
We motivate the relevance of our problem in this section.
Suppose we have an ensemble of classifiers, say,
{fj }N
all trained on the training sample
j=1 ,
tr
{(Xitr , Yitr )}ni=1
.
A useful task is to compare,
hence rank, the classifiers by their generalization
errors. This is usually done by assessing the classifiers
te
on some hold out test sample {(Xite , Yite )}ni=1
. It is
not uncommon that the test sample is drawn from
some different probability measure than the training
sample, i.e. covariate shift has happened. Since it
could be too costly to re-train the classifiers when the
test sample is available, we nevertheless still like to
1
A thorough background on the theory of reproducing
kernels can be found in Aronszajn (1950).

Analysis of Kernel Mean Matching under Covariate Shift

where recall that m(x) is the regression function defined in (2) and β(x) is the true RND.

have a principled way to rank the classifiers.
Let `(·, ·) be the user’s favourite loss function, and set
tr
te
Zij
= `(fj (Xitr ), Yitr ), Zij
= `(fj (Xite ), Yite ), then we
te nte
}i=1 to estimate
can use the empirical average of {Zij
te
the generalization error, that is E(Zij
), of classifier
fj . But what if we do not have access to Yite hence
te
consequently Zij
? Can we still accomplish the ranking
job?

The equality in (5) indeed holds under at least two conditions (respectively). First, if the regression function
m ∈ H, then taking inner products with m in (4) and
applying the reproducing property we get (5). Second,
if the kernel k is characteristicR(Sriperumbudur et al.,
2010), meaning that the map X Φ(x)P(dx) from the
space of probability measures to the RKHS H is injective, then we conclude β̂ ∗ = β from (4) hence follows
(5).

The answer is yes, and it is precisely the covariate
shift problem under our consideration. To see that,
tr ntr
te
. Under
consider the pair {Xitr , Zij
}i=1 and {Xite }ni=1
the covariate shift assumption, that is Ptr (dy|x) =
Pte (dy|x), it is not hard to see that Ptr (dz|x) =
Pte (dz|x), hence the covariate shift assumption holds
for the ranking problem, therefore the confidence
bounds derived in the next section provide an effective solution.

The above two cases suggest the possibility of solving our problem by KMM. Of course, in reality one
only has finite samples from the underlying probability
measures, thus calls for a thorough study of the empirical KMM, i.e. (3). Interestingly, our analysis reveals
that in the first case above, we indeed can have a parametric rate while in the second case the rate becomes
nonparametric, hence inferior (but does not seem to
rely on the characteristic property of the kernel).

We do not report numerical experiments in this paper
for two reasons: 1). Our main interest is on theoretical
analysis; 2). Exhaustive experimental results on KMM
can already be found in Gretton et al. (2009).

4.2. The empirical version
In this subsection we analyze KMM in details. The
following assumption will be needed:

4. Theoretical Analysis
This section contains our main contribution, i.e., a
theoretical analysis of the KMM estimator for EY te .

Assumption 3 (Compactness assumption) X is
a compact metrizable space, Y ⊆ [0, 1], and the kernel k is continuous, whence kkk∞ ≤ C 2 < ∞.

4.1. The population version
Let us first take a look at the population version of
KMM2 , which is much easier to analyze and provides
valuable insights:
Z
Φ(x)β̂(x)Ptr (dx) −
Φ(x)Pte (dx)
β̂
X
X
Z
s.t. 0 ≤ β̂ ≤ B,
β̂(x)Ptr (dx) = 1.

β̂ ∗ ∈ arg min

Z

X

The minimum value is 0 since the true RND β(x) is
apparently feasible, hence at optimum we always have
Z
Z
Φ(x)β̂ ∗ (x)Ptr (dx) =
Φ(x)Pte (dx).
(4)
X

X

The
question is whether the natural estimator
R
β̂ ∗ (x)y Ptr (dx, dy) is consistent? In other words,
X ×Y
is
Z
Z
?
m(x)β̂ ∗ (x)Ptr (dx) = EY te = m(x)β(x)Ptr (dx),
X

X

(5)
2
All Hilbert space valued integrals in this paper are to
be understood as the Bochner integral (Yosida, 1980).

H

We use k·k∞ for the supremum norm. Under the above
assumption, the feature map Φ is continuous hence
measurable (with respect to the Borel σ-fields), and
the RKHS is separable, therefore the Bochner integrals
in the previous subsection are well-defined. Moreover,
the conditional probability measure indeed exists under our assumption.
We are now ready to deriveP
a finite sample confidence
ntr
β̂i Yitr − EY te |, where
bound for our estimate | n1tr i=1
β̂i is a minimizer of (3). We start by splitting the sum:
ntr
1 X
β̂i Yitr − EY te
ntr i=1

=

ntr
1 X
β̂i (Yitr − m(Xitr ))
ntr i=1

+

ntr
1 X
(β̂i − βi )(m(Xitr ) − h(Xitr ))
ntr i=1

+

ntr
1 X
(β̂i − βi )h(Xitr )
ntr i=1

ntr
1 X
βi m(Xitr ) − EY te ,
+
ntr i=1

(6)

where βi := β(Xitr ) and h ∈ H is to be specified later.

Analysis of Kernel Mean Matching under Covariate Shift

We bound each term individually. For the last term
in (6), we can apply Hoeffding’s inequality (Hoeffding,
1963) to conclude that with probability at least 1 − δ,
ntr
1 X
βi m(Xitr ) − EY te ≤ B
ntr i=1

r

1
2
log .
2ntr
δ

(7)

The first term in (6) can be bounded similarly. Conditioned on {Xitr } and {Xite }, we apply again Hoeffding’s inequality. Note that β̂i (Yitr − m(Xitr )) ∈
[−β̂i m(Xitr ), β̂i (1 − m(Xitr ))], therefore its range is of
size β̂i . With probability at least 1 − δ,
v
r
u
ntr
ntr
u 1 X
1 X
1
tr
tr
2
t
β̂i (Yi − m(Xi )) ≤
β̂i ·
log
ntr i=1
ntr i=1
2ntr
r
1
2
(8)
≤B
log .
2ntr
δ
The second and third terms in (6) require more work.
Consider first the third term:

expectation, and then bound the expectation straightforwardly. In general, Pinelis’s inequality will lead to
(slightly) tighter bounds due to its known optimality
(in certain sense).
Finally, we come to the second term left in (6), which
is roughly the approximation error in learning theory (Cucker & Zhou, 2007). Note that all confidence
bounds we p
have derived so far shrink at the parametric rate O( 1/ntr + 1/nte ). However, from here on
we will have to tolerate nonparametric rates. Since
we are going to apply different approximation error
bounds to the second term in (6), it seems more convenient to collect the results separately. We start with
2 an encouraging result:
δ
Theorem 1 Under Assumptions 1-3, if the regression
function m ∈ H (the RKHS induced by the kernel k),
then with probability at least3 1 − δ,
s 

ntr
B2
1
6
1 X
tr
te
β̂i Yi − EY
≤M· 2
+
log ,
ntr i=1
ntr
nte
δ

ntr
ntr
1 X
1 X
(β̂i − βi )h(Xitr ) =
(β̂i − βi )hh, Φ(Xitr )i
ntr i=1
ntr i=1

≤ khkH ·

ntr
1 X
(β̂i − βi )Φ(Xitr )
ntr i=1

H

≤ khkH · [L̂(β̂) + L̂(β1:ntr )]
≤ khkH · 2L̂(β1:ntr ),

(9)

where β1:ntr denotes the restriction of β to the training
sample {Xitr }, L̂(·) is defined in (3), and the equality
is because h ∈ H (and the reproducing property of
the canonical feature map), the first inequality is by
the Cauchy-Schwarz inequality, the second inequality
is due to the triangle inequality, and the last inequality
is by the optimality of β̂ and the feasibility of β1:ntr in
problem (3). Next, we bound L̂(β1:ntr ):
ntr
nte
1 X
1 X
βi Φ(Xitr ) −
Φ(Xite )
ntr i=1
nte i=1
H
s 

B2
1
2
≤C 2
+
log
(10)
ntr
nte
δ

L̂(β1:ntr ) :=

with probability at least 1 − δ, where the inequality
follows from the Hilbert space valued Hoeffding inequality in (Pinelis, 1994, Theorem 3.5). Note that
Pinelis proved his inequality for martingales in any
2-smooth separable Banach space (Hilbert spaces are
bona fide 2-smooth). We remark that another way, see
for instance (Gretton et al., 2009, Lemma 1.5), is to
use McDiarmid’s inequality to bound L̂(β1:ntr ) by its

where M := 1+2CkmkH and β̂i is computed from (3).
Proof: By assumption, setting h = m zeros out the
second term in (6). A standard union bound combining (7)-(10) completes the proof (and we simplified the
bound by slightly worsening the constant).
The confidence bound shrinks at the parametric rate,
although the constant depends on kmkH , which in general is not computable, but can be estimated from the
training sample {(Xitr , Yitr )} at a rate worse than parametric. Since this estimate inevitably introduces other
uncomputable quantities, we omit the relevant discussion. On the other hand, our bound suggests that if
a priori information about m is indeed available, one
should choose a kernel that minimizes its induced norm
on m.
The case when m 6∈ H is less satisfactory, despite of its
practicality. We point out that a denseness argument
cannot resolve this difficulty. To be more precise, let
us assume for a moment m ∈ C (X ) (the space of continuous functions on X ) and k be a universal kernel
(Steinwart, 2002), meaning that the RKHS induced by
k is dense in (C (X ), k · k∞ ). By the assumed universal property of the kernel, there exists suitable h ∈ H
that makes the second term in (6) arbitrarily small (in
fact, can be made vanishing), however, on the other
hand, recall that the bound (9) on the third term in
(6) depends on khkH hence could blow up. If we trade
3
Throughout this paper, the confidence parameter δ is
always taken arbitrarily in (0, 1).

Analysis of Kernel Mean Matching under Covariate Shift

off the two terms appropriately, we might get a rate
that is acceptable (but worse than parametric). The
next theorem concretizes this idea.

since 0 ≤ m ≤ 1 by Assumption 3. The quantity A2 (m, R) is called the approximation error in
learning theory and its polynomial decay is known
θ

Theorem 2 Under Assumptions 1-3, if A2 (m, R) :=
inf km − gkLP2 ≤ C2 R−θ/2 for some θ > 0 and
kgkH ≤R

tr

constant C2 ≥ 0, then with probability at least 1 − δ,
ntr
1 X
≤
β̂i Yitr − EY te
ntr i=1
r
θ
2
9
8
B
log + Cθ (BC2 ) θ+2 D2θ+2 ,
2ntr
δ
r 

q
2
where D2 := 2C 2 nBtr + n1te log 8δ +BC 2n1tr log 8δ ,
 2
Cθ := (1 + 2/θ) θ2 θ+2 and β̂i is computed from (3).

Proof: By the triangle inequality,
ntr
1 X
(β̂i − βi )(m(Xitr ) − h(Xitr ))
ntr i=1

≤B·

ntr
1 X
|m(Xitr ) − h(Xitr )|.
ntr i=1

to be (almost) equivalent to m ∈ Range(Tk2θ+4 ),
see for instance Theorem 4.1 of Cucker & Zhou
operator (Tk f )(x0 ) =
R(2007).0 Here Tk is the integral
2
k(x , x)f (x)Ptr (dx) on LPtr . The smoothness paX
rameter θ > 0 measures the regularity of the regression function, and as it increases, the range space of
θ

Tk2θ+4 becomes smaller, hence our decay assumption
on A2 (m, R) becomes more stringent. Note that the
θ
is necessarily smaller than 1/2 (but apexponent 2θ+4
proaches 1/2 when θ → ∞) because by Mercer’s theo1

rem Tk2 is onto H (in which case the range assumption
would bring us back to Theorem 1).
Theorem 2 shows that the confidence boundθ now
−
shrinks at a slower rate, roughly O(ntr 2(θ+2) +
−

θ

nte 2(θ+2) ), which, as θ → ∞, approaches the paramet−1

−1

ric rate O(ntr 2 + nte 2 ) derived in Theorem 1 where
we assume m ∈ H. We point out that the source of
this slower rate comes from the irregular nature of the
regression function (in the eye of the kernel k).

The polynomial decay assumption on A2 (m, R) is not
always satisfied, for instance, it is shown in Theorem
Not surprisingly, we apply yet again Hoeffding’s in6.2 of Cucker & Zhou (2007) that for C ∞ (indefinite
equality to relate the last term above to its expectatimes differentiable) kernels (such as the popular Gaustion. Since
sian kernel), polynomial decay implies that the regression function m ∈ C ∞ (X ) (under mild assumptions on
km − hk∞ ≤ 1 + k hh, Φ(·)i k∞ ≤ 1 + CkhkH ,
X and Ptr (dx)). Therefore, as long as one works with
we have with probability at least 1 − δ,
smooth kernels but nonsmooth regression functions,
the approximation error has to decay logarithmically
r
ntr
1 X
1
2
slowly. We give a logarithmic bound for such cases.
tr
tr
|m(Xi )−h(Xi )| ≤ (1+CR)
log +A2 (m, R),
ntr i=1
2ntr
δ
Theorem 3 Under Assumptions 1-3, if A∞ (m, R) :=
inf km − gk∞ ≤ C∞ (log R)−s for some s > 0 and
where R := khkH . Combining this bound with (7)kgkH ≤R
(10) and applying our assumption on A2 (m, R):
constant C∞ ≥ 0 (assuming R ≥ 1), then (for ntr and
ntr
1 X
(β̂i − βi )(m(Xitr ) − h(Xitr ))
ntr i=1
s 
r

2
8
B2
1
8
≤B
log + 2RC 2
+
log
ntr
δ
ntr
nte
δ
r
1
8
+ BC2 R−θ/2 + B(1 + CR)
log .
2ntr
δ

Setting R =



θBC2
2D2

2
 θ+2

nte larger than some constant),

s

−s
ntr
1 X
1
sBC∞
tr
te
BC∞ log
β̂i Yi − EY
≤ 1+
ntr i=1
s
D∞
r
1
s
2
6
s+1
+B
log + (sBC∞ ) s+1 D∞
ntr
δ
holds
probability at least 1 − δ, where D∞ =
r with


B2
2C 2 ntr + n1te log 6δ and β̂i is computed from (3).

completes the proof.

In Theorem 2 we do not even assume m ∈ C (X ); all
we need is m ∈ LP2tr , the space of Ptr (dx) square integrable functions. The latter condition always holds

The proof is similar as
that of Theorem 2 except that
s
∞ s+1
we set R = ( sBC
)
.
D∞
Theorem 3 shows that in such unfavourable cases,
the confidence bound shrinks at an exceedingly slow

Analysis of Kernel Mean Matching under Covariate Shift
·nte
rate, roughly, O(log−s nntrtr+n
). The reason, of course,
te
is due to the slow decay of the approximation error
A∞ (m, R). It is proved in Theorem 6.1 of Cucker &
Zhou (2007) that for the Gaussian kernel k(x0 , x) =
exp(−kx − x0 k22 /σ 2 ), if X ⊆ Rd has smooth boundary and the regression function m ∈ H s (X ) with index s > d/2, then the logarithmic decay assumed in
Theorem 3 holds. Here H s (X ) is the Sobolev space
(the completion of C ∞ (X ) under the inner product
R P
α
α
hf, gis := X |α|≤s ddxf ddxg , assuming s ∈ N). Similar bounds also hold for the inverse multiquadrics kernel k(x0 , x) = (c2 + kx − x0 k22 )−α with α > 0. We
remark that in this regard Theorem 3 disrespects the
popular Gaussian kernel used ubiquitously in practice
and should draw the attention of researchers.

4.3. Discussion
It seems worthwhile to devote a subsection to discussing a very natural question that the reader might
already have: why not estimate the regression function
m on the training set and then plug into the test set,
after all m does not change under the covariate shift
assumption? Algorithmically, this is perfectly doable,
perhaps conceptually even simpler since the algorithm
does not need to see the test data beforehand. We note
that estimating the regression function from i.i.d. samples has been well studied in the learning theory literature, see for instance, Chapter 8 of Cucker & Zhou
(2007) and the many references therein.
The difficulty, though, lies in the appropriate error
metric on the estimate. Recall that when estimating
the regression function from i.i.d. training samples,
one usually measures the progress (i.e. the discrepancy between the estimate m̂ and m) by the L 2 norm
under the training probability measure Ptr (dx), while
what we really want is a confidence bound on the term
nte
1 X
m̂(Xite ) − EY te .
nte i=1

(11)

Since Ptr 6= Pte , there is evidently a probability measure mismatch between the bound we have from estimating m and the true interested quantity. Indeed,
conditioned on the training sample {(Xitr , Yitr )}, using
the triangle inequality we can bound (11) by :
Z
nte
1 X
te
m̂(Xi ) − m̂(x)Pte (dx) + km̂ − mkLP2 .
te
nte i=1
The first term above can be bounded again through
Hoeffding’s inequality, while the second term is
close to what we usually have from estimating m: the only difference being that the L 2

norm is now under the test probability measure
Pte (dx). Fortunately, since the norm of the identity
map id : ([−1, 1]X , k · kLP2 ) 7→ ([−1, 1]X , k · kLP2 ) is
te
tr
√
bounded by B (see Assumption 2), we can deduce
a bound for (11) based upon results from estimating
m, though less appealingly, a much looser bound than
the one given in Theorem 2. We record such a result
for the purpose of comparison:
Theorem 4 Under Assumptions 1-3, if the regression
θ

function m ∈ Range(Tk2θ+4 ) for some θ > 0, then with
probability at least 1 − δ,
nte
1 X
m̂(Yite ) − EY te ≤
nte i=1

r

4 √
1
− 3θ
log + BC1 ntr 12θ+16 ,
2nte
δ

where C1 is some constant that does not depend on
ntr , nte , and m̂ is the (regularized least-squares) estimate of m in Smale & Zhou (2007).
The theorem follows from the bound on km̂ − mkLP2
tr
in Corollary 3.2 of Sun & Wu (2009), which is an improvement over Smale & Zhou (2007).
Carefully comparing the current theorem with Theorem 2, we observe: 1). Theorem 4, which is based
on the regularized least-squares estimate of the regression function, needs to know in advance the parameter
θ (in order to tune the regularization constant) while
Theorem 2, derived for KMM, does not require any
such information, hence in some sense KMM is “adaptive”; 2). Theorem 4 has much worse dependence on
the training sample size ntr ; it does not recover the
parametric rate even when the smoothness parameter
−1/4
−1/2
θ goes to ∞ (we get ntr , instead of ntr ). On the
other hand, Theorem 4 has better dependence on the
test sample size nte , which is, however, probably not so
important since usually one has much more test samples than training samples because the lack of labels
make the former much easier to acquire; 3). Theorem
4 seems to have better dependence on the parameter
B; 4). Given the fact that KMM utilizes both the
training data and the test data in the learning phase,
it is not entirely a surprise that KMM wins in terms of
convergence rate, nevertheless, we find it quite stunning that by sacrificing the rate slightly on nte , KMM
is able to improve the rate on ntr so significantly.

5. Conclusion
For estimating the expected value of the output on
the test set where covariate shift has happened, we
have derived high probability confidence bounds for
the kernel mean matching (KMM) estimator, which

Analysis of Kernel Mean Matching under Covariate Shift
−1

−1

converges, roughly O(ntr 2 + nte 2 ) when the regression function lies in the RKHS, and more generally
−

θ

−

Heckman, James J. Sample selection bias as a specification error. Econometrica, 47(1):153–161, 1979.

θ

O(ntr 2(θ+2) + nte 2(θ+2) ) when the regression function
exhibits certain regularity measured by θ. An ex·nte
), is also
tremely slow rate, roughly O(log−s nntrtr+n
te
provided, calling attention of choosing the right kernel. From the comparison of the bounds, KMM proves
to be much more superior than the plug-in estimator hence provides concrete evidence/understanding to
the effectiveness of KMM under covariate shift.

Hoeffding, Wassily. Probability inequalities for sums of
bounded random variables. Journal of the American
Statistical Association, 58(301):13–30, 1963.

Although it is unclear to us if it is possible to avoid
approximating the regression function, we suspect the
bound in Theorem 2 is in some sense optimal and we
are currently investigating it. We also plan to generalize our results to the least-squares estimation problem.

Kanamori, Takafumi, Hido, Shohei, and Sugiyama,
Masashi. A least-squares approach to direct importance estimation. JMLR, 10:1391–1445, 2009.

Acknowledgements

Huang, Jiayuan, Smola, Alexander J., Gretton,
Arthur, Borgwardt, Karsten M., and Schölkopf,
Bernhard. Correcting sample selection bias by unlabeled data. In NIPS, pp. 601–608. 2007.

Kanamori, Takafumi, Suzuki, Taiji, and Sugiyama,
Masashi. Statistical analysis of kernel-based leastsquares density-ratio estimation. Machine Learning,
86:335–367, 2012.

This work was supported by Alberta Innovates Technology Futures and NSERC.

Pinelis, Iosif. Optimum bounds for the distributions of
martingales in Banach spaces. The Annals of Probability, 22(4):1679–1706, 1994.



In this paper we propose a novel gradient algorithm to learn a policy from an expert’s
observed behavior assuming that the expert
behaves optimally with respect to some unknown reward function of a Markovian Decision Problem. The algorithm’s aim is to
find a reward function such that the resulting
optimal policy matches well the expert’s observed behavior. The main difficulty is that
the mapping from the parameters to policies is both nonsmooth and highly redundant. Resorting to subdifferentials solves the
first difficulty, while the second one is overcome by computing natural gradients. We
tested the proposed method in two artificial
domains and found it to be more reliable and
efficient than some previous methods.

1

INTRODUCTION

The aim of apprenticeship learning is to estimate a
policy of an expert based on samples of the expert’s
behavior. This problem has been studied in the field of
robotics for a long time and due to the lack of space we
cannot give an overview of the literature. The interested reader might find a short overview in the paper
by Abbeel and Ng (2004).
In apprenticeship learning (a.k.a. imitation learning)
one can distinguish between direct and indirect approaches. Direct methods attempt to learn the policy (as a mapping from states, or features describing
states to actions) by resorting to a supervised learning
method. They do this by optimizing some loss function that measures the deviation between the expert’s
∗

Computer and Automation Research Institute of the
Hungarian Academy of Sciences, Kende u. 13-17, Budapest 1111, Hungary

Csaba Szepesvári∗
Department of Computing Science
University of Alberta
Edmonton T6G 2E8, AB, Canada

policy and the policy chosen. The main problem then
is that in parts of the state space that the expert tends
to avoid the samples are sparse and hence these methods may have difficulties with learning a good policy
at such places.
In an indirect method it is assumed that the expert
is acting optimally in the environment. In particular,
in inverse reinforcement learning the environment is
modelled as a Markovian decision problem (MDP) (Ng
and Russell, 2000). The dynamics of the environment
is assumed to be known (or it could be learnt from
samples which might even be unrelated to the samples
come from the expert). However, the reward function
that the expert is using is unknown. Recently Abbeel
and Ng (2004) gave an algorithm which was proven to
produce a policy which performs almost as well as the
expert, even though it is not guaranteed to recover the
expert’s reward function (recovering the reward function is an ill-posed problem). This approach might
work with less data since it makes use of the knowledge of model of the environment, which can help it
in generalizing to the less frequently visited parts of
the state space. One problem is that the algorithm of
Abbeel and Ng (2004) relies on the precise knowledge
of the features describing the reward function, which
is not a realistic assumption (for a discussion of this,
see Section 6). In particular, we will show that even
the correct scales of the features have to be known.
In this paper we propose a gradient algorithm that
combines the two approaches by minimizing a loss
function that penalizes deviations from the expert’s
policy like in supervised learning, but the policy is obtained by tuning a reward function and solving the
resulting MDP, instead of finding the parameters of a
policy. We will demonstrate that this combination can
unify the advantages of the two approaches in that it
can be both sample efficient and work even when the
features are just vaguely known.

296

NEU & SZEPESVÁRI

2

BACKGROUND

Let us first introduce some notation: For a subset
S of some topological space, S ◦ will be used to denote its
For a finite dimensional vector x,
Pinterior.
d
kxk = i=1 x2i shall denote its `2 -norm. Random variables will be denoted by capital letters (e.g., X,A), E [·]
stands for expectations.
We assume that the reader is familiar with basic concepts underlying Markovian decision processes
(MDPs) (e.g., Puterman 1994), hence we introduce
these concepts only to fix the notation. A finite, discounted infinite-horizon total reward MDP is defined
by a 5-tuple M = (X , A, γ, P, r), where
X is a finite set of states,
A is a finite set of actions,
γ ∈ [0, 1) is the discount factor,
P gives the transition probabilities; P (x0 |x, a) stands
for the probability of the transition from state x
to x0 upon taking action a (x, x0 ∈ X , a ∈ A),
r is the reward function; r : X × A → R; r(x, a)
gives the reward incurred when action a ∈ A is
executed from state x ∈ X .
A stationary stochastic policy (in short:
P policy) is a
mapping π : A × X → [0, 1] satisfying a∈A π(a|x) =
1, ∀x ∈ X .1 The value of π(a|x) is the probability of
taking action a in state x. A policy is called deterministic if for any x, π(·|x) is concentrated on a single
action. The class of all stationary stochastic policies
will be denoted by Π.
For a fixed policy, the value of a state x ∈ X is defined
by
"∞
#
¯
X
¯
γ t r(Xt , At )¯¯ X0 = x ,
(1)
V π (x) = E
t=0

where (Xt , At )t≥0 is the sequence of random stateaction pairs generated by executing the policy π. The
function V π : X → R is called the value function underlying policy π.
We will also need action-value functions. The actionvalue function, Qπ : X × A → R, underlying policy π
is defined by
"∞
#
¯
X
¯
π
t
Q (x, a) = E
γ r(Xt , At )¯¯ X0 = x, A0 = a (2)
t=0
1

Instead of π(a, x) we use π(a|x) to emphasize that
π(·, x) is a probability distribution. Note that in finite
MDPs one can always find optimal (stochastic) stationary
policies (Puterman, 1994).

with the understanding that for t > 0, At ∼ π(·|Xt ).
A policy that maximizes the expected total discounted
reward over all states is called an optimal policy.
The optimal value function is defined by V ∗ (x) =
supπ V π (x), while the optimal action-value function is
defined by Q∗ (x, a) = supπ Qπ (x, a).
It turns out that V ∗ and Q∗ satisfy the so-called Bellman optimality equations (e.g., Puterman 1994). In
particular,
X
Q∗ (x, a) = r(x, a) + γ
P (y|x, a) max Q∗ (x, b). (3)
y∈X

b∈A

P
We call a policy that satisfies a∈A π(a|x)Q(x, a) =
maxa∈A Q(x, a) at all states x ∈ X greedy w.r.t. the
function Q. It is known that all policies that are greedy
w.r.t. Q∗ are optimal and all stationary optimal policies can be obtained these way.

3

APPRENTICESHIP LEARNING

Assume that we observe a sequence of state-action
pairs (Xt , At )0≤t≤T , the ‘trace’ of some expert. We
assume that the expert selects the actions by some
unknown policy πE : At ∼ πE (·|Xt ). The goal is to recover πE from the observed trace. The simplest solution is of course to use a supervised learning approach:
we select a parametric class of policies, (πθ )θ , πθ ∈ Π,
θ ∈ Rd , and try to tune the parameters so as to minimize some loss JT (πθ ), such as
X
JT (π) =
µ̂T (x)(π(a|x) − π̂E,T (a|x))2 , (4)
x∈X ,a∈A

where µ̂T (x) could be defined by µ̂T (x) = 1/(T +
PT
1) t=0 I{Xt =x} are the empirical occupation frequencies under the expert’s policy and π̂E,T (a|x) =
PT
PT
t=0 I{Xt =x} is the empirical est=0 I{Xt =x,At =a} /
timate of the expert’s policy.2 It is easy to see that JT
approximates the squared loss
X
J(π) =
µE (x)(π(a|x) − πE (a|x))2 (5)
x∈X ,a∈A

uniformly in π (the usual concentration results hold
for JT , e.g. Györfi et al. (2002)).
The reason π̂E,T is not used directly as a ‘solution’ is
that if the state space is large then it will be undefined for a large number of states (where µ̂E,T (x) = 0)
with high probability unless the number of samples is
enormous.
An alternative to direct policy learning is inverse reinforcement learning (Ng and Russell, 2000). The idea
2
If a state is not visited by the expert, the policy is
defined arbitrarily.

NEU & SZEPESVÁRI
is that given the expert’s trace, we find a reward function that can be used to explain the performance of
the expert. More precisely, the problem is to find a
reward function that the behavior of the expert is optimal for. Once the reward function is found, existing
algorithms are used to find a behavior that is optimal
with respect to it.
One difficulty in IRL is that solutions are non-unique:
e.g. if r is a reward function that recovers the expert’s
policy then for any λ ≥ 0, λr is also a solution (r = 0
is always a solution). For non-trivial problems there
are many solutions besides the variants that differ in
their scale only.
We propose here to unify the advantages of the direct and indirect approaches by (i) taking it seriously
that we would like to recover the expert’s policy and
(ii) achieve this through IRL so that we can achieve
good generalization at parts of the state space avoided
by the expert. We thus propose to find the parameters given a parametric family of rewards (rθ )θ ∈ Θ
such that the corresponding (near) optimal policy, πθ ,
matches the expert’s policy πE (more precisely, it’s
empirical estimate). The proposed method can be
written succinctly as the optimization problem
θ

J(πθ ) → min! s.t.

πθ = G(Q∗θ ),

(6)

where J is a loss function (such as (5) or (4)) aimed at
measuring the distance of πE and its argument, Q∗θ is
the optimal action-value function corresponding to the
reward function rθ and G is a suitable smooth mapping
that returns (near) greedy policies with respect to its
argument. One possibility, utilized in our experiments,
is to use Boltzmann action-selection policies (see (7)).3
In this paper we consider gradient methods to solve
the above optimization problem. One difficulty with
such an approach is that there could be many parameterizations that yield to the same loss. This will be
helped with the method of natural gradients, for which
the theory is worked out in the next section.
Another difficulty is that the mapping θ 7→ Q∗θ is nondifferentiable.We will, however, show that it is Lipschitz when rθ is Lipschitz and hence, by Rademacher’s
theorem it is differentiable almost everywhere (w.r.t.
the Lebesgue measure).
3.1

NATURAL GRADIENTS

Our ultimate goal is to find some parameters θ in a
parameter space Θ ⊂ Rd such that the policy πθ determined by θ matches the expert’s policy πE . For

297

facilitating the discussion let us denote the map from
the parameter space Θ to the policy space by h (i.e.,
h(θ) = πθ ). Thus, our objective function can be writ˜ = J(h(θ)), where J : Π → R is a (differenten as J(θ)
tiable) objective function defined over Π (such as (5))
˜ Incremental gradient
and the goal is to minimize J.
methods implement θt+1 = θt − αt gt , where αt ≥ 0
is an appropriate step-size sequence and gt = g(θ)
points in the direction of steepest ascent on the surface
˜
(θ, J(θ))
θ.
The gradient method with an infinitesimal step-size
gives rise to a trajectory (θ(t))t≥0 . This in turn determines a trajectory (π(t))t≥0 in the policy space, where
π(t) = h(θ(t)). Since our primary interest is the trajectory in the policy state, it makes sense to determine
the gradient direction g in each step such that π(t)
moves in the steepest descent direction on the surface
of (π, J(π))π . We call g = g(θ) the natural gradient if
this holds. Amari (1998) gives a method to find the
natural gradients using the formalism of Riemannian
spaces.
The advantage of this procedure is that the resulting
trajectories will be the same for any equivalent parameterization (i.e., if the parameter space is replaced
by some other space that is related to the first one
through a smooth invertible mapping, with a smooth
inverse). In addition, the gradient algorithm that uses
natural gradients can be proven to be asymptotically
efficient in a probabilistic sense and has the tendency
to alleviate the problem of ‘plateaus’ (Amari, 1998).
In order to define natural gradients we need some definitions. First, we need the generalization of derivatives for mappings f between Banach spaces.4 The
underlying idea is that the gradient (derivative) of
f : U → V provides a linear approximation to the
change f (u + h) − f (u):
Definition 1 (Fréchet derivative). Let U, V be Banach
spaces. A is the Fréchet-derivative of f at u if A :
U → V is a bounded linear operator and kf (u + h) −
f (u) − AhkV = o(khkU ). The mapping f then is called
Fréchet differentiable at u.
In what follows we view Π both as a vector space and
a complete metric space with some metric d. In our
application this metirc will be derived from the (unweighted) `2 -norm, but other choices would also work.
The following definition suggests a geometry induced
on Θ:
Definition 2 (Induced metric). Let Θ ⊂ Rd , θ ∈ Θ◦ .
We say that Gθ ∈ Rd×d is a pseudo-metric induced by

3

The benefit of choosing strictly stochastic policies is
that if the expert’s policy is deterministic, they force the
uniqueness of the solution.

4
A Banach space is a complete normed vector space. In
our case it will usually be a Euclidean space, e.g. Rd .

298

NEU & SZEPESVÁRI

(h, Π, d) at θ if Gθ is positive semidefinite and
d(h(θ + ∆), h(θ)) = ∆T Gθ ∆ + o(k∆k2 ).
The essence of this definition is that if the ‘distance’
between θ and θ + ∆ is given by ∆T Gθ ∆ then this
distance will match the distance of h(θ) and h(θ + ∆),
as k∆k → 0. It follows from the definition that the
induced pseudo-metric is unique.
In the rest of the paper we assume that Π is finite
dimensional to make the presentation of the results
easier. The following proposition is an immediate consequence of the definition of induced pseudo-metrics
and the definition of Fréchet differentiability:
Proposition 1. Assume that h : Θ → Π is Fréchet
differentiable at θ ∈ Θ◦ , Θ ⊂ Rd , Π = (Π, d) is a
complete, linear metric space. Then h0 (θ)T h0 (θ) is the
pseudo-metric induced by (h, Π, d) at θ.
Natural gradients can be obtained by the follow˜ +
ing procedure: Let g(θ; ε) = argmax∆∈S̃(θ,ε) J(θ
˜
∆) − J(θ)
be the direction of steepest ascent
over the ‘warped sphere’ S̃(θ, ε) = {∆ ∈
Rd | kh(θ + ∆) − h(θ)k = ε}.5 Then the set of natural gradients is given by
˜ (h) J(θ)
˜ def
∇
= lim inf 1ε g(θ; ε).
ε→0+

Here the limes inferior of the sets (g(θ; ε))ε>0 is meant
in the sense of the Painlevé-Kuratowski convergence
(Kuratowski, 1966): It then holds that no matter how
˜ (h) J(θ)
˜
ε converges to zero, g ∈ ∇
defines a direction
of steepest ascent on the surface of J at h(θ).
The following theorem holds:
Theorem 1. Let J : Π → R, h : Θ → Π, J˜ = J ◦
h. Assume that J is Fréchet differentiable and locally
Lipschitz and h : Θ → Π is Fréchet differentiable at
θ ∈ Θ◦ . Let Gθ = h0 (θ)T h0 (θ) be the pseudo-metric
˜
˜ ∈∇
˜ (h) J(θ),
at θ induced by (h, Π, d). Then G†θ ∇J(θ)
˜
˜
where ∇J(θ) is the ordinary gradient of J at θ and G†θ
denotes the Moore-Penrose generalized inverse of Gθ .
For the sake of specificity, when it does not cause con˜ the natural gradient of J˜ at θ.
fusion, we call G†θ ∇J(θ)
Note that from the construction it follows immediately
˜ are covariant for
that the trajectories of θ̇ = G†θ ∇J(θ)
any initial condition.
The proof borrows some ideas from the proof of Theorem 1 in (Amari, 1998). In order to spare some
space we only give an outline here: The basic idea
is to replace the warped sphere S̃(θ, ε) by the ‘sphere’
5

Note that g(θ; ε) is set-valued.

SGθ (θ, ε) = {∆ ∈ Rd | ∆T Gθ ∆ = ε2 }. This is justified since the ‘sphere’ SGθ (θ, ε) becomes arbitrarily
close to S̃(θ, ε) as ε → 0 and J˜ is sufficiently regular. The next step is to show that for some C > 0,
˜
CεG†θ ∇J(θ)
is a solution of the optimization problem argmax∆∈SG (θ,ε) J˜0 (θ)∆, and this solution tracks
θ
˜ + ∆) − J(θ)
˜ when
closely that of argmax∆∈SG (θ,ε) J(θ
θ
ε → 0.

4

CALCULATING THE GRADIENT

In order to calculate the natural gradient we need to
calculate the (Fréchet) derivative of h(θ) = G(Q∗θ ) and
the gradient of J(h(θ)).6 By the chain rule we obtain ∇J(h(θ)) = J 0 (h(θ))h0 (θ). Since calculating the
derivative of J (or JT ) is trivial, we are left with calculating the derivative of h(θ). As suggested previously,
we use a smooth mapping G. One specific proposal,
that we actually used in the experiments assigns Boltzmann policies to the action-value functions:
G(Q)(a|x) = P

exp[βQ(x, a)]
,
b∈A exp[βQ(x, b)]

(7)

where β > 0 is a parameter that controls how close
G(Q) is to a greedy action selection. With this choice
∂πθ
∂ ln[πθ (a|x)]
(a|x) = πθ (a|x)
∂θk
∂θk
Ã
!
∗
∂Q∗θ (x, b)
∂Qθ (x, a) X
= πθ (a|x)β
−
πθ (b|x)
.
∂θk
∂θk
b∈A

(8)
Hence, we are left with calculating ∂Q∗θ (x, a)/∂θk . We
will show that these derivatives can be calculated almost everywhere on Θ by solving some fixed-point
equations similar to the Bellman-optimality equations.
For this, we will need the concept of subdifferentials
and some basic facts:
Definition 3 (Fréchet Subdifferentials). Let U be a
Banach space, U ∗ be its topological dual.7 The Fréchet
subdifferential of f : U → R at u ∈ U , denoted by
∂ − f (u) is the set of u∗ ∈ U ∗ such that
lim inf khk−1 [f (u + h) − f (u) − hu∗ , ui] ≥ 0.

h→0,h6=0

The following elementary properties follow immediately from the definition (e.g., Kruger 2003):
Proposition 2. Let (fi )i∈I be a family of real-valued
functions defined over U and let f (u) = maxi∈I fi (u).
6

Remember that G maps action-value functions to policies and J measures deviations to the expert’s policy.
7
When U = Rd with the `2 -norm then U ∗ = Rd and for
u ∈ U, v ∗ ∈ U ∗ , hv ∗ , vi is the normal inner product.

NEU & SZEPESVÁRI
Then if u∗ ∈ ∂ − fi (u) and fi (u) = f (u) then u∗ ∈
∂ − f (u). If f1 , f2 : U → R, α1 , α2 ≥ 0 then α1 ∂ − f1 +
α2 ∂ − f2 ⊂ ∂ − (α1 f1 + α2 f2 ).
The next result states some conditions under which,
in a generalized sense, ‘taking a derivative and a limit
is interchangeable’. It is extracted from the proof of
Proposition 3.4 of Penot (1995):
Proposition 3. Assume that (fn )n is a sequence of
real-valued functions over U which converge to some
function f pointwise. Let u ∈ U , u∗n ∈ ∂ − fn (u)
and assume that (u∗n ) is weak∗ -convergent to u∗ and
is bounded. Further, assume that the following holds
at u: For any ε > 0, there exists some index N > 0
and a real number δ > 0 such that for any n ≥ N ,
h ∈ BU (0, δ),
fn (u + h) ≥ fn (u) +

hu∗n , hi

− εkhk.

Then u∗ ∈ ∂ − f (u).
Now, we state the main result of this section:
Proposition 4. Assume that the reward function rθ is
differentiable w.r.t. θ with uniformly bounded derivatives: sup(θ,x,a)∈Rd ×X ×A krθ0 (x, a)k < +∞. The following statements hold:
(1) Q∗θ is uniformly Lipschitz-continuous as a function of θ in the sense that for any (x, a) pair,
θ, θ0 ∈ Rd , |Q∗θ (x, a) − Q∗θ0 (x, a)| ≤ L0 kθ − θ0 k with
some L0 > 0;
(2) Except on a set of measure zero, the gradient,
∇θ Q∗θ , is given by the solution of the following
fixed-point equation:
ϕθ (x, a) = (rθ0 (x, a))T
P
P
+γ y∈X P (y|x, a) b∈A π(b|y)ϕθ (y, b), (9)
where π is any policy that is greedy with respect
to Qθ .
Note that (rθ0 (x, a))T ∈ Rd . In fact, the above equation
can be solved componentwise: The kth component of
the derivative can be obtained computing the action0
value function for the policy π using rθ,k
in place of
8
the reward function.
Proof. Let T : RX ×A → RX ×A be the Bellman operator
X
(T Q)(x, a) = rθ (x, a) + γ
P (y|x, a) max Q(y, b).
y∈X

b∈A

8
0
Here rθ,k
is the kth component of the derivative of the
reward function with respect to θ. We also note in passing
that if rθ is convex in θ then so is Qθ . This follows with
the reasoning followed in the proof of the first part.

299

By elementary arguments, if Q is L-Lipschitz in θ, then
T Q is R + γL-Lipschitz in θ, where R is such that for
any θ, θ0 ∈ Rd , (x, a) ∈ X × A, |rθ (x, a) − rθ0 (x, a)| ≤
Rkθ − θ0 k. Choose Q0 = 0. As is well known (e.g.,
Puterman (1994)), Qn = T n Q0 converges to Q∗ : Q∗θ =
limn→∞ T n Q0 . Hence, by the previous argument Q∗
is R + γR + γ 2 R + . . . = R/(1 − γ)-Lipschitz, proving
the first part of the statement.
For the second part, for a policy π, let us define the
operator Sπ , acting over the space of functions φ :
X × A → Rd , by
(Sπ φ)(x, a) = (rθ0 (x, a))T
P
P
+γ y∈X P (y|x, a) b∈A π(b|y)φ(y, b).
Let π denote a greedy policy w.r.t. Q∗θ and let πn
be a sequence of policies that are
Pgreedy w.r.t. Qn
and where ties are broken so that x∈X ,a∈A |π(a|x) −
πn (a|x)| is minimized. It follows that for n large
enough, πn = π. Now, consider the sequence ϕ0 = 0,
ϕn+1 = Sπn ϕn . Then for n large enough we have
ϕn+1 = Sπ ϕn . By induction, ϕn (x, a) ∈ ∂θ− Qn (x, a)
holds for any n ≥ 0. Indeed, this clearly holds for
n = 0, while the general case follows by Proposition 2.
Now, observe that Sπ acts separately on each of the d
components of its argument and when it is restricted
to any of these components, it is a contraction. Hence,
ϕn converges to the fixed point of Sπ , i.e., the solution
of (9). By Proposition 3 the limit is a subdifferential of
limn→∞ Qn = Q∗θ (that the condition of this proposition is satisfied follows from the uniform convergence of
ϕn in θ, which follows since krθ0 k is uniformly bounded
in both θ and (x, a)). Now, since by the first part Q∗θ
is Lipschitz-continuous in θ, by Rademacher’s theorem
it is differentiable almost everywhere. It is well-known
that if a function is differentiable then its subderivative
coincides with its derivative (see e.g. Kruger (2003)).
This finishes the proof of the statement.

5

COMPUTER EXPERIMENTS

The goal of the experiments was to assess the efficiency
of the algorithm and to test its robustness. We were
also interested in how it compares with the algorithm
of Abbeel and Ng (2004).
We have implemented three versions of our algorithm:
(i) gradient descent using plain gradients, (ii) gradient descent using natural gradients (iii) RPROP using plain gradients.9 RPROP is a popular adaptive
step-size selection algorithm that proved to be very
competitive in a number of settings Riedmiller and
9

We tried a “natural RPROP” variant as well (RPROP
using natural gradients), but perhaps suprisingly, it give
much poorer results than the other algorithms.

NEU & SZEPESVÁRI

Braun (1993). We have implemented the variant described in Igel and Hüsken (2000). We also implemented the “max margin” and the “projection” algorithms described in Abbeel and Ng (2004) to be able
to compare the different approaches. Results will be
shown for “max margin”. The projection algorithm is
computationally more efficient, but we have found it
less reliable and less data efficient.

−1

10

−2

10
J(πθ)

300

Natural grad.

We decided to use two test environments: The familiar
grid world that has also been used by Abbeel and Ng
(2004) and the sailing problem due Vanderbei (1996).
The reward function was linear in the unknown parameters.
5.1

−3

10

RPROP
−4

10

Plain grad.
Max margin
2

4

10
10
Number of training samples

GRID WORLD

We have run the first series of experiments in grid
worlds, where each state is a grid square and the four
actions correspond to moves in the four compass directions with 70% success. We constructed the reward function as a linear combination of 5 features
(φi : X → R, i = 1, . . . , 5), where the features were essentially randomly constructed. The optimal parameter vector θ∗ consists of evenly distributed random
values from [−1, 1]. In general we try to approximate
the reward function with the use of the same set of
features that has been used to construct it, but we
also examine the situation of unprecisely known features. The size10 of the grid worlds was set to 10 × 10.
Value iteration was used for finding the optimal policy
(or gradients) in all cases. Unless otherwise stated the
data consists of 10 independent trajectories following
the optimal policy, each having a length of 100 steps.
The learning rate was hand-tuned (with a little effort)
and the number of iterations is kept at 100 (usually,
convergence happens much earlier). In all cases, the
performance measure is the error function JE , defined
by (5) and we measure the performance of the optimal policy computed for the found reward function.
For the “max margin” algorithm we show the performance of the overall best policy found during the first
100 iterations, thus optimistically biasing these measurements.
We examined the algorithms’ behavior when (i) the
number of the training samples was varied (Figure 1),
(ii) the features were linearly transformed (Figure 2,
Table 1, row 2), and when (iii) the features were perturbed (Table 1, row 3).
We see from Figure 1 that for small sample sizes plain
gradient is doing the best, while eventually natural
gradient becomes the winner. Note that the scale on
the y axis is logarithmic, so the differences between
10
Preliminary experiments confirm that our conclusions
would not change significantly for other sizes.

Figure 1: Performance as a function of the number of
training samples. Each curve is an average of 10 runs
using different samples, with 1/10 s.e. error bars.
these algorithms is not big. “Max margin” also catches
up at the end, just like RPROP.
Figure 2 shows the effect of transforming the features
linearly (the true reward function still remains in the
span of the features). Clearly, “Max Marging” suffers badly, while the natural gradient algorithm and
RPROP are little affected. Plain gradient descent is
slowed down, but eventually converges to good solutions.
In practice, it is not realistic to assume that a subspace containing the reward function is known. To
test how the algorithms behave without this assumption we perturbed the features by adding uniform
[− max(φi )/2, max(φi )/2] random numbers to them.
Results are shown in row 3 of Table 1. The results indicate the robustness of natural gradients and RPROP.
Both plain gradients and “max margin” suffer large
losses under these adverse conditions.
5.2

SAILING

We also applied the algorithms to the problem of “sailing” proposed by Vanderbei (1996). In this problem
the task is to navigate a boat from one point to another
in the shortest possible time. Thus, this is a stochastic shortest path (SSP) problem. Formally, we have
a grid of waypoints connected by legs, at each waypoint the sailor has to select one of these eight legs to
move on to the next waypoint. The state space in this
setting is constructed from the actual situation of the
boat and the direction from where the wind is blowing
at the specific moment. The eight actions of selecting
the next waypoint have different costs depending on
the direction of the wind: e.g. it costs more time to

NEU & SZEPESVÁRI

Original
Transformed
Perturbed

Natural
Mean
0.0051
0
0.0163

gradients
Deviation
0.0010
0
0.0165

RPROP
Mean
0.0130
0.0110
0.0197

301

Plain gradients
Mean
Deviation
0.0011 0.0068
0.0256 0.0237
0.1377 0.3428

Deviation
0.0134
0.0076
0.0179

Max margin
Mean
Deviation
0.0473 0.1476
0.0702 0.0228
0.2473 0.3007

Table 1: Means and deviations of errors. The row marked ’original’ gives results for the original features, the
row marked ‘transformed’ gives results when features are linearly transformed, the row marked ‘perturbed’ gives
results when they are perturbed by some noise.

1.4
0.25

1.2

Natural gradients
Natural gradients

0.8

Rprop
0.6

Plain gradients

0.4

Error rate

J(πθ)

1

Max margin

0.15
0.1

Maximum margin

0.2
0

0.2

0.05
20

40
60
Number of iterations

0

80

10

1

10

2

3

4

10
10
10
Number of training episodes

Figure 2: Performance with linearly transformed features. The features were transformed by a (nonsingular) square matrix with uniform [0, 1] random elements. Each curve is an average of 25 runs with different scalings of the features, the 1/10 s.e. error bars
are also plotted.

Figure 3: Performance as a function of the number
of training episodes. The fraction of states where the
found policy differs from the actual optimal policy is
plotted against the number of episodes observed., measured by the mean of 5 runs. The 1/2-s.e. error bars
are also plotted for both methods.

sail 45 degrees against the wind than to sail 45 degrees
in the wind direction etc.. We assume that the wind
changes follow a Markov process. The reward function
is given using a linear combination of the six features of
(away, down, cross, up, into, delay), as defined in Vanderbei (1996) (all defined as a map φ : X × A → R).
The following weighting was used in the experiments:
θ∗ = (−1, −2, −3, −4, −100000, −3)T .

again that the gradient method outperforms the “max
margin” algorithm by a significant amount.

Results as a function of the number of episodes is
shown in Figure 3 for natural gradients and the “max
margin” algorithm. In this case the number of iterations is set to 1000 and we again computed the optimal
policy with the reward found by the algorithm. As a
more tangible performance measure in this case, we
show the number of states where the actions selected
by the found policy differ from the ones selected by
the policy followed by the expert. The results here are
shown fro a small lake of size 4×4.11 The conclusion is
11
Our preliminary experiments show that the new algorithm performs reasonably for larger problems, too.

6

RELATED WORK

Our main concern in this section is the algorithm of
Abbeel and Ng (2004). This algorithm returns policies that come with the guarantee that their average
total discounted reward computed from the expert’s
unknown reward function is in the ε-vicinity of the expert’s performance. We claim that this guarantee will
be met only when the scaling of the features in the
method and the ‘true’ scaling match each other. Actually, this observation led us to the algorithms proposed
here.
In order to explain why the algorithm of Abbeel and
Ng (2004) is sensitive to scalings, we need some background on the algorithm. A crucial assumption in this
algorithm is that the reward function is linearly parameterized, i.e., r(x) = θ∗T φ(x), where φ : X → Rd

302

NEU & SZEPESVÁRI

and θ∗ ∈ Rd is the vector of unknown parameters. It
follows that the expected total discounted reward is
θT φE , where φE ∈ Rd is the so-called feature expectation underlying the expert. From the trajectory of
the expert this can be estimated. In fact, we can define φπ for any policy π and express the expected total
discounted reward as θT φπ . The main idea of Abbeel
and Ng (2004) is then that it suffices to find a policy π whose feature expectations φπ matches φE since
|θ∗T φπ − θ∗T φE | ≤ kθ∗ k2 kφπ − φE k2 .
However, a major underlying hidden assumption (implicit in the formalism of Abbeel and Ng (2004)) is that
the scaling of the features is√known. To see this assume
that d = 2, θ∗,1 = θ∗,2 = 2/2, kφπ − φE k2 ≤ ε and
in particular φE,1 = 0, φE,2 > 0, φπ,1 = −ε, φπ,2 = 0.
Further, assume that the features are rescaled by
λ = (λ1 , λ2 ). In the
scale the expert’s perfor√ new
T
φ
mance is √
ρE (λ) = 2/2λ
√ E and π’s performance is
ρπ (λ) = 2/2λT φπ = 2/2(λT φE − λ1 ε). A natural requirement is that for any scaling λ, ρπ (λ)/ρE (λ)
should be lower bound by a positive number (or rather
a number close to 1 − ε). By straightforward calculations, ρπ (λ)/ρE (λ) = 1−(λ1 /λ2 )ε/φE,2 → −∞, hence
although kφπ − φE k2 ≤ ε, the actual performance of π
can be quite far from the performance of the expert if
the scaling of the features does not match the scaling
used in the algorithm.
More recently, Ratliff et al. (2006) have proposed an algorithm which uses similar ideas to the ones of Abbeel
and Ng (2004). Just like Abbeel and Ng (2004) they
measure performance with respect to the original reward function and not by the difference of the expert’s
policy and the policy returned.

7

CONCLUSIONS

In the paper we have argued for the advantages of unifying the direct and indirect approaches to apprenticeship learning. The proposed procedure attempts to optimize a cost function, yet it chooses the policy based
on a model and thus may overcome problems usually
associated with method that directly try to match the
expert’s policy. Although our method has shown stable behaviour in our experiments, more work is needed
to fully explore the limitations of the method. One
significant barrier for applying the method (as well as
other methods based on IRL) is that it needs to solve
MDPs many times. This is problematic since solving
an MDP is a challenging problem on its own. One
idea is to turn to two time-scale algorithms that run
two incremental procedures in parallel, exploiting that
a small change to the parameters would likely cause
small changes in the solutions; as confirmed by our
theoretical results. There are many important direc-

tions to continue this work: The present work assumed
that states are observed. This could be replaced by the
assumption that sufficiently rich features are observed,
however, when this is not satisfied the method won’t
work. For large state-spaces one needs to use function
approximation techniques to carry out the computations. It is an open question if the methods would
generalize to such settings. Another important direction is to consider infinite MDPs. This presents some
technical difficulties, but we expect that the methods
could still be generalized to such settings. Yet another interesting direction is to replace the parametric
framework with a non-parametric one.
Acknowledgements
Csaba Szepesvári greatly acknowledges the support received through the Alberta Ingenuity Center for Machine Learning (AICML). This work was supported in
part by the PASCAL pump priming project “Sequential Forecasting and Partial Feedback: Applications to
Machine Learning”. This publication only reflects the
authors’ views.



We present a new anytime algorithm that
achieves near-optimal regret for any instance
of finite stochastic partial monitoring. In particular, the new algorithm achieves the minimax regret, within logarithmic factors, for
both “easy” and “hard” problems. For easy
problems, it additionally achieves logarithmic
individual regret. Most importantly, the algorithm is adaptive in the sense that if the
opponent strategy is in an “easy region” of
the strategy space then the regret grows as if
the problem was easy. As an implication, we
show that under some reasonable additional
√
assumptions, the algorithm enjoys an O( T )
regret in Dynamic Pricing, proven to be hard
by Bartók et al. (2011).

1. Introduction
Partial monitoring can be cast as a sequential game
played by a learner and an opponent. In every time
step, the learner chooses an action and simultaneously
the opponent chooses an outcome. Then, based on the
action and the outcome, the learner suffers some loss
and receives some feedback. Neither the outcome nor
the loss are revealed to the learner. Thus, a partialmonitoring game with N actions and M outcomes is
defined with the pair G = (L, H), where L ∈ RN ×M
is the loss matrix, and H ∈ ΣN ×M is the feedback
matrix over some arbitrary set of symbols Σ. These
matrices are announced to both the learner and the
opponent before the game starts. At time step t, if
It ∈ N = {1, 2, . . . , N } and Jt ∈ M = {1, 2, . . . , M }
denote the (possibly random) choices of the learner
and the opponent, respectively then, the loss suffered
by the learner in that time step is L[It , Jt ], while the
Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012.
Copyright 2012 by the author(s)/owner(s).

feedback received is H[It , Jt ].
The goal of the learner (or player) is to minimize his
PT
cumulative loss t=1 L[It , Jt ]. The performance of the
learner is measured in terms of the regret, defined as
the excess cumulative loss he suffers compared to that
of the best fixed action in hindsight:
RT =

T
X
t=1

L[It , Jt ] − min
i∈N

T
X

L[i, Jt ] .

t=1

The regret usually grows with the time horizon T .
What distinguishes between a “successful” and an “unsuccessful” learner is the growth rate of the regret. A
regret linear in T means that the learner does not approach the performance of the optimal action. On the
other hand, if the growth rate is sublinear, it is said
that the learner can learn the game.
In this paper we restrict our attention to stochastic
games, adding the extra assumption that the opponent generates the outcomes with a sequence of independent and identically distributed random variables.
This distribution will be called the opponent strategy.
As for the player, a player strategy (or algorithm) is
a (possibly random) function from the set of feedback
sequences (observation histories) to the set of actions.
In stochastic games, we use a slightly different notion
of regret: we compare the cumulative loss with that of
the action with the lowest expected loss.
RT =

T
X
t=1

L[It , Jt ] − T min E[L[i, J1 ]] .
i∈N

The “hardness” of a game is defined in terms of the
minimax expected regret (or minimax regret for short):
RT (G) = min max E[RT ] ,
A p∈∆M

where ∆M is the space of opponent strategies, and
A is any strategy of the player. In other words, the

Adaptive Stochastic Partial Monitoring

minimax regret is the worst-case expected regret of the
best algorithm.
A question of major importance is how the minimax
regret scales with the parameters of the game, such as
the time horizon T , the number of actions N , the number of outcomes M . In the stochastic setting, another
measure of “hardness” is worth studying, namely the
individual or problem-dependent regret, defined as the
expected regret given a fixed opponent strategy.
1.1. Related work
Two special cases of partial monitoring have been
extensively studied for a long time: full-information
games, where the feedback carries enough information for the learner to infer the outcome for any
action-outcome pair, and bandit games, where the
learner receives the loss of the chosen action as feedback. Since Vovk (1990) and Littlestone & Warmuth
(1994) we know that for full-information
games, the
√
minimax regret scales as Θ( T log N ). For bandit
games,
√ the minimax regret has been proven to scale as
Θ( N T ) (Audibert & Bubeck, 2009).1 The individual regret of these kind of games has also been studied:
Auer et al. (2002) showed that given any opponent
strategy,
P the expected regret can be upper bounded
by c i∈N :δi 6=0 δ1i log T , where δi is the expected difference between the loss of action i and an optimal
action.
Finite partial monitoring problems were introduced by
Piccolboni & Schindelhauer (2001). They proved that
a game is either “hopeless” (that is, its minimax regret scales linearly with T ), or the regret can be upper
bounded by O(T 3/4 ). They also give a characterization of hopeless games. Namely, a game is hopeless
if it does not satisfy the global observability condition
(see Definition 5 in Section 2). Their upper bound
for non-hopeless games was tightened to O(T 2/3 ) by
Cesa-Bianchi et al. (2006), who also showed that there
exists a game with a matching lower bound.
Cesa-Bianchi et al. (2006) posted the problem of characterizing partial-monitoring games with minimax regret less than Θ(T 2/3 ). This problem has been solved
since then. The first steps towards classifying partialmonitoring games were made by Bartók et al. (2010),
who characterized almost all games with two outcomes.
They proved that there are only
√ four categories: games
e T ), Θ(T 2/3 ), and Θ(T ),
with minimax regret 0, Θ(
and named them trivial, easy, hard, and hopeless, re1

The Exp3 algorithm due to Auer et al. (2003) achieves
almost the same regret, with an extra logarithmic term.

spectively.2 They also found that there exist games
that are easy, but can not easily be “transformed” to
a bandit or full-information game. Later, Bartók et al.
(2011) proved the same results for finite stochastic partial monitoring, with any finite number of outcomes.
The condition that separates easy games from hard
games is the local observability condition (see Definition 6). The algorithm Balaton introduced there
works by eliminating actions that are thought to be
suboptimal with high confidence. They conjectured in
their paper that the same classification holds for nonstochastic games, without changing the condition. Recently, Foster & Rakhlin (2011) designed the algorithm
NeighborhoodWatch that proves this conjecture to
be true. Foster & Rakhlin prove an upper bound on a
stronger notion of regret, called internal regret.
1.2. Contributions
In this paper, we extend the results of Bartók et al.
(2011). We introduce a new algorithm, called CBP
for “Confidence Bound Partial monitoring”, with various desirable properties. First of all, while Balaton
only works on easy games, CBP can be run on any
non-hopeless game, and it achieves (up to logarithmic
factors) the minimax regret rates both for easy and
hard games (see Corollaries 3 and 2). Furthermore, it
also achieves logarithmic problem-dependent regret for
easy games (see Corollary 1). It is also an “anytime”
algorithm, meaning that it does not have to know the
time horizon, nor does it have to use the doubling trick,
to achieve the desired performance.
The final, and potentially most impactful, aspect of
our algorithm is that through additional assumptions
on the set of opponent strategies, the minimax regret
√
e T )!
of even hard games can be brought down to Θ(
While this statement may seem to contradict the result of Bartók et al. (2011), in fact it does not. For the
precise statement, see Theorem 2. We call this property “adaptiveness” to emphasize that the algorithm
does not even have to know that the set of opponent
strategies is restricted.

2. Definitions and notations
Recall from the introduction that an instance of partial
monitoring with N actions and M outcomes is defined
by the pair of matrices L ∈ RN ×M and H ∈ ΣN ×M ,
where Σ is an arbitrary set of symbols. In each round
t, the opponent chooses an outcome Jt ∈ M and simultaneously the learner chooses an action It ∈ N . Then,
2

Note that these results do not concern the growth rate
in terms of other parameters (like N ).

Adaptive Stochastic Partial Monitoring

the feedback H[It , Jt ] is revealed and the learner suffers the loss L[It , Jt ]. It is important to note that the
loss is not revealed to the learner.

+
Note that the neighborhood action set Ni,j
naturally
+
contains i and j. If Ni,j contains some other action k
then either Ck = Ci , Ck = Cj , or Ck = Ci ∩ Cj .

As it was previously mentioned, in this paper we deal
with stochastic opponents only. In this case, the choice
of the opponent is governed by a sequence J1 , J2 , . . .
of i.i.d. random variables. The distribution of these
variables p ∈ ∆M is called an opponent strategy, where
∆M , also called the probability simplex, is the set of
all distributions over the M outcomes. It is easy to
see that, given opponent strategy p, the expected loss
of action i can be expressed as `>
i p, where `i is defined
as the column vector consisting of the ith row of L.

In general, the elements of the feedback matrix H can
be arbitrary symbols. Nevertheless, the nature of the
symbols themselves does not matter in terms of the
structure of the game. What determines the feedback
structure of a game is the occurrence of identical symbols in each row of H. To “standardize” the feedback
structure, the signal matrix is defined for each action:

The following definitions, taken from Bartók et al.
(2011), are essential for understanding how the structure of L and H determines the “hardness” of a game.
Action i is called optimal under strategy p if its expected loss is not greater than that of any other ac>
tion i0 ∈ N . That is, `>
i p ≤ `i0 p. Determining which
action is optimal under opponent strategies yields the
cell decomposition 3 of the probability simplex ∆M :
Definition
1
(Cell
decomposition). For
every action i
∈
N , let Ci
=
{p
∈
∆M
:
action i is optimal under p}.
The sets
C1 , . . . , CN constitute the cell decomposition of ∆M .
Now we can define the following important properties
of actions:
Definition 2 (Properties of actions).
• Action i is
called dominated if Ci = ∅. If an action is not
dominated then it is called non-dominated.
• Action i is called degenerate if it is nondominated and there exists an action i0 such that
Ci ( Ci0 .
• If an action is neither dominated nor degenerate then it is called Pareto-optimal. The set of
Pareto-optimal actions is denoted by P.
From the definition of cells we see that a cell is either
empty or it is a closed polytope. Furthermore, Paretooptimal actions have (M − 1)-dimensional cells. The
following definition, important for our algorithm, also
uses the dimensionality of polytopes:
Definition 3 (Neighbors). Two Pareto-optimal actions i and j are neighbors if Ci ∩ Cj is an (M − 2)dimensional polytope. Let N be the set of unordered
pairs over N that contains neighboring action-pairs.
The neighborhood action set of two neighboring ac+
tions i, j is defined as Ni,j
= {k ∈ N : Ci ∩ Cj ⊆ Ck }.
3
The concept of cell decomposition also appears in Piccolboni & Schindelhauer (2001).

Definition 4. Let si be the number of distinct symbols in the ith row of H and let σ1 , . . . , σsi ∈ Σ
be an enumeration of those symbols. Then the signal matrix Si ∈ {0, 1}si ×M of action i is defined as
Si [k, l] = I{H[i,l]=σk } .
The idea of this definition is that if p ∈ ∆M is the opponent’s strategy then Si p gives the distribution over
the symbols underlying action i. In fact, it is also true
that observing H[It , Jt ] is equivalent to observing the
vector SIt eJt , where ek is the k th unit vector in the
standard basis of RM . From now on we assume without loss of generality that the learner’s observation at
time step t is the random vector Yt = SIt eJt . Note
that the dimensionality of this vector depends on the
action chosen by the learner, namely Yt ∈ RsIt .
The following two definitions play a key role in classifying partial-monitoring games based on their difficulty.
Definition 5 (Global observability (Piccolboni &
Schindelhauer, 2001)). A partial-monitoring game
(L, H) admits the global observability condition, if for
all pairs i, j of actions, `i − `j ∈ ⊕k∈N Im Sk> .
Definition 6 (Local observability (Bartók et al.,
2011)). A pair of neighboring actions i, j is said to
be locally observable if `i − `j ∈ ⊕k∈N + Im Sk> . We
i,j
denote by L ⊂ N the set of locally observable pairs of
actions (the pairs are unordered). A game satisfies the
local observability condition if every pair of neighboring actions is locally observable, i.e., if L = N .
The main result of Bartók et al.
√ (2011) is that loe T ) minimax regret.
cally observable games have O(
It is easy to see that local observability implies global
observability. Also, from Piccolboni & Schindelhauer
(2001) we know that if global observability does not
hold then the game has linear minimax regret. From
now on, we only deal with games that admit the global
observability condition.
A collection of the concepts and symbols introduced
in this section is shown in Table 1.

Adaptive Stochastic Partial Monitoring
Table 1. List of basic symbols

Symbol
N, M ∈ N
N
∆ M ⊂ RM
p∗ ∈ ∆M
L ∈ RN ×M
H ∈ ΣN ×M
`i ∈ RM
C i ⊆ ∆M
P⊆N
N ⊆ N2
+
Ni,j
⊆N
Si ∈ {0, 1}si ×M
L⊆N
Vi,j ⊆ N
vi,j,k ∈ Rsk , k ∈ Vi,j
Wi ∈ R

Definition
number of actions and outcomes
{1, . . . , N }, set of actions
M -dim. simplex, set of opponent strategies
opponent strategy
loss matrix
feedback matrix
`i = L[i, :], loss vector underlying action i
cell of action i
set of Pareto-optimal actions
set of unordered neighboring action-pairs
neighborhood action set of {i, j} ∈ N
signal matrix of action i
set of locally observable action pairs
observer actions underlying {i, j} ∈ N
observer vectors
confidence width for action i ∈ N

3. The proposed algorithm
Our algorithm builds on the core idea underlying algorithm Balaton of Bartók et al. (2011), so we start
with a brief review of Balaton. Balaton uses
sweeps to successively eliminate suboptimal actions.
This is done by estimating the differences between
the expected losses of pairs of actions, i.e., δi,j =
(`i −`j )> p∗ (i, j ∈ N ). In fact, Balaton exploits that
it suffices to keep track of δi,j for neighboring pairs of
actions (i.e., for action pairs i, j such that {i, j} ∈ N ).
This is because if an action i is suboptimal, it will
have a neighbor j that has a smaller expected loss
and so the action i will get eliminated when δi,j is
checked. Now, to estimate δi,j for some {i, j} ∈ N one
observes that under thePlocal observability condition,
it holds that `i − `j = k∈N + Si> vi,j,k for some veci,j

tors vi,j,k ∈ Rσk . This yields that δi,j = (`i − `j )> p∗ =
P
def
>
∗
∗
+ v
i,j,k Sk p . Since νk = Sk p is the vector
k∈Ni,j
of the distribution of symbols under action k, which
can be estimated by νk (t), the empirical frequencies of
the individual symbols
under k up to time
P observed
>
νk (t) to estimate δi,j .
t, Balaton uses k∈N + vi,j,k
i,j

+
Since none of the actions in Ni,j
can get eliminated
before one of {i, j} gets eliminated, the estimate of
δi,j gets refined until one of {i, j} is eliminated.

The essence of why Balaton achieves a low regret is
as follows: When i is not a neighbor of the optimal
action i∗ one can show that it will be eliminated before all neighbors j “between i and i∗ ” get eliminated.
Thus, the contribution of such “far” actions to the re-

Found in/at

Definition
Definition
Definition
Definition
Definition
Definition
Definition
Definition
Definition

1
2
3
3
4
6
7
7
7

gret is minimal. When i is a neighbor of i∗ , it will
−2
be eliminated in time proportional to δi,i
∗ . Thus the
contribution to the regret of such an action is propordef
tional to δi−1 , where δi = δi,i∗ . It also holds that the
contribution to the regret of i cannot be larger than
δi T . Thus, the contribution
of i to the regret is at
√
most min(δi T, δi−1 ) ≤ T .
When some pairs {i, j} ∈ N are not locally observable,
+
one needs to use actions other than those in Ni,j
to
construct anP
estimate of δi,j . Under global observability, `i −`j = k∈Vi,j Si> vi,j,k for an appropriate subset
Vi,j ⊂ N and an appropriate set of vectors vi,j,· . Thus,
if the actions in Vi,j are kept in play,
Pone can >estimate
the difference δi,j as before, using k∈N + vi,j,k
νk (t).
i,j
This motivates the following definition:
Definition 7 (Observer sets and observer vectors).
The observer set Vi,j ⊂ N underlying a pair of neighboring actions {i, j} ∈ N is a set of actions such that
`i − `j ∈ ⊕k∈Vi,j Im Sk> .
The observer vectors (vi,j,k )P
k∈Vi,j are defined to satisfy the equation `i − `j = k∈Vi,j Sk> vi,j,k . In particular, vi,j,k ∈ Rsk . In what follows, the choice
of the observer sets and vectors is restricted so that
Vi,j = Vj,i and vi,j,k = −vj,i,k . Furthermore, the ob+
server set Vi,j is constrained to be a superset of Ni,j
and in particular when a pair {i, j} is locally observ+
able, Vi,j = Ni,j
must hold. Finally, for any action
S
+
k ∈ {i,j}∈N Ni,j
, let Wk = maxi,j:k∈Vi ,j kvi,j,k k∞ be
the confidence width of action k.
+
The reason of the particular choice Vi,j = Ni,j
for lo-

Adaptive Stochastic Partial Monitoring

cally observable pairs {i, j} is that we plan to use Vi,j
(and the vectors vi,j,· ) in the case of locally observable
pairs, too. For not locally observable pairs, the whole
action set N is always a valid observer set (thus, Vi,j
can be found). However, whenever possible, it is better to use a smaller set. The actual choice of Vi,j (and
vi,j,k ) is postponed until the effect of this choice on the
regret becomes clear.
With the observer sets, the basic idea of the algorithm
becomes as follows: (i) Eliminate the suboptimal actions in successive sweeps; (ii) In each sweep, enrich
the set of remaining actions P(t) by adding the observer actions underlying theS remaining neighboring
pairs {i, j} ∈ N (t): V(t) = {i,j}∈N (t) Vi,j ; (iii) Explore the actions in P(t) ∪ V(t) to update the symbol
frequency estimate vectors νk (t). Another refinement
is to eliminate the sweeps so as to make the algorithm
enjoy an advantageous anytime property. This can be
achieved by selecting in each step only one action. We
propose the action to be chosen should be the one that
maximizes the reduction of the remaining uncertainty.
√
This algorithm could be shown to enjoy T regret for
locally observable games. However, if we run it on a
non-locally observable game and the opponent strategy is on Ci ∩ Cj for {i, j} ∈ N \ L, it will suffer linear
regret! The reason is that if both actions i and j are
optimal, and thus never get eliminated, the algorithm
+
will choose actions from Vi,j \ Ni,j
too often. Furthermore, even if the opponent strategy is not on the
boundary the regret can be too high: say action i is
optimal but δj is small, while {i, j} ∈ N \ L. Then
a third action k ∈ Vi,j with large δk will be chosen
proportional to 1/δj2 times, causing high regret. To
combat this we restrict the frequency with which an
action can be used for “information seeking purposes”.
For this, we introduce the set of rarely chosen actions,
R(t) = {k ∈ N : nk (t) ≤ ηk f (t)} ,
where ηk ∈ R, f : N → R are tuning parameters to be
chosen later. Then, the set of actions available at time
t is restricted
N + (t) ∪ (V(t) ∩ R(t)), where
S to P(t) ∪ +
+
N (t) = {i,j}∈N (t) Ni,j . We will show that with
these modifications, the algorithm achieves O(T 2/3 )
regret in the general
√ case, while it will also be shown
to achieve an O( T ) regret when the opponent uses
a benign strategy. A pseudocode for the algorithm is
given in Algorithm 1.
It remains to specify the function getPolytope. It
gets the array halfSpace as input. The array halfSpace stores which neighboring action pairs have a
confident estimate on the difference of their expected
losses, along with the sign of the difference (if confi-

Algorithm 1 CBP
Input: L, H, α, η1 , . . . , ηN , f = f (·)
Calculate P, N , Vi,j , vi,j,k , Wk
for t = 1 to N do
Choose It = t and observe Yt
{Initialization}
nIt ← 1
{# times the action is chosen}
νIt ← Yt
{Cumulative observations}
end for
for t = N + 1, N + 2, . . . do
for eachP
{i, j} ∈ N do
νk
>
δ̃i,j ← k∈Vi,j vi,j,k
nk q{Loss diff. estimate}
P
t
{Confidence}
ci,j ← k∈Vi,j kvi,j,k k∞ α nlog
k
if |δ̃i,j | ≥ ci,j then
half Space(i, j) ← sgn δ̃i,j
else
half Space(i, j) ← 0
end if
end for
[P(t), N (t)] ← getPolytope(P, N , half Space)
+
N + (t) = ∪{i,j}∈N (t) Nij
V(t) = ∪{i,j}∈N (t) Vij
R(t) = {k ∈ N : nk (t) ≤ ηk f (t)}
S(t) = P(t) ∪ N + (t) ∪ (V(t) ∩ R(t))
W2
Choose It = argmaxi∈S(t) nii and observe Yt
νIt ← νIt + Yt
nIt ← nIt + 1
end for
dent). Each of these confident pairs define an open
halfspace, namely

∆{i,j} = p ∈ ∆M : half Space(i, j)(`i − `j )> p > 0 .
The function getPolytope calculates the open polytope defined as the intersection of the above halfspaces.
Then for all i ∈ P it checks if Ci intersects with the
open polytope. If so, then i will be an element of P(t).
Similarly, for every {i, j} ∈ N , it checks if Ci ∩ Cj intersects with the open polytope and puts the pair in
N (t) if it does.
Note that it is not enough to compute P(t) and then
drop from N those pairs {k, l} where one of k or l is
excluded from P(t): it is possible that the boundary
Ck ∩ Cl between the cells of two actions k, l ∈ P(t) is
included in the rejected region. For an illustration of
cell decomposition and excluding cells, see Figure 1.
Computational complexity The computationally
heavy parts of the algorithm are the initial calculation
of the cell decomposition and the function getPolytope. All of these require linear programming. In the
preprocessing phase we need to solve N + N 2 linear

Adaptive Stochastic Partial Monitoring
(0, 0, 1)

(0, 0, 1)

(0, 1, 0)

(0, 1, 0)

(1, 0, 0)

(1, 0, 0)

(a) Cell decomposition.

(b) Gray indicates
cluded area.

ex-

Figure 1. An example of cell decomposition (M = 3).

programs to determine cells and neighboring pairs of
cells. Then in every round, at most N 2 linear programs are needed. The algorithm can be sped up by
“caching” previously solved linear programs.

short explanation of the different terms in the bound.
The first term corresponds to the confidence interval
failure event. The second term comes from the initialization phase of the algorithm. The remaining four
terms come from categorizing the choices of the algorithm by two criteria: (1) Would It be different if R(t)
was defined as R(t) = N ? (2) Is It ∈ P(t) ∪ Nt+ ?
These two binary events lead to four different cases in
the proof, resulting in the last four terms of the bound.
An implication of Theorem 1 is an upper bound on the
individual regret of locally observable games:
Corollary 1. If G is locally observable then


X
1
E[RT ] ≤
2|Vi,j | 1 +
2α − 2
{i,j}∈N

+

N
X

δk + 4Wk2

k=1

d2k
α log T .
δk

4. Analysis of the algorithm
The first theorem in this section is an individual upper
bound on the regret of CBP.
Theorem 1. Let (L, H) be an N by M partialmonitoring game. For a fixed opponent strategy p∗ ∈
∆M , let δi denote the difference between the expected
loss of action i and an optimal action. For any time
horizon T , algorithm CBP with parameters α > 1,
2/3
νk = Wk , f (t) = α1/3 t2/3 log1/3 t has expected regret

2|Vi,j | 1 +

X

E[RT ] ≤

{i,j}∈N
N
X

+

k=1
δk >0

+

X

4Wk2

1
2α − 2


+

N
X

δk

k∈V\N +

d2l(k)
2
δl(k)

α log T,
!

α
+

1/3

2/3
Wk T 2/3

X

log

1/3

The following corollary is an upper bound on the minimax regret of any globally observable game.
Corollary 2. Let G be a globally observable game.
Then there exists a constant c such that the expected
regret can be upper bounded independently of the choice
of p∗ as
E[RT ] ≤ cT 2/3 log1/3 T .

k=1

d2k
α log T
δk

δk min 4Wk2

Proof. If a game is locally observable then V \N + = ∅,
leaving the last two sums of the statement of Theorem 1 zero.

T

2/3

δk α1/3 Wk T 2/3 log1/3 T

k∈V\N +

+ 2dk α1/3 W 2/3 T 2/3 log1/3 T ,
where W = maxk∈N Wk , V = ∪{i,j}∈N Vi,j , N + =
+
∪{i,j}∈N Ni,j
, and d1 , . . . , dN are game-dependent constants.
The proof is omitted for lack of space.4 Here we give a
4
For complete proofs we refer the reader to the supplementary material.

The following theorem is an upper bound on the minimax regret of any globally observable game against
“benign” opponents. To state the theorem, we need a
new definition. Let A be some subsetTof actions in G.
We call A a point-local game in G if i∈A Ci 6= ∅.
Theorem 2. Let G be a globally observable game. Let
∆0 ⊆ ∆M be some subset of the probability simplex
such that its topological closure ∆0 has ∆0 ∩ Ci ∩ Cj = ∅
for every {i, j} ∈ N \ L. Then there exists a constant
c such that for every p∗ ∈ ∆0 , algorithm CBP with
2/3
parameters α > 1, νk = Wk , f (t) = α1/3 t2/3 log1/3 t
achieves
p
E[RT ] ≤ cdpmax bT log T ,
where b is the size of the largest point-local game, and
dpmax is a game-dependent constant.
In a nutshell, the proof revisits the four cases of the
proof of Theorem 1, and shows that the terms which
would yield T 2/3 upper bound can be non-zero only
for a limited number of time steps.

Adaptive Stochastic Partial Monitoring

Remark 1. Note that the above theorem implies that
CBP does not√need to have any prior knowledge about
∆0 to achieve T regret. This is why we say our algorithm is “adaptive”.
An immediate implication of Theorem 2 is the following minimax bound for locally observable games:
Corollary 3. Let G be a locally observable finite partial monitoring game. Then there exists a constant c
such that for every p ∈ ∆M ,
p
E[RT ] ≤ c T log T .
Remark 2. The upper bounds in Corollaries 2 and 3
both have matching lower bounds up to logarithmic factors (Bartók et al., 2011), proving that CBP
achieves near optimal regret in both locally observable
and non-locally observable games.

5. Experiments
We demonstrate the results of the previous sections using instances of Dynamic Pricing, as well as a locally
observable game. We compare the results of CBP to
two other algorithms: Balaton (Bartók et al., 2011)
which is, as mentioned earlier
√ in the paper, the first
e T ) minimax regret for all
algorithm that achieves O(
locally observable finite stochastic partial-monitoring
games; and FeedExp3 (Piccolboni & Schindelhauer,
2001), which achieves O(T 2/3 ) minimax regret on
all non-hopeless finite partial-monitoring games, even
against adversarial opponents.
5.1. A locally observable game
The game we use to compare CBP and Balaton has
3 actions and 3 outcomes. The game is described with
the loss and feedback matrices:




1 1 0
a b b
L = 0 1 1 ;
H = b a b .
1 0 1
b b a
We ran the algorithms 10 times for 15 different
stochastic strategies. We averaged the results for each
strategy and then took pointwise maximum over the 15
strategies. Figure 2(a) shows the empirical minimax
regret calculated the way described above. In addition,
Figure 2(b) shows the regret of the algorithms against
one of the opponents, averaged over 100 runs. The
results indicate that CBP outperforms both FeedExp
and Balaton. We also observe that, although the
asymptotic performace of Balaton is proven to be
better than that of FeedExp, a larger constant factor
makes Balaton lose against FeedExp even at time
step ten million.

5.2. Dynamic Pricing
In Dynamic Pricing, at every time step a seller (player)
sets a price for his product while a buyer (opponent)
secretly sets a maximum price he is willing to pay. The
feedback for the seller is “buy” or “no-buy”, while his
loss is either a preset constant (no-buy) or the difference between the prices (buy). The finite version of the
game can be described with the following matrices:




0 1 ··· N − 1
y y ··· y
 c 0 · · · N − 2
n y · · · y 




L = . .
H = . .

.
.
..
..
. . . . . ... 
.. 
 ..
 ..

c ···
c
0
n ···
n y
This game is not locally observable and thus it is
“hard” (Bartók et al., 2011). Simple linear algebra
gives that the locally observable action pairs are the
“consecutive” actions (L = {{i, i + 1} : i ∈ N − 1}),
while quite surprisingly, all action pairs are neighbors.
We compare CBP with FeedExp on Dynamic Pricing with N = M = 5 and c = 2. Since Balaton
is undefined on not locally observable games, we can
not include it in the comparison. To demonstrate the
adaptiveness of CBP, we use two sets of opponent
strategies. The “benign” setting is a set of opponents
which are far away from “dangerous” regions, that is,
from boundaries between cells of non-locally observable neighboring action pairs. The “harsh” settings,
however, include opponent strategies that are close or
on the boundary between two such actions. For each
setting we maximize over 15 strategies and average
over 10 runs. We also compare the individual regret of
the two algorithms against one benign and one harsh
strategy. We averaged over 100 runs and plotted the
90 percent confidence intervals.
The results (shown in Figures 3 and 4) indicate that
CBP has a significant advantage over FeedExp on
benign settings. Nevertheless, for the harsh settings
FeedExp slightly outperforms CBP, which we think is
a reasonable price to pay for the benefit of adaptivity.




incomplete on each step, still efficiently computes optimal
actions in a timely manner.

We consider the problem of efficiently learning
optimal control policies and value functions over
large state spaces in an online setting in which
estimates must be available after each interaction
with the world. This paper develops an explicitly
model-based approach extending the Dyna architecture to linear function approximation. Dynastyle planning proceeds by generating imaginary
experience from the world model and then applying model-free reinforcement learning algorithms to the imagined state transitions. Our
main results are to prove that linear Dyna-style
planning converges to a unique solution independent of the generating distribution, under natural conditions. In the policy evaluation setting,
we prove that the limit point is the least-squares
(LSTD) solution. An implication of our results
is that prioritized-sweeping can be soundly extended to the linear approximation case, backing
up to preceding features rather than to preceding
states. We introduce two versions of prioritized
sweeping with linear Dyna and briefly illustrate
their performance empirically on the Mountain
Car and Boyan Chain problems.

The Dyna architecture (Sutton 1990) provides an effective
and flexible approach to incremental planning while maintaining responsiveness. There are two ideas underlying the
Dyna architecture. One is that planning, acting, and learning are all continual, operating as fast as they can without
waiting for each other. In practice, on conventional computers, each time step is shared between planning, acting,
and learning, with proportions that can be set arbitrarily according to available resources and required response times.

Online learning and planning

Efficient decision making when interacting with an incompletely known world can be thought of as an online learning
and planning problem. Each interaction provides additional
information that can be used to learn a better model of the
world’s dynamics, and because this change could result in a
different action being best (given the model), the planning
process should be repeated to take this into account. However, planning is inherently a complex process; on large
problems it not possible to repeat it on every time step without greatly slowing down the response time of the system.
Some form of incremental planning is required that, though

The second idea underlying the Dyna architecture is that
learning and planning are similar in a radical sense. Planning in the Dyna architecture consists of using the model
to generate imaginary experience and then processing the
transitions of the imaginary experience by model-free reinforcement learning algorithms as if they had actually occurred. This can be shown, under various conditions, to
produce exactly the same results as dynamic-programming
methods in the limit of infinite imaginary experience.
The original papers on the Dyna architecture and most subsequent extensions (e.g., Singh 1992; Peng & Williams
1993; Moore & Atkeson 1993; Kuvayev & Sutton 1996)
assumed a Markov environment with a tabular representation of states. This table-lookup representation limits the
applicability of the methods to relatively small problems.
Reinforcement learning has been combined with function
approximation to make it applicable to vastly larger problems than could be addressed with a tabular approach.
The most popular form of function approximation is linear function approximation, in which states or state-action
pairs are first mapped to feature vectors, which are then
mapped in a linear way, with learned parameters, to value
or next-state estimates. Linear methods have been used
in many of the successful large-scale applications of reinforcement learning (e.g., Silver, Sutton & Müller 2007;
Schaeffer, Hlynka & Jussila 2001). Linear function approximation is also simple, easy to understand, and possesses some of the strongest convergence and performance
guarantees among function approximation methods. It is

natural then to consider extending Dyna for use with linear
function approximation, as we do in this paper.
There has been little previous work addressing planning
with linear function approximation in an online setting.
Paduraru (2007) treated this case, focusing mainly on sampling stochastic models of a cascading linear form, but
also briefly discussing deterministic linear models. Degris,
Sigaud and Wuillemin (2006) developed a version of Dyna
based on approximations in the form of dynamic Bayes networks and decision trees. Their system, SPITI, included
online learning and planning based on an incremental version of structured value iteration (Boutilier, Dearden &
Goldszmidt 2000). Singh (1992) developed a version of
Dyna for variable resolution but still tabular models. Others
have proposed linear least-squares methods for policy evaluation that are efficient in the amount of data used (Bradtke
& Barto 1996; Boyan 1999, 2002; Geramifard, Bowling &
Sutton 2006). These methods can be interpreted as forming and then planning with a linear model of the world’s
dynamics, but so far their extensions to the control case
have not been well suited to online use (Lagoudakis &
Parr 2003; Peters, Vijayakumar & Schaal 2005; Bowling,
Geramifard, & Wingate 2008), whereas our linear Dyna
methods are naturally adapted to this case. We discuss
more specifically the relationship of our work to LSTD
methods in a later section. Finally, Atkeson (1993) and others have explored linear, learned models with off-line planning methods suited to low-dimensional continuous systems.

2

Notation

We use the standard framework for reinforcement learning with linear function approximation (Sutton & Barto
1998), in which experience consists of the time indexed
stream s0 , a0 , r1 , s1 , a1 , r2 , s2 , . . ., where st ∈ S is a state,
at ∈ A is an action, and rt ∈ R is a reward. The actions are selected by a learning agent, and the states and rewards are selected by a stationary environment. The agent
does not have access to the states directly but only through
a corresponding feature vector φt ∈ Rn = φ(st ). The
n
agent selects actions
P according to a policy, π : R × A →
[0, 1] such that a∈A π(φ, a) = 1, ∀φ. An important step
towards finding a good policy is to estimate the value function for a given policy (policy evaluation). The value function is approximated as a linear function with parameter
vector θ ∈ Rn :
(∞
)
X
θ> φ(s) ≈ V π (s) = Eπ
γ t−1 rt | s0 = s ,
t=1

where γ ∈ [0, 1). In this paper we consider policies that are
greedy or -greedy with respect to the approximate statevalue function.

Algorithm 1 : Linear Dyna for policy evaluation, with random sampling and gradient-descent model learning
Obtain initial φ, θ, F, b
For each time step:
Take action a according to the policy. Receive r, φ0
θ ← θ + α[r + γθ> φ0 − θ> φ]φ
F ← F + α(φ0 − F φ)φ>
b ← b + α(r − b> φ)φ
temp ← φ0
Repeat p times (planning):
Generate a sample φ from some distribution µ
φ0 ← F φ
r ← b> φ
θ ← θ + α[r + γθ> φ0 − θ> φ]φ
φ ← temp

3

Theory for policy evaluation

The natural place to begin a study of Dyna-style planning
is with the policy evaluation problem of estimating a statevalue function from a linear model of the world. The model
consists of a forward transition matrix F ∈ Rn × Rn (incorporating both environment and policy) and an expected
reward vector b ∈ Rn , constructed such that F φ and b> φ
can be used as estimates of the feature vector and reward
that follow φ. A Dyna algorithm for policy evaluation goes
through a sequence of planning steps, on each of which a
starting feature vector φ is generated according to a probability distribution µ, and then a next feature vector φ0 = F φ
and next reward r = b> φ are generated from the model.
Given this imaginary experience, a conventional modelfree update is performed, for example, according to the linear TD(0) algorithm (Sutton 1988):
θ ← θ + α(r + γθ> φ0 − θ> φ)φ,

(1)

or according to the residual gradient algorithm (Baird
1995):
θ ← θ + α(r + γθ> φ0 − θ> φ)(φ − γφ0 ),

(2)

where α > 0 is a step-size parameter. A complete algorithm using TD(0), including learning of the model, is
given in Algorithm 1.
3.1

Convergence and fixed point

There are two salient theoretical questions about the Dyna
planning iterations (1) and (2): Under what conditions on
µ and F do they converge? and What do they converge
to? Both of these questions turn out to have interesting answers. First, note that the convergence of (1) is in question
in part because it is known that linear TD(0) may diverge
if the distribution of starting states during training does not
match the distribution created by the normal dynamics of

the system, that is, if TD(0) is used off-policy. This suggests that the sampling distribution used here, µ, might
have to be strongly constrained in order for the iteration
to be stable. On the other hand, the data here is from the
model, and the model is not a general system: it is deterministic1 and linear. This special case could be much better
behaved. In fact, convergence of linear Dyna-style policy
evaluation, with either the TD(0) or residual-gradient iterations, is not affected by µ, but only by F , as long as µ exercises all directions in the full n-dimensional vector space.
Moreover, not only is the fact of convergence unaffected by
µ, but so is the value converged to. In fact, we show below
that convergence is to a deterministic fixed point, a value
of θ such that the iterations (1) and (2) leave it unchanged
not just in expected value, but for every individual φ that
could be generated by µ. The only way this could be true is
if the TD error (the first expression in parentheses in each
iteration) were exactly zero, that is, if
0

=

r + γθ> φ0 − θ> φ
>

>

b φ + γθ F φ − θ φ

=

(b + γF > θ − θ)> φ.

And the only way that this can be true for all φ is for the
expression in parenthesis above to be zero:
0

Before verifying the conditions of this result, let us
rewrite (4) in terms of the matrix G = I − γF :
θk+1

= θk + αk (b> φk + θk> (γF − I)φk )φk

= b + (γF > − I)θ,

= θk + αk sk .

=

(I − γF > )−1 b,

(4)

where θ0 ∈ Rn is P
arbitrary. AssumeP
that (i) the step-size
∞
∞
sequence satisfies k=0 αk = ∞, k=0 αk2 < ∞, (ii)
r(F ) ≤ 1, (iii) (φk ) are uniformly
 bounded
 i.i.d. random
variables, and that (iv) C = E φk φ>
is non-singular.
k
Then the parameter vector θk converges with probability
one to (I − γF > )−1 b.

= θk + αk (b> φk − θk> Gφk )φk

Here sk is defined by the last equation.
(3)

assuming that the inverse exists. Note that this expression
for the fixed point does not depend on µ, as promised.
If I − γF > is nonsingular, then there might be no fixed
point. This could happen for example if F were an expansion, or more generally if the limit (γF )∞ were not
zero. These cases correspond to world models that say the
feature vectors diverge to infinity over time. Failure to converge in these cases should not be considered a problem for
the Dyna iterations as planning algorithms; these are cases
in which the planning problem is ill posed. If the feature
vectors diverge, then so too may the rewards, in which case
the true values given the model are infinite. No real finite
Markov decision process could behave in this way.
It remains to show the conditions on F under which the iterations converge to the fixed point if one exists. We prove
next that under the TD(0) iteration (1), convergence is guaranteed if the numerical radius of F is less than one,2 and
1

θk+1 = θk + αk (b> φk + γθk> F φk − θk> φk )φk ,

= b + γF > θ − θ

which immediately implies that
θ

Theorem 3.1 (Convergence of linear TD(0) Dyna for policy evaluation). Consider the TD(0) iteration with a nonnegative step-size sequence (αk ):

Proof. The idea of the proof is to view the algorithm as a
stochastic gradient descent method. In particular, we apply
Proposition 4.1 of (Bertsekas & Tsitsiklis 1996).

>

=

then that under the residual-gradient iteration (2), convergence is guaranteed for any F as long as the fixed point exists. That F ’s numerical radius be less than 1 is a stronger
condition than nonsingularity of I − γF > , but it is similar
in that both conditions pertain to the matrix trending toward
expansion when multiplied by itself.

The model is deterministic because it generates the expectation of the next feature vector; the system itself may be stochastic.
2
The numerical radius of a real-valued square matrix A is defined by r(A) = maxkxk2 =1 xT Ax.

The cited proposition requires the definition of a potential function J(θ) and will allow us to conclude that
limk→∞ ∇J(θk) = 0 with probability one. Let
 us choose
J(θ) = 1/2 E (b> φk + γθ> F φk − θ> φk )2 . Note that
by our i.i.d. assumptions on the features, J(θ) is welldefined. We need to check four conditions (because the
step-size conditions are automatically satisfied): (i) The
nonnegativity of the potential function; (ii) The Lipschitz
continuity of ∇J(θ); (iii) The pseudo-gradient property of
the expected update direction; and (iv) The boundedness of
the expected magnitude of the update, more precisely that
E ksk k22 |θk ≤ O(k∇J(θk )k22 ). Nonnegativity is satisfied
by definition and the boundedness condition (iv) is satisfied
thanks to the boundedness of the features.
Let us show now that the pseudo-gradient property (iii) is
satisfied. This condition requires the demonstration of a
positive constant c such that
ck∇J(θk )k22 ≤ −∇J(θk )> E [sk |θk ] .

(5)

Define sk = E [sk |θk ] = Cb − CG> θk . A simple calculation gives ∇J(θk ) = −Gsk . Hence k∇J(θk )k22 =
>
>
>
s>
k G Gsk and −(∇J(θk )) sk = sk Gsk . Therefore (5)
> >
>
is equivalent to c sk G Gsk ≤ sk Gsk . In order to make
this true with a sufficiently small c, it suffices to show that

s> Gs > 0 holds for any non-zero vector s. An elementary
reasoning shows that this is equivalent to 1/2(G + G> ) being positive definite, which in turn is equivalent to r(F ) ≤
1, showing that (iii) is satisfied.
Hence, we have verified all the assumptions of the
cited proposition and can therefore we conclude that
limk→∞ ∇J(θk ) = 0 with probability one. Plugging in the
expression of ∇J(θk ), we get limt→∞ (Cb−CG> θk ) = 0.
Because C and G are invertible (this latter follows from
r(F ) ≤ 1), it follows that the limit of θk exists and
limk→∞ θk = (G> )−1 b = (I − γF > )−1 b.

verges with probability one to (I − γF > )−1 b, assuming
that (I − γF > ) is non-singular.
Proof. As all the conditions of Proposition 4.1 of (Bertsekas & Tsitsiklis 1996) are trivially satisfied with the
choice J(θ) = E [J(θ, φk )], we can conclude that θk converges w.p.1 to the minimizer of J(θ). In the previous theorem we have seen that the minimizer of J(θ) is indeed
θ = (I − γF > )−1 b, finishing the proof.
3.2

Convergence to the LSTD solution

Several extensions of this result are possible. First, the requirement of i.i.d. sampling can be considerably relaxed.
With an essentially unchanged proof, it is possible to show
that the theorem remains true if the feature vectors are generated by a Markov process given that they satisfy appropriate ergodicity conditions. Moreover, building on a result by Delyon (1996), one can show that the result continues to hold even if the sequence of features is generated in an algorithmic manner, again provided that some
ergodicity conditions are met.
PKThe major assumption then
is that C = limK→∞ 1/K k=1 φk φ>
k exists and is nonsingular. Further, because there is no “noise” to reject, there
is noP
need to decay the step-sizes towards zero (the condi∞
tion k=0 αk2 < +∞ in the proofs is used to “filter out
noise”). In particular, we conjecture that sufficiently small
constant step-sizes would work as well (for a result of this
type see Proposition 3.4 by Bertsekas & Tsitsiklis 1996).

So far we have discussed the convergence of planning given
a model, but we have said nothing about the relationship
of the model to data, or about the quality of the resultant
solution. Suppose the model were the best linear fit to a
finite dataset of observed feature-vector-to-feature-vector
transitions with accompanying rewards. In this case we can
show that the fixed point of the Dyna updates is the least
squares temporal-difference solution. This is the solution
for which the mean TD(0) update is zero and is also the solution found by the LSTD(0) algorithm (Barto & Bradtke
1996).

On the other hand the requirement on the numerical radius of F seems to be necessary for the convergence of the
TD(0) iteration. By studying the ODE associated with (4),
we see that it is stable if and only if CG is a positive stable
matrix (i.e., iff all its eigenvalues have positive real part).
From this it seems necessary to require that G is positive
stable. However, to ensure that CG is positive stable the
strictly stronger condition that G + G> is positive definite must be satisfied. This latter condition is equivalent
to r(F ) ≤ 1.

Proof. It suffices to show that the respective solution sets
of the equations

We turn now to consider the convergence of Dyna planning
using the residual-gradient Dyna iteration (2). This update
rule can be derived by taking the gradient of J(θ, φk ) =
(b> φk + γθ> φk − θ> φk )2 w.r.t. θ. Thus, as an immediate
consequence of Proposition 4.1 of (Bertsekas & Tsitsiklis
1996) we get the following result:
Theorem 3.2 (Convergence of residual-gradient Dyna for
policy evaluation). Assume that θk is updated according to
θk+1 = θk + αk (b> φk + γθk> F φk − θk> φk )(φk − γF φk ),
where θ0 ∈ Rn is arbitrary. Assume that the non-negative
step-size sequence (αk ) satisfies the summability condition
(i) of Theorem 3.1 and that (φk ) are uniformly bounded
i.i.d. random variables. Then the parameter vector θk con-

Theorem 3.3. Given a training dataset of feature, reward,
next-state feature triples D = [φ1 , r1 , φ01 , . . . , φn , rn , φ0n ],
let F, bPbe the least-squares model built on D. Assume that
n
C = k=1 φk φ>
k has full rank. Then the solution (3) is
the same as the LSTD solution on this training set.

0

=

n
X

φk (rk + γ(φ0k )> θ − φ>
k θ),

(6)

k=1

0

=

b + (γF > − I)θ

(7)

are the same. This is because the LSTD parameter vectors
are obtained by solving the first equation and the TD(0)
Dyna solutions are derived from the second equation.
Pn
Pn
Let D = k=1 φk (φ0k )> , and r = k=1 φk rk . A standard calculation shows that
F>

=

C −1 D

and b = C −1 r.

Plugging in C, D into (6) and factoring out θ shows that
any solution of (6) also satisfies
0

=

r + (γD − C) θ.

(8)

If we multiply both sides of (8) by C −1 from the left we
get (7). Hence any solution of (6) is also a solution of (7).
Because all the steps of the above derivation are reversible,
we get that the reverse statement holds as well.

Algorithm 2 : Linear Dyna with PWMA prioritized
sweeping (policy evaluation)
Obtain initial φ, θ, F, b
For each time step:
Take action a according to the policy. Receive r, φ0
δ ← r + γθ> φ0 − θ> φ
θ ← θ + αδφ
F ← F + α(φ0 − F φ)φ>
b ← b + α(r − b> φ)φ
For all i such that φ(i) 6= 0:
For all j such that F ij 6= 0:
Put j on the PQueue with priority |F ij δφ(i)|
Repeat p times while PQueue is not empty:
i ← pop the PQueue
δ ← b(i) + γθ> F ei − θ(i)
θ(i) ← θ(i) + αδ
For all j such that F ij 6= 0:
Put j on the queue with priority |F ij δ|
φ ← φ0

4

Algorithm 3 : Linear Dyna with MG prioritized sweeping
(policy evaluation)
Obtain initial φ, θ, F, b
For each time step:
Take action a according to the policy. Receive r, φ0
δ ← r + γθ> φ0 − θ> φ
θ ← θ + αδφ
F ← F + α(φ0 − F φ)φ>
b ← b + α(r − b> φ)φ
For all i such that φ(i) 6= 0:
Put i on the PQueue with priority |δφ(i)|
Repeat p times while PQueue is not empty:
i ← pop the PQueue
For all j such that F ij 6= 0:
δ ← b(j) + γθ> F ej − θ(j)
θ(j) ← θ(j) + αδ
Put j on the PQueue with priority |δ|
φ ← φ0

Linear prioritized sweeping

We have shown that the convergence and fixed point of policy evaluation by linear Dyna are not affected by the way
the starting feature vectors are chosen. This opens the possibility of selecting them cleverly so as to speed the convergence of the planning process. One natural idea—the
idea behind prioritized sweeping—is to work backwards
from states that have changed in value to the states that
lead into them. The lead-in states are given priority for being updated because an update there is likely to change the
state’s value (because they lead to a state that has changed
in value). If a lead-in state is updated and its value is
changed, then its lead-in states are in turn given priority
for updating, and so on. In the table-lookup context in
which this idea was developed (Moore & Atkeson 1993;
Peng 1993; see also Wingate & Seppi 2005), there could
be many states preceding each changed state, but only one
could be updated at a time. The states waiting to be updated were kept in a queue, prioritized by the size of their
likely effect on the value function. As high-priority states
were popped off the queue and updated, it would sometimes give rise to highly efficient sweeps of updates across
the state space; this is what gave rise to the name “prioritized sweeping”.
With function approximation it is not possible to identify
and work backwards from individual states, but alternatively one could work backwards feature by feature. If
there has just been a large change in θ(i), the component of
the parameter vector corresponding to the ith feature, then
one can look backwards through the model to find the features j whose components θ(j) are likely to have changed
as a result. These are the features j for which the elements
F ij of F are large. One can then preferentially construct

starting feature vectors φ that have non-zero entries at these
j components. In our algorithms we choose the starting
vectors to be the unit basis vectors ej , all of whose components are zero except the jth, which is 1. (Our theoretical
results assure us that this cannot affect the result of convergence.) Using unit basis vectors is very efficient computationally, as the vector matrix multiplication F φ is reduced
to pulling out a single column of F .
There are two tabular prioritized sweeping algorithms in
the literature. The first, due simultaneously to Peng and
Williams (1993) and to Moore and Atkeson (1993), which
we call PWMA prioritized sweeping, adds the predecessors
of every state encountered in real experience to the priority queue whether or not the value of the encountered state
was significantly changed. The second form of prioritized
sweeping, due to McMahan and Gordon (2005), and which
we call MG prioritized sweeping, puts each encountered
state on the queue, but not its predecessors. For McMahan and Gordon this resulted in a more efficient planner.
A complete specification of our feature-by-feature versions
of these two forms of prioritized sweeping are given above,
with TD(0) updates and gradient-descent model learning,
as Algorithms 2 and 3. These algorithms differ slightly
from previous prioritized sweeping algorithms in that they
update the value function from the real experiences and not
just from model-generated experience. With function approximation, real experience is always more informative
than model-generated experience, which will be distorted
by the function approximator. We found this to be a significant effect in our empirical experiments (Section 6).

Algorithm 4: Linear Dyna with MG prioritized sweeping
and TD(0) updates (control)
Obtain initial φ, θ, F, b
For each time step:

>
a ← arg maxa b>
(or -greedy)
a φ + γθ Fa φ
Take action a, receive r, φ0
δ ← r + γθ> φ0 − θ> φ
θ ← θ + αδφ
Fa ← Fa + α(φ0 − Fa φ)φ>
ba ← ba + α(r − b>
a φ)φ
For all i such that φ(i) 6= 0:
Put i on the PQueue with priority |δφ(i)|
Repeat p times while PQueue is not empty:
i ← pop the PQueue
ij
For all j s.t. there
 exists an>a s.t. F
 a 6= 0:
δ ← maxa ba (j) + γθ Fa ej − θ(j)
θ(j) ← θ(j) + αδ
Put j on the PQueue with priority |δ|
φ ← φ0

5

Theory for Control

We now turn to the full case of control, in which separate
models Fa , ba are learned and are then available for each
action a. These are constructed such that Fa φ and b>
a φ can
be used as estimates of the feature vector and reward that
follow φ if action a is taken. A linear Dyna algorithm for
the control case goes through a sequence of planning steps
on each of which a starting feature vector φ and an action
a are chosen, and then a next feature vector φ0 = Fa φ and
next reward r = ba φ are generated from the model. Given
this imaginary experience, a conventional model-free update is performed. The simplest case is to again apply
(1). A complete algorithm including prioritized sweeping
is given in Algorithm 4.
The theory for the control case is less clear than for policy evaluation. The main issue is the stability of the “mixture” of the forward model matrices. The corollary below
is stated for an i.i.d. sequence of features, but by the remark after Theorem 3.1 it can be readily extended to the
case where the policy to be evaluated is used to generate
the trajectories.
Corollary 5.1 (Convergence of linear TD(0) Dyna with
action models). Consider the Dyna recursion (4) with
the modification that in each step, instead of F φk ,
we use Fπ(φk ) φk , where π is a policy mapping feature vectors to actions and {Fa } is a collection of
forward-model matrices. Similarly, b> φk is replaced by
b>
π(φk ) φk . As before, assume that φk is an unspecified
i.i.d. process. Let (F, b)
 be the least squares
 model of
π: F = harg minG E kGφk − iFπ(φk ) φk k22 and b =
2
arg minu E (u> φk − b>
If the numerical radius
π(φk ) φk )
of F is bounded by one, then the conclusions of Theo-

-3

N

1
0
.
.
0
0

-3

N-1

-3

-3

.75
.25
.
.
0
0

-3

3

0
0
.
.
.5
.5

-3

-3

2

0
0
.
.
.25
.75

-2

1

0
0
.
.
0
1

0

0

0
0
.
.
0
0

Figure 1: The general Boyan Chain problem.
rem 3.1 hold: the parameter vector θk converges with probability one to (I − γF > )−1 b.
Proof. The proof is immediate
from
equation

 the normal


for F , which states that E F φk φ>
= E Fπ(φk ) φk φ>
k
k ,
and once we observe that, in the proof of Theorem 3.1, F
appears only in expressions of the form E F φk φ>
k .
As in the case of policy evaluation, there is a corresponding
corollary for the residual gradient iteration, with an immediate proof. These corollaries say that, for any policy with a
corresponding model that is stable, the Dyna recursion can
be used to compute its value function. Thus we can perform a form of policy iteration—continually computing an
approximation to the value function for the greedy policy.

6

Empirical results

In this section we illustrate the empirical behavior of the
four Dyna algorithms and make comparisons to model-free
methods using variations of two standard test problems:
Boyan Chain and Mountain Car. Our Boyan Chain environment is an extension of that by Boyan (1999, 2002)
from 13 to 98 states, and from 4 to 25 features (Geramifard, Bowling & Sutton 2006). Figure 1 depicts this environment in the general form. Each episode starts at state
N = 98 and terminates in state 0. For all states s > 2,
there is an equal probability of transitioning to states s − 1
or s − 2 with a reward of −3. From states 2 and 1, there are
deterministic transitions to states 1 and 0 with respective
rewards of −2 and 0. Our Mountain Car environment is exactly as described by Sutton (1996; Sutton & Barto 1998),
re-implemented in Matlab. An underpowered car must be
driven to the top of a hill by rocking back and forth in a
valley. The state variables are a pair (position,velocity) initialized to (−0.5, 0.0) at the beginning of each episode. The
reward is −1 per time step. There are three discrete actions
(accelerate, reverse, and coast). We used a value function
representation based on tile-coding feature vectors exactly
as in Sutton’s (1996) experiments, with 10 tilings over the
combined (position, velocity) pair, and with the tiles hashed
down to 10,000 features. In the policy evaluation experiments with this domain, the policy was to accelerate in

4

2

Boyan chain

10

9

Mountain Car

x 10

Dyna-Random
TD

7

1

Loss

Dyna-Random
Dyna-PWMA

Loss

10

5

TD

0

10

Dyna-MG

Dyna-PWMA

3

Dyna-MG
−1

10

0

20

40

60
Episode

80

100

1
0

200

400

600
Episode

800

1000

Figure 2: Performance of policy evaluation methods on the Boyan Chain and Mountain Car environments
the direction of the current velocity, and we added noise to
the domain that switched the selected action to a random
action with 10% probability. Complete code for our test
problems as standard RL-Glue environments is available
from the RL-Library hosted at the University of Alberta.
In all experiments, the step size parameter α took the form
0 +1
αt = α0 NN0 +t
1.1 , in which t is the episode number and
the pair (N0 , α0 ) was selected based on empirically finding the best combination out of α0 ∈ {.01, .1, 1} and
N0 ∈ {100, 1000, 106 } separately for each algorithm and
domain. All methods observed the same trajectories in policy evaluation. All graphs are averages of 30 runs; error
bars indicate standard errors in the means. Other parameter
settings were  = 0.1, γ = 1, and λ = 0.
We performed policy evaluation experiments with four algorithms: Dyna-Random, Dyna-PWMA, Dyna-MG (as in
Algorithms 1–3), and model-free TD(0). In the case of
the Dyna-Random algorithm, the starting feature vectors
in planning were chosen to be unit basis vectors with the 1
in a random location. Figure 2 shows the policy evaluation
performance of the four methods in the Boyan Chain and
Mountain Car environments. For the Boyan Chain domain,
the loss was the root-mean-squared error of the learned
value function compared to the exact analytical value, averaged over all states. In the Mountain Car domain, the
states are visited very non-uniformly, and a more sophisticated measure is needed. Note that all of the methods
drive θ toward an asymptotic value in which the expected
TD(0) update is zero; we can use the distance from this
as a loss measure. Specifically, we evaluated each learned
value function by freezing it and then running a fixed set
of 200,000 episodes with it while running the TD(0) algorithm (but not allowing θ to actually change). The norm of
the sum of the (attempted) update vectors was then computed and used as the loss. In practice, this measure can be
computed very efficiently as ||A∗ θ − b∗ || (in the notation of

LSTD(0), see Bradtke & Barto 1996).
In the Boyan Chain environment, the Dyna algorithms generally learned more rapidly than model-free TD(0). DynaMG was initially slower than the other algorithms, then
caught up and surpassed them. The relatively poor early
performance of Dyna-MG was actually due to its being
a better planning method. After few episodes the model
tends to be of very high variance, and so therefore is the
best value-function estimate given it. We tested this hypothesis by running the Dyna methods starting with a fixed,
well-learned model; in this case Dyna-MG was the best of
all the methods from the beginning. All of these data are
for one step of planning for each real step of interaction
with the world (p = 1). In preliminary experiments with
larger values of p, up to p = 10, we found further improvements in learning rate of the Dyna algorithms over TD(0),
and again Dyna-MG was best.
The results for Mountain Car are less clear. Dyna-MG
quickly does significantly better than TD(0), but the other
Dyna algorithms lag initially and never surpass TD(0).
Note that, for any value of p, Dyna-MG does many more θ
updates than the other two Dyna algorithms (because these
updates are in an inner loop, cf. Algorithms 2 and 3). Even
so, because of its other efficiencies Dyna-MG tended to run
faster overall in our implementation. Obviously, there is a
lot more interesting empirical work that could be done here.
We performed one Mountain Car experiment with DynaMG as a control algorithm (Algorithm 4), comparing it
with model-free Sarsa (i.e., Algorithm 4 with p = 0). The
results are shown in Figure 3. As before, Dyna-MG showed
a distinct advantage over the model-free method in terms
of learning rate. There was no clear advantage for either
method in the second half of the experiment. We note
that, asymptotically, model-free methods are never worse
than model-based methods, and are often better because the
model does not converge exactly to the true system because

7

−120
−140

Return

−160

Dyna-MG

−180
−200

Sarsa
−220
−240
−260
0

20

40

60
Episode

80

100

Figure 3: Control performance on Mountain Car

Conclusion

In this paper we have taken important steps toward establishing the theoretical and algorithmic foundations of
Dyna-style planning with linear function approximation.
We have established that Dyna-style planning with familiar
reinforcement learning update rules converges under weak
conditions corresponding roughly, in some cases, to the existence of a finite solution to the planning problem, and
that convergence is to a unique least-squares solution independent of the distribution used to generate hypothetical experience. These results make possible our second
main contribution: the introduction of algorithms that extend prioritized sweeping to linear function approximation,
with correctness guarantees. Our empirical results illustrate
the use of these algorithms and their potential for accelerating reinforcement learning. Overall, our results support
the conclusion that Dyna-style planning may be a practical
and competitive approach to achieving rapid, online control
in stochastic sequential decision problems with large state
spaces.
Acknowledgements

of structural modeling assumptions. (The case we treat
here—linear models and value functions with one-step TD
methods—is a rare case in which asymptotic performance
of model-based and model-free methods should be identical.) The benefit of models, and of planning generally, is in
rapid adaptation to new problems and situations.

The authors gratefully acknowledge the substantial contributions of Cosmin Paduraru and Mark Ring to the early
stages of this work. This research was supported by
iCORE, NSERC and Alberta Ingenuity.

These empirical results are not extensive and in some cases
are preliminary, but they nevertheless illustrate some of the
potential of linear Dyna methods. The results on the Boyan
Chain domain show that Dyna-style planning can result in
a significant improvement in learning speed over modelfree methods. In addition, we can see trends that have been
observed in the tabular case re-occurring here with linear
function approximation. In particular, prioritized sweeping can result in more efficient learning than simply updating features at random, and the MG version of prioritized
sweeping seems to be better than the PWMA version.

Atkeson, C. (1993). Using local trajectory optimizers to
speed up global optimization in dynamic programming.
Advances in Neural Information Processing Systems, 5,
663–670.
Baird, L. C. (1995). Residual algorithms: Reinforcement
learning with function approximation. In Proceedings of the Twelfth International Conference on Machine Learning, pp. 30–37.
Bertsekas, Dimitri P., Tsitsiklis. J. (1996). Neuro-Dynamic
Programming. Athena Scientific, 1996.
Boutilier, C., Dearden, R., Goldszmidt, M. (2000).
Stochastic dynamic programming with factored representations. Artificial Intelligence 121: 49–107.
Bowling, M., Geramifard, A., Wingate, D. (2008). Sigma
point policy iteration. In Proceedings of the Seventh
International Conference on Autonomous Agents and
Multiagent Systems.
Boyan, J. A. (1999). Least-squares temporal difference
learning. In Proceedings of the Sixteenth International
Conference on Machine Learning, 49–56.
Boyan, J. A. (2002). Technical update: Least-squares temporal difference learning. Machine Learning, 49:233–
246.
Bradtke, S., Barto, A. G. (1996). Linear least-squares al-

Finally, we would like to note that we have done extensive experimental work (not reported here) attempting to
adapt least squares methods such as LSTD to online control domains, in particular to the Mountain Car problem. A
major difficulty with these methods is that they place equal
weight on all past data whereas, in a control setting, the policy changes and older data becomes less relevant and may
even be misleading. Although we have tried a variety of
forgetting strategies, it is not easy to obtain online control
performance with these methods that is superior to modelfree methods. One reason we consider the Dyna approach
to be promising is that no special changes are required for
this case; it seems to adapt much more naturally and effectively to the online control setting.


ions

Alejandro Isaza and Csaba Szepesvári and Vadim Bulitko and Russell Greiner
Department of Computing Science, University of Alberta
Edmonton, Alberta, T6G 2E8, CANADA
{isaza,szepesva,bulitko,greiner}@cs.ualberta.ca

Abstract
In this paper, we consider planning in stochastic
shortest path (SSP) problems, a subclass of
Markov Decision Problems (MDP). We focus on
medium-size problems whose state space can be
fully enumerated. This problem has numerous
important applications, such as navigation and
planning under uncertainty. We propose a new
approach for constructing a multi-level hierarchy
of progressively simpler abstractions of the
original problem. Once computed, the hierarchy
can be used to speed up planning by first finding
a policy for the most abstract level and then recursively refining it into a solution to the original
problem. This approach is fully automated and
delivers a speed-up of two orders of magnitude
over a state-of-the-art MDP solver on sample
problems while returning near-optimal solutions.
We also prove theoretical bounds on the loss
of solution optimality resulting from the use of
abstractions.

1

Introduction and Motivation

We focus on planning in stochastic shortest path problems
(the problem of reaching some goal state under uncertainty)
when planning time is critical — a situation that arises, for
instance, in path planning for agents in commercial video
games, where map congestions are modeled as uncertainty
of transitions. Another example is path planning for
multi-link robotic manipulators, where the uncertainty
comes from unmodeled dynamics as well as sensor and
actuator noise. More specifically, we consider the problem
of finding optimal policies in a sequence of stochastic
shortest-path problems (Bertsekas & Tsitsiklis, 1996),
where the problems share the same dynamics and transition
costs, and differ only in the location of the goal-state.
When the state space underlying the problems is sufficiently large, exact planning methods are unable to
deliver a solution within the required time, forcing the
user to resort to approximate methods in order to scale to
large domains. Exploiting the fact that multiple planning

problems share the same dynamics and transition costs, we
build an abstracted representation of the shared structure
where planning is faster, then map the individual planning
problem into the abstract space and derive a solution there.
The solution is then refined back into the original space.
In a related problem of path planning under real-time
constraints in deterministic environments (e.g., Sturtevant,
2007), a particularly successful approach is implemented
in the PR LRTS algorithm (Bulitko, Sturtevant, Lu, & Yau,
2007), which builds an abstract state space by partitioning
the set of states into cliques (i.e., each state within each
cluster is connected to each other state in that cluster
with a single action). Each such cluster becomes a single
abstract state. Two abstract states are connected by an
abstract transition if there is a pair of non-abstract states
(one from each abstract state) connected by a single
action. The resulting abstract space is smaller and simpler,
yet captures some of the structure of the original search
problem. Thus, an abstract solution can be used to guide
and constrain the search in the original problem, yielding
a significant speed-up. Further speed-ups can be obtained
by building abstractions on top of abstractions, which
creates a hierarchy of progressively smaller abstract search
spaces. PR LRTS can then be tuned to meet strict real-time
constraints while minimizing solution suboptimality.
Note that state cliques produced by PR LRTS make good
abstract states because landing anywhere in such a cluster
puts the agent a single action away from any other state in
the clique. This also means that the costs of the resulting
actions are similar, and that the cost of a single action
is negligible compared with the cost of a typical path.
Finally, any (optimal) path in the original problem can
be closely approximated at the abstract level, as an agent
following an (optimal) path has to traverse from cluster to
cluster. Since all neighboring clusters are connected in the
abstract problem, it is always possible to find a path in the
abstract problem that is “close” to the original path.
Given the attractive properties or PR LRTS, it is natural
to ask whether the ideas underlying it can be extended
to stochastic shortest path problems, with arbitrary cost
structures.
In a stochastic problem, the result of planning is a closed
loop policy that assigns actions to states. A successful ab-

straction must be suitable for approximating the execution
trace of an optimal policy. Imagine that clustering has been
done in some way. The idea is again to have abstract actions that connect neighboring clusters cheaply — that is,
the system should not produce expensive connections.
Intuitively, we want to connect one cluster to another if,
from any state of the first cluster, we can reliably get to
some state of the second cluster at roughly a fixed cost (the
same for any state in the first cluster). This way, “simulating” a policy of the original problem becomes possible
at a small additional cost (the meaning of simulation will
become clear later). This means that a connection between
clusters is implemented by a policy with a specific set
of initial states that brings the agent from any state of
the source cluster to some state of the target cluster. We
will use options (Sutton, Precup, & Singh, 1999) for such
policies, and choose clusters to allow such policies for any
two neighboring clusters. Thus, it is natural to look for
clusters of states that allow one to reliably simulate any
trajectory from any of the states to any other state.
Finally, we need an extra mechanism, the “goal approach”,
that deals with the challenge of reaching the base-level
goal itself from states that are close to the goal. Thus, our
planner first plans in the abstract space to reach the “goal
cluster”. After arriving at some state of the “goal approach
region”, the planner then uses the “goal-approach policy”
that, with high probability, moves the agent to the goal
state itself. These ideas form the core of our algorithm.

The MDP is undiscounted if γ = 1. An action
a ∈ ∪x∈X A(x) is called admissible in state x if a ∈ A(x).
Definition 2 A (generic) policy is a mapping that assigns
to each history (x0 , a0 , c0 , . . . , xt−1 , at−1 , ct−1 , xt ) an
action admissible in the most recent state xt . In general, a
mapping that maps possible histories to some set is called
a history dependent mapping.
Under mild conditions, it suffices to consider only stationary, deterministic policies (Bertsekas & Tsitsiklis, 1996),
on which we will focus:
Definition 3 A stationary and deterministic policy π is a
mapping of states to actions such that π(x) ∈ A(x) holds
for any state x ∈ X.
In what follows, we will use “policy” to mean stationary
and deterministic policies, unless otherwise mentioned.
The expected cost of policyPπ when the system starts
∞
in state x0 is vπ (x0 ) = E [ t=0 γ t c(Xt , π(Xt ), Xt+1 )]
where Xt is a Markov chain with P (Xt+1 = y|Xt = x) =
p(y|x, π(x)). The function vπ is called the value-function
underlying policy π.
One “solves” an MDP by finding a policy that minimizes
the cost from every state, simultaneously. In this paper
we deal only with stochastic shortest path problems, a
subclass of MDPs. In these MDPs the problem is to get to
a goal state with the least cost:

The three major contributions of the paper are: (i) a
novel theoretical analysis of option-based abstractions,
(ii) an effective algorithm for constructing high-quality
option-based abstractions, and (iii) experimental results
demonstrating that our algorithm performs effectively over
a range of problems of varying size and difficulty.

Definition 4 A finite stochastic shortest path (SSP)
problem is a finite undiscounted MDP that has a special
state, called the goal state g, such that ∀a ∈ A(g), we have
p(g|g, a) = 1 and c(g, a, g) = 0 and the immediate costs
for all the other transitions are positive.

Section 2 formally describes our problem, and provides
the theoretical underpinning of our approach. Section 3
then presents our algorithm for automatically building
options-based abstractions, and Section 4, our planning
algorithm that uses these abstractions. Section 5 empirically evaluates this approach, in terms of both efficiency
and effectiveness (suboptimality). Finally, Section 6
summarizes related work.

Consider a finite SSP (X, A, p, c). Let π be a stationary
policy. We say that this policy is proper if it reaches the
goal state g with probability one, regardless of the initial
state. Let Tπ : RX → RX be the policy’s evaluation
operator:
X
(Tπ v)(x) =
p(y|x, π(x)) [c(x, π(x), y) + v(y)] .
y∈X

2

Problem Formulation and Theory

This section formally defines stochastic shortest path
problems and the abstractions that we will consider. It
also presents a theoretical result that characterizes the
relationship between the performance of abstract policies
and policies of the original problem.
Definition 1 A Markov Decision Process (MDP) is defined by a finite state space X = {1, . . . , n}; a finite set of
actions A(x) for each state x ∈ X; transition probabilities
p(y|x, a) ∈ [0, 1] that correspond to the probability that
the next state is y when action a is applied in state x;
immediate cost c(x, a, y) ∈ ℜ for all x, y ∈ X and all
a ∈ A(x) and a discount factor γ ∈ (0, 1].

Bertsekas and Tsitsiklis (1996) prove that Tπ is a contraction with respect to a weighted maximum norm,
k·kw,∞ , with some positive weights, w ∈ RX
+ , where
kvkw,∞ = maxx |v(x)|/w(x). In particular, w(x) can be
chosen to be the expected number of steps until π reaches
the goal state when started from x. The contraction coefficient of Tπ , γπ , satisfies 1/(1 − γπ ) = maxx∈X w(x).
Thus, 1/(1 − γπ ) is the maximum of the expected number
of steps to reach the goal state, or in other words, the
maximum expected time policy π spends in the MDP (cf.
Prop 2.2 in Bertsekas & Tsitsiklis, 1996).
We adopt the notion of options from Sutton et al. (1999):
Definition 5 An option is a triple (π, I, ψ), where I ⊂ X
is the set of initial states, π is a (generic) policy that is

defined for histories that start with a state in I and ψ is
a history dependent mapping with range {0, 1}, called
the terminating condition. We say that the terminating
condition fires when ψ(ht ) = 1. Let T > 0 denote the
random time when the terminating condition fires for the
first time while following π. (Note that T = 0 is not
allowed.) We assume that P (T < +∞) = 1, independent
of the initial state when the policy π is started (i.e., the
option terminates in finite time with probability one).
As suggested in the introduction, an abstraction is a way to
group states and the abstract actions correspond to options:
Definition 6 We say that the MDP (X̃ , Ã, p̃, c̃) is an
option-based abstraction of (X, A, p, c), if there exists a
mapping, S : X̃ → 2X specifying the states S(x̃) ⊂ X
that correspond to an abstract state x̃ ∈ X̃ , a set Π of
options abstracting the actions of the MDP and a mapping
Ψ : ∪x̃∈X̃ Ã(x̃) → Π such that for any ã ∈ Ã(x̃), if
Ψ(ã) = (I, π, ψ) then S(x̃) ⊂ I.1
Henceforth we will use “abstraction” instead of “optionbased abstraction” and will call (X̃ , Ã, p̃, c̃) the “abstract
MDP”, X̃ the set of abstract states, Ã the set of abstract
actions, etc. Notationally, we call (X, A, p, c) the ground
level MDP, and we will identify quantities related to the
abstract MDP by using a tilde ( ˜ ). For simplicity, we
will identify the abstract actions with their corresponding
options. In particular, we will call ã both an abstract action
and an option, depending on the context.
In the following, we will assume that {S(x̃) | x̃ ∈ X̃ } is
a partition of X; we can then let x̃ : X → X̃ denote the
(unique) abstract state that includes x: x̃(x) ∈ X̃ such that
x ∈ S(x̃(x)), and say that (X̃ , S) is an aggregation of the
states in X. We also define S(x) = S(x̃(x)) as the set of
states in X that are in the same partition with x.
The restriction on Ψ in the above definition ensures that
the execution of any policy π̃ in the abstract MDP is
well-defined and proceeds as follows. Initially, there is
no active option. In general, whenever there is no active
option, we look up the abstract state x̃ = x̃(x) based on
the current state x and activate the option Ψ(π̃(x̃)). When
there is an active option, the option remains active until
the corresponding terminating condition fires. When an
option is active, the option’s policy selects the actions in
the ground level MDP. This way a policy π̃ in the abstract
MDP induces a policy in the ground level MDP.
Our goal now is to characterize what makes an abstraction
accurate. The following theoretical analysis is novel as it
considers abstractions where the action set is changed. In
particular, the action set can potentially be reduced and
the abstract actions can be options. To our knowledge,
such options-based abstractions have not been analyzed
previously; the closest results are probably Theorem 2 of
Kim and Dean (2003) and Theorem 4 of Dean, Givan, and
Leach (1997). The proof is rather technical and is given
1 X

2

denotes the power set of X: the set of all subsets of X.

in the extended version of our paper (Isaza, Szepesvári,
Bulitko, & Greiner, 2008).
Consider a proper policy π of the ground level MDP. We
want abstractions such that one can always find a policy in
the abstract MDP (X̃ , Ã, p̃, c̃) that approximates π well, no
matter how π was chosen. Clearly, this depends on how the
action set Ã and the corresponding transitions and costs
are defined in the abstract MDP. Quantifying this requires a
few definitions: Let p̃π (x̃, ỹ) be the probability of landing
in some state of S(ỹ) when following policy π until it
leaves the states of S(x̃), when the initial state is selected
at random from the states of S(x̃) based on the distribution
µS(x̃) . Let c̃π (x̃) denote the corresponding expected
“immediate” cost. Now pick a proper policy π̃ of the
abstract MDP. Let w̃ be the weight vector that makes Tπ̃ a
contraction in the abstract MDP.
P Further, define p̃π̃ (x̃, ỹ) =
p̃(ỹ|x̃, π̃(x̃)) and c̃π̃ (x̃) = ỹ∈X̃ p̃π̃ (x̃, ỹ)c̃π̃ (x̃, ỹ) and the
mixed ℓ1 /ℓ∞ norm k·kw̃,1/∞ :
kp̃1 − p̃2 kw̃,1/∞ = max
x̃∈X̃

X

ỹ∈X̃

|p̃1 (x̃, ỹ) − p̃2 (x̃, ỹ)|

w̃(ỹ)
.
w̃(x̃)

Let
επ,π̃ = kc̃π − c̃π̃ kw̃,∞ + cmax kp̃π − p̃π̃ kw̃,1/∞ ,

(1)

where cmax is the maximum of the immediate costs in the
ground level MDP. Hence, επ,π̃ measures how well the
costs and the transition probabilities induced by π “after
state aggregation” match those of π̃. Introduce c(x, π) as
the expected total cost incurred, conditioned on that policy
π starting in state x and stopping when it exits S(x). Further, introduce p(ỹ|x, π) as the probability that, given that
policy π is started in state x, when it exits S(x) it enters
S(ỹ) (ỹ 6= x̃(x)). Now fix an abstract state x̃ ∈ X̃ . If the
costs {c(x, π)}x∈S(x̃) and probabilities {p(ỹ|x, π)}x∈S(x̃) ,
ỹ 6= x̃, have a small range then we can model closely the
behavior of π locally at S(x̃) by introducing an option
with initial states in S(x̃) which mimics the “expected”
behavior of π as it leaves S(x̃), assuming, say, that the
initial state in S(x̃) is selected at random according to
the distribution µS(x̃) . If we do so for all abstract states
x̃ ∈ X̃ then we can make sure that minπ επ,π̃ is small. If
the above range conditions hold for all policies π of the
ground level MDP and all abstract states x̃ ∈ X̃ then by
introducing a sufficiently large number of abstract actions
it is possible to keep maxπ minπ̃ επ,π̃ small. Further,
notice that maxπ p(ỹ|x, π) is zero unless there exists a
transition from some state of S(x) to some state of S(ỹ),
in which case we say that S(x) is connected to S(ỹ).
Hence, no abstract action is needed “between” x̃ and ỹ,
unless S(x̃) is connected in the ground level MDP to S(ỹ).
Define T̃P
: B(X̃) → B(X̃), (T̃π ṽ)(x̃) =
π
c̃π (x̃) +
p̃
Since π is proper in the
ỹ π̃ (x̃, ỹ)ṽ(ỹ).
ground level MDP, it is not difficult to show that T̃π is
a contraction with respect to an appropriately defined
weighted supremum norm.
The next result gives a bound on the difference of value
functions of π and π̃ in terms of επ,π̃ :

Theorem 1 Let π be a proper policy in the ground level
MDP and let π̃ be a proper policy in the abstract MDP. Let
wπ (resp., w̃π ) be the weight vector that makes Tπ (resp.,
T̃π ) a contraction and let the corresponding contraction
factor be γπ (resp., γ̃π ). Let vπ be the value function of π
and ṽπ̃ be the value function of π̃. Then
kvπ − Eṽπ̃ kw,∞ ≤

kAvπ − vπ kw,∞
1 − γπ

+ λπ

επ,π̃
,
1 − γ̃π

where the operator E extends functions defined over X̃
to functions defined over X in a piecewise constant manner: E : B(X̃ ) → B(X), (Eṽ)(x) = ṽ(x̃(x)), and
A : B(X) → B(X) is the aggregation operator defined by
X
(AV )(x) =
µS(x) (z)V (z),
z∈S(x)

and λπ = maxx∈X w̃π (x̃(x))/wπ (x).
The factor λπ measures how many more steps are needed
to reach the goal if the execution of policy π is modified
such that, whenever the policy enters a new cluster x̃, the
state gets perturbed, by choosing a random state according
to µS(x̃) .
The theorem provides a bound on the difference between
the value function of a ground-level policy π and the value
function of an abstract policy when its value function is
extended to the ground-level states. The bound has two
terms: The first bounds the loss due to state abstraction,
while the second bounds the loss due to action abstraction.
When a similar range condition holds for the abstract
actions, too, then it is possible to bound the difference
between the value function of the policy induced in the
ground level MDP by π̃ and Eṽπ̃ , yielding a difference on
the value functions of π and the policy induced by π̃. Isaza
et al. (2008) provides further details.
If we apply this result to an optimal policy π ∗ of the
ground level MDP, we immediately get a bound on the
quality of the abstraction. We may conclude then that
the quality of abstraction is determined by the following
factors: (i) whether states with different optimal values are
aggregated; (ii) whether the random perturbation described
in the previous paragraph can increase the number of steps
to the goal substantially; and (iii) whether the immediate
costs c̃π∗ and transition probabilities p̃π∗ can be matched
in the abstract MDP.
Since we want to build abstractions that work independently of where the goal is placed, the knowledge
of the optimal policy with respect to a particular goal
cannot be exploited when constructing the abstractions.
In order to prevent large errors due to (i) and (ii), we
restrict aggregation such that only a few states are grouped
together. This makes the job of creating an aggregation
easier. Fortunately, we can achieve higher compression by
adding additional layers of abstractions. We can address
(iii) by creating a sufficiently large number of abstract
actions. Here, we use the simplifying assumption that we
only create abstract actions that bring the agent from some
cluster of states to some neighboring cluster. These can

serve as a “basis” for matching any complex next-state
distribution over the clusters by choosing an appropriate
stochastic policy in the abstract MDP. We also want to
ensure that the initial state within a cluster has a small
influence on the probability of transitioning to some
neighboring cluster and the associated costs. We use two
constants, ε and µ, to bound the amount of variation with
respect to initial states; note this allows us to control the
difference between the value function of a policy induced
in the ground level MDP by some abstract policy π̃ and the
extension of the value function of π̃ defined in the abstract
MDP to the ground level states, Eṽπ̃ . This is necessary to
ensure that a good policy in the abstract MDP produces a
good policy in the ground-level MDP, ultimately assuring
that the optimal policy of the abstract MDP will give rise
to a close to optimal policy in the ground-level MDP. The
resulting procedure is described in the next section.

3

Abstracting an SSP

This section describes our algorithm BuildAbstraction
for automatically building options-based abstractions.
These abstractions are goal-independent and thus apply
to a series of SSPs that share the state space and transition dynamics. The process consists of four main steps
(Figure 1): (1) Cluster proposes candidates for abstract
states; (2) GenerateLinkCandidates proposes candidates
for abstract actions (or “links”); (3) Repair validates and, if
necessary, repairs the links in order to satisfy the so-called
(ε, µ)-connectivity property (the formal definition is given
later) and Prune discards excessive links.
Once an abstraction is built, we use a special-purpose
planning procedure (described in Section 4) to solve
specific SSPs. The rest of this section describes the four
steps of our BuildAbstraction algorithm in detail.
Step 1: Cluster. A straightforward cluster-er will cluster a
state with some of its immediate neighbors. Unfortunately,
this approach may group states with diverging trajectories
(the trajectories from one state can differ from those of the
other state). By looking for the peers of a state (predecessors of its successors, line 2, Figure 1) we hope to find a
peer whose trajectories are similar to the trajectories of the
first state. Note that the clustering routine creates minimal
clusters. This is advantageous as it means the subsequent
steps, which connect clusters, is more likely to succeed.
Unfortunately, it also means relatively low reduction in the
number of states. Several layers of abstractions can help
increase this reduction.
Step 2: Generate Link Candidates. After forming the
initial clusters (i.e., the initial abstract states), BuildAbstraction generates candidates for abstract actions. One
approach is simply to propose abstract actions for all pairs
of abstract states, in the hope that only important ones will
remain after pruning. We use a less expensive strategy and
propose abstract action candidates only for “nearby” clusters (line 8). For each such pair we add two candidate links:
one in the forward and another in the backward direction —
this heuristic quickly generates reasonable link candidates.
We typically use k = 1. Our experiments confirm this is

BuildAbstraction(k, p, M ) // M – ground level MDP
–Cluster–
1 for each unmarked ground state x do
2
Find P (x), all the predecessors of successors of x
3
Find y ∈ P (x) that has the most successors
in common with x
4
Add x̃ to X̃ with S(x̃) = {x, y}
5
Mark states {x, y}
6 end for
–GenerateLinkCandidates–
7 repairQ ← ∅
8 for every x̃, ỹ ∈ X̃, where any state in S(ỹ) is
within k ground transitions of some state in S(x̃) do
9
repairQ ← repairQ ∪ {(x̃, ỹ), (ỹ, x̃)}
10 end for
–Repair–
11 while repairQ 6= ∅ do
12
(x̃, ỹ) ← pop an element from repairQ
13
set up an SSP, S, with domain R ⊂ X
where S(x̃)∪S(ỹ) ⊂ R with states in S(ỹ) as goals
14
attempt to find an optimal policy πS in S with IPS
15
if no policy found then
16
continue
17
else if πS does not meet the (ε, µ) conditions then
18
split the cluster adding both parts to repairQ
19
else
20
add ã to Ã(x̃) with Ψ(ã) = (S(x̃), πS , IS(ỹ) ) 2
21
set c̃(x̃, ã) to be the expected cost of
executing ã from a random state of x̃
22
set p̃(ỹ|x̃, ã) = 1, p̃(ỹ ′ |x̃, ã) = 0 for ỹ ′ 6= ỹ.
23
end if
24 end while
–Prune–
25 for each state x̃ do
26
find Ã∗ (x̃) = {ã1 , . . . , ãm }, all abstract actions
that connect clusters that are neighbors in M
27
order Ã(x̃) \ Ã∗ (x̃) to create [ãm+1 , . . . , ãn ] such
that c̃(x̃, ãi ) ≤ c̃(x̃, ãi+1 ), i = m + 1, . . . , n − 1
28
let Ã(x̃) = {ã1 , . . . , ãp }
29 end for
30 return (X̃, Ã, p̃, c̃).

Figure 1: The abstraction algorithm.
sufficient; increasing k results in slightly better quality, but
slower running times when solving the planning problems.
Step 3: Repair. For each candidate abstract action connecting abstract states x̃ and ỹ, we first need to derive an option
that, starting in any state in cluster x̃ leads the agent to some
state in cluster ỹ with a minimum total expected cost. We
derive this option by setting up a shortest path problem S,
whose domain includes S(x̃) and S(ỹ). We set the domain
of S to be sufficiently large that a policy within this domain
can reliably take the agent from any state of S(x̃) to some
state of S(ỹ). BuildAbstraction builds this domain by
performing a breadth-first search from S(ỹ), proceeding
backwards along the transitions, stopping at depth D + m,
where D is the search depth from S(ỹ) and m is the margin
to leave after all states of S(x̃) were added to the domain.
If there is any state of S(x̃) that was not included at depth
D, the Repair routine reports ‘no solution’. The transitions,
actions and costs of S are inherited from the MDP M . We
also add a new terminating state, which is the destination
2
Here IS is the characteristic function of S: IS (x) = 1 iff
x ∈ S and IS (x) = 0 otherwise.

of transitions leaving the region — i.e., those transitions
are redirected to this new terminal, with a transition cost
that exceeds the maximum of the total expected costs of the
ground level MDP. The high cost discourages the solutions
to enter the extra terminating state. The optimal solution
to S is obtained by using the Improved Prioritized Sweeping (IPS) algorithm of McMahan and Gordon (2005),
(line 14). We selected this algorithm based on its excellent
performance and known optimality properties (IPS reduces
to Dijkstra’s method in deterministic problems). The
resulting policy π is checked against (ε, µ)-connectivity,
defined as follows: we first compute the expected total
cost of reaching some states in S(ỹ) for all states of S(x̃);
let the resulting costs be c(x, π). Similarly, we compute
the probabilities p(S(ỹ)|x, π) for every x ∈ S(x̃). Then
we check if maxx,x′ ∈S(x̃) |c(x, π) − c(x′ , π)| ≤ ε and
maxx,x′ ∈S(x̃) |p(S(ỹ)|x, π) − p(S(ỹ)|x′ , π)| ≤ µ both
hold. If these constraints are met, a new abstract action
is created and is added to the set of admissible actions
at x̃ and the policy is stored as the option corresponding
to this new abstract action (lines 20–22). Otherwise, the
cluster is split (since every cluster has two states, this is
trivial) and the appropriate link candidates are added to the
repair queue so that no link between potentially connected
clusters is missed.
Step 4: Prune. After step 3, we have an abstract SSP whose
abstract states are (ε, µ)-connected. However, our abstract
action generation mechanism may produce too many
actions, which may slow down the planning algorithm (see
Section 4). We address this problem using a pruning step
that leaves only the “critical” and cheapest abstract actions.
An action is “critical” if it connects clusters that are
connected at the ground level with a single transition; these
actions are important to keep the structure of the ground
level MDP. We also keep the cheapest abstract actions
as they are likely to help achieve high quality solutions.
The “pruning parameter”, p, specifies the total number of
actions to keep. (If p is smaller or equal than the number of
ground actions, then only the “critical” actions are kept.)
BuildAbstraction runs in time linear in the size of the
input MDP, as every step is restricted to some fixed size
neighborhood of some state (i.e., every step is local). Further, employing a suitable data structure, the memory requirements can also be kept linear in the size of the input.
These properties are important when scaling up to realistic,
real-world problem sizes.

4

Planning with an Abstraction

After building an abstraction, we can use it to solve
particular SSP problems. When we specify a new goal,
our abstraction planner, AbsPlanner, then creates a
goal-approach region in the abstract MDP that includes
the goal and is large enough to include all states of the
cluster containing the goal. AbsPlanner builds this region
by starting with the ground goal and adding states and
transitions in a breadth-first fashion to a certain depth,
proceeding backwards along the transitions, stopping only
after adding all states of the goal-cluster. After building the
region, AbsPlanner produces an SSP. The domain of this

SSP includes the states found in the breadth-first search,
and also a new terminal state that becomes the destination
of transitions leaving the region — i.e., those transitions
are redirected to this new terminal, with a high transition
cost. All other costs and transitions of this SSP are inherited from the ground level MDP. AbsPlanner uses IPS to
solve the local MDP, and saves the resulting goal-approach
policy. It then solves the abstract MDP, where the goal
cluster is set as the goal. When executing the resulting
policy π̃, AbsPlanner proceeds normally until reaching a
state of the goal-approach region; it then switches to the
goal-approach policy, which it follows until reaching the
goal or leaving the region. When this latter event happens
and the state is x, execution switches to the option π̃(x̃(x)).
When using multiple levels of abstraction, AbsPlanner’s
execution follows a recursive, hierarchical strategy. Note
that the size of the goal-approach region is independent of
the size of the MDP. Thus, the planning time will depend
on the size of the top-level abstract MDP. For an MDP of
size n, by using log n levels of hierarchy, in theory it is then
possible to achieve planning times that scale with O(log n).
However, depending on the problem, it might be hard to
guarantee high quality solutions when using many levels
of abstraction. Furthermore, in practice (over the problems
used in our tests), the computation time is dominated by the
time needed to set up and solve the goal-approach SSPs,
which is required for even one layer of abstraction. This
is partly because our abstractions result in deterministic
shortest path problems, whose solutions can be found
significantly faster than those of stochastic problems.

5

Empirical Evaluation

This section summarizes our empirical evaluation of this
approach, in terms of the quality (suboptimality) of the
solutions and the solution times. Here we report the tradeoffs of using different levels of abstraction as well as the
dependence on the “stochasticity” of the transitions. (Note
that stochasticity makes it difficult to build abstractions.)
We also tested the performance of the algorithm on more
practical problems. In addition to the results presented
here, we conducted extensive experiments, studying the
trade off between solution quality and solution time as a
function of the various parameters of our algorithm (e.g.,
the values of p, k, or the number of abstraction levels), the
scaling behavior of our algorithm in terms of its resource
usage, the quality of solutions and the solution time. These
results, appearing in (Isaza et al., 2008), confirm that the
algorithm is robust to the choices of its parameters and
scales as expected by increasing problem sizes.
We run experiments over three domains: noisy gridworlds,
a “river” and congested game maps. The gridworlds are
empty and have four actions: up, down, left, and right,
each with cost 1. The probability that an action leaded to
the expected position (e.g., the action up moves the agent
up one cell) is 0.7, while the probability of reaching any of
the other three adjacent cells is 0.1.
The river is similar to the gridworld: its dimensions are
w × h, but there is a current flowing from left to right

and a fork corresponding to a line connecting the points
(w/2, h/2) and (w, h/2).3 The flow is represented by
modifying both the cost structure and the transition probabilities of the actions: action forward costs 1, backward
costs 5, diagonally-up-and-forward
and diagonally-down√
and-forward each cost 2. These actions are also stochastic: For the backward action, the probabilities are 0.7 for
going back and 0.1 for each of the other actions. For the
other three actions, the anticipated move occurs with probability 0.6 and the other moves except backwards occur
each with probability 0.2, and backwards has probability
0. We include the river domain to determine whether our
system can deal with non-uniform structures and because
the fork complicates the task of creating abstractions. We
empirically found the time to build abstractions for the
n-state gridworld was close to n/100 seconds, and around
n/50 for an n-state river domain. The build time for the
maps, using k = 1, was between 75 and 100 seconds.4
The congested game maps are again similar to gridworlds,
but with obstacles and with transitions probabilities that
depend on the congestion. The obstacle layout comes
from commercial game maps, and the stochastic dynamics
simulate what happens if multiple units traverse the same
map: in narrow passages, the units to become congested,
which means an agent trying to traverse such a passage
is likely to be blocked. We model this by modifying
each action by including a probability that the action will
“fail” and cause the agent to stay at the same position.
This “failure probability” depends on the position on the
game map, calculated by simulating many units randomly
traversing the game maps and measuring the average occupation of the individual cells, then turning the occupation
numbers into probabilities. The optimal policy of an agent
in a congested game map will then try to avoid narrow
passages, since the higher probability of traffic congestion
in such regions means an agent takes much longer to get
through those regions.
The baseline performance measures are obtained by
running the state-of-the-art SSP solver algorithm IPS. For
each study, we generate the abstraction and then use it
to solve 1,000 problems, whose start and goal locations
are selected uniformly at random. For each problem we
measure the solution time in seconds and the total solution
cost for both IPS and our method, then compute the
geometric average of the individual suboptimalities and
the individual solution time ratios.
5.1

Abstraction level trade-offs

We used a 100 × 100 gridworld to analyze the trade-offs of
different abstraction levels, with several different parameter configurations. We say a configuration is “dominant” if
it was a Pareto optimal — i.e., if no other configuration is
better in both time and suboptimality.
Figure 2 presents properties of the dominant configurations
3
See (Isaza et al., 2008) for more details, including relevant
pictures.
4
We ran all experiments on a 2GHz AMD Opteron(tm)
Processor with 4GB of RAM running Linux with kernel 2.6.18.

1.25
1.2
1.15

0.46
0.52

0.55
0.58
0.64

0.88

0.76

1.1

0.82

0.94

0.85

1.05
0

0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008
Solution time ratio

Figure 2: Subobtimality versus the solution time ratio as
compared to IPS for different parameter configurations.
The dominant configurations are shown for different levels
of abstraction.
for various abstraction levels. We see that using a smaller
number of abstractions required more time but produced
better solutions (i.e., lower suboptimality), and higher
levels of abstractions required less solution time but
produced inferior solutions (i.e., increased suboptimality).
Note that there are dominant configurations for every level
of abstraction, from 0 to 5.
We obtain a “level 0” abstraction by converting the given
ground-level SSP to deterministic shortest path problem
with the same states. (Recall that our abstraction process
abstracts the state space and produces a deterministic SSP;
here we just used the original state space.) Figure 2 shows
that this transformation provides solutions whose quality
is slightly inferior to the original problem, but it finds this
solution significantly faster (e.g., in 0.005 to 0.0073 of
the time). We also see that these “level 0” solutions are
superior to those based on higher abstraction levels, but
one can obtain these level-i solutions in yet less time.
5.2

0.40

1.3
Suboptimality

level 0
level 1
level 2
level 3
level 4
level 5

Suboptimality vs. speed-up on a 50x50 gridworld
1.35

0.02

Figure 3 plots the suboptimality and the speed-up of
finding a solution using our method, as compared to IPS,
for different values of P . We see that our method loses
optimality as the dynamics becomes noisier (i.e., when P
gets smaller). This is because our abstract actions, trying
to move the agent from one abstract state to the next will
fail with higher probability for noisier dynamics. Note

0.05

that the advantage of our method, in terms of planning
time, becomes larger with increased stochasticity. This is
because our abstractions are deterministic and planning in
a deterministic system is much faster than planning with a
stochastic system.
Figure 4, plotting the absolute values of cost and time
for both our method and IPS, provides another insight: It
shows that for increasing stochasticity both methods are
slowed down, but our method can cope better with this
situation. This figure also confirms that this leads to a loss
in solution quality.
For our method the typical parameters produce a suboptimality of around 1.4 for the river, and around 1.25 for
the gridworld domain. The speed-up for the gridworld is
around 30, while for the river it is around 800.

Sensitivity to Stochasticity of the Dynamics

As the environment becomes noisier, it becomes more difficult to construct a high quality abstraction. This section
quantifies how the solution quality and construction time
relate to noise in the dynamics. In general, we consider an
action “successful” if the agent moves to the appropriate
direction; our gridworld model set the success probability
to P = 0.7, leaving a probability of (1 − P )/3 to moving
in each of the other three directions. Here, we vary the
value of P . All of these experiments use a 50 × 50
gridworld with k = 2 and p = 4 (which means we keep
only the “critical” actions; see Section 3).

0.03
0.04
Solution time ratio

Figure 3: Subobtimality versus the solution time ratio as
compared to IPS for different values of P .

Cost

Suboptimality

Suboptimality vs. Speed-up on a 100x100 gridworld
1.7
1.65
1.6
1.55
1.5
1.45
1.4
1.35
1.3
1.25
1.2
1.15

Cost vs. solution time trade-off in a 50x50 gridworld
200
0.40
IPS
180
Abstraction
160
140
0.40
120
0.46
100
0.46
0.52
80
0.52
0.58
60
0.64
0.58
0.64
0.76
40
0.76
0.94
0.94
20
0.01
0.1
1
10
100
Solution time (s)

Figure 4: Cost versus solution time for IPS and abstraction
at different values of P .
5.3

Congested Game Maps

To test the performance of our approach in a more practical
application, we used maps modeled after game environments from commercial video games. We first created
simplified gridworlds that resemble some levels from a

Figure 5: A congested game map.
Darker/redder
color refers to high congestion. Dark blue regions are
impassable obstacles.
popular role-playing and real-time-strategy game. We then
converted the gridworlds into congested maps as described
earlier. This produced maps with state space sizes of
6176 (BG1), 5672 (BG2), 5852 (BG3), 20249 (WC1) and
9848 (WC2). Figure 5 provides one such map, where
each state’s color indicates the associated congestion:
warmer/redder colors indicates high congestion (i.e.,
low probability of success P ) while colder/bluer colors
indicates low congestion (i.e., high value of P ). Very dark
blue indicates impassable obstacles. We see that many
of the states in cluttered regions are highly congested and
should therefore be avoided.
Figure 6 shows the solution time and the solution suboptimalities for both our method and IPS, for two maps from
WarCraft (Blizzard Entertainment, 2002) and three maps
from Baldur’s Gate (BioWare Corp., 1998), including
Figure 5, using only a single layer of abstraction. We see
that our approach is indeed successful in speeding up the
planning process, while keeping the quality of the resulting
solutions high.

6

Related Work

Due to space constraints we review only the most relevant
work; references to other related works can be found in the
extensive bibliography lists of the cited works. Dean et al.
(1997) introduced the notion of ε-homogeneous partitions
and analyzed its properties, but without giving explicit
loss bounds. Kim and Dean (2003) developed some loss
bounds. Their Theorem 2 can be strengthened with our
proof method to kv ∗ − vP∗ k∞ ≤ kT vP∗ − vP∗ k∞ /(1 − γ)
(using our notation), basically dropping the first term
in their bound. Here v ∗ is the optimal value function
in the original MDP, vP∗ is the optimal value function
of the aggregated MDP extended back to the original
state space in a piecewise constant manner and T is the
Bellman-optimality operator in the original MDP. This
bound is problematic as it does not show how the quality of

partitions influences the loss. Our bound improves on this
bound in this respect, and also by extending it to the case
when the abstract actions correspond to options. While
Asadi and Huber (2004) also considered such optionsbased abstractions, they assume that the abstract actions
(options) are given externally (possibly by specifying goal
states for each of them) and they do not develop bounds.
In a number of subsequent papers, the authors refined
their methods. In particular, they became increasingly
focussed on learning problems. For example, in the recent
follow-up work, Asadi and Huber (2007) provide a method
to learn an abstract hierarchical representation that uses
state aggregation and options. Since they are interested
in skill transfer through a series of related problems that
can differ in their cost structure, they introduce a heuristic
to discover subgoals based on bottleneck states. They
learn options for achieving the discovered subgoals and
introduce a partitioning that respects the learned options
(in the clusters typically there are many states). The
success of the approach relies critically on the existence of
meaningful bottleneck states. This leads to a granularity
issue: identifying the bottleneck states requires computing
a statistic for each state visited, meaning bottlenecks
will not be pronounced if resolution is increased in
narrow pathways. Nevertheless, the approach has been
successfully tested in a non-trivial domain of 20,000 states.
Hauskrecht, Meuleau, Kaelbling, Dean, and Boutilier
(1998) introduce a method that also uses options, but the
abstract states correspond to boundary states of regions.
The regions are assumed to be given a priori. The idea is
similar to using bottleneck states. In contrast to that work,
we do not assume any prior knowledge, but construct
the abstractions completely autonomously. Further, we
deal with undiscounted SSPs, while Hauskrecht et al.
(1998) dealt with discounted MDPs (but this difference is
probably not crucial).

7

Discussion and Future Directions

In the approach presented, options serve as closed-loop
abstract actions. Another way to use an abstract solution
would be to use the abstract value function to guide local
search initiated from the current state. These ideas has
proven successful in pattern-database research where
the cost of an optimal solution of an abstract problem
is used as a powerful heuristic for the original problem.
Such a procedure has the potential to improve solution
quality, while keeping low the cost of the planning steps
interleaved with execution. Another idea is to use the abstraction to select the amount of such local search (i.e., the
depth of the rollouts); these ideas has proven successful in
deterministic environments (Bulitko, Björnsson, Luštrek,
Schaeffer, & Sigmundarson, 2007; Bulitko, Luštrek,
Schaeffer, Björnsson, & Sigmundarson, 2008).
Presently, our abstractions are deterministic. This suggests
two avenues for future work. First, applying advanced
heuristic search methods to such abstractions may lead to
performance gains. Second, in highly stochastic domains,
the abstraction’s determinism may lead to a poor quality
of solution, as the cost of ensuring arrival at an abstract

Solution time for different game maps

Solution suboptimality for different game maps
1.05

IPS
Abstraction

1.04
Suboptimality

Solution time (s)

10

1

0.1

1.03
1.02
1.01

0.01

1
WC 1

WC 2

BG 1

BG 2

BG 3

WC 1

Map

WC 2

BG 1

BG 2

BG 3

Map

Figure 6: Solution times (left) and suboptimalities (right) for several game maps.
state with certainty (or very high probability) can lead to
very conservative and costly paths. Thus, it would be of
interest to investigate stochastic abstractions. One idea
is to modify the way abstract actions are defined: When
planning to connect to abstract states after a solution of the
local SSP is found, with a little extra work we can compute
the probabilities of reaching various neighboring abstract
states under the policy found when the policy leaves the
region of interest.
Yet another avenue for future work would be to move
from a state-based problem formulation to a feature-based
one, assuming that the features describe the states. The
challenge is to design an algorithm that can construct an
abstraction without enumerating all the states, as ours
currently does. Although this paper has not attempted
to address this problem, we believe that the approach
proposed here (i.e., incremental clustering and defining
options by solving local planning problems) is applicable.
Finally, although the present paper dealt only with undiscounted, stochastic shortest path problems, the approach
can be extended to work for discounted problems. This
holds because a discounted problem can always be viewed
as an undiscounted stochastic shortest path problem where
every time step a transition is made to some terminal state
with probability 1 − γ, where 0 < γ < 1 is the discount
factor.

8

Conclusions

This paper has explored ways to speed up planning in
SSP problems via goal-independent state and action
abstraction. We strengthen existing theoretical results, then
provide an algorithm for building abstraction hierarchies
automatically. Finally, we empirically demonstrate the
advantages of this approach by showing that it works
effectively on SSPs of varying size and difficulty.

Acknowledgements
We gratefully acknowledge the insightful comments by the
reviewers. This research was funded in part by the National
Science and Engineering Research Council (NSERC), iCore and
the Alberta Ingenuity Fund.


