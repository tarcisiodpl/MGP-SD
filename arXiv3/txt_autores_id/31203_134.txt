
Previous work has shown that the problem of
learning the optimal structure of a Bayesian network can be formulated as a shortest path finding problem in a graph and solved using A*
search. In this paper, we improve the scalability of this approach by developing a memoryefficient heuristic search algorithm for learning
the structure of a Bayesian network. Instead of
using A*, we propose a frontier breadth-first
branch and bound search that leverages the layered structure of the search graph of this problem so that no more than two layers of the graph,
plus solution reconstruction information, need to
be stored in memory at a time. To further improve
scalability, the algorithm stores most of the graph
in external memory, such as hard disk, when it
does not fit in RAM. Experimental results show
that the resulting algorithm solves significantly
larger problems than the current state of the art.

1 INTRODUCTION
Bayesian networks are a common machine learning technique used to represent relationships among variables in
data sets. When these relationships are not known a priori,
the structure of the network must be learned. A common
learning approach entails searching for a structure which
optimizes a particular scoring function (Cooper and Herskovits 1992; Heckerman, Geiger, and Chickering 1995).
Because of the difficulty of the problem, early approaches
focused on approximation techniques to learn “good” networks (Cooper and Herskovits 1992; Heckerman, Geiger,
and Chickering 1995; Heckerman 1998; Friedman, Nachman, and Peer 1999; Tsamardinos, Brown, and Aliferis
2006). Unfortunately, these algorithms are unable to guarantee anything about the quality of the learned networks.
Exact dynamic programming algorithms have been developed to learn provably optimal Bayesian network struc-

tures (Ott, Imoto, and Miyano 2004; Koivisto and Sood
2004; Singh and Moore 2005; Silander and Myllymaki
2006). These algorithms identify optimal small subnetworks and add optimal leaves to find large optimal networks until finding the optimal network including all variables. Unfortunately, all of these algorithms must store an
exponential number of subnetworks and associated information in memory. Parviainen and Koivisto (2009) recently
proposed a divide-and-conquer algorithm in which fewer
subnetworks are stored in memory at once at the expense
of longer running time. Theoretical results suggest that this
algorithm is slower than dynamic programming when an
exponential number of processors is not available.
Yuan et al. (2011) developed an A* heuristic search formulation based on the dynamic programming recurrences to
learn optimal network structures. The algorithm formulates
the learning problem as a shortest-path finding problem in
a search graph. Each path in the graph corresponds to an
ordering of the variables, and each edge on the path has a
cost that corresponds to the choice of an optimal parent set
for one variable out of the variables that appear earlier on
the path. Together, all the edges on a path encode an optimal directed acyclic graph that is consistent with the path.
The solution to the shortest-path finding problem then corresponds to an optimal Bayesian network structure. The A*
algorithm also uses a consistent heuristic function to prune
provably suboptimal solutions during the search so as to
improve its efficiency.
de Campos et al. (2009) proposed a systematic search algorithm to identify optimal network structures. The algorithm begins by calculating optimal parent sets for all variables. These sets are represented as a directed graph that
may have cycles. Cycles are then repeatedly broken by removing one edge at a time. The algorithm terminates with
an optimal Bayesian network. However, this algorithm is
shown to often learn the optimal structure slower than the
dynamic programming algorithm (de Campos, Zeng, and Ji
2009).
Optimal networks have also been learned using linear programming (Jaakkola et al. 2010). This technique reformu-

lates the structure learning problem as a linear program. An
exponential number of constraints are used to define a convex hull in which each vertex corresponds to a DAG. Coordinate descent is used to identify the vertex which corresponds to the optimal DAG structure. Furthermore, the
dual of their formulation provides an upper bound which
can help guide the descent algorithm. This algorithm was
shown to have similar or slightly better runtime performance as dynamic programming (Jaakkola et al. 2010).
This paper describes a novel frontier breadth-first branch
and bound algorithm using delayed duplicate detection for
learning optimal Bayesian network structures. The basic
idea is to formulate the learning task as a graph search problem. The search graph decomposes into natural layers and
allows searching one layer at a time. This algorithm improves the scalability of learning optimal Bayesian network
structures in three ways. First, the frontier search approach
allows us to reduce the memory complexity by working
with only a single layer of search graphs at a time during
the search. In particular, we store one layer of each search
graph, the scores required for that layer and information for
solution reconstruction from every previous layer. Other information is deleted. In comparison, previous dynamic programming algorithms have to store an entire exponentiallysized graph in memory. Second, branch and bound techniques allow us to safely prune unpromising search nodes
from the search graphs, while dynamic programming algorithms have to evaluate the whole search space. Finally,
we use a delayed duplicate detection method to ensure that,
given enough hard disk space, optimal network structures
can be learned regardless of the amount of RAM. Previous algorithms fail if an exponential amount of RAM is not
available.
The remainder of this paper is structured as follows. Section 2 provides an overview of the task of Bayesian network learning. Section 3 and 4 introduce two formulations
for solving the learning task: dynamic programming and
graph search. Section 5 discusses the details of the externalmemory frontier breadth-first branch and bound algorithm
we propose in this paper. Section 6 compares the algorithm
against several existing approaches on a set of benchmark
machine learning datasets. Finally, Section 7 concludes the
paper.

2 BACKGROUND
A Bayesian network consists of a directed acyclic graph
(DAG) structure and a set of parameters. The vertices
of the graph each correspond to a random variable
V = {X1 , ..., Xn }. All parents of Xi are referred
to as P Ai . A variable is conditionally independent of
its non-descendants given its parents. The parameters of
the network specify a conditional probability distribution,
P (Xi |P Ai ) for each Xi .

Given a dataset D = {D1 , ..., DN }, where Di is an instantiation of all the variables in V, the optimal structure is the
DAG over all of the variables which best fits D (Heckerman 1998). A scoring function measures the fit of a network structure to D. For example, the minimum description length (MDL) scoring function (Rissanen 1978) uses
one term to reward structures with low entropy and another
to penalize complex structures. Optimal structures minimize the score. Let ri be the number of states of the variable
Xi , let Npai be the number of data records consistent with
P Ai = pai , and let Nxi ,pai be the number of data records
consistent with P Ai = pai and Xi = xi . The MDL score
for a structure G is defined as follows (Tian 2000),

M DL(G) =

X

M DL(Xi |P Ai ),

(1)

i

where

M DL(Xi |P Ai ) =
H(Xi |P Ai ) =
K(Xi |P Ai ) =

log N
H(Xi |P Ai ) +
K(Xi |P Ai ),
2
X
Nxi ,pai
,
(2)
Nxi ,pai log
−
Npai
xi ,pai
Y
(ri − 1)
rl .
Xl ∈P Ai

MDL is decomposable (Heckerman 1998), so the score for
a structure is simply the sum of the score for each variable. Our algorithm can be adapted to use any decomposable function. Some sets of parents cannot form an optimal
parent for any variable, as described in the following theorems from Tian (2000) and de Campos et al. (2009).
Theorem 1. In an optimal Bayesian network based on
the MDL scoring function, each variable has at most
2N
log( log
N ) parents, where N is the number of data points.
Theorem 2. Let U ⊂ V and X ∈
/ U. If
BestM DL(X, U) < BestM DL(X, V), V cannot be the
optimal parent set for X.

3 DYNAMIC PROGRAMMING
Learning an optimal Bayesian network structure is NPHard (Chickering 1996). Dynamic programming algorithms learn optimal network structures in O(n2n ) time
and memory (Ott, Imoto, and Miyano 2004; Koivisto and
Sood 2004; Singh and Moore 2005; Silander and Myllymaki 2006). Because a network structure is a DAG, the optimal structure can be divided into an optimal leaf vertex
and its parents as well as an optimal subnetwork for the rest
of the variables. This subnetwork is also a DAG, so it can
recursively be divided until the subnetwork is only a single

vertex. At that point, the optimal parents have been found
for all variables in the network and the optimal structure
can be constructed. It has been shown (Silander and Myllymaki 2006) that a more efficient algorithm begins with a 0variable subnetwork and exhaustively adds optimal leaves.
For the MDL scoring function and variables V, this recurrence can be expressed as follows (Ott, Imoto, and Miyano
2004),

M DL(V) =

min {M DL(V \ {X}) +

X∈V

Figure 1: Parent graph for variable X1

BestM DL(X, V \ {X})},
where
BestM DL(X, V \ {X}) =

min

P AX ⊆V\{X}

M DL(X|P AX ).

As this recurrence suggests, all dynamic programming algorithms must perform three steps. First, they must calculate the score of each variable given all subsets of the other
variables as parents. There are n2n−1 of these scores. Then,
BestM DL must be calculated. For a variable X and set
of possible parents V, this function returns the subset of
those parents which minimizes the score for X as well as
that score. There are n2n−1 of these optimal parent sets.
Finally, the optimal subnetworks must be learned. These
subnetworks use BestM DL to learn the optimal leaf for
every possible subnetwork, including the optimal network
with all of the variables. There are 2n optimal subnetworks.
Figure 2: An order graph of four variables

4 GRAPH SEARCH FORMULATION
We first formulate each phase of the dynamic programming
algorithm as a separate search problem, including calculating parent scores, identifying the optimal parent sets, and
learning the optimal subnetworks.
We use an AD-tree-like search to calculate all of the parent scores. An AD-tree (Moore and Lee 1998) is an unbalanced tree which contains AD-nodes and varying nodes.
The tree is used to collect count statistics from a dataset.
An AD-node stores the number of records consistent with
the variable instantiation of the node, while a varying node
assigns a value to a variable. As shown in Equation 2, the
entropy component of a score can be calculated based on
variable instantiation counts. Each AD-node has an instantiation of a set of variables U and the count of records consistent with that instantiation. That count is a value of pai
for all X ∈ V \ U. Furthermore, it is a value of xi , pai
for all X ∈ U with parents U \ {X}. We can use a depthfirst traversal of the AD-tree to compute the scores. Theorem 1 states that only small parent sets can possibly be optimal parents when using the MDL score. All nodes below
the depth specified in the theorem are pruned. The scores
which are not pruned are written to disk for retrieval when

identifying optimal parent sets. We call this data structure
a score cache. Each entry in the score cache contains one
value of M DL(X|P A).
A parent graph is a lattice in which each node stores one
value of BestM DL for different candidate sets of variables. The score cache is used to quickly look up the scores
for candidate parent sets. Figure 1 shows the construction
of the parent graph for variable X1 as a lattice. All 2n−1
subsets of all other variables are present in the graph. Each
node contains one value for BestM DL of X1 and the set
of candidate parents shown. That is, each node stores the
subset of parents from the given candidate set which minimizes the score of X1 , as well as that score. The lattice
divides the nodes into layers. We call the first layer of the
graph, the layer with the single node for {} in Figure 1,
layer 0. A node in layer l has l predecessors, all in layer
l − 1, and considers candidate parent sets of size l. Layer
l has C(n − 1, l) nodes, where C(n, k) is the binomial coefficient. Each variable has a separate parent graph. The
complete set of parent graphs stores n2n−1 parent sets.
An order graph is also a lattice. Each node contains
M DL(V) and the associated optimal subnetwor for one

subset of variables. Figure 2 displays an order graph for
four variables. Its lattice structure is similar to that of
the parent graphs; because it contains subsets of all variables, though, the order graph has 2n nodes. The topmost node in layer 0 containing no variables is the start
node. The bottom-most node containing all variables is the
goal node. A directed path in the order graph from the
start node to any other node induces an ordering on the
variables in the path with new variables appearing later
in the ordering. For example, the path traversing nodes
{}, {X1}, {X1 , X2 }, {X1 , X2 , X3 } stands for the variable
ordering X1 , X2 , X3 . All variables which precede a variable in the ordering are candidate parents of that variable.
Each edge on the path has a cost equal to BestM DL for
the new variable in the child node given the variables in
the parent node as candidate parents. The parent graphs are
used to quickly retrieve these costs. For example, the edge
between {X1 , X2 } and {X1 , X2 , X3 } has a cost equal to
BestM DL(X3 , {X1 , X2 }). Each node contains a subset
of variables, the cost of the best path from the start node
to that node, a leaf variable and its optimal parent set. The
shortest paths from the start node to all the other nodes correspond to the optimal subnetworks, so the shortest path
to the goal node corresponds to the optimal Bayesian network. The lattice again divides the nodes into layers. Nodes
in layer l contain optimal subnetworks of l variables. Layer
l has C(n, l) nodes.

5 AN EXTERNAL-MEMORY FRONTIER
BREADTH-FIRST BRANCH AND
BOUND ALGORITHM
Finding an optimal Bayesian network structure can be considered a search through the order graph. This formulation
allows the application of any graph search algorithm, such
as A* (Yuan, Malone, and Wu 2011), to find the best path
from the start node to the goal node. In particular, such a
formulation allows us to treat the order and parent graphs as
implicit search graphs. That is, we do not have to keep the
entire graphs in memory at once. Dynamic programming
can be considered as a breadth-first search through this
graph (Malone, Yuan, and Hansen 2011). Previous results
show that the scalability of existing algorithms for learning optimal Bayesian networks is typically limited by the
amount of RAM available. To eliminate the constraint of
limited RAM, we introduce a frontier breadth-first branch
and bound algorithm with delayed duplicate detection to
do the search by adapting the breadth-first heuristic search
algorithm proposed by Zhou and Hansen (2003; 2006). It is
also similar to the frontier search described by Korf (2005).
Breadth-first heuristic search expands a search space in order of layers of increasing g-cost with each layer comprising all nodes with a same g-cost. As each node is generated,
a heuristic function is used to calculate a lower bound for

Algorithm 1 A Frontier BFBnB Search Algorithm
procedure EXPAND O RDER G RAPH(l, isP resent, upper, lb, maxSize)
for each MDLl (U) ∈ MDLl do
for each X ∈ V \ U do
s ← MDLl (U) + BMDLl (X|U) − lb(X)
if s > upper then continue
isP resent(U ∪ {X}) ← true
if s < MDLl+1 (U ∪ {X}) then
MDLl+1 (U ∪ {X}) ← s
MDLP l+1 (U ∪ {X}) ← BMDLP l (X|U)
end if
if |MDLl+1 | > maxSize then
writeTempFile(MDLl+1 , MDLP l+1 )
end if
end for
end for
writeTempFile(MDLl+1 , MDLP l+1 )
MDLl+1 , MDLP l+1 ← mergeTempFiles
delete MDLl
end procedure
procedure EXPAND PARENT G RAPH(l, p, isP resent, maxSize)
for each BestMDLl (p|U) ∈ BestMDLl (p do
for each X ∈ V \ U and X 6= p do
S ← U ∪ {X}
if !isP resent(S) then continue
if MDL(p|S) < BMDLl+1 (p|S) then
BMDLl+1 (p|S) ← MDL(p|S)
BMDLP l+1 (p|S) ← S
end if
if BMDLl (p|U) < BMDLl+1 (p|S) then
BMDLl+1 (p|S) ← BMDLl (p|U)
BMDLP l+1 (p|S) ← BMDLP l (p|U)
end if
if |BMDLl+1 (p)| > maxSize then
writeTempFile(BMDLl+1 (p), BMDLSl+1 (p))
end if
end for
end for
writeTempFile(BMDLl+1 (p), BMDLP l+1 (p))
BMDLl+1 , BMDLP l+1 (p) ←mergeTempFiles
delete BMDLl , BMDLP l (p)
end procedure
procedure EXPANDADN ODE(i, U, Du , d)
For j = i + 1 → n do expandVaryNode(j, U, Du , d)
end procedure
procedure EXPANDVARY N ODE(i, U, Du , d)
for j = 0 → ri do
updateScores(U ∪ {Xi }, DXi =j,u )
if d > 0 then expandADNode(i, U ∪ {Xi }, DXi =j,u , d − 1)
end for
end procedure
procedure UPDATESCORES (U, Du )
for X ∈ V \ U do
if MDL(X|U) is null then MDL(X|U) ← K(X|U)
MDL(X|U) ← MDL(X|U) + Nu ∗ log Nu
end for
for X ∈ U do
if M DL(X|U \ {X}) is null M DL(X|U \ {X}) ← K(X|U \ {X})

MDL(X|U \ {X}) ← MDL(X|U \ {X}) − Nu ∗ log Nu
end for
end procedure
procedure MAIN(D, upper, maxSize)
2N
maxP arents ← log log
N
expandADNode(−1, {}, D, maxP arents)
lb ← getBestScores
writeScoresToDisk
isP resent ← {}
for l = 1 → n do
for p = 1 → n do
expandParentGraph(l, p, isP resent, maxSize)
end for
expandOrderGraph(l, isP resent, upper, lb, maxSize)
end for
optimalStructure ← reconstructSolution
end procedure

that node. If the lower bound is worse than a given upper
bound on the optimal solution, the node is pruned; otherwise, the node is added to the open list for further search.
After the search, a divide-and-conquer method is used to
reconstruct the optimal solution.
Algorithm 1 gives the pseudocode for our BFBnB search
algorithm for learning optimal Bayesian networks. The algorithm is very similar to the breadth-first heuristic search
algorithm but has several subtle and important differences.
First, the layers in our search graphs (the parent and order
graphs) do not correspond to the g-costs of nodes; rather,
layer l corresponds to variable sets (candidate parent sets
or optimal subnetworks) of size l. For the order graph,
though, we can calculate both a g- and h-cost for pruning, as described in Section 5.1. We also describe how to
propagate this pruning from the order graph to the parent
graphs. Another difference is that our search problem is a
nested search of order and parent graphs. The layered parent and order graph searches have to be carefully orchestrated to ensure the correct nodes can be accessed easily at
the correct time, as described in Section 5.2. This further
requires the parent scores are stored in particular order, as
described in Section 5.3. Yet another difference is that we
use a variant of delayed duplicate detection (Korf 2008) in
which a hash table is used to detect as many duplicates in
RAM as possible before resorting to external memory, as
described in Section 5.4. Finally, we store a portion of each
order graph node to reconstruct the optimal network structure after the search, as described in Section 5.5.
5.1 BRANCH AND BOUND
We need a heuristic function f (U) = g(U) + h(U) that
estimates the cost of the best path from the start node to
a goal node using order node U. The g cost is simply the
sum of the edge costs of the best path from the start node to
U. The h cost provides a lower bound on the cost from U
to the goal node. We use the following heuristic function h
from Yuan et al. (2011).
Definition 1.
h(U) =

X

BestM DL(X, V\{X}).

(3)

X∈V\U

This heuristic function relaxes the acyclic constraint on the
remaining variables in V \ U and allows them to choose
parents from all of the variables in V. The following theorem from Yuan et al. (2011) proves that the function is
consistent. Consistent heuristics are guaranteed to be admissible.
Theorem 3. h is consistent.
In order to calculate this bound, we must know
BestM DL(X, V\{X}). Fortunately, these scores are calculated during the first phase of the algorithm. Because

the score cache contains every score which could possibly be optimal for all variables, it is guaranteed to have
the optimal score for all variables given any set of parents,
which is BestM DL(X, V \ {X}). Thus, we can identify these scores while calculating the scores when expanding the AD-tree and store them in an array for reuse. The
pseudocode uses the function getBestScores to find these
scores and the array lb to store them.
We can apply BFBnB to prune nodes in the order graph
using the lower bound function in Equation 3; however,
pruning is not directly applicable to the parent graphs. An
optimal parent score BestM DL(X, U) is only necessary
if a node for U is in the order graph. Consequently, if U
is pruned from the order graph, then the nodes for U are
also pruned from the parent graphs. The pseudocode uses
isP resent to track which nodes were not pruned.
We also need an upper bound score on the optimal Bayesian
network for pruning. A search node U whose heuristic
value f (U) is higher than the upper bound is immediately pruned. Numerous fast, approximate methods exist
for learning a locally optimal Bayesian network. We use
a greedy beam search algorithm based on a local search
algorithm described by Heckerman (1998) to quickly find
the upper bound. A more sophisticated algorithm could be
used to find a better bound and improve pruning. The input
argument upper is this bound in the pseudocode.
5.2 COORDINATING THE GRAPH SEARCHES
The parent and order graph searches must be carefully coordinated to ensure that the parent graphs contain the necessary nodes to expand nodes in the order graph. In particular, expanding a node U in layer l in the order graph
requires BestM DL(X, U), which is stored in the node U
of the parent graph for X. Hence, before expanding layer
|U| in the order graph, that layer of the parent graphs must
already exist. Therefore, the algorithm alternates between
expanding layers of the parent graphs and order graph.
Expanding a node U in the parent graph amounts to generating successor nodes with candidate parents U ∪ {X}
for all X in V \ U. For each successor S = U ∪ {X},
the hash table for the next layer is first checked to see if
S has already been generated. If not, the score of using all
of S as parents of X is retrieved from the score cache and
compared to the score of using the parents specified in U.
If using all of the variables has a better score, then an entry
is added to the hash table indicating that, for possible parents S, using all of them is best. Otherwise, according to
Theorem 2, the hash table stores a mapping from S to the
parents in U. Similarly, if S has already been generated,
the score of the existing best parent set for S is compared
to the score using the parents in U. If the score of the parents in U is better, then the hash table mapping is updated
accordingly. Once a layer of the parent graph is expanded,

the whole layer can be discarded as it is no longer needed.
The pseudocode uses BM DLl to store the optimal scores
and BM DLP l to store the optimal parents.

there is no additional work required to arrange the nodes
when writing them to disk.

Expanding a node U in the order graph amounts to generating successor nodes U ∪ {X} for all X in V \ U. To
calculate the score of successor S = U ∪ {X}, the score of
the existing node U is added to BestM DL(X, U), which
is retrieved from parent graph node U for variable X. The
optimal parent set out of U is also recorded. This is equivalent to trying X as the leaf and U as the subnetwork. Next,
the hash table for the next layer is consulted. If it contains
an entry for S, then a node for this set of variables has
already been generated using another variable as the leaf.
The score of that node is compared to the score for S. If
the score for S is better, or the hash table did not contain
an entry for S, then the mapping in the hash table is updated. Unlike the parent graph, however, a portion of each
order graph node is used to reconstruct the optimal network
at the end of the search, as described in Section 5.5. This
information is written to disk, while the other information
is deleted. The pseudocode uses M DLl to store the score
for each subnetwork and M DLP l to store the associated
parent information.

5.3 ORDERING THE SCORES ON DISK

Additional care is needed to ensure that parent and order
graph nodes for a particular layer are accessed in a regular,
structured pattern. We arrange the nodes in the parent and
order graphs in queues such that when node U is removed
from the order graph queue, the head of each parent graph
queue for all X in V \ U is U. So all of the successors of
U can be generated by combining it with the head of each
of those parent graph queues. Once the parent graph nodes
are used, they can be removed, and the queues will be ready
to expand the next node in the order graph queue. Because
the nodes are removed from the heads of the queues, these
invariants hold throughout the expansion of the layer. Regulating such access patterns improves the scalability of the
algorithm because these queues can be stored on disk and
accessed sequentially to reduce the requirement of RAM.
The regular accesses also reduce disk seek time. The pseudocode assumes the nodes are written to disk in this order
to easily retrieve the next necessary node.

Duplicate nodes are generated during the graph searches.
Duplicates in the parent and order graphs correspond to
nodes which consider the same sets of variables (candidate parent sets and optimal subnetworks, respectively).
Because the successors of a node always consider exactly
one more variable in both the parent and order graphs, the
successors of a node in layer l are always in layer l + 1.
Therefore, when a node is generated, it could only be a duplicate of a node in the open list for layer l + 1. In both the
parent and order graphs, the duplicate with the best score
should be kept.

The lexicographic ordering (Knuth 2009) of nodes
within each layer is one possible ordering that ensures the queues remain synchronized. For example,
the lexicographic ordering of 4 variables of size 2
is {{X1 , X2 }, {X1 , X3 }, {X2 , X3 }, {X1 , X4 }, {X2 , X4 },
{X3 , X4 }}. The order graph queue for layer 2 of a dataset
with 4 variables should be arranged in that order. The parent graph queue for variable X should have the same sequence, but without subsets containing X. In the example, the parent graph queue for variable X1 should be
{{X2 , X3 }, {X2 , X4 }, {X3 , X4 }}. As described in more
detail in Section 5.4, the nodes of the graphs must be sorted
to detect duplicates; the lexicographic order ensures that

For large datasets, the score cache can grow quite large.
We write it to disk to reduce RAM usage. Each score
M DL(X, U) is used once, when node U is first generated
in the parent graph for X. As described in Section 5.2, the
parent graph nodes are expanded in lexicographic order;
however, they are not generated in that order. The score
M DL(X, U), U = {Y1 . . . Yl } is needed when expanding node U \ {Yl } in the parent graph for X. Therefore,
the scores must be written to disk in that order. The pseudocode uses the writeScoresT oDisk function to sort and
write the scores to disk in this order.
A file is created for each variable for each layer to store
these sorted scores. The file for a particular layer can be
deleted after expanding that layer in the appropriate parent
graph.
5.4 DUPLICATE DETECTION

For large datasets, it is possible that even one layer of the
parent or order graph is too large to fit in RAM. We use
a variant of the delayed duplicate detection (DDD) (Korf
2008) in our algorithm to utilize external memory to solve
such large learning problems. In DDD, search nodes are
written to a file on disk as they are generated. After expanding a layer, an external-memory sorting algorithm is used to
detect and remove duplicate nodes in the file. The nodes in
the file are then expanded to generate the next layer of the
search. Consequently, the search uses a minimal amount of
RAM; however, all generated nodes are written to disk, so
much work is done reading and writing duplicates.
Rather than immediately writing all generated nodes to
disk, we instead detect duplicates in RAM as usual with a
hash table. Once the open list reaches a user-defined maximum size, its contents are sorted and written to a temporary
file on disk. The open list is then cleared. At the end of each
layer, the remaining contents of the open list and the temporary files are sorted and merged into a single file which
contains the sorted list of nodes from that layer. For rea-

sons described in Section 5.2, the lexicographic ordering
of nodes within a layer is used when sorting. The hash table reduces the number of nodes written to and read from
disk by detecting as many duplicates as possible in RAM.
The pseudocode uses maxSize as the user-defined maximum size. The function writeT empF ile sorts, writes to
disk and clears the open list provided as its argument.
The scores and optimal parent sets are written together on
disk. The function mergeT empF iles performs an external memory merge to detect duplicates in the temp files.
For the parent graphs, both the scores and optimal parent
sets are kept in a single file; however, as described in Section 5.5, the parent information of the order graph must be
stored for the entire search, while the score information can
be deleted after use. Therefore, two separate files are used
to allow the information to easily be deleted.
5.5 RECONSTRUCTING THE OPTIMAL
NETWORK STRUCTURE
In order to trace back the optimal path and reconstruct the
optimal network structure, we write a portion of each node
of the order graph to a disk file once it is expanded during the order graph search. For each order graph node we
write the subset of variables, the leaf variable and its optimal parents. Solution reconstruction works as follows.
The final leaf variable X and its optimal parent set are
retrieved from the goal node. Because the goal node considers all variables, its predecessor in the optimal path is
U = V \ {X}. This predecessor is retrieved from the file
for layer |U|. That node has the optimal leaf and parent
set for that subnetwork. Recursively, the optimal leaves and
parent sets are retrieved until reconstructing the entire network structure. We use this approach instead of the standard divide-and-conquer solution reconstruction because,
as shown in Section 6, it requires relatively little memory.
Furthermore, divide-and-conquer would require regeneration of the parent graphs, which is quite expensive in terms
of time and memory. The pseudocode uses the function
reconstructSolution to extract this information from the
M DLP l files.
5.6 ADVANTAGES OF OUR ALGORITHM
Our frontier breadth-first branch and bound algorithm has
several advantages over previous algorithms for learning
optimal Bayesian networks.
First, our top-down search of the AD-tree for calculating
scores ensures we never need to calculate scores or counts
of large variable sets. The AD-tree method is in contrast to
the bottom-up method used by other algorithms (Silander
and Myllymaki 2006). Bottom-up methods must always
compute the scores, or at least the counts, of large parent
sets in order to correctly calculate the counts required for
the smaller ones. Since our algorithm neither calculates nor

stores these counts and scores, it both runs more quickly
and uses less memory.
Second, the layered search strategy reduces the memory requirements by working with one layer of the parent and
order graphs at a time. Other information can be either discarded immediately or stored in hard disk files for later
use, e.g., the information needed to reconstruct the optimal network structure. Previous formulations, such as PCaches (Singh and Moore 2005) and arrays (Silander and
Myllymaki 2006), could not take advantage of this structure. Singh and Moore propose a depth-first search through
the P-Caches, while Silander and Myllymaki’s approach
identifies the sets according to their lexicographic ordering.
(We use the lexicographic order within each layer, not over
all of the variables.) These approaches can identify neither
optimal parent sets nor optimal subnetworks one layer at a
time. Thus, they must both keep all of the optimal parent
sets and subnetworks in memory.
Third, we prune the order graph using an admissible heuristic function; this further reduces the memory complexity of
the algorithm. Pruning unpromising nodes from the order
graph not only reduces the amount of computation but also
reduces the memory requirement. Furthermore, the savings in running time and memory also propagate to parent
graphs. Dynamic programming algorithms always evaluate
the full order graph.
The duplicate detection method we use lifts the requirement that open lists fit in RAM to detect duplicates. Because our algorithm does not resort to delayed duplicate
detection until RAM is full, our algorithm can still take
advantage of large amounts of RAM. By writing nodes to
disk, we can learn optimal Bayesian networks even when
single layers of the search graphs do not fit in RAM.
Our algorithm also has advantages over other learning formulations. In contrast to the A* algorithm of Yuan et
al.( 2011), we only keep one layer of the order graph in
memory at a time. The open and closed lists of A* keep
all generated nodes in memory to perform duplicate detection. Unlike the systematic search algorithm of de Campos
et al. (de Campos, Zeng, and Ji 2009), we always search
in the space of DAGs, which is smaller than the space of
directed graphs in which that algorithm searches. The LP
algorithm (Jaakkola et al. 2010) uses the same mechanism
to identify optimal parent sets as DP; therefore, it cannot
complete when all optimal parent sets do not fit in memory.

6 EXPERIMENTS
We compared a Java implementation of the externalmemory frontier BFBnB search with DDD (BFBnB) to
an efficient version (Silander and Myllymaki 2006) of dynamic programming which uses external memory written

Dataset
dataset
n
wine
14
adult
14
zoo
17
houseVotes
17
letter
17
statlog
19
hepatitis
20
segment
20
meta
22
imports
22
horseColic
23
spect (heart)
23
mushroom
23
parkinsons
23
sensorReadings
25
autos
26
horseColic (full) 28
steelPlatesFaults 28
flag
29
wdbc
31
epigenetic
33

N
178
30,162
101
435
20,000
752
126
2,310
528
205
300
267
8,124
195
5,456
159
300
1,941
194
569
72,228

DP
1
1
1
7
29
23
27
44
52
123
468
413
438
297
12,747
2,737
30,064
78,487
41,733
OD
OD

Timing Results (s)
BFBnB
A*
0
0
18
11
1
0
5
3
87 116
9
12
9
6
28
42
57
41
54
55
93 117
131 139
372 508
103 130
3,061 OM
1,184 OM
4,251 OM
9,252 OM
12,935 OM
93,682 OM
570,760 OM

SS
171
OT
OT
5,824
OT
OT
202
2,482
OT
3,723
1,410
OT
OT
OT
OT
OT
OT
OT
OT
OT
OT

Space Results (bytes)
DP
BFBnB
1.16E+07 2.72E+05
1.16E+07 1.36E+06
4.81E+07 2.30E+06
4.81E+07 4.39E+06
4.81E+07 9.12E+06
1.82E+08 1.82E+07
3.79E+08 2.73E+07
3.79E+08 3.67E+07
1.67E+09 1.55E+08
1.67E+09 1.52E+08
3.48E+09 2.41E+08
3.48E+09 3.06E+08
3.48E+09 3.14E+08
3.48E+09 2.63E+08
1.51E+10 1.30E+09
3.15E+10 2.19E+09
1.36E+11 1.09E+10
1.36E+11 1.09E+10
2.81E+11 1.55E+10
OD 6.86E+10
OD 2.74E+11

Table 1: A comparison of the running time (in seconds) for Silander and Myllymaki’s dynamic programming implementation (DP), Yuan et al.’s A* algorithm (A*), de Campos et al.’s systematic search algorithm (SS) and our external-memory
frontier breadth-first branch and bound algorithm (BFBnB). The run times are given for all algorithms. Maximum external
memory usage is given for DP and BFBnB. For reference, 1E+09 is 1 gigabyte. ‘n’ is the number of variables. ‘N’ is the
number of records. ‘OT’ means failure to find optimal solutions due to running for more than 2 hours (7,200 seconds, less
than 25 variables) or 24 hours (86,400 seconds, 25 - 29 variables) and not producing a provably optimal solution. ‘OM’
means failure to find optimal solutions due to running out of RAM (16GB). ‘OD’ means failure to find optimal solutions
due to running out of hard disk space (500GB).
in C downloaded from http:/b-course.hiit.fi/bene. We refer to it as DP. Previous results (Silander and Myllymaki
2006) have shown DP is more efficient than other dynamic programming implementations. We also compared
to Yuan et al.’s A* implementation (2011) (A*) and de
Campos et al.’s branch and bound systematic search algorithm (de Campos, Zeng, and Ji 2009) (SS) downloaded
from http://www.ecse.rpi.edu/ cvrl/structlearning.html. We
did not include comparison to the DP implementation of
Malone et al. (2011) (MDP) because the codebase is similar; however, MDP does not incorporate pruning or delayed
duplicate detection. The running times of BFBnB and MDP
are similar on datasets which both complete, but, due to
duplicate detection, MDP fails when an entire layer of the
order graph does not fit in RAM.
Benchmark datasets from the UCI repository (Frank
and Asuncion 2010) were used to test the algorithms.
We also constructed a biological dataset consisting of
ChIP-Seq data for epigenetic features downloaded from
http://dir.nhlbi.nih.gov/papers/lmi/epigenomes/hgtcell.html

and
http://dir.nhlbi.nih.gov/papers/lmi/epigenomes/
hgtcellacetylation.aspx. The experimental datasets
were normalized using linear regression using the IgG control dataset downloaded from
http://home.gwu.edu/∼wpeng/Software.htm. The largest
datasets in the comparison have up to 33 variables and
over 70,000 records. Continuous and discrete variables
with more than four states were discretized into two
states around the mean. Records with missing values were
removed.
DP and SS do not calculate the MDL score for a network;
however, they can calculate BIC. The score uses an equivalent calculation as MDL, so the algorithms always learned
equivalent networks. The experiments were performed on a
3.07 GHz Intel i7 with 16GB of RAM, 500GB of hard disk
space and running Ubuntu version 10.10. On datasets with
less than 25 variables, all algorithms were given a maximum runtime of 2 hours (7,200 seconds). On datasets with
25 to 29 variables, all algorithms were given a maximum
runtime of 24 hours (86,400 seconds).

We empirically evaluated the algorithms for both space and
time requirements. For the algorithms which used external memory (BFBnB and DP), we compared the maximum
hard disk usage. We also compared the running times of the
algorithms. The results are given in Table 1.
Previous results found that memory is the main bottleneck
restricting the size of learnable networks (Parviainen and
Koivisto 2009). As the results show, algorithms which attempt to store entire parent or order graphs in RAM, such
as A* and SS, are limited to smaller sets of variables. BFBnB’s duplicate detection strategy allows it to write parital
search layers to hard disk when the layers are too large to
fit in RAM, so it can learn optimal Bayesian network structures regardless of the amount of RAM. Consequently, hard
disk space is its only memory limitation. The inexpensive
cost of hard disks coupled with distributed file systems can
potentially erase the effect of memory on the scalability of
the algorithm.
For the datasets which it could solve, A* was sometimes
faster than the other algorithms. This is unsurprising since
it uses only RAM; however, it is unable to solve the larger
datasets that cannot fit entirely in RAM. Even on many of
the smaller datasets, though, A* runs more slowly than BFBnB because it has the overhead cost to keep its open list
in sorted order.
BFBnB not only takes an order of magnitude less external
memory, but runs several times faster than the DP algorithm on most of the datasets. DP is faster on the adult,
letter and meta datasets. These datasets have a small number of variables and a large number of records. The large
number of records limits the pruning of the AD-tree from
Theorem 1 and increases the runtime of BFBnB. However,
BFBnB runs faster on both mushroom (8,000 records) and
sensorReadings (5,000 records). Therefore, as the number
of variables increases, the number of records impacts the
runtime less.
The SS algorithm ran much more slowly than the other algorithms. It searches in the space of directed graphs rather
than DAGs. These results suggest that search in the space
of DAGs is more efficient than the space of directed graphs.
To demonstrate that our algorithm is applicable to larger
datasets, we also tested it using the wdbc dataset (31 variables, 569 records) and a biological dataset (33 variables,
72,228 records), epigenetic. We learned the optimal network for wdbc in 93,682 seconds (about 26 hours) and the
optimal network for epigenetic in 570,760 seconds (about 6
days). We also attempted to use DP, but its hard disk usage
exceeded the 500GB of free hard disk space on the server.
Figure 3 shows the total memory consumption of our algorithm for wdbc. Very little memory is used before layer
9, and after layer 22, the memory consumption does not
change much because the layer sizes decrease. As the figure shows, both of the middle layers use nearly 70 giga-

Figure 3: Hard disk usage for the wdbc dataset

bytes of disk space. Most of this space is consumed by the
parent graphs, so it is is freed after each layer. Assuming
that the running time and size of the middle layers double for each additional variable, which is a rough pattern
from Table 1, our algorithm could learn a 36-variable network in about 50 days using approximately 2 terabytes of
hard disk space and a single processor. This suggests that
our method should scale to larger networks better than the
method of Parviainen and Koivisto (2009). They observe
that their implementation would take 4 weeks on 100 processors to learn a 31-variable network, and, even with coding improvements and massive parallelization, only networks up to 34 variables would be possible.

7 CONCLUSION
Learning optimal Bayesian network structures has been
thought of in terms of dynamic programming; however,
such a formulation naively requires O(n2n ) memory. Other
formulations have been shown to have similar or slower
runtimes or require other exponential resources, such as
processors. This paper formulates the structure learning
problem as a frontier breadth-first branch and bound search.
The layered search technique allows us to work with one
layer of the score cache, parent and order graphs at a time.
Consequently, we delete layers of the parent graphs after expanding them and store only a portion of each order graph node to hard disk files to reduce the memory
complexity. The delayed duplicate detection strategy further improves the scalability of the algorithm by writing
partial layers to disk rather than requiring an entire layer fit
in RAM at once. Additionally, a heuristic function allows
parts of the order graph to be ignored entirely; this also reduces memory complexity and improves scalability.
Experimental results demonstrate that this algorithm outperforms the previous best implementation of dynamic programming for learning optimal Bayesian networks. Our algorithm not only runs faster than the existing approach, but
also takes much less space. The LP formulation exhibits
similar runtime behavior as DP, so our algorithm should

similarly outperform it. It also scales to more variables than
A*. Additionally, by searching in the space of DAGs instead of the space of directed graphs with cycles, it proves
the optimality of the learned network more quickly than SS.
Future work will investigate better upper bounds and
heuristic functions to further increase the size of learnable
optimal networks. Also, like existing methods (Parviainen
and Koivisto 2009; Silander and Myllymaki 2006), our algorithm can benefit from parallel computing. In addition,
distributed computing can scale up our algorithm to even
larger learning problems. Networks learned from our algorithm could also be used as a “gold standard” in studying the assumptions of approximate structure learning algorithms.
Acknowledgements This work was supported by NSF
CAREER grant IIS-0953723 and EPSCoR grant EPS0903787.




nostic system. Given that so many variables are involved,
even the best solution by MAP or MPE may have an ex-

Most Relevant Explanation (MRE) is a method
for nding multivariate explanations for given
evidence in Bayesian networks [12].

tremely low probability, say in the order of

10−6 .

It is hard

to make any decision based on such hypotheses.

This pa-

In real-world problems, it is observed that usually only a

per studies the theoretical properties of MRE

few target variables are most relevant in explaining any

and develops an algorithm for nding multiple

given evidence. For example, there are many possible dis-

top MRE solutions. Our study shows that MRE

eases in a medical domain, but a patient can have at most

relies on an implicit soft relevance measure in

a few diseases at one time, as long as he or she does not

automatically identifying the most relevant tar-

delay treatments for too long. It is desirable to nd diag-

get variables and pruning less relevant variables

nostic hypotheses containing only those relevant diseases.

from an explanation. The soft measure also en-

Other diseases should be excluded from further tests or

ables MRE to capture the intuitive phenomenon

treatments. In a recent work, Yuan and Lu [12] propose

of explaining away encoded in Bayesian net-

an approach called Most Relevant Explanation (MRE) to

works.

Furthermore, our study shows that the

generate explanations containing only the most relevant tar-

solution space of MRE has a special lattice struc-

get variables for given evidence in Bayesian networks. Its

ture which yields interesting dominance relations

main idea is to traverse a trans-dimensional space contain-

among the solutions. A K-MRE algorithm based

ing all the partial instantiations of the target variables and

on these dominance relations is developed for

nd one instantiation that maximizes a relevance measure

generating a set of top solutions that are more

called generalized Bayes factor [3].

representative. Our empirical results show that

shown in [12] to be able to nd precise and concise ex-

MRE methods are promising approaches for ex-

planations. This paper provides a study of the theoretical

planation in Bayesian networks.

properties of MRE and offers further evidence for its valid-

The approach was

ity. The study shows that MRE relies on an implicit soft
relevance measure that enables the automatic identication

1

Introduction

of the most relevant target variables and pruning of less relevant variables from an explanation. Furthermore, the solu-

Bayesian networks offer compact and intuitive graphical

tion space of MRE has a special lattice structure that allows

representations of uncertain relations among the random

two interesting dominance relations among the solutions to

variables of a domain and provide a foundation for many

be dened. These dominance relations are used to design

diagnostic expert systems.

and develop a K-MRE algorithm for nding a set of top

However, these systems typi-

cally focus on disambiguating single-fault diagnostic hy-

explanations that are more representative.

potheses because it is hard to generate just right multiple-

results show that MRE methods are promising approaches

fault hypotheses that contain only the most relevant faults.

for explanation in Bayesian networks.

Maximum a Posteriori (MAP) assignment and Most Probable Explanation (MPE) are two explanation methods for
Bayesian networks that nd a complete assignment to a
set of target variables as the best explanation for given evidence and can be applied to generate multiple-fault hypotheses. A priori, the set of target variables is often large
and can be in tens or even hundreds for a real-world diag-

Our empirical

The remainder of the paper is structured as follows. We
rst review methods for explanation in Bayesian networks,
including Most Relevant Explanation. Then we introduce
several theoretical properties of Most Relevant Explanation. We also develop a K-MRE algorithm for generating
multiple top explanations and evaluate it empirically.

632

YUAN ET AL.
3

A (0.016)
Input

Output

C (0.15)

networks. However, they often fail to nd just-right ex-

D (0.1)

planations containing the most relevant target variables.

(a)

Many existing methods make simplifying assumptions and

Input
current100%
noCurr 0%

def 10%
ok 90%

Related Work

Many methods exist for explaining evidence in Bayesian

B (0.1)

B

UAI 2009

focus on singleton explanations [5, 7]. However, singleton

A
def 2%
ok 98%

explanations may be underspecied and are unable to fully
explain given evidence. For the running example, the pos-

D

C

output of B

def 10%
ok 90%

noCurr 90%

output of D

A, B, C, and D failing independently
0.391, 0.649, 0.446, and 0.301 respectively. Therefore,
(¬B) is the best singleton explanation1 . However, B alone
does not fully explain the evidence. C or D has to be interior probabilities of

output of A

def 15%
ok 85%

current 10%

current 2%

are

noCurr 98%

output of C

current 1%

current 1%

noCurr 99%

noCurr 99%

volved. Actually, if we are not focusing on faulty states,

(D)
Total Output

(0.699) is the best singleton explanation. It is clearly

not an adequate explanation for the evidence.

current 4%
noCurr 96%

For a domain in which target variables are interdependent,

(b)

multivariate explanations are often more natural for exFigure 1: (a) A probabilistic digital circuit and (b) a corresponding diagnostic Bayesian network

plaining given evidence.

However, existing methods of-

ten produce hypotheses that are overspecied. MAP nds
a conguration of a set of target variables that maximize
the joint posterior probability given partial evidence on
the other variables.

2

For the running example, if we set

A, B, C and D as the target variables, MAP will nd
(A∧¬B∧¬C ∧D) as the best explanation. However, given
that B and C are faulty, A and D are somewhat redundant

A Running Example

for explaining the evidence. MPE nds an explanation with
Let us rst introduce a running example used throughout
this paper.

Consider the circuit in Figure 1(a) adapted

from [9, 12]. Gates

A, B, C

and

D are defective if they are

closed. The prior probabilities that the gates close independently are

0.016, 0.1, 0.15

and

0.1

respectively. Connec-

tions between the gates may not work properly with certain
small probabilities. The circuit can be modeled with a diagnostic Bayesian network as shown in Figure 1(b). Nodes

A, B, C

and

D

correspond to the gates in the circuit and

each has two states: defective and ok. Others are input

even more variables. Several other approaches use the dependence relations encoded in Bayesian networks to prune
independent variables [10, 11].

They will nd the same

explanation as MAP because all of the target variables are
dependent on the evidence. Yet several other methods measure the quality of an explanation using the likelihood of the
evidence [1]. Unfortunately they will overt and choose

(¬A ∧ ¬B ∧ ¬C ∧ ¬D)

as the explanation, because the

likelihood of the evidence given that all the target variables
fail is almost

1.0.

or output nodes and have two states: current or noCurr.

There have been efforts trying to generate more appropri-

Uncertainty is introduced to the model such that an output

ate explanations. Henrion and Druzdzel [6] assume that a

node is in state current with a certain probability less than

system has a set of pre-dened scenarios as potential ex-

1.0

if its parent gate, when exists, is defective and any

planations and nd the scenario with the highest posterior

of its other parents is in state current. Otherwise, it is

probability. Flores et al. [4] propose to grow an explanation

in noCurr state with probability

1.0.

For example, node

output of B takes state current with probability 0.99 if
parent gate B is in state defective and parent Input is in
state current.

Input

and

T otal Output

in the

Bayesian network are both in the state current. The task
is to diagnose the system and nd the best fault hypotheses.
Based on our knowledge of the domain, we know there are
three basic scenarios that most likely lead to the observa-

A is defective; (2) B
B and D are defective.
tion: (1)

able at each step while maintaining the probability of each
explanation above certain threshold. Nielsen et al. [8] use
a different measure called causal information ow to grow

Suppose we observe that current ows through the circuit,
which means that nodes

tree incrementally by branching the most informative vari-

and

C

are defective; and (3)

the explanation trees. Because the explanations in the trees
have to branch on the same variable(s), they may still contain redundant variables. Finding more concise hypotheses
also have been studied in model-based diagnosis [2]. The
approach focus on truth-based systems and cannot be easily
generalized to deal with Bayesian networks.
1

We use a variable and its negation to stand for its ok and

defective states respectively

UAI 2009
4

YUAN ET AL.

Most Relevant Explanation

633
5

There are two most essential properties for a good expla-

A Theoretical Study

5.1

Theoretical properties of MRE

nation. First, the explanation should be precise, meaning
it should explain the presence of the evidence well. Sec-

We now discuss several theoretical properties of MRE.

ond, the explanation should be concise and only contain the

Since MRE relies heavily on the

most relevant variables. The above discussions show that

ating its explanations, it is not surprising that these proper-

existing approaches for explaining evidence in Bayesian

ties are mostly originated from

networks often generate explanations that are either under-

properties can be found in the appendix.

specied (imprecise) or overspecied (inconcise).

GBF

GBF .

measure in gener-

The proofs of these

First, we note that GBF can be expressed in a different way

To address the limitations, Yuan and Lu [12] propose a

using the belief update ratio.

method called Most Relevant Explanation (MRE) to au-

Denition 3. The belief update ratio of

tomatically identify the most relevant target variables for

r(x1:k1 ; e), is dened as

x1:k1

given

e,

given evidence in Bayesian networks. First, explanation in
Bayesian networks is formally dened as follows.
Denition 1.

Given a set of target variables

Bayesian network and evidence

e

X

in a

x1:k

of

X, i.e., X1:k ⊆ X and X1:k 6= ∅.

(4)

GBF can then be expressed as the ratio between the belief
update ratios of
given

MRE is then dened as follows [12].
Denition 2. Let

P (x1:k |e)
.
P (x1:k )

on the remaining vari-

ables, an explanation for the evidence is a partial instantiation

r(x1:k ; e) ≡

X be a set of target variables,

and

x1:k1

and alternative explanations

x1:k1

e, i.e.,

e be

GBF (x1:k1 ; e) =

the evidence on the remaining variables in a Bayesian net-

r(x1:k1 ; e)
.
r(x1:k1 ; e)

(5)

work. Most Relevant Explanation is the problem of nding an explanation
Bayes Factor score

x1:k that has the maximum Generalized
GBF (x1:k ; e), i.e.,

M RE(X, e) ≡ arg maxx1:k ,X1:k ⊆X,X1:k 6=∅ GBF (x1:k ; e) ,
(1)
where

GBF

The most important property of MRE is that it is able to
weigh the relative importance of multiple variables and
only include the most relevant variables in explaining the
given evidence. The degree of relevance is evaluated using
a measure called conditional Bayes factor (CBF) implicitly

is dened as

GBF (x1:k1 ; e) ≡

encoded in the GBF measure and dened as follows.

P (e|x1:k1 )
.
P (e|x1:k1 )

(2)

Denition 4. The conditional Bayes factor of hypothesis

y1:m for given evidence e conditional on x1:k is dened as

Therefore, MRE traverses the trans-dimensional space containing all the partial assignments of
ment that maximizes the

GBF

score.

Potentially, MRE

can use any measure that provides a common ground for
comparing the partial instantiations of the target variables.

GBF

CBF (y1:m ; e|x1:k ) ≡

X and nds an assign-

is chosen because it is shown to provide a plausible

P (e|y1:m , x1:k )
.
P (e|y1:m , x1:k )

Then, we have the following theorem.
Theorem 1. Let the conditional Bayes factor of y1:m given

measure for representing the degree of evidential support

x1:k

in recent studies in Bayesian conrmation theory [3].

ratio of the alternative explanations

be less than or equal to inverse of the belief update

MRE was shown to be able to generate precise and con-

CBF (y1:m ; e|x1:k ) ≤

cise explanations for the running example [12]. The best
explanation according to MRE is:

GBF (¬B, ¬C; e) = 42.62 .

(3)

e and write GBF (¬B, ¬C).
explanation than both (¬A) (39.44)

For simplicity we often omit
(¬B, ¬C ) is a better

(6)

x1:k , i.e.,
1
,
r(x1:k ; e)

(7)

the following holds

GBF (x1:k ∪ y1:m ; e) ≤ GBF (x1:k ; e).

(8)

and (¬B, ¬D ) (35.88), because its prior and posterior probabilities are both relatively high; The posterior probabilspectively.

0.394, 0.391,

0.266

Therefore,

CBF (y1:m , e|x1:k ) provides a soft measure on

re-

the relevance of a new set of variable states with regard to

Therefore, MRE seems able to automatically

an existing explanation and can be used to decide whether

ities of the explanations are

and

GBF

identify the most relevant target variables and states as the

or not to include them in an existing explanation.

explanations for given evidence.

also encodes a decision boundary, the inverse belief update

634

YUAN ET AL.

ratio of alternative explanations

x1:k

given

e,

UAI 2009

which pro-

a

A

b

B

c

C

vides a threshold on how important the remaining variables
ab

should be in order to be included in the current explanation.

aB

ac

aC

Ab

AB

Ac

AC

bc

bC

Bc

BC

1

CBF (y1:m ; e|x1:k ) is greater than or equal to r(x1:k ;e) ,
y1:m is regarded as relevant and will be included. Otherwise, y1:m will be excluded from the explanation.

If

abc

abC

aBc

aBC

Abc

AbC

ABc

ABC

Figure 2: Solution space of Most Relevant Explanation

Theorem 1 has several intuitive and desirable corollaries.
First, the following corollary shows that, for any explanation

x1:k

with belief update ratio greater than or equal to

1.0, adding any independent variable to the explanation will
decrease its GBF score [12].

x1:k be an explanation with r(x1:k ; e) ≥
1.0, and y be a state of a variable Y such that
P (y|x1:k , e) ≤ P (y|x1:k ). Then

Corollary 3. Let

x1:k be an explanation with r(x1:k ; e) ≥
1.0, and y be a state of variable Y independent from variables in x1:k and e. Then

Corollary 1. Let

GBF (x1:k ∪ {y}; e) ≤ GBF (x1:k ; e).

GBF (x1:k ∪ {y}; e) ≤ GBF (x1:k ; e).

(11)

(9)
This is again an intuitive result; a variable state whose posterior probability decreases for given evidence should not
be part of an explanation for the evidence.

Therefore, adding an irrelevant variable dilutes the explanative power of an existing explanation. MRE is able to
automatically prune such variables. This is clearly a desir-

The above theoretical results can be veried using the running example. For example,

able property.
Note that we focus on the explanations with belief update
ratio greater than or equal to 1.0. We believe that an explanation whose probability decreases given the evidence

>

GBF (¬B, ¬C)
GBF (¬B, ¬C, A) & GBF (¬B, ¬C, D)

>

GBF (¬B, ¬C, A, D) .

is unlikely to be a good explanation for the evidence.
Corollary 1 requires the additional variable
pendent from both

X1:k

and

E.

Y

to be inde-

The assumption is rather

strong. The following corollary relaxes it to be that
conditionally independent from

E

given

X1:k

Y

is

The results suggest that GBF has the intrinsic capability
to penalize higher-dimensional explanations and prune less
relevant variables.

and shows

the same result still holds.

5.2

x1:k be an explanation with r(x1:k ; e) ≥
1.0, and y be a state of a variable Y conditionally independent from variables in e given x1:k . Then

Corollary 2. Let

GBF (x1:k ∪ {y}; e) ≤ GBF (x1:k ; e).

(10)

Explaining away

One unique property of Bayesian networks is that they can
model the so called explaining away phenomenon using the

V

structure, i.e., a single variable with two or more parents.

This structure intuitively captures the situation where an
effect has multiple causes. Observing the presence of the
effect and one of the causes reduces the likelihood of the
presence of the other causes. It is desirable to capture this

Corollary 2 is a more general result than corollary 1 and
captures the intuition that conditionally independent variables add no additional information to an explanation in
explaining given evidence, even though the variable may
be marginally dependent on the evidence. Also note that
these properties are all relative to an existing explanation.
It is possible that a variable is independent from the evidence given one explanation, but becomes dependent on
the evidence given another explanation.

In other words,

GBF score is not monotonic. Looking at variables one by
one does not guarantee to nd the optimal solution.
The above results can be further relaxed to accommodate
cases where the posterior probability of
than its prior, i.e.,

y given e is smaller

phenomenon when generating explanations.
MRE seems able to capture the explaining away effect using CBF. CBF provides a measure on how relevant a new
variable is to an existing explanation.

In an explaining-

away situation, if one of the causes is already present in
the current explanation, other causes typically do not receive high CBF scores.

Again for the running example,

(¬B, ¬C) and (¬A) are both good explanations for the evidence by themselves. The CBF of ¬A given only e (the
effect) is equal to its GBF (39.44), which is rather high.

(¬B, ¬C) (one of the causes) is also obCBF (¬A; e|¬B, ¬C) becomes rather low and is
only equal to 1.03. Clearly, CBF is able to capture the exHowever, when
served,

plaining away phenomenon in this example.

UAI 2009
5.3

YUAN ET AL.

635
GBF(¬ B,

¬ C) = 42.62
¬ B, ¬ C) = 42.15
GBF(¬ B, ¬ C, D) = 39.93
GBF(A, ¬ B, ¬ C, D) = 39.56

Dominance relations

GBF(A,

MRE has a solution space with an interesting lattice structure similar to the graph in Figure 2 for three binary target

GBF(¬ A) = 39.44
GBF(¬ A, B) = 36.98
GBF(¬ A, C) = 35.99
GBF(¬ B,

¬ D) = 35.88

variables. The graph contains all the partial assignments of
the target variables. Two explanations are linked together

Table 1: The top solutions ranked by GBF. The solutions in

if they only have a local difference, meaning they either

boldface are the top minimal solutions.

have the same set of variables with one variable in different
states, or one explanation has one fewer variable than the
other explanation with all the other variables being in the
same states.
There are two dominance relations among these potential
solutions that are implied by Figure 2. The rst concept is
strong dominance.
Denition 5. An explanation

x1:k

other explanation y1:m if and
GBF (x1:k ) ≥ GBF (y1:m ).
If

x1:k

strongly dominates an-

only if

x1:k ⊂ y1:m

and

y1:m , x1:k is clearly a better
y1:m , because it not only has a no-worse

strongly dominates

explanation than

explanative score but also is more concise. We only need
to consider

x1:k

when nding multiple top MRE explana-

tions. The second concept is weak dominance.
Denition 6. An explanation

x1:k

y1:m if and
GBF (x1:k ) > GBF (y1:m ).
other explanation

In this case,

x1:k

the solutions.
The dominance relations dened in the last section allow
us to develop a K-MRE algorithm to nd a set of top solutions that are more representative.

Let us look at the

running example again to illustrate the idea.

The expla-

nations in Table 1 have the highest GBF scores. If we simply select top three explanations solely based on GBF, we
will obtain these rather similar explanations: (¬B, ¬C),
(A, ¬B, ¬C), and (¬B, ¬C, D), which are rather similar.
Since (A, ¬B, ¬C), (¬B, ¬C, D), and (A, ¬B, ¬C, D)
are strongly dominated by (¬B, ¬C), we should only consider (¬B, ¬C) out of those four explanations. Similarly,
(¬A, B) and (¬A, C) are strongly dominated by (¬A).
These dominated explanations should be excluded from the

weakly dominates an-

only if

output all the top solutions rather than selecting any one of

x1:k ⊃ y1:m

and

top solution set. In the end, we get the set of top explanations shown in boldface in Table 1, which is clearly more
diverse and representative than the original set. MAP and

has a strictly larger

GBF

score than

MPE clearly do not have this nice property.

but the latter is more concise. It is possible that we

Therefore, our proposed K-MRE algorithm works as fol-

can include them both and let the decision makers to decide

lows. Whenever we generate a new explanation, we check

whether they prefer higher score or conciseness. However,

its score against the best solution pool. If it is lower than

y1:m ,

we believe that we only need to include

x1:k ,

because its

the worst score in the pool, reject the new explanation. If

K

higher GBF score indicates that the extra variable states are

there are fewer than

relevant to explain given evidence and should be included

new explanation is higher than the worst score in the pool,

in the explanation.

we consider adding the new explanation to the top pool. We

Based on the two kinds of dominance relations, we dene
the concept minimal.

best solutions or if the score of the

rst check whether the new solution is strongly or weakly
dominated by any of the top explanations. If so, reject the
new explanation. Otherwise, we add the new explanation

Denition 7. An explanation is minimal if it is neither

to the top pool. However, we then need to check whether

strongly nor weakly dominated by any other explanation.

there are existing top explanations that are dominated by
the newly added explanation. If yes, these existing expla-

In case we want to nd multiple top explanations, we only

nations should be excluded. Otherwise we delete the top

need to consider the minimal explanations, because they

explanation with the least score.

are the most representative ones.

6

7

K-MRE Algorithm

7.1

Empirical Results
Experimental design

In many decision problems, outputting the single top solution may not be the best practice.

Decision makers typ-

We tested the K-MRE algorithm on a set of benchmark

ically would like multiple competing options to choose

models, including Alarm, Circuit, Hepar, Munin, and

from. This is especially important when there are multi-

SmallHepar. We chose these several models because we

ple solutions that are almost equally good. For the circuit

have the diagnostic versions of these networks, whose vari-

example, all three basic explanations will lead to the same

ables have been annotated into three categories: target, ob-

observation. However, we can only recover one explana-

servation, and auxiliary. For generating the test cases, we

tion if we are satised with one top solution. It is better to

used the networks as generative models and sampled with-

636

YUAN ET AL.

UAI 2009

Precision

1.0

K=1
F 1

0.8

1.0

1.0

1.0

1.0

0.6

0.8

0.8

0.8

0.8

0.4

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

1 0.0

0.0

0.0

Singleton
F-MAP

0.2

P-MAP
MRE

0.0
0

0.5

0

Recall

K=3
F=0
K=3
F 1

0.5

1

0

0.5

1

0.0
0

0.5

1

1.0

1.0

1.0

0.8

0.8

0.8

1.0

0.8

0.6

0.6

0.6

0.8

0.6

0.4

0.4

0.6

0.4

0.2

0.4

0.5

1

0.0
0

0.5

1

0.5

1

0

0.5

1

1.0

1.0

1.0

1.0

1.0

0.8

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.2

0.0

0.0

0.0
0

0.5

1

0

0.5

1

0

0.5

0.5

1

1.0

1.0

1.0

1.0

0.8

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.2

0.0

0.0

0.0

0.0

0.5

0

1

1

0

0.5

1

0

0.5

1

0.0

0

1

1.0

0

0.5

0.0

0.0

0



0.0

0

0.2


K=1
. 
F=2
)





K=3
. 
F=2
)




1

0.2

0.2

0.0
0

0.5

0.4

0.2

0.0

0

1.0

0.5

1

0

0.5

0.0
0

1

0.5

1

1.0

Singleton

P MAP

1.0

1.0

1.0

1.0

0.8

F MAP

MRE

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.6

I-rate 0.4
K1F1 0.2

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0

0.0

0.0

0.0

0.0

Alarm

Circuit

Hepar

Munin

SmallHepar

Figure 3: Precision vs recall plots of the results by four algorithms, Marginal, P-MAP, F-MAP, and MRE, on a set of
benchmark diagnostic Bayesian networks.K shows the number of top solutions generated. F shows the least number
of faulty target variables in test cases. F Score shows the F-Scores of the results of the algorithms. Marginal algorithm
did not appear in rows K3F1 and K3F2 because it has only one solution.

out replacement from their prior probability distributions.

centage of faulty states correctly identied among all faulty

We only kept those test cases with at least one abnormal ob-

explanation variables) and recall (the percentage of faulty

servation and used the abnormal observations as evidence.

states correctly identied among all faulty variables in test

Since Circuit and SmallHepar have 4 and 3 target variables

cases) of these algorithms in Figure 3.

respectively, we collected as many test cases as possible.

sample results on F-Score, which is dened as

Munin also has 4 target variables but each with many more
states. Hepar and Alarm have 9 and 12 target variables re-

F-Score

spectively. We collected 50 test cases for the last three net-

=

We also include

2 × (precision × recall)
.
(precision + recall)

(12)

works. We also extracted from them the test cases which
contain at least two faulty target variables for separate ex-

7.2

Results and analysis

periments on multiple-fault test cases.
Our experiments compared MRE with MAP given their
similarities. We tested two versions of the MAP algorithm,
one focusing on all the target variables (F-MAP) and the
other only on the target variables selected by MRE (PMAP). In addition, we compared with the Marginal algorithm, which neglects the interdependence among the target variables and uses the marginal posterior probabilities
to determine the most likely states of the target variables.
We plot the accuracy statistics, including precision (the per-

We make the following observations from these results.
First, MRE is able to achieve higher precision and/or recall
rates in identifying the faulty target variables than the other
algorithms on all the networks except Munin. An outstanding example is the SmallHepar network. Marginal, F-MAP
and P-MAP all failed badly on this model in identifying the
faulty variables, while MRE was able to achieve reasonable
performance. It is clearly desirable given that one major
goal of diagnosis or explanation is to identify problems,
e.g.

faulty states.

We investigated the results of Munin

UAI 2009

YUAN ET AL.

network further and found that all target variables of these

637
8

Concluding Remarks

test cases are in faulty states. Marginal and F-MAP have
exactly the same statistics, which suggests that the target

In this paper, we discuss several theoretical properties of

variables may have weak correlations with each other. This

Most Relevant Explanation (MRE) and develop an algo-

puts MRE in disadvantage because MRE takes into account

rithm for nding multiple top MRE solutions. Our study

such weak correlations and generate concise explanations

shows that MRE relies on an implicit soft relevance mea-

with fewer target variables. On average, the explanations of

sure in automatically identifying the most relevant target

4.3 variables out of 12 target variables for
Alarm, 1.7/4 for Circuit, 4/9 for Hepar, 2.5/4 for Munin,
and 2.3/3 for SmallHepar. For networks with strong correMRE identies

variables and pruning less relevant variables from an explanation.

The soft measure also enables MRE to cap-

ture the intuitive phenomenon of explaining away encoded

lations among the target variables, e.g. Circuit and Hepar,

in Bayesian networks. Furthermore, we dene two dom-

MRE has much higher precision/recall rates. The sample

inance relations among the explanations that are implied

F-score results in the case of K1F1 further conrmed the

by the structure of the solution space of MRE. These rela-

observation.

tions allow us to design and develop a K-MRE algorithm

Second, by comparing rows K1F1 vs.
K1F2 vs.

K3F1 and

K3F2, we found that using multiple top

for nding top MRE solutions that are much more representative.

solutions helps MRE signicantly in improving the preci-

Our empirical results agree quite well with the theoretical

sion/recall rates than the other algorithms. With multiple

understanding of MRE. The results show that MRE is ef-

solutions, we kept the results with the maximum precision

fective in identifying the most relevant target variables, es-

rates. The results seem to support our claim that K-MRE

pecially the true faulty target variables. Furthermore, K-

was able to generate solutions that are more representative.

MRE seems able to generate more representative top ex-

It is somewhat surprising that the precision/recall rates of

planations than K-MAP methods.

F-MAP were not improved at all on the networks, but those

is especially suitable for systems in which target variables

of P-MAP were improved. Our hypothesis is that, since the

are strong correlated with each other and can generate more

explanations by F-MAP are more grained because more

precise and concise explanations for these systems.

variables are involved, its top explanations tend to agree
with each other on the faulty variables and differ mostly
in the less important non-faulty variables. Generating multiple top solutions could not really help F-MAP much in
improving its accuracy statistics.

We believe that MRE

This research has many future works. It is desirable to understand the theoretical complexity of MRE. It has a solution space even larger than MAP and is believed to be at
least as hard. Currently we rely on an exhaustive search
algorithm for solving MRE and K-MRE. More efcient

Third, although P-MAP gets the target variables identied

methods for solving MRE need be developed to make it

by MRE as input, it still failed badly on the SmallHepar

applicable to large real-world problems.

network in identifying faulty states of the target variables.
It did not show any signicant advantage over F-MAP on

Acknowledgement

other networks either. The results suggest that relying on

National Science Foundation grant IIS-0842480.

posterior probabilities may not work well in certain diag-

experimental data have been obtained using SMILE, a

nostic systems.

Bayesian inference engine developed at the Decision Sys-

Fourth, although multiple-fault cases are believed to be

tems Laboratory at University of Pittsburgh and available

more difcult because of their low likelihood, the algo-

at

This research was supported by the
All

http://genie.sis.pitt.edu.

rithms in our experiments seem able to maintain the same
level of accuracy rates in face of multiple-fault test cases
(rows K1F2 and K3F2).



A branch-and-bound approach to solving influence diagrams has been previously proposed in
the literature, but appears to have never been
implemented and evaluated – apparently due to
the difficulties of computing effective bounds
for the branch-and-bound search. In this paper,
we describe how to efficiently compute effective
bounds, and we develop a practical implementation of depth-first branch-and-bound search for
influence diagram evaluation that outperforms
existing methods for solving influence diagrams
with multiple stages.

1

Introduction

An influence diagram [7] is a compact representation of
the relations among random variables, decisions, and preferences in a domain that provides a framework for decision
making under uncertainty. Many algorithms have been developed to solve influence diagrams [2, 3, 8, 15, 17, 18,
19, 21]. Most of these algorithms, whether they build a
secondary structure or not, are based on the bottom-up dynamic programming approach. They start by solving small
low-level decision problems and gradually build on these
results to solve larger problems until the solution to the
global-level decision problem is found. The drawback of
these methods is that they can waste computation in solving decision scenarios that have zero probability or that are
unreachable from any initial state by following an optimal
decision policy.
This drawback can be overcome by adopting a branch-andbound approach to solving an influence diagram that uses
a search tree to represent all possible decision scenarios.
This approach can use upper bounds on maximum utility
to prune branches of the search tree that correspond to lowquality decisions that cannot be part of an optimal policy;
it can also prune branches that have zero probability.

A branch-and-bound approach to influence diagram evaluation appears to have been first suggested by Pearl [16].
He proposed it as an improvement over the classic method
of unfolding an influence diagram into a decision tree and
solving it using the rollback method, which itself is a form
of dynamic programming [7]. In Pearl’s words:
A hybrid method of evaluating influence diagrams naturally suggests itself. It is based on
the realization that decision trees need not actually be generated and stored in their totality to produce the optimal policy. A decision
tree can also be evaluated by traversing it in a
depth-first, backtracking manner using a meager amount of storage space (proportional to the
depth of the tree). Moreover, branch-and-bound
techniques can be employed to prune the search
space and permit an evaluation without exhaustively traversing the entire tree... an influence diagram can be evaluated by sequentially instantiating the decision and observation nodes (in
chronological order) while treating the remaining chance nodes as a Bayesian network that supplies the probabilistic parameters necessary for
tree evaluation. (p. 311)
However, neither Pearl nor anyone else appears to have
followed up on this suggestion and implemented such an
algorithm. The apparent reason is the difficulty of computing effective bounds to prune the search tree. Qi and
Poole [17] proposed a similar search-based method for
solving influence diagrams, but with no method for computing bounds; in fact, their implementation relied on the
trivial infinity upper bound to guide the search. Recently,
Marinescu [12] proposed a related search-based approach
to influence diagram evaluation. But again, he proposed no
method for computing bounds; his implementation relies
on brute-force search. Even without bounds to prune the
search space, note that both Qi and Poole and Marinescu
argue that a search-based approach has advantages – for
example, it can prune branches that have zero probability.

In this paper, we describe an implemented depth-first
branch-and-bound search algorithm for influence diagram
evaluation that includes efficient techniques for computing bounds to prune the search tree. To compute effective bounds, our algorithm adapts and integrates two previous contributions. First, we adapt the work of Nilsson
and Höhle [14] on computing an upper bound on the maximum expected utility of an influence diagram. The motivation for their work was to bound the quality of strategies
found by an approximation algorithm for solving limitedmemory influence diagrams, and their bounds are not in
a form that can be directly used for branch-and-bound
search. We show how to adapt their approach to branchand-bound search. Second, we adapt the recent work of
Yuan and Hansen [20] on solving the MAP problem for
Bayesian networks using branch-and-bound search. Their
work describes an incremental method for computing upper
bounds based on join tree evaluation that we show allows
such bounds to be computed efficiently during branch-andbound search. In addition, we describe some novel methods
for constructing the search tree and computing probabilities
and bounds that contribute to an efficient implementation.
Our experimental results show that this approach leads to
an exact algorithm for solving influence diagrams that outperforms existing methods for solving multistage influence
diagrams.

2

Background

We begin with a brief review of influence diagrams and
algorithms for solving them. We also introduce an example
of multi-stage decision making that will serve to illustrate
the results of the paper.
2.1

Influence Diagrams

An influence diagram is a directed acyclic graph G containing variables V of a decision domain. The variables can be
classified into three groups, V = X ∪ D ∪ U, where X is
the set of oval-shaped chance variables that specify the uncertain decision environment, D is the set of square-shaped
decision variables that specify the possible decisions to be
made in the domain, and U are the diamond-shaped utility variables representing a decision maker’s preferences.
As in a Bayesian network, each chance variable Xi ∈ X
is associated with a conditional probability distribution
P (Xi |P a(Xi )), where P a(Xi ) is the set of parents of Xi
in G. Each decision variable Dj ∈ D has multiple information states, where an information state is an instantiation of
the variables with arcs leading into Dj ; the selected action
is conditioned on the information state. Incoming arcs into
a decision variable are called information arcs; variables at
the origin of these arcs are assumed to be observed before
the decision is made. These variables are called the information variables of the decision. No-forgetting is typically

assumed for an influence diagram, which means the information variables of earlier decisions are also information
variables of later decisions. We call these past information
variables the history, and, for convenience, we assume that
there are explicit information arcs from history information
variables to decision variables. Finally, each utility node
Ui ∈ U represents a function that maps each configuration
of its parents to a utility value the represents the preference of the decision maker. (Utility variables typically do
not have other variables as children except multi-attribute
utility/super-value variables.)
The decision variables in an influence diagram are typically
assumed to be temporally ordered, i.e., the decisions have
to be made in a particular order. Suppose there are n decision variables D1 , D2 , ..., Dn in an influence diagram. The
decision variables partition the variables in X into a collection of disjoint sets I0 , I1 , ..., In . For each k, where
0 < k < n, Ik is the set of chance variables that must
be observed between Dk and Dk+1 . I0 is the set of initial
evidence variables that must be observed before D1 . In is
the set of variables left unobserved when decision Dn is
made. Therefore, a partial order ≺ is defined on the influence diagram over X ∪ D, as follows:
I0 ≺ D1 ≺ I1 ≺ ... ≺ Dn ≺ In .

(1)

A solution to the decision problem defined by an influence
diagram is a series of decision rules for the decision variables. A decision rule for Dk is a mapping from each configuration of its parents to one of the actions defined by
the decision variable. A decision policy (or strategy) is a
series of decision rules with one decision rule for each decision variable. The goal of solving an influence diagram
is to find an optimal decision policy that maximizes the expected utility. The maximum expected utility is equal to
∑
∑
∑
∑
max
P (X|D)
Uj (P a(Uj )).
max ...
I0

D1

In−1

Dn

In

j

In general, the summations and maximizations are not
commutable. The methods presented in Section 2.3 differ
in the various techniques they use to carry out the summations and maximizations in this order.
Recent research has begun to relax the assumption of ordered decisions. In particular, Jensen proposes the framework of unconstrained influence diagrams to allow a partial
ordering among the decisions [9]. Other research relaxes
the no-forgetting assumption, in particular, the framework
of limited-memory influence diagrams [10]. Although the
approach we develop can be extended to these frameworks,
we do not consider the extension in this paper.
2.2

Example

To illustrate decision making using multi-stage influence
diagrams, consider a simple maze navigation problem [6,

(1,1)

ns_0 (2)

x_0 (0)

ns_1 (9)

es_0 (3)

x_1 (7)

(7,9)

(a)

Figure 1: Two maze domains. A star represents the goal.
14]. Figure 1 shows four instances of the problem. The
shaded tiles represent walls, the white tiles represent movable space, and the white tiles with a star represent goal
states. An agent is randomly placed in a non-goal state. It
has five available actions that it can use to move toward the
goal: it can move a single step in any of the four compass
directions, or it can stay in place. The effect of a movement
action is stochastic. The agent successfully moves in the
intended direction with probability 0.89. It fails to move
with probability 0.089, it moves sideways with probability 0.02 (0.01 for each side), and it moves backward with
probability 0.001. If movement in some direction would
take it into a wall, that movement has probability zero, and
the remaining probabilities are normalized. The agent has
four sensors, one for each direction, which sense whether
the neighboring tile in that direction is a wall. Each sensor is noisy; it detects the presence of a wall correctly with
probability 0.9 and mistakenly senses a wall when none is
present with probability 0.05. The agent chooses an action
at each of a sequence of stages. If the agent is in the goal
state after the final stage, it receives a utility value of 1.0;
otherwise, it receives a utility value of 0.0.
Figure 2(a) shows the influence diagram for a two-stage
version of the maze problem. The variables xi and yi represent the location coordinates of the agent at time i, the variables {nsi , esi , ssi , wsi } are the sensor readings in four
directions at the same time point, and the variable di represents the action taken by the agent. The utility variable
u assigns a value depending on whether or not the agent
is in the goal state after the last action is taken. Since the
same variables occur at each stage, we can make the influence diagram arbitrarily large by increasing the number of
stages.
2.3

Evaluation algorithms

Many approaches have been developed for solving influence diagrams. The simplest is to unfold an influence diagram into an equivalent decision tree and solve it in that
form [7]. Another approach called arc reversal solves an
influence diagram directly using techniques such as arcreversal and node-removal [15, 18]; when a decision variable is removed, we obtain the optimal decision rule for the
decision. Several methods reduce influence diagrams into

y_2 (15)

y_1 (8)
ss_0 (4)

ss_1 (11)

ws_0 (5)

ws_1 (12)

(b)

u

d_1 (13)

d_0 (6)
y_0 (1)

x_2 (14)

es_1 (10)

(a)
R2345678
9 10 11 12 13

01234
5678

7 8 13
14 15

(b)
Figure 2: (a) An example influence diagram and (b) its
strong join tree. The numbers in both figures stand for the
indices of the variables. The node with “R” is the strong
root.

Bayesian networks by converting decision nodes into random variables such that the solution of an inference problem in the Bayesian network correspond to the optimal decision policy for the influence diagram [3, 21]. Another
method [19] transforms an influence diagram into a valuation network and applies variable elimination to solve the
valuation network. Recent work compiles influence diagrams into decision circuits and uses the decision circuits
to compute optimal policies [2]; this approach takes advantage of local structure present in an influence diagram, such
as deterministic relations.
In the following, we describe a state-of-the-art method for
solving an influence diagram using a strong join tree [8].
This method is viewed by many as the fastest general algorithm, and we use its performance as a benchmark for our
branch-and-bound approach. A join tree is strong if it has
at least one clique, R, called the strong root, such that for
any pair of adjacent cliques, C1 and C2 , with C1 closer to
R than C2 , the variables in separator S = C1 ∩ C2 must
appear earlier in the partial order defined in Equation (1)
than C2 \ C1 . A strong join tree for the influence diagram
in Figure 2(a) is shown in Figure 2(b).
An influence diagram can be solved exactly by message
passing on the strong join tree. Each clique C in the join
tree contains two potentials, a probability potential ϕC and
a utility potential ψC . For clique C2 to send a message to
clique C1 , ϕC1 and ψC1 should be updated as follows [8]:

′
ϕ′C1 = ϕC1 × ψS ; ψC
= ψC1 +
1

ψS
;
ϕS

where
ϕS =

∑

ϕC2 ; ψS = max ϕC2 × ψC2 .

C2 \S

C2 \S

information arcs from each variable in R to D is guaranteed to have a maximum expected utility that is greater than
or equal to the maximum expected utility for G. We call G∗
an upper-bound influence diagram for G.

In contrast to the join tree algorithm for Bayesian networks [11], only the collection phase of the algorithm is
needed to solve an influence diagram. After the collection
phase, we can obtain the maximum expected utility by carrying out the remaining summations and maximizations in
the root. In addition, we can extract the optimal decision
rules for the decision variables from some of the cliques
that contain these variables.

Use of an upper-bound influence diagram to compute
bounds only makes sense if the upper-bound influence diagram is simpler and much easier to solve than the original
influence diagram. In fact, adding information arcs to an
influence diagram can simplify its evaluation by making
some other information variables non-requisite. An information variable Ii is said to be non-requisite [10, 13] for a
decision node D if

Building a join tree for a Bayesian network may fail if the
join tree is too large to fit in memory. This is also true
for influence diagrams. In fact, the memory requirement of
a strong join tree for an influence diagram is even higher
because of the constrained order in Equation (1). Consequently, the join tree algorithm is typically infeasible for
solving all but very small influence diagrams.

Ii ⊥ (U ∩ de(D))|D ∪ (P a(D) \ {Ii }),

3

Computing bounds

To implement a branch-and-bound algorithm for solving
influence diagrams, we need a method for computing
bounds – in particular, for computing upper bounds on the
utility that can be achieved beginning at a particular stage
of the problem, given the history up to that stage. A trivial upper bound is the largest state-dependent value of the
utility node of the influence diagram. In this section, we
discuss how to compute more informative bounds. There
has been little previous work on this topic. Nilsson and
Höhle [14] develop an approach to bounding the suboptimality of policies for limited-memory influence diagrams
that are found by an approximation algorithm. Dechter [4]
describes an approach to computing upper bounds in an
influence diagram that is based on mini-bucket partitioning. Neither work considers how to use these bounds in a
branch-and-bound search algorithm.
The approach we develop in the rest of this paper is based
on the work of Nilsson and Höhle [14], which we extend by
showing how it can be used to compute bounds for branchand-bound search. The general strategy is to create an influence diagram with a value that is guaranteed to be an
upper bound on the value of the original influence diagram,
but that is also much easier to solve. We use the fact that
additional information can only increase the value of an influence diagram. Since this reflects the well-known fact
that information value is non-negative, we omit (for space
reasons) a proof of the following theorem.
Theorem 1. Let G be an influence diagram and D a decision variable in G. Let I be D’s information variables
and R another set of random variables in G that are nondescendants of D. Then the influence diagram G∗ that results from making R into information variables by adding

(2)

where de(D) are the descendants of D. A reduction of
an influence diagram is obtained by deleting all the nonrequisite information arcs [14].
Because of the no-forgetting assumption, a decision variable at a late stage may have a large number of history information variables. For decision making under imperfect
information, all of these information variables are typically
requisite. As a result, the size of the decision rules grows
exponentially as the number of decision stages increases,
which is the primary reason multi-stage influence diagrams
are very difficult to solve.
In constructing an upper-bound influence diagram, we want
to add information arcs that make some or all of the history
information variables for a decision node non-requisite, in
order to simplify the influence diagram and make it easier to solve. We adopt the strategy proposed by Nilsson
and Höhle [14]. Let nd(X) be the non-descendant variables of variable X, let f a(X) = P a(X) ∪ {X} be the
family of variable X (i.e., the variable and its parents), let
f a(X) = ∪Xi ∈X f a(Xi ) be the family of the set of variables X (i.e., the variables and their parents), and let △j be
{D1 , ...Dj } be a sequence of decision variables from stage
1 to stage j. The following theorem is proved by Nilsson
and Höhle [14].
Theorem 2. For an influence diagram with the constrained
order in Equation (1), if we add to each decision variable
Dj the following new information variables in the order of
j = n, ..., 1,
Nj = arg minB⊆(Bj ∩nd(Dj )) {|B||f a(△j )
⊥ (U ∩ de(Dj ))|(B ∪ {Dj })},

(3)

where

{
U∪D
j=k,
Bj =
∩ki=j+1 {n ∈ V |n ⊥ (U ∩ de(Dj ))|f a(Di )} j<k,
the following holds for any Dj in the resulting influence
diagram:
(de(Dj ) ∩ U) ⊥ f a(△i−1 )|f a(Dj ).

(4)

ns_0 (2)

3
Past state

1

Sufficient
Information D
Set

Future state

U

2

x_0 (0)

ns_1 (9)

es_0 (3)

x_1 (7)

ss_0 (4)

ss_1 (11)

ws_0 (5)

ws_1 (12)

(a)

What this theorem means is that for each decision variable Dj in an influence diagram, there is a set of information variables Nj such that the optimal policy for Dj depends only on these information variables, and is otherwise
history-independent. Note that the set Nj for decision variable Dj can contain both information variables from the
original influence diagram and information variables created by adding new information arcs to the diagram. The
set Nj of information variables can be interpreted as the
current “state” of the decision problem, such that if the decision maker knows the current state, it does not need to
know any previous history in order to select an optimal action; in this sense, the state satisfies the Markov property.

(b)

We call Nj a sufficient information set (SIS) for Dj . The intuition behind this approach is illustrated by Figure 3. The
shaded oval shows the SIS set ND for decision D. The
past state affects the variables in ND ∪ {D}, illustrated by
the arc labeled 1, and ND ∪ {D} affects the future state,
as illustrated by arc 2. The future state further determines
the values of the utility variables. ND ∪ {D} d-separates
the past and future states and prevents the direct influence
shown by arc 3. The concept of a sufficient information set
is related to the concept of extremality, as defined in [21],
and the concept of blocking, as defined in [13].
To construct an upper-bound influence diagram, we find the
SIS set for each decision in the order of Dn , ..., D1 and
make the variables in each SIS set information variables
for the corresponding decisions. We then delete the nonrequisite information arcs. Consider the influence diagram
in Figure 4(a) as an example. The information set for d1 is
originally {ns0 , es0 , ss0 , ws0 , d0 , ns1 , es1 , ss1 , ws1 }. We
find that its sufficient information (SIS) set is {x1 , y1 }. We

y_2 (15)

y_1 (8)

Figure 3: Relations between past and future information
states and the minimum sufficient information set.

Consider a decision-making problem in a partially observable domain, such as the maze domain of Section 2.2. The
agent cannot directly observe its location in the maze and
must rely on imperfect sensor readings to infer its location.
In this domain, adding information arcs from the location
variables to a decision variable, so that the agent know the
current location at the time it chooses an action, makes both
current and history sensor readings non-requisite, which results in a much simpler influence diagram, which in this
case serves as an upper-bound influence diagram.

u

d_1 (13)

d_0 (6)
y_0 (1)

x_2 (14)

es_1 (10)

Figure 4: (a) the upper-bound influence diagram for the
diagram in Figure 2(a), and (b) its strong join tree.

also find that the SIS set for d0 is {x0 , y0 }. By making
{x1 , y1 } and {x0 , y0 } information variables for d1 and d0
respectively, and reducing the influence diagram, we obtain
the much simpler influence diagram in Figure 4(a). The
strong join tree for the new influence diagram is shown in
Figure 4(b), which is also much smaller than the strong
join tree for the original model. Since the upper-bound influence diagram assumes the actual location is directly observable to the agent, it effectively transforms a partially
observable decision problem into a fully observable one.
The resulting influence diagram and, hence, its join tree is
much easier to solve.
Finding the sufficient information set (SIS) for a decision
variable in an influence diagram is equivalent to finding a
minimum separating set in the moralized graph of the influence diagram [14]. Acid and de Campos [1] propose
an algorithm based on the Max-flow Min-cut algorithm [5]
for finding a minimum separating set between two sets of
nodes in a Bayesian network with some of the separating
variables being fixed. We use their algorithm to find the SIS
sets. The two sets of nodes are f a(△j ) and U ∩ de(Dj ).
The only fixed separating variable is Dj . The algorithm
first introduces two dummy variables, source and sink, to
the moralized graph. The source is connected to the neighboring variables of f a(△j ), and the sink to the variables in
de(Dj ) ∩ an(U ∩ de(Dj )). We then create a max-flow network out of the undirected graph by assigning each edge
capacity 1.0. A solution gives a minimum separating set
between the sink and source that contains Dj .
We briefly mention some issues that are not described by
Nilsson and Höhle [14], but that need to be considered in
an implementation. The first issue is how to define the size
of an SIS set. Theorem 2 uses the cardinality of the SIS
set as the minimization criterion. Another viable choice is
to use weight, defined as the product of number of states,
of the variables in an SIS set as the minimization criterion.

The relation between these two criteria is similar to the relation between treewidth and weight in constructing a junction tree. While treewidth tells us how complex a Bayesian
network is at the structure level, weight provides an idea
on how large the potentials of the junction tree are at the
quantitative level. Both methods have been used. In our
implementation, we use the cardinality.
A second issue is that multiple candidate SIS sets may exist
for a decision variable. In that case, we need some criterion
for selecting the best one. In our implementation, we select
the candidate SIS set that is closest to the descendant utility
variables of the decision. Note that other candidate sets are
all d-separated from the utility node by the closest SIS set.
This choice has the advantage that the resulting influence
diagram is easiest to evaluate; however, other choices may
result in an influence diagram that gives a tighter bound.

4

Branch-and-bound search

In this section, we describe how to use the upper-bound
influence diagram to compute bounds for a depth-first
branch-and-bound search algorithm that solves the original influence diagram. We begin by showing how to represent the search space as an AND/OR tree. A naive approach to computing bounds requires evaluating the entire upper-bound influence diagram at each OR node of the
search tree, which is computationally prohibitive. To make
branch-and-bound search feasible, we rely on an incremental approach to computing bounds proposed by Yuan and
Hansen [20] for solving the MAP problem in Bayesian networks using branch-and-bound search. We show how to
adapt that approach in order to solve influence diagrams
efficiently.
4.1

AND/OR tree search

We represent the search space for influence diagram evaluation as an AND/OR tree. The nodes in an AND/OR tree
are of two types: AND nodes and OR nodes. AND nodes
correspond to chance variables; a probability is associated
with each arc originating from an AND node and the probabilities of all the arcs from an AND node sum to 1.0. The
OR nodes correspond to decision variables. Each of the
leaf nodes of the tree has a utility value that is derived from
the utility node of the influence diagram.
Qi and Poole [17] create an AND/OR tree in which each
layer of AND nodes alternates with a layer of OR nodes.
Each AND node in this tree corresponds to the information
set of a decision variable in the influence diagram, which
is a set of information variables. To compute the probability for each arc emanating from an AND node in this
tree, however, it is necessary to have the joint probability distributions of all the information sets; these are often
not readily available, since variables in the same informa-

ns0
es0
ss0

ws0
d0

es0
ss0

ss0

ws0

ws0

d0

d0

ss0

ws0
d0

Figure 5: The AND/OR tree used in our approach. Ovalshaped nodes are AND nodes, and square-shaped nodes are
OR nodes.
tion set can belong to different clusters of a join tree. For
computational convenience, our AND/OR tree is based on
the structure of the strong join tree in Figure 4(b). For the
maze example, we order the variables in the information set
{ns0 , es0 , ss0 , ws0 } according to the order in Equation (5).
Note that the join tree does not have a clique that contains
all four variables; in fact, they are all in different cliques.
So we consider the variables one by one. That means that
our AND/OR tree allows several layers of AND nodes to
alternate with a layer of OR nodes. See Figure 5 for an
example of the kind of AND/OR tree constructed by our
search algorithm, and note that the first four layers of this
AND/OR tree are all AND layers.
Each path from the root of the AND/OR tree to a leaf corresponds to a complete instantiation of the information variables and decision variables of the influence diagram, that
is, a complete history. Since we use the AND/OR tree to
find an optimal decision policy for the original influence
diagram, we have to construct an AND/OR tree that is consistent with the original constrained order. For the influence
diagram in Figure 2(a), the partial order is
{ns0 , es0 , ss0 , ws0 } ≺ {d0 } ≺ {ns1 , es1 , ss1 , ws1 }
≺ {d1 } ≺ {x0 , y0 , x1 , y1 , x2 , y2 , u},
(5)
and the decision variables must occur in this order along
any branch.
We define a valuation function for each node in an
AND/OR tree as follows: (a) for a leaf node, the value is
its utility value; (b) for an AND node, the value of is the
sum of the values of its child nodes weighted by the probabilities of the outgoing arcs; (c) for an OR node, the value
is the maximum of the summed utility values of each child
node and corresponding arc. We use this valuation function
to find an optimal strategy for the influence diagram.
We represent a strategy for a multi-stage influence diagram
as a policy tree that is defined as follows. A policy tree of
an AND/OR tree is a subtree such that: (a) it consists of
the root of the AND/OR tree; (b) if a non-terminal AND
node is in the policy tree, all its children are in the policy
tree; and (c) if a non-terminal OR node is in the policy

tree, exactly one of its children is in the policy tree. Given
an AND/OR tree that represents all possible histories and
strategies for an influence diagram, the influence diagram
is solved by finding a policy tree with the maximum value
at the root, where the value of the policy tree is computed
based on the valuation function. Depth-first branch-andbound search can be used to find an optimal policy tree.
The AND/OR tree is constructed on-the-fly during the
branch-and-bound search, and it is important to do so in a
way that allows the probabilities and values to be computed
as efficiently as possible. We use the maze problem and the
AND/OR tree in Figure 5 as an example. (The upper-bound
influence diagram and join tree are shown in Figure 4.) We
have already pointed out that including more layers in our
AND/OR tree allows us to more easily use the probabilities
and utilities computed by the join tree. If we start by expanding ns0 (where expanding a node refers to generating
its child nodes in the AND/OR tree), we need the probabilities of P (ns0 ) to label the outgoing arcs. We can readily
look up the probabilities from clique (0, 1, 2) after an initial full join tree evaluation. Note that we use the join tree
of the upper-bound influence diagram to compute the probabilities. We can do that because these probabilities are the
same as those computed from the original influence diagram. This is due to the fact that the same set of actions
will reduce both models into the same Bayesian networks.
Adding information arcs to an influence diagram, in order
to create an upper-bound influence diagram, only changes
the expected utilities of the decision variables.
After expanding ns0 , we expand any of {es0 , ss0 , ws0 }.
Suppose the next variable is es0 , we need the conditional
probabilities of es0 given ns0 . These probabilities can be
computed by setting the state of ns0 as new evidence to
the join tree and evaluating the join tree again. The same
process is used in expanding {ss0 , ws0 }.
Note that we do not have to expand one variable at a time.
If a clique has multiple variables in the same information set, the variables can be expanded together because a
joint probability distribution over them can be easily computed. Expanding them together also saves the need to
do marginalization. For example, variables x1 , y1 , x2 , y2
(7,8,14,15) are in the same information group and also reside in a same clique. In this case, we can expand them as
a single layer in the AND/OR tree.
After {ns0 , es0 , ss0 , ws0 } are expanded, we expand d0 as
an OR layer. This is where the upper bounds are needed.
We set the states of {ns0 , es0 , ss0 , ws0 } as evidence to the
join tree and compute the expected utility values for d0 by
reevaluating the join tree. The expected utilities of d0 are
upper bounds for the same decision scenarios of the original model. We can use the upper bounds to select the most
promising decision alternative to search first. The exact
value will be returned once the subtree is searched. If the

value is higher than the upper bounds of the remaining decision alternatives, these alternatives are immediately pruned
because they cannot be part of an optimal decision policy.
We repeat the above process until a complete policy tree is
found.
4.2

Incremental join tree evaluation

It is clear that repeated join tree evaluation has to be performed in computing the upper bounds and conditional
probabilities. A naive approach is at each time to set the
states of instantiated variables as evidence and perform a
full join tree evaluation. However, that is too costly. We can
use an efficient incremental join tree evaluation method to
compute the probabilities and upper-bound utilities, based
on the incremental join tree evaluation method proposed by
Yuan and Hansen [20] for solving the MAP problem.
The key idea is that we can choose a particular order of the
variables that satisfies the constraints of Equation (5) such
that an incremental join tree evaluation scheme can be used
to compute the information. Given such an order, we know
exactly which variables have been searched and which variable will be searched next at each search step. We only
need to send messages from the parts of the join tree that
contain the already searched variables to a clique with the
next search variable. For example, after we search es0 , the
only message needs to be sent to obtain P (ns0 |es0 ) is the
message from clique (0, 1, 3) to (0, 1, 2). There is no need
to evaluate the whole join tree. If we choose the following
search order for the maze problem
ns0 , es0 , ss0 , ws0 , d0 , ns1 , es1 , ss1 , ws1 , d1 , x0 , y0 ,
x1 , y1 , x2 , y2 ,
we can use an incremental message passing in the direction
of the dashed arc in Figure 4(b) to compute the probabilities
and upper bounds needed in one downward pass of a depthfirst search.
Both message collection and distribution are needed in this
new scheme, unlike evaluating a strong join tree for the
original influence diagram. The messages being propagated contain two parts: utility potentials and probability
potentials. The distribution phase is typically needed to
compute the conditional probabilities. For example, suppose we decide to expand es0 before ns0 , we have to
send a message from clique (0, 1, 3) to (0, 1, 2) to obtain
P (ns0 |es0 ). We only need to send the probability potential in message distribution. We do not need to send utility
potentials because past payoffs do not count towards the
expected utilities of future decisions.
4.3

Efficient backtracking

We use depth-first branch-and-bound (DFBnB) to utilize
the efficient incremental bound computation. Depth-first

a

b

stages
2
3
4
5
2
3
4
5

Join tree
time mem.
15
8.0
640 238.4
16
8.0
640 238.4
-

time
125
1s812
59s610
32m55s766
109
2s109
43s641
18m00s437

mem.
3.2
4.7
23.9
343.0
3.2
4.7
23.9
323.1

DFBnB
policy
783
12,559
200,975
3,215,631
783
12,559
200,975
3,215,631

#bounds
816
13,104
446,255
14,546,815
816
14,734
325,820
7,883,235

#zeros
0
0
0
0
0
0
0
0

Exhaustive search
time mem. #zeros
703
3.1
0
43s218
4.7
0
47m42s625
25.1
0
688
3.1
0
43s953
4.7
0
49m00s516
25.1
0
-

Table 1: Comparison of three algorithms (join tree algorithm, DFBnB using the join tree bounds, and exhaustive search of
the AND/OR tree) in solving maze problems a and b for different numbers of stages. The symbol ‘-’ indicates the problem
could not be solved, due to memory limits in the case of the join tree algorithm, or due to a three-hour time limit in the
case of the search algorithms. Running time is in hours (h), minutes (m), seconds (s) and milliseconds. Memory (mem.) is
in megabytes and is the peak amount of memory used by the algorithm; “policy” is the count of nodes (both OR and AND
nodes) in the part of the search tree containing the optimal policy tree – it is the same for both DFBnB and exhaustive
search; “#bounds” is the count of times a branch from an OR node was pruned based on bounds; “#zeros” is the count of
times a branch from an AND node was pruned because it had zero probability.
search makes sure that the search need not jump to a different search branch before backtracking is needed. In other
words, the join tree only needs to work with one search
history at a time.
We do need to backtrack to a previous search node once
we finish a search branch or realize that a search branch is
not promising and should be pruned. We need to retract all
the newly set evidence since the generation of that search
node and restore the join tree to a previous state. One way
to achieve this is to reinitialize the join tree with correct
evidence and perform a full join tree evaluation, which we
have already pointed out is too costly. Instead, we cache
the potentials and separators along the message propagation path before changing them by either setting evidence
or overriding them with new messages. When backtracking, we simply restore the most recently cached potentials
and separators in the reverse order. The join tree will be
restored to the previous state with no additional computations. This backtracking method is much more efficient
than reevaluating the whole join tree. For solving the MAP
problem for Bayesian networks, Yuan and Hansen [20]
show that the incremental method is more than ten times
faster than full join tree evaluation in depth-first branchand-bound search.

5

Empirical Evaluation

We compared the performance of our algorithm against
both the join tree algorithm [8] and exhaustive search of
the AND/OR tree (i.e., no bounds are used for pruning).
Experiments were run on a 2.4 GHz Duo processor with 3
gigabytes of RAM running Windows XP.
The results in Table 1 are for the two maze problems in
Figure 1, which we solved for different numbers of stages.
When there are only two or three stages, the join tree al-

gorithm is most efficient. This is because the strong join
trees for these influence diagrams are rather small and can
be built successfully. Once the join trees are built, only
one collection phase is necessary to solve the influence diagram; by contrast, the depth-first branch-and-bound algorithm (DFBnB) algorithm must perform repeated message
propagations to compute upper bounds and probabilities
during the search. For more then three stages, however, the
join tree algorithm cannot solve the maze models because
the strong join trees are too large to fit in memory. Because the exhaustive search algorithm only needs to store
the search tree and policy, it can solve the maze models
for up to four stages, although it takes more then 45 minutes to do so. The DFBnB algorithm can solve the maze
models for up to five stages in less time than it takes the
exhaustive search algorithm to solve them for four stages,
demonstrating the advantage of using bounds to prune the
search tree. Table 1 includes the number of times a branch
of the search tree is pruned based on bounds, as well as the
number of times a branch with zero probability is pruned.
For the maze models with their original parameter settings,
every branch of the search tree has non-zero probability.
Previous work has argued that one of the advantages of
search algorithms for influence diagram evaluation is that
they can prune branches of the search tree that have zero
probability, even without bounds [12, 17]. To test this argument, as well as to illustrate the effect of different problem
characteristics on algorithm performance, we modified the
maze models described in Section 2.2. First, we removed
some noise from the sensors. Each of the four sensors reports a wall in the corresponding direction of the compass.
In the original problem, each sensor is noisy; it detects the
presence of a wall correctly with probability 0.9 and mistakenly senses a wall when none is present with probability
0.05. As a result, every sensor reading is possible in every state and there are no zero-probability branches. We

stages
2
3
4
5
6
2
3
4
5
6

a

b

a

b

stages
2
3
4
5
6
7
2
3
4
5
6
7

Join tree
time mem.
16
8.0
641 238.4
15
8.0
641 238.4
-

Maze domains modified by removing some noise from sensors
DFBnB
Exhaustive search
time mem.
policy #bounds
#zeros
time mem.
#zeros
140
3.1
224
34
246
62
3.1
312
1s172
3.6
849
107
2,598
781
3.5
4,616
15s281
4.3
2,843
798
28,282
9s828
4.1
61,320
2m25s266
5.5
9,076
6,282
325,146
4m04s062
4.3
773,768
12m03s828
6.3 28,413
41,921 2,885,925
1h0m26s078
7.9
9,652,872
109
3.2
261
51
175
79
3.1
292
1s203
3.6
1,136
186
2,507
1s109
3.5
5,876
14s203
4.4
4,244
660
29,458
16s391
4.3
88,948
1m39s046
5.9 15,030
2,302
229,003
7m59s218
6.0
1,255,284
5m14s047
10.0 52,240
58,058 1,442,212 1h57m22s625
10.9 17,418,100

Maze domains modified by removing some noise from both actions and sensors
Join tree
DFBnB
Exhaustive search
time mem.
time mem.
policy #bounds
#zeros
time mem.
#zeros
16
8.0
110
3.2
177
30
206
109
3.1
276
594 238.4
750
3.7
562
99
1,818
1s015
3.5
3,192
5s359
4.1
1,504
276
13,276
10s234
4.0
33,660
50s453
4.8
4,216
807
124,312
1m44s406
4.7
343,843
5m09s328
6.0 11,198
2,236
767,084 17m39s812
5.9 3,490,703
- 36m59s906
8.3 29,653
5,947 4,821,057
15
8.0
94
3.1
203
45
164
156
3.1
278
500 238.4
672
3.7
727
165
1,602
1s391
3.5
4,047
5s297
4.1
2,029
524
13,601
14s984
4.0
46,812
27s688
5.1
5,540
1,557
72,518
2m36s313
4.9
504,264
2m04s812
6.7 15,625
4,941
335,185 30m24s453
6.6 5,312,539
- 18m13s157
10.5 43,673
16,639 2,984,966
-

Table 2: Comparison of three algorithms (join tree algorithm, DFBnB using the join tree bounds, and exhaustive search
of the AND/OR tree) in solving maze problems a and b with modified parameters; for the results in the top table, some
noise is removed from the sensors only; for the results in the bottom table, some noise is removed from the actions and the
sensors. Removing some noise from the actions and sensors results in more zero-probability branches that can be pruned,
allowing the search algorithms (but not the join tree algorithm) to solve the problem for a larger number of stages.
modified the model so that each sensor accurately detects
whether a wall is present in its direction of the compass.
With this change, the maze problem remains partially observable, but the search tree contains many zero-probability
branches, as can be seen from the results in Table 2. Since
the search algorithms can prune zero-probability branches,
the exhaustive search algorithm can now solve the problem
for up to five stages and the DFBnB algorithm can solve
the problem for up to six stages.
We next made an additional change to the transition probabilities for actions. In the original problem, the agent successfully moves in the intended direction with probability
0.89 (as long as there is not a wall). It fails to move with
probability 0.089, it moves sideways with probability 0.02
(0.01 for each side), and it moves backward with probability 0.001. We modified these transition probabilities so that
the agent still moves in the intended direction with probability 0.89; but otherwise, it stays in the same position with
probability 0.11. The effects of the agent’s actions are still
stochastic, but they are more predictable, and this allows
the search tree to be pruned even further. As a result, the

exhaustive search algorithm can solve the problem for up to
six stages and the DFBnB algorithm can solve the problem
for up to seven stages.
Note that changing the problem characteristics has no effect on the performance of the join tree algorithm. The
join tree algorithm solves the influence diagram for all information states, including those that have zero probability
and those that are unreachable from the initial state; as a
result, its memory requirements explode exponentially in
the number of stages and the algorithm quickly becomes
infeasible. Although the policy tree that is returned by the
search algorithms can also grow exponentially in the number of stages, it does so much more slowly because so many
branches can be pruned. As is vividly shown by the results
for the two different mazes and for different parameter settings, the performance of the search algorithms is sensitive
to problem characteristics – precisely because the search
algorithms exploit a form of problem structure that is not
exploited by the join tree algorithm. In addition, the results
show the effectiveness of bounds in scaling up the searchbased approach.

6

Conclusion

We have described the first implementation of a depthfirst branch-and-bound search algorithm for influence diagram evaluation. Although the idea has been proposed
before, we adapted and integrated contributions from related work and introduced a number of new ideas to make
the approach computationally feasible. In particular, we
described an efficient approach for using the join tree algorithm to compute upper bounds to prune the search tree.
The idea is to generate an upper-bound influence diagram
by allowing each decision variable to be conditioned on additional information that makes the remaining history nonrequisite, thus simplifying the influence diagram. Then
a join tree of the upper-bound influence diagram is used
to incrementally compute upper bounds for the depth-first
branch-and-bound search. We have also described a new
approach to constructing the search tree based on the structure of the strong join tree of the upper-bound influence
diagram. Experiments show that the resulting depth-first
branch-and-bound search algorithm outperforms the stateof-the-art join tree algorithm in solving multistage influence diagrams, at least when there are more than three
stages.
We are currently considering how to extend this approach
to solve limited-memory influence diagrams [10], which
typically have many more stages. We are also exploring approaches to compute more accurate bounds for pruning the
search tree. Finally, we are considering approximate and
bounded-optimal search algorithms for solving larger influence diagrams using the same upper bounds and AND/OR
search tree.
Acknowledgments This research was support in part by
NSF grants IIS-0953723 and EPS-0903787, and by the
Mississippi Space Grant Consortium and NASA EPSCoR
program.




Ï

 "
 ! #%$&')(*,+-!'.*/0132546879:! ";
/0,"<=>,# ?@ ,> "
 1ABC ED48,# !

FG HJI K9G 
L M:N HJIOHJI POQRHTSULWVYX,Z [SUN \]P \]L_^
`6acb#dfegd h]ij_kTemlnao@e"prqBs9h]tuqUlnh]tnk
vwiWlnax x d y]aiWl6j_kTemlnao@e"z{tnh]y]tuqBo|qBi~}jTbuJh_h]xhB vwiTh]tno@qUlnd h]ijTb#d ai~b#ace
"iJd ]atuegd lmk-hB z{d lgluegsJJtny]
z{d lgluegsJJtny]Jz*]B]
]UW~B8)]J]9UJTW  )¡_¡¢g]£]
¤¦¥>§]¨T©Jª,«9¨
d o®­9h]tgluqBi~b#a>Ji~b´lnd h]iËq]eq ¾q]b´lnh]tnd ¿cqUlnd h]i_d» a]» 9q ­JtnhT}T~b´l
hBïuðUñ)òUóõôó¾ðUñ)öU÷ø)ùnð]úuö]ú#óõ÷ óõôûôwö]ú#÷ ü´ý@À Á8z¢¼euÂ´»Ä>d ]ai¸}TdfqByB¯
iJhWemlndfb3a_df}Tai~b#a]8q]}J}Td lnd h]i~qBx}Ta­9ai~}Tai~b#atnaxfqUlnd h]i~e ²d x x
¬ iJa{hB_lnJa{o@qBd i:­Jtnh]sJx ao@ehBTd o®­9h]tgluqBi~b#a¢enqBo ¯
s9a d iWlntnhT}T~b#ac}5qBo®h]iJy@lnJa UqBtndfqBsJx ace»Á¢h]i~egacÎWJaiWlnx k3²8a
­Jx d iJy3d i±°'qk]acegdfqBiiJa#lm²8h]tn³Te6dfe>tna­JtnacegaiWluqUlnd h]i
}ThiJhBl®~q]aa#ÅT­Jx dfb#d l h]tno@eh]t lnJaËÁ8z¢¼ed iYlnJa3iJa#lg¯
hBJlnJa8d o®­9h]tgluqBi~b#a¢Ji~b´lnd h]iB²JdfbuegJh]Jxf}df}Ta#¯
8
²
h]tn³Te»ËÇaÉ~tueml®}Tatnd ]a-lnJaa#ÅJq]b´lh]tnoþh]tlnJaËÁ8z¢¼e
qBx x kµs9a¶q]e·b#x hWegaOq]e¸­9hWenegd sJx a¦lnh¹lnJaO­9hWemlna#¯
B
h

lnJah]­Tlnd o@qBxd o®­9h]tgluqBi~b#aÆJi~b´lnd h]i»'j_d i~b#abqBxfb#JxfqUlnd iJy
tnd h]t6ºmh]d iWl-}Tdfemlntnd sJTlnd h]i»Y¼¢k_­JdfbqBx x k]{²8a3tna­Jtna#¯
n
l
J


a
oÿdfe{­Jtuq]b´lndfbqBx x kacÎWJd UqBx aiWl{lnh:a#ÅJq]b´l¢d iTatnai~b#a"d i®lnJa
egaiWl-qBi½d o®­9h]tgluqBi~b#a3Ji~b´lnd h]i¦q]e@q5¾q]b´lnh]tnd ¿cqU¯
J
i
#
a
m
l
8
²
h]tn³Te¢²8a~eg~qBx x k·h]iJx kY~egalnJad tqB­J­JtnhÅTd o@qUlnd h]i~e»
lnd h]i d» a]»  ­JtnhT}T~b´lhB"b#h]i~}Td lnd h]i~qBx{­Jtnh]s~qBsJd x d lmk

Ç
±
a
n
t
a_d a²0ega]atuqBx>a#ÅTdfemlnd iJy¦qB­J­JtnhÅTd o@qUlnd h]i¶emlntuqUlnay]d ace
luqBsJx ace3À Á8z¢¼euÂ´» Ã|Ä>d ]ai·}TdfqBy]iJhWemlndfb@a_df}Tai~b#a]


]
h

t
n
l
J

ab#h]i~}Td lnd h]i~qBx{h]tno@eqBi~}¸­9h]d iWl h]TllnJad t x d o®d luqU¯
²8a@}ThiJhBl:~q]a®a#ÅT­Jx dfb#d l:h]tno@e>h]tÆlnJa3Á8z¢¼e
n
l
d
]
h
~
i

e

»
õlnat qËegd o®­Jx aqBi~qBx kTegdfe:hB'lnJa-d iTÌ~Jai~b#ahBaW¯
d i¦lnJa5iJa#lm²8h]tn³Te»ÈÇaÉ~tueml3}Tatnd ]alnJa5a#ÅJq]b´l
f
d
T
}

a
~
i
#
b
@
a
d i±°'qk]acegdfqBi5iJa#lm²8h]tn³Te²8a®­Jtnh]­9hWega®qBi±qB­J­JtnhÅTd ¯
h]tno¹h]t lnJa>Á8z¢¼e hB~lnJah]­Tlnd o@qBxJd o®­9h]tgluqBi~b#a
@
o
U
q
n
l
d
]
h
i
emlntuqUlnay]k>ln~qUl,o®hT}Td É~ace lnJa'iJa#lm²8h]tn³:emlntn~b´lnJtna8d i
Ji~b´lnd h]i»Êj_d i~b#alnJaËbqBxfb#JxfqUlnd h]i½dfe@~qBtu}8²8a
]
h
u
t
T
}

a
'
t
n
l
h
q]bb#h]o®o®hT}JqUlnaÆlnJa:o®hWeml6d o®­9h]tgluqBiWl6q]}J}Td lnd h]i~qBx
~eg~qBx x kYh]iJx kY~egalnJad t3qB­J­JtnhÅTd o@qUlnd h]i~e»OÇa
T
}

a
9
­

a
~
i
T
}

a
~
i
b#a6tnaxfqUlnd h]i~e,d iWlntnhT}T~b#ac}@s_klnJa6a_df}Tai~b#a]» ¬ Jt
tna_d a²Oega]atuqBxJ­9h]­JJxfqBt¢qB­J­JtnhÅTd o@qUlnd h]i@emlntuqUlna#¯
#
a
T
Å
9
­

a
n
t
d
®
o

a
W
i
luqBx"tnacegJx lue-egJhU²ln~qUl@lnJaËiJa²CemlntuqUlnay]k¸hBõ¯
y]d ace qBi~}·­9h]d iWl@h]Tl lnJad t®x d o®d luqUlnd h]i~e»·°'q]egac}



a
u
t

e
B
q
±
i
d
®
o
o®ac}TdfqUlna-d o®­JtnhU]ao®aiWl:d i±lnJa-ÎW~qBx d lmk5hB'lnJa
h]i¦qBi¦qBi~qBx kTegdfe hB6lnJad iTÌ~Jai~b#ahB>a_df}Tai~b#a]
d
®
o
9
­
]
h
g
t
u
l
B
q
~
i
#
b
>
a
Ji~b´lnd h]i»
²8a ­Jtnh]­9hWega q-o®a#lnJhT}Ëh]tÆqB­J­JtnhÅTd o@qUlnd iJy-lnJa
a#ÅJq]b´l8h]tno|hB d o®­9h]tgluqBi~b#a6Ji~b´lnd h]i3s_k@a#ÅT­Jx dfb´¯
d lnx k3o®hT}Tax d iJy@lnJao®hWeml6d o®­9h]tgluqBiWl>q]}J}Td lnd h]i~qBx
Ð  Ò{©T¨Tª Ñ « 'ª  Õ Ñ

}Ta­9ai~}Tai~b#a'tnaxfqUlnd h]i~erd iWlntnhT}T~b#ac}s_k:a_df}Tai~b#a]»
¬ Jt6a#ÅT­9atnd o®aiWluqBx tnacegJx lue>egJhU²Èln~qUl6lnJa iJa²
ÇaemluqBtgl ²d lnlnJa'lnJah]tna#lndfbqBxWtnh_hBlue hB9d o®­9h]tgluqBi~b#a'enqBo ¯
qB­J­JtnhÅTd o@qUlnd h]i¸emlntuqUlnay]khBÍ)atueqBi·d o®o®ac}TdfqUlna
­Jx d iJy~»®Ça@~ega@bqB­Jd luqBx,x a#lglnatue>h]t:UqBtndfqBsJx aceÆqBi~}5x hU²8atg¯
d o®­JtnhU]ao®aiWlrd i:lnJa'ÎW~qBx d lmk>hBTlnJa8d o®­9h]tgluqBi~b#a
bq]ega-x a#lglnatueh]tlnJad t®emluqUlnace»5°8h]xf}±x a#lglnatue }TaiJhBlnaega#lue
Ji~b´lnd h]i»
hBÆUqBtndfqBsJx ace®h]t-emluqUlnace»¶pa#
l À 3Â s9a5q5Ji~b´lnd h]iOhB
 
UqBtndfqBsJx ac
e    Ã     9 hU]atlnJa®}Th]o@qBd 
i  "!  »
Á¢h]i~egdf}Tat'lnJa­Jtnh]sJx ao hB acemlnd o@qUlnd iJy®lnJa:oJx lnd ­Jx ad iWlna#¯
ÐWÑ ¨T©JÒÓ6Ô6«9¨JÕmÒ Ñ
y]tuqBx

vwo®­9h]tgluqBi~b#aenqBo®­Jx d iJyB¯ s~q]egac}OqBx y]h]tnd lnJo@e~q]aËs9acb#h]o®a
qBiYd o®­9h]tgluqBiWl ¾qBo®d x k·hBÆqB­J­JtnhÅTd o@qUlna-d iTatnai~b#ao®a#lnT¯
hT}Je8h]t°'qk]acegdfqBi-iJa#lm²8h]tn³Te» ¬ iJaÆhBrlnJaÆo@qBd i3­Jtnh]sJx ao@e
hBd o®­9h]tgluqBi~b#a6enqBo®­Jx d iJyd i-°'qk]acegdfqBi®iJa#lm²8h]tn³Te,dfe{tna­Jtna#¯
egaiWluqUlnd h]i-hBlnJa>d o®­9h]tgluqBi~b#a"Ji~b´lnd h]i»,v l8dfe8²8ax x9³_iJhU²i
ln~qUllnJa3ÎW~qBx d lmk±hBd o®­9h]tgluqBi~b#a-Ji~b´lnd h]i·dfe b#tnd lndfbqBx{lnh
d o®­9h]tgluqBi~b#a3enqBo®­Jx d iJyqBx y]h]tnd lnJo@eÖ@¼'Ja3b#x hWegatlnJa3d o ¯
­9h]tgluqBi~b#a"Ji~b´lnd h]ilnhlnJaÆq]b´ln~qBx9­9hWemlnatnd h]t'}Tdfemlntnd sJTlnd h]i
lnJa>s9a#lglnat8d lue8­9atgh]tno@qBi~b#a]» ¼¢k_­JdfbqBx x k]W²8a"tna­JtnacegaiWl8qBi
×wØ8ÙnÚmÙ'Û Ù'ÜcÝWÞ ß àuÜcÝWáwâ ã]ÙnÚ8ã]â áwànÚmÙnämÙà´åáwÙcæWØ8Ü´Û ÙuçÙnÚ´èWáwÜcé:Ù'Üê
Ücë]Ú{ÚmÙuáwëWÞ ämá¢å#ÚmÙ"åÞ áwÜåìWìWÞ â à´åíWÞ ÙêfÜÚ¢àuÜcÝUämâ ÝBëWÜcëWá{à´åáwÙcî

#"$&%' À)(rÂ+*,( 

ÀmÂ

Ça±q]enegJo®aËln~qUl3lnJa±}Th]o@qBd i hBd iWlnay]tuqUlnd h]iOhB- À3Â
dfeÆs9h]Ji~}Tac}rd» a]» ln~qU.
l # a#ÅTdfemlue»®vwo®­9h]tgluqBi~b#a@enqBo®­Jx d iJy
qB­J­JtnhWq]buJace{lnJdfe­Jtnh]sJx aos_kacemlnd o@qUlnd iJy
#/$&%

)À (rÂ
À)(rÂ+*,( 
À0 )(rÂ 0

À]Â

²Jatna 0 À3Â´r²Jdfbudfe:bqBx x ac}ËlnJaó21"ø~ðUù´ôwöUñ)ïuü354Tñ)ï#ôó¾ðUñ~
dfeq­Jtnh]s~qBsJd x d lmk5}Tai~egd lmk5Ji~b´lnd h]i·eg~buln~qUl 0 À3Â-6ÿ
q]b#tnhWenelnJa'aiWlnd tna'}Th]o@qBd i >» ¬ iJa8­Jtuq]b´lndfbqBx_tnacÎWJd tnao®aiWl

hB 0 À3Â>dfe>ln~qUl:d legJh]Jxf}5s9a®acq]egklnhenqBo®­Jx a tnh]o» vwi
]h tu}Tat-lnh½acemlnd o@qUlnaËlnJad iWlnay]tuqBx²8ay]aiJatuqUlna5enqBo®­Jx ace
( 7Ã  (98     (9:¹tnh]o 0 À 3Â'qBi~}~egaÆlnJa:y]aiJatuqUlnac}UqBx Jace
d i3lnJaenqBo®­Jx a#¯ o®acqBi-h]tnoJxfq
 => ?

>

:

À)( > Â
; 
#
À@WÂ
<
À0 )( Â 
Ã
¼'Jaacemlnd o@qUlnh]t{qBx o®hWeml{egJtnax kb#h]i_]atny]acelnhA#¶Ji~}Tat¢b#atg¯
luqBd i²8acqB³-q]enegJo®­Tlnd h]i~eCB DFE »
¼'Ja-­9atgh]tno@qBi~b#a®hB'lnJa@acemlnd o@qUlnh]td H
i G¢ÎW~qUlnd h]I
i @bqBi
s9a:o®acq]egJtnac}s_k-d lueUqBtndfqBi~b#a
8 À)(rÂ
À3Â
8
*,(PQ# 
J&KLM B À3Â EN/$O%
)
À
r
(
Â
0
0

RJsJd i~emlnad i/B cSE'egJhU²"eÆln~qUld 

À3Â-6*J lnJa-h]­Tlnd o@qBx

d o®­9h]tgluqBi~b#a>Ji~b´lnd h]idfe

À3Â

0 À3ÂT

À)D_Â

À]Â


#

vwi:lnJdfe bq]ega]clnJa¢UqBtndfqBi~b#a{hBTlnJa¢acemlnd o@qUlnh]trdfer¿atnh~»VU"hU²'¯
a]atcclnJa'b#h]i~b#a­Tl hBJlnJa8h]­Tlnd o@qBxWd o®­9h]tgluqBi~b#a{Ji~b´lnd h]idfe
hB¢tuqUlnJat"lnJah]tna#lndfbqBx egd y]iJd É9bqBi~b#as9acbqB~egaÉ~i~}Td iJ
y #dfe
acÎWJd UqBx aiWl'lnh®É~i~}Td iJy lnJa:­9hWemlnatnd h]t}Tdfemlntnd sJTlnd h]iJ²Jdfbu
dfe-lnJa­Jtnh]sJx ao ln~qUl²8a5qBtna¾q]b#d iJy~
» W"a]atglnJax acene"d l
egJy]y]acemlue ln~qUl¢d ²8aÉ~i~}@d i~emlnacq]}@q>Ji~b´lnd h]i@ln~qUl8dfe¢b#x hWega
aiJh]Jy]¦lnh¸lnJaËh]­Tlnd o@qBx6d o®­9h]tgluqBi~b#aJi~b´lnd h]i"²8aËbqBi
emlnd x xa#ÅT­9acb´l"y]h_hT}3b#h]i_]atny]ai~b#a6tuqUlnace»
X

¤ZY[ Ñ ©Jª \ Ò{© ^
Ò ©T¨Tª Ñ «

] Ò{© Ð  {
ª abr§_Õmª Ñdc F¨ e±Ò{©gf§
\ Ô Ñ «9¨JÕmÒ Ñ §·Õ Ñ`_ N

vwo®­9h]tgluqBi~b#aenqBo®­Jx d iJy±bqBi½s9aacq]egd x kYq]}JqB­Tlnac}¸lnh¸egh]x ]a
qCUqBtnd a#lmk0hBOd iTatnai~b#a­Jtnh]sJx ao@e¹d iE°'qk]acegdfqBiþiJa#lg¯
²8h]tn³Te¢aceg­9acb#dfqBx x k·É~i~}Td iJy¸­9hWemlnatnd h]t®o@qBtny]d i~qBxfe h]t@JiT¯
h]s~egatn]ac}ËUqBtndfqBsJx ace» ¼rh3o@qB³]a®d o®­9h]tgluqBi~b#a®enqBo®­Jx d iJyd i
°'qk]acegdfqBi·iJa#lm²8h]tn³Te­Jtuq]b´lndfbqBx{²8a3lmk_­JdfbqBx x k¸iJaac}½q±ega#¯
ÎWJai~b#a hBÁ8z¢¼e)²Jdfbu-ºmh]d iWlnx kËeg­9acb#d kËqBi5d o®­9h]tgluqBi~b#a
Ji~b´lnd h]ii
» hh]tnah]tno@qBx x k]egJ­J­9hWegk
a j@À 3Â@o®hT}Taxfe-lnJa
ºmh]d iWl­Jtnh]s~qBsJd x d lmk¦}Tdfemlntnd sJTlnd h]i¶hU]at3qYega#lhBUqBtndfqBsJx ace
 

9 » °8klnJa@bu~qBd i5tnJx a]²8a@bqBiË¾q]b´lnh]tnd ¿a
SÃ  8    
d l6q]e8h]x x hU²"e»
j@À3ÂTlj@À  Ã Â > m ?



8

j@À 

>on



ÃS   



>)p

ÃÂ 

À¾WÂ

>on h]tË²8>)a®
p }Ttuq²¹enqBo®­Jx ace"tnh]oCacq]bu
¼rh3}Ttuq²Rq3enqBo®­Jx a
hB'lnJaÁ8z¢¼qj@À   ÃS     Ã Â:egacÎWJaiWlndfqBx x k]»ÇabqBi
acq]egd x k@y]a#l'lnJa Á8z¢¼e'd i°'qk]acegdfqBi-iJa#lm²8h]tn³Te8²d lniJh®aW¯
df}Tai~b#a]_s9acbqB~ega6d   SÃ   8      qBtna6d i@lnJa6lnh]­9h]x h]y]dfbqBx
h]tu}Tat8hBrlnJaÆiJa#lm²8h]tn³)_²8a:bqBi3egd o®­Jx d k@lnJaqBs9hU]a6acÎW~qU¯
lnd h]i3lnh

>5n
>
m
>
?
j@À 3T
Â 
j@À  jCrÀ  ÂgÂ 
tÀ sBÂ
Ã

>on

>

²Jatnauj@À  jCrÀ  ÂgÂ qBtnaa#ÅT­Jx dfb#d lnx k o®hT}Tax ac}?d i
°'qk]acegdfqBiiJa#lm²8h]tn³Te»Ë¼'JaqBs9hU]a-egd o®­Jx d É9bqUlnd h]i·tna#Ì~acb´lue
egh¶bqBx x ac
} v5öUoù w]7ð x ïuðUñ)òUóõôó¾ðUq
ñ B  D7E ²JdfbuRenqkTeln~qUlq
iJhT}TaÆdfe8d i~}Ta­9ai~}TaiWlhB d lue'iJh]iT¯w}Tacenb#ai~}JqBiWlue¢y]d ]ai-h]iJx k
d lue6­~qBtnaiWlue»"vwo®­9h]tgluqBi~b#a enqBo®­Jx d iJy-Ji~}TatÆeg~bu5b#d tub#Jo ¯
emluqBi~b#ace@dfe@acq]egk·lnh¸d o®­Jx ao®aiWlcy
» U"hU²8a]atc¢²Jai }TdfqByB¯
iJhWemlndfb:a_df}Tai~b#aa#ÅTdfemlue9d lÆbqBi5}TtuqBo@qUlndfbqBx x k3bu~qBiJy]a:lnJa
}Ta­9ai~}Tai~b#a5tnaxfqUlnd h]i~e-qBo®h]iJy±lnJa5UqBtndfqBsJx ace»Rj_J­J­9hWega
ln~qUld i¶q]}J}Td lnd h]i lnh·lnJa5JiJh]s~egatn]ac}OUqBtndfqBsJx acz
e Ë"²8a
qBxfegh ~q]aÆqBi3a_df}Tai~b#aega#l {" S | SÃ     | } » Ça:³_iJhU²
ln~qUl8lnJa>­9hWemlnatnd h]t8}Tdfemlntnd sJTlnd h]ihBlnJa>iJa#lm²8h]tn³®bqBi3emlnd x x
s9aÆ¾q]b´lnh]tnd ¿ac}~egd iJy lnJabu~qBd i3tnJx a]»
j@À

n


n
m
>
?

{ÆÂb[j@À Ã Æ
{ Â

8

j À
@

>on



ÃS   



>)p

Ã7 {ÆÂ 

À~WÂ

U"hU²8a]atc)lnJa-egd o®­Jx d É9bqUlnd h]io@q]}Ta®h]'
t G¢ÎW~qUlnd h]Q
i sbqBi
>)p ²8a6bqBiJiJhBl> ºm~eml¢lnJtnhU²
iJhx h]iJy]at{s9a6o@q]}Ta"Jatna]Ws9acbqB~ega"
q²'qk>lnJaUqB> tndfqBsJx ace d i   SÃ      Ã F  jCrÀ  Â´]h]i®egh]o®a
hB²Jdfbu  o@qk±}Ta­9ai~}¸y]d ]ai>5n lnJaa_df}Ta>)i~p b#a]»Ë°8a#h]tna
²8a¢qBi~qBx k_¿a{JhU²lnh6egd o®­Jx d C
k j@À   SÃ      Ã Â´U²8a,É~tueml
d iWlntnhT}T~b#a>lnJaÆh]x x hU²d iJy@}Ta#É~iJd lnd h]i»

[3L I)9I"q ó2xUü#ñÊö6öUû]ü´ý´ó¾öUñÈñ)ü#ô¢ðUùowQ,óõô)4Tñ)ð]ú
ýü#ù xUüuò.xUöUù´ó¾ö]ú#÷ ü´ý  ÃS    9 öUñ)ò@üxcó¾ò]ü#ñ)ïuüÆýü#ô9{-
rðU> ùËöUñÈðUùnò]ü#ù´óõ&
ñ Y+ð 3  > ÃS     ô)Jü@tnax aUqBiWl®¾q]b´lnh]t®hB
  ò]ü#ñ)ðUôwüuò®ö> b
ý !@À  Â  ófý")ô Jü"ýü#ô,+ð 3 xUöUù´ó¾ö]ú#÷ ü´ý>)ô JöUô,önø 
> 7ò gïuðUñ~ñ)üuï#ôwüuò S
ø~üuöUù6> úuü 3#ðUùnü  óõñ-)ô Jü>ðUùnò]ü#ù´óõ&ñ  öUñ)òöUùnü>
ôwð  ïuðUñ)òUóõôó¾ðUñ)öU÷)ðUñ)ô Jü ø~öUùnü#ñ~ô¾ý"+ð > 3   )ô Jü6ü xcó¾ò]ü#ñ)ïu
ü { 
öUñ)ò@)ô Jü ðU)ô Jü#'
ù xUöUù´ó¾ö]ú#÷ ü´ýóõ
ñ !@À  Â 
>

wv iWlnJd lnd ]> ax k]b!@À  Âd i~b#x ~}TacelnJaq]}J}Td lnd h]i~qB>)x8p UqBtndfqBsJx ace
ln~qUl  iJaac> }Je¢lnh@b#h]i~}Td lnd h]i3h]i3d i  Ã     Ã  »W"hBlna
ln~qUl !@À  Â¢dfe'eg­9acb#d É9b"lnh@q­~qBtglndfb#JxfqBt8h]tu}Tatnd iJyhBrlnJa
UqBtndfqBsJx ace Jv l"o@qk3b#h]iWluqBd i}Td Í)atnaiWl"UqBtndfqBsJx ace8h]t6}Td Í)atg¯
aiWl@h]tu}Tatnd iJyWe»¦Ä>d ]aiYlnJaË}Ta#É~iJd lnd h]i G¢ÎW~qUlnd h]
i ~bqBi
iJhU²¶s9aegd o®­Jx d É~ac}lnh
j@À

n

{ÆÂT >m ?



j@À 

>5n

jCrÀ 

>

>

Â  {  !@À  gÂ Â 

À¡WÂ

Ã
U"hU²8a]atc~²8aiJhU²¹~q]a:iJha#ÅT­Jx dfb#d l6h]tno@e6hB lnJa-Á8z¢¼e
d ilnJaiJa#lm²8h]tn³qBi_ko®h]tna]»'v ,²8a²'qBiWllnhb#h]o®­JTlnaqBi~}
emlnh]tna3lnJa±Á8z¢¼e8²8aiJaac}½lnh¸sJtnacqB³¸lnJaËb#h]i~emlntuqBd iWl@hB
lnJa-h]tnd y]d i~qBx,iJa#lm²8h]tn³Ëemlntn~b´lnJtna-qBi~}±q]bb#h]o®o®hT}JqUlna®lnJa
q]}J}Td lnd h]i~qBx~}Ta­9ai~}Tai~b#a6qBo®h]iJyÆlnJa"UqBtndfqBsJx ace» ¬ iJa6egh]x T¯
lnd h]> idfe{lnh b#h]i~emlntn~b´l'qiJa² iJa#lm²8h]tn³ d i²Jdfbu-acq]bu-iJhT}T> a
 ~q]e,qBtube,> b#h]o®d iJy>tnh]o¹lnJaUqBtndfqBsJx ace d i®s9hBln
 jCrÀ  Â
qBi~
} !@À  Â´»,ÇabqBx xeg~bu3iJa#lm²8h]tnA
³ 3#ö]ï#ôwðUù´ó ¢cö]ú#÷ ü#»
[3L I)9Il£¤ ó2xUü#ñÊö6öUû]ü´ý´ó¾öUñÈñ)ü#ô¢ðUùowQ,óõô)4Tñ)ð]ú
ýü#ù xUüuò.xUöUù´ó¾ö]ú#÷ ü´ý  ÃS    9 öUñ)ò@üxcó¾ò]ü#ñ)ïuüÆýü#ô9{-
¥ ¾q]b´lnh]tnd ¿cqBsJx a±ñ)ü#ô ¢ðUùow¸ófýËö¸ñ)ü*ñ)ü#ô¢ðUùow±ô)JöUôùnü ø)ùnü
ýü#ñ~ô¾ý8)ô Jü'ý7ö 1@ü"òUófý´ôù´ó¾ú 4Tôó¾ðUñöý')ô Jü"ðUn ù´ó Bóõñ)öU÷~ñ)ü#ô ¢ðUoù w:öUñ)ò
Jðýü>ø~ðý´ôwü#ù´ó¾ðUù-òUófý´ôù´ó¾ú 4Tôó¾ðUk
ñ j@À  {ÆÂ@ïuöUñYúuü 354T÷õ÷ C
û 3#ö]ï 
> ø)ùnðc7ò 4~ï#ô8+ð 3.¦V§'¨ý  ðUñ)
ôwðUù´ó ¢cüuò ôwð@ö"
ü 3#ðUùüuö]5ï z4Tñ)ð]ú´ýü#ù > xUüuò
xUöUù´ó¾ö]ú#÷ ü 
óõ
ñ  ïuðUñ)òUóõôó¾ðUñ)öU÷ ðUñ5óõô¾ý8ø~öUùnü#ñ~ô¾
ý jCrÀ  Â 

©~q]b´lnh]tnd ¿cqBsJx a3emlntn~b´lnJtnadfe iJhBl®JiJdfÎWJa]» ¬ iJa3qBx y]h]tnd lnJo
lnh¦b#h]i~emlntn~b´lq·¾q]b´lnh]tnd ¿cqBsJx aemlntn~b´lnJtna5h]tq½°'qk]acegdfqBi
iJa#lm²8h]tn³@dfe"}Tacenb#tnd s9ac}s9ax hU²:»
ª^¾K~S7)cG«u

4Tóõ÷ òUóõñ&ö.rö]ï#ôwðUù´ó ¢cö]ú#÷ üA¬9ôù4~ï#ô4Tùnü

­ IV® NV&¯ 6
 öUû]ü´ý´ó¾öUñ¶ñ)ü#ô¢ðUùow°  ö5ýü#ôð+3üxcó¾ò]ü#ñ)ïuüxUöUù´ó2
ö]ú#÷ ü´ý{  Uö ñ)òö®ýü#ô'ð+3C4Tñ)ð]ú´ýü#ùxUüuòxUöUù´ó¾ö]ú#÷ ü´ý °±
²@NV® NV&¯ ¥ 3#ö]ï#ôwðUù´ó ¢cö]ú#÷ üý´ôù4~ï#ô4Tùnü
,³'ùnò]ü#ù6ô)JüÆñ)ðcò]ü´ý6óõñ´þóõñô)Jü>ùnüxUü#ùuýüð+36ô)Jü#óõù>ôwðnø~ð7
÷ ðoB ó¾ïuöU÷ ðUùnò]ü#ù
µ v5öUùowÈô)Jü¦ñ)ðcò]ü´ý½ô)JöUô5öUùnü öUñ)ïuü´ý´ôwðUùuý¦ð+3Oüxcó¾ò]ü#ñ)ïuü
ñ)ðcò]ü´ý:óõñ{-
¶  rðUù6ô)Jü:ðUùnò]ü#ù´óõñ&®óõñ¬9ôwü ø  ï5Jüuï5w@üuö]ï5´4Tñ)ð]ú´ýü#ùxUüuò
ñ)ðcò]üó 3
3 óõôófý@
1 öUùo]w üuò,[·)3ýð  ö]ò]òYöUñ¶öUùnïËúuü#ô¢üuü#ñ
üuö]ï55
 ø~öUóõù5ð+3
3 óõô¾ýø~öUùnü#ñ~ô¾ýB ó2Ux ü#ñ¶ô)JöUôñ)ð·öUùnï5üo¸]ófý´ô¾ý
úuü#ô¢
 üuü#ñ3ô)J
 ü1  ý~
4 ï5 ô)J
 öUô,ô)J
 ü:ðUù´ó¾ü#ñ~ôwöUôó¾ðUñËð+6
3 ô)J
 ü:öUùnï
ófýu3 ùnð7E
1 ô)J
 ü3ñ)ðcò]üËönø]ø~üuöUù´óõñ&¸
 ÷ öUôwü#ù3óõñ¦ô)J
 üËðUùnò]ü#ù3ôwð
ô)J ü üuöUù´÷ ó¾ü#ù ðUñ)ü
°¹´Jü#ñ-ö]ò]òUóõñ&:öUñ-öUùnïúuü#ô¢üuü#ñ ô¢ð"ñ)ðcò]ü´ý  ¢üüo¸´ø~öUñ)ò
ô)J üA¦V§'±
¨ ð+"
3 ô)J
 üÆï5_
 óõ÷ ò®ú#û ò7c4 ø)÷ ó¾ïuöUôóõñ& ô)JüÆü#ñ~ôù´ó¾ü´ý93#ðUù
òUó 6
º ü#ùnü#ñ~ô¢ý´ôwöUôwü´ýð+:
3 ô)J
 ü'ø~öUùnü#ñ~ô

v l"dfeemlntuqBd y]Wlgh]tn²'qBtu}®lnh@­JtnhU]a"lnJaÆh]x x hU²d iJy lnJah]tnao»
» G L&~SUL&«u ¥ ø]ø)÷ ûUóõñ& ¥ ÷ WðUù´óõô)&1¼:ôwð ö6öUû]ü´ý´ó¾öUññ)ü#ô
¢ðUùowk,óõô)½üxcó¾ò]ü#ñ)ïuü-{&öUñ)òk4Tñ)ð]ú´ýü#ùxUüuò½xUöUù´ó¾ö]ú#÷ ü´ý¾
 ÃS    ~  ûUó¾ü#÷ òý ö #3 ö]ï#ôwðUù´ó c¢ ö]ú#÷ üý´ôù~4 ï#ôT4 ùnü

"x y]h]tnd lnJo dfe>egd o®d xfqBt"lnh-lnJ
a Bùnön¿ø 5ùnüu7ò 4~ï#ôó¾ðUñ3o®a#lnJhT}
­Jtnh]­9hWegac}-d iB ]B~SE »,vwi~}Taac}Th]Jt'­JtnhTb#ac}TJtna6²d x xd iWlntnhB¯
}T~b#alnJa®enqBo®aq]}J}Td lnd h]i~qBx qBtube6q]elnJa y]tuqB­Jtnac}T~b´lnd h]i
o®a#lnJhT}5d {lm²8h3o®a#lnJhT}Je>~ega lnJa®enqBo®a h]tu}Tatnd iJy~C
» U"hU²'¯
a]atclnJa}Td Í)atnai~b#a8dferln~qUl,y]tuqB­Jtnac}T~b´lnd h]i qBs~egh]tns~elnJa
a_df}Tai~b#a:²Jd x atnac}T~b#d iJy®lnJay]tuqB­JT²Jdfbu~d ilnJaai~}
tnacegJx lued iYqËiJa²*iJa#lm²8h]tn³²d lnJh]Tl a_df}Tai~b#a-UqBtndfqBsJx ace»
vwi®h]Jt{­JtnhTb#ac}TJtna]B²8aega­~qBtuqUlna'y]tuqB­J tnac}T~b´lnd h]i@qBi~} aW¯
df}Tai~b#aÆqBs~egh]tn­Tlnd h]i» Ça6É~tueml'b#tnacqUlna>qiJa² iJa#lm²8h]tn³ln~qUl
emlnd x xWtna­JtnacegaiWllnJa¢enqBo®a¢}Tdfemlntnd sJTlnd h]iq]elnJa¢h]tnd y]d i~qBxUh]iJa]»
Ä>d ]ailnJaiJa²½emlntn~b´lnJtna]B²8abqBi¾q]b´lnh]tnd ¿a¢lnJa'Jx xUºmh]d iWl
­9hWemlnatnd h]t"}Tdfemlntnd sJTlnd h]i~egd iJy-bu~qBd itnJx aqBi~}qBs~egh]tnsaW¯
df}Tai~b#a@d iWlnhacq]bu·Á8z¢¼ÿega­~qBtuqUlnax k]» ¬ tglnd ½
¿ B  E¢­JtnacegaiWlue
q®egd o®d xfqBt"df}Tacqh]t"b#h]i~emlntn~b´lnd iJy@qBih]­Tlnd o@qBxd o®­9h]tgluqBi~b#a
Ji~b´lnd h]i d i·²Jdfbu¸JaegJy]y]acemlueÆÉ~tuemllntndfqBiJy]JxfqUlnd iJylnJa
°'qk]acegdfqBi>iJa#lm²8h]tn³6qBi~}>o@qB³_d iJyd l buJh]tu}JqBxcqBi~}>lnJaib#h]iT¯
emlntn~b´lnd iJylnJaiJa² emlntn~b´lnJtna3tnh]o lnJaËbuJh]tu}JqBx8y]tuqB­J»
¬ Jt"qB­J­JtnhWq]bu3q]h]df}Je¢Jdfed iWlnatno®ac}TdfqUlna:emlna­»
vwijWlna°
­ @ hB "x y]h]tnd lnJo ]T²8a:q]}J}3qBtube8s9a#lm²8aaiqBx x)lnJa
­~qBtnaiWlue¢hB qiJhT}Ta]T
» U"ai~b#a]_lnJa>xfq]eml­~qBtnaiWl8²d x xy]a#lqBtube
b#h]o®d iJy®tnh]o qBx xlnJahBlnJat"­~qBtnaiWlue»¢v  lnJa®Á8z¢¼Èegd ¿a:hB
lnJa®xfq]eml:­~qBtnaiWlÆdfeÆd iJd lndfqBx x kËxfqBtny]a]h]tÆd 8lnJatna@qBtna o@qBi_k
­~qBtnaiWlue lnJaiJa² Á8z¢¼*h]tlnJaxfq]eml®­~qBtnaiWl o@qk¸sJx hU²

J­»G{]ai iJhBlcBlnJaiJa²¦emlntn~b´lnJtna'²d x xJo@qB³]ad o®­9h]tgluqBi~b#a
enqBo®­Jx d iJy-d iJaÀ-b#d aiWlc»"¼rh-tnao®ac}TklnJa­Jtnh]sJx ao~²8a­JtnhB¯
­9hWega-ega]atuqBx JaJtndfemlndfbeÆh]t­Jtna­JtnhTb#acenegd iJylnJa-h]tu}Tatnd iJy
hB"lnJa­~qBtnaiWlue»¸"x x8lnJaJaJtndfemlndfbe qBtnaegJsTºmacb´l lnh5lnJa
­~qBtglndfqBx¢b#h]i~emlntuqBd iWlue:hB'lnJah]tnd y]d i~qBx{iJa#lm²8h]tn³)r²Jdfbu¸d iT¯
b#x ~}TaqBtube'h]t"}Td tnacb´lnac}3­~qUln~e'ln~qUl>qBx tnacq]}Tk-a#ÅTdfeml6qBo®h]iJy
­~qBtnaiWlue»r¼'Ja{É~tueml o®a#lnJhT}dfelnh"h]tu}TatlnJa¢­~qBtnaiWlued ilnJa
}Tacenb#ai~}Td iJy-h]tu}Tat6hB,lnJai_Jos9at6hB,lnJad t>hU²i­~qBtnaiWlue»
°8k½}Th]d iJy·egh~8²8aËqBtna3lntnk_d iJy±lnh¸o@qB³]a3lnJaËxfq]eml-­~qBtnaiWl
~q]a@q]e:x acene:d i~b#h]o®d iJyËqBtube:q]eÆ­9hWenegd sJx a] ²Jdfbu¸bqBi±tna#¯
}T~b#a lnJa@egd ¿a®hB¢lnJaÁ8z¢¼e» ¼'Ja@egacb#h]i~}Ëo®a#lnJhT}5dfe>lnh
h]tu}Tat"lnJa ­~qBtnaiWlue6d iq}Tacenb#ai~}Td iJy3h]tu}Tat6hB{lnJa®egd ¿ahB
lnJad t>Á8z¢¼e»{j_d i~b#aÆh]Jt8­JJtn­9hWegaÆdfe¢lnh®o®d iJd o®d ¿a>lnJa:egd ¿a
hB¢Á8z¢¼e_lnJaegacb#h]i~}3JaJtndfemlndfb>dfeo®h]tnaÆa#Í)acb´lnd ]a]J²Jdfbu
²8aÆ²d x x~egaÆd iqBx xlnJa:a#ÅT­9atnd o®aiWlue'hBrlnJdfe­~qB­9atc»
© ÒVÄ"Õ  ª ¨JÕmÒ Ñ 8¨T©Jª ¨O
Â ©gNÃ"ÕmÒ¢Ô6§·¤ 
  J
] Ò{© Ð Ò{©T¨Tª Ñ « \ Ô Ñ «9¨JÕmÒ Ñ §
Á

Õ+r§

©Jtnh]oG¢ÎW~qUlnd h]i°¡J~²8abqBiËegaaÆln~qUl"lnh-sJJd xf}q ¾q]b´lnh]tnd ¿#¯

qBsJx a emlntn~b´lnJtna])²8a iJaac}lnh3q]}J}Ëo@qBi_kqBtubelnhlnJa iJa²
emlntn~b´lnJtnaË²JaiO²8a~q]aË}TdfqBy]iJhWemlndfba_df}Tai~b#a]»¹¼'Jacega
a#Å_lntuqqBtube®o@qB³]aqiJa#lm²8h]tn³¸o®h]tnab#h]o®­Jx a#Å½qBi~}Yo@qB³]a
lnJa®bqBxfb#JxfqUlnd h]i5hB,lnJaÁ8z¢¼e6o~bu5o®h]tna }Td À-b#Jx lc»"x ¯
lnJh]Jy]²8a>­Jtnh]­9hWega>egh]o®a6JaJtndfemlndfbe¢h]t8o®d iJd o®d ¿d iJy lnJa
egd ¿a@hB6Á8z¢¼elnJacega-bqBi±emlnd x x¢y]tnhU²¹lnh_hxfqBtny]a]»@vwi¾q]b´lc
lnJdfe{­JtnhTb#acene dfe,­Jtuq]b´lndfbqBx x kacÎWJd UqBx aiWl,lnh:a#ÅJq]b´l{d iTatnai~b#a
d ilnJa'iJa#lm²8h]tn³)»r¼'Jatna#h]tna]²8a8~eg~qBx x kÆh]iJx k:~ega'qB­J­JtnhÅ_¯
d o@qUlnd h]i~e8hBlnJa6Jx x)h]tno@e
» U"atna>²8a6²d x xtna_d a² ega]atuqBx
qB­J­JtnhÅTd o@qUlnd h]iemlntuqUlnay]d ace6~egac}s_klnJa@a#ÅTdfemlnd iJyd o®­9h]tg¯
luqBi~b#a:enqBo®­Jx d iJy®qBx y]h]tnd lnJo@e{h]t°'qk]acegdfqBi-iJa#lm²8h]tn³Te» Ça
²d x x{~ega-qtnJiJiJd iJya#ÅJqBo®­Jx a®lnhd x x ~emlntuqUlna®lnJacega-emlntuqUlna#¯
y]d ace»
H «Q® ^¾L Å Èegd o®­Jx aÆ°'qk]acegdfqBi@iJa#lm²8h]tn³®²d ln-lnJtnaa>sJd ¯
{( g
i~qBtnkYUqBtndfqBsJx ace®d [
i © d y]Jtna¸­~qBtuqBo®a#lnatnd ¿ac}½q]e®h]x x hU²"eÖ
K
Æ K
j@ÀÈ

b

n

J» 
J» ~

ÆÇ
K

r oÉ Â

J» Ç ¡,¡
J» J

ÆËÊ

ÎÏ
ÌÔ Í

Ð ÐÑ

Æ K
Æ

J» JÇ 
J» ¡,¡

Ò
ÎÏ Ò Ó
ÌÖ Í

Ç

J» s
J» @

JÇ » 
J» ¡

Æ

J» Ç¡
J» 

ÎÏ
ÌÕ Í

© d y]Jtna®]Ö{¹egd o®­Jx a°'qk]acegdfqBi-iJa#lm²8h]tn³)»

×{qBtndfqBsJx a'È¹dfeh]s~egatn]ac}-d iemluqUlna ÆËÊ »,ÇabqBi3acq]egd x kbqBx ¯
b#JxfqUlnaÆlnJa­9hWemlnatnd h]t,ºmh]d iWl6}Tdfemlntnd sJTlnd h]ihU]at R
r qBi~} É »
n
j Àr oÉ ÆËÊ Â
@
K
Æ K
J» ]W7D J» W~ B]
J» c],¡ J» FWD ,~
ÆÇ
Ç
Ø
²®7S ¾
K ¾I HJ^%A
F Ù »ÚoÛ gH ÜcL_P ­ «Q®b~SS cHJ
I Ý]ß
L ÞrN 
I ÝF9I Ö
z tnh]s~qBsJd x dfemlndfb x h]y]dfb@enqBo®­Jx d iJyHB SE'q]enegJo®ace>ln~qUllnJa-d o ¯
{
­9h]tgluqBi~b#a6Ji~b´lnd h]i~q]e8lnJa:enqBo®aÁ8z¢¼e¢h]t"qBx x)lnJaÆUqBtnd ¯
qBsJx ace»,pd ³]ax d Jh_hT}3²8ad y]Wlnd iJ
y B @J ¡SEy]h_aceq®emlna­3JtglnJat
qBi~}@q]enegJo®ace,ln~qUl¢lnJa6d o®­9h]tgluqBi~b#aJi~b´lnd h]i-~q]e,lnJa"h]x ¯
x hU²d iJy h]tno»
j@À

n

>m ?



{ÆÂT

j@À 

>5n

Ã

>

jCrÀ 

>

Â  {  {HàjCrÀ  ÂgÂ

ÇÆ

K

J» D
J» ]

Ç

Æ K

J» B
J» 7 D
Ø

­ FAÙ »ÚoÛ HgÜcL_P

­ «Q®b~SScHJIÝ]LÞrN IÝF9I ÖÆj_a]atuqBx qBx y]hB¯

tnd lnJo@eÆiJhBlndfb#alnJa®x d o®d luqUlnd h]i~e>hB¢lnJa®d o®­9h]tgluqBi~b#aJi~b´¯
lnd h]i·~egac}·s_k±x d ³]ax d Jh_hT}¸²8ad y]Wlnd iJy5qBi~}·­Jtnh]­9hWegaq5}Td õ¯
atnaiWl'h]tno|hB d o®­9h]tgluqBi~b#a6Ji~b´lnd h]i»{¼'Jakemlnd x xq]enegJo®a
lnJa:enqBo®a:emlntn~b´lnJtna6h]t8lnJa:d o®­9h]tgluqBi~b#a6Ji~b´lnd h]iq]e8lnJa
h]tnd y]d i~qBx]°'qk]acegdfqBi>iJa#lm²8h]tn³)sJTlrlnJak>tnacqBx d ¿a,ln~qUllnJa¢aW¯
df}Tai~b#a>~q]e8d iTÌ~Jai~b#aÆh]i-lnJaÁ8z¢¼e¢hB qBx x9lnJaÆiJhT}Tace'qBi~}
­Jtnh]­9hWega6lnJaÆh]x x hU²d iJy h]tnohB d o®­9h]tgluqBi~b#a>Ji~b´lnd h]i»
j@À
>5n

n

{ÆÂb > m ?
>



Ã

j@À 

>on

jCrÀ 

>

Â{ Æ
{ Â 

j@Àr oÉ Â
ÇÆ
Ç

ÀmcWÂ

v l"~egace8lnJaenqBo®aÁ8z¢¼e'q]e8lnJa:h]tnd y]d i~qBx}Tdfemlntnd sJTlnd h]i3h]t
iJhT}Tace'²d ln3iJh®a_df}Tai~b#aÆ­~qBtnaiWlue» ¬ lnJatn²dfega]_d l6egJtnd iJ³Te
Á8z¢¼eh]t lnJhWegaiJhT}Tace ²d ln¦a_df}Tai~b#a3­~qBtnaiWlue» ¬ s__d ¯
h]~egx k]~lnJdfeÆqB­J­JtnhÅTd o@qUlnd h]ih]iJx kluqB³]ace6d iWlnhq]bb#h]JiWl6lnJa
­Jtnd o®d lnd ]a'd iTÌ~Jai~b#a'hB~lnJa'a_df}Tai~b#a]UlnJa'd iTÌ~Jai~b#a'hB~lnJa
a_df}Tai~b#aÆiJhT}Taceh]ilnJa Á8z¢¼e'hBrlnJad t6buJd xf}Ttnai»
H «Q® ^¾L Å ¬ tnd y]d i~qBx9Á8z¢¼8¯ s~q]egac}vwo®­9h]tgluqBi~b#a ©JJi~b´lnd h]i
{( g
h]t'lnJa:tnJiJiJd iJy®a#ÅJqBo®­Jx a]»
j@Àr oÉ Â

qBi~}È`6tnJ¿c}T¿az
x B BS E}Tatnd ]alnJah]tnoJxfq½hB®bqBxfb#JxfqUlnd iJy
lnJa:vnÁ8z¢¼e'~egd iJy@s9ax d a#,­Jtnh]­~qByWqUlnd h]i-o®acenenqBy]aceCB cTD7E »
¼'Ja-d o®­9h]tgluqBi~b#a@Ji~b´lnd h]i·d H
i G¢ÎW~qUlnd h]i¦]@hBÍ)atueqsJd y
d o®­JtnhU]ao®aiWlÆhU]at>lnJa@tna­JtnacegaiWluqUlnd h]iËhB
 G¢ÎW~qUlnd h]iYcJ»
U"hU²8a]atcrlnJdfe tna­JtnacegaiWluqUlnd h]i·emlnd x x8h]iJx kluqB³]aced iWlnhq]b´¯
b#h]JiWl-­~qBtglndfqBx"d iTÌ~Jai~b#aËhB>lnJaËa_df}Tai~b#a]»¶vwi bq]egalnJa
a_df}Tai~b#a½}TtuqBo@qUlndfbqBx x kÊbu~qBiJy]ace5}Ta­9ai~}Tai~b#a½tnaxfqUlnd h]i~e
qBo®h]iJylnJa@UqBtndfqBsJx ace)lnJdfeqB­J­JtnhÅTd o@qUlnd h]iË²d x x{s9a@egJsT¯
h]­Tlnd o@qBxq]e'²8ax x»
H «Q® ^¾L Å vnÁ8z¢¼8¯ s~q]egac}-vwo®­9h]tgluqBi~b#C
{( g
a ©JJi~b´lnd h]i3h]t'lnJa
tnJiJiJd iJy®a#ÅJqBo®­Jx a]»

Àm]Â

G¢q]buj@À  jCrÀ  Â  {  {ÆÂ dfeÊbqBx x ac}0qBi ó21"ø~ðUù´ôwöUñ)ïuü
¦V§'¨¹ÀvnÁ8z¢¼"Â´{q±b#h]i~b#a­Tl É~tueml@­Jtnh]­9hWegac}·d iyB E »áU"hU²'¯

a]atcTlnJatnaqBtnaq]b´ln~qBx x ko@qBi_k3qBx y]h]tnd lnJo@e8ln~qUl6~ega:lnJa
qBs9hU]a3d o®­9h]tgluqBi~b#a3Ji~b´lnd h]i'd i¦eg­Jd lnahB6lnJa¾q]b´l@ln~qUl
lnJak }Td Í)atd i lnJa±o®a#lnJhT}Je3hBacemlnd o@qUlnd iJyYlnJa¸q]b´ln~qBx
luqBsJx ace» j_a]atuqBxÆ}Tk_i~qBo®dfb5d o®­9h]tgluqBi~b#aenqBo®­Jx d iJy¦qBx y]hB¯
tnd lnJo@e d i~b#x ~}Td iJyÈ"vgjW¯w° WâB  E ®egax õ¯ d o®­9h]tgluqBi~b#a½enqBo ¯
­Jx d iJ
y B  ¡SE _qBi~}-q]}JqB­Tlnd ]a"d o®­9h]tgluqBi~b#a6enqBo®­Jx d iJ
y B  @SE W~ega
}Td Í)atnaiWlÆx acqBtniJd iJyo®a#lnJhT}Je6lnhx acqBtnilnJa vnÁ8z¢¼e» ã{~qBi

K

J» ,~,~]
J» J DW

Æ K

J» sU,¡&s
J»  sU
Ø

|}Tk_i~qBo®dfb d o®­9h]tgluqBi~b#a®enqBo®­Jx d iJyqBx y]h]tnd lnJoC~egd iJy3lnJa
qBs9hU]a'h]tnoÿo@qk x acqBtni@q}Td Í)atnaiWl8d o®­9h]tgluqBi~b#a'Ji~b´lnd h]i
}Ta­9ai~}Td iJy3h]iË²~qUlÆ}TdfemluqBi~b#ao®acq]egJtnad l>lntnd ace"lnho®d iJd ¯
o®d ¿a]» ©Jh]t>lnJa®tnJiJiJd iJya#ÅJqBo®­Jx a]²8a®iJaac}lm²8h3­~qBtuqBo ¯
a#lnatue>lnh­~qBtuqBo®a#lnatnd ¿alnJa@d o®­9h]tgluqBi~b#a Ji~b´lnd h]i»®v 8²8a
~eg
a ä>¯wp}Td ]atny]ai~b#a¢q]elnJa'}TdfemluqBi~b#a¢o®acq]egJtna]²8a¢y]a#l lnJa
enqBo®aegh]x Tlnd h]i~e>q]e6qBs9hU]a]»"v ¢²8ao®d iJd o®d ¿alnJaUqBtndfqBi~b#a
hB)lnJa6d o®­9h]tgluqBi~b#a6enqBo®­Jx d iJyacemlnd o@qUlnh]tcBlnJa6x acqBtniJac}®d o ¯
­9h]tgluqBi~b#a-Ji~b´lnd h]iY~q]elnJa3h]x x hU²d iJy@ºmh]d iWl®­Jtnh]s~qBsJd x d lmk
}Tdfemlntnd sJTlnd h]i»
H «Q® ^¾L Å vwo®­9h]tgluqBi~b#a ©JJi~b´lnd h]i-x acqBtniJac}®s_k®o®d iJd o®d ¿#¯
{( g
d iJy:lnJa6UqBtndfqBi~b#ahB)lnJa6d o®­9h]tgluqBi~b#a"enqBo®­Jx d iJyacemlnd o@qUlnh]t
h]t'lnJa:tnJiJiJd iJy®a#ÅJqBo®­Jx a]»
j@Àr oÉ Â
ÇÆ
Ç

K

J» F@J
J» , @&s7¡

Æ K

J» FD~&s
J» c]F D
Ø

ÇaËbqBi¦egaaln~qUl-²8abqBiJiJhBl-q]buJd a]a3lnJah]­Tlnd o@qBxd o ¯
­9h]tgluqBi~b#aÆJi~b´lnd h]is_kx acqBtniJd iJy~»¢¼'Ja:tnacq]egh]i3dfe'ln~qUl"lnJa
q]b´ln~qBxr­9hWemlnatnd h]t6}Tdfemlntnd sJTlnd h]ilmk_­JdfbqBx x k3iJaac}Je"o®h]tna­~qU¯
tuqBo®a#lnatuelnh±­~qBtuqBo®a#lnatnd ¿a-ln~qBi½lnJad o®­9h]tgluqBi~b#a3Ji~b´¯
lnd h]i
» ©Jh]t lnJatnJiJiJd iJy±a#ÅJqBo®­Jx a],lnJaq]b´ln~qBx­9hWemlnatnd h]t
}Tdfemlntnd sJTlnd h]i¸iJaac}Je:lnJtnaa@­~qBtuqBo®a#lnatue» ¬ s__d h]~egx k]rd ldfe
d i y]aiJatuqBx_d o®­9hWenegd sJx a¢lnhÆ­9atgacb´lnx k:ÉJl¢q"lnJtnaa#¯ ­~qBtuqBo®a#lnat
}Tdfemlntnd sJTlnd h]i²d lnqlm²8hB¯ ­~qBtuqBo®a#lnat'}Tdfemlntnd sJTlnd h]i»
H 9I ÚoÛ gH ÜcL_P ­ «Q®b~SS cHJ
I Ý]L ÞrN 
I ÝÚ
å HT7S ¾H Û ^¾L { ^ «H¾I O
9I b
Ö U"atni~qBi~}Ta¿Æa#l>qBxV
» B ~SEr­Jtnh]­9hWega>lnh-~egaÆlnJa:UqBtndfqBsJx a
ax d o®d i~qUlnd h]i B T E6qBx y]h]tnd lnJo&lnh¸ax d o®d i~qUlna3lnJaUqBtndfqBsJx ace
h]iJa's_kh]iJa'd i®h]tu}Tat lnhÆy]a#l lnJa>Á8z¢¼e»rv 9lnJa"bqBxfb#JxfqUlnd h]i
bqBi s9abqBtntnd ac}h]Tl,a#ÅJq]b´lnx k]lnJak²d x xTy]a#l lnJa'a#ÅJq]b´l h]tno
hB~lnJad o®­9h]tgluqBi~b#a8Ji~b´lnd h]i@q]e d 
i G¢ÎW~qUlnd h].
i ¡JË
» U"hU²8a]atc
UqBtndfqBsJx a8ax d o®d i~qUlnd h]i dfe d iTacq]egd sJx a8h]t xfqBtny]a8b#h]o®­Jx a#ÅiJa#lg¯
²8h]tn³Te»¼'Jatna#h]tna]lnJakÆega#l q'lnJtnacegJh]xf}6lnhlnJaÁ8z¢¼egd ¿a]»

ÇOJaiJa]at-lnJa5egd ¿aËhBq½Á8z¢¼ y]aiJatuqUlnac}¦²JaiOax d o®d ¯
i~qUlnd iJy®qUqBtndfqBsJx a6a#ÅJb#aac}Je{lnJa>lnJtnacegJh]xf}WlnJak@y]aiJatuqUlna
oJx lnd ­Jx a-ego@qBx x at>luqBsJx ace>lnhqB­J­JtnhÅTd o@qUlnalnJa-egd iJy]x a sJd y
luqBsJx a]TeghlnJatna>dfe¢iJh a#ÅT­Jx dfb#d l¢h]tno*h]t¢lnJad t'd o®­9h]tgluqBi~b#a
Ji~b´lnd h]i»
©Jh]t6lnJa@egd o®­Jx a tnJiJiJd iJya#ÅJqBo®­Jx a]egd i~b#aUqBtndfqBsJx aax d o®d ¯
i~qUlnd h]i¸bqBi±s9a-bqBtntnd ac}5h]Tla#ÅJq]b´lnx k]
 U"atni~qBi~}Ta¿@a#l qBx» æ e
o®a#lnJhT}3dfe"qBsJx a>lnh®y]aiJatuqUlna>lnJa:a#ÅJq]b´l>Á8z¢¼e»
H «Q® ^¾L Å ×{qBtndfqBsJx H
{( g
a G{x d o®d i~qUlnd h]iT¯ s~q]egac}Èvwo®­9h]tgluqBi~b#a
©JJi~b´lnd h]iËh]t>lnJa tnJiJiJd iJy3a#ÅJqBo®­Jx a]»:v ¢²8a ax d o®d i~qUlnan É
s9a#h]tna r_²8a>y]a#l'b#h]i~}Td lnd h]i~qBx~h]tno@
e j@À r>Â¢qBi~´
} j@À É r>Â´
²JdfbuqBtna
K

j@À É
ÇÆ
Ç

n

Æ K
r>Â

n

n
j@À)ë n íUÂ
ï !®À)ë ì)ÂTðñ

j@À)ì Uí Â 
ñ

K

J» WF@W
J» ¡&sU, ~

Àm]Â

U"aiJtnd h]iQB sE ~q]e"egJhU²i3ln~qUl"h]t"lnJabqB~enqBxx d iJ³d ik© d yB¯
JtnaT_lnJa:egai~egd lnd _d lmk-tuqBiJy]a ï !®À)ë  ì)Â8²d ln3tnaceg­9acb´l'lnhí
enqUlndfemÉ~ace8lnJaÆh]x x hU²d iJy d iJacÎW~qBx d lmk]»
n

ÎÏ
Ì ôÍ


J» c,@,@
J» ~,¡]& s
Æ K

ï !®À)ë )
ì Â

Ð ÐÑ

n&ò

 

ó

ÎÏ
Ìõ Í

Àm@WÂ

ÎÏ
Ìö Í

© d y]JtnaTÖ,¹bqB~enqBxx d iJ³)»

J» ¡W7D_
J» F D_]
Ø

¹egd o®­Jx abqBxfb#JxfqUlnd h]iegJhU²"e8ln~qUllnJa:d o®­9h]tgluqBi~b#aÆJi~b´¯
lnd h]iEdfeRd i~}Taac}&acÎWJd UqBx aiWlRlnh lnJa q]b´ln~qBxY­9hWemlnatnd h]t
}Tdfemlntnd sJTlnd h]iç
» U"hU²8a]atc®h]t·xfqBtny]at¸o®hT}TaxfeUqBtndfqBsJx a
ax d o®d i~qUlnd h]iT¯ s~q]egac}·d o®­9h]tgluqBi~b#aJi~b´lnd h]i¦hBõlnai½iJaac}·lnh
~egaega]atuqBx¢luqBsJx acelnh±qB­J­JtnhÅTd o@qUlna3qegd iJy]x a3sJd y5luqBsJx a]»
j_d i~b#a8lnJaqB­J­JtnhÅTd o@qUlnd h]idfe,}Ttnd ]aio®hWemlnx k:s_k:luqBsJx a'egd ¿a]
lnJa:qB­J­JtnhÅTd o@qUlnd h]ibqBis9a:qBxfegh®egJsT¯ h]­Tlnd o@qBx»{jTqBx o®atnh]i
ü#ô'öU÷ VB 7 sExfqUlnat­Jtnh]­9hWega"lnh®d o®­JtnhU]a"lnJa:qB­J­JtnhÅTd o@qUlnd h]i
~egd iJy®­Jtnh]s~qBsJd x d lmk@lntnaace8lnh®tna­JtnacegaiWl'lnJa Á8z¢¼e»
è

n

hBj@À)ë íUÂ¢²d lntnaceg­9acb´l'lnh´j@À)ì íUÂ´Ö

G¢enegaiWlndfqBx x k]7 U"aiJtnd h]iegJhU²"eln~qUl lnJa8a_df}Tai~b#a8h]i q"iJhT}Ta
~ q]e"o®h]tna:d iTÌ~Jai~b#ah]id lue"d o®o®ac}TdfqUlna buJd xf}Ttnailn~qBid lue
JtglnJat}Tacenb#ai~}JqBiWlue»¤W"hU²:6²8aa#Å_lnai~} lnJatnacegJx l3lnh
o®h]tna3y]aiJatuqBx'enb#ai~qBtnd hWeQ
» © d tuemlc{x a#l@~e x h_h]³·qUl lnJa}Td ¯
qBy]iJhWemlndfb:x d iJn ³d k
i © d y]Jtnn.
a @J»>Ä>d ]ai5b#h]i~}Td lnd h]i~qBx d i~}Ta­9aiT¯
}Tai~b#a]¿ j@)À ë ì)T
Â lj@)À ë ì  íUÂ´»,Ça:~q]a
n

n

n

n

j@À)ë íUÂlj@À)ë ì)Â+j@À)ì íUÂV÷Hj@À)ë Æ ì)Â#Àm PIj@À)ì íUÂgÂ  ÀmD_Â
ÎÏ
Ì ôÍ


¤ Ñ WÐ Ñé Ô  Ñ «ê _ ª,§rÓ0¤ 
© ÒVÄ"Õ  ª ¨JÕmÒ Ñ
  J
8¨T©Jª O
¨  a ] Ò{© Ð Ò{©T¨Tª Ñ «  \ Ô Ñ «9¨JÕmÒ Ñ §

vwi-lnJaÆ­Jtna_d h]~e8egacb´lnd h]iT²8aÆqBtny]Jac}®ln~qUlegh]o®aÆqB­J­JtnhÅTd ¯
o@qUlnd h]i~e{o@qk iJhBl¢s9a>qBsJx alnhqB­J­JtnhÅTd o@qUlna'lnJa6­9hWemlnatnd h]t
}Tdfemlntnd sJTlnd h]i²8ax x»rÇa8iJhU²¸s9ay]d i:lnhÆ}Tdfenb#~enerh]iJa8qB­J­JtnhÅ_¯
d o@qUlnd h]i3emlntuqUlnay]kln~qUl'dfe8s~q]egac}@h]i-lnJa>d iTÌ~Jai~b#aÆqBo®h]iJy
UqBtndfqBsJx ace8d iq®°'qk]acegdfqBiiJa#lm²8h]tn³)»
© d tuemlc,²8a­JtnhU_df}TaqBiYqBi~qBx kTegdfe:hBlnJa3d iTÌ~Jai~b#ahB6a_d ¯
}Tai~b#a]»3Ça³_iJhU²µln~qUl }TdfqBy]iJhWemlndfb a_df}Tai~b#a-o@qB³]aceÆlnJa
qBi~b#acemlnh]tue:hBa_df}Tai~b#a-iJhT}Tace b#h]i~}Td lnd h]i~qBx x k}Ta­9ai~}TaiWlc»
Ça>iJaac}@lnho®hT}Tax9lnJa>o®hWeml8d o®­9h]tgluqBiWl}Ta­9ai~}Tai~b#a>tna#¯
xfqUlnd h]i~e{d ih]tu}Tat,lnhh]sTluqBd iqy]h_hT}®d o®­9h]tgluqBi~b#aJi~b´lnd h]i»
¬ iJa ~ega#Jx,o®acq]egJtnalnh3o®hT}Tax lnJa®tnaxfqUlnd ]a emlntnaiJyBln5hB
lnJaË}Ta­9ai~}Tai~b#atnaxfqUlnd h]i~e@qBo®h]iJylnJaUqBtndfqBsJx ace dfe®lnJa
ýü#ñJý´óõô2ó xcóõôûùnöU&ñ WühB lnJa­Jtnh]s~qBsJd x d lmkhB{qBia]aiWl ë3²d ln
tnaceg­9acb´l¢lnh®qBi-a]aiW
l ìQB sE 
» hh]tna"h]tno@qBx x k]TegJ­J­9hWega"ln~qUl
| líÆdfe¢lnJaÆh]s~egatn]ac}@a_df}Tai~b#a>²Jdfbuo®d y]WlqUn Í)acb´l'lnJa
q]enegacenego®aiWl hB9lnJa­Jtnh]s~qBsJd x d lmk:hB ìr]y]d _d iJC
y j@)À ì íUÂ´»{j_J­T¯
­9hWega{ln~qU
l î dfe b#h]i~}Td lnd h]i~qBx x kÆd i~}Ta­9ai~}TaiWl hB | y]d ]ai  »
¼'Jai3lnJaegai~egd lnd _d lmktuqBiJy]a>dfe"}Ta#É~iJac}q]e8lnJa}Tatnd UqUlnd ]a

n

Ðø Ð

ÎÏ
Ì õÍ


ù

ÎÏ
Ì öÍ


© d y]JtnaC@JÖ{¹}TdfqBy]iJhWemlndfb6x d iJ³)»
n

¼ qB³_d iJy lnJa}Tatnd UqUlnd ]a>²d lntnaceg­9acb´l'lnhj@À)ì íUÂ´~²8aÆy]a#lc
n
n
ï !®À)ë )
ì ÂT[j@À)ë )
ì ÂËPIj@À)ë Æ ì)Â 

Àm]Â

¬ s__d h]~egx k]
 G¢ÎW~qUlnd h]iO @5qBxfeghËJh]xf}Jeh]tlnJa}TdfqBy]iJhWemlndfb
x d iJ³)»®v legJhU²"e6ln~qUl:lnJa®a_df}Tai~b#a®h]i±q3iJhT}Ta®~q]eÆo®h]tna
d iTÌ~Jai~b#a"h]i@d lue,d o®o®ac}TdfqUlna6­~qBtnaiWlue ln~qBi@d lue,JtglnJat8qBiT¯
b#acemlnh]tue
» W"hU²:rx a#l~ex h_h]³qUlqo®h]tna@d iWlnatnacemlnd iJyËbq]ega]»
j_J­J­9hWega¢²8a8~q]a¢qBid iWlnatubqB~enqBx]iJa#lm²8h]tn³Æq]erd .
i © d y]JtnT
a D~
²8aÆ~q]a
n
nú
í UÂuûj@À)ë n ì)Â+j@À)ì  íUÂV÷ n ú
j À)ë Æ ì)Â#Àm PI@
@
j À)ì  íUÂgÂ 
ÀmcWÂ
nú
¼ qB³_d iJy"lnJa}Tatnd UqUlnd ]a¢²d ln tnaceg­9acb´l lnh@
j À)ì  íUÂ´U²8a8y]a#lc
n
n
ï ®
! À)ë  )
ì ÂT[@
j À)ë )
ì ÂËPI@
j À)ë Æ )
ì Â 
Àm7sBÂ
j@À)ë

nú

Ò
ÎÏ Ò Ó
ô
Ì Í

Ðø Ð

ÎÏ
Ì üÍ


ÎÏ
Ì õÍ


ù

ÎÏ
Ì öÍ


© d y]JtnaD~Ö,"id iWlnatubqB~enqBx)x d iJ³)»

"yWqBd i7G¢ÎW~qUlnd h]i-@"qBxfegh6Jh]xf}Jeh]trlnJdfe bq]ega]» ¼'JdfertnacegJx l
egJhU²"e¢ln~qUlc~qBx lnJh]Jy]qBi3a_df}Tai~b#a:iJhT}TaÆd iWlntnhT}T~b#ace}Ta#¯
­9ai~}Tai~b#aqBo®h]iJy®d lue"qBi~b#acemlnh]tueWlnJaemlntnaiJyBln3hB lnJa}Ta#¯
­9ai~}Tai~b#a:²d x xs9acb#h]o®aÆ²8acqB³]atq]e¢lnJa}TdfemluqBi~b#aÆs9a#lm²8aai
lnJaqBi~b#acemlnh]tue8d i~b#tnacq]egace»
¼rh5egJo®o@qBtnd ¿a] h]Jt}Tdfenb#~enegd h]i¸acenegaiWlndfqBx x kegJhU²"eÆln~qUlc
d iËy]aiJatuqBx9q]elnJa }TdfemluqBi~b#atnh]o q-UqBtndfqBsJx a:lnh@lnJa a_d ¯
}Tai~b#a s9acb#h]o®ace6xfqBtny]at6d iq°'qk]acegdfqBiiJa#lm²8h]tn³)~lnJa®a_d ¯
}Tai~b#a~eg~qBx x k~q]e x acene,d iTÌ~Jai~b#ah]i lnJa­9hWemlnatnd h]t{}Tdfemlntnd ¯
sJTlnd h]ihBrlnJaÆUqBtndfqBsJx a]»,"xfegh~_lnJa}Ta­9ai~}Tai~b#aÆtnaxfqUlnd h]i~e
qBo®h]iJylnJa>d o®o®ac}TdfqUlna6­~qBtnaiWlue¢hB qBi-a_df}Tai~b#a6iJhT}TaÆqBtna
emlntnh]iJy]at>ln~qBi5lnJhWega@qBo®h]iJyd lue>JtglnJatqBi~b#acemlnh]tue» Ça
bqBi¸qBxfeghegJhU²µlnJaenqBo®a®tnacegJx lue:~egd iJyËqBiJhBlnJat}Ta­9aiT¯
}Tai~b#aËo®acq]egJtna5bqBx x acy
} 1.4Tô 4~öU÷óõñ 3#ðUù 1@öUôó¾ðUñ~6²JdfbuO²8a
h]o®d lh]tsJtna_d lmk]»
>5n
>
>
©Jh]t·acq]bu Á8z¢>¾
¼ j@À  jCrÀ  Â  {  !@À  ÂgÂ±> d ¤
i G¢ÎW~qU¯
lnd h]k
i ¡J jCrÀ  Â"qBtnad o®o®ac}TdfqUlna­~qBtnaiWluehB  )egh@lnJad t
d iTÌ~Jai~b#a"qBtna'~eg~qBx x kemlntnh]iJy~» | b#h]iWluqBd i~e h]s~egatn]ac}UqBtnd ¯
qBsJx ace eghd lh]iJx k5tnac}T~b#aceÆlnJa-b#h]o®> ­Jx a#ÅTd lmk5hB'lnJaÁ8z¢¼Æ»
U"hU²8a]atc{lnJaU> qBtndfqBsJx ace d [
i !@À  Â ~q]aUqBtnk_d iJy±}Tdfem¯
luqBi~b#acetnh]o  
» ©Jtnh]o&h]Jt@qBi~qBx kTegdfe{²8a3s9ax d a]a3ln~qUl
s_klnJtnhU>²d iJy-q²'qk@lnJaUqBtndfqBsJx ace'ln~qUlÆqBtnaÆJtglnJat>q²'qk
tnh]o  T²8abqBiemlnd x xtnacegatn]a>q y]h_hT}3qB­J­JtnhÅTd o@qUlnd h]ihB
lnJah]tnd y]d i~qBx'Á8z¢¼Æ» ¼'Jatna#h]tna]r²8a-­Jtnh]­9hWega@lnh5qB­J­JtnhÅTd ¯
o@qUlna,lnJa{Jx xWd o®­9h]tgluqBi~b#a Ji~b´lnd h]is_kÆq]}J}Td iJy6q]}J}Td lnd h]i~qBx
qBtube h]iJx k®qBo®h]iJy>lnJa"­~qBtnaiWlue,hB)a_df}Tai~b#a]»,°8ko®hT}Tax d iJy
lnJaÆo®hWeml'd o®­9h]tgluqBiWl'q]}J}Td lnd h]i~qBx}Ta­9ai~}Tai~b#a:qBo®h]iJylnJa
UqBtndfqBsJx ace]²8a>bqBi-qBiWlndfb#d ­~qUlna"ln~qUl¢lnJa6d o®­9h]tgluqBi~b#a"Ji~b´¯
lnd h]i¸bqBis9a-ÎWJd lna-b#x hWega lnh3lnJa-q]b´ln~qBx,­9hWemlnatnd h]t}Tdfemlntnd ¯
sJTlnd h]iT
» äÆaa­Jd iJy@q]}J}Td iJy@qBtube8o@qB³]ace8lnJa:iJa#lm²8h]tn³@o®h]tna
b#h]o®­Jx a#Å_²Jdfbuo@qk@h]iJx k®sJtnd iJy o®d iJd o@qBx)d o®­JtnhU]ao®aiWl
sJTl"h]iJx k-o@qB³]ace8lnJab#h]o®­JTluqUlnd h]i3o®h]tna:b#hWemlnx k]»
ý

þ Ä  ©~Õ 

 Ñ ¨Tª Aÿ r§_Ô

¨J§

¼rh5ºm~emlnd k h]Jt3­Jtnh]­9hWegac} qB­J­JtnhÅTd o@qUlnd h]i emlntuqUlnay]k]"²8a
lnacemlnac}d l:h]i5lnJa 
qBx y]h]tnd lnJo» Ça@­9atgh]tno®ac}
h]Jt"a#ÅT­9atnd o®aiWlue"h]ilnJaQB SE  B SE 9qBi~}
"!$#½B S EJiJa#lm²8h]tn³Te» ¬ Jt,b#h]o®­~qBtndfegh]i²'q]e s~q]egac}
h]iÊlnJaYq]atuqBy]&
a %ü#÷õ÷ óõ&ñ Wü#$ù ' ý½òUófý´ôwöUñ)ïuH
ü B ¡7E s9a#lm²8aai¹a#Å_¯
q]b´l,­9hWemlnatnd h]t o@qBtny]d i~qBxfe hB)qBx xTJiJh]s~egatn]ac}:UqBtndfqBsJx ace qBi~}
enqBo®­Jx d iJy3tnacegJx lue» U"ax x d iJy]at æ eÆ}TdfemluqBi~b#a®k_d axf}Je>df}TaiWlndfbqBx
tnacegJx lue q]A
e äÆJx x s~q]bu³W¯wpad sJx at}Td ]atny]ai~b#a-d i·o®hWeml®bq]egace

sJTlrd lueo@qUºmh]trq]}TUqBiWluqBy]a dfeln~qUlrd l bqBi:~qBi~}Tx a,¿atnh­Jtnh]sT¯
qBsJd x d lnd aceB²Jdfbu@qBtna'b#h]o®o®h]i d i@°'qk]acegdfqBiiJa#lm²8h]tn³Te»rÇa
d o®­Jx ao®aiWlnac}:h]JtrqBx y]h]tnd lnJo¶d i 
Á ÷C÷·qBi~}Æ­9atgh]tno®ac}Æh]Jt
lnacemlue'h]i3q@T» ~®
Ä U"¿ (6ah]i3ÇOd i~}ThU²")
e (>z¦b#h]o®­JTlnat²d ln
]ÄÆ°Oo®ao®h]tnk]»

* Z,+L.-9¾L./ 1 0cG L½Ù6LWS20t~S7«·HJIÝ]L½10 { Ù ­43 Ú657
¼'Ja& 
qBx y]h]tnd lnJodfe­Jtnh]­9hWegac} s_káã{~qBiÊqBi~}

`6tnJ¿c}T¿axÆd iB BSE 6²JhWega5o@qBd i¶df}TacqYdfe-lnh½~ega±ega]atuqBx
emlna­~e3hB-÷ ðcðnø)û úuü#÷ ó¾ü 3@ø)ùnðnø~ö WöUôó¾ðUñRÀ¾pr°'z8k
Â B cS EÆlnhYacemlnd ¯
o@qUlna8qBid o®­9h]tgluqBi~b#a{Ji~b´lnd h]ih]t d o®­9h]tgluqBi~b#a8enqBo®­Jx d iJy~»
G,ÅT­9atnd o®aiWluqBxrtnacegJx lue6d i B BS E,egJhU²Êln~qUl6lnJ
a  
qBx y]h]tnd lnJo d o®­JtnhU]aceJ­9h]iËpr°'z qBi~}q]buJd a]ace"q-b#h]i~egdf}_¯
atuqBsJx a'd o®­JtnhU]ao®aiWl hU]at lnJa"emluqUlnahB~lnJa"qBtgl{qBx y]h]tnd lnJo
lnJairlnJ8
a  
B  E 
» ©JJtglnJatno®h]tna]lnJa@tnacegJx lueqBxfegh
egJhU²ln~qUlrlnJ)
a   OqBx y]h]tnd lnJoÈqBx tnacq]}Tk>qB­J­JtnhWq]buJace
lnJaOx d o®d l¸ln~qUlYenqBo®­Jx d iJyÊqBx y]h]tnd lnJo@e±bqBi*q]buJd a]a½h]i
·qBi~9
} "!$#  s9acbqB~ega®lnJa-­Jtnacb#dfegd h]i5ln~qUl
d l@q]buJd a]aceh]i¸lnJacega3iJa#lm²8h]tn³Tedfe®qBx tnacq]}Tkd i·lnJaenqBo®a
h]tu}Tat®q]elnJhWega3hB6­Jtnh]s~qBsJd x dfemlndfb-x h]y]dfbenqBo®­Jx d iJyh]i·lnJa
enqBo®aiJa#lm²8h]tn³Te²d lnJh]Tl®a_df}Tai~b#a]»±vwi·lnJaxfqUlglnat®bq]ega]
egd i~b#alnJatnadfe6iJha_df}Tai~b#ad iËlnJaiJa#lm²8h]tn³Te9x h]y]dfbenqBo ¯
­Jx d iJyËenqBo®­Jx ace>tnh]o lnJa@h]­Tlnd o@qBx{d o®­9h]tgluqBi~b#a Ji~b´lnd h]i
lnJaË­Jtnd h]t}Tdfemlntnd sJTlnd h]i»ÈÇaËs9ax d a]aln~qUl­Jtnacb#dfegd h]iOegh
q]buJd a]ac}dfe lnJax d o®d l¢hBenqBo®­Jx d iJyqBx y]h]tnd lnJo@e
» U"hU²8a]atc
}TJa,lnhlnJa¢­9hBlnaiWlndfqBx]d i~emluqBsJd x d lmk>hBJpr°'zËqBi~}Jai~b#a]­9hWem¯
egd sJx k·­9h_h]t®d o®­9h]tgluqBi~b#aJi~b´lnd h]i~e
   0bqBi¦emlnd x x
­9atgh]tno egJsT¯ h]­Tlnd o@qBx x k]_h]td i~emluqBi~b#a:h]:
i r»

* Z2£;+L&ÜcN ^)Ü-10[=<¢LWSUL_Iª´®®
3 SUHOcL_K¾L&Ü9I 

S7 ( «·HO9I

vwiÆlnJdfea#ÅT­9atnd o®aiWlc²8a,y]aiJatuqUlnac}Æq8lnhBluqBxBhBsB¢lnaceml bq]egace
h]tlnJa& iJa#lm²8h]tn³)»ÿ¼'Jacegabq]egaceb#h]i~egdfemlnac}OhB
É~]aÆegacÎWJai~b#ace8hB¢bq]egace¢acq]bu»©Jh]t'acq]buegacÎWJai~b#a]_²8a
tuqBi~}Th]o®x kbuJhWega:q®}Td Í)atnaiWl"i_Jos9at"hB a_df}Tai~b#aÆiJhT}TaceÖ
  B  ]  @]  @W tnaceg­9acb´lnd ]ax k]»®Ça-~egac}ËlnJtnaa-}Td Í)atnaiWl
h]tno@e hB9d o®­9h]tgluqBi~b#a8Ji~b´lnd h]i®d i lnJdfe a#ÅT­9atnd o®aiWlue» ¼'Ja
É~tueml,h]iJa²'q]e tna­JtnacegaiWlnac}s_kvnÁ8z¢¼e]q]e d 
i G¢ÎW~qUlnd h]i3]]»
©Jh]t8lnJa:egacb#h]i~}@h]iJa]T²8a>h]iJx k-q]}J}Tac}q]}J}Td lnd h]i~qBxqBtube¢s9a#¯
lm²8aai lnJa6­~qBtnaiWlue hB)lnJa"a_df}Tai~b#a"iJhT}TaceË
» ©Jh]t{lnJalnJd tu}
h]iJa]{²8a3bqBtntnd ac}¸h]Tl@"x y]h]tnd lnJo -Jx x kYqBi~}Yq]}J}Tac}·qBx x
lnJaiJacb#acenenqBtnk½qBtube»µÇa5lnJai tuqB>
i   &h]iOlnJa
lnJtnaad o®­9h]tgluqBi~b#a:Ji~b´lnd h]i~e»6¼'JatnacegJx lue6qBtnaegJhU²id i
© d y]JtnaT»
6ea#ÅT­9acb´lnac}6}Td Í)atnaiWltna­JtnacegaiWluqUlnd h]i~e@hB:lnJa5d o®­9h]tg¯
luqBi~b#a@Ji~b´lnd h]i~ek_d axf}Tac}¸}Td Í)atnaiWl atntnh]tue»6}J}Td iJy5qBtube
s9a#lm²8aaiËlnJa®­~qBtnaiWlue>hB¢a_df}Tai~b#a iJhT}Tace>sJtnd iJyWeÆb#h]i~egdf}_¯
atuqBsJx a@tnac}T~b´lnd h]i¸d i¸atntnh]tc»3|­~qBd tnac}±h]iJa#¯luqBd x,lg¯lnaceml qUl
? ÿ  ]]WF ¡®x a]ax,egJhU²"e6ln~qUl>lnJa®d o®­JtnhU]ao®aiWlÆdfeÆegd yB¯
iJd É9bqBiWlc»Ëj_d i~b#a@lnJaiJa²*d o®­9h]tgluqBi~b#a@Ji~b´lnd h]i¸~q]e:a²
iJa²ÿqBtubed lueÆd iTÌ~Jai~b#a@h]iËlnJa@tnJiJiJd iJylnd o®a@²'q]e>o®d iT¯
d o@qBx»-6}J}Td iJyo®h]tna@qBtube6lnhy]a#lÆlnJa@a#ÅJq]b´ld o®­9h]tgluqBi~b#a
Ji~b´lnd h]i½h]tno }Tdf}½iJhBl-d o®­JtnhU]a3lnJatnacegJx lue8sJTl-h]iJx k

−3

−3

x 10

x 10
10

4.5

9

4

8

3.5

Hellinger’s distance

Hellinger’s distance

7
6
5

3

2.5

2

4
1.5

0.0032

3

0.0027

0.0027
1

2

0.00077

0.00074

0.00074

0.5

1
EPIS

Parents
Algorithm

EPIS

All

 

All

x 10

© d y]Jtna-TÖAU"ax x d iJy]atæ e:}TdfemluqBi~b#a@hB8lnJa

5.5
5
4.5
4
Hellinger’s distance

Bq x y]hB¯
tnd lnJo h]i3lnJtnaa}Td Í)atnaiWl"h]tno@ehB d o®­9h]tgluqBi~b#a:Ji~b´lnd h]i
h]i@r»§öUùnü#ñ~ô¾ý"emluqBi~}Je"h]t"lnJad o®­9h]tgluqBi~b#aJi~b´¯
lnd h]i²d ln3q]}J}Td lnd h]i~qBxqBtube¢s9a#lm²8aai-­~qBtnaiWlue¢hBra_df}Tai~b#a]»
¥ ÷õ÷8emluqBi~}Jeh]tlnJad o®­9h]tgluqBi~b#aËJi~b´lnd h]iÈ²d lnÈqBx x:q]}_¯
}Td lnd h]i~qBx>qBtube» W"Jos9atue-s9acegdf}TalnJa5s9hÅT­Jx hBlue-qBtnalnJa
o®ac}TdfqBiatntnh]tue»

Parents
Algorithm

−3

3.5
3
2.5
2

o@q]}TaÆlnJaqBx y]h]tnd lnJo x aceneaÀ-b#d aiWlc»'z,¯ UqBx JaÆd ilnJdfe6bq]ega
dfeÆ  ]J ~J»:¼'Ja tnacegJx lueÆb#x acqBtnx kqBy]tnaa²d lnh]JtÆqBi~qBx kTegdfe
hB lnJa:d iTÌ~Jai~b#aÆhB a_df}Tai~b#a]»

* BZ AC+L&ÜcN ^)Ü-10[=<¢LWSUL_Iª´®® S7 ( «·HO9I
3 SUHOcL_K¾L&Ü9I  HJI P  "!$#
vwi lnJdfea#ÅT­9atnd o®aiWlc6²8a~egac}OlnJa¸enqBo®aa#ÅT­9atnd o®aiWluqBx
ega#lnJ­ qBi~}O}Tdf}Oegh]o®aa#ÅT­9atnd o®aiWlue@h]i¦lnJa& qBi~}
"!$#iJa#lm²8h]tn³Te» ¼'JaOtnacegJx lue¸qBtna egJhU²iÿd i
© d y]JtnaJ»¶"yWqBd i'q]}J}Td iJy·qBtube@qBo®h]iJylnJa­~qBtnaiWlue®hB
a_df}Tai~b#a¦iJhT}TacesJtnd iJyWed o®o®ac}TdfqUlna¦d o®­JtnhU]ao®aiWlue5h]t
  -»~6}J}Td iJy-o®h]tnaqBtube8lnhy]a#llnJaa#ÅJq]b´l6d o®­9h]tg¯
luqBi~b#a®Ji~b´lnd h]i±h]tno0}Tdf}±iJhBld o®­JtnhU]a lnJa@tnacegJx luersJTl
h]iJx k-o@q]}TaÆlnJaqBx y]h]tnd lnJo|x acene'a À-b#d aiWlc»

D

E

Ò Ñ « Ô6§_ÕmÒ Ñ

vwi®lnJdfe{­~qB­9atc]²8a"q]}J}Ttnacene,qÆ³]ak­Jtnh]sJx aoµhBd o®­9h]tgluqBi~b#a
enqBo®­Jx d iJyd i±°'qk]acegdfqBi5iJa#lm²8h]tn³Te)lnJa®tna­JtnacegaiWluqUlnd h]i5hB
lnJa3d o®­9h]tgluqBi~b#a@Ji~b´lnd h]i»±¼¢k_­JdfbqBx x k],²8a-tna­JtnacegaiWl qBi
d o®­9h]tgluqBi~b#a'Ji~b´lnd h]i@q]e{q6¾q]b´lnh]tnd ¿cqUlnd h]iBd» a]» Wq:egacÎWJai~b#a
hB8b#h]i~}Td lnd h]i~qBx ­Jtnh]s~qBsJd x d lmk3luqBsJx ace®À Á8z¢¼euÂ´»:Ça®~eg~qBx x k
bqBiJiJhBl8qUÍ)h]tu}lnhbqBxfb#JxfqUlna6qBi~}@emlnh]tna'lnJa6a#ÅJq]b´l,h]tno@e{hB
lnJaËÁ8z¢¼e»·¼'Jatna#h]tna],}Td Í)atnaiWl@qB­J­JtnhÅTd o@qUlnd h]i~e~q]a
s9aaiËluqB³]ai»6Çatna_d a²8ac}ega]atuqBxr­9h]­JJxfqBt>qB­J­JtnhÅTd o@qU¯
lnd h]iËemlntuqUlnay]d ace8h]t"lnJa®Á8z¢¼e"qBi~}­9h]d iWl6h]Tl"lnJad t6x d o®d ¯
luqUlnd h]i~e»"õlnat6ln~qUlc9s~q]egac}h]iËqBiËqBi~qBx kTegdfehB lnJad iTÌ~T¯
ai~b#a:hB a_df}Tai~b#aÆd iË°'qk]acegdfqBiiJa#lm²8h]tn³Te_²8a:­Jtnh]­9hWegaÆqBi

1.5
1
0.00074

0.00072

0.00072

0.5
EPIS

Parents
Algorithm

All

© d y]Jtna@JÖAU"ax x d iJy]atæ e:}TdfemluqBi~b#a@hB8lnJa 
qBx y]hB¯
tnd lnJo h]i3lnJtnaa}Td Í)atnaiWl"h]tno@ehB d o®­9h]tgluqBi~b#a:Ji~b´lnd h]i
h]i±À H ÂqBi~}±À Û Â"!$# »

qB­J­JtnhÅTd o@qUlnd h]i±emlntuqUlnay]kln~qUl qBd o@eqUlq]bb#h]o®o®hT}JqUlnd iJy
lnJa6o®hWeml8d o®­9h]tgluqBiWl'q]}J}Td lnd h]i~qBx9}Ta­9ai~}Tai~b#a6d iWlntnhT}T~b#ac}
s_k±lnJa3a_df}Tai~b#a]»·¼'Ja3­Jtnh]­9hWegac}¸d o®­9h]tgluqBi~b#aJi~b´lnd h]i
dfe®acq]egd at lnh±d iWlnatn­Jtna#lc» ¬ Jt@a#ÅT­9atnd o®aiWluqBx'tnacegJx lue@qBxfegh
egJhU²|ln~qUl@lnJaiJa² qB­J­JtnhÅTd o@qUlnd h]i½emlntuqUlnay]k¸hBÍ)atue®qBi
d o®o®ac}TdfqUlnad o®­JtnhU]ao®aiWlhBlnJa3ÎW~qBx d lmkhBlnJad o®­9h]tg¯
luqBi~b#aJi~b´lnd h]i»Æ°8kd iWlntnhT}T~b#d iJyo®h]tna­~qBtuqBo®a#lnatue~lnJa
d o®­JtnhU]ac}d o®­9h]tgluqBi~b#aJi~b´lnd h]i5h]tnoCqBxfeghsJtnd iJyWe6o~bu
­9hBlnaiWlndfqBxBh]tr}Tk_i~qBo®dfb,enqBo®­Jx d iJy"qBx y]h]tnd lnJo@eq]e)lnJak>bqBi
x acqBtnilnJah]tna#lndfbqBx x k@s9a#lglnat"d o®­9h]tgluqBi~b#a>Ji~b´lnd h]i~e»
¤O«gf Ñ ÒVe

rÓ

 

 Ñ ¨J§

¼'JdfetnacegacqBtubu±²'q]eegJ­J­9h]tglnac}±s_klnJa"d t ©Jh]tub#a ¬ À-b#a
Bh 6jTb#d aiWlnd É9bzRacegacqBtubu±y]tuqBiWlue'©D¡]WBGFT,@GF)HFTJ~&sW»@Ça
ln~qBiJ³-ega]atuqBx9qBiJh]i_k_o®h]~e¢tna_d a²8atue{hBlnJa:6"vmWb#h]iT¯
atnai~b#a"h]t'ega]atuqBxJd i~egd y]WlgJx)b#h]o®o®aiWlue{ln~qUl'x ac}®lnhd o ¯
­JtnhU]ao®aiWlue,d i-lnJa>­~qB­9atc»¹"x x)a#ÅT­9atnd o®aiWluqBx)}JqUluq:~q]a
s9aaih]sTluqBd iJac}@~egd iJ
y IJ6KLWq°'qk]acegdfqBi®d iTatnai~b#a6aiT¯

y]d iJa±}Ta]ax h]­9ac}ÈqUl3lnJa¸`6acb#dfegd h]i¹j_kTemlnao@eprqBs9h]tuqUlnh]tnk
 N OPO2QPQPQ'uTW  )¡W¡8g]£] OPRTSJB)B,
qBi~}:qUqBd xfqBsJx a,qUl M ¡_¡W
ÿ  ] ©g Ñ «r§

VU

$U

B E ~»~Á¢JaiJy qBi~}zh¸» ~»T`6tnJ¿c}T¿ax» ° W¯w"vgj)ÖT"iq]}JqB­T¯

lnd ]a'd o®­9h]tgluqBi~b#aenqBo®­Jx d iJy:qBx y]h]tnd lnJoÊh]t,a_df}TaiWlndfqBx
tnacq]egh]iJd iJy d ixfqBtny]a:°'qk]acegdfqBi3iJa#lm²8h]tn³Te
» WT7ð 4Tù´ñ)öU÷ +ð 3
¥ ù´ôó X8ï#ó¾öU÷ ·´ñ~ôwü#÷õ÷ ó Wü#ñ)ïu
ü Y6ü´ýüuöUùn5ï _~ @JÖ ]4 F) ~,~JUB]]J»

U

B SE Á6»~Á¢h]i~qUlndT»Jj)»JÄ>atglniJatcOä®»&×{qBi~paJiTqBi~}zh¸» ~»

` tnJ¿c}T¿ax» ¬ iT¯ x d iJa@emln~}TaiWlo®hT}Tax d iJyh]tb#hWq]buJac}
6
­Jtnh]sJx aoµegh]x _d iJyÆ~egd iJy:°'qk]acegdfqBiiJa#lm²8h]tn³Te»Tvwiz§8ùnð7
ïuüuüuòUóõ&ñ Uý+ð 3>)ô J
ü ¬9ó ¸])ô -·´ñ~ôwü#ù´ñ)öUôó¾ðUñ)öU
÷ ¦{ðUñ 3#ü#ùnü#ñ)ïuüðUñ
Zrýü#ù v5ðcò]ü#÷ óõ&ñ \[4ZVv:].^P_ `UJ­~qBy]aceF @JH FJ7 D_T, ×6d aiJi~qJ
W"al
² ã{h]tn³) ¡,¡&s_»Jj_­Jtnd iJy]a
t ×,atnxfqBy~»
B @7E'R:» ©JJiJy>qBi~'
} ä®» ¯gÁ6»UÁ¢~qBiJy~»BÇad y]Jd iJy6qBi~}Æd iWlnay]tuqUlg¯
d iJy:a_df}Tai~b#a'h]t¢emlnhTbu~q]emlndfbegd oJxfqUlnd h]i d i-°'qk]acegdfqBi
iJa#lm²8h]tn³Te»{vw½
i h¸¿» U"aiJtnd h]i R:»)j_~q]buWlnatc~p{¿» ä:qBi~qBx
qBi~8
} U~»~pao®o®atcJac}Td lnh]tuea
 Z ñ)ïuü#ù´ôwöUóõñ~ôû®óõñ ¥ ù´ôó X8ï#ó¾öU÷
·´ñ~ôwü#÷õ÷ ó Wü#ñ)ïuc
ü bU ­~qBy]aceB, ¡GFJT ¡J9 W"a
² ã{h]tn³)
 W9
» ã »
 ¡,~,¡J&
» G{xfega_d at6jTb#d ai~b#a:z{JsJx dfegJd iJyÁ¢h]o®­~qBi_k]Tvwi~bB»
B DFEVU~»:Ä>a²8a³]a]» °'qk]acegdfqBiÊd iTatnai~b#a½d iRacb#h]iJh]o®a#lntndfb
o®hT}Taxfe~egd iJá
y hh]iWlna¸Á8qBtnx hYd iWlnay]tuqUlnd h]ie
» dïuðUñ)7ð 
1@ü#ôù´ó¾ïuöU9 sTÀ¾WÂ´Ö  @J7 sfF) @,@,¡JT ¡,~,¡J»
B S E`¿
» U"acbu³]atno@qBi»¢z{tnh]s~qBsJd x dfemlndfb>egd o®d xfqBtnd lmkiJa#lm²8h]tn³Te»
gü#ô ¢ðUoù wý´9B~À]Â´Ö ]& sfFT, @]J]"Jy~» ¡,¡]J»
B 7 E'h¸,» U"aiJtnd h]i»9z{tnh]­~qByWqUlnd iJy6Ji~b#atgluqBd iWlmkd i@°'qk]acegdfqBi
iJa#lm²8h]tn³Te,s_k®­Jtnh]s~qBx dfemlndfbx h]y]dfb"enqBo®­Jx d iJy~»vwh
i Z ñ)ïuü#ù 
ôwöUóõñ~ôû-óõñ ¥ ù´ôó X8ï#ó¾öUN÷ ·´ñ~ôwü#÷õ÷ ó Wü#ñ)ïuü µ )­~qBy]ace: D¡GF)c, @J
W"a² ã{h]tn³)9
 W» ã » { ¡,~,~J» G{xfega_d at:jTb#d ai~b#a@z{JsJx dfegT¯
d iJyÁ¢h]o®­~qBi_k]Tvwi~bB»
B sE'h¸F» U"aiJtnd h]i»9j_h]o®a'­Jtuq]b´lndfbqBx_dfenegJace d i@b#h]i~emlntn~b´lnd iJy
s9ax d a#"iJa#lm²8h]tn³Te»vwe
i Z ñ)ïuü#ù´ôwöUóõñ~ôûËóõñ ¥ ù´ôó X8ï#ó¾öU
÷ ·´¿ñ 
ôwü#÷õ÷ ó Wü#ñ)ïuü ¶ W­~qBy]ace'cJH F)7 s7@J7 G{xfega_d at{jTb#d ai~b#a6z{JsT¯
x dfegJatue'°>» ×» ¿ W"h]tgln°
 U"h]x xfqBi~} ¡,~,¡J»
B ~7Ep{»)`N
» U"atni~qBi~}Ta¿]j)N» hh]tuqBx)qBi~}Ë»jTqBx o®atnh]i»Æ
hh]iWlnaÁ8qBtnx h qBx y]h]tnd lnJo*h]t'­Jtnh]s~qBsJd x dfemlndfb6­Jtnh]­~qByWqU¯
lnd h]id is9ax d a#riJa#lm²8h]tn³Te¢s~q]egac}@h]id o®­9h]tgluqBi~b#a>enqBo ¯
­Jx d iJy qBi~}ÊemlntuqUlnd É~ac}Èegd oJxfqUlnd h]iÈlnacbuJiJdfÎWJace» ·´¿ñ 
ôwü#ù´ñ)öUôó¾ðUñ)öUi
÷ WT7ð 4Tù´ñ)öU÷-+ð 3 ¥ ø]ø)ùnð ¸]2ó 1@öUôwj
ü Y6üuöýðUñ~óõ&ñ B
 ~JÖ F @GFO¡J]~ ¡,¡,~J»
B ¡7EÄ N
» äÆh]³]h]xfqB³_dfe"qBi~}Ëz N» W6qBiJh]­9h]Jx hWe»Æ°'qk]acegdfqBioJx ¯
lnd UqBtndfqUlnao®dfb#tnhB¯wqBy]y]tnayWqUlnd h]i3Ji~}Tat6lnJa U"ax x d iJy]at æ e
}TdfemluqBi~b#a@b#tnd lnatnd h]i
» Y6ü´ýüuöUùn5ï ±óõñ½lð k¦ï#ó¾öU÷{ý´ôwöUôófý´ôó¾ï´ý´
D9ÀmÂ´Ö ]7 sfF)BJ_B]J]»
B c7 E'ä®g
» hJtn­J_k]& ã »JÇadfene~qBi~
} h¸» U]h]tu}JqBi» ph_h]­_k-s9a#¯
x d a# ­Jtnh]­~qByWqUlnd h]iOh]tqB­J­JtnhÅTd o@qUlna5d iTatnai~b#a]ÖO"i
ao®­Jd tndfbqBx'emln~}Tk]»vw[
i §8ùnðcïuüuüuòUóõ&ñ Uý+ð 3)ô J´
ü {ó 3uôwüuü#ñ~)ô 
¥ ñ~¿
ñ 4~öUA
÷ ¦{ðUñ 3#ü#ùnü#ñ)ïuü·ðUm
ñ Z ñ)ïuü#ù´ôwöUóõñ~ôû·óõñ ¥ ù´ôó X8ï#ó¾öU÷
·´ñ~ôwü#÷õ÷ ó Wü#ñ)ïu8
ü [4Z ¥ ·H].^n^H`U)­~qBy]ace DW& sfF&DOsBT~jTqBk
i ©JtuqBiT¯
b#dfenb#h~9Á8 ¡,¡,¡JO» hh]tnyWqB
i ä:qBTo@qBiJiz{JsJx dfegJatue»

B ]Ej)»9h¸» ¬ x o@emlnac}»k³'ñ¸ùnü ø)ùnü´ýü#ñ~ôóõñ&5öUñ)ò3ýðU÷ xcóõñ&ò]ü
ï#ófý´ó¾ðUñ¦ø)ùnð]ú#÷ ü
1 ý´» z{~`þlnJacegdfejWluqBiTh]tu}È"iJd ]atg¯
egd lmk]{
G iJy]d iJaatnd iJyB¯¢
G b#h]iJh]o®dfb8j_kTemlnao@e¢`6a­~qBtglno®aiWlc
¡,~,J@ »
B SEp{» ¬ tglnd ¿]»¬ü#÷ üuï#ôóõñ& ¥ ø]ø)ùnð¸]ó21@öUôwü#÷ û7³rø)ôó21@öU÷ ¥ ï
ôó¾ðUñJýóõñ{
¦ ð7"
1 ø)÷ üo¸9
¬ ôù4~ï#ô4Tùnüuò
ð71@öUóõñJý´»6z{~`¹lnJa#¯

Vo

ge dfeT°8tnhU²i3"iJd ]atuegd lmk]~Á¢h]o®­JTlnat6jTb#d ai~b#a:`6a­~qBtglg¯
o®aiWlc9B]J]»
B @7Ep{» ¬ tglnd ¿ qBi~}5p{
» ä:qBax sJx d iJy~»Æ6}JqB­Tlnd ]a d o®­9h]tgluqBi~b#a
enqBo®­Jx d iJyYh]tacemlnd o@qUlnd h]iÈd iÊemlntn~b´lnJtnac}È}Th]o@qBd i~e»
vwk
i §8ùnðcïuüuüuòUóõ&ñ Uý +ð 3:)ô J.
ü ._)ô  ¥ ñ~¿ñ 4~öU
÷ ¦{ðUñ 3#ü#ùnü#ñ)ïuü®ðUñ
Z ñ)ïuü#ù´ôwöUóõñ~ôû½óõñ ¥ ù´ôó X8ï#ó¾öU÷ ·´ñ~ôwü#÷õ÷ ó Wü#ñ)ïuh
ü [4Z ¥ ·H]Ppnp6`U
­~qBy]acNe D,DWG F&D_7 D~ hh]tnyWqBC
i ä:qBTo@qBiJi:z{JsJx dfegJatuerjTqBi
©JtuqBi~b#dfenb#h~~Á8qBx d h]tniJdfqJJB]]J»
B  DFEVU~»"z acqBtnx» §8ùnð]úuö]ú#óõ÷ ófý´ôó¾9
ï Y6üuöýðUñ~óõ&ñ Oóõ"
ñ ·´ñ~ôwü#÷õ÷ ó Wü#ñ~ô
¬9ûý´ôwü 1ý ¯ gü#ô ¢ðUoù wý+ð 3-§8÷ 7ö 4_ý´ó¾ú#÷ 
ü ·´ñ 3#ü#ùnü#ñ)ïuü#°
» hh]tg¯
yWqB½
i ä:qBTo@qBiJiËz{JsJx dfegJatue9vwi~bB» jTqBk
i hËqUlnah~Á8
 ¡,~,~J»
B S E'h¸»5z{tuq]}T~qBi¦Ä »5z{tnhUUqBi½°>½
» hdf}J}Tx a#lnh]iOqBi~}
h¸¿
» U"aiJtnd h]i
» äÆiJhU²x ac}Ty]a>aiJy]d iJaatnd iJy h]txfqBtny]a>s9a#¯
x d a#iJa#lm²8h]tn³Te»Êvwy
i §8ùnðcïuüuüuòUóõ&ñ Uý±+ð 3Ë)ô J[
ü ¨~ü#ñ~)ô  ¥ ¿ñ 
ñ 4~öU
¿
÷ ¦{ðUñ 3#ü#ùnü#ñ)ïuüðUq
ñ Z ñ)ïuü#ù´ôwöUóõñ~ôûóõñ ¥ ù´ôó X8ï#ó¾öUË
÷ ·´¿ñ 
ôwü#÷õ÷ ó Wü#ñ)ïu\
ü [4Z ¥ ·H].^2`U,­~qBy]acC
e D~FD2F&D¡]JrjTqBH
i hËqUlnah~
Á8 ¡,¡FD~&» hh]tnyWqB
i ä:qBTo@qBiJiz{JsJx dfegJatue_vwi~bB»
B c7 E'R:
» ã A
» RJsJd i~emlnad i» ¬92ó 1.4T÷ öUôó¾ðUñ öUñ)òÊ)ô J
ü v5ðUñ~ôwü
¦{öUù´÷ 
ð v5ü#)ô JðcòUr
» U]h]Ji3ÇOd x a8
k sÿj_h]i~e ¡,~J]»
B 7 sE»TjTqBx o®atnh]iW»JÁ8qBiJh~WqBi~}-j)&
» hh]tuqBx»)vwo®­9h]tgluqBi~b#a
enqBo®­Jx d iJy3d i±°'qk]acegdfqBiËiJa#lm²8h]tn³Te6~egd iJy3­Jtnh]s~qBsJd x d lmk
lntnaace
» ¦{7ð 1"ø 4TôwöUôó¾ðUñ)öU
÷ ¬9ôwöUôófý´ôó¾ï´ý®öUñ)i
ò o öUôwö ¥ ñ)öU÷ 7û 
ý´ófý´¿ @FD~Ö @,~&sfF&D~ @JWB]]J»
B  ~7E'R:» j_~q]buWlnatc °>»rA
` æ "osJtnhWegd h~ qBi~}±°>» }Tab
x ©~q]atnh~»
j_k_os9h]x dfbÈ­Jtnh]s~qBsJd x dfemlndfbÈd iTatnai~b#a¹d iC°8ax d a#±iJa#lg¯
²8h]tn³Te» vwi ¥¥¥ ·l^.pUT­~qBy]ace>BG F) @J]9 ¡,¡]J»
B  ¡7E'R:» `»@j_~q]buWlnat½qBi~} h¸»®» z ahBlc» j_d oJxfqUlnd h]i
qB­J­JtnhWq]buJacelnh¸y]aiJatuqBx­Jtnh]s~qBsJd x dfemlndfb3d iTatnai~b#ah]i
s9ax d a#3iJa#lm²8h]tn³Te»=vwq
i h¸'
» U"aiJtnd h]i
 R:»j_~q]buWlnatc
p{» ä:qBi~qBx"qBi~t
} U~»pao®o®atc"ac}Td lnh]tuec
 Z ñ)ïuü#ù´ôwöUóõñ~ôû
óõñ ¥ ù´ôó X8ï#ó¾öU
÷ ·´ñ~ôwü#÷õ÷ ó Wü#ñ)ïu@
ü bU'­~qBy]ace-]TH FJF @J]T
 W"a²
ã{h]tn³)C
 W» ã » @ ¡,~,¡J» G{xfega_d atËjTb#d ai~b#aYz{JsJx dfegJd iJy
Á¢h]o®­~qBi_k]_vwi~bB»
B B7 E Á6I
» ã{~qBi qBi~¾
} h¸J
» U~»·`6tnJ¿c}T¿ax»
"i d o®­9h]tg¯
luqBi~b#aenqBo®­Jx d iJy-qBx y]h]tnd lnJos~q]egac}3h]ia_df}Tai~b#a:­Jtna#¯
­Jtnh]­~qByWqUlnd h]i» vwi §8ùnðcïuüuüuòUóõ&ñ Uý¦+ð 3Y)ô Já
ü u^B)ô ¼¦{ðU¿ñ 
3#ü#ùnü#ñ)ïuü¸ðUv
ñ Z ñ)ïuü#ù´ôwöUóõñ~ôû·óõñ ¥ ù´ôó X8ï#ó¾öU
÷ ·´ñ~ôwü#÷õ÷ ó Wü#ñ)ïuü
[4Z ¥ ·H]Pp ¶ `U¸­~qBy]ace¶W7 D2FT, @J]k
 hh]tnyWqBd
i ä:qBTo@qBiJi
z{JsJx dfegJatuejTqB°
i ©JtuqBi~b#dfenb#h~~Á8qBx d h]tniJdfqJJB], @J»
B T E'W»Jp{1
» w9~qBiJy®qBi~}3`»~z h_h]x a]» ¹egd o®­Jx a:qB­J­JtnhWq]bu@lnh
°'qk]acegdfqBiËiJa#lm²8h]tn³Ëb#h]o®­JTluqUlnd h]i~e»®vwi §8ùnðcï +ð 3®)ô Jü
¨~ü#ñ~)ô  ¦{öUñ)ö]òUó¾öU/
ñ ¦{ðUñ 3#ü#ùnü#ñ)ïuüðUñ ¥ ù´ôó X8ï#ó¾öUV
÷ ·´ñ~ôwü#÷õ÷ 2ó 
Wü#ñ)ïuü#J­~qBy]ace>7 s_H F)7 s7~J~ ¡,¡FD~»



