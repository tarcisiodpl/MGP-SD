

Many AI synthesis problems such as planning
or scheduling may be modelized as constraint
satisfaction problems (CSP). A CSP is typically
defined as the problem of finding any consis­
tent labeling for a fixed set of variables satisfy­
ing all given constraints between these variables.
However, for many real tasks such as job-shop
scheduling, time-table scheduling, design ... , all
these constraints have not the same significance
and have not to be necessarily satisfied. A first
distinction can be made between hard constraints,
which every solution should satisfy and soft con­
straints, whose satisfaction has not to be certain.
In this paper, we formalize the notion of possi­
bilistic constraint satisfaction problems that al­
lows the modeling of uncertainly satisfied con­
straints. We use a possibility distribution over
labelings to represent respective possibilities of
each labeling. Necessity-valued constraints al­
low a simple expression of the respective cer­
tainty degrees of each constraint.
The main advantage of our approach is its integra­
tion in the CSP technical framework. Most clas­
sical techniques, such as Backtracking (BT}, arc­
consistency enforcing (AC) or Forward Checking
have been extended to handle possibilistics CSP
and are effectively implemented. The utility of
our approach is demonstrated on a simple design
problem.
1

Introduction

There are a lot of publications about constraints, and more
specifically in the CSP framework, but most of these papers
try to tackle the higly combinatorial nature (NP-Hard) of
such problems only considerin g hard constraints.
,

This paper gives a clear meaning to what could be a soft
constraint, how it may be expressed and how soft constraint
satisfaction problems may be solved. Our aim is not to
*e-mail: schiex@cert. fr

or

schiex@ir it. fr

give a "general" theoretical framework for expressing soft
constraints (such approaches may be found in [Satoh90]
using first and second order logic to express preferences or
in [Freuder89], relying on a problem space and a general
measure on this space}, but to give a specific (and hopefully
useful) meaning to such constraints leading to "efficient"
solving techniques.
Non standard logics are manyfold that allows the expression
of probabilities [Nilsson85], orpreferences [Shoham87]. In
particular, zadeh 's possibility theory [zadeh78] has already
been successfully used for modeling uncertainty and pref­
erences in the frame of propositional and first-order logic
by Dubois, Prade and Lang leading to the so-called "pos­
sibilistic logic" [Lang91b]. One of the desirable feature
of possibilistic semantics is the tolerance to "partial" con­
sistency, which allows a sort of paraconsistent reasoning.
Another interesting feature of possibilistic logic is the close
relationships between necessity measures and Gardenfors
"epistemic entrechment" relation [Gardenfors et al.88].
The main idea is to encapsulate preferences (or respective
certainty degree) among labelings in a "possibility distribu­
tion" over labelings. Such a distribution naturally induces
two (possibility and necessity) measures over constraints.
However, it is not clear how to simply express such a dis­
tribution.
A possible answer is to express bounds on necessity (or
possibility) measures of constraints, defining a set of possi­
bility distribution among labelings. One can then define a
set of "most possible" Iabelings satisfying these bounds.
The structure of the paper is as follows : the section 2.1 re­
calls how a Constraint Satisfaction Problem may be defined
and which objects are involved ; the section 2.2 presents
how a possibility distribution implicitly defines measures
on constraints ; the section 2.3 show s how bounds on ne­
cessity measures over constraints define "best" labelings.
The next section rapidly presents algorithmic issues for pos­
sibilistic CSP solving and shows how specific satisfaction
(Backtrack) and consistency enforcing (Arc-consistency
[Mackworth77]) techniques may be built, taking into ac­
count the induced possibility distribution.
In section 3, we give an example of application of possibilis-

Possibilistic Constraint Satisfaction Problems
and k1 are satisfied. k; 1\ ki is simply defined as the
set of every labeling 1 over v; U Vj such that I f= k;
and IF= k1;

tic CSP to a simple design problem. Both representation
and solving issues are addressed. Section 4 compares our
results with related works and is followed by a presentation
of further possible researchs.

2

•

Possibilistic constraint satisfaction
problems

2.1

IF kj;

An informal meaning

Let us breafty recall the definition of a classical Constraint
Satisfaction Problem as definitions may change among au­
thors.

The usual problem is then to find a labeling of the domain­
variables in V that satisfies the conjunction of every con­
straint in C. Cryptograms (such as SEND+MORE=MONEY,

LYNDON*B=JOHNSON ) , then queens problem, ...are typ­

ical instances of CSP.

A typical CSP involves a set X= {xi, ... , x.,} of n vari­
ables and a set D of n associated domains. Each variable x;
takes its value in its associated domain d;. In the following,
we shall restrict ourself to finite domains.

Domains and variables are intrinsically bound together to
form what we will call a domain-variable. The domain­
variable i is defined as a pair ( x;, d;) where x; is a variable
and d; its associated domain. We will call V the set of
domain-variables defined by X and D.
The fact that some domain-variables take some specific
values in their domains will be represented by a labeling.
A labeling lw of a set W of domain-variables is simply
defined as an application on W such that :

ViEW, lw(i) Ed;
Alternatively, a labeling lw wil l be considered as its map
(the set {( x, lw(x))/x E W}).
Further, a set of constraints C is considered. Each con­
straint k;(it,... , i.,.) on the set of domain-variables v; =
{it, ... , inJ is a set of labelings of v;.

lw of W satisfies a
lw F= k;) iff v; c W

We will say that a labeling

k;(it I .
k;(ij . .
.

1

. 1

• 1

i.,;) (noted
i.,;)fl c lw

2.2

Let

Let I\ be the set of every possible constraints on any non­
empty subset of V. The possibility distribution 11' on Lv
induces two functions on K called possibility and necessity
measures respectively noted n,. and N.. defined as follows:
: K ---. [o, 1]
Vk E K1
n,.(k) = Sup({1r(l),/ E Lv,l f= k}

n,,.

E

Let us consider lA and IB two partiallabe/­
ings (A C V, B c V). We will say that IA is more defined
than IB (noted /A � IB) iff IB C IA.
partial labeling

I

typically represents the set of every

•

For any given constraint

k;(i1, ... , in;), we will note

Lv).
•

•

1

Given two constraints

k; and k1, we will note k; 1\ kJ
Vj that is satisfied when both k;

N,.(T) is obviously equal to 1 i.e., ever satisfied constraints
are satisfied ;

N" ( .1.) = SN ( 1r) which is generally not equalto 0 l This
means that unsatisfiable constraint may be somewhat re­
quired to be satisfied. This is dependant upon the fact that
the possibility distribution 1r is not required to be nonnalized.
This choice has been made to cope with partial inconsisten­

1

the constraint on v; u

{0})

Let us denote by .1. any unsatisfi.able constraint (there is no I E
Lv /1 f= .li.e, .lis a constraint that contains no labeling) and by
T the ever satisfied constraint (i.e, the set of alllabelings on V,

-.k;( i1, ... i.,.) the constraint on V; that is unsatisfied
when k; is satisfied. -.k;( it .. . , in.) is simply the
complement of k;(i1, ... , in.) in the set Lv, of every
labelings over v; ;

•

u

N" : K---. [0, I]
Vk E K,
N"(k) = Inf({1-?r(l),/ E Lv,l f= -.k} U {l})
N"(k) = 1- n,.(-.k)

complete labeling that are more defined than/.
We finally define the following algebra over constraints :

be the finite set of all possible labeling of the

malized if and only if 3/ E Lv such that 1r(l) = I. We
define the sub-normalization degree of 1r as the quantity
SN(?r) = 1- Sup({7r(/)f/ E L}). Obviously,SN(?r) = 0
iff ?r is normalized.

•

Definition 2.1

Lv

domain-variables in V. A possibility distribution on Lv
is a function 1r from Lv to [0, 1). 1r is said to be nor­

constraint
and 3/

Modeling soft constraint with possibility and
necessity measures

In classical first order logic, soft constraint may be formal­
ized through interpretation ordering [Satoh90]. P ossibilis­
tic CSP, as indicated by their name, relies on a possibility
distribution over labelings.

We will say that a labeling is complete iff it is defined on
V, it will be partial otherwise.

A

Given two constraints k; and k1, we will note k, V ki
the constraint on V; u Vi that is satisfied when one of
k; or ki are satisfied. k, V ki is simply defined as the
set of every labeling I over V; u Vj such that I f= k; or

cies.

•

Yk1,k2 E K,
N,.(k1/\k2)= Inf({l-11'(1),1 E

Lv,l

� ktl\kz}

u

{1})

269

270

Schiex

1r (l), l E Lv,l Vo k1}
U{l- 1r(l),l E Lv,l � kz} U {1})
::= Inf({N,.(kJ), N,.(kz)}).

=

Inf ({l -

The possibility n .. ( k) represents what its name suggests
i.e., the possibility for the constraint k to be satisfied accord­
ing to the knowledge of reference. The necessity N.. (k)
tends towards 1 when the possibility that k being unsatisfied
tends toward 0, measuring to what extent the satisfaction of
k is entailed by the knowledge of reference (given by 1r).
Clearly, possibilisticCSP, as
ssibilistic logic, is not meant
to express fuzzy cons
n
as measures are attached to
precise constraints. The statement "It is 0.7 necessary that
the product be delivered before the 21th" may be translated
in possibilistic CSP to something like N.. ( D :5 21 ) = 0.7
where Dis the variable corresponding to the delivering day.
Possibilistic CSP is not intended to modelize a statement
such as 'The product should be delivered not too late after
the 2lth"2•

r,
trai ts

Because of the min and max operators, the precise values
of neces sity or possibility is not so important. The essen­
tial is the total pre order induced by them. Thus, necessity
degrees express preference degrees, N.. (k) > N.. (k') ex­
pressing that the satisfaction of k is preferred to the satisfac­
tion of k'. Therefore, possibilistic CSP are closely related
to Hierarchical CSP as described in the frame of Constraint
Logic Programming in (Boming et al.89].

The notion of "constraint satisfaction will now depend on
a possibility distribution 1r on Lv. Let us consider ( k, a) a
necessity-valued constraint.
"

We will say
(k, a) is satisfied by 1r (noted 1r F (k, a )
itT the necessity measure Nf( induced by 1r on K verifies
N.-(k) ;::: a. Considering the whole constraint set C, we
will say that the CSP ( V, C) is satisfied by a possibility
measure 1r iff the necessity measure induced by 1r verifies :

that

V'(k, a) E C, N.-(k) 2::

Possibilistic CSP : definition and semantics

•

•

where ki (it,

.

.

.

.

.

ti

-

1

.,

Vague relations seen

as

a fu:u.y set of authaurized labeling.

See [MC92,Dubois et al.89,Rosenfeld et al.76J.

2In fact, such a predicate may be decomposed in a set of

crisp predicates (a-cuts of the vague constraint). In our case,
the domains being finite, the set of a-cuts is finite and a given
fuzzy constraint may be decomposed in a finite set of possibilistic
constraints. However, this (possibly automatic) conversion may
be heavy and the result is (from an expressive view-point) quite
distant from the original knowledge.

C(V, C) = Sup*c(Sup1eLv ( 1r( l)))
Sup.-Fc(l- SN(1r))
Sup iELv(rt(l));

=
=

II(V,C)

1- C(V,C)
Inf.-�:=c(SN(7r))
= Inf .-�:= c ( N.- ( 1. ) )
= nf
I reLv(J ( l) ) ;

=

=

(0, 1].

A typical possibilistic CSP is then defined by a finite set
X of variables, a finite set D of associated finite domains
(defining a set V of domain-variables) and by a fi nite set
C of necessity valued constraints . It will be noted either
(X, D, C) or (V, C).
The necessity-valued constraint (k, a ) ex pres ses that
N.. ( k) ;::: a i e that the satisfaction of k is at least a­
necessary. The necessity-valued constraint (k, 1) expresses
that k should absolutely be satisfied, and therefore takes the
usual meaning of a constraint in classical CSP ; (k, 0 ) is
totally useless as it expresses that the necessity measure of
k should be at least 0, which is always true.
.

•

)

te

degree:

•

E

On another hand, if we consider a specific (complete)
labeling 1, its compatibility with the knowledge of
reference (noted rt( /))will be the maximum of 1r ( l) for

Thus the degree of consistency of the possibilis c CSP or
qv, C) may be defined as the maximum of 1 SN ( 1r for
every 1r which satisfies C or, equivalently, as the compat­
ibility of the most compatible labeling. Its inconsistency
degree ll(V,C) will be the complement to I of its consis ncy

, in.), a)

, in,) is a classical constraint and a

If we consider a specific distribution 1r, the most
possible labeling will have a possibility equal to
(1- SN(1r));

every 1r which satisfies C. Its incompatibility (noted
J(/)) will be its complement to I.

The only difference between a classical and a possibilistic
CSP is the introduction of necessity-valued constraint in­
stead of simple constraint. A necessity valued constraint is
a pair:

(ki(il, .

a

Thus a possibilistic CSP has not a set of consistent labeling,
but a set of possibility distributions on the set of all labelings
on V.

-

2.3

)

Thus, the inconsistency degree of a possibilistic CSP is
equal to the smallest necessity degree of the unsatisfiable
constraint l. for all possibility distribution satisfying C.

ti

The computa on of the inconsistency degree of a possi­
bilistic CSP is made easier by the fact that one can define a
maximal possibility distribution among all possibility dis­
tribution satisfying (V, C).
Theorem 2.1 Let 'P = (V, C) be a possibilistic CSP, we
define the possibility distribution 1r:p on Lv by :

V'l E

Lv,

7rp(l)

=

lnf(l,,a;)ec({( 1

-

a ;)/1 F •ki} U { 1})

Thenfor any possibility distribution 1r on Lv,
if/11" s 'll"p.
Proof:
ll'

satisfies 1'

iff 'v'( k;,ai) E C,

iff'v'(k;, a; ) E

C,

ll'

satisfies

N,.(k;);:::

( k;, a;)
a;

1r

satisfies 'P

Possibilistic Constraint Satisfaction Problems

obtained by extending the labeling lon ( 1, . .. , i) with a new
variable i + 1 and every possible label in di+l· The leafs
of the trees are complete labelings that may (or not) satisfy
every constraint. In a depth first exploration of the tree the
first labeling that satisfies every constraint is retained.

iff'v'(k;,a;) E C, Inf({1- 7r:(l)Jll= -.k;} U {1});::: a;
iff'v'(k;,a;) E C,V'll= -.k;,7r(l) $1- a;
iff'v'l E Lv, 'll'(l) $1nf(k;,<>;)€c({1- a;/ll= -.k;} U {1 })
iff'v'l E Lv,'ll'(l) $ 'll'p(l).D
Corollary 2.1

We simply conclude that :

•

It(/)

=

11"p (l) ;

•

J(l)

=

(1 - 11";,(1));

•

,

C(P)= SupiELv(1Tp(l))
= 1- SN(1ri>)
= Su plE L v ({/1!/(k;,cri)Ec({(1 - O:i)/1 F -.k t}
U{1})})

Proo f: The two first points are immediate.
According to theorem

2.1, we know that :

"11r that satisfies P, 1r $ "Kp, i.e.,
'v'1r that satisfies P, '11 E Lv, (1- 1r(l));:::
'v''ll' that satisfies P, SN(lr) ;::: SN( ll"p ).

So:

The

corresponding

result

for

the

U

degree

Test and Generate

The next step towards sophistication (and efficiency) is the
"test and generate" approach, often referred as the "Back­
track" algorithm (BT). The obvious idea is to cut each
branch that will necessarily lead to complete labelings that
do not satisfy every constraint. Each non-terminal node
corresponds to a partial labeling I. To possibly lead to a
complete labeling that satisfies every constraint, a partial
labeling should be consistent :

If a partial labeling /looses its consistency property, every
labeling I' more defined than I will also be non-consistent.
In the case of complete labeling, non-consistency is equiv­
alent with non-satisfaction.

{0})} ).

consistency

2.4.2

Definition 2.2 Given a classical CSP (V, C), a partial la­
beling lA on the set of domain-variables A C V will be
consistenti.ffVki E C such that V; C A, lA f: kj.

(1- 1rp(l))

II(P)= Inf,.l=c(SN(7r))
= SN('ll'p)
= lnf1eLv (Sup(k; ,ai)ec( {a; /I f= -.k;}

The corresponding approach in possibilistic CSP will be an
optimization problem on the same tree. For each leaf of the
tree, we may compute the value of 1Tp on the corresponding
complete labeling. In a depth first exploration of the tree,
we will retain the set of the labelings that maximize 1Tp (l).

is

immediate.O

Then, computing the inconsistency degree of a CSP means
computing the sub-nonnalization degree of the distribu­
tion 1r;,. The set of all labeling Lv being finite, we
can define the set Lv of all Jabelings of V such that
VI* E Ly,1rp(l) = 1-SN(7r;,). Thiswill be called the set
of the best labelings of 'P. Its elements are the most com­
patible labelings with the CSP 'P = (V, C) among every
labeling. The problem of finding a "best labeling" may be
reduced to find a labeling r that solve any of the following
equivalent Min-Max optimisation problems :

<l depth first tree exploration, the property
of consistency is simply checked at each node. Backtrack
occurs when it is not verified. One should note that if a
labeling I on { 1 , ... , i} is consistent, each labelings I' on
{1, . . . , i, i + 1} is consistent iff it satisfies the constraints
k1 such that V; c {1, . .. , i, i + 1} and V; rt {1, ... , i} .

In t!1e case of

In the framework of possibilistic CSP, we extend the notion
of compatibility to partial labelings.
Definition 2.3

The compatibility ofa partiallabeling l A on
A is defined as the maximum of the compatibility of every
complete labeling more defined than /A :

C(V,C) =SUPIELv(Inf(k;,a;)EC({1- o:ifl F -.ki} U {I}))
II(V,C) = InfleLv(Sup(k,.a;)EC( {ad I F -,k;} U {0}))
Such problems may be tackled through many classical tree­
search algorithms, namely Depth first Branch and Bound
(DFBB), a - {3, or SSS•...
2.4
2.4.1

Our aim will be to compute, for each node (i.e. each partial
labeling) an upper bound on the compatibility of the partial
labeling. An easily computed upper bound of this value is
given by:

Extending classical CSP algorithms
Generate and Test

The more obvious algorithm to solve classical CSP is the
"generate and test" algorithm. It traverses the domain­
variables in a predetermined order (1 , . .. , n). In the tree
explored, each node corresponds to a labeling. The root
of the tree is the empty labeling, the sons of a node l are

Infc.c .. cr ; ) e c ({ ( l - ai)/Vi

C

A, lA F ...,k,} U {1})

This bound is exact for complete labelings. Moreover,
it may be incrementally computed as the tree is traversed
downwards : if a labeling I on A has been granted an upper­
bound f3, each labeling [I on A u {( :ci , di )}, more defined
than l is granted the upper-bound {3' :

271

272

Schiex

!3'

=

lnf( {/3}

E C,
C AU {(xj,dj)},

A classical n-ary CSP is said to be arc-consistent iff for
every domain-variable j, the domain d; is not empty, and
for every label v E di, for every constraint k; such that
j E V;, there is a labeling I on V;, more defined than the
labeling {(j, v ) } , that satisfies k,.

U {(1 - a;)/ (k;, a;)
V;

V; q: A,
I' I== •k;})

For the sake of clarity, given an explicit ordering 0

=

(1, . .. , n) on the domain-variables, we will note cf+1 the
set of the necessity valued constraints (k;, a;) such that
V; c {1, . . .,j,j + 1} and V; ct. {1, ... ,j}. If we note
{3 the upper-bound previously computed on a labeling I
on { 1 , ... , j} and l' a labeling on {1 , ... , j + 1}, more
defined than I, we may compute the upper-bound !3' on the
compatibility of I' via :
{3' = Inf({iJ} U {(1-a;)/(k;,a;)

E

'+

CJ

I

,I' I== •k;})

'lllis decreasing bound is used in a DFBB algorithm to
compute one (or every) best labeling. The algorithm sim­
ply stans with the empty labeling and extends it according
to the vertical ordering 0. It maintains two parameters of
importance : the number a under which a cutoff should
take place (increased each time a complete labeling with
an augmented compatibility has been found. It should be
initially set to zero to ensure optimality, a cutoff takes place
as soon as /3 � a) and the number {3 over which no im­
provement is possible (the bound on the compatibility of
the current partial labeling. It should initially be set to 1).
These two bounds offer a great deal of flexibility :
•

•

Typically, if a labeling whose possibility is lower than
a is considered as useless, the algorithm should be
called with a set to a, allowing a more efficient pruning
of the tree;
On the opposite side, if a labeling of possibility b is
considered as enough, the algorithm should be called
with f3 set to b, allowing the algorithm to stop as soon
as a {3 consistent labeling has been found.

Naturally, in the first case we may fail to find a best labeling
if its consistency degree is lower than a ; in the second case,
we have no garantee that the best labeling has been found.
Alternatively, one may stop the algorithm execution upon
any event (time exhausted,... ) and get the best labeling
found up to the occurrence of the event (getting closer to an
"anytime algorithm" [Dean et al.88]).

The algorithm that converts a CSP in an equivalent3 arc­
consistent CSP (if it exists) is usually embodied in a single
procedure Revise, apply ing to a domain-variable j and a
constraint k; (j E V;), that suppresses every label in the
domain di that does not satisfy the previous property. This
procedure is applied repetedly on the whole CSP until sta­
bilization (ACl to AC3).
In our case, such a label may stil l be possible if the constraint
k; is not 1-necessary. In general, the knowledge we may
extract is an upper bound on the compatibility of the partial
labelings that maps a single variable to a label.
If we consider a variable

k, such that j

E

b( {(j, v)}, k;)

=

V;

and

j and a (non-unary) constraint
if we note Uv, the set of unary

constraint on any of the variables in V;, the upper bound4
on the compatibility of the partial labeling { (j, v ) }, v E d;,
taking into account k; and every unary constraint in Uv; is
equal to:
Supi'ELv, ,l't{(j,v)}

(Inf(k, ,a, }E(Vv, u{ ( l:.,ao)})

({(1- an)/f' F -,kn} U {1}))

A possibilistic CSP will be said arc-consistent if for every
domain-variable j, the domain di is not empty,and for every
label v E di, the compatibility of {(j, v ) } with respect to
the possibilistic CSP ( {j}, U{j}) is strictly positive and
equal to the minimum of the
such that j E V;.

b( { (j, v)}, k;)

for every

k;

More precisely, if we note Pi the CSP defined by
( {j}, U{j} ), a possibilistic CSP is 8-arc-consistent if it is
arc-consistent, and :

8

=

InfjEv(SupvEdj(ll'j,i( {(j, v)})) )

It may be shown that 6 is an upper-bound on the overall
consistency <C(V, C).
The main idea to convert a possibilistic CSP into an
equivalent5 arc-consistent possibilistic CSP is then to add
unary necessity-valued constraints (rather than suppressing
labels) reflecting this bound and to take these new unary
constraints in account when the process is repetedly ap­

Every usual vertical heuristic ordering (max. cardinality,
max. degree, ... ), may be applied to this tree search. An
horizontal heuristic is given by the current bounds obtained
for the various labels, but its efficiency is y et to be evalu­
ated (it is welllrnown that horizontal ordering has a strong
influence on the efficiency of Min-Max problems solving,
e.g. in Alpha-Beta algorithm applied to games [Pearl85]).

The Revise"' procedure we have defined not only filters out
necessarily inconsistent labels, but also compute for each
label v Edithe upper bound b( {(j, v ) }, k;) on the compat­
ibility of the partial labeling I that maps the variable j being

2.4.3

:J.rwo CSP 1'1 and 1'2 are equivalent if they have the same set
of solutions, i,e, VI, ll= 1'1 '¢> II= 1'z.

Consistency enforcing

A further step is to extend the various local consis­
tency notions (node, arc, path and k consistency [Mack­
worth77,Montanari?4]) and their corresponding filtering al­
gorithms[Mohr et al.86,Deville et al.91].

plied.

•rt is precisely the compatibility of the labeling { (j, v)} in the
CSP ({j} U V,, {k;} u Uv;).
5Two possibilistic CSP 1'1 and 1'2 are equivalent if they have
the same set of satisfying possibility distributions, i.e. V1r, 1r I=
p1 # 'II' 1= 1'2, or equivalently 'll'p1 = ll'p1·

Possibilistic Constraint Satisfaction Problems

filtered to this label v taking into account the constraint k,.
This bound b may be simply encoded in the CSP by adding
a simple unary constraint6 on j indicating that this label is
forbidden with a necessity 1 - b(l, ki)·

,
I
I
I
\
'

,..•o·

._

..

\
I
I
,
,

The additionnal information obtained is taken into account
in the tree search algorithm and may greatly enhance the
performances of the algorithm (tighter bounds on partial in­
consistencies are obtained earlier). The termination (which
is quite trivial) and complexity of the algorithm, the unicity
of the problem obtained are yet to be formaly determined.
Limited applications of the Reviser procedure during the
tree search exploration (so-called Forward Checking, or
Partial Look-Ahead) that are usual in CSP are immediately
usable in possibilistic CSP and have been implemented.

b

should be noted that a possibilistic CSP containing
only HARD constraints is strictly equivalent to a classi­
cal CSP and that extended algorithms (tree exploration,
arc-consistency) behave exactly as corresponding classical
CSP algorithms. Therefore "softness" costs (almost) noth­
ing when left unused. The only overhead is due to the
manipulation of the floating point numbers 1.0 and 0.0 and
the operators min/max instead of boolean true and false and
logical operators and/or.

It

3

appJe.pie
ice
fruit

Figure 1: Gastronomic CSP hypergraph

A design problem

A great restaurant want to offer to its clients a computer
aided menu designer. The system should integrate "know­
how" knowledge and customer desires to compose a "best
menu"composed of a drink (white or red wine, beer or
water), an entrance (smoked salmon, caviar, "foie gras",
oysters or nothing), a main dish (fish of the day, leg of wild
boar, sauerkraut) and a dessert (apple-pie, strawberry ice,
fruit or nothing}.

The sauerkraut should be accompanied by a beer

(a,

0.8),

white whine may be possibly considered (b, 0.3), or even
water (c, 0.2);
•

Fish may not be eaten twice in the menu (caviar and oys­

ters will be considered

as

"fishes") (d, 0.7), and should be

•

•

I would like to eat some fish (n,

1.0);

0.8);

I would like to taste the sauerkraut (o, 0.2);

ate (Cf. figure 1). The basic constraints are represented by
the continuous arcs, the client constraints are represented
by dotted arcs). The tree explored with the previously out­
lined "DFBB" algorithm using the ordering (Dish, Drink,
is given figure 2. Labels are given by
their capitals, cutoffs are indicated by thick lines. The first
"best menu" found (compatibility 0.8) is as follows:
•

main dish : Fish of the day ;

Foie gras should be accompanied by a soft white wine (h,

•

drink : White wine ;

0.9);

•

entrance : Foie gras ;

•

dessert : Apple-pie ;

Meat should (almost) certainly be eaten with a red wine (g,
0.9);

•

•

I surely do not want any oysters in my menu (m,

Entrance, Dessert)

accompanied with white wine (e, 0.9) or water (f, 0.2);
•

•

The encoding in a 4 variables possibilistic CSP is immedi­

We shall first consider the following knowledge:
•

Our client now integrate its preferences :

After the leg of wild boar, a strawbeny ice

as

very good

digestive effects (i, 0.5);
•

No entrance or no dessert is not appreciated (by the restau­
rant) (j, 0.4), having both no entrance and dessert is even

•

less appreciated

(k, 0.6);

Having water

a drink is no good (1, 0.5);

as

6

0ne may also define 1-weak arc-consistency enforcing by
limiting the Revise1r to the inference of unary constraints whose

necessity is greater or equal than "Y· 1-weak arc-consistency leads
to label suppression. 0-weak arc-consistency is possibilistic arc­

consistency.

The overall consistency degree of the CSP is therefore equal
to 0.8. As the knowledge introduced makes no difference
between a soft and a dry white wine, our customer will
either drink its "foie gras" with a dry wine or its fish with a
soft wine. Nobody is perfect ...
The size of this problem makes arc-consistency enforcing
and forward-checking useless. Nevertheless, one may note
that the problem is actually 0.8 arc-consistent. As an exam­
ple, one of the unary constraint infered by arc-consistency

273

274

Schiex

been extended to take in account fuzzy antagonist temporal
constraints.
F

�

A·
;1/� ~
I I I I I
�

aooO.l WW
RW B
�

-o.2s
�

WB

o-0.2
p.oAP

s

aoo0.8
ww
p.o.2

W

CFG

a::-�

I

a:-11

0

N

=0.3
�.2 AP

Further researchs

We are currentl y working on the conversion of the possi­
bilistic AC1 like algorithm to more sophisticated schemes
as AC4 [Mohr et al.86]. A matter of study is also the fix­
point semantics of the possibilisticarc-consistency as is has
been done for classical CSP [Gusgen et al.88].
Several extension of possibilistic CSPs may be considered :

;:l''
a::-P

5

•

a::o-P

AP

=0.8
�.J

Figure 2: DFBB search

•

•

enforcing is a constraint that forbids the label "white vine"
with a 0.2 necessity (as b( { (drink, white wine ) }, a) = 0.8).
We are currently trying to apply these techniques in the
frame of job-shop scheduling. It is clear that the particular
nature of the constraints that appears in this framework
could (and should) be taken into account in the propagation
process, as it may be done in the AC-5 [Deville et al.91]
algorithm in classical CSP.
4

Related works

The obviously related work is ..possibilistic logic"
[Lang91b] which has been a fundamental basis for pos­
sibilistic CSP definition. J. Lang [Lang91a] has applied
propositional possibilistic logic to constraint satisfaction
problems. In our opinion, our approach offers greater
expressive power (let us recall that the encoding of the
SEND+MORE=MONEY problem in propositional logic leads
to no more than 2060 clauses and 88 propositional vari­
ables) and more varied and powerful techniques (the only
resolution technique used in propositional possibilistic logic
being essentialy a "backjumping-Iike" algorithm [Oxusoff
et al.89,Gashnig79]).
Other related works include Hierarchical Constraint Logic
Programming [Boming et al.89] that allows the expression
of prioritized constraints in the body of an Hom clause.
Satoh [Satoh90] proposes a formalisation of soft constraints
based on an interpretations ordering but does not provide
any algorithmic issue.
The system GARI [Descottes et al.85] which is more ori­
ented towards production rules is very close to ours as it
compute a solution that is the best compromise under a
set of antagonist constraints. It is also close to the OPAL
scheduling system [Bel et al.89,Bel et al.88] which has

•

•

Many CSP techniques (AC-n, path or k-consistency,
backjumping, learning, tree clustering, cycle cutset)
and useful properties (Freuder theorems [Freuder82])
should be adapted or extended to possibilistic CSP ;
The integration of fuzzy constraints (defined as a fuzzy
set of authorized labelings) is almost immediate and
leads to an even greater expressive power.
As is has been shown for possibilistic logic [Dubois
et al.91b], the pre-order induced by necessity-valued
constraints is a numerical "epistemic entrenchment"
relation [Gardenfors et al.88]. The consistency degree
of a possibilistic CSP may be considered as an indica­
tor of the constraints that should be suppressed for the
"contraction" of a CSP upon revision. However, as an
anonymous referee pointed out, that means excluding
every constraint below the inconsistency degree. This
is somewhat too drastic, for some of these constraints
may be not "involved" in inconsistencies. This could
be corrected by an adequate redefinition of the label­
ing compatibility, or by complete redefinition of the
mesure used. However, algorithmic issues will have
to be reconsidered.
Possibility and necessity measures may be seen as spe­
cific decomposable measures [Dubois et a1.82]. We
think that most of this work could be easily extended
to such measures (including probabilistic measures).
Algorithmic issues will again have to be reconsidered.
Possibilistic logic programming as been experimented
in [Dubois et al.91a]. The integration of Possibilistic
logic programming and possibilistic CSP is a first step
toward Possibilistic Constraint Logic Programming.

Acknowledgements

The author wants to thank Helene Fargier, Jerome Lang
and the anonymous referees for their fruitful comments on
previous releases of this paper.


The Weighted Constraint Satisfaction Problem (WCSP) framework allows representing
and solving problems involving both hard constraints and cost functions. It has been applied to various problems, including resource allocation, bioinformatics, scheduling, etc. To
solve such problems, solvers usually rely on branch-and-bound algorithms equipped with
local consistency filtering, mostly soft arc consistency. However, these techniques are not
well suited to solve problems with very large domains. Motivated by the resolution of an
RNA gene localization problem inside large genomic sequences, and in the spirit of bounds
consistency for large domains in crisp CSPs, we introduce soft bounds arc consistency, a
new weighted local consistency specifically designed for WCSP with very large domains.
Compared to soft arc consistency, BAC provides significantly improved time and space
asymptotic complexity. In this paper, we show how the semantics of cost functions can
be exploited to further improve the time complexity of BAC. We also compare both in
theory and in practice the efficiency of BAC on a WCSP with bounds consistency enforced
on a crisp CSP using cost variables. On two different real problems modeled as WCSP,
including our RNA gene localization problem, we observe that maintaining bounds arc consistency outperforms arc consistency and also improves over bounds consistency enforced
on a constraint model with cost variables.

1. Introduction
The Weighted Constraint Satisfaction Problem (WCSP) is an extension of the crisp Constraint Satisfaction Problem (CSP) that allows the direct representation of hard constraints
and cost functions. The WCSP defines a simple optimization (minimization) framework
with a wide range of applications in resource allocation, scheduling, bioinformatics (Sànchez,
de Givry, & Schiex, 2008; Zytnicki, Gaspin, & Schiex, 2008), electronic markets (Sandholm,
1999), etc. It also captures fundamental AI and statistical problems such as Maximum
Probability Explanation in Bayesian nets and Markov Random Fields (Chellappa & Jain,
1993).
As in crisp CSP, the two main approaches to solve WCSP are inference and search.
This last approach is usually embodied in a branch-and-bound algorithm. This algorithm
estimates at each node of the search tree a lower bound of the cost of the solutions of the
sub-tree.
c 2009 AI Access Foundation. All rights reserved.

Zytnicki, Gaspin, de Givry & Schiex

One of the most successful approaches to build lower bounds has been obtained by
extending the notion of local consistency to WCSP (Meseguer, Rossi, & Schiex, 2006).
This includes soft AC (Schiex, 2000), AC* (Larrosa, 2002), FDAC* (Larrosa & Schiex,
2004), EDAC* (Heras, Larrosa, de Givry, & Zytnicki, 2005), OSAC (Cooper, de Givry,
& Schiex, 2007) and VAC (Cooper, de Givry, Sànchez, Schiex, & Zytnicki, 2008) among
others. Unfortunately, the worst case time complexity bounds of the associated enforcing
algorithms are at least cubic in the domain size and use an amount of space which is at
least linear in the domain size. This makes these consistencies useless for problems with
very large domains.
The motivation for designing a local consistency which can be enforced efficiently on
problems with large domains follows from our interest in the RNA gene localization problem. Initially modeled as a crisp CSP, this problem has been tackled using bounds consistency (Choi, Harvey, Lee, & Stuckey, 2006; Lhomme, 1993) and dedicated propagators
using efficient pattern matching algorithms (Thébault, de Givry, Schiex, & Gaspin, 2006).
The domain sizes are related to the size of the genomic sequences considered and can reach
hundreds of millions of values. In order to enhance this tool with scoring capabilities and
improved quality of localization, a shift from crisp to weighted CSP is a natural step which
requires the extension of bounds consistency to WCSP. Beyond this direct motivation, this
extension is also useful in other domains where large domains occur naturally such as temporal reasoning or scheduling.
The local consistencies we define combine the principles of bounds consistency with the
principles of soft local consistencies. These definitions are general and are not restricted
to binary cost functions. The corresponding enforcing algorithms improve over the time
and space complexity of AC* by a factor of d and also have the nice but rare property, for
WCSP local consistencies, of being confluent.
As it has been done for AC-5 by Van Hentenryck, Deville, and Teng (1992) for functional
or monotonic constraints, we show that different forms of cost functions (largely captured
by the notion of semi-convex cost functions) can be processed more efficiently. We also
show that the most powerful of these bounds arc consistencies is strictly stronger than the
application of bounds consistency to the reified representation of the WCSP as proposed
by Petit, Régin, and Bessière (2000).
To conclude, we experimentally compare the efficiency of algorithms that maintain these
different local consistencies inside branch-and-bound on agile satellite scheduling problems (Verfaillie & Lemaı̂tre, 2001) and RNA gene localization problems (Zytnicki et al.,
2008) and observe clear speedups compared to different existing local consistencies.

2. Definitions and Notations
This section will introduce the main notions that will be used throughout the paper. We
will define the (Weighted) Constraint Satisfaction Problems, as well as a local consistency
property frequently used for solving the Weighted Constraint Satisfaction Problem: arc
consistency (AC*).
594

Bounds Arc Consistency for Weighted CSPs

2.1 Constraint Networks
Classic and weighted constraint networks share finite domain variables as one of their components. In this paper, the domain of a variable xi is denoted by D(xi ). To denote a value in
D(xi ), we use an index i as in vi , vi′ ,. . . For each variable xi , we assume that the domain of xi
is totally ordered by ≺i and we denote by inf(xi ) and sup(xi ) the minimum (resp. maximum)
values of the domain D(xi ). An assignment tS of a set of variables S = {xi1 , . . . , xir } is a
function that maps variables to elements of their domains: tS = (xi1 ← vi1 , . . . , xir ← vir )
with ∀i ∈ {i1 , . . . , ir }, tS (xi ) = vi ∈ D(xi ). For a given assignment tS such that xi ∈ S, we
simply say that a value vi ∈ D(xi ) belongs to tS to mean that tS (xi ) = vi . We denote by
ℓS , the set of all possible assignments on S.
Definition 2.1 A constraint network (CN) is a tuple P = hX , D, Ci, where X = {x1 , . . . , xn }
is a set of variables and D = {D(x1 ), . . . , D(xn )} is the set of the finite domains of each
variable. C is a set of constraints. A constraint cS ∈ C defines the set of all authorized
combinations of values for the variables in S as a subset of ℓS . S is called the scope of cS .
|S| is called the arity of cS . For simplicity, unary (arity 1) and binary (arity 2) constraints
may be denoted by ci and cij instead of c{xi } and c{xi ,xj } respectively. We denote by d the
maximum domain size, n, the number of variables in the network and e, the number of
constraints. The central problem on constraint networks is to find a solution, defined as
an assignment tX of all variables such that for any constraint cS ∈ C, the restriction of tX
to S is authorized by cS (all constraints are satisfied). This is the Constraint Satisfaction
Problem (CSP).
Definition 2.2 Two CNs with the same variables are equivalent if they have the same set
of solutions.
A CN will be said to be “empty” if one of its variables has an empty domain. This
may happen following local consistency enforcement. For CN with large domains, the use
of bounds consistency is the most usual approach. Historically, different variants of bounds
consistency have been introduced, generating some confusion. Using the terminology introduced by Choi et al. (2006), the bounds consistency considered in this paper is the
bounds(D) consistency. Because we only consider large domains defining intervals, this is
actually equivalent to bounds(Z) consistency. For simplicity, in the rest of the paper we
denote this as “bounds consistency”.
Definition 2.3 (Bounds consistency) A variable xi is bounds consistent iff every constraint cS ∈ C such that xi ∈ S contains a pair of assignments (t, t′ ) ∈ ℓS × ℓS such that
inf(xi ) ∈ t and sup(xi ) ∈ t′ . In this case, t and t′ are called the supports of the two bounds
of xi ’s domain.
A CN is bounds consistent iff all its variables are bounds consistent.
To enforce bounds consistency on a given CN, any domain bound that does not satisfy
the above properties is deleted until a fixed point is reached.
595

Zytnicki, Gaspin, de Givry & Schiex

2.2 Weighted Constraint Networks
Weighted constraint networks are obtained by using cost functions (also referred as “soft
constraints”) instead of constraints.
Definition 2.4 A weighted constraint network (WCN) is a tuple P = hX , D, W, ki, where
X = {x1 , . . . , xn } is a set of variables and D = {D(x1 ), . . . , D(xn )} is the set of the finite
domains of each variable. W is a set of cost functions. A cost function wS ∈ W associates
an integer cost wS (tS ) ∈ [0, k] to every assignment tS of the variables in S. The positive
number k defines a maximum (intolerable) cost.
The cost k, which may be finite or infinite, is the cost associated with forbidden assignments. This cost is used to represent hard constraints. Unary and binary cost functions
may be denoted by wi and wij instead of w{xi } and w{xi ,xj } respectively. As usually for
WCNs, we assume the existence of a zero-arity cost function, w∅ ∈ [0, k], a constant cost
whose initial value is usually equal to 0. The cost of an assignment tX of all variables is
obtained by combining the costs of all the cost functions wS ∈ W applied to the restriction
of tX to S. The combination is done using the function ⊕ defined as a ⊕ b = min(k, a + b).
Definition 2.5 A solution of a WCN is an assignment tX of all variables whose cost is
less than k. It is optimal if no other assignment of X has a strictly lower cost.
The central problem in WCN is to find an optimal solution.
Definition 2.6 Two WCNs with the same variables are equivalent if they give the same
cost to any assignments of all their variables.
Initially introduced by Schiex (2000), the extension of arc consistency to WCSP has
been refined by Larrosa (2002) leading to the definition of AC*. It can be decomposed into
two sub-properties: node and arc consistency itself.
Definition 2.7 (Larrosa, 2002) A variable xi is node consistent iff:
• ∀vi ∈ D(xi ), w∅ ⊕ wi (vi ) < k.
• ∃vi ∈ D(xi ) such that wi (vi ) = 0. The value vi is called the unary support of xi .
A WCN is node consistent iff every variable is node consistent.
To enforce NC on a WCN, values that violate the first property are simply deleted.
Value deletion alone is not capable of enforcing the second property. As shown by Cooper
and Schiex (2004), the fundamental mechanism required here is the ability to move costs
between different scopes. A cost b can be subtracted from a greater cost a by the function
⊖ defined by a ⊖ b = (a − b) if a 6= k and k otherwise. Using ⊖, a unary support for a
variable xi can be created by subtracting the smallest unary cost minvi ∈D(xi ) wi (vi ) from
all wi (vi ) and adding it (using ⊕) to w∅. This operation that shifts costs from variables
to w∅, creating a unary support, is called a projection from wi to w∅. Because ⊖ and ⊕
cancel out, defining a fair valuation structure (Cooper & Schiex, 2004), the obtained WCN
is equivalent to the original one. This equivalence preserving transformation (Cooper and
Schiex) is more precisely described as the ProjectUnary() function in Algorithm 1.
We are now able to define arc and AC* consistency on WCN.
596

Bounds Arc Consistency for Weighted CSPs

Algorithm 1: Projections at unary and binary levels
1
2
3
4
5
6
7
8
9
10

Procedure ProjectUnary(xi )
min ← minvi ∈D(xi ) {wi (vi )} ;
if (min = 0) then return;
foreach vi ∈ D(xi ) do wi (vi ) ← wi (vi ) ⊖ min ;
w∅ ← w∅ ⊕ min ;

[ Find the unary support of xi ]

Procedure Project(xi , vi , xj )
[ Find the support of vi w.r.t. wij ]
min ← minvj ∈D(xj ) {wij (vi , vj )} ;
if (min = 0) then return;
foreach vj ∈ D(xj ) do wij (vi , vj ) ← wij (vi , vj ) ⊖ min ;
wi (vi ) ← wi (vi ) ⊕ min ;

Definition 2.8 A variable xi is arc consistent iff for every cost function wS ∈ W such that
xi ∈ S, and for every value vi ∈ D(xi ), there exists an assignment t ∈ ℓS such that vi ∈ t
and wS (t) = 0. The assignment t is called the support of vi on wS . A WCN is AC* iff
every variable is arc and node consistent.
To enforce arc consistency, a support for a given value vi of xi on a cost function wS
can be created by subtracting (using ⊖) the cost mint∈ℓS ,vi ∈t wS (t) from the costs of all
assignments containing vi in ℓS and adding it to wi (vi ). These cost movements, applied
for all values vi of D(xi ), define the projection from wS to wi . Again, this transformation
preserves equivalence between problems. It is more precisely described (for simplicity, in
the case of binary cost functions) as the Project() function in Algorithm 1.

Example 2.9 Consider the WCN in Figure 1(a). It contains two variables (x1 and x2 ),
each with two possible values (a and b, represented by vertices). A unary cost function is
associated with each variable, the cost of a value being represented inside the corresponding
vertex. A binary cost function between the two variables is represented by weighted edges
connecting pairs of values. The absence of edge between two values represents a zero cost.
Assume k is equal to 4 and w∅ is equal to 0.
Since the cost w1 (x1 ← a) is equal to k, the value a can be deleted from the domain
of x1 (by NC, first property). The resulting WCN is represented in Figure 1(b). Then,
since x2 has no unary support (second line of the definition of NC), we can project a cost
of 1 to w∅ (cf. Figure 1(c)). The instance is now NC. To enforce AC*, we project 1 from
the binary cost function w12 to the value a of x1 since this value has no support on w12
(cf. Figure 1(d)). Finally, we project 1 from w1 to w∅, as seen on Figure 1(e). Ultimately,
we note that the value b of x2 has no support. To enforce AC*, we project a binary cost
of 1 to this value and remove it since it has a unary cost of 2 which, combined with w∅
reaches k = 4.

597

Zytnicki, Gaspin, de Givry & Schiex

w∅ = 0, k = 4
x1
x2

w∅ = 0, k = 4
x2
x1
a

1

4

a

1

a

1

0

2

b

2

b

a
1

0

2

2

b

b

0

a

a

1

b

b

0

1

0

x1

0

a

a

1

b

b

(e) find unary support using
ProjectUnary(x1 )

(d) find support for (x1 ←
b) using Project(x1 , b, x2 )
(AC*)

0

a

2

1

b

(c) find unary support using
ProjectUnary(x2 ) (NC*)

w∅ = 2, k = 4
x2
x1

w∅ = 1, k = 4
x1
x2

1

a
1

(b) prune forbidden values
(NC*)

(a) original instance

b

a

1

1

b

w∅ = 1, k = 4
x2
x1

w∅ = 2, k = 4
x2
0

0

a
b

(f) Arc consistency enforced

Figure 1: Enforcing Arc Consistency.

3. Bounds Arc Consistency (BAC)
In crisp CSP, the bounds consistency enforcing process just deletes bounds that are not
supported in one constraint. In weighted CSP, enforcement is more complex. If a similar
value deletion process exists based on the first node consistency property violation (whenever
w∅ ⊕ wi (vi ) reaches k), additional cost movements are performed to enforce node and arc
consistency.
As shown for AC*, these projections require the ability to represent an arbitrary unary
cost function wi for every variable xi . This requires space in O(d) in general since projections
can lead to arbitrary changes in the original wi cost function (even if they have an efficient
internal representation). To prevent this, we therefore avoid to move cost from cost functions
with arity greater than one to unary constraints. Instead of such projections, we only keep
a value deletion mechanism applied to the bounds of the current domain that takes into
account all the cost functions involving the variable considered. For a given variable xi
involved in a cost function wS , the choice of a given value vi will at least induce a cost
increase of mintS ∈ℓS ,vi ∈tS wS (tS ). If these minimum costs, combined on all the cost functions
involving xi , together with w∅, reach the intolerable cost of k, then the value can be deleted.
As in bounds consistency, this is just done for the two bounds of the domain. This leads to
the following definition of BAC (bounds arc consistency) in WCSP:
Definition 3.1 In a WCN P = hX , D, W, ki, a variable xi is bounds arc consistent iff:


X
wS (tS ) < k
w∅ ⊕
min
tS ∈ℓS ,inf(xi )∈tS

wS ∈W,xi ∈S

w∅ ⊕

X

wS ∈W,xi ∈S



min

tS ∈ℓS ,sup(xi )∈tS


wS (tS ) < k

A WCN is bounds arc consistent if every variable is bounds arc consistent.
598

Bounds Arc Consistency for Weighted CSPs

One can note that this definition is a proper generalization of bounds consistency since
when k = 1, it is actually equivalent to the definition of bounds(D) consistency for crisp
CSP (Choi et al., 2006) (also equivalent to bounds(Z) consistency since domains are defined
as intervals).
The algorithm enforcing BAC is described as Algorithm 2. Because enforcing BAC only
uses value deletion, it is very similar in structure to bounds consistency enforcement. We
maintain a queue Q of variables whose domain has been modified (or is untested). For
better efficiency, we use extra data-structures to efficiently maintain the combined cost associated with the domain bound inf(xi ), denoted winf (xi ). For a cost function wS involving
xi , the contribution of wS to this combined cost is equal to mintS ∈ℓS ,inf(xi )∈tS wS (tS ). This
contribution is maintained in a data-structure ∆inf (xi , wS ) and updated whenever the minimum cost may change because of value removals. Notice that, in Algorithm 2, the line 14
is a concise way to denote the hidden loops which initialize the winf , wsup , ∆inf and ∆sup
data-structures to zero.
Domain pruning is achieved by function PruneInf() which also resets the data-structures
associated with the variable at line 35 and these data-structures are recomputed when the
variable is extracted from the queue. Indeed, inside the loop of line 20, the contributions
∆inf (xi , wS ) to the cost winf (xi ) from the cost functions wS involving xj are reset. The
Function pop removes an element from the queue and returns it.
Proposition 3.2 (Time and space complexity) For a WCN with maximum arity r of
the constraints, enforcing BAC with Algorithm 2 is time O(er2 dr ) and space O(n + er).
Proof: Regarding time, every variable can be pushed into Q at most d + 1 times: once
at the beginning, and when one of its values has been removed. As a consequence, the
foreach loop on line 18 iterates O(erd) times, and the foreach loop on line 20 iterates
O(er2 d) times. The min computation on line 22 takes time O(dr−1 ) and thus, the overall
time spent at this line takes time O(er2 dr ). PruneInf() is called at most O(er2 d) times.
The condition on line 32 is true at most O(nd) times and so, line 35 takes time O(ed)
(resetting ∆inf (xi , ·) on line 35 hides a loop on all cost functions involving xi ). The total
time complexity is thus O(er2 dr ).
Regarding space, we only used winf , wsup and ∆ data-structures. The space complexity
is thus O(n + er).

Note that exploiting the information of last supports as in AC2001 (Bessière & Régin,
2001) does not reduce the worst-case time complexity because the minimum cost of a cost
function must be recomputed from scratch each time a domain has been reduced and the
last support has been lost (Larrosa, 2002). However, using last supports helps in practice
to reduce mean computation time and this has been done in our implementation.
Compared to AC*, which can be enforced in O(n2 d3 ) time and O(ed) space for binary
WCN, BAC can be enforced d times faster, and the space complexity becomes independent
of d which is a requirement for problems with very large domains.
Another interesting difference with AC* is that BAC is confluent — just as bounds
consistency is. Considering AC*, it is known that there may exist several different AC*
closures with possibly different associated lower bounds w∅ (Cooper & Schiex, 2004). Note
that although OSAC (Cooper et al., 2007) is able to find an optimal w∅ (at much higher
599

Zytnicki, Gaspin, de Givry & Schiex

Algorithm 2: Algorithm enforcing BAC.
11
12
14
15
16
18
20
22
23
24
25
26
27
28
29

30
32
33
35
36
37
38
39
40
41
42
43

Procedure BAC(X , D, W, k)
Q←X ;
winf (·) ← 0 ; wsup (·) ← 0 ; ∆inf (·, ·) ← 0 ; ∆sup (·, ·) ← 0 ;
while (Q 6= ∅) do
xj ← pop(Q) ;
foreach wS ∈ W, xj ∈ S do
foreach xi ∈ S do
α ← mintS ∈ℓS ,inf(xi )∈tS wS (tS ) ;
winf (xi ) ← winf (xi ) ⊖ ∆inf (xi , wS ) ⊕ α ;
∆inf (xi , wS ) ← α ;
if PruneInf(xi ) then Q ← Q ∪ {xi } ;
α ← mintS ∈ℓS ,sup(xi )∈tS wS (tS ) ;
wsup (xi ) ← wsup (xi ) ⊖ ∆sup (xi , wS ) ⊕ α ;
∆sup (xi , wS ) ← α ;
if PruneSup(xi ) then Q ← Q ∪ {xi } ;
Function PruneInf(xi ) : boolean
if (w∅ ⊕ winf (xi ) = k) then
delete inf(xi ) ;
winf (xi ) ← 0 ; ∆inf (xi , ·) ← 0 ;
return true;
else return false;
Function PruneSup(xi ) : boolean
if (w∅ ⊕ wsup (xi ) = k) then
delete sup(xi ) ;
wsup (xi ) ← 0 ; ∆sup (xi , ·) ← 0 ;
return true;
else return false;

600

Bounds Arc Consistency for Weighted CSPs

computational cost), it is still not confluent. The following property shows that BAC is
confluent.
Proposition 3.3 (Confluence) Enforcing BAC on a given problem always leads to a
unique WCN.
Proof: We will prove the proposition as follows. We will first define a set of problems
which contains all the problems that can be reached from the original WCN through BAC
enforcement. Notice that, at each step of BAC enforcement, in the general case, several
operations can be performed and no specific order is imposed. Therefore, a set of problems
can be reached at each step. We will show that the set of problems has a lattice structure
and ultimately show that the closure of BAC is the lower bound of this lattice, and is
therefore unique, which proves the property. This proof technique is usual for proving
convergence of the chaotic iteration of a collection of suitable functions and has been used
for characterizing CSP local consistency by Apt (1999).
During the enforcement of BAC, the original problem P = hX , D, W, ki is iteratively
transformed into a set of different problems which are all equivalent to P, and obtained
by deleting values violating BAC. Because these problems are obtained by value removals,
they belong to the set ℘1 (P ) defined by: {hX , D′ , W, ki : D′ ⊆ D}.
We now define a relation, denoted ⊑, on the set ℘1 (P ):
∀(P1 , P2 ) ∈ ℘21 (P), P1 ⊑ P2 ⇔ ∀i ∈ [1, n], D1 (xi ) ⊆ D2 (xi )
It is easy to see that this relation defines a partial order. Furthermore, each pair of
elements has a greatest lower bound glb and a least upper bound lub in ℘1 (P), defined by:
∀(P1 , P2 ) ∈ ℘21 (P),
glb(P1 , P2 ) = hX , {D1 (xi ) ∩ D2 (xi ) : i ∈ [1, n]}, W, ki ∈ ℘1 (P)
lub(P1 , P2 ) = hX , {D1 (xi ) ∪ D2 (xi ) : i ∈ [1, n]}, W, ki ∈ ℘1 (P)
h℘1 (P), ⊑i is thus a complete lattice.
BAC filtering works by removing values violating the BAC properties, transforming
an original problem into a succession of equivalent problems. Each transformation can be
described by the application of dedicated functions from ℘1 (P) to ℘1 (P). More precisely,
there are two such functions for each variable, one for the minimum bound inf(xi ) of the
domain of xi and a symmetrical one for the maximum bound. For inf(xi ), the associated
function keeps the instance unchanged if inf(xi ) satisfies the condition of Definition 3.1 and
it otherwise returns a WCN where inf(xi ) alone has been deleted. The collection of all those
functions defines a set of functions from ℘1 (P ) to ℘1 (P ) which we denote by FBAC .
Obviously, every function f ∈ FBAC is order preserving:
∀(P1 , P2 ) ∈ ℘21 (P), P1 ⊑ P2 ⇒ f (P1 ) ⊑ f (P2 )
By application of the Tarski-Knaster theorem (Tarski, 1955), it is known that every
function f ∈ FBAC (applied until quiescence during BAC enforcement) has at least one
fixpoint, and that the set of these fixed points forms a lattice for ⊑. Moreover, the intersection of the lattices of fixed points of the functions f ∈ FBAC , denoted by ℘⋆1 (P), is also
601

Zytnicki, Gaspin, de Givry & Schiex

a lattice. ℘⋆1 (P) is not empty since the problem hX , {∅, . . . , ∅}, Wi is a fixpoint for every
filtering function in FBAC . ℘⋆1 (P) is exactly the set of fixed points of FBAC .
We now show that, if the algorithm reaches a fixpoint, it reaches the greatest element
of ℘⋆1 (P). We will prove by induction that any successive application of elements of FBAC
on P yields problems which are greater than any element of ℘⋆1 (P) for the order ⊑. Let
us consider any fixpoint P ⋆ of ℘⋆1 (P). Initially, the algorithm applies on P, which is the
greatest element of ℘1 (P), and thus P ⋆ ⊑ P. This is the base case of the induction. Let
us now consider any problem P1 obtained during the execution of the algorithm. We have,
by induction, P ⋆ ⊑ P1 . Since ⊑ is order preserving, we know that, for any function f of
FBAC , f (P ⋆ ) = P ⋆ ⊑ f (P1 ). This therefore proves the induction.
To conclude, if the algorithm terminates, then it gives the maximum element of ℘⋆1 (P).
Since proposition 3.2 showed that the algorithm actually terminates, we can conclude that
it is confluent.

If enforcing BAC may reduce domains, it never increases the lower bound w∅. This is an
important limitation given that each increase in w∅ may generate further value deletions and
possibly, failure detection. Note that even when a cost function becomes totally assigned,
the cost of the corresponding assignment is not projected to w∅ by BAC enforcement. This
can be simply done by maintaining a form of backward checking as in the most simple
WCSP branch-and-bound algorithm (Freuder & Wallace, 1992). To go beyond this simple
approach, we consider the combination of BAC with another WCSP local consistency which,
similarly to AC*, requires cost movements to be enforced but which avoids the modification
of unary cost functions to keep a reasonable space complexity. This is achieved by directly
moving costs to w∅.

4. Enhancing BAC
In many cases, BAC may be very weak compared to AC* in situations where it seems to
be possible to infer a decent w∅ value. Consider for example the following cost function:

D(x1 ) × D(x2 ) →
E
D(x1 ) = D(x2 ) = [1, 10]
w12 :
(v1 , v2 )
7→ v1 + v2
AC* can increase w∅ by 2, by projecting a cost of 2 from w12 to the unary constraint w1
on every value, and then projecting these costs from w1 to w∅ by enforcing NC. However,
if w∅ = w1 = w2 = 0 and k is strictly greater than 11, BAC remains idle here. We can
however simply improve BAC by directly taking into account the minimum possible cost of
the cost function w12 over all possible assignments given the current domains.
Definition 4.1 A cost function wS is ∅-inverse consistent (∅-IC) iff:
∃tS ∈ ℓS , wS (tS ) = 0
Such a tuple tS is called a support for wS . A WCN is ∅-IC iff every cost function (except
w∅) is ∅-IC.
Enforcing ∅-IC can always be done as follows: for every cost function wS with a non
empty scope, the minimum cost assignment of wS given the current variable domains is
602

Bounds Arc Consistency for Weighted CSPs

computed. The cost α of this assignment is then subtracted from all the tuple costs in wS
and added to w∅. This creates at least one support in wS and makes the cost function
∅-IC. For a given cost function wS , this is done by the Project() function of Algorithm 3.
In order to strengthen BAC, a natural idea is to combine it with ∅-IC. We will call BAC∅
the resulting combination of BAC and ∅-IC. To enforce BAC∅, the previous algorithm
is modified by first adding a call to the Project() function (see line 53 of Algorithm 3).
Moreover, to maintain BAC whenever w∅ is modified by projection, every variable is tested
for possible pruning at line 66 and put back in Q in case of domain change. Note that
the subtraction applied to all constraint tuples at line 75 can be done in constant time
without modifying the constraint by using an additional ∆wS data-structure, similar to
the ∆ data-structure introduced by Cooper and Schiex (2004). This data-structure keeps
track of the cost which has been projected from wS to w∅. This feature makes it possible
to leave the original costs unchanged during the enforcement of the local consistency. For
example, for any tS ∈ ℓS , wS (t) refers to wS (t) ⊖ ∆wS , where wS (t) denotes the original
cost. Note that ∆wS , which will be later used in a confluence proof, precisely contains the
amount of cost which has been moved from wS to w∅. The whole algorithm is described
in Algorithm 3. We highlighted in black the parts which are different from Algorithm 2
whereas the unchanged parts are in gray.
Proposition 4.2 (Time and space complexity) For a WCN with maximum arity r of
the constraints, enforcing BAC∅ with Algorithm 3 can be enforced in O(n2 r2 dr+1 ) time
using O(n + er) memory space.
Proof: Every variable is pushed at most O(d) times in Q, thus the foreach at line 51
(resp. line 55) loops at most O(erd) (resp. O(er2 d)) times. The projection on line 53 takes
O(dr ) time. The operation at line 57 can be carried out in O(dr−1 ) time. The overall time
spent inside the if of the PruneInf() function is bounded by O(ed). Thus the overall time
spent in the loop at line 51 (resp. line 55) is bounded by O(er2 dr+1 ) (resp. O(er2 dr )).
The flag on line 66 is true when w∅ increases, and so it cannot be true more than k times
(assuming integer costs). If the flag is true, then we spend O(n) time to check all the bounds
of the variables. Thus, the time complexity under the if is bounded by O(min{k, nd} × n).
To sum up, the overall time complexity is O(er2 dr+1 + min{k, nd} × n), which is bounded
by O(n2 r2 dr+1 ).
The space complexity is given by the ∆, winf , wsup and ∆wS data-structures which sums
up to O(n + re) for a WCN with an arity bounded by r.

The time complexity of the algorithm enforcing BAC∅ is multiplied by d compared
to BAC without ∅-IC. This is a usual trade-off between the strength of a local property
and the time spent to enforce it. However, the space complexity is still independent of d.
Moreover, like BAC, BAC∅ is confluent.
Proposition 4.3 (Confluence) Enforcing BAC∅ on a given problem always leads to a
unique WCN.
Proof: The proof is similar to the proof of Proposition 3.3. However, because of the
possible cost movements induced by projections, BAC∅ transforms the original problem P
in more complex ways, allowing either pruning domains (BAC) or moving costs from cost
603

Zytnicki, Gaspin, de Givry & Schiex

Algorithm 3: Algorithm enforcing BAC∅
44
45
46
47
48
49
51
53
55
57
58
59
60
61
62
63
64
66
67
68
69

70
71
72
73
75
76
77

Procedure BAC∅(X , D, W, k)
Q←X ;
winf (·) ← 0 ; wsup (·) ← 0 ; ∆inf (·, ·) ← 0 ; ∆sup (·, ·) ← 0 ;
while (Q 6= ∅) do
xj ← pop(Q) ;
flag ← false ;
foreach wS ∈ W, xj ∈ S do
if Project(wS ) then flag ← true ;
foreach xi ∈ S do
α ← mintS ∈ℓS ,inf(xi )∈tS wS (tS ) ;
winf (xi ) ← winf (xi ) ⊖ ∆inf (xi , wS ) ⊕ α ;
∆inf (xi , wS ) ← α ;
if PruneInf(xi ) then Q ← Q ∪ {xi } ;
α ← mintS ∈ℓS ,sup(xi )∈tS wS (tS ) ;
wsup (xi ) ← wsup (xi ) ⊖ ∆sup (xi , wS ) ⊕ α ;
∆sup (xi , wS ) ← α ;
if PruneSup(xi ) then Q ← Q ∪ {xi } ;
if (flag) then
foreach xi ∈ X do
if PruneInf(xi ) then Q ← Q ∪ {xi } ;
if PruneSup(xi ) then Q ← Q ∪ {xi } ;
Function Project(wS ) : boolean
α ← mintS ∈ℓS wS (tS ) ;
if (α > 0) then
w∅ ← w∅ ⊕ α ;
wS (·) ← wS (·) ⊖ α ;
return true;
else return false;

604

Bounds Arc Consistency for Weighted CSPs

functions to w∅. The set of problems that will be considered needs therefore to take this
into account. Instead of being just defined by its domains, a WCN reached by BAC∅ is
also characterized by the amount of cost that has been moved from each cost function wS
to w∅. This quantity is already denoted by ∆wS in Section 4, on page 603. We therefore
consider the set ℘2 (P) defined by:

(hX , D′ , W, ki, {∆w : w ∈ W}) : ∀i ∈ [1, n], D′ (xi ) ⊆ D(xi ), ∀w ∈ W, ∆w ∈ [0, k]
We can now define the relation ⊑ on ℘2 (P):

w
P1 ⊑ P2 ⇔ ((∀w ∈ W, ∆w
1 ≥ ∆2 ) ∧ (∀xi ∈ X , D1 (xi ) ⊆ D2 (xi )))

This relation is reflexive, transitive and antisymmetric. The first two properties can be
easily verified. Suppose now that (P1 , P2 ) ∈ ℘22 (P) and that (P1 ⊑ P2 ) ∧ (P2 ⊑ P1 ). We
have thus (∀w ∈ W, ∆w = ∆′w )∧(∀xi ∈ X , D(xi ) = D′ (xi )). This ensures that the domains,
as well as the amounts of cost projected by each cost function, are the same. Thus, the
problems are the same and ⊑ is antisymmetric.
Besides, h℘2 (P), ⊑i is a complete lattice, since:
∀(P1 , P2 ) ∈ ℘22 (P),
w
glb(P1 , P2 ) = (hX , {D1 (xi ) ∩ D2 (xi ) : i ∈ [1, n]}, W, ki, {max{∆w
1 , ∆2 } : w ∈ W})
w
lub(P1 , P2 ) = (hX , {D1 (xi ) ∪ D2 (xi ) : i ∈ [1, n]}, W, ki, {min{∆w
1 , ∆2 } : w ∈ W})

and both of them are in ℘2 (P).
Every enforcement of BAC∅ follows from the application of functions from a set of functions FBAC ∅ which may remove the maximum or minimum domain bound (same definition
as for BAC) or may project cost from cost functions to w∅. For a given cost function
w ∈ W, such a function keeps the instance unchanged if the minimum α of w is 0 over
possible tuples. Otherwise, if α > 0, the problem returned is derived from P by projecting
an amount of cost α from w to w∅. These functions are easily shown to be order preserving
for ⊑.
As in the proof of Proposition 3.3, we can define the lattice ℘⋆2 (P), which is the intersection of the sets of fixed points of the functions f ∈ FBAC ∅ . ℘⋆2 (P) is not empty, since
(hX , {∅, . . . , ∅}, W, ki, {k, . . . , k}) is in it. As in the proof of proposition 3.3, and since Algorithm 3 terminates, we can conclude that this algorithm is confluent, and that it results
in lub(℘⋆2 (P)).


5. Exploiting Cost Function Semantics in BAC∅
In crisp AC, several classes of binary constraints make it possible to enforce AC significantly
faster (in O(ed) instead of O(ed2 ), as shown by Van Hentenryck et al., 1992). Similarly,
it is possible to exploit the semantics of the cost functions to improve the time complexity
of BAC∅ enforcement. As the proof of Proposition 4.2 shows, the dominating factors in
this complexity comes from the complexity of computing the minimum of cost functions
during projection at lines 53 and 57 of Algorithm 3. Therefore, any cost function property
605

Zytnicki, Gaspin, de Givry & Schiex

that makes these computations less costly may lead to an improvement of the overall time
complexity.
Proposition 5.1 In a binary WCN, if for any cost function wij ∈ W and for any subintervals Ei ⊆ D(xi ), Ej ⊆ D(xj ), the minimum of wij over Ei × Ej can be found in time
O(d), then the time complexity of enforcing BAC∅ is O(n2 d2 ).
Proof: This follows directly from the proof of Proposition 4.2. In this case, the complexity
of projection at line 53 is only in O(d) instead of O(d2 ). Thus the overall time spent in the
loop at line 51 is bounded by O(ed2 ) and the overall complexity is O(ed2 + n2 d) ≤ O(n2 d2 ).

Proposition 5.2 In a binary WCN, if for any cost function wij ∈ W and for any subintervals Ei ⊆ D(xi ), Ej ⊆ D(xj ), the minimum of wij over Ei × Ej can be found in
constant time, then the time complexity of enforcing BAC∅ is O(n2 d).
Proof: This follows again from the proof of Proposition 4.2. In this case, the complexity
of projection at line 53 is only in O(1) instead of O(d2 ). Moreover, the operation at line 57
can be carried out in time O(1) instead of O(d). Thus, the overall time spent in the loop
at line 51 is bounded by O(ed) and the overall complexity is O(ed + n2 d) = O(n2 d).

These two properties are quite straightforward and one may wonder if they have non
trivial usage. They can actually be directly exploited to generalize the results presented
by Van Hentenryck et al. (1992) for functional, anti-functional and monotonic constraints.
In the following sections, we show that functional, anti-functional and semi-convex cost functions (which include monotonic cost functions) can indeed benefit from an O(d) speedup
factor by application of Proposition 5.1. For monotonic cost functions and more generally
any convex cost function, a stronger speedup factor of O(d2 ) can be obtained by Proposition 5.2.
5.1 Functional Cost Functions
The notion of functional constraint can be extended to cost functions as follows:
Definition 5.3 A cost function wij is functional w.r.t. xi iff:
• ∀(vi , vj ) ∈ D(xi ) × D(xj ), wij (vi , vj ) ∈ {0, α} with α ∈ [1, k]
• ∀vi ∈ D(xi ), there is at most one value vj ∈ D(xj ) such that wij (vi , vj ) = 0. When it
exists, this value is called the functional support of vi .
We assume in the rest of the paper that the functional
support can be computed in constant
(
0 if xi = xj
= =
time. For example, the cost function wij
is functional. In this case, the
1 otherwise
functional support of vi is itself. Note that for k = 1, functional cost functions represent
functional constraints.
Proposition 5.4 The minimum of a functional cost function wij w.r.t. xi can always be
found in O(d).
606

Bounds Arc Consistency for Weighted CSPs

Proof: For every value vi of xi , one can just check if the functional support of vi belongs
to the domain of xj . This requires O(d) checks. If this is never the case, then the minimum
of the cost function is known to be α. Otherwise, it is 0. The result follows.

5.2 Anti-Functional and Semi-Convex Cost Functions
Definition 5.5 A cost function wij is anti-functional w.r.t. the variable xi iff:
• ∀(vi , vj ) ∈ D(xi ) × D(xj ), wij (vi , vj ) ∈ {0, α} with α ∈ [1, k]
• ∀vi ∈ D(xi ), there is at most one value vj ∈ D(xj ) such that wij (vi , vj ) = α. When it
exists, this value is called the anti-support of vi .
(
0 if xi 6= xj
6=
The cost function wij =
is an example of an anti-functional cost function.
1 otherwise
In this case, the anti-support of vi is itself. Note that for k = 1, anti-functional cost functions
represent anti-functional constraints.
Anti-functional cost functions are actually a specific case of semi-convex cost functions,
a class of cost functions that appear for example in temporal constraint networks with
preferences (Khatib, Morris, Morris, & Rossi, 2001).
Definition 5.6 Assume that the domain D(xj ) is contained in a set Dj totally ordered by
the order <j .
A function wij is semi-convex w.r.t. xi iff ∀β ∈ [0, k], ∀vi ∈ Di , the set {vj ∈ Dj :
wij (vi , vj ) ≥ β}, called the β-support of vi , defines an interval over Dj according to <j .
Semi-convexity relies on the definition of intervals defined in a totally ordered discrete
set denoted Dj , and ordered by <j . Even if they may be identical, it is important to avoid
confusion between the order ≺j over D(xj ), used to define interval domains for bounds
arc consistency, and the order <j over Dj used to define intervals for semi-convexity. In
order to guarantee constant time access to the minimum and maximum elements of D(xj )
according to <j (called the <j -bounds of the domain), we assume that <j =≺j or <j =≻j 1 .
In this case, the <j -bounds and the domain bounds are identical.
One can simply check that anti-functional cost functions are indeed semi-convex: in
this case, the β-support of any value is either the whole domain (β = 0), reduced to one
point (0 < β ≤ α) or to the empty set (otherwise). Another example is the cost function
wij = x2i − x2j which is semi-convex w.r.t. xi .
Proposition 5.7 The minimum of a cost function wij which is semi-convex w.r.t. one of
its variables can always be found in O(d).
Proof: We will first show that, if wij is semi-convex w.r.t. to one of its variables (let
say xi ), then for any value vi of xi , the cost function wij must be minimum at one of the
<j -bounds of Dj .
1. This restriction could be removed using for example a doubly-linked list data-structure over the values
in D(xj ), keeping the domain sorted according to <j and allowing constant time access and deletion but
this would be at the cost of linear space which we cannot afford in the context of BAC.

607

Zytnicki, Gaspin, de Givry & Schiex

Assume xi is set to vi . Let βb be the lowest cost reached on either of the two <j -bounds
of the domain. Since wij is semi-convex, then {vj ∈ Dj : wij (vi , vj ) ≥ βb } is an interval,
and thus every cost wij (vi , vj ) is not less than βb for every value of Dj . Therefore, at least
one of the two <j -bounds has a minimum cost.
In order to find the global minimum of wij , we can restrict ourselves to the <j -bounds
of the domain of xj for every value of xi . Therefore, only 2d costs need to be checked. 
From Proposition 5.1, we can conclude
Corollary 5.8 In a binary WCN, if all cost functions are functional, anti-functional or
semi-convex, the time complexity of enforcing BAC∅ is O(n2 d2 ) only.
5.3 Monotonic and Convex Cost Functions
Definition 5.9 Assume that the domain D(xi ) (resp. D(xj )) is contained in a set Di (resp.
Dj ) totally ordered by the order <i (resp. <j ).
A cost function wij is monotonic iff:
∀(vi , vi′ , vj , vj′ ) ∈ Di2 × Dj2 , vi′ ≤i vi ∧ vj′ ≥j vj ⇒ wij (vi′ , vj′ ) ≤ wij (vi , vj )
(

0 if xi ≤ xj
is an example of a monotonic cost function.
1 otherwise
Monotonic cost functions are actually instances of a larger class of functions called convex
functions.
The cost function

≤
wij

=

Definition 5.10 A function wij is convex iff it is semi-convex w.r.t. each of its variables.
For example, wij = xi + xj is convex.
Proposition 5.11 The minimum of a convex cost function can always be found in constant
time.
Proof: Since the cost function is semi-convex w.r.t. each of its variable, we know from the
proof of Proposition 5.7 that it must reach a minimum cost on one of the <j -bounds of the
domain of xj and similarly for xi . There are therefore only four costs to check in order to
compute the minimum cost.

From Proposition 5.2, we conclude that
Corollary 5.12 In a binary WCN, if all cost functions are convex, then the time complexity
of enforcing BAC∅ is O(n2 d) only.
One interesting example for a convex cost function is wij = max{xi − xj + cst, 0}. This
type of cost function, which can be efficiently filtered by BAC∅, may occur in temporal
reasoning problems and is also used in our RNA gene localization problem for specifying
preferred distances between elements of a gene.
608

Bounds Arc Consistency for Weighted CSPs

6. Comparison with Crisp Bounds Consistency
Petit et al. (2000) have proposed to transform WCNs into crisp constraint networks with
extra cost variables. In this transformation, every cost function is reified into a constraint,
which applies on the original cost function scope augmented by one extra variable representing the assignment cost. This reification of costs into domain variables transforms a WCN
in a crisp CN with more variables and augmented arities. As proposed by Petit et al., it
can be achieved using meta-constraints, i.e. logical operators applied to constraints. Given
this relation between WCNs and crisp CNs and the relation between BAC∅ and bounds
consistency, it is natural to wonder how BAC∅ enforcing relates to just enforcing bounds
consistency on the reified version of a WCN.
In this section we show that BAC∅ is in some precise sense stronger than enforcing
bounds consistency on the reified form. This is a natural consequence of the fact that the
domain filtering in BAC is based on the combined cost of several cost functions instead of
taking each constraint separately in bounds consistency. We first define the reification process precisely. We then show that BAC∅ can be stronger than the reified bounds consistency
on one example and conclude by proving that it can never be weaker.
The following example introduces the cost reification process.
Example 6.1 Consider the WCN in Figure 2(a). It contains two variables x1 and x2 , one
binary cost function w12 , and two unary cost functions w1 and w2 . For the sake of clarity,
every variable or constraint in the reified hard model, described on Figure 2(b), will be
indexed by the letter R.
First of all, we model every cost function by a hard constraint, and express that assigning
b to x1 yields a cost of 1. We create a new variable x1 C
R , the cost variable of w1 , that stores
the cost of any assignment of x1 . Then, we replace the unary cost function w1 by a binary
constraint c1R that involves x1 and x1 C
R , such that if a value v1 is assigned to x1 , then
x1 C
should
take
the
value
w
(v
).
We
do the same for the unary cost function w2 . The
1
1
R
idea is the same for the binary cost function w12 : we create a new variable x12 C
R , and we
replace w12 by a ternary constraint c12R , that makes sure that for any assignment of x1
and x2 to v1 and v2 respectively, x12 C
R takes the value w12 (v1 , v2 ). Finally, a global cost
C
constraint cR that states that the sum of the cost variables should be less than k is added:
C
C
x1 C
R + x2 R + x12 R < k. This completes the description of the reified cost hard constraint
network.
We can now define more formally the reification process of a WCN.
Definition 6.2 Consider the WCN P = hX , D, W, ki. Let reify(P) = hXR , DR , WR i be
the crisp CN such that:
• the set XR contains one variable xi R for every variable xi ∈ X , augmented with an
extra cost variable xS C
R per cost function wS ∈ W − {w∅}.
• the domains DR are:
– DR (xiR ) = D(xi ) for the xiR variables, with domain bounds lbiR and ubiR ,
C
C
– [lbS C
R , ubS R ] = [0, k − 1] for the xS R variables.

609

Zytnicki, Gaspin, de Givry & Schiex

x2 C
R

x1 C
R
x2R

x1R

0

0
a

a

b

b

1

1

2

2

k=3
x1

x2

a

0

1

a

b

1

0

b

x12 C
R
0

1

2

cC
R

(a) a small cost function
network

(b) the reified constraint network

Figure 2: A small cost function network and its reified counterpart.
• the set WR of constraints contains:
– cS R = {(t, wS (t)) : t ∈ ℓS , w∅ ⊕ wS (t) < k}, with scope S ∪ {xS C
R }, for every cost
function wS ∈ W,
P
C
– cC
wS ∈W xS R < k), an extra constraint that makes sure
R is defined as (w∅ ⊕
that the sum of the cost variables is strictly less than k.
It is simple to check that the problem reify(P) has a solution iff P has a solution and
the sum of the cost variables in a solution is the cost of the corresponding solution (defined
by the values of the xiR variables) in the original WCN.
Definition 6.3 Let P be a problem, ℓ and ℓ′ two local consistency properties. Let ℓ(P)
be the problem obtained after filtering P by ℓ. ℓ is said to be not weaker than ℓ′ iff ℓ′ (P)
emptiness implies ℓ(P) emptiness.
ℓ is said to be stronger than ℓ′ iff it is not weaker than ℓ′ , and if there exists a problem
P such that ℓ′ (P) is not empty but ℓ(P) is empty.
This definition is practically very significant since the emptiness of a filtered problem is
the event that generates backtracking in tree search algorithms used for solving CSP and
WCSP.
Example 6.4 Consider the WCN defined by three variables (x1 , x2 and x3 ) and two binary
cost functions (w12 and w13 ). D(x1 ) = {a, b, c, d}, D(x2 ) = D(x3 ) = {a, b, c} (we assume
that a ≺ b ≺ c ≺ d). The costs of the binary cost functions are described in Figure 3.
Assume that k = 2 and w∅ = 0.
One can check that the associated reified problem is already bounds consistent and
obviously not empty. For example, a support of the minimum bound of the domain of
x1 R w.r.t. c12 R is (a, a, 1), a support of its maximum bound is (d, a, 1). Supports of the
maximum and minimum bounds of the domain of x12 C
R w.r.t. c12R are (b, a, 0) and (a, a, 1)
respectively. Similarly, one can check that all other variable bounds are also supported on
all the constraints that involve them.
610

Bounds Arc Consistency for Weighted CSPs

a
a 1
(x2 ) b 1
c 1

(x1 )
b c
0 2
0 2
0 2

a
a 1
(x3 ) b 1
c 1

d
1
1
1

(x1 )
b c
2 0
2 0
2 0

d
1
1
1

Figure 3: Two cost matrices.
However, the original problem is not BAC since for example, the value a, the minimum
bound of the domain of x1 , does not satisfy the BAC property:
w∅ ⊕

X

wS ∈W,x1 ∈S



min

tS ∈ℓS ,a∈tS


wS (tS ) < k

This means that the value a can be deleted by BAC filtering. By symmetry, the same applies
to the maximum bound of x1 and ultimately, the problem inconsistency will be proved by
BAC. This shows that bounds consistency on the reified problem cannot be stronger than
BAC on the original problem.
We will now show that BAC∅ is actually stronger than bounds consistency applied
on the reified WCN. Because BAC∅ consistency implies non-emptiness (since it requires
the existence of assignments of cost 0 in every cost function) we will start from any BAC∅
consistent WCN P (therefore not empty) and prove that filtering the reified problem reify(P)
by bounds consistency does not lead to an empty problem.
Lemma 6.5 Let P be a BAC∅ consistent binary WCN. Then filtering reify(P) by bounds
consistency does not produce an empty problem.
Proof: We will prove here that bounds consistency will just reduce the maximum bounds
of the domains of the cost variables xS C
R to a non empty set and leave all other domains
unchanged.
More precisely, the final domain of xS C
R will become [0, max{wS (t) : t ∈ ℓS , w∅ ⊕wS (t) <
k}]. Note that this interval is not empty because the network is BAC∅ consistent which
means that every cost function has an assignment of cost 0 (by ∅-IC) and w∅ < k (or else
the bounds of the domains could not have supports and the problem would not be BAC).
To prove that bounds consistency will not reduce the problem by more than this, we
simply prove that the problem defined by these domain reductions only is actually bounds
consistent.
All the bounds consistency required properties apply to the bounds of the domains of the
variables of reify(P). Let us consider every type of variable in this reified reduced problem:
• reified variables xiR . Without loss of generality, assume that the minimum bound lbiR
of xiR is not bounds consistent (the symmetrical reasoning applies to the maximum
bound). This means it would have no support with respect to a given reified constraint
611

Zytnicki, Gaspin, de Givry & Schiex

cS R , xi ∈ S. However, by BAC, we have
w∅ ⊕

min

t∈ℓS ,lbiR ∈t

wS (t) < k

and so ∃t ∈ ℓS , lbiR ∈ t,

wS (t) ≤ max{wS (t) : t ∈ ℓS , w∅ ⊕ wS (t) < k}

which means that lbi R is supported w.r.t. cS R .
• cost variables. The minimum bound of all cost variables are always bounds consistent
w.r.t. the global constraint cC
R because this constraint is a “less than” inequality.
Moreover, since the minimum bounds of the cost variables are set to 0, they are also
consistent w.r.t. the reified constraints, by the definition of ∅-inverse consistency.
Consider the maximum bound ubS C
R of a cost variable in the reduced reified problem.
Remember it is defined as max{wS (t) : t ∈ ℓS , w∅ ⊕wS (t) < k}, and so w∅ ⊕ubS C
R < k.
The minimum bounds of all other cost variables in the reified problem, which are 0,
C
C
form a support of ubS C
R w.r.t. the global constraint cR . So ubS R cannot be removed
by bounds consistency.

We will now prove the final assertion:
Proposition 6.6 BAC∅ is stronger than bounds consistency.
Proof: Lemma 6.5 shows that BAC∅ is not weaker than bounds consistency. Then,
example 6.4 is an instance where BAC, and therefore BAC∅ is actually stronger than
bounds consistency after reification.

A filtering related to BAC∅ could be achieved in the reified approach by an extra shaving
process where each variable is assigned to one of its domain bounds and this bound is deleted
if an inconsistency is found after enforcing bounds consistency (Lhomme, 1993).

7. Other Related Works
The Definition 3.1 of BAC is closely related to the notion of arc consistency counts introduced by Freuder and Wallace (1992) for Max-CSP processing. The Max-CSP can be seen
as a very simplified form of WCN where cost functions only generate costs of 0 or 1 (when
the associated constraint is violated). Our definition of BAC can be seen as an extension of
AC counts allowing dealing with arbitrary cost functions, including the usage of w∅ and k,
and applied only to domain bounds as in bounds consistency. The addition of ∅-IC makes
BAC∅ more powerful.
Dealing with large domains in Max-CSP has also been considered in the Range-Based
Algorithm, again designed for Max-CSP by Petit, Régin, and Bessière (2002). This algorithm uses reversible directed arc consistency (DAC) counts and exploits the fact that
in Max-CSP, several successive values in a domain may have the same DAC counts. The
algorithm intimately relies on the fact that the problem is a Max-CSP problem, defined
by a set of constraints and actively uses bounds consistency dedicated propagators for the
constraints in the Max-CSP. In this case the number of different values reachable by the
DAC counters of a variable is bounded by the degree of the variable, which can be much
612

Bounds Arc Consistency for Weighted CSPs

smaller than the domain size. Handling intervals of values with a same DAC cost as one
value allows space and time savings. For arbitrary binary cost functions, the translation
into constraints could generate up to d2 constraints for a single cost function and makes the
scheme totally impractical.
Several alternative definition of bounds consistency exist in crisp CSPs (Choi et al.,
2006). Our extension to WCSP is based on bounds(D) or bounds(Z) consistencies (which
are equivalent on intervals). For numerical domains, another possible weaker definition
of bounds consistency is bounds(R) consistency, which is obtained by a relaxation to real
numbers. It has been shown by Choi et al. that bounds(R) consistency can be checked
in polynomial time on some constraints whereas bounds(D) or bounds(Z) is NP-hard (eg.
for linear equality). The use of this relaxed version in the WCSP context together with
intentional description of cost functions would have the side effect of extending the cost
domain from integer to real numbers. Because extensional or algorithmical description of
integer cost functions is more general and frequent in our problems, this possibility was
not considered. Since cost comparison is the fundamental mechanism used for pruning in
WCSP, a shift to real numbers for costs would require a safe floating number implementation
both in the local consistency enforcing algorithms and in the branch and bound algorithm.

8. Experimental Results
We experimented bounds arc consistency on two benchmarks translated into weighted CSPs.
The first benchmark is from AI planning and scheduling. It is a mission management
benchmark for agile satellites (Verfaillie & Lemaı̂tre, 2001; de Givry & Jeannin, 2006). The
maximum domain size of the temporal variables is 201. This reasonable size and the fact
that there are only binary cost functions allows us to compare BAC∅ with strong local
consistencies such as EDAC*. Additionally, this benchmark has also been modeled using
the reified version of WCN, thus allowing for an experimental counterpart of the theoretical
comparison of Section 6.
The second benchmark comes from bioinformatics and models the problem of the localization of non-coding RNA molecules in genomes (Thébault et al., 2006; Zytnicki et al.,
2008). Our aim here is mostly to confirm that bounds arc consistency is useful and practical
on a real complex problem with huge domains, which can reach several millions.
8.1 A Mission Management Benchmark for Agile Satellites
We solved a simplified version described by de Givry and Jeannin (2006) of a problem of
selecting and scheduling earth observations for agile satellites. A complete description of
the problem is given by Verfaillie and Lemaı̂tre (2001). The satellite has a pool of candidate
photographs to take. It must select and schedule a subset of them on each pass above a
certain strip of territory. The satellite can only take one photograph at a time (disjunctive
scheduling). A photograph can only be taken during a time window that depends on the
location photographed. Minimal repositioning times are required between two consecutive
photographs. All physical constraints (time windows and repositioning times) must be
met, and the sum of the revenues of the selected photographs must be maximized. This is
equivalent to minimizing the “rejected revenues” of the non selected photographs.
613

Zytnicki, Gaspin, de Givry & Schiex

Let N be the number of candidate photographs. We define N decision variables representing the acquisition starting times of the candidate photographs. The domain of each
variable is defined by the time window of its corresponding photograph plus an extra domain
value which represents the fact that the photograph is not selected. As proposed by de Givry
and Jeannin (2006), we create a binary hard constraint for every pair of photographs (resulting in a complete constraint graph) which enforces the minimal repositioning times if both
photographs are selected (represented by a disjunctive constraint). For each photograph, a
unary cost function associates its rejected revenue to the corresponding extra value.
In order to have a better filtering, we moved costs from unary cost functions inside
the binary hard constraints in a preprocessing step. This allows bounds arc consistency
filtering to exploit the revenue information and the repositioning times jointly, possibly
increasing w∅ and the starting times of some photographs. To achieve this, for each variable
xi , the unary cost function wi is successively combined (using ⊕) with each binary hard
′ defined
constraint wij that involves xi . This yields N − 1 new binary cost functions wij
′
as wij (t) = wij (t) ⊕ wi (t[xi ]), having both hard (+∞) and soft weights. These binary
′ replace the unary cost function w and the N − 1 original binary hard
cost functions wij
i
constraints wij . Notice that this transformation has the side effect of multiplying all soft
weights by N − 1. This does preserve the equivalence with the original problem since all
finite weights are just multiplied by the same constant (N − 1).
The search procedure is an exact depth-first branch-and-bound dedicated to scheduling
problems, using a schedule or postpone strategy as described by de Givry and Jeannin (2006)
which avoids the enumeration of all possible starting time values. No initial upper bound
was provided (k = +∞).
We generated 100 random instances for different numbers of candidate photographs
(N varying from 10 to 30)2 . We compared BAC∅ (denoted by BAC0 in the experimental
results) with EDAC* (Heras et al., 2005) (denoted by EDAC*). Note that FDAC* and VAC
(applied in preprocessing and during search, in addition to EDAC*) were also tested on
these instances, but did not improve over EDAC* (FDAC* was slightly faster than EDAC*
but developed more search nodes and VAC was significantly slower than EDAC*, without
improving w∅ in preprocessing). OSAC is not practical on this benchmark (for N = 20,
it has to solve a linear problem with 50, 000 variables and about 4 million constraints).
All the algorithms are using the same search procedure. They are implemented in the
toulbar2 C++ solver3 . Finding the minimum cost of the previously-described binary cost
functions (which are convex if we consider the extra domain values for rejected photographs
separately), is done in constant time for BAC∅. It is done in time O(d2 ) for EDAC*
(d = 201).
We also report the results obtained by maintaining bounds consistency on the reified
problem using meta-constraints as described by de Givry and Jeannin (2006), using the
claire/Eclair C++ constraint programming solver (de Givry, Jeannin, Josset, Mattioli,
Museux, & Savéant, 2002) developed by THALES (denoted by B-consistency).
The results are presented in Figure 4, using a log-scale. These results were obtained on
a 3 GHz Intel Xeon with 4 GB of RAM. Figure 4 shows the mean CPU time in seconds and
the mean number of backtracks performed by the search procedure to find the optimum
2. These instances are available at http://www.inra.fr/mia/ftp/T/bep/.
3. See http://carlit.toulouse.inra.fr/cgi-bin/awki.cgi/ToolBarIntro.

614

Bounds Arc Consistency for Weighted CSPs

Satellite benchmark
10000

EDAC*
B consistency
BAC0

Cpu time in seconds

1000
100
10
1
0.1
0.01
0.001
1e-04
10

15
20
25
Number of candidate photographs

30

Satellite benchmark
1e+08

B consistency
BAC0
EDAC*

Number of backtracks

1e+07
1e+06
100000
10000
1000
100
10
10

15

20

25

30

Number of candidate photographs

Figure 4: Comparing various local consistencies on a satellite benchmark. Cpu-time (top)
and number of backtracks (bottom) are given.

615

Zytnicki, Gaspin, de Givry & Schiex

and prove optimality as the problem size increases. In the legends, algorithms are sorted
by increasing efficiency.
The analysis of experimental results shows that BAC∅ was up to 35 times faster than
EDAC* while doing only 25% more backtracks than EDAC* (for N = 30, no backtrack results are reported as EDAC* does not solve any instance within the time limit of 6 hours).
It shows that bounds arc consistency can prune almost as many search nodes as a stronger
local consistency does in much less time for temporal reasoning problems where the semantic of the cost functions can be exploited, as explained in Section 5. The second fastest
approach was bounds consistency on the reified representation which was at least 2.3 worse
than BAC∅ in terms of speed and number of backtracks when N ≥ 25. This is a practical confirmation of the comparison of Section 6. The reified approach used with bounds
consistency introduces Boolean decision variables for representing photograph selection and
uses a criteria defined as a linear function of these variables. Contrarily to BAC∅, bounds
consistency is by definition unable to reason simultaneously on the combination of several
constraints to prune the starting times.
8.2 Non-coding RNA Gene Localization
A non-coding RNA (ncRNA) gene is a functional molecule composed of smaller molecules,
called nucleotides, linked together by covalent bonds. There are four types of these nucleotides, commonly identified by a single letter: A, U, G and C. Thus, an RNA can be
represented as a word built from the four letters. This sequence defines what is called the
primary structure of the RNA molecule.
RNA molecules have the ability to fold back on themselves by developing interactions
between nucleotides, forming pairs. The most frequently interacting pairs are: a G interacts
with a C, or a U interacts with an A. A sequence of such interactions forms a structure
called a helix. Helices are a fundamental structural element in ncRNA genes and are the
basis for more complex structures. The set of interactions is often displayed by a graph
where vertices represent nucleotides and edges represent either covalent bonds linking successive nucleotides (represented as plain lines in Figure 5) or interacting nucleotide pairs
(represented as dotted lines). This representation is usually called the molecule’s secondary
structure. See the graph of a helix in Figure 5(a).
The set of ncRNAs that have a common biological function is called a family. The
signature of a gene family is the set of conserved elements either in the sequence or the
secondary structure. It can be expressed as a collection of properties that must be satisfied
by a set of regions occurring on a sequence. Given the signature of a family, the problem we
are interested in involves searching for new members of a gene family in existing genomes,
where these members are in fact the set of regions appearing in the genome which satisfy the
signature properties. Genomic sequences are themselves long texts composed of nucleotides.
They can be thousand of nucleotides long for the simplest organisms up to several hundred
million nucleotides for the more complex ones. The problem of searching for an occurrence
of a gene signature in a genomic sequence is NP-complete for complex combinations of helix
structures (Vialette, 2004).
In order to find ncRNAs, we can build a weighted constraint network that scans a
genome, and detects the regions of the genome where the signature elements are present
616

Bounds Arc Consistency for Weighted CSPs

A U A
C

A G C G U C

G

U C G C A G

helix

C
G A U

A G A C UA U U
xi
xj

loop

(cost: 1)
(b)
pattern(xi , xj , ACGUA)
cost function.

(a) An helix with its loop.

G

xi

U C

U A G

A G

xl

cost

xj

A G C

k

xk
(cost: 2)

(c) The helix(xi , xj , xk , xl , 6) cost
function.

The

0

xj − xi
d1 d2

d3

d4

(d)
The
cost
profile
spacer(xi , xj , d1 , d2 , d3 , d4 )
cost
tion.

of
func-

Figure 5: Examples of signature elements with their cost functions.
and correctly positioned. The variables are the positions of the signature elements in the
sequence. The size of the domains is the size of the genomic sequence. Cost functions
enforce the presence of the signature elements between the positions taken by the variables
involved. Examples of cost functions are given in Figure 5.
• The pattern(xi , xj , p) function states that a fixed word p, given as parameter, should
be found between the positions indicated by the variables xi and xj . The cost given
by the function is the edit distance between the word found at xi :xj and the word p
(see the cost function pattern with the word ACGUA in Figure 5(b)).
• The helix(xi , xj , xk , xl , m) function states that the nucleotides between positions xi
and xj should be able to bind with the nucleotides between xk and xl . Parameter
m specifies a minimum helix length. The cost given is the number of mismatches or
nucleotides left unmatched (see the helix function with 5 interacting nucleotide pairs
in Figure 5(c)).
• Finally, the function, spacer(xi , xj , d1 , d2 , d3 , d4 ) specifies a favorite range of distances
between positions xi and xj using a trapezoidal cost function as shown in Figure 5(d).
See the work of Zytnicki et al. (2008) for a complete description of the cost functions.
Because of the sheer domain size, and given that the complex pattern matching oriented
cost functions do not have any specific property that could speedup filtering, BAC alone
has been used for filtering these cost functions (Zytnicki et al., 2008). The exception is the
617

Zytnicki, Gaspin, de Givry & Schiex

piecewise linear spacer cost function: its minimum can be computed in constant time for
BAC∅ enforcement. The resulting C++ solver is called DARN!4 .
Size
# of solutions
AC*
Time
# of backtracks
BAC
Time (sec.)
# of backtracks

10k
32

50k
33

100k
33

500k
33

1M
41

4.9M
274

1hour 25min.
93

44 hours
101

-

-

-

-

0.016
93

0.036
101

0.064
102

0.25
137

0.50
223

2.58
1159

Table 1: Searching all the solutions of a tRNA motif in Escherichia coli genome.
A typical benchmark for the ncRNA localization problem is the transfer RNA (tRNA)
localization. The tRNA signature (Gautheret, Major, & Cedergren, 1990) can be modelled
by 22 variables, 3 nucleotide words, 4 helices, and 7 spacers. DARN! searched for all the
solutions with a cost strictly lower than the maximum cost k = 3. Just to illustrate the
absolute necessity of using bounds arc consistency in this problem, we compared bounds
arc consistency enforcement with AC* (Larrosa, 2002) on sub-sequences of the genome of
Escherichia coli, which is 4.9 million nucleotides long. Because of their identical space
complexity and because they have not been defined nor implemented on non-binary cost
functions (helix is a quaternarycost function), DAC, FDAC or EDAC have not been tested
(see the work of Sànchez et al., 2008, however for an extension of FDAC to ternary cost
functions).
The results are displayed in Table 1. For different beginning sub-sequences of the complete sequence, we report the size of the sub-sequence in which the signature is searched
for (10k is a sequence of 10,000 nucleotides), as well as the number of solutions found.
We also show the number of backtracks and the time spent on a 3 GHz Intel Xeon with
2 GB. A “-” means the instance could not be solved due to memory reasons, and despite
memory optimizations. BAC solved the complete sequence in less than 3 seconds. BAC
is approximately 300, 000 (resp. 4, 400, 000) times faster than AC* for the 10k (resp. 50k)
sub-sequence. More results on other genomes and ncRNA signatures can be found in the
work of Zytnicki et al. (2008).
The reason of the superiority of BAC over AC* is twofold. First, AC* needs to store all
the unary costs for every variable and projects costs from binary cost functions to unary
cost functions. Thus, the space complexity of AC* is at least O(nd). For very large domains
(in our experiments, greater than 100,000 values), the computer cannot allocate a sufficient
memory and the program is aborted. For the same kind of projection, BAC only needs to
store the costs of the bounds of the domains, leading to a space complexity of O(n).
Second, BAC does not care about the interior values and focuses on the bounds of the
domains only. On the other hand, AC* projects all the binary costs to all the interior values,
4. DARN!,
and
several
genomic
http://carlit.toulouse.inra.fr/Darn/.

sequences

618

and

family

signatures

are

available

at

Bounds Arc Consistency for Weighted CSPs

which takes a lot of time, but should remove more values and detect inconsistencies earlier.
However, Table 1 shows that the number of backtracks performed by AC* and BAC are
the same. This can be explained as follows. Due to the nature of the cost functions used in
these problems, the supports of the bounds of the domains of the variables usually are the
bounds of the other variables. Thus, removing the values which are inside the domains, as
AC* does, do not help removing the bounds of the variables. As a consequence, the bounds
founds by BAC are the same as those found by AC*. This explains why enforcing of AC*
generally does not lead to new domain wipe out compared to BAC, and finding the support
inside the bounds of the domains is useless.
Notice that the spacer cost functions dramatically reduce the size of the domains. When
a single variable is assigned, all the other domain sizes are dramatically reduced, and the
instance becomes quickly tractable. Moreover, the helix constraint has the extra knowledge
of a maximum distance djk between its variables xj and xk (see Fig. 5(c)) which bounds the
time complexity of finding the minimum cost w.r.t. djk and not the length of the sequence.

9. Conclusions and Future Work
We have presented here new local consistencies for weighted CSPs dedicated to large domains as well as algorithms to enforce these properties. The first local consistency, BAC,
has a time complexity which can be easily reduced if the semantics of the cost function
is appropriate. A possible enhancement of this property, ∅-IC, has also been presented.
Our experiments showed that maintaining bounds arc consistency is much better than AC*
for problems with large domains, such as ncRNA localization and scheduling for Earth observation satellites. This is due to the fact that AC* cannot handle problems with large
domains, especially because of its high memory complexity, but also because BAC∅ behaves
particularly well with specific classes of cost functions.
Similarly to bounds consistency, which is implemented on almost all state-of-the-art
CSP solvers, this new local property has been implemented in the open source toulbar2
WCSP solver.5
BAC, BAC∅ and ∅-inverse consistency allowed us to transfer bounds consistency CSP to
weighted CSP, including improved propagation for specific classes of binary cost functions.
Our implementation for RNA gene finding is also able to filter non-binary constraints. It
would therefore be quite natural to try to define efficient algorithms for enforcing BAC,
BAC∅ or ∅-inverse consistency on specific cost functions of arbitrary arity such as the soft
global constraints derived from All-Diff, GCC or regular (Régin, 1994; Van Hoeve, Pesant,
& Rousseau, 2006). This line of research has been recently explored by Lee and Leung
(2009).
Finally, another interesting extension of this work would be to better exploit the connection between BAC and bounds consistency by exploiting the idea of Virtual Arc Consistency
introduced by Cooper et al. (2008). The connection established by Virtual AC between crisp
CNs and WCNs is much finer grained than in the reification approach considered by Petit
et al. (2000) and could provide strong practical and theoretical results.
5. Available at http://carlit.toulouse.inra.fr/cgi-bin/awki.cgi/ToolBarIntro.

619

Zytnicki, Gaspin, de Givry & Schiex



There exist several architectures to solve influence diagrams using local computations,
such as the Shenoy-Shafer, the HUGIN, or
the Lazy Propagation architectures. They all
extend usual variable elimination algorithms
thanks to the use of so-called “potentials”.
In this paper, we introduce a new architecture, called the Multi-operator Cluster DAG
architecture, which can produce decompositions with an improved constrained inducedwidth, and therefore induce potentially exponential gains. Its principle is to benefit from
the composite nature of influence diagrams,
instead of using uniform potentials, in order
to better analyze the problem structure.

1

INTRODUCTION

Since the first algorithms based on decision trees or
arc-reversal operations [Shachter, 1986], several exact
methods have been proposed to solve influence diagrams using local computations, such as the ones based
on the Shenoy-Shafer, the HUGIN, or the Lazy Propagation architectures [Shenoy, 1992; Jensen et al., 1994;
Madsen and Jensen, 1999]. These methods have successfully adapted classical Variable Elimination (VE)
techniques (which are basically designed to compute
one type of marginalization on a combination of local
functions with only one type of combination operator), in order to handle the multiple types of information (probabilities and utilities), the multiple types
of marginalizations (sum and max), and the multiple
types of combination (× for probabilities, + for utilities) involved in an influence diagram. The key mechanism used for such an extension consists in using elements known as potentials [Ndilikilikesha, 1994].
In this paper, we define a new architecture, called the
Multi-operator Cluster DAG (MCDAG) architecture,

Gérard Verfaillie
ONERA
Toulouse, France

which does not use potentials, but still relies on VE.
Compared to existing schemes, MCDAGs actively exploit the composite nature of influence diagrams. We
first present the potential-based approach and motivate the need for a new architecture (Section 2). Then,
MCDAGs are introduced (Section 3) and a VE algorithm is defined (Section 4). Finally, this work is
compared with existing approaches (Section 5) and extended to other frameworks (Section 6). All proofs are
available in [Pralet et al., 2006b].

2

MOTIVATIONS

Notations and definitions An influence diagram [Howard and Matheson, 1984] is a composite
graphical model defined on three sets of variables organized in a Directed Acyclic Graph (DAG) G: (1) a set
C of chance variables x ∈ C, for each of which a conditional probability distribution Px | pa(x) on x given its
parents in G is specified; (2) a set D = {D1 , . . . , Dq }
(indices represent the order in which decisions are
made) of decision variables x ∈ D, for each of which
pa(x) is the set of variables observed before decision
x is made; (3) a set Γ of utility variables u ∈ Γ, each
of which is associated with a utility function Upa(u) on
pa(u) (and utility variables are leaves in the DAG).
We consider influence diagrams where the parents of a
decision variable are parents of all subsequent decision
variables (no-forgetting). The set of conditional probability distributions (one for each x ∈ C) is denoted
P and the set of utility functions (one for each u ∈ Γ)
is denoted U . Each function φ ∈ P ∪ U holds on a set
of variables sc(φ) called its scope, and is consequently
called a scoped function (sc(Px | pa(x) ) = {x} ∪ pa(x)
and sc(Upa(u) ) = pa(u)). The set of chance variables
observed before the first decision is denoted I0 , the
set of chance variables observed between decisions Dk
and Dk+1 is denoted Ik , and the set of chance variables unobserved before the last decision is denoted
Iq . We use dom(x) to denote the domain of a vari-

able x ∈ C Q
∪ D, and by extension, for W ⊂ C ∪ D,
dom(W ) = x∈W dom(x).

The usual problem associated with an influence diagram is to find decision rules maximizing the expected
utility (a decision rule for a decision Dk is a function
associating a value in dom(Dk ) with any assignment
of the variables observed before making decision Dk )
As shown in [Jensen et al., 1994], this is equivalent to
computing optimal decision rules for the quantity
!
!!
Y
X
X
X
X
(1)
Pi ×
Ui
max
max . . .
I0

2.1

D1

Iq−1

Dq

Iq

Pi ∈P

Ui ∈U

THE “POTENTIAL” APPROACH

With this approach, Equation 1 is reformulated using
so-called potentials in order to use only one combination and one marginalization operator. A potential on
a set of variables W is a pair πW = (pW , uW ), where
pW and uW are respectively a nonnegative real function and a real function, whose scopes are included
in W . The initial conditional probability distributions
Pi ∈ P are transformed into potentials (Pi , 0), whereas
the initial utility functions Ui ∈ U are transformed into
potentials (1, Ui ). On these potentials, a combination
operation ⊗ and a marginalization (or elimination)
operation ↑ are defined:
• the combination of πW1 = (pW1 , uW1 ) and πW2 =
(pW2 , uW2 ) is the potential on W1 ∪ W2 given by
πW1 ⊗ πW2 = (pW1 × pW2 , uW1 + uW2 );
• the marginalization of πW = (pP
W , uW ) over
 W1 ⊂
P
↑W1
W 1 pW u W
P
(with
C equals πW =
W1 p W ,
pW
W1

the convention 0/0 = 0), whereas the marginalization of πW = (pW , uW ) over W1 ⊂ D is given
↑W1
by πW
= (pW , maxW1 uW ).

Solving the problem associated with an influence
diagram is then equivalent to computing β =
((· · · ((πC∪D ↑Iq )↑Dq )↑Iq−1 · · · )↑D1 )↑I0 , where πC∪D =
(⊗Pi ∈P (Pi , 0)) ⊗ (⊗Ui ∈U (1, Ui )) is the combination of
the initial potentials. As ⊗ and ↑ satisfy the ShenoyShafer axioms defined in [Shenoy, 1990], β can be computed using usual VE algorithms [Jensen et al., 1994].
This explains why existing architectures like ShenoyShafer, HUGIN, or Lazy Propagation (LP1 ) use potentials to solve influence diagrams.
2.2

QUANTIFYING THE COMPLEXITY

In the case of influence diagrams, the alternation of
sum and max marginalizations, which do not gener1

The LP architecture actually uses potentials defined as
pairs of set of functions (instead of pairs of functions).

ally commute, prevents from eliminating variables in
any order. The complexity of VE can then be quantified using constrained induced-width [Jensen et al.,
1994; Park and Darwiche, 2004] (instead of inducedwidth [Dechter and Fattah, 2001]).
Definition 1. Let G = (VG , HG ) be a hypergraph2
and let  be a partial order on VG . The constrained
induced-width of G with constraints on the elimination
order given by  (“x ≺ y” stands for “y must be eliminated before x”) is a parameter denoted wG (). It is
defined as wG () = mino∈lin() wG (o), lin() being
the set of linearizations of  to a total order on VG and
wG (o) being the induced-width of G for the elimination
order o (i.e. the size of the largest hyperedge created
when eliminating variables in the order given by o).
The constrained induced-width can be used to give an
upper bound on the complexity of existing potentialbased VE algorithms. Let Gp = (C ∪ D, {sc(φ)|φ ∈
P ∪ U }) be the hypergraph corresponding to the “untyped” influence diagram. Let p be the partial order
defined by I0 ≺p D1 , (Ik 6= ∅) → (Dk ≺p Ik ≺p Dk+1 ),
and Dq ≺p Iq . Finally, let d be the maximum size of
the variables domains. Then, with classical approaches
based on potentials and strong junction trees [Jensen
et al., 1994], which are junction trees with constraints
on the marginalization order, the theoretical complexity is O(|P ∪ U | · d1+wGp (p ) ) (the number of elements
of a finite set E is denoted |E|).
2.3

DECREASING THE CONSTRAINED
INDUCED-WIDTH

The constrained-induced width is a guideline to show
how the complexity can be decreased. In this direction, one can work on the two parameters on which it
depends: the partial order , and the hypergraph G.
Weakening the partial order 
Proposition 1. Let G = (VG , HG ) be a hypergraph
and let 1 , 2 be two partial orders on VG such that
∀(x, y) ∈ VG × VG , (x 2 y) → (x 1 y) (2 is weaker
than 1 ). Then, wG (1 ) ≥ wG (2 ).
Proposition 1 means that if one weakens , i.e. if one
reveals some extra freedoms in the elimination order
(e.g. by proving that some marginalizations with sum
and max can commute), then the theoretical complexity may decrease. Though such a technique is known
to be useless in contexts like Maximum A Posteriori
hypothesis [Park and Darwiche, 2004], where there is
only one alternation of max and sum marginalizations,
2

This means that VG is the set of variables (or vertices),
and HG is a set of hyperedges on VG , i.e. a subset of 2VG .

it can lead to an exponential gain as soon as there are
more than two levels of alternation.
Indeed, assume
that one wants
P to compute
P
maxx1 ,...,xn y maxxn+1 Py (Ux1 ,y + 1≤i≤n Uxi ,xn+1 ).
On one hand, using 1 defined by {x1 , . . . , xn } ≺1
y ≺1 xn+1 provides us with the constrained inducedwidth wG (1 ) = n, since xn+1 is then necessarily
eliminated first. On the other hand, the scopes of
the functions involved enable us to infer that with 2
defined by x1 ≺2 y, one is guaranteed to compute the
same value, since y is “linked” only with x1 . The constrained induced-width is then wG (2 ) = 1, e.g. with
the elimination order x1 ≺ y ≺ xn+1 ≺ xn ≺ . . . ≺ x2 .
Therefore, the theoretical complexity decreases from
O((n + 2) · dn+1 ) to O((n + 2) · d2 ), thanks to the
weakening of the partial order (the (n + 2) factor
corresponds to the number of scoped functions).
Working on the hypergraph The second
possible mechanism is to work on the hypergraph G, either by eliminating
so-called “barren”
P
variables (computing
x Px | pa(x) is useless because of normalization), or by better decomposing
the problem.
To illustrate the latter, P
assume
that one wants to compute maxx1 ,...,xn y Py ·
(Uy,x1 + · · · + Uy,xn ). The basic hypergraph G1 =
({x1 , . . . , xn , y}, {{y, x1}, . . . , {y, xn }}), together with
1 defined by {x1 , . . . , xn } ≺1 y, gives a theoretical
complexity O((n + 1) · dwG1 (1 )+1 ) = O((n + 1) · dn+1 ).
However, one can write:
P
maxx1 ,...,xn y Py · (Uy,x1 + · · · + Uy,xn )
P
P
= (maxx1 y Py · Uy,x1 ) + · · · + (maxxn y Py · Uy,xn )
Thus, an implicit duplication of y makes the complexity decrease to O((n + 1)d2 ) = O((n + 1)d1+wG2 (2 ) ),
where G2 is the hypergraph defined by the variables {x1 , . . . , xn , y (1) , . . . , y (n) } and by the hyperedges
{{x1 , y (1) }, . . . , {xn , y (n) }}, and where 2 is given by
x1 ≺2 y (1) , . .P
. , xn ≺2 y (n) . This
P method,Pwhich uses
the property S (U1 + U2 ) = ( S U1P
) +( S U2 ), duplicates variables “quantified” with , so that computations become more local. Proposition 2 shows the
possible exponential gain obtained by duplication.
Proposition 2. Let φx,Si be a scoped function of
scope {x} ∪ Si P
for any i ∈ [1, m]. The direct
computation of
x (φx,S1 + · · · + φx,Sm ) always requires
more
sums
than
P the direct computation of
P
·
·
·
+
(
( x φx,S1 ) +
x φx,Sm ). Moreover, the comP
putation of x (φx,S1 + · · · + φx,Sm ) results in a complexity O(m · d1+|S1 ∪...∪Sm | ), whereas
P the computation
of the m quantities in the set { x φx,Si | 1 ≤ i ≤ m}
results in a complexity O(m · d1+maxi∈[1,m] |Si | ).
Why not use potentials? Though weakening the
constraints on the elimination order could be done

with potentials, the duplication mechanism cannot
be used if potentials are. Indeed, one cannot write
↑W3
↑W3
(πW1 ⊗ πW2 )↑W3 = (πW
) ⊗ (πW
) even if W3 ⊂ C.
1
2
The duplication mechanism has actually already been
proposed in the influence diagram litterature [Dechter,
2000] where it was applied ”on the fly” during elimination. In this paper, the duplication is exploited in a
global preliminary analysis which may reveal new degrees of freedom in the elimination order, in synergism
with the application of other mechanisms. The new architecture we introduce, which does not use potentials
to solve influence diagrams, is called the Multi-operator
Cluster DAG (MCDAG) architecture.

3
3.1

THE MCDAG ARCHITECTURE
MACROSTRUCTURING AN
INFLUENCE DIAGRAM

The first step to build the MCDAG architecture is to
analyze the macrostructure of the influence diagram,
by detecting the possible reordering freedoms in the
elimination order, while using the duplication technique and the normalization conditions on conditional
probability distributions. This macrostructure is represented with a DAG of computation nodes.
Definition 2. An atomic computation node n is a
scoped function φ in P ∪ U . In this case, the value of
n is val(n) = φ, and its scope is sc(n) = sc(φ). A computation node is either an atomic computation node or
a triple n = (Sov, ~, N ), where Sov is a sequence of
operator-variables pairs, ~ is an associative and commutative operator with an identity, and where N is a
set of computation nodes. In the latter, the value of n
is given by val(n) = Sov (~n0 ∈N val(n0 )), and its scope
is given by sc(n) = (∪n0 ∈N sc(n0 )) − {x | opx ∈ Sov}.
Informally, a computation node (Sov, ~, N ) defines a
sequence of marginalizations on a combination of computation nodes with a specific operator. It can be
represented as in Figure 1. Given a set of computation nodes N , we define N +x (resp. N −x ) as the
set of nodes of N whose scope contains x (resp. does
not contain x): N +x = {n ∈ N | x ∈ sc(n)} (resp.
N −x = {n ∈ N | x ∈
/ sc(n)}).
Sov ~
n1

n2

φ1 φ2

φk
nl

Figure 1: A computation node (Sov, ~, N ), where
{φ1 , . . . , φk } (resp. {n1 , . . . , nl }) is the set of atomic
(resp. non-atomic) computation nodes in N .

3.1.1

From influence diagrams to
computation nodes

Without loss of generality, we assume that U 6= ∅ (if
this is not the case, one can add U0 = 0 to U ).

maxd
P
∅ × Prr1|r Ud,r1
2 1

3.1.2

P
Rewriting rules for
x When a sum-marginalization must be performed, a decomposition rule DΣ ,
a recomposition rule RΣ , and two simplification rules
2
1
are used. These are illustrated in Figure 2,
and SΣ
SΣ
which corresponds to the influence diagram example
introduced in 3.1.1.
P
DΣ
(Sov. x , +, {(∅, ×, N ) , N ∈ N})



P
+x
,N ∈ N
Sov, +, ∅, ×, N −x ∪
x , ×, N
RΣ [ Prec.: (S 0 ∩ (S ∪ sc(N1 )) = ∅) ∧ (N1 ∩ N2 = ∅)]
P
P
P
( S∪S 0 , ×, N1 ∪ N2 )
( S , ×, N1 ∪ {( S 0 , ×, N2 )})
1
SΣ
[ Prec.: x ∈
/ S ∪ sc(N ) ]

P
P
( {x}∪S , ×, N ∪ Px | pa(x) )
( S , ×, N )
 P
 
2
∅, ×, N ∪
SΣ
(∅, ×, N )
∅ , ×, ∅

Example In the example of Figure 2, the first rule to
be applied is the decomposition rule DΣ , which treats

P
∅ × Prr1|r Ud,r2
2 1

maxd

P

P

P
∅ × Prr1|r Ud
2 1

+

r2

∅ × Ud,r2

∅ ×
P
× Prr1|r Ud,r1
2 1

r1

P

∅ ×

r1

Ud

P
× Prr1|r
2 1

DΣ
maxd

Macrostructuring the initial node

In order to exhibit the macrostructure of the influence
diagram, we analyze the sequence of computations performed by n0 . To do so, we successively consider the
eliminations in Sov0 from the right to the left and use
three types of rewriting rules, preserving nodes values, to make the macrostructure explicit: (1) decomposition rules, which decompose the structure using
namely the duplication technique; (2) recomposition
rules, which reveal freedoms in the elimination order;
(3) simplification rules, which remove useless computations from the architecture, by using normalization
conditions. Rewriting rules are presented first for the
case of sum-marginalizations, and then for the case of
max-marginalizations. A rewriting rule may be preceded by preconditions restricting its applicability.

+

r2 ,r1

DΣ

Proposition P
3. Let Sov0 P
be the initial sequence
P
I0 maxD1 . . .
Iq−1 maxDq
Iq of operator-variables
pairs defined by the influence diagram. The value of
Equation 1 is equal to the value of the computation
node n0 = (Sov0 , +, {(∅, ×, P ∪ {Ui }), Ui ∈ U }).
For the influence
P diagram associated with the computation of maxd r2 ,r1 Pr1 ·Pr2 |r1 ·(Ud,r1 +Ud,r2 +Ud), n0
corresponds to the first computation node in Figure 2.

P

∅ ×

P

+
∅ ×

∅ ×

×

P

r2

r1

P
× Prr1|r Ud,r1
2 1

P

r2

× Ud,r2
P

r1

P

r2

Ud
×

P
× Prr1|r
2 1

RΣ
maxd
∅ ×
P

r1 ,r2 ×

+
∅ ×

∅ ×

Ud

P r1
Pr1 Ud,r1 P
Pr1 Ud,r2 P
r1 ,r2 × Pr2 |r1
r1 ,r2 × P
Pr2 |r1
r2 |r1
1 + S2
SΣ
Σ

maxd
∅ ×

∅ ×
P

r1

× Pr1 Ud,r1

+

P

r1 ,r2 ×

∅ ×

Ud

Pr1 Ud,r2
Pr2 |r1

Figure 2: Application of rewriting rules for

P

.

P
the operator-variable pair
r1 .Such a rule uses the
duplication mechanism and the distributivity property
of × over +. It provides us with a DAG of computation nodes. It is a DAG since common computation
nodes are merged (and it is not hard to detect such
nodes when applying
P the rules). Then, DΣ can be
applied again for
r2 . One can infer from the obtained architecture that there is no reason for r1 to be
eliminated before r2 . Using the recomposition rule RΣ
makes this clear in the structure. Basically, RΣ uses
1
the distributivity of × over +. Last, applying SΣ
and
2
SΣ , which use the normalization of conditional probability distributions, simplifies some nodes in the architecture. In the end, no computation involves more
than
P two variables if one eliminates r1 first in the node
( r1 ,r2 , ×, {Pr1 , Pr2 |r1 , Ud,r2 }), whereas with a poten-

tial-based approach, it would be necessary to process
three variables simultaneously (since r1 would be involved in the potentials (Pr1 , 0), (Pr2 |r1 , 0), (1, Ud,r1 ) if
eliminated first, and r2 would be involved in the potentials (Pr2 |r1 , 0), (1, Ud,r2 ) if eliminated first).

maxd1

P

maxd2

r2

P r1
∅ × Pr2 |r1
U d1

P

r1

+

maxd3

P r1
∅ × Pr2 |r1
Ud2 ,d3

P r1
∅ × Pr2 |r1
Ur2 ,d1 ,d3

P r1
∅ × Pr2 |r1
Ur1 ,d2

Dmax
Rewriting rules for maxx When a max-marginalization must be performed, a decomposition rule Dmax
and a recomposition rule Rmax are used (there is no
simplification rule since there is no normalization condition to use for decision variables). These rules are
a bit more complex than the previous ones and are
illustrated in Figure P
3, which corresponds
to the influP
ence diagram maxd1 r2 maxd2 r1 maxd3 Pr1 · Pr2 |r1 ·
(Ud1 + Ud2 ,d3 + Ur2 ,d1 ,d3 + Ur1 ,d2 ).

maxd1

Rmax [ Prec.: (S 0 ∩(S∪sc(N1 )∪sc(N2 )) = ∅)∧(∀N3 ∈
N, N2 ∩ N3 = ∅) ∧ (∀n ∈ N2 , val(n) ≥ 0) ]
(maxS , +, N1 ∪
{(∅, ×, N2 ∪ {(maxS 0 , +, {(∅, ×, N3), N3 ∈ N})})})

r2

maxd2

P

+

r1

∅ × Pr1 Pr2 |r1

P r1
∅ × Pr2 |r1
U d1

P r1
∅ × Pr2 |r1
Ur1 ,d2

maxd3 +
∅ × Ud2 ,d3

∅ × Ur2 ,d1 ,d3

DΣ
maxd1

Dmax [ Prec.: ∀N ∈ N+x ∀n ∈ N −x , val(n) ≥ 0 ]
(Sov.maxx , +, {(∅, ×, N ) , N ∈ N})

 (Sov, +, {(∅, ×, N ) , N ∈ N}) if N+x = ∅
(Sov, +, {(∅, ×, N ) , N ∈ N−x }

∪ {(∅, ×, N1 ∪ {(maxx , +, N2 )})}) otherwise

N1 = ∩N ∈N+x N −x
where
N2 = {(∅, ×, N − N1 ) , N ∈ N+x }

P

P

∅ × U d1
P

r1

r2

maxd2

+

∅ ×

P r1
× Pr |r
2

∅ ×
P

maxd3 +

1

∅ × Ud2 ,d3

r1

Pr Ur ,d
× Pr 1|r 1 2
2

1

∅ × Ur2 ,d1 ,d3

Dmax
maxd1

P

+

r2

∅ ×

∅ × U d1
P

P r1
r1 × Pr |r
2

maxd2 +

∅ ×
1

maxd3 +

(maxS∪S 0 , +, N1 ∪ {(∅, ×, N2 ∪ N3 ) , N3 ∈ N})

∅ × Ud2 ,d3

Example In the example of Figure 3, one first applies the decomposition rule Dmax , in order to treat the
operator-variable pair maxd3 . Such a rule uses first the
monotonicity of + (max(a + b, a + c) = a + max(b, c)),
and then both the distributivity of × over + and
the monotonicity of × (so as to write things like
maxd3 ((Pr1 · Pr2 |r1 · Ud2 ,d3 ) + (Pr1 · Pr2 |r1 · Ur2 ,d1 ,d3 )) =
·maxd3 (Ud2 ,d3 +Ur2 ,d1 ,d3 )). Then, DΣ can be
Pr1 ·Pr2 |rP
1
used for r1 , and Dmax can be used for maxd2 . After
those steps, the recomposition rule Rmax , which uses
the monotonicities of × and +, reveals that the elimination order between d2 and d3 is actually free. This
was not obvious from the initial Sov sequence. The
approach using potentials is unable to make such freedoms explicit, which may induce exponential increase
in complexity as shown in 2.3.

P

P

r1

∅ ×
×

Pr1Ur1 ,d2
Pr2 |r1

∅ × Ur2 ,d1 ,d3

Rmax

Rule application order A chaotic iteration of the
rules does not converge, since e.g., rules Dmax and
Rmax may be infinitely alternately applied. Hence, we
specify an order in which we apply rules to converge
to a unique final DAG of computation nodes (we have

maxd1
∅ × U d1

r2

+
∅ ×
maxd2 ,d3 +

∅ × Ud2 ,d3
P

r1

P r1
× Pr |r
2

1

∅ ×

∅ × Ur2 ,d1 ,d3
P

r1

×

Pr1Ur1 ,d2
Pr2 |r1

Figure 3: Application of rewriting rules for max
(the application of the rules may create nodes looking like (∅, ×, {n}), which perform no computations;
these nodes can be eliminated at a final step).

used this order in the previous examples). We successively consider each operator-variable pair of the initial
sequence Sov
P 0 from the right to the left
P (marginalizaP
tions like x1 ,...,xn can be split into x1 · · · xn ).
If the rightmost marginalization
in the Sov sequence of
P
the root node is x , then rule DΣ is applied once. It
creates new grandchildren nodes for the root, for each

of which, we try to apply rule RΣ in order to reveal
freedoms in the elimination order. If RΣ is applied,
this creates new computation nodes, on each of which
1
2
simplification rules SΣ
and then SΣ
are applied (until
they cannot be applied anymore).
If the rightmost marginalization in the Sov sequence of
the root node is maxx , then rule Dmax is applied once.
This creates a new child and a new grandchild for the
root. For the created grandchild, we try to weaken
constraints on the elimination order using Rmax .
Therefore, the rewriting rules are applied in a deterministic order, except from the freedom left when
choosing the next variable
in S to consider for
P
marginalizations like S or maxS . It can be shown
that this freedom does not change the final structure.
The soundness of the macrostructure obtained is provided by the soundness of the rewriting rules:
1
2
Proposition 4. Rewriting rules DΣ , RΣ , SΣ
, SΣ
,
Dmax and Rmax are sound, i.e. for any of these
rules n1
n2 , if the preconditions are satisfied, then
val(n1 ) = val(n2 ) holds. Moreover, rules Dmax and
Rmax leave the set of optimal decision rules unchanged.

Complexity issues An architecture is usable only
if it is reasonable to build it. Proposition 5 makes
it possible to save some tests during the application
of the rewriting rules, and Proposition 6 gives upper
bounds on the complexity.
1
, the preconditions of
Proposition 5. Except for SΣ
the rewriting rules are always satisfied.

Proposition 6. The time and space complexity of the
application of the rewriting rules are O(|C ∪ D| · |U | ·
(1 + |P |)2 ) and O(|C ∪ D| · (|U | + |P |)) respectively.
3.2

TOWARDS MCDAGS

The rewriting rules enable us to transform the initial multi-operator computation node n0 into a DAG
of mono-operator
P computation nodes looking like
(maxS , +, N ), ( S , ×, N ), (∅,
P×, N ), or φ ∈ P ∪ U .
For nodes (maxS , +, N ) or ( S , ×, N ), it is time to
use freedoms in the elimination order. To do so, usual
junction tree construction techniques can be used,
since on one hand, (R, max, +) and (R, +, ×) are commutative semirings, and since on the other hand, there
are no constraints on the elimination order inside each
of these nodes (the only slight difference with usual
junction trees is that only a subset of the variables
involved in a computation node may have to be eliminated, but it is quite easy to cope with this).
To obtain a goodPdecomposition for nodes n like
(maxS , +, N ) or ( S , ×, N ), one can build a junction tree to eliminate S from the hypergraph G =

(sc(N ), {sc(n0 ) | n0 ∈ N }). The optimal induced-width
which can be obtained for n is w(n) = wG,S , the
induced-width of G for the elimination of the variables in S.3 The induced-width of the MCDAG architecture is then defined by wmcdag = maxn∈N w(n),
where
P N is the set of nodes looking like (maxS , +, N )
or ( S , ×, N ).
After the decomposition of each mono-operator computation node, one obtains a Multi-operator Cluster
DAG.
Definition 3. A Multi-operator Cluster DAG is a
DAG where every vertex c (called a cluster) is labeled
with four elements: a set of variables V (c), a set of
scoped functions Ψ(c) taking values in a set E, a set
of son clusters Sons(c), and a couple (⊕c , ⊗c ) of operators on E s.t. (E, ⊕c , ⊗c ) is a commutative semiring.
Definition
4. The
value
of
a
cluster
c of a MCDAG is given by val(c)
=

⊕c V (c)−V (pa(c)) ⊗c ψ∈Ψ(c) ψ ⊗c ⊗c s∈Sons(c) val(s) .
The value of a MCDAG is the value of its root node.
Thanks to Proposition 7, working on MCDAGs is sufficient to solve influence diagrams.
Proposition 7. The value of the MCDAG obtained
after having decomposed the macrostructure is equal
to the maximal expected utility. Moreover, for any decision variable Dk , the set of optimal decision rules
for Dk in the influence diagram is equal to the set of
optimal decision rules for Dk in the MCDAG.
3.3

MERGING SOME COMPUTATIONS

There may exist MCDAG clusters performing exactly
the same computations, even if the computation nodes
they come from are
Pdistinct. For instance, a computation node n1 = ( x,y , ×, {Px , Py|x , Uy,z ) may be decomposed into clusters c1 = ({x}, {Px , Py|x }, ∅, (+, ×))
0
and c01 = ({y},
P {Uy,z }, {c1 }, (+, ×)). A computation
node n2 = ( x,y , ×, {Px , Py|x , Uy,t ) may be decomposed into clusters c2 = ({x}, {Px , Py|x }, ∅, (+, ×))
and c02 = ({y}, {Uy,t}, {c02 }, (+, ×)). As c1 = c2 , some
computations can be saved by merging clusters c1 and
c2 in the MCDAG. Detecting common clusters is not
as easy as detecting common computation nodes.
3

For (maxS , +, N ) nodes, which actually always look
like (maxS , +, {(∅, ×, N 0 ), N 0 ∈ N}), better decompositions can be obtained by using another hypergraph. In
fact, for each N 0 ∈ N, there exists a unique n ∈ N 0 ,
denoted N 0 [u], s.t. n or its children involve a utility
function. It is then better to consider the hypergraph
(sc(N ), {sc(N 0 [u]) | N 0 ∈ N}). This enables to figure out
that e.g. only two variables (x and y) must be considered if one eliminates x first in a node like (maxxy , +, N ) =
(maxxy , +, {(∅, ×, Uy,z ), (∅, ×, {nz , Ux,y }), (∅, ×, {nz , Ux })}),
since nz is a factor of both Ux,y and Ux . We do not further
develop this technical point.

this notion is not used only for decision rules conciseness reasons: it is also used to reveal reordering freedoms, which can decrease the time complexity. Note that some of the ordering freedom here
is obtained by synergism with the duplication.

To sum up, there are three steps to build the architecture. First, the initial multi-operator computation
node is transformed into a DAG of mono-operator
computation nodes (via sound rewriting rules). Then,
each computation node is decomposed with a usual
junction tree construction. It provides us with a
MCDAG, in which some clusters can finally be merged.

4

1
• Thanks to simplification rule SΣ
, the normalization conditions enable us not only to avoid useless computations, but also to improve the ar1
may indirectly weaken
chitecture structure (SΣ
some constraints on the elimination order). This
is stronger than Lazy Propagation architectures [Madsen and Jensen, 1999], which use the
first point only, during the message passing phase.
Note that with MCDAGs, once the DAG of computation nodes is built, there are no remaining
normalization conditions to be used.

VE ALGORITHM ON MCDAGs

Defining a VE algorithm on a MCDAG is simple. The
only difference with existing VE algorithms is the
multi-operator aspect for both the marginalization
and the combination operators used. As in usual
architectures, nodes send messages to their parents.
Whenever a node c has received all messages val(s)
from its children, c can compute
 its own value val(c) =

⊕c V (c)−V (pa(c)) ⊗c ψ∈Ψ(c) ψ ⊗c ⊗c s∈Sons(c) val(s)
and send it to its parents. As a result, messages go
from leaves to root, and the value computed by the
root is the maximal expected utility.

5

COMPARISON WITH EXISTING
ARCHITECTURES

Compared to existing architectures on influence diagrams, MCDAGs can be exponentially more efficient
by strongly decreasing the constrained induced-width
(cf Section 2.3), thanks to (1) the duplication technique, (2) the analysis of extra reordering freedoms,
and (3) the use of normalizations conditions. One can
compare these three points with existing works:
• The idea behind duplication is to use all the decompositions (independences) available in influence diagrams. An influence diagram actually expresses independences on one hand on the global
probability distribution PC | D , and on the other
hand on the global utility function. MCDAGs
separately use these two kinds of independences,
whereas a potential-based approach uses a kind
of weaker “mixed” independence relation. Using
the duplication mechanism during the MCDAG
building is better, in terms of induced-width, than
using it “on the fly” as in [Dechter, 2000].4
• Weakening constraints on the elimination order
can be linked with the usual notion of relevant information for decision variables. With MCDAGs,
4

E.g., for the quite simple influence diagram introduced
in Section 3.1.1, the algorithm in [Dechter, 2000] gives 2
as an induced-width, whereas MCDAGs give an inducedwidth 1. The reason is that MCDAGs allow to eliminate
both x1 before x2 in the subproblem corresponding to Ud,x2
and x2 before x1 in the subproblem corresponding to Ud,x1 .

Compared to existing architectures, MCDAGs actually always produce the best decomposition in terms
of constrained induced-width, as Theorem 1 shows.
Theorem 1. Let wGp (p ) be the constrained inducedwidth associated with the potential-based approach (cf
Section 2.2). Let wmcdag be the induced-width associated with the MCDAG (cf Section 3.2). Then,
wmcdag ≤ wGp (p ).
Last, the MCDAG architecture contradicts a common
belief that using division operations is necessary to
solve influence diagrams with VE algorithms.

6

POSSIBLE EXTENSIONS

The MCDAG architecture has actually been developed in a generic algebraic framework which subsumes influence diagrams. This framework, called
the Plausibility-Feasibility-Utility networks (PFUs)
framework [Pralet et al., 2006a], is a generic framework for sequential decision making with possibly uncertainties (plausibility part), asymmetries in the decision process (feasibility part), and utilities. PFUs
subsume formalisms from quantified boolean formulas
or Bayesian networks to stochastic constraint satisfaction problems, and even define new frameworks like
possibilistic influence diagrams. This subsumption is
possible because the questions raised in many existing
formalisms often reduce to a sequence of marginalizations on a combination of scoped functions. Such sequences, a particular case of which is Equation 1, can
be structured using rewriting rules as the ones previously presented, which actively exploit the algebraic
properties of the operators at stake.
Thanks to the generic nature of PFUs, extending the
previous work to a possibilistic version of influence
diagrams is trivial. If one uses the possibilistic

pessimistic expected utility [Dubois and Prade, 1995],
the optimal utility can be defined by (the probability
distributions Pi become possibility distributions, and
the utilities Ui become preference
degrees in [0, 1]):



min max . . . min max min max max (1 − Pi ), min U .
I0

D1

Iq−1 Dq

Iq

Pi ∈P

Ui ∈U

These eliminations can be structured via a MCDAG.
The only difference in the rewriting rules is that ×
becomes max and + becomes min. The computation
nodes then look like (min, max, N ), (max, min, N ),
or (∅, max, N ), and the MCDAG clusters use
(⊕c , ⊗c ) = (min, max), (max, min), or (∅, max).

7

CONCLUSION

To solve influence diagrams, using potentials allows
one to reuse existing VE schemes, but may be exponentially sub-optimal. The key point is that taking
advantage of the composite nature of graphical models such as influence diagrams, and namely of the algebraic properties of the elimination and combination
operators at stake, is essential to obtain an efficient
architecture for local computations. The direct handling of several elimination and combination operators in a kind of composite architecture is the key
mechanism which allows MCDAGs to always produce
the best constrained induced-width when compared to
potential-based schemes.
The authors are currently working to obtain experimental results on MCDAGs in the context of the PFU
framework (the construction of MCDAG architectures
is currently implemented). Future directions could be
first to adapt the MCDAG architecture to the case of
Limited Memory Influence Diagrams (LIMIDs) [Lauritzen and Nilsson, 2001], and then to use the MCDAG
architecture in the context of approximate resolution.
Acknowledgments
We would like to thank the reviewers of this article
for their helpful comments on related works. This
work was partially conducted within the EU IP COGNIRON (“The Cognitive Companion”) funded by the
European Commission Division FP6-IST Future and
Emerging Technologies under Contract FP6-002020.



level, with the condition that violating however many
formulas at a given level is always more acceptable

Penalty logic, introduced by Pinkas

[17],

than violating only one formula at a strictly higher

as­

sociates to each formula of a knowledge base

level:

the price to pay if this formula is violated .

e.,

z.

Penalties may be used as a criterion for se­

apparently very appealing (besides it has already been

consistent knowledge base, thus inducing a

used several times in the literature) consists in weight­

A pre­

ing formulas with positive numbers called

cise formalization and the main properties
of penalty logic and of its associated non­

since they are

additive:

ties of the rejected formulas. Moreover, inviolable (or
unrejectable) formulas are given an infinite penalty.
The additive combination of penalties leads to an in­

pecially in the infinitesimal case.

terpretation in terms of

itarist,

Introduction
appears when

the available knowledge base - KB for short - (here
a set of propositional formulas) is inconsistent. Most
approaches come up with the inconsistency by select­
ing among the consistent subsets of KB some

preferred

subsets; the selection criterion generally makes use of
uncertainty considerations, sometimes by using explic­
itly uncertainty measures (such as W ilson
ferhat and Smets

[2)),

expressed qualitatively as
back to Rescher

[20] and

priorities

(the idea comes

[3],

Nebel

[16],

Benferhat, Cayrol, Dubois, Lang, Prade

Lehmann

[14]).

Ben­

has been developed by many

authors, among them Brewka

[4],

[2 7 ] ,

or more often using measures

Cayrol

[1]

and

Although these priorities are gener­

ally not given a semantics in terms of uncertainty mea­
sures (however see

[1]

for a comparative study of the

priority-based and possibilistic approaches to inconsis­
tency handling), their intuitive interpretation is clearly
in terms of gradual uncertainty: the least prioritary
formulas

( i.e., the

ones which are most likely to be re­

jected in case of inconsistency) are clearly the ones we
are the least confident in, i.e., the least certain ones.

cost,

thus this criterion is

util­

contrarily to priority-based approaches which

are rather

inconsistency handling

the global penalty for rejecting

a set of formulas is the sum of the elementary penal­

first part. We also show that penalty logic
and Dempster-Shafer theory at·e related, es­

The problem of

penalties.

Contrarily to priorities, penalties are compensatory

monotonic inference relation are given in the

1

non-compensatory,

An alternative approach, more or less empirical but

lecting preferred consistent subsets in an in­
non-monotonic inference relation.

thus these approaches are

levels never interact.

egalitarist.

This additive criterion is very in­

tuitive, since rejecting a formula generally causes some
"additive" trouble with the experts which provided the
f{ B with the formulas, or some real financial cost, or

another kind of additive cost. Note that a degenerate
case of penalties (all penalties being equal to

1) prefers

subsets of maximum cardinality. Moreover, and as we
will see later, these penalties can sometimes be inter­
preted as the "probability of fault" of the source which
provided us with the information (all sources failing in­
dependently), up to a logarithmic transformation. In
any case, these penalties can be viewed as measuring

uncertainty

since, again, the less expensive to reject,

the more uncertain the piece of information.
penalty logic

Thus,

expresses uncertainty in terms of costs.

However a formal connection of penalties with classical
theories of uncertainty has not really been made.
Penalty-based approaches have been already used sev­
eral times in the literature, first by Pinkas

91 [17] (from

whom we borrowed the terminology "penalty") who
uses them for inconsistency handling and for mod­
elling symmetric neural networks behavior, and also

All aforementioned priority-based approaches consist

by Eiter and Gotlob 94 [10] for cost-based abduction,
by Sandewall 92 [21] for cost-based minimization of

the set

Satisfaction Problems. Moreover, penalties associated

in ranking the f{ B in n priority levels (assume that 1
is the highest priority and n the lowest) and maximize

or

the number of formulas satisfied at each

surprises in temporal reasoning and by Freuder and
Wallace

[12]

for tackling inconsistencies in Constraint

Penalty Logic and its Link with Dempster-Shafer Theory

to formulas have also been used for guiding the search
in randomized algorithms dedicated to the satisfiabil­
ity problems, such as GSAT [23, 22].

Lastly, there

should clearly be a link between penalties and utility
theory (the latter has been recently used in AI, espe­
cially in decision-theoretic planning - see e.g. [18]);
however, in this paper we do not investigate this pos­
sible link.

205

Since PK is a multi-set of pairs (and not a. set), it. is
possible for a pair{'{), a:) to appear several times in PI\;

for example, PK = {{a, 1), {a, 1)} is not equivalent to
PK' = {{a, 1}} since using PK, it costs 2 to delete a
'
and using P K , it costs only 1.
However, as we will see in 2.1.4, if a formula'{) appears
several times in PK then we may replace all the occur­
rences of the formula 'P by only one occurrence of 'P

In this paper we revisit penalties by giving a further

formalization of Pinkas' work; we also go further in the

annotated with the sum of the penalties associated to
this formula in the previous base. The new knowledge

theoretical study of penalty-based inconsistency han­

base obtained is equivalent to the initial base.

dling and non-monotonic reasoning.

We briefly give

a formalization in penalty logic of an additive
problem. Lastly, we establish

a

O.R.

link between penalties

and Dempster-Shafer theory; this link is twofold: first,
the penalty function (on interpretations) is equivalent,
up to a log- transformation, to a contour function (i.e.,
the plausibility function restricted to singletons); then
penalty functions on formulas coincide with plausibil­
ity functions of an infinitesimal t'ersion of Dempst.er­
Shafer theory.

2

based on a finite number of propositional variables. T

.2' wi ll be writt e n '{), 1j!, etc.
The set of interpretations attached to .2' will be de­
noted by n, and an interpretation by w. 'P F= 1/! and
Formulas of

'P f=l'lj! will represent logical consequence and logical
equivalence between the formulas

I==

'P and 1/! respectively.

will also be used between an interpretation and a
formula to denote satisfiahility. The set of models of
a formula 'P will be denoted by M ( 'P); the set of for­
mulas of .2' satisfied by w, i.e., {'P I w I== 'P} will be
denoted by [w].

A classical knowledge base 91 is a set of formulas of
.2'. A sub-theory of 91 is a consistent subset of §9. A

§9 is a consistent subset of 91
T, T U { 'P} is inconsistent. Given

maximal sub-theory T of
such that 'if'{) E 91 \

a formula 1/!, T is said 'ljl-consistent iff T U { 1j!} is c o n­
sistent; Tis maximal'lj!-consistent if it is 1/J-consistent

and V'P E §9 \ T, T U { '{), 1/!} is inconsistent. w+ will
be the union of the set of all the strictly positive real
particular, if

{ +oo}, equipped with the usual order
#- +oo then a: < +oo).

a:

(in

A penalty knowledge base PK is a finite multi-set of
pairs {'P;, a:; ) where '{); E .2' and a:; E w+. a; is the
penalty associated to 'Pi; it represents intuitively what
we s hould pay in order to get rid of 'Pi, if we pay the

requested price we do not need any longer to satisfy
'Pi; so the larger

a:;

In particular, if a:;

move

the p en al ti es ex;). Also, in the expressions sub-theory of
PK, subset of PK and PK \A we will refer to the set of

is, the more important 'Pi is.

:::

Cost of an interpretation

Let PK

and .l will represent taut ology and contradiction re­

numbers and

Lastly, we will say that PK E !JlJc is co nsistent if the set
of formulas 'Pi of PK is consistent ( without mentioning

=

{{'{);,a:;},i =

1

.. . n}

be a penalty knowl­

edge base.

following, .2' will be a propositional language

spectively.

violated).

2.1.1

Formal definitions

In the

logic comes down to classical logic (no formula can be

formulas obtained from PK by ignoring the pena.lties.

Penalty logic

2.1

91c will be the set of all the penalty knowledge bases.
Note that when the penalties are all infinite, penalty

+oo then it is forbidden to re­

i.p; from PK ('Pi is inviolable).

Definition 1 (Pinkas 91 [17]) The cost of an in­
w E Q with respect to PK , denoted by
kpK(w ), is equal to the sum of the penalties of the for­

terpretation

mulas zn PK violated by

(with

the

w:

corn,enfion L:1P,E0 a:;= 0)

Definition 2 A PK-preferred interpretation is an in­
terpretation of minimal cost w.r.t. PK, i.e. an inter­
pretation minimizing

kpl(.

As an example, let us consider the following penalty
k n owledge base P K1:

'PI =a
y2 =b v c
y3 = -.b
'P4 = -.c

0:1 = +oo
0:2

= 10
=5
0:4 = 7
0: 3

Here are the corresponding interpretations costs:

kpr<, ({-,a, b, c})
kpr<, ( {...,a, b, .....,c})
kpK,({a ,-,b,--.c})
kpK,({a,b,-.c})
kpK,({a,•b,c})
kPK,({a,b,c})

kpK, ({ •a, •b,c}) = +oo
kpK, ({ •a, •b, •c}) = +oo
10

5
7

5 + 7=

12

If the interpretations are decisions to make (for exam­
ple if the knowledge base is made of constraints con­
cerning the construction of a timetable), then a min­
imum cost interpretation corresponds to the cheapest

206

Dupin de Saint-Cyr, Lang, and Schiex

decision, i.e., the most interesting one. The cheapest
interpretation is generally not unique. Besides, if the
penalties are all equal to 1 then a cheapest interpreta­
tion satisfies a maximum consistent subset of PK w .r. t.
cardinality.
2.1.2

Cost of consistency of a formula

3 The cost of consistency of a formula r.p
with respect to PK, denoted by f{PK(cp), is the mini­
mum cost with respect to PK of an interpretation sat­
isfying r.p:
KpK(C,O) =min kpK(w)

Definition

wi='P

(with the convention min0 kPK(w)

=

+oo)

Example:

f{PK1 (a!\ b)
KpK,(a-+c)
KPK,(-.a)

7
+oo

KpK('f') of a formula r.p, is the minimal price
to pay in order to make PK consistent with cp. For
example, in order to make PK1 consistent with a -+ c,
the least expensive way is to remove r,o4.
+oo and J(PK ( T)

1 J(PK(l..)
minwEn{kPK(w)}

All proofs can be found (in French) in Dupin de Saint­
Cyr, Lang and Schiex 94 [8] and in Dupin de Saint-Cyr
93 [7).

KPK(l_) = +oo is easy to understand, because it is
impossible to have PK consistent with 1... Let us note
that /(pK ( T) is the cost of any PK-preferred inter­
pretation; it is thus the mini mum cost to make PK
consistent.
Property 2

KpK(T)

is inconsistent.

=

+oo

¢}

{cp;

E

PK,a;:::: +oo}

This quantity KPK(T) is important, because it mea­
sures the strength of the inconsistency of PK (i.e., how
expensive it will be to recover the consistency). If the
penalties are all equal to +oo then KpK(T) can only
take two values: 0 if and only if PK is consistent, and
+oo if and only if PK is inconsistent.
Example: J(PK, (T) = 5; the only minimum cost in­
terpretation is {a, b, -.c}. To make PK1 consistent, the
least expensive solution is to take off (or to ignore) the
formula 'f'3·
Property

3 /(pK(T)

=

1.

Property 4 'Vi.p, ¢ E .!f,

(cp f= ¢)

:::::}

KpJ<('P) 2:

'Vr,o, ¢

E

2:

KPK(IO !\ ¢) 2: max(I{pK('P), Kpr<(?/;))

2. KpK(\0

V

¢)

= mi n(KP K ( cp ) ,

KpK(¢ ))

3. f<pK(l..) 2: /{pK(IO) 2: f{pK(T)

Note that, up to its interval of definition and its or­
dering convention w.r.t. Proposition 5 (((0, +oo), ::::)
instead of ((0, 1), S)), /{pK is actually a possibil­
ity measure. Note also that Spohn's ordinal condi­
tional functions x: verify property 2 1. e. x:( A U B) =
min(x:(A), x:(B)) [26).
Cost of a sub-theory

Definition 4 (Pinkas 91 [17J) The cost CPK (A) of
a sub-theory A of PK, is the sum of the penalties of
the formulas of PK that are not in A:

{!f'.,<:>,)EPK\A
For instance, considering the knowledge base PK1,
given A1 = {i.pl,i.p2,'P3} and A2 = {r,o2,i.p4}, we have
CPK1 (Al) = a4 = 7 and CpK1 (A2) = 0:1 + a3 = +oo.
Definition 5

'VA, B � PK,

B 2:PK A (B is preferred to A) ifJCPK(B) S CPK(A).
'VA, B � PK , B >h A if and only if B 2:PK A and
not A 2:PK B.
Definition 6 (Pinkas 91 [17]) A � PK
ferred sub-theory relatively to PK

is

a

pre­

(or 2:PK­
preferred) if and only if A is consistent and ,3 B � PK ,
such that B is consistent and B >f,K A.

Note that there may be several preferred sub-theories
(in the previous example, { cp1, 'P2, 1p4} is the only one

2:PK1-preferred sub-theory).
Property 6

'VPK

E

fflc,

If J<pK(T) ::f +oo, then any 2:PK -preferred sub-theory
is a maximal sub-theory of PK w. r.t. inclusion.
•

0 ¢} PK is consistent.

Indeed, if KpK(T) = 0 then there is no need to delete
any formula in order to make PK consistent, therefore
PK is consistent (and conversely).

KPK(?/;)

Property 5

2.1.3

5

The cost

Property

This property is the monotonicity of K with respect to
classical entailment.

•

Let us notice that when KPK(T) = +oo, every
sub-theory of PK has an infinite cost, therefore
every sub-theory of PK is 2:PK-preferred, but ob­
viously every sub-theory is not necessarily maxi­
mal w.r.t. inclusion.
Besides, if PK is consistent, then I<PK(T) = 0,
and then the only >pK-preferred sub-theory of
PK is PK itself (its Zost is 0).

Example (continued): A3 = {r,ol,'f'2,'P4} is a 2:PK,­
preferred sub-theory and it is maximal w.r.t. inclusion.

207

Penalty Logic and its Link with Dempster-Shafer Theory

But, although {cp2,cp3,cp4} is a maximalsub-t.heory of
PK1 (w.r.t. inclusion), it is not 2:-PK,-preferred (be­
cause its cost is infinite).

If we add the formula

(cps

=

-,a, a5

=

+oo}

to

PK1

Besides, we define a pre-ordering relation <<c
as follows:
Definition 9

then the subset of infinite cost formulas is inconsistent,
therefore every sub-theory has an infi ni t e cost, and
a

every sub-theory is

preferred

PK

interpretation
cost of the sub­

theory of PK composed of all the formulas satisfying

w:

As

PK4

an example,

PK n

7.2 A is a maximal sub-theory of PK
Vw f= A, kpK(w) = CPK(A).
Corollary

7.3 KPK('P) is equal to the
of a 10-consistent sub-theory of PK:

Corollary

of a formula

minim11m cost

cp wit h respect to the

base PK is the cost of a cp-consist.ent

sub-theory of

=>

min
CrK(A)
.
A<;PI<,A '1"-conH•tent

Therefore, the cost

PK.

2:rK-preferred

7.4 VA � PK,

Corollary

A

is

a
¢::>

(cf. corollary

Definition

7.3,

2:PK -preferred sub-theory

KPK(T)

=

with cp =

7 Add(PK, cp)

CpJ<(A).

T).
=

PK U { (<p, +oo)}

10

ta.b}
_ia,-.bl
J-.a, bl
l...., a,...., bl

Therefore, the cost to make the knowledge base consis­
tent with a given formula, can be computed by adding
with

an

infinite penalty and then evalu­

ating the cost of the new knowledge base consistency.

Two

alent

Equivalence between penalty
knowledge bases
k n o w ledge bases are .semantically equiv­
if they i n d u ce the same cost function on 0, i.e.:

penalty

Definition 8 VPK, PK' E !fie,
PK �c PK'

PK:J, PK3
as

.

,

0

10
8
18

and

follows:

b

10

0

10
8

18

the fol­

0

18
18
18

So we have PK2 �c PK3 and PK3 «c
is not equivalent to PK4).

PI\4

(but Pl\3

N.B.: the previous example shows th at it is impos­
sible to transform equivalently a penalty knowledgr

base containing several non-equivalent formulas in a
penalty knowledge base containing the conjunction of
those formulas.

But, if a knowledge base contains se1'eral times the
same form1tla (or an eq11ivalent on e), it is possible to
transform it equi1•alent/y in a knowhdge base contain­
ing this formula only one time with a penalty equal to
the sum of the penalties of this form.ula in the prel•ious
base.
Property 9 VPK, PK' E :JlJc,

PK

2.2

2.1.4

consider

�c

PK'

=>

A{cpiJ�;

E

PK} f=ll\ {'PiliPi

E

PK'}

The con verse is obviously false.

Property 8

this formula

b

3

w

2:-PK·preferred sub-theory with respfCI to
all the sub-theories of PK

=

us

lowing:

<=>

KpK(rp)

let

The cost func t i ons incluceu by those bases are

E 0,

has a minimal cost U.'.r.f. PK

a

[w] is

less expensive than PK ')
$ kpK'

the penalty knowledge bases defined
PK2 :
PKa :
PI-.:4 :
a
5
18
a
8
a 1\

b

w

is

E !Jic,

¢:> kpK

a

Vw

PK' (PK

sub-theory.

Property 7 The cost kpK(w) of an
w E 0 with respect to PK is equal to the

Corollary 7.1

«c

VPK, PK'

on 5!�

(l>K is semantically equivalent to PK ')
{:::>

kpi<

==

kpw.

Inconsistency handling with penalty logic

Using penalties to handle inconsistency is a syntax­
based approach, in the sense of [16], which means that

the way a know ledge base be hav es is dependent on the
syntax of the input (this is justified by the fact that
each formula is considered as an independent piece of

information); for instance, {p,q,....,pY -,q} will not be­
have as {p 1\ q, ....,p Y ....,q}, since in the first. case we ca.n
re

move independently the formulas

{p,

p and q

( {p, q}.

p V -.q} and { q, ....,p Y ....,q} are the maximal sub­
theories), but in the second case we must rem ove or
....,

whole fo rm ula p 1\ q ( {p 1\ q} and { ...,p V -.q}
the maximal sub-theories).

keep the
are

In order to deal with inconsistency, the basic idea de­
veloped with syntax-ba.'led approaches is to define a

208

Dupin de Saint-Cyr, Lang, and Schiex

nonmonotonic inference relation as follows: 1/J can be
deduced nonmonotonicaly from a knowledge base iff
all the maximal sub-theories of this base entails (clas­
sically) 1/;.
2.2.1

Given

Nonmonotonic inference relation
induced by a penalty knowledge base

PK

E

Definition

�c·

10 'r/cp,1/J

E 2',

2.3

In this section, we will see that penalty logic is not
only a tool for inconsistency handling but also a good
way to represent, in a logical language, discrete opti­
mization problems (for instance issued from operation
research), in which minimum cost interpretations cor­
respond to optimum solutions.

We consider an undirected graph

¢}

'v'A � PK, if A is a ?:.PI< -preferred cp-consistent

sub-theory among all the cp-consistent sub-theories of
PK, then AU {cp} f= 1/J.

sub-graph

N.B.: 'r/1/;, ..L
Property

In penalty logic we can represent it like this:
•

1/J.

s

that this vertex

f---�K1/J.

10 'v'cp, 1/J

0, if w f=

to each vertex

sitional variable

•

2',

E

E U, we can associate

s

a

propo­

which truth assignation means

belongs to

the clique we are look­

minimum of vertices: to each vertex we associate
the penalty formula {s, 1).

f-.- �K 1/J

cp

we are searching for a set of vertices which is max­
imum for cardinality, so we have to exclude the

¢}

•

and w is a ?:.PI< -preferred
interpretation satisfying <p, then w f= 1./J.
E

every vertex is connected with every

ing for.

'P

'r/w

then A F=

(i.e.,

other vertex). Finding a maximum cardinality clique
is a classical N P-ha.rd problem in operational research.

In particular, if cp = T, the definition becomes:
f-.- � K1/J {:} if A is a ?:.PK-preferred sub-theory among

PK,

G, i.e., a set of ver­

tices U and a set of edges V connecting those vertices.
A clique of G is a subset of V which define a complete

cp f-.- �K1/J

all the sub-theories of

An application of penalty logic:
maximum clique in a graph

the resulting set

(x, y)

graph

must be a clique so for each pair

of vertices that are not connected in the

G (i.e., (x, y) � V), at least either

x

y

or

This property shows that the nonmonotonic inference

does not belong to the clique. In consequence, we

relation

can associate to each pair

f-.- �K

belongs to the set of relations based on

preferential models in the sense of [15]. As �Pl< is a
complete pre-ordering, we immediately get the follow­
ing result:

Property

Let

f-.- �K

11

relation1.
Property
'P

12 Given

* 1.,

zs

a

comparative

inference

(...,x V -,y, +oo).

formula

PK(G)

=

{(s, l),s

E

(x, y)

� V the penalty

U}U{(...,xV---.y,+oo),(x,y)

�

V}.

13 (see [8]) Every minimum cost inter­
pretation with respect to PI<( G) corresponds to a max­
imum clique of G and conversely.

Property
PK E

�c and cp, 1/J

E 2',

with

Example:

For instance,

let us consider the following penalty

knowledge base

e

( a , l)(b, l}(c, 1}
(d, l)(e, 1}

5),

--+ c

The minimum cost interpretation

1)}

a f-.- �I<

(al\b)

f---�K

is {-.a, b, c, d, -.e}.

This example shows the ability of penalty logic to en­

It can be checked that:

f-.- �I<

c

b

--.c

(a

( •a V •c, +oo)
{---.a V ---.d, +oo)
( •a V •e, +oo}
(-,b V •e, +oo)
( --.c V •e, +oo)

, +oo),

(-,av--.b , 4),
{b--+
2),

•

code discrete optimization problems. One could ar­
gue that, in operation research, algorithms for solv­

c

c

ing classical problems (as maximum clique, minimum
vertex cover... ) do already exist. Those algorithms
are probably more efficient than the one consisting in

•c

comparative inference relation (13] is a rational rela­

tion [15] that also satisfies supraclassicality: if 'P

'Pb--�K'f/;.

d

PK:

{(a V b
( ---.a

1A

a

F '1/1

then

finding the best interpretation
oped in

[7]).

in penalty logic (devel­

However, the logical representation of this

kind of problems presents at least two advantages: the

209

Penalty Logic and its Link with Dempster-Shafer Theory

great power of expression of logi c allows us to spec­
ify many complicated problems which could not easily
be specified within the operational research language;
and the best solution search method is independent of
the given problem.

3

Relating penalties to
Dempster-Shafer theory

In this section we are going to show:

ties are used to induce a preference r el at ion on

•

first, that the cost of an interpretation kpK : rl
[0, +oo) induced by a penalty knowledge base PK
c ons i s t ing of n weighted formulas corresponds ac­
tually to the contour function pl : rl ---+ [0, 1]
induced by Dempster's combination of n simple
support functions (one for each formula rpi);

•

th en , that moreover, the f unc tion Kp){

---+

:

.5f>

-

[0, +oo) corresponds to a plausibility me as ur e in
an infinitesimal version of Depmster-Shafer the­
ory.

3.1

Interpretation costs and contour
functions

Let PK = { (rp;, a;}, i = 1 .. . n} be a penalty kn owl­
edge base. Let us define, for each i, the body of evi­
dence m;:

m;(rp; ) = 1- e-a,
m; (T) = e -a•

00
By convention we take e= 0. S ince a; E [0, +oo],
it can be seen that m;(cpi) E (0, 1) and m;(T) E [0, 1).

note that lim<>,-+co m;(cp;) = 1. m; is
called a simple support functi on [24]. Let m = m1 EfJ
·EBmn be the result of Dempster's combination of the
m; 's (9] without re-normalization. The contour func­
tion pl : n
[0, 1] associated to m i s the restriction

M oreover,
· ·

of

the

-+

plausibility function to sin g le ton s ,

pl(w)
Now,

=

P l ( { w })

=

L

i.e.,

m (rp)

it is well-known [24] that
=

IT Pl;({w})

( II

1).(

IT

i,wl=...,<+>•

II

i,wl=-.cp;

i,wl=cp;

e-

port functions in order to rank interpretations can be

d on e a.lt.erna.ti vely with penalty logic.

This also brings to light. a relation bet.ween penalties
and [25] where each formula 'Pi of the knowledge base is
considered to be given by a distinct source, this source
having the pro bability p; to be faulty (i.e., the infor­
mation it provides us with is not pertinent), and all

so u rces being independent (which gives the simple sup­
port function m;(cpi) = (1- p;) and m;(T) = p,:). So
if the task is only to find the most plausible interpre­
tation (as in [11] which i s the C o ns tr aint Satisfaction
counterpart of [25]), it can thus be done equivalently
with penalti. Ps.

3.2

Formula costs as infinitesimal
plausibilities

Let us consider an infinitesimal version of Dempster­

Shafer theory, where the masses involved are all in­
finitely close to 0 or to 1. Let c be an infinitely small
quantity2•3. Again, let PK = {(cp;,a;},i = 1. .. n}.
Let us define, for each i, the infinitesimal body of evi­
dence m,,;:

i=l

where P l; is the plausibility function induced by m;.
Moreover, Pl;(w) = 1 if w f= 'Pi and Pl;(w) = e-c., if
w f= -,'Pi. Thus,

pl(w)

rl, a.nd

then possibly to select one of the (or all ) cheapest in­
t.erpretat.ion(s). Namely, this is eno ugh for inducing
the inference relation \--- �K, for solving discrete op­
timization proble m s, and also for applying p en alties
to constraint satisfaction problems or abduction. So,
ha ndlin g penalties in su ch a purpose is nothing but
performing Dempster's combination on s impl e sup­
port functions. Reciprocally, combining simple sup­

m,,; (r,o;) = 1 - ca'
m,,;(T) = ca'

n

Pl({w})

Therefore, kpK(w) = -ln(p!(w)): up to a logarithmic
transformation, kPK is a contour function, or mo re pre­
cisely, the process consisting in computing kpr< corre­
sponds to applyin g Dempster's combination without
re-norma.lization on simple support functions. Thi s
equivalence does not extend to an equivalence between
]{pK and a plausibility function (see subsec ti on 3.2}.
but this result is already significant, since in most prac­
t ic al applications of penalty logic, only the contour
function kpK is useful: this is the case when penal­

e-c.,

Lt.�.�t==-.'Pi O'i

e-kPK(w)

e-Ct·)

Let m, = m , 1 tfl· · · ffi m, n be the result of Dempster's
combination �f the m; 's '[9) without re-normalization.
Let us show now that J(PK has the same or d er of mag­
nitude (w.r.t. c ) as ln(P/,), where ln(Pl,) is the plau­
sibility function induced by m,.
Let us note that the set of focal elements of
exactly {"-iEI'Pi, It;::; {1 ... n}}.

m,

is

2More formally, this consists in considering a family of

e's tending towards 0; indeed what we are interested in is
only the limit of the considered
3We recall that ft(e:)

�

f( I!)

when

h(e) iff lim,_o

I!

tends to 0.

};i:j

==

1.

210

Dupin de Saint-Cyr, Lang, and Schiex

4

Now, let us define
R(PK, w)

=

{I � { l .. n}, J\ (<p;)

i\

.

w consistent}

iEI

Now,
II m;(<p;). II m;(T)
i<l/
IER(PK,t/J) iE/

Pl('!j!)

IER(PK,!j;) iEJ

As c: is infinitely small and
.::a•) R:: 1, therefore:
PI(¢)

R::

I

is always finite, f};E1(1-

/ER(PK,t/J)

{J

E

as

R(PK, !J;), La; is minimum}
i<ll

and let r(PK, !J;)
Since

c: is

=

IRminpen(PK, ¢)1.

infinitely small, we have

PI(¢)

R::

!ERm'""'"(PK,,P)

r(PK, '!j!).maxiER(PI<,.;,).::L,er a,
r(PK,

Used to handle inconsistency and perform non­
monotonic inferences, penalty logic has shown to have
interesting properties. Using penalties for selecting
preferred sub-theories of an inconsistent knowledge
base not only allows to distinguish between the degree
of importance of various formulas, as usual priority­
based approaches do, but also to express possible com­
pensations between formulas. The non-monotonic in­
ference relation defined satisfies the usual postulates
[13] and is (logarithmically) related to an infinitesimal
version of Dempster-Shafer theory.
Furthermore, the complexity of the penalty non­
monotonic deduction problem has been considered in
[5] and is ranked as one of the most simple non­
monotonic inference problem (in��).

/ER(PK,!f;) i<tl

Let us now define Rminpen(PK, !/•)

Conclusion

��).c:miniER(PK,�) Z.v a,

Now,

Penalty logic may also been considered as a logical lan­
guage for expressing discrete optimiza tion problems.
The search for a preferred interpretation has been im­
plemented using an A -like variant of Davis and Put­
nam p ro ced u re (6] and has been tested on small ex­
amples. Randomized search algorithms such as GSAT
[23, 22] could also be considered, but they do not guar­
antee that an optimum is actually reached.
•

As shown in [5], solving the problem of searching a
preferred interpretation allows to simply solve the non­
monotonic inference problem, without any restriction
on the language of the formulas expressed4. Any­
way, even the limited ll.� complexity can be consid­
ered as excessive when faced to practical applications.
A reasonable approach would then consist in defining
a gradual inference relation and in trying only to solve
an approximation of the resulting gradual inference
problem.
Among the other possible extensions of penalty logic,

minJER(PK,tiJ)

La;
i<l/

min

BCPK
,BAtjJ
-

c onsistent

min

B�PK,BAtjJ consi•tent

La;

'Pi<lB

CpJ<(B)

one could consider associating many unrelated penal­
ties to a single formula. Partially ordered penalty vec­
tors would then replace penalties. Another possible ex­
tension consists in taking into account not only penal­
ties caused by violations but also profits associated to
satisfactions (which could be expressed using negative
penalties).
Acknowledgements

Therefore,

Note that r(PK, ¢) does not depend on .::, and more­
r(PK, '¢) > 0. So, up to a logarithmic
transformation and a multiplicative constant (in other
terms, if we consider only the orders of magnitude
w. r. t. c::), Kpl( is equivalent to an infinitesimal plausi­
bility function.

We would like to express our thanks to Didier Dubois
and Henri Prade for helpful suggestions concerning
the link between penalties and Dempster-Shafer the­
ory, and Michel Cayrol for having found an error in
a preliminary version. This work has been partially
supported by the ESPRIT BRA project DRUMS-2.

over that

4

Using an ATMS for computing candidates and pre­

ferred sub-theories could also be considered, but the re­
sulting complexity is more important in the general case

[19].

Penalty Logi c and its Link w ith Dempster-Shafer Theory


