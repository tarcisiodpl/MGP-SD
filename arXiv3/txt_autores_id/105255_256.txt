

•

The paper deals with optimality issues in con­

these labels are called separators (see Figure la);

nection with updating beliefs in networks. We
address two processes: triangulation and con­
struction of junction trees. In the first part,

•

network.

In the second part, we argue that

any exact method based on local calculations
must either be less efficient than the junction
tree method, or it has an optimality problem
equivalent to that of triangulation.

1

attaching a pote ntial to all separators (initially

the neutral potential consisting of ones);

we give a simple algorithm for constructing
an optimal junction tree from a triangulated

giving all links in the junction tree a label con­
sisting of the intersection of the adjacent nodes;

•

letting the nodes communicate via the separa­
tors:

a message from U to V with separator S

has the form that <Pu is marginalized down to S,
resulting in ¢5; <Psis placed on the separator and
¢'(S)/¢(S) is multiplied on ¢v (see Figure 1b).

INTRODUCTION

The junction tree propagation method (Jensen et al.,
1990; Lauritzen and Spiegelhalter, 1988) is designed
for propagation in Markov networks:
•

cjl'(S)!cjl(S)

cjl'(S)

an undirected graph with discrete variables as
nodes;

•

for each clique

U

in the graph there is a poten­

tial <Pu, which is a non-vanishing function from
the set of configurations of

U

to the set of non­

negative reals.

{a)

The compilation part of the method is to
•

FIGURE 1.

triangulate the graph (i.e., add extra links such
that every cycle of length greater than three has
a chord);

•

construct

(a) A junction tree. (b) Message passing in junction
trees.
It is so, that after a finite number of message passes

form a potential <Pu for each c lique

U

of the tri­

angulated graph;
•

(b)

a

junction tree over the cliques.

A junction tree over the cliques is characterized by the

so-called junction tree property: For each pair U, V
of cliques with intersection S, all cliques on the path
between U and

V containS.

The propagation part of the method consists of

between neighbours in the junction tree, each po­
tential in the junction tree holds the (possibly non­
normalized) marginal of the joint probability distribu­
tion for the entire set of variables. In fact, the message
passing can be organized so that it is sufficient with
exactly one pass in each direction of the links in the
junction tree. Therefore, in complexity considerations
for propagation in junction trees, one can associate a
local measure

C(U, V) to

links

(U, V),

where

C(U, V)

indicates time/space consumption for the two passes.

361

Optimal Junction Trees

The compilation is not deterministic. Markov net­
works may have several different triangulations yield­
ing different sets of cliques, and a triangulated network
may have several different junction trees. We therefore
would like to have algorithms yielding optimal trian­
gulations and optimal junction trees with respect to
complexity. However, the optimality problem for tri­
angulations isN'J'-complete (Arnborg et al., 1987).
In the first part of the paper, we address the optimal­
ity problem for junction trees given the triangulated
graph, and we present a simple algorithm which is
quadratic in the number of cliques.
In the last section, we address the triangulation pro­
cess and ask the question whether it may be possible
to come up with a propagation method which does not
contain anN'J'-hard optimality problem. The answer
is discouraging. We show that any local calculation
method must involve a hidden triangulation, and we
use this to conclude that the method is either less ef­
ficient than the junction tree method, or it has an
N'J'-hard optimality problem.
2

JUNCTION TREES AND
MAXIMAL SPANNING TREES

Throughout the remainder of the paper, we consider a

triangulated connected graph G with clique set e. The
cliques of G are denoted b.I the letters U, V, W, ll1,
etc. We shall not distinguish between a clique and
its set of variables. So we talk of the intersection of
cliques meaning the set of variables common to the
cliques. Intersections are denoted by letters R,S, R1,
etc.
Definition 1 The junction graph for G has e as nodes,
and for each pair U, V of cliques with nonempty inter­
section R there is a link with label R. Each link has a
weight which is the number of variables in the label.
Theorem 1 A spanning tree for the junction graph
of G is a junction tree if �nd only if it is a spanning
tree of maximal weight.

Theorem 1 has been proved independently by Shibata
{1988) and Jensen (1988). Here we will give a proof
much simpler than the o:tiginal ones. Before giving the
proof, we shall recall two algorithms for the construc­
tion of maximal spanning trees.

.

.

®-0--cb
FIGURE 2.

Paths in T and T1•

Prim's algorithm constructs a sequence To �
� Tn.
of maximal spanning trees for the subgraph deter­
mined byN.
· ·

·

Algorithm 2 (Kruskal)

Choose successively a link of maximal weight not pro­
ducing a cycle.
Kruskal's algorithm works with a forest of partial max­
imal weight spanning trees. Whenever a link is cho­
sen, two partial trees are connected into a new partial
spanning tree of maximal weight.
Both algorithms result in maximal weight spanning
trees, and each maximal weight spanning tree can
be constructed through any of the two algorithms.
[Proofs can be found in many textbooks on graph
algorithms, e.g., (Goudran and Minoux, 1984) and
(McHugh, 1990)].
Proof of Theorem 1:
Let T be a spanning tree of
maximal weight. Let it be constructed by Prim's al­
gorithm such that T1 �
� Tn =T is a sequence of
partial maximal weight spanning trees.
· ·

·

Assume that T is not a junction tree. Then, at some
stage m, we have that Tm. can be extended to a junc­
tion tree T1 while Tm.+1 cannot. Let (U, V) with la­
belS be the link chosen at this stage; V E Tm.+ 1 (see
Figure 2).
Since Tm.+ 1 cannot be extended to a junction tree, the
link (U, V) is not a link in T1• So, there is a path in T1
between U and V not containing (U, V). This path
must contain a link (U1, V') with labelS' such that
U 1 E T m. and V1 (j_ Tm. (see Figure 2).
Since T 1 is a junction tree, we must haveS �S1, and
sinceS was chosen through Prim's algorithm at this
stage, we also have S
l I;::: 1511. Hence, S =51•

Algorithm 1 (Prim)
(1) Put N {U}, where U is an arbitrary node.

Now, remove the link (U1, V1) from T' and add the
link (U, V). The result is a junction tree extending
Tm.+ 1, contradicting the assumption that it cannot be
extended to a junction tree.

(2) Choose successively a link (W, V) of maximal
weight such that W E N and V (j_ N, and add V
toN.

Next, let T be any non maximal spanning tree. We
shall prove that T is not a junction tree. Again, let
T1 �
� T 1 be a sequence of maximal trees con-

=

-

•

·

·

362

Jensen and Jensen

FIGURE 3. The thinning task at stage

i+ 1

in Kruskal's algorithm.

structed through Prim's algorithm. Let the construc­

trees of maximal weight. Note that any thinning at a

tion be so that a link from

given stage will result in the same connected compo­

ble. Let

m

T is chosen whenever possi­

be the first stage where this is not possible,

and let

(U, V} with separator S be the link actually
chosen (U E Tm• V � Tm). In T there is a path be­
tween U and V. As in the first part of the proof, we
have that this path contains a link [U 1, V 1) with la­
belS' such that U' E Tm and V' � Tm (see Figure 2).
Since (U' , V') could not be chosen, we have S
I 'I < S
l I,
and thereforeS contains variables not inS '. Hence, T
does not satisfy the junction tree condition.

3

1

OPTIMAl JUNCTION TREES

W henever the junction graph has several spanning
trees of maximal weight, there are accordingly several
junction trees. Assume that there is a real-valued mea­

sure on junction trees yielding a priority among them,
and assume that this measure can be decomposed to
a local measure

C(U, V}

call the measure a

cost.

attached to the links.

We

We may also assume that

the entire measure is strictly increasing in the local
measures, and that an optimal junction tree is one of
minimal cost.

nents, and therefore the thinning chosen has no impact
on the next stage.

Hence, if we in the construction

have a secondary priority (cost, say), we can perform
the thinning by using Kruskal's algorithm according
to cost. In this way we will end up with a maximal
weight spanning tree of minimal cost (see Figure

3).

We conclude these considerations with

Theorem

2 Any minimal cost juncti on tree can be
constructed by successively choosing a link of max­
imal weight not introducing cycles, and if several
links may be chosen then a link of minimal cost is
selected.

A proof of Theorem
stages.

2

is an induction proof over the

The induction hypothesis is that at the end

of each stage, the forest consists of partial maximal
distance junction trees.

Remark 1

An analoguous algorithm based on Prim's

algorithm will also construct minimal cost junction
trees.

Let us take a closer look at the construction of junction
trees through Kruskal's algorithm. Let w, , ... , Wn be
the different weights of

G in

decreasing order. The al­

gorithm can be considered as running through n stages

Corollary 1 All juncti on trees over the same triangu­
lated graph have the same separators (also counting
multiplicity).

characterized by the weight of the links chosen. At the
end of stage

i,

all links possible of weight w,

have been chosen, and a forest

T},

. .

.

, T�,

,

... , Wi

of partial

Proof:

Consider stage i+ 1 (Figure

3).

A cycle can be

broken by removing any link of weight Wi+ 1· If

(U, V)

maximal weight spanning trees has been constructed.

with separatorS is removed, then all separators in the

Now, the task at stage

remaining paths between

i+ 1

can be considered in the fol­

lowing way. Add all links of weight Wi+ 1 to the forest,

and break the cycles by removing links of weight Wi+ 1 .

Any thinning will result in a forest of partial spanning

U

and

V

must contain S.

This means that any separator of weight Wi+ 1 on these
paths must equalS. By thinning we therefore remove
the same separators.

1

Optimal Junction Trees

•

363

For each separator, establish links to all cliques
and separators containing it.

•

For each separator (with multiplicity n), choose
n+ 1 links to supersets without introducing cycles.

Theorem 3 Any minimal cost Almond tree can be
constructed by successively choosing links for sepa­
rators of maximal weight, and if several links may
be chosen, take one of minimal complexity.

A proof of Theorem

(b)

(a)

3 is

an induction proof along the

same line as a proof of Theorem

2.

FIGURE 4.

(a) Contraction of the junction tree from Figure 1.

THE NECESSITY OF TRIANGULATION

(b) An Almond tree.

5

4

for constructing optimal junction trees given the tri­

In the former sections we gave an efficient algorithm

ALMOND TREES

angulated graph. Thereby all steps from DAG to junc­

Almond and Kong

(1993) suggest another type of junc­

tion tree. Compared to the junction trees in (Jensen
et al.,

1990),

they give some reduction in computa­

tional complexity.

tion tree is covered by efficient algorithms yielding an
optimal output-except for the triangulation. Since
this problem is N:P-complete, we cannot hope for an
efficient algorithm yielding an optimal triangulation.
It appears that a one-step look-ahead heuristic pro­

Observation 1 If n links have the same separator, the

vides the best triangulations. An alternative propaga­

communication scheme can be contracted (Figure 4a).

tion scheme is conditioning (Pearl,

1988).

The N:P­

complete part of conditioning is the determination of
In junction trees, each separator holds exactly one po­

a cut set for the DAG, and Becker and Geiger

tential table where the marginal last communicated

have given an algorithm which guarantees a cut set

is stored.

space no larger than the square of the space for an

In contracted junction trees, a separator

with n neighbours must hold at least n

-

1

potential

optimal cut set. Other schemes exist, like, e.g., arc­

1990); however, as has been shown
(1991), all known methods do in fact

tables to store marginals communicated from neigh­

reversal (Shachter,

bours. This means that there is no saving in space.

by Shachter et al.

There is, however, a saving in time, since a number of

contain a hidden triangulation.

marginalizations are avoided.

(1994)

Since belief updating in Bayesian networks is N'.P-hard

Observation 2 If a separator is a subset of another sep­

arator, they can be linked (Figure

4b).

(Cooper,

1990),

there is not much hope of finding a

scheme avoiding an N::P-hard step. However, Cooper's
result does not yield that any scheme will contain such
a step.

Cooper showed that through belief updat­

The type of calculations are the same for links between

ing, the satisfiability problem for propositional calcu­

separators as for links between separators and cliques.

lus

can

be solved, but it may still be so that a search

S,

for an optimal structure for belief updating is poly­

the number of supersets to which it shall be linked,
and for each link (S, S'}, we can associate a local cost

nomially solvable. Note namely that the space of the

Due to the corollary, we know for each separator

C(S, S').

Also, new schemes are proposed (Zhang and Poole,

Junction trees simplified through these two observa­
tions we call

cliques are exponential in their presentation.

Almond trees.

The construction of an

Almond tree may go as follows:

1992)

which may seem as if they avoid the triangula­

tion problem. We will in this section argue that

any

scheme for belief updating- meeting certain require­
ments- will contain a hidden triangulation. Then, if

•

From the triangulated graph, the set of cliques

the complexity ordering of the hidden triangulations

and the set of separators (including multiplicity)

follows the ordering in the original scheme, we can con­

This can be done through elim­

dude that if the scheme has a polynomially solvable

ination in the triangulated graph, but it is not

optimality problem, then the junction tree method ei­

important for our considerations.

ther provides more efficient solutions or '.P

is established.

=

N::P.

364

Jensen and Jensen

The considerations to come are somewhat specula­
tive and at places they need further precision. Hence,
we call the results 'statements' rather than theorems.
However, a reader looking for alternative propagation
methods can use them as guidelines preventing inves­
tigations of several alternatives.
FIGURE 5.

Specifications

A graph representing a general propagation task.

U {A, . . . , B} is a universe consisting of a finite set
of discrete variables. The joint probability P(U} is a
distribution over the configurations Xu =Ax · · · x B.
=

A local representation of P(U) consists of a set
{P(U,), ... , P(Un)}, where U, . . . , lin is a covering
of U, and P(U;,} is the marginal distribution of Ui.
A local representation can be visualized by a graph G
with the variables as nodes and with a link between
two variables if there is a Ui containing both; G is
called the representing graph.
The propagation task can be formulated as follows.
Let P 1(Ui) be substituted forP(Uil; ifP1(U} P(U} x
P1(Ui)/P(Ui) is well-defined, then calculate the new
marginals P1(U,), ... , P'(Unl·
=

By a scene for a propagation task, we understand a
universe U together with a covering U1, ... , lin such
that the covering equals the cliques in the representing
graphs. An instance of a propagation task is a pair
(G, P), where G is an undirected graph, and P is a set
of marginals of a joint distribution P(U) to the cliques
of G.
Let U be a universe. By a local method on U, we un­
derstand an algorithm working only on subsets of U.
More precisely: The algorithm consists of a control
structure and a fixed set Pr1, ... , Pr = of proce­
dures such that each Pri only processes information
on Vi c; U. We call Vi the scope of Pri. The repre­
senting graph G1 for a local method is defined as the
graph with U as nodes, and with links between vari­
ables if there is a scope containing them. Notice that
the cliques of G 1 need not be scopes.
We have defined a local method such that the control
structure mainly consists of controlling message pass­
ing between procedures. Note that between Pr;. and
Pri only information on Vi n Vi is worth passing.

First, we shall transform the problem to propositional
calculus.
Lemma 1 Let P(U,), ... , P(Uml be projections of
the joint probability table P(U). Let Pos(U} be the
table of possible configurations of U:

Pos(u} =

{�

if P(u) > 0
otherwise

{�

if P (ud > 0
otherwise

Define Pos(U;.) as:
Pos(u;,}

=

Then Pos(Ui} 1 if and only if Ui is a projection
of a possible configuration.
=

Proof: Since P(U;.) is the marginal of P(U), we have
that P(u;,} > 0 if and only if ut is the projection of at
least one configuration with positive probability.
1
The lemma shows that any scheme for belief updating
has the calculus of possible configurations in proposi­
tional calculus as a special case. So, if we can prove
Statement 1 for this calculus, we are done.
We shall start with an example which is the corner­
stone of the proof.
Example 1 Let the graph in Figure 5 represent a gen­
eral propagation task over the propositional calculus,
and let Pas be the potential giving 1 for possible con­
figurations and 0 for impossible ones.

Let PrAs, PrAc, Prsn, Prnc be procedures for solv­
ing the task {the index indicates the scope, see Fig­
ure 6).

A general local belief updating method for a scene
represented by G is a local method solving the propa­
gation task for each instance (G, P}.

We shall construct an instance which cannot be solved
by the procedures. For each variable we only use the
first two states. This means that all other states are
impossible.

We aim at the following:

Initially, we have for i, j ::; 2

Statement 1

Let G represent a scene, and let a gen­
eral local belief updating method be represented by
the graph G 1• Then G1 contains a triangulation
of G.

Pas( at, bj) 1
Pas(Ut, Cj ) 1
Pos(bi, dj) 1
Pos(ci,dj)=l
=

=

=

for all i, j
if and only if i
if and only if i
for all i, j

=

=

j
j

Optimal Junction Trees

365

Proof of Statement 1:
Assume that G 1 does not
contain a triangulation of G. Then there is a cycle C
in G such that the subgraph of G 1 consisting of the
nodes in C is not triangulated. Let C' be a chordless
cycle of length greater than three in that subgraph.
Let A 1 , ... , An be the nodes of C'.

FIGURE 6.

The scopes for the procedures and the communication
channels.
That is, A and C as well as B and D are forced into
the same state, and everything else is possible. Note
that the Pas-relations above are projections of the Pas­
relation over the universe:

if and only if
Pos ( U>. bi}

=

Pos ( ai, ck}
=

Pos(bj, df)

=

Pos(ck, de)

=

1

Now, assume we get the information that the config­
urations (a1, b2) and (a2, b1) are impossible. This is
equivalent to replacing the relation Pos(ai, bj) by
Pos1(ai, bj)

=

1

if and only if

i

=

j

(and i,j � 2).

Now, the propagation task is to determine Pos' (A, C),
Pos ' (B , D), and Pos ' (C, D) such that these local rela­
tions are projections of the unique universal relation
Pos'(A, B, C, D), satisfying the relations Pos'(A, B),
Pos(A, C), Pos(B, D), and Pos(C, D).
Clearly, Pos'(ai,bi>ck,dt)
l if and only if i
j
e, and therefore Pos'(ck.de)
l if and only
=

k

=

k=€.

=

=

=

if

The tool for achieving this result is the set PrA s ,
PrAc, Prso, and Prco of procedures. Since PrAB can
only process information on the variables A and B, and
PrAc can only process information on A and C, then
the only valuable information to communicate be­
tween the two procedures is information on A (see Fig­
ure 6). That is, between Pr1 and Prz with scopes V1
and V2, respectively, only information on V 1 n V2 need
to be communicated. The new relation Pos'(A, B) in­
troduces a constraint between the state of A and the
state of B, but since only information on A alone and
B alone can be communicated, the constraint cannot
be communicated to Prc0.
Note that if a cycle contains more than 4 variables, the
construction can be extended by clamping the states
of further intermediate variables.

We now can construct an instantiation, which cannot
be propagated correctly: (1) Let a configuration be
possible if and only if its projection to A1 x · · · x An
is possible. (2) Perform the construction as shown in
the example.
1
By the proof of Statement 1, we see that it can be
generalized to systems with other uncertainty calculi
like, e.g., Dempster-Shafer belief functions or fuzzy
systems. In fact, the reasoning can be applied to any
calculus having propositional calculus as a special case.
An axiomatization of these possible calculi is outside
the scope of this paper, but the axioms in (Shenoy and
Shafer, 1990) form a good starting point.
Concerning complexity we still have a couple of loose
ends. Although a general scheme involves a hidden tri­
angulation, the computational complexity needs not
be of the same kind as for the junction tree scheme.
In the junction tree scheme the complexity is propor­
tional to the number of configurations in the cliques.
Therefore a general local scheme has an equivalent
computational complexity if it is proportional to the
number of configurations in the scopes. This is the
case if each configuration has an impact on the mes­
sages sent in the algorithm. In this paper we shall not
give sufficient conditions for this to hold.
The second loose end has to do with optimality. A gen­
eral scheme is, e.g., to work with P(U) only. This cor­
responds to working with the complete graph over U.
This scheme has a trivial optimality problem, but the
junction tree method can do much better even for sub­
optimal triangulations. Therefore we conclude:

Statement 2 If a general local propagation scheme
has a complexity at least proportional to the num­
ber of configurations in the scopes, and its opti­
mality problem can be solved in polynomial time,
then either the junction tree scheme can do better
or 'J' = N'J'.

Acknowledgements
The work is part of the ODIN-project at Aalborg Uni­
versity, and we thank our colleagues in the group for
inspiring discussions.
The work is partially funded by the Danish Research
Councils through the PIFT-programme.

366

Jensen and Jensen




quacy of the model and the reliability of data used.

After a brief introduction to causal proba­

system comes up with. At least there will be kept

Therefore, no expert will blindly accept what the
bilistic networks and the HUG IN approach,

a critical eye on the data, and mainly one will

I

the problem of conflicting data is discussed.

look for conflicts in the data or conflicts with the

A measure of conflict is defined,

model.

I

MUNIN. Finally it is discussed how to dis­

I
I
I
I
I
I
I
I
I
I

and it

is used in the medical diagnostic system
tinguish between conflicting data and a rare
case.

In this paper we present

a

way of building such

a critical eye into a system with a CPN model.
Our suggestion requires an easy way of calculating
probabilities for specific configurations. We start
with a brief introduction to the HUGIN approach.

1

In section 3 we discuss CPN's and data conflict.

Introduction

In section

4

a measure of conflict is defined, and

it is shown that this measure is easy to calculate
It has for many years been widely recognized that

in HUGIN and that it supports a decomposition

causal probabilistic networks (CPN's), have many

of global conflict into local conflicts.. Section

virtues with respect to expert systems mainly due

reports on experience with a large CPN, and in

5

to the transparency of the knowledge embedded

section 6 we discuss how to distinguish between

and their ability to unify almost all domain knowl­

conflicts in data and data originating from a rare

edge relevant for an expert system (Pearl 1988).

case.

However, the calculation of revised probability dis­
tributions after the arrival of new evidence was
for a long period intractable and therefore an ob-

2

Causal probabilistic Networks

stacle for pursuing these virtues. Theoretical de­

and the HUGIN approach

velopments in the 80ies have overcome this diffi­
culty (Kim and Pearl1983, Lauritzen and Spiegel­

A causal probabilistic network (CPN) is con­

halter 1988, Schachter 1988, Cooper 1984, Shafer

structed over a

universe,

consisting of a set of

With the

states. The
variables. The universe is or­
ganized as a directed acyclic graph. The set of
parents of A is denoted by pa(A) . To each vari­

HUGIN approach efficient methods have been im­

able is attached a conditional probability table for

plemented for calculation of revised probability
distributions for variables in a CPN without di­

P(Ajpa(A)) .
Let V be a set of variables.

rected cycles (Andersen et a.l. 1989).

Cartesian product of the state sets of the elements

the results infered from the model rely on the ade-

tables are considered as functions and they are de-

and Shenoy 1989).

The Lauritzen and Spiegel­

halter method has been further developed to the
HUGIN approach (Andersen et al. 1987, Jensen
et al.

1990a, Jensen et al. 1990b)

.

As always when modelling real world domains,

nodes each node having a finite set of
nodes are called

in

V

and is denoted by

The

Sp(V).

space of V

is the

The probabilitie

547

noted by greek letters </> and 1/J. If A is a variable,
then ¢>A= P(A!pa(A)) maps Sp(pa(A)U{A}) into
the unit interval [0, 1]. It is convenient to consider
functions which are not normalized and take arbi­
trary non-negative values. So in the sequel, <P and
1/J denote such functions.
Evidence can by entered to a CPN in the form
of findings. Usually a finding is a statement, that
a certain variable is in a particular state.
After evidence has been entered to the CPN one
should update the probabilities for the variables in
the CPN. It would be preferable to have a local
method sending messages to neighbours in the net­
work. However, such methods do not exist when
there are multiple paths in the network.
The HUGIN approach which is an extension of
the work of Lauritzen and Spiegelhalter ( 1988)
(Jensen et al 1990a; Jensen et al 1990b) repre­
sents one way of achieving a local propagation
method also for CPN's with multiple paths. This
is done by constructing a so-called junction tree
which represents the same joint probability distri­
bution as the CPN.
The nodes in a junction tree are sets of variables
rather than single variables. Each node V has a
belief table <Pv : Sp(V) - Ro attached to it. The
pair ( V, <Pv) is called a belief universe.
The crucial property of junction trees is that
for any pair ( U, V) of nodes, all nodes on the path
between U and V contain U n V.
A belief table is a (non-normalized) assessment
of joint probabilities for a node. If S C V, then an
(non-normalized) assessment of joint probabilities
for Sp(S) can be obtained from <Pv by marginal­
ization: <Ps = E V\S <Pv
Evidence can be transmitted between belief uni­
verses through the absorption operation: ( U, <Pu)
absorbs from (V, <Pv ) , ... , (W, <Pw) by modifying
<Pu with the functions L: V\S <Pv, . .. , LW\U <Pw.
Actually, the new belief function <Pu is defined as
•

<Pu =

<P'u *

:Evw <Pv

LU\V </>u

*

w
. .. * E ww <P
LU\w <Pu

where the product ¢> * 1/J is defined as
(¢> * 1/J)(x)= ¢>(x),P(x)

Based on the local operation of absorption the two
propagation operations CollectEvidence and Dis­
tributeEvidence are constructed. When CollectEv­
idence in Vis called (from a neighbour W) then V
calls CollectEvidence in all its neighbours (except
W), and when they have finished their CollectEv­
idence, V absorbs from them (see figure 1).

I
I
I
I
I
I

- Direction of �bsorption

� Ca.ll of COLLECT EVIDENCE

Figure 1: The calls and evidence passing in Col­
lectEvidence
When DistributeEvidence is called in V from a
neighbour W then V absorbs from W and calls
DistributeEvidence in all its other neighbours.
Having constructed a junction tree, we need not
be as restrictive with findings as in the case of
CPN's:
Let V be a belief universe in the junction tree.
A finding on V is a function
Fv

:

Actually, the more general notion of likelihood can be

entered: Evidence is a function Ev :
not pursue this in the present paper.

Sp(V)

I
I
I
I
I
I
I

Sp(V)- {0, 1}

So, a finding is a statement that some configu­
rations of Sp(V) are impossible. Note that the
product of two findings f : Sp(V) - {0, 1} and
9 : Sp(W) - {0, 1} is a finding f * 9 : Sp(V U
W) - { 0, 1}, and f* 9 corresponds to the conjun­
cion f 1\ g.
Using the HUGIN approach, it is possible to en­
ter findings to the CPN (or the junction tree)1,
update the probabilities for all variables, and to
1

with ¢> and 1/J extended to the relevant space (if
necessary).

I

- &.

We will

I
I
I
I
I

548

I
I
I
I
I
I
I
I
I
I
I

I
I
I
I

I
I
I
I

Following the tradition in probabilistic reason­
achieve joint probability tables for all sets of vari­
ables which are subsets of nodes in the junction ing to take examples from California, where bur­
tree. The method has proved itself very efficient glary and earthquake are everyday experiences, we
even for fairly large CPN's like MUNIN (see Ole­ have constructed the following example:
sen et al. 1989, Andersen et al. 1989).
When Mr. Holmes is at his office he fre­
The main theorem behind the method is the fol­
quently gets phone calls from his neigh­
lowing.
bour Dr. Watson telling him that his
burglar alarm has gone off, and Mr.
Theorem 1
Holmes rushing home hears on the ra­
dio that there has been an earthquake
Let T be any junction tree over the universe U,
nearby. Knowing that earthquakes have
and let <Pu be the joint probability table for U.
a tendency to cause false alarm, he then
has
returned to his office leaving his
(a) If CollectEvidence is evoked in any node
neighbours
with the pleasure of the noise
V and <Pv is the resulting belief table,
from the alarm. Mr. Holmes has now in­
then <Pv is proportional to LU\ v <Pu.
stalled a seismometer in his house with a
(b) If further, DistributeEvidence is evoked
direct line to the office. The seismometer
in V, then for any node W the result­
has three states:
ing belief table <Pw is proportional to
LU\W ¢U·
0 for no vibrations
0

Before we proceed with data conflict, we will state
an observation proved in Jensen et al. (1990b),
but first noted by Lauritzen and Spiegelhalter
(1988) in their reply to the discussion.

1 for small vibrations (caused by
earthquakes or passing cars.)
2

for larger vibrations (caused by ma­
jor earthquakes or persons walking
around in the house.)

The CPN for this alarm system is shown
in figure 2:

Theorem 2

One afternoon Dr. Watson calls again
Let T be a junction tree with all belief tables nor­
and tells that the alarm has gone off. Mr.
malized, and let x, . . , y be findings with prior
Holmes
checks the seismometer, it is in
joint probability P(x * ... * y). Enter x, . . . , y to
state
0!
T and activate CollectEvidence in any belief uni­
verse for V. Let <l>v be the resulting belief universe
From our knowledge of the CPN, we would say
for V.
that
the two findings are in conflict. Performing
0
Then :Z::::v <l>v = P(x * .. . * y).
an evidence propagation does not disclose that.
The posterior probabilities are given in figure 3.
Only
in the rare situations of inconsistent data, an
CPN's and data conflict
3
evidence propagation will show that something is
A CPN represents a closed world with a finite set wrong. The problem for Mr. Holmes is whether
of variables and causal relations between them. he should believe that the data originate from a
These causal relations are not universal, but re­ rare case covered by the model, or he should reject
flect relations under certain constraints. Take for that.
From a CPN m_pdel's point of view there is no
example a diagnostic system which on the basis of
blood analysis monitors pregnancy. Only diseases difference between a case not covered by the model
relevant for pregnant women are represented in and flawed data. So what we can hope for to pro­
the model. If the blood originates from a man, the vide Mr. Holmes with is a measure indicating pos­
constraints are not satisfied, and the case is not sible conflicts in the data given the CPN.
In MUNIN (Olesen et al. 1989) an attempt to
covered by the model . A similar situation appears
incorporate conflict analysis in the CPN is made.
if the test results are flawed (e. g. red herrings).
.

549

This is done by introducing 'other'-states and
'other'-variables. In the example of Mr. Holmes'
alarm system, an 'other'-variable covering lighten­
ing, flood, baseballs breaking windows etc. could
be introduced to represent unknown causes for the
alarm to go off, and the Burglar variable could
have an 'other'-state covering Mr. Holmes' mis­
tress having forgotten the code for switching off
the burglary alarm.
Though this approach is claimed to be fairly
successful, it raises several problems. First of all
there is a modelling problem. The effect of an
'other'-statement is hard to model without know­
ing what 'other' actually stands for . What should
the conditional probabilities be? In fact, these
Burglary: <I>B : (50, 50); Earthquake: ¢E(90, 10)
probabilities were in MUNIN constructed by feed­
ing the network with conflicting data and thereby
tuning the tables as to make 'other' light up ap­
E
<l>s
propriately.
y
N
A second problem is that conflict in data is a
N (97, 2, 1) ( 1, 97, 2)
global property, and the introduction of 'other'­
B
statements in the CPN gives only a possibility of
y (1, 2, 97) (0, 3, 97)
evaluating
evidence locally. In order to combine
Seismometer
the local 'other' statements to a global one, the
CPN has to be extended drastically.
E
<I>A
This leads to the third major problem, which
N
y
is more of a technical kind. The introduction of
N (99, 1) (1, 99)
'other'-statements to the CPN can cause a dra­
B
matic increase in the size of the junction tree. Be­
y (1, 99) (0, 100)
Alarm
sides, the technique with 'other'-states is hard to
use if the variables are not discrete.
Figure 2: Mr. Holmes' Alarm system with seisAnother approach has been suggested by
mometer.
Habbena ( 1976). It consists of calculating a sur­
prise index for the set of findings. Essentially, the
surprise index off: V --. {0, 1} is the sum of the
probabilities of all findings on V with probabilities
no higher thanf's.
Habbena suggests that a threshold between 1%
and 10% should be realistic. In the seismometer
E
<I>E,B
case, the surprise index for (a, s) is 3%. However,
N
y
the calculation of a surprise index is exponential in
N .47 .05
the number variables in V and must be considered
B
as intractable in general.
y .48 0
Figure 3: Joint probabilities for earthquake and 4
The conflict measure conf
burglary posterior to a : 'alarm = Y' and s : 'Seis­
Our approach to the problem is that correct find­
mometer = 0'.
ings originating from a coherent case covered by
the model should conform to certain expected pat­
terns. If x, · · ·, y are the findings, we therefore

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

550

I
I
I
I
I
I

I
I

should expect:
P(x * ···* y)

>

P(x)

x

·· ·

x

P(y)

Hence we define the conflict measure conf as:

conf(x, ···,y) =log

P(x) X··· XP(y)
P( X*. * y)
•

.

(where log is with base 2).
This means that a positive conf(x, · ··,y) is an in­
dicator of a possible conflict.
For the data in section 3 we have conf(a, s ) =
4.5.
Using theorem 2, conf(x, · ··,y) is very easy
to calculate in HUGIN. The prior probabilities
P(x),···,P(y) are available before the findings
are entered, and P(x,· ··,y) is the ratio between
the prior and the posterior normalizing constant
for any belief universe.

I

P(x. y,

z. u.

consists of two sets of findings, namely {x,y,z}
and { u,v}. Since the product of findings is also a
finding, we can say that the two findings x * y * z
and u * v meet in V.
The conflict in the data meeting in V is therefore
composed of the conflict between x*y*z and u*v
, the conflict inside {x,y,z} and inside {u,v}. It
is easy to show that:
conf(x, y,z,u,v) =conf(x * y * z, u *z)
+conf(x, y,z) + conf(u,v)
Furthermore, as indicated at figure 4, P( x *y * z)
and P(u * v) can be calculated as ratios between
prior and posterior normalizing constants, and
therefore conf(x,y,z) and conf(u,v) as well as
conf(x * y * z, u * z) are easy to calculate.
In general: If evidence is propagated to any belief
universe U from neighbours V, ···, W originating
from findings (v, .. ·v') . . . ( w,·· ·, w') respectively,
then

vl

conf(v,· · ·,v',··· , W , ··· W 1) =

I
I

conf(v * ···* v',· ·, w * · · ·* w')
·

+conf(v,···, v') + · ·· + conf( w, ···, w')
All terms are in HUGIN easy to calculate by use
of Theorem 2.
We call conf(v,· ··, v',···, w, ·· ·, w') the global
conflict and conf(v * ·· ·* v',· ··, w * · · ·* w') the
local conflict.
The calculation of conf has been implemented
in HUGIN to follow the calls of CollectEvidence.
The overhead to the propagation methods m
terms of time and space is neglectable.

P(I. y.z)

I
I
t

I

I
I
I
I

t

y

t
l

t
u

t

v

Figure 4: A junction tree with findings x, y, z, u,v
entered. Theorem 2 provides the joint probabili­
ties indicated at nodes V, U, W' and W".
The conflict analysis can be further refined. In
figure 4 is shown a junction tree with findings
x,y, z, u, v entered. If CollectEvidence is evoked
in the node V, then the evidence flowing to V

5

Example: APB-MUNIN

The conflict measure has been tested on small
fictions examples and on a large subnetwork of
MUNIN, namely the network for the muscle Ab­
ductor Pollicis Brevis (APB). The network is
shown in figure 5.
The rightmost variables in figure 5 are finding
variables. This means that evidence is entered at
the right hand side of the CPN and propagates to
the left. However, as described in section 2, the
propagation takes place in a junction tree of belief
universes. In the test, CollectEvidence was called

551

I
I
I
I
I
I
I

5: The DAG in
ductor Pollicis Brevis.

Figure

MUNIN for

I

Medianus Ab­

The attached numbers in­

entered (see figure 6.)

I

in universe number 59, and the call propagates

I

dicate the belief universe to which the finding is

recursively down the junction tree. In figure 6 is
shown the junction tree.

I

(Only belief universes

where evidence meet are shown).
First we asked the model builder, Steen An­

I

dreassen, to provide us with a complete set of nor­
mal findings. They were entered, and global and
universal conflict values were calculated. The re­
sults are shown in figure
a global conflict of

7.

23.3 for

I

Surprisingly we got
the entire set of find­

ings and apparently the conflict can be traced to

I

belief universe no. 45. Further, the evidence from
15 and 17 looks conflicting.

Returning to Steen

Andreassen with our surprise, he recognized that

I

he had given us a wrong value for the finding
qual.mup.amp. which was entered to belief uni­
verse 15. It should have been

540 J.LV

rather than

200 pV.

We entered the corrected finding and got a
global conflict value

-1.5

for the entire set of find­

ings with local and subglobal values ranging be­
tween

0

and

-1.4.

Then typical findings for a patient suffering
from moderate proximal myopathy were entered.
As can be seen in figure 8, this resulted in large

Figure 6: The part of the MUNIN junction tree
for APB where evidence meet. The numbers are
labels of belief universes. Bold numbers indicate
entrance of findings.

I
I
I
I'

552

I
I

negative conflict values confirming the coherence
of the findings.

I
I
I
I
I
I
I
I
I

Figure

I

I
I
I
I
I

Typical findings for a patient s uffer ing

Finally, we simulated hypothesizing.

We en­

tered a set of findings originating from a healthy
patient, and we also entered the disease state

I
I

8:

from moderate proximal myopathy entered.

'moderate proximal myopathy'.
shown in figure
Figure 7: The conflict measures from the first test
example. The italiced values are local conflict val­

ues and the bold figures are the global ones.

9.

The result is

The disease finding is entered to

belief universe 58, and it can be seen that the dis­
ease does not contradict a couple of normal find­
ings, but indeed the whole set.

6

Conflict or rare case?

It can happen that typical data from a very rare
case might cause a high value of conf. In the case
of Mr. Holmes' alarm system a flood (with proba­

10-3 could be entered to the CPN explaining
the data (see figure 10).
For this system we get conf(a, s) = 4.5. It is

bility

still indicating a possible conflict. The reason is
that though P(a, s) is possible, it is under the

553

rare

Mr.

condition of flood.

of the window.

Holmes looks out

It rains cats and dogs, and he

has resolved the problem; the model gives a

P(Flood )

I

n ew

0.84.

=

The problem above call s for more than a pos­
analysis .

refined conflict

sibility for

We need a

method to point out whether a conflict

can

be ex­

plained away through a rare cause.

(x,.. .,y) be findings with a positive conflict
H be a hypothesis which could
explain the findings: conf ( x, . . . , y, H) < 0
Let

measure, and let

We have

con1'r( x, . . . , y, H) =

log

P(x) x . . . x P(y) x P(H)
P( X* .. ·*Y*H )

=

conf(x, ..., y)+ log

P(H )
P(HIx,...,y )

og

then

P(Hjx,.. . ,y)
P(H)

H can

> conf( x,

explain away the

variables (in

conflict.

t he flood example the

value is 5.6). This means that there is no need
Figure

9:

Findings for a healthy patient, and

the hypothesis 'moderate proximal myopathy' en­
t er ed.

for manually to formulate explaining hy pot hesis
in terms of states of variables. More complex hy

­

pot hesis can also be monitored if they can be ex­

pressed as findings.

7

Conclusion

· · ·

, y)

=

log

P(;�

P( Y)
X* ... * y)
x

·

·

·

x

has many promising properties. It is easy to cal­

culate in HUGIN, it is independent of the order in

which fi nd ings are entered , it can be used for both
global and loc al analysis of conflicts in data, and

it has a natural interpretation which supports the

usual mental way of inspecting data for flaws or
for originating from sources outside the scope of
the current investigation.
Figure 10: Mr. Holmes' revised CPN.

I
I
I

I
I
I
I

The measure o f confli ct

con f ( x,

I

I

. . . , y)

The left-hand ratio can be monitored automat­
ic ally for all

I

I

This means that i f

1

I

However, still some practical and theoretical
work is needed in order to understand the signifi­
cance of specific positive conflict values. Also, the

I
I
I
I
I
I

554

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

detailed conflict analysis a it is nowconnected to Kim, J. H. and Pearl, J. (1983). A computational
the structure of the junction tree rather than to model for causal and diagnostic reasoning in infer­
the CPN itself. This should be relaxed.
ence systems. In Proceedings of the 8th Interna­
tional Joint Conference on Artificial Intelligence.,

8

Acknowledgements

190-193.

Lauritzen, S. L. and Spiegelhalter, D. J. (1988) P.
We thank Steffen Lauritzen for many valuable dis­ Local computations with probabilities on graphi­
cussions on the subjects of this paper, and Steen cal structures and their applications to expert sys­
Andreassen for helping with the MUNIN experi­ tems (with discussion). J. Roy. Statis. Soc. B.
ment.
50, 157-224.
Olesen, K. G., Kjrerulff, U., Jensen, F., Jensen, F.
V., Falck, B., Andreassen, S. and Andersen, S. K.
(1989). A MUNIN network for the median nerve
Andersen, S. K., Jensen, F. V. and Olesen, K. G. - a case study on loops. Applied Artificial Intel­
(1987). The HUGIN core- preliminary consider­
ligence 3, 385-404. Special issue: Towards Causal
ations on systems for fast manipulations of prob­ AI
Models in Practice.
abilities. In Proceedings of Workshop on Induc­

9




When expert systems bas ed on causal pr obabilistic
networks (CPNs) re ach a cer tai n size and complex­
ity, the "combinatori al explos ion monster" tends to
be present. We propose an approximation scheme
that identifies rarely occurring cases and excludes
these from being processed as ordinary cases in
a CPN-based expert system. Depending on the
topology and the probability distributions of the
CPN, the numbers (representing probabilities of
state combinations) in the underlying numerical rep­
resentation can become very small. Annihilating
these numbers and utilizing the resulting sparseness
through data structuring techniques often results in
several orders of mag nitude of improvement in the
consumption of comp uter resources. Bounds on the
errors introduced into a CPN-based expert system
through approximations are established. Finally, re­
ports on empirical studies of appl yi ng the approxi­
mation scheme to a real-world CPN are given.
Keywords: Approximative reasoning, belief net­
work, causal probabilistic network, expert system,
knowledge-based system, influence diagram, junc­
tion tree, probability propagation, reasoning under
uncertainty.
1

Introduction

Expert systems, using causal probabilistic networks
(CPNs)1 for knowledge representation, are reaching
the state where it is feasible to handle domains mod­
eled by large-scale networks (e.g., MUNIN [Andreas­
sen et al., 1987; O lesen et al., 1989]). When building
such large networks, it is (for reasons of practicality)
often necessary to introduce approximations besides
those inherent in the process of modeling a domain.
Two main approaches have been i�westigated: fo­
cusing on the development of an approximative al­
gorithm for propagation of information (e.g., [Hen­
rion, 1989]), and focusing on approximations in the
1 Synonyms:

belief

networks, causal networks, and

probabilistic influence diagrams.

underlying network representation and then using
exact inference algorithm.
The objective of this paper is to p res ent. a.n ap­
proximation scheme that takes t.he latter approach.
T he scheme is tailored to the Bayesian belief uni­
verse approach [Jensen et a/., 19t\9] as used in HUGIN
[ Andersen et al., 1989]. The met hod operates by ap­
proximations in the quantitative part. of t he underly­
ing representation, whereas the qualit.at.ive structure
remains unchanged. Within thi� framewor k , we can
assess the accuracy of the appr o xim ate d probabili­
ties, which is not possible with heuristic methods.
Application of the method ofte n results in a. sub­
stantial decrease in the usage of computer resources;
the amount of decrease depends on domain charac­
teristics, such as network topology and prob ability
distributions.
It is known that, in general, probabilistic infer­
ence in CPNs is NP-hard [Cooper, 1987], and ex­
act calculations will eventually become intractable.
This fact emphasizes the importance of approxima­
tive methods.
A domain model in the causal probabilistic net­
work approach consists of a graph with nodes repre­
senting the domain variables and the (directed) ar cs
representing the causal relatious between the do­
main variables. Conditional probabilities are used to
describe the dependency of domain variables given
their immediate predecessors (parents). Different
inference methods have been developed to propa­
gate information in such a network: If the topology
is simple (singly connected) [Pearl, 1986], propaga­
tion can be done directly in tht> CPN; otherwise, a
secondary structure for topologies, including nondi­
rected loops [Lauritzen and Spiegelhalter, 1988;
Jensen et al., 1989; Shafer and Shenoy, 1988], can
be used. Alternatively, for the btter kind of topolo­
gies, the inference could also take place in a set of
conditioned networks [Suermondt aud Cooper, 1988]
or through manipulation of the uetwork with an arc­
reversing technique [Shachter, 1988].
The method of Bayesian belief universes splits the
inference task into two phases: a compilation phase
and a run-time phase. The proposed approximaan

163

tion scheme adds another phase to this task: The
approximation and compression phase. The phases
are thus
Based on the CPN
• The compil ation phase:
domain model, a secondary structure is con­
structed-a so-called junction tree of belief uni­
verses.
•

The approximation and compression phase:

Small numbers, representing the probabilities
of very rare cases, are annihilated (set to zero),
thereby effectively eliminating these cases from
the domain model. Through use of data struc­
turing techniques for sparse tables, the under­
lying numerical tables (the belief tables) of the
junction tree are compressed.
• The run-time phase: The actual inference takes
place in the junction tree, using the modified
belief tables.
In Section 2, we review the basic belief universe
concepts essential for the proposed approximation
scheme. Section 3 describes how to perform the
approximation and establishes some worst-case er­
ror bounds on probabilities obtained from the ap­
proximated junction tree. Finally, Section 4 reports
empirical results we obtained by applying the pro­
posed approximation scheme to a real-world CPN­
namely, one of the networks of the MUNIN knowledge
base.
2

Belief Universes

This section reviews some of the basi c concepts of
the belief universe approach.
The domain represented by the CPN is divided
into a set of subdomains called belief universes. A
belief universe U consists of two parts: a set o f
nodes2 and a belief table, which contains an assess­
ment of the joint probabilities for the state space
of U (i.e., the Cartesian product of the state sets for
the nodes of U).
The construction of a system of belief universes,
equivalent to the original CPN domain model, con­
sists of the following steps:
• Form the moral graph: For each node in the
network, add links between all of its parents
that are not already li nked. Drop the directions.
• Triangulate3 the moral graph: Add links to the
moral graph until a triangulated graph is ob­
tained.
• Form the system of belief universes: The node
sets are the cliques4 of the triangulated graph.
2We sha.ll use U to denote both the belief universe
itself and its set of nodes.

3 A graph is triangulated if every cycle of length
greater than three has a chord.

4 A clique is a maximal set of nodes, a.ll of which are
pairwise linked.

The initial belief tables are calculated as ap­
propriate products of the conditional probabil­
ity tables [Lauritzen and Spiegelhalter, 1988;
.Jensen et al., 1989].
•

Organize the system as a junction tree: Links

between belief universes are introduced, such
t.bat a tree with the following property results:
For each pair (U, V) of belief universes, each
belief universe on the unique path between U
and V contains the nodes U n V. As shown
in [.Jensen, 1988], a junction tree can be con­
structed by a maximal spanning-tree algorithm.
All steps except the second are deterministic: There
is only one moral graph, and the set of cliques of a
triangulated graph is unique. There may be several
junction trees, but the differences among them are
minor (the major cost of a junction tree is the repre­
sentation of t.he belief tables for the belief universes).
The second step is important: A good triangulation
can save substantial space and time [Kjrerulff, 1990].
Let U he a belief universe with belief table B, and
let S C U. We can obtain the joint probabilities
for S from B by summing up all beliefs in B for S.
This operation is called marginalization. In partic­
ular, the belief in a single node can be obtained by
marginalization of the belief table of any belief uni­
verse containing it.
Let U be a belief universe, and let V � U. A
5
finding on 1/ is a subset of the state space of V.
The finding is entered into U6 through annihilation
of the elements in the belief table of U corresponding
to state combinations not in V.
A set of one or more findings is called a case.
A junction tree is said to be consistent if marginal­
ization of two distinct belief universes U and U' with
respect to some set of nodes V (contained in both U
and U') yield "identical" (i.e., proportional) results.
This property is (re)established through the global
propagation operation. This operation refers to a
local propagation method for transmitting evidence
between neighbors in a junction tree.
Absorption is the local propagation method: If
we have entered evidence into a belief universe V,
then an adjacent belief universe U absorbs from V
through the following steps:
1. Calculate the belief table for U n V by marginal­
ization of the belief table of U.
2. Calculate the belief table for the same intersec­
tion by marginalization of the belief table of V.
3. Multiply the belief table of U by the ratio of the
table achieved by Step 1 and the table achieved
by Step 2.
5Typically, a finding is a statement that a node is
known to be in a particular state.
6We shall also use the phrase "evidence is entered
into U."

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

164

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

When absorbing from several neighbors simultane­
ously, these steps must proceed in "parallel" (imply­
ing use of the same version of the belief table of U
in Step 1).
Global propagation is described in terms of
two operations: Col/ectEvidence and Distribute­
Evidence. CollectEv·idence is used when evidence
from the entire system must be propagated to a sin­
gle belief universe U: U asks neighbors to Collect­
Evidence; when they are done, U absorbs from them.
DistributeEvidence is used when evidence from a sin­
gle belief universe U must propagate to the entire
system: U asks each neighbor to absorb from U and
then DistributcEvidence to its other neighbors.
A global propagation operation consists of Collect­
Evidence operation followed by a DistributeEvidence
operation initiated from an arbitrary belief universe.
CollectEvidence has an important property. As­
sume that we have a consistent and normalized junc­
tion tree, and that. we enter evidence into some of
the belief universes of the junction tree. If we in­
voke CollectEvidence from some belief universe U,
then the normalizing constant for the belief table
of U, after CollectEvidence has terminated, is equal
to the (prior) probability of the evidence.
3

The Approximation Scheme

As described in the previous section, the numbers
in the belief tables of the belief universes repre­
sent probabilities in joint probability distributions.
One might expect that excluding the smallest num­
bers (representing rare state combinations) will lead
to substantial improvements in the requirements of
computer resources. In this section, we shall inves­
tigate some properties of such a scheme.
Assuming we have a consistent junction tree, an
approximation is performed in the following way:
1. For each belief universe in the junction tree, we
select some elements of its belief table and an­
nihilate those; the rest are left unchanged.
2. The junction tree is made consistent again by a
global propagation.
3. (Optional] The belief tables of the belief uni­
verses are compressed in order to take advan­
tage of the introduced zeros. (This step will
not be described here; see [Jensen and Ander­
sen, 1990] for details.)
How Do We Select the Numbers to Be
Annihilated?

As previously mentioned, we are interested in the
small numbers. A simple way to do tl,e selection is to
use a threshold value to separate the numbers to be
annihilated from the numbers to be kept. However,
we cannot choose a global threshold value, as the
size of tables and their distribution of numbers may
vary substantially. So instead we shall use a local
threshold value for each table.

We observe that, annihilating an element of a be­
lief table, corresponds to entering a finding that says
that the state combinations, corresponding to this
element, are "impossible" (or are considered unin­
teresting). Moreover, the sum of the annihilated el­
ements in a given belief table is t.he probability of all
the state combinations (the finding) corresponding
to those elements. This probability is a measure of
the (local) error, we commit.. We can control this
error by choosing a suitable threshold value.
Suppose we want to retain 1 s of the probability
mass of each belief table. Then, a simple method is
to compute a threshold value 6 by repeatedly halv­
ing 5 (using c: as the initial value for li) until the
sum of the elements less than li is no greater than c:;7
these elements will be annihilated (we believe that
either all or no elements with the same value in a
given table should be eliminated) . A more costly
method is to sort the elements of the table and to
repeat. annihilating the smallest. number(s) as long
as the sum of the annihilated numbers does not ex­
ceed c:.
The global errore (the total amount of probability
mass removed) is computed as t = 1- J.l, where J.L is
the normaliza.tion constant. found during the global
propagation step of the approximation algorithm.
Given an arbitrary case, we can determine if it is
one of the cases that have been completely excluded
from consideration by detecting a zero normalization
constant. The probability of such a case occurring
(assuming the assessed conditional probabilities are
correct) is e.
For each remaining case, some of the state com­
binations supporting the case may have been elimi­
nated. The accumulated probability for those state
combinations determines the error on the posterior
probabilities as shown in the following.
-

How Good Is the Approximation?

Assume that we have approximated the belief
universes and have propagated the approximations
throughout the junction tree. We now have a con­
sistent junction tree.
Let A denote the approximation performed, and
let F denote a set of findings to be entered into
the (consistent) approximated junction tree. Enter­
ing such a set of findings is a common operation
when using the junction tree (or rather the under­
lying CPN) as an expert system. After F has been
entered, and the junction tree has been made con­
sistent by propagation, we want to query the sys­
tem for probabilities of the form P(HIF), where
H is some hypothesis. 8 However, the probabil7This method is used in Hugin [Andersen et al.,

1989].

8ln a real application, the CPN might model the re­
lationships between some diseases and the associated

symptoms; F then would be the set of symptoms found,
H typically would be of the form "the patient has dis­
ease X," and

P(HIF)

would denote the probability that

I

165

ity P(H/F) is not available; instead, we get the
probability P(H/F, A) (that is, the probability for H
given the findings F and the appr oximation A).
We therefore want to find an upper bound on

jP(H/F)-P(H/F, A)j:

/P(H/F)- P(H/F, A)j
/P(H/F, A)P(A/F)
+ P(H/F, A)P(A/F)-P(HJF, A)j
\P(H!F, A)[P(A/F)-1] + P(H\F,A)P(A\F)/
P(A/F)/P(H/F, A) - P(HJF, A) I
� P(A\F)
=

=

=

The quantity

P(A/F) can be rewritten as

P(F n A)
P(Fn A)+ P(F n A)
P(F n A)
<
P(F n A)+ P(F\A)P(A) =

4.1

e
-----

e + p.(l-

e

)

where e = P(A) and JJ = P(F\A). These quanti­
ties are known: e is the appr ox imation error found
at approximation time, and JJ is the normalization
constant found during propagation of F. Unfortu­
nately, JJ is almost always small (� e), so this upper
bound is not a good indicator of the approximation
error.
In practice, however, F is almost. always of the
form !I n . . . n fn, where fi (1 � i � n ) states that
"node Xi is in state Yi ." Thus

P(F n A) S min{ P(fl n A), ... , P(fn n A)}
We can compute these quantities for all combina­
tions of nodes and states at approximation time (the
space required to store these quantities is small).
Although this gives us a better upper bound for
the approximation error, it is, however, strictly a
worst-case bound, and we may have to rely on em­
pirical studies to determine the actual errors. In the
next section, we shall investigate this issue for a real
application.
4

electromyographic findings, this model is cap abl e of
diagnosing three local nerve lesions and one diffuse
disorder in the median nerve in the arm. The CPN
contains 57 nodes; the disease nodes each have be­
tween three and five states, and the finding nodes
have from 15 to 21 states.
The specification of the conditional prob ab ili t.y ta­
bles requires 8126 numbers, of which 67.1 pe rc en t. are
assessed as zeros; however, most of these numbers
have been generated by local models from a much
smaller set of parameters, which has been assessed
by domain experts [ Andreassen et al., 1987].
An explanation of the domain concepts, as well
as a description of the medical performance, can
be found in [ Andreassen et al., 1989; Olesen et a/.,
1989].

An Application

We shall use a network from the MUNIN knowledge
base to study the effect of the proposed approxima­
tion scheme on a real-world CPN.
The domain ofMUNIN is electromyography, a tech­
nique for diagnosing peripheral ·muscle and nerve
disorders. We have chosen a network describing dis­
orders in the median nerve. 9 On the basis of four
the patient has disease X given that he/ she exhibits the
symptoms F.
.
9
It is our impression that this network is a "typical"
network, in the sense that the benefits of approximation
are neither negligible nor excessively large

Junction Trees

Based on different triangulations of the median­
nerve CPN, we have created four junction trees,
yielding different starting points for approximation.
We have used a maximum-cardinalit.v seatch [Tar­
jan and Yannakakis, 1984] and two h�uristic search
strategies that minimize the clique cardinality (the
min-size heuristic) and the size of the state space of
the nodes in the cliques (the min-weight heuristic),
respectively; see [ Kjrerulff, 1990] for details.
Triangulation Method
Clique
Size
14
13
10
9
8
7
6
5

MaxCard 1

MaxCard 2

MinSize

lVIin-

Weight

Number of Cliques

I
I
I
I
I
I
I
I
I
I

1
2
1
1
4
4
2
9

6
7
4
2

3
2
5
7

3
2
4
9

I

4849

10.7

1 .6

1.6

Zeros
(Percent)

I

93

71

77

Max Statespace (106)

4.0

0.45

0.54

Total Statespace {106)

Table 1: Statistics of junction trees for the median­
nerve knowledge base generated from different tri­
angulations.
Table 1 summarizes key parameters of junction
trees, based on different triangulations. We have ob­
tained two maximal-cardinality searches using differ-

I

I
I
I
I
I

166

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

ent starting nodes. However (for obvious reasons),
we consider only the second one, referred to as "max­
card," in the following subsections. The data in Ta­
ble 1 apply to the initial consistent (i.e., after ini­
tialization) junction trees before any approximation
or compression has been done.
4.2

Effect on Resources

We shall focus on two aspects of resources: (1) the

propagation time needed to make the junction tree
consistent after a set of findings has been entered,
and (2) the storage space needed to represent the
knowledge base in a suitable compact form (see [.Jen­
sen and Andersen, 1990] for details).
The global error e, defined in Section 3, is used
to characterize the approximation; we shall use the
term total removed probability mass to refer to this
value.
U)
Q)

108

�----�

>.
:3
Q)
(.)

co
a.
(f)

/

/

/

/

/

l>
0
�--

/

/

/

0

:J
rr
Q)
a:

10 5 �---.--.-�,�.---,--.-.,-rorrl
100
10
1
Propagation Time (seconds)

Figure 2: The relation between required storage
space and propagation time for the median-nerve
knowledge base for various approximations. The line
corresponds to a linear relationship between propa­
gation time and storage space.

Min-Weight
Min-Size
Max-Card
Compression

10 6+-����
1
10
Propagation Time (seconds)

Figure 1: The effect of compression on required stor­
age space and propagation time for the median-nerve
knowledge base.
The time and space measurements reported are for
an implementation of HUGIN [Andersen et al., 1989]
in C for a Sun 3 workstation; however, we are only
interested in relative improvements, so the space and
(in particular) time units should be regarded as ar­
bitrary.
Figure 1 illustrates the effect of the initial com­
pression on required storage space and propagation
time for three different junction trees. As expected,
the gain varies according to the different ratio of ze­
ros in the junction trees (see Table 1).
Figure 2 shows the relation between propagation
time and storage space needed for the three different
triangulation methods at different approximations.
The total removed probability mass (e) varied be­
tween 0.001 and 1 percent. At each data point, the
corresponding approximated and compressed run­
time system was created, and the time and space
·

U)

107

Q)

---�
�-

Unapproximated Values

>.
:3
Q)

c.
�
(f)
Q)

106

g>

a

"0
Q)
·=
:J

105

g

a:

-a--

Min-Weight

--

Min-Size

-o--

Max-Card

104
10-4

1 o·3

10"

2

1
10"

10°

10

Total Removed Probability Mass

1

10

2

(%)

Figure 3: The space requirement as a function of
the probability mass removed for different junction
trees. The arrows indicate the storage requirements
for unapproximated but compressed junction trees

I

167

cha.racteristics were measured. \Ve observe a linear
relationship between propagation time and storage
space needed; thus, we cha racterize resource require­
ments in term of storage space only.
The resource requirements for approximatedjunc­
tion trees as a function of the total removed prob­
ability mass is the subject of Figure 3. Each data
point in this figure corresponds to a data point in
Figure 2, except. for points corresponding to e > 2
p ercen t.. The values corresponding to no approxi­

mation

for the compressed
indicated.

junction trees are also

We observe that, fore less than �0.1 percent, the
approximation is equally efficient for the three junc­
tion trees. For each junction tr ee , e = 0.25 percent
yields about one order of magn itu de in reduction of
the required space. However, for a sufficiently large
value of e, the differences between the junction trees
disapp ear.
Table 2 shows the effect. of the method applied to
the different junction trees at e = 0.1 percent.

Triangulation

MaxCard

MinSize

Method
MinWeight

Space
Initial ( Mby t.e s)
Approx. (Mbytes)

Reduction

46

8.5

7.2

0.95
0.989

0.71
0.916

0.60
0.916

Figure 4 di sp lay s the results of entering a typical
case into various approximated junction trees. The
p r obability of the case is 4.1 x 10-4•

The observed error in the beliefs caused by the
approximation is shown as a funct ion of the total
removed probability mass (e). The figure shows ob­
served errors in the beliefs of states representing ex­
act beliefs between 0.9189 and 0.0005. The worst­
case error bound (Section 3) for each approxi mation
and case also has been computed. '�'e ob se rve that
the difference between the worst-case bound and t he

worst measured absolute error is about
of ma gnit ude for e :::; 0.1 percent.

Approx. ( seconds)
Reduction

1100
9.5
0.991

213
7.1
0.967

175
6.0
0.966

Table 2: The effect of approximation and compres­
sion on junction trees generated from the median­
ne rve CPN.
4.3

Effect

on

the Quality

Whenever we commit ourselves to making an ap­
proximation, we want to know the risk that we will
make serious errors. Unfortunately, the basis on
which we calculate the theoretical worst-case error
bounds might be too coarse, and it is highly unlikely
that the worst-case situation will appear in a real ap­
plication. If we had some method that could warn us
when the situation was questionable, we might take
the risk and make approximations beyond the mag­
nitude imposed by a given worst-case error bound.
We shall use our median-nerve knowledge base, and
shall make a diagnosis on the basis of a set of find­
ings, thus showing how our theoretical estimate on
upper bounds on errors compares to practical values.

orders

I
I
I
I

.!!?
Q)
co

1 o·1

.s:
....
0
....
....
w
"'0
Q)
>
....
Q)
!I)
.0

0

I

10·2
10'

I

3

10-4

I

10·5
-a
10

10""

3
1 0"

10"

2

10" 1

10°

101

Total Removed Probability Mass (%)

Time
Initial (seconds)

three

I

Exact Beliefs:

Exact Beliefs:

-a-- 0.9189 -0-- 0.0444
Ell

0.4478

-6---

0.0153

0.2933

11!1

0.0031

--o- 0.0620

Worst-case
Error Bound:

0.0005

Figure 4: The errors observed in the beliefs for var­
ious states of a local nerve lesion given a standard
case. The probability for this case is 4.1 x 10-4•
Figure 5 shows triples of the worst-case bound
(filled square) , maximal observed error ( diamond) ,
and average observed error ( open square) for 18 dif­
ferent randomly generated cases as a function of the
case-specific normalizing constant, J..lcase· The ap­
proximation used corresponds to a decrease in re­
source requirements by a factor of four relative to
an unapproximated but compressed junction tree.
Figure 5 shows that the observed errors on com­
puted beliefs for the displayed cases are much
smaller than that predicted by the worst-case er­
ror bound derived in Section 3. This difference
shows that it is very unlikely, by picking a ran­
domly generated case with a given J..lcase, to get the

I
I
I
I
I
I
I
I
I
I

168

I
I
I
I

10 °

�

10 "

Ci5
!l) 10-2
.!:
....
0
10-3
....
....
w
-c 10-4
Q)

I

10-7

I
I
I
I
I
I
I
I
I
I
I

0

•
•

...

•

For the median-nerve knowledge base and the fo­
cus on the hypothesis of a lesion at the wrist, a de­
mand of 0.01 as the upper limit of error in a state,
would allow us set the alert threshold as low as
J.Lcase = 10-7 for e = 2 X 10-4.

•
•

0
[J

0 �
0

<>

[J [)

[)

[Ji'

c:

I

I

.

1

Q)
5
(/') 10.c

I

-

10 �

10

[J

Average

0

Maximal

•

Worst Case

-B

7
10 "

�
10

5

..

0

Q:>

[)

co

o<>
DE!

0
<>
0
[)

o<lf>

\
5
10-

10-4

10-

3

·2

1o

Normalizing Constant

Figure 5: For e = 2 x 10-4, triples of worst-case er­
ror, maximal observed error, and average observed
error in the beliefs of the states of the disorder nodes
used for the case in Figure 4 are shown for 18 differ­
ent cases.
worst-case configuration. In the present CPN, the
ratio between the worst-case bound and the max­
imal observed error is three orders of magnitude
for J.Lcase ;::: 10-6• Decreasing the normalizing con­
stant (J.Lcase) implies increasing the error in beliefs
for the specific case, as well as for the worst-case er­
ror. W hen J.Lcase approaches zero, the error in beliefs
approaches one, corresponding to excluding the case
from the domain model.
These empirical studies show, that if we have a
specific hypothesis in mind (for example the diag­
nosis of a local nerve lesion at the wrist) and a set
of test cases which provides us with a span of J.Lcase,
we can get empirical values for the actual expected
error in a specific case, given J.Lcase·
Given a specific approximation e, we would have
the following situations: If we insert a set of findings,
and the theoretical worst-case error bound are below
an accepted level, we can use the approximated junc­
tion tree. If we insert a set of findings which already
has been taken out of the domain model by "zeroing
out," the violation on the model will be recognized
by a zero normalizing constant, and we have to use
a less approximated junction tree. If we insert a set
of findings yielding an unacceptably high worst-case
error, we have to rely on empirical studies, such as
those above, to estimate the error based on J.Lcase,
and on basis of this, decide whether to fall back on
a less approximated junction tree or accept the risk
of committing an error. This approach allow us to
obtain a graceful degradation of the quality of diag­
noses as the limit of the approximation is reached.

Conclusion

We have presented a scheme for approximation in
the numerical part of a CPN-based expert. system.
Our approach eliminates the (small) numbers rep­
resenting probabilities of rare combinations of find­
ings, thereby preventing these findings from being
trea.ted as ordinary findings in the expert system.
The approximation has two effects: (1) we may gain
several orders of magnitude in improvement of re­
source usage, and (2) we may lose some accuracy
in the computed beliefs. However, we can estimate
case-specific upper bounds for the errors made on
the computed beliefs, although these bounds may be
too pessimistic, as the studies reported in Section 4
show.
If the case has been completely excluded by the
approximation process, we will detect it by fiudiug
a zero normalizing constant during propagation; if
the case is one of the common cases, we know that
the computed beliefs can be trusted to a large de­
gree. The problematic cases are the ones that have
a nonzero probability outside the "trusted range''
of probabilities (remember that the probability of
a case is equal to the normalization constant found
during propagation). We suggest that, when a prob­
lematic case occur, we should reenter the case into a
less approximated (maybe even a nonapproximated)
junction tree; however, this solution should rarely be
necessary.
It would be nice to find an upper bound on the
error of beliefs that is better (and still easily com­
putable) than is the one presented in Section 3. Cal­
culation of this bound involves the errors made on
individual findings. We might be able to do better if
we considered two or more findings simultaneously;
however, a straightforward approach would require
O(sn) space, where s is the total number of states
in the nodes, and n is the number of findings con­
sidered.
There might be a clever technique to avoid con­
sidering all these combinations of findings and at the
same time to provide a better error bound. We shall
leave this topic for future research.
6

Acknowledgements

We thank Steffen L. Lauritzen, Kristian G. Olesen,
and Finn V. Jensen for valuable comments, sugges­
tions, and inspiring discussions on the subject of this
paper.
We are grateful for the inspiring environment pro­
vided by the Medical Computer Science Group at

169

Stanford University to one of us (SKA) from Au­
gust, 1989, through June, 1990. Computer support
was partly provided by the SUMEX-AIM resource,
under NIH grant LM05208.
We also thank Lyn Dupre, Stanford University, for
the many improvements of the prose she cont.ri bu ted
to this paper.


We present an approach to the solution of de­
cision problems formulated as influence dia­
grams.

This approach involves a special tri­

angulation of the underlying graph, the con­
struction of a junction tree with special prop­

tials normalized. Ndilikilikesha (Shachter and Ndiliki­
likesha, 1993; Ndilikilikesha, 1994) modified the node­
removal/ arc-reversal algorithm to avoid these extra di­
visions; the result is an algorithm that is equivalent to
Shenoys algorithm with respect to computational effi­
ciency.

erties, and a message passing algorithm op­

Our work builds primarily on the work of Shenoy

erating on the junction tree for computa­

(1992) and Shachter and Peat {1992), in addition

tion of expected utilities and optimal decision

to our previous work on propagation algorithms for

policies.

the expert system shell Hugin (Andersen et al., 1989;
Jensen et al., 1990) .

1

INTRODUCTION

Influence diagrams were introduced by Howard and
Matheson (1981) as a formalism to model decision

2

INFLUENCE DIAGRAMS

An

influence diagram is a belief network augmented

problems with uncertainty for a single decision maker.

with decision variables and a utility function.

The original way to evaluate such problems involved

The structure of a decision problem is determined by

unfolding the influence diagram into a decision tree
and using the "average-out and fold-back" algorithm
on that tree. Shachter (1986) describes a way to eval­

uate an influence diagram without tranforming it into
a decision tree. The method operates directly on the
influence diagram by means of the node-removal and
arc-reversal operations. These operations successively
transform the diagram, ending with a diagram with
only one utility node that holds the utility of the op­
timal decision policy; the policies for the individual
decisions are computed during the operation of the
algorithm (when decision nodes are removed).
Shenoy (1992) describes another approach to the eval­
uation of influence diagrams: the influence diagram is
converted to a valuation network, and the nodes are
removed from this network by fusing the valuations
bearing on the node (variable) to be removed. Shenoys
algorithm is slightly more efficient than Shachters al­
gorithm in that it maintains a system of valuations,

an acyclic directed graph G. The vertices of G repre­
sent either random variables (also known as chance or
probabilistic variables) or decision variables, and the
edges represent probabilistic dependencies between
variables. Decision variables represent actions that are
under the full control of the decision maker; hence, we
do not allow decision variables to have parents in the
graph.
Let

UR

be the set of random variables, and let the

set of decision variables be

Uo

=

{0 1, .

.

.

, Dn},

with

the decisions to be made in the order of their index.
Let the universe of all variables be denoted by U

UR U Uo.

sets lo, . ..

We partition

, In;

for

0<

UR

=

into a collection of disjoint

Ik is the set of variables
Dk and Dk+ 1;
variables, and In is the set

k <

n,

that will be observed1 between decision

Io is the initial evidence

of variables that will never be observed (or will be
observed after the last decision). This induces a partial
order..:: on

U:

whereas Shachters algorithm maintains a system of
conditional probability functions (in addition to the
utility functions), and some extra work (some division
op erations) is required to keep the probability paten-

1 By

'observed,'

will be revealed.

we

mean that the true state of the variable

368

Jensen, Jensen, and Dittmer

We associate with each random variable A a condi­
tional probability function ¢A=P(AI:PA), where 'J'A
denotes the set of parents of A in G .
The state space Xv for V � U i s defined a s the Carte­
sian product of the sets of possible outcomes/decision
alternatives for the individual variables in V. A po­
tential ¢v for a set V of variables is a function from
Xv to the set of real numbers.
The potential <Pv can be extended to a potential

¢w (V � W) by simply ignoring the extra variables:
¢w(w) =¢v(v) ifv is the projection ofw on V.
Given two potentials, <P and tf!. The product ¢ * tV
and the quotient ¢/tf! are defined in the natural way,
except that 0/0 is defined to be 0 (x/0 for x -1= 0 is
undefined) .
The (a priori) joint probability function <Pu is defined
as
¢A·
<Pu
=

IT

AEUR

For each instance of Uo (i.e., each element of
<Pu defines a joint probability function on UR.

Xu0 ),

A solution to the decision problem consists of a series
of decisions that maximizes some objective function.
Such a function is called a utility function. Without
loss of generality, we may assume that the utility func­
tion tfJ is a potential that may be written as a sum of
(possibly) simpler potentials:

The independence restriction imposed on the decision
problem can be verified by checking that, in the influ­
ence diagram, there is no directed path from a deci­
sion Ok to a decision Di (i < k).
3

DECISION MAKING

Assume we have to choose an alternative for deci­
sion On (i.e., the last decision). We have already
observed the random variables 10, . . . , In-1, and we
have chosen alternatives for decisions D1 , ..., 0 n-1.
The maximum expected utility principle2 says that
we should choose the alternative that maximizes the
expected utility. The maximum expected utility for
decision Dn is given by

Pn =max L P(Inllo, · · ·, In-1, 01, ..., Dn) * tf!.
On

I,

Obviously, Pn is a function of previous observations
and decisions. We calculate the maximum expected
utility for decision Dk (k < n) in a similar way:

Pk =Df,ax L P(Ikllo, ... , Ik-1, o, . . . , Dk) *Pk+1·
k

I<

We note that Pk is well-defined because of ( 1).
By expansion of ( 2 ) , we get

Pk =max L P(Ikllo,... , Ik-1, D 1, ... , Dk)
Dk

m

Ik

*max
Dk+l

=max
We need to impose a restriction on the decision prob­
lem, namely that a decision cannot have an impact on
a variable already observed. This translates into the
property

P(Ikllo, ... , Ik-1, D1, ... , On)
=P(Ikllo, ... ,Ik-1.01, ... ,0kl·

(1)

In words: we can calculate the joint distribution for I k
without knowledge of the states of Dk+ 1, ... , Dn (i.e.,
the future decisions).
2.1

GRAPHICAL REPRESENTATION

In Figure 1, an example of an influence diagram is
shown. Random variables are depicted as circles, and
decision variables are depicted as squares. Moreover,
each term of the utility function is depicted as a dia­
mond, and the domain of the term is indicated by its
parent set. The partial order-< is indicated by making
I k-1 the parent set of D k, and we shall use the con­
vention that the temporal order of the decisions are
read from left to right.

(2 )

o.

L
h+•

P(Ik+11Io, ... ,Ik,
D1, ... ,0k+1 ) *Pk+2

L max L P(Ik, Ik+11Io, ... , Ik-1,
l•

Dk+l

Ik+l

D1, ... , 0k+1) *Pk+2·

The last step follows from ( 1) and the chain rule of
probability theory: P(AjB,C)P(BIC) =P(A,BIC). By
further expansion, we get

Pk =max L ···max L P(Ik,..., Inllo, ... , Ik-1,
Dk h
Dn
DnJ * t!J.
D1,
In
•

• •

1

From this formula, we see that in order to calculate
the maximum expected utility for a decision, we have
to perform a series of marginalizations (alternately
sum- and max-marginalizations), thereby eliminating
the variables.
When we eliminate a variable A from a function ¢,
expressible as a product of simpler functions, we par­
tition the factors into two groups: the factors that
involve A, and the factors that do not; call (the prod­
uct of) these factors ¢ :t_ and ¢A, respectively. The
marginal LA¢ is then equal to <PA: *LA¢:t_; LA <Pt
2There are good arguments for adhering to this principle.
See, e.g.,

(Pearl, 1988).

From Influence Diagrams to Junction Trees

369

FIGURE 1.
An influence diagram for a decision problem with four decisions. The set of variables is partitioned into the

sets:

I0

=

{b}, I 1

=

{ e, f}, Iz

=

0,

I3 = {g}, and I4 =

{a, c, d, h, i, j, k, e}.

four local utilities, three of which are associated with single variables (D 1 ,
the pair

(j, k).

then becomes a new factor that replaces the prod­
uct

cj:J:t_ in the expression for ¢1.

This also holds true for

max-marginalizations, provided

cPA.

does not assume

negative values.

The utility function is a sum of

D3, and €), and one associated with

marginalizations; but- in general- we cannot inter­
change the order of a max- and a sum-marginalization;
this fact imposes some restrictions on the elimination
order.

The product cjJ may be represented by an undirected
graph, where each maximal complete set (clique) of
nodes (the nodes being variables) corresponds to a fac­

4

INFLUENCE DIAGRAMS

tor (or a group of factors) of cjJ with that set as its do­
main. Marginalizing a variable

A out of ¢1 then corre­

COMPILATION OF

We first form the moral graph of G. This means adding

sponds to the following operation on the graph: the set

(undirected) edges between vertices with a common

A is

child. We also complete the vertex sets corresponding

of neighbors of A in the graph is completed, and

removed. It is a well-known result that all variables

to the domains of the utility potentials. Finally, we

can be eliminated in this manner without adding edges

drop directions on all edges.

if and only if the graph is triangulated (Rose, 1970).

Next, we triangulate the moral graph in such a way

Obviously, it is desirable to eliminate all variables

that it facilitates the computation of the maximum

without adding extra edges to the graph since this

expected utility. This is equivalent to the selection of

means that we do not create new factors with a larger

a

domain than the original factors (the complexity of

verse of the elimination order must be some extension

representing and manipulating a factor is exponen­

of -< to a total order.

tial in the number of variables comprising its domain).
However, in most cases, this is not possible: we have
to add some edges, and the elimination order chosen

special elimination order for the moral graph: the re­

Finally, we organize the cliques of the triangulated
graph in a strong junction tree:

A tree of cliques

optimal elimination order for all reasonable criteria of
optimality.

( C 1 , C2l of
C2 is contained in every clique on the
path connecting C1 and C2. For two adjacent cliques,
C1 and C2, the intersection C, n Cz is called a sepa­

W hen we perform inference in a belief network (i.e.,

least one distinguished clique R, called a strong root,

will determine how many and hence also the size of
the cliques.

Unfortunately, it is :N'Jl-hard to find an

is called a junction tree if for each pair

cliques,

C1

n

rator. A junction tree is said to be strong if it has at

(C1, Cz) of adjacent cliques in
C1 closer to R than C2, there exists
an ordering of C2 that respects -< and with the ver­
tices of the separator C1 n Cz preceding the vertices
of C2\C1 . This property ensures that the computation

calculation of the marginal probability of some vari­

such that for each pair

able given evidence on other variables), the computa­

the tree, with

tion only involves sum-marginalizations. In this case,
we can eliminate the variables in any order, since the
order of two marginalizations of the same kind can be
interchanged.

However, the calculation of the max­

imum expected utility involves both max- and sum-

of the maximum expected utility can be done by local
message passing in the junction tree (see Section 5).

370

Jensen, Jensen, and Dittmer

that no two cliques have the same index. Moreover,
unless index( C) = 1, the set

{ vE C I !X(v) <index( C)}

will be a proper subset of some other clique with a

lower index than

C.

Let the collection of cliques of the triangulated graph
be

C1, ... , Cm, ordered in

increasing order according

to their index. As a consequence of the above construc­
tion, this ordering will have the running intersection
property (Beeri et

al., 1983),

meaning that

k-1
for all

FIGURE 2.
The moral graph for the decision problem in Figure 1.
Edges added by the moralization process are indicated
by dashed lines.

k

>

1: Sl<

=

C 1< n

U

Ci � C;

for some

i=1

j < k.

It is now easy to construct a strong junction tree: we
start with
each clique

C1 (the root); then we successively attach
Ck to some clique C; that contains Sl<.

Consider the decision problem in Figure 1. Figure

2

shows the moral graph for this problem: edges have

been added between vertices with a common child (in­
cluding utility vertices), utility vertices have been re­
moved, and directions on all edges have been dropped.
Note that the time precedence edges leading into de­
cision vertices are not part of the graph and are thus
not shown.
Figure

shows the strong triangulation of the graph

3

2 generated by the elimination sequence e,
j, k, i (fill-ins: D2
04 and g
04), lt (fill-in:
f
03), a, c (fill-in: b
e), d (fill-ins: 01
e,
01 - f, b f, and e
f), 04, g (fill-in: e --. Oz),
03, 02, f, e, 01, and b. This graph has the fol­
lowing cliques: C16
{04,i,e}, C,s = {lt,k,j},
C14 = {03,lt,k}, C11
{b,c,a}, C10
{b,e,d,c},
Cs
{Oz, g, 04, i}, C6
{f, 03, lt}, Cs
{e, 02, g},
and C1 = {b,O,e,f,d}. Using the above algorithm,
in Figure

�

�

�

�

�

FIGURE 3.

�

�

=

The triangulated graph of the moral graph in Figure 2.
Fill-in edges added during triangulation are indicated
by dashed lines.

=

=

=

=

=

we get the strong junction tree shown in Figure

4

for

this collection of cliques. (There exists another strong

4.1
Let

CONSTRUCTION OF STRONG JUNCTION TREES

!X be

a numbering of

{1, ... , lUI})

U

such that for all

plies !X(u) <

!X(v).

!X: U H
U, u -< v im­

(i.e., a bijection

u,v

E

We assume that !X is the elimi­

nation order used to produce the triangulated graph

of G: vertices with higher numbers are eliminated be­
fore vertices with lower numbers.
Let C
vE C

the edge

Cs

--1

C1

by the edge

Cs

--1

C10.

This tree is

computationally slightly more efficient, but- unfor­
tunately- it cannot be constructed by the algorithm
given in this paper.)
In general, previous observations and decisions will be
relevant when making a decision. However, sometimes
only a subset of these observations and decisions are

be a clique of the triangulated graph, and let
be the highest-numbered vertex such that the

{wE C I !X(w) < !X(v)} have a common neigh­
bor u (j. C with !X(u) < !X(v). If such a vertex v exists,
we define the index for Cas index( C)
!X(v); other­
wise, we define index( C)
1. Intuitively, the index for
a clique C identifies the step in the elimination process
that causes C to "disappear" from the graph. It is easy
to see that the index for a clique C is well-defined, and
vertices

=

=

junction tree for this collection, obtained by replacing

needed to make an optimal decision. For example, for
the decision problem in Figure 1, the variable

e

sum­

marizes all relevant information available when deci­
sion

02

has to be made: although

before decision

0 2,

f

is observed just

it has no relevance for that deci­

sion (it does, however, have relevance for decision 03).
This fact is detected by the compilation algorithm: the
only link from 02 to past observations and decisions
goes to

e.

371

From Influence Diagrams to Junction Trees

FIGURE 4. A strong junction tree for the cliques of the graph in Figure 3.
5

USING THE STRONG JUNCTION TREE

Now, letT be a strong junction tree, and let C1 and C2

FOR COMPUTATIONS

be adjacent cliques with separator S inT. We say that

We perform computations in the junction tree as a spe­
cial'collect' operation from the leaves of the junction

C1 absorbs from Cz if
and tVc as follows:

Q:Jc,

and tVc, change to

,

cPc,

tree to some strong root of the tree.
To each clique

C in the junction tree, we associate a

probability potential QJc and a utility potential tVc
defined on

Xc.

Let e be the set of cliques. We define

where

the joint potentials q, and tV for the junction tree as

tVs

=

M

Cz\S

cPCz *tV c z

·

Note that this definition of absorption is 'asy mmetric'
We initialize the junction tree as follows: each variable

A E U R is assigned to a clique that contains A U PA.
T he probability potential for

a

clique is the product of

the conditional probability functions for the variables
assigned to it. For cliques with no variables assigned
to them, the probability potentials are unit functions.
In this way, the joint probability potential for the junc­
tion tree becomes equal to the joint probability func­
tion for the influence diagram. Similarly, each utility

in the sense that information only flows in the direc­
tion permitted by the partial order -<:. It is possible
to generalize this definition of absorption to a sym­
metric definition similar to the one given in (Jensen
et

al.,

1990} for the case of pure probabilistic influence

diagrams.
Clearly, the complexity of an absorption operation is

O{I Xc,l + IXsl + 1Xc21).

Note in particular that the
contribution from the division operation plays a much
smaller role than in (Shenoy, 1992), since division op­

function tVk is assigned to some clique that can ac­
commodate it. The utility potential for a clique is the

erations are performed on separators only.

sum of the utility functions assigned to it; for cliques

We will need the following lemma, which we shall state

with no utility functions assigned to them, the utility

without proof.

potential is a null function.
We need a generalized marginalization operation that
acts differently on random and decision variables. We
denote the operation by

'M'

.

For random variable

A

and decision variable D, we define

M q,
D

For a set

V of variables,

we define

=

maxQ:l.
D

Mv q,

as a series of

single-variable marginalizations, in the inverse order
as determined by the relation -<:. Note that although
-<: is only a partial order,

Mv q,

is well-defined.

Lemma 1 Let D be a decision variable, and let

V

a set of variables that includes all descendants of
in G .
of

D

Then

Mv\{ D} Q:lu,

be

0

considered as a function

alone, is a non-negative constant.

Let T be a strong junction tree with at least two
cliques; let

cPT

be the joint probability potential and

tVT the joint utility potential on T. Choose a strong
root R for T and some leaf l (=I R); let T \ l denote
the strong junction tree obtained by absorbing l into

its neighbor N and removing l; denote the separator
between N and l by S.

372

Jensen, Jensen, and Dittmer

Theorem 1 After absorption of l into T, we have

M <Pr

tl>r = <PT\L

*

*

(2) Xk

is a decision variable. By induction, we get

tVT\L·

L\S
Proof: Let

<iJL

IJ

=

1
Because of Lemma 1, <P ( k+ l, considered as a

<Pc;

function of xk alone, is a non-negative constant,

CEe\{L}

and we get
Since <PL does not assume negative values, we get

(mxax<P(k+1l)

*

,

L\S

L\S

(

maxx,

tj>lk+1l

maxx. <j)lk+ 1 J

- .+,(k)
-�

*

We have to show that

M <PL
L\S

*

(ti>L + lj}L)

=

<Ps

*

(�5�S

+ 'iJL

),

+\i))

( tj>(k)
<tJ(k)

L

+

::1:

�L

)

·

I

By successively absorbing leaves into a strong junction
tree, we obtain probability and utility potentials on
the intermediate strong junction trees that are equal
to the marginals of the original potentials with respect

where

to the universes of these intermediate trees.

This is

ensured by the construction of the junction tree in
which variables to be marginalized out early are lo­
We shall prove this by induction. Let X1

,

... , Xt be

some ordering of l \ S that respects -<. Now, consider

the equation:

cated farther away from the root than variables to be
marginalized out later.

The optimal p olicy for a decision variable can be deter­

mined from the potentials on the clique that is closest

to the strong root and contains the decision variable
(that clique may be the root itself), since all variables
that the decision variable may depend on will also be
members of that clique.

where

For our example decision problem (Figure 4), we can
determine the optimal policy for 01 from {the poten­
tials on) clique C 1 (the root), and the optimal policies
for the remaining decisions can be determined from

(For k = 1, (3) is equivalent to the desired result.) For
k > e, (3) is clearly true; for 1 :s k :s e, we have two

cases:

(1) Xk

cliques Cs (decision 02), C6 (decision 03), and Cs

(decision 0 4).

If only the maximum expected utility is desired, it
is a random variable. By induction, we get

should be noted that only storage for the 'active' part
of the junction tree during the collect operation needs
to be reserved; this means that storage for at most
two adjacent cliques and each clique that corresponds
to a branch point on the currently active path from

the root to a leaf

must be reserved. Since elimination

of a group of variables can be implemented more effi­

ciently than the corresponding series of single-variable
eliminations, it is still useful to organize the computa­
tions according to the structure of the strong junction
tree as compared to (Shenoy,

The correctness of the last step follows from the
fact that

<P(kl (x)

=

0 implies tj>lkl (x)

=

0 (so that

our division-by-zero convention applies).

6

1992).

CONCLUSION

We have described an algorithm to transform a deci­
sion problem formulated as an influence diagram into a

From Influence Diagrams to Junction Trees

secondary structure, a strong junction tree, that is par­
ticularly well-suited for efficient computation of max­

networks by local computations.
4:269-282.

373

Computational

Statistics Quarterly,

imum expected utilities and optimal decision policies.
The algorithm is a refinement of the work by Shenoy
{1992) and Shachter and Peot {1992); in particular,
the construction of the strong junction tree and its
use for computations has been elaborated upon.

Kjcerulff, U. {1990). Triangulation of graphs-algo­
rithms giving small total state space. Research
Report R-90-09, Department of Mathematics and
Computer Science, Aalborg University, Denmark.

T he present work forms the basis for an efficient com­
puter implementation of Bayesian decision analysis in
the expert system shell Hugin (Andersen et al., 1989).

Lauritzen, S. L. and Spiegelhalter, D. J. (1988). Lo­
cal computations with probabilities on graphical
structures and their application to expert systems.

We have not given an algorithm to construct the elim­
ination sequence that generates the strong triangula­
tion. However, the triangulation problem is simpler
than for ordinary probability propagation, since the
set of admissible elimination sequences is smaller; at
this stage, it appears that simple adaptations of the
heuristic algorithms described by Kjcerulff (1990) work
very well. Moreover, even given a triangulation, there
might exist several strong junction trees for the collec­
tion of diques.
Besides the use of the strong junction tree for compu­
tation of expected utilities and optimal decision poli­
cies, it should be possible to exploit the junction tree
for computation of probabilities for random variables
that only depend on decisions that have already been
made. Ideally, this should be done through a 'dis­
tribute' operation from the root towards the leaves of
the junction tree.
Work regarding these problems is in progress.
Acknowledgements

This work has been partially funded by the Danish
research councils through the PIFT programme.

