
We consider the problem of diagnosing faults
in a system represented by a Bayesian network,
where diagnosis corresponds to recovering the
most likely state of unobserved nodes given the
outcomes of tests (observed nodes). Finding
an optimal subset of tests in this setting is intractable in general. We show that it is difficult
even to compute the next most-informative test
using greedy test selection, as it involves several
entropy terms whose exact computation is intractable. We propose an approximate approach
that utilizes the loopy belief propagation infrastructure to simultaneously compute approximations of marginal and conditional entropies on
multiple subsets of nodes. We apply our method
to fault diagnosis in computer networks, and
show the algorithm to be very effective on realistic Internet-like topologies. We also provide theoretical justification for the greedy test selection
approach, along with some performance guarantees.

1 Introduction
The problem of fault diagnosis appears in many places under various guises. Examples include medical diagnosis,
computer system troubleshooting, decoding messages sent
through a noisy channel, etc. In recent years, diagnosis
has often been formulated as an inference problem on a
Bayesian network, with the goal of assigning most likely
states to unobserved nodes based on outcome of test nodes.
An important issue in diagnosis is the trade-off between
the cost of performing tests and the achieved accuracy of
diagnosis. It is often too expensive or even impossible to
perform all tests. In this paper, we concentrate on the problem of active diagnosis, in which tests are selected sequentially to minimize the cost of testing. We use entropy as

Alina Beygelzimer
IBM T.J. Watson Research Center
19 Skyline Drive
Hawthorne, NY 10532
beygel@us.ibm.com

the cost function and select a set of tests providing maximum information, or minimum conditional entropy, about
the unknown variables.
However, exact computation of conditional entropies in a
general Bayesian network can be intractable. While much
existing research has addressed the problem of efficient and
accurate probabilistic inference, other probabilistic quantities, such as conditional entropy and information gain,
have not received nearly as much attention. There is a
vast amount of literature on value-of-information and mostinformative test selection [10, 4, 9, 11], but none of the previous work appears to focus on the computational complexity of most-informative test selection in a general Bayesian
network setting.
We propose an approximation algorithm for computing
marginal conditional entropy. The algorithm is based on
loopy belief propagation, a successful approximate inference method. We illustrate the algorithm at work in the setting of fault diagnosis for distributed computer networks,
and demonstrate promising empirical results. We also apply existing theoretical results on the optimality of certain
greedy algorithms to our test selection problem, and analyze the effect of approximation error on the expected cost
of active diagnosis. Our method is general enough to apply
to other applications of Bayesian networks that require the
computation of information gain and conditional entropies
of subsets of nodes. In our application, it can efficiently
compute the information gain for all candidate tests simultaneously.
The paper is structured as follows. Section 2 introduces
necessary background and definitions. In section 3, we describe the general problem of active diagnosis and the computational complexity issue thereof. We propose a solution
to this problem in section 4. Section 5 discusses an application of our approach in the context of distributed computer system diagnosis, while section 6 presents empirical
results. We survey related work in section 7, and conclude
in section 8.

2 Background and Definitions

process could diverge; convergence is guaranteed only for
polytrees.

Let X = {X1 , X2 , . . . , XN } denote a set of N discrete random variables and x a possible realization of X. A Bayesian
network is a directed acyclic graph (DAG) G with nodes
corresponding to X 1 , X2 , . . . , XN and edges representing
direct dependencies [16]. The dependencies are quantified
by associating each node X i with a local conditional probability distribution P (xi | pai ), where pai is an assignment
to the parents of X i (nodes pointing to X i in the Bayesian
network). The set of nodes {x i , pai } is called a family.
The joint probability distribution function (PDF) over X is
given as product

Let a denote a factor node and i one of its variable nodes.
Let N (a) represent the neighbors of a, i.e., the set of variable nodes connected to that factor. Let N (i) denote the
neighbors of i, i.e., the set of factor nodes to which variable node i belongs. The BP message from node i to factor
a is defined as (see, e.g., [12])

mc→i (xi ),
(3)
ni→a (xi ) :=

P (x) =

N


P (xi | pai ).

(1)

i=1

We use E ⊆ X to denote a possibly empty set of evidence
nodes for which observation is available.
For ease of presentation, we will also use the terminology
of factor graphs [6], which unifies directed and undirected
graphical representations of joint PDFs. A factor graph
is an undirected bipartite graph that contains factor nodes
(usually shown as squares) and variable nodes (shown as
circles). (See Fig. 1 for an example.) There is an edge
between a variable node and a factor node if and only if
the variable participates in the potential function of the corresponding factor. The joint distribution is assumed to be
written in a factored form
P (x) =

1 
fa (xa ),
Z a

(2)

where Z is a normalization constant called the partition
function, and the index a ranges over all factors f a (xa ),
defined on the corresponding subsets X a of X.
The computation complexity of many probabilistic inference problems can be related to graphical properties. Exact
inference algorithms require time and space exponential in
the treewidth [16] of the graph, which is defined to be the
size of the largest clique induced by inference, and can be
as large as the size of the graph. Many common probabilistic inference problems are NP-complete. [1] This includes our problem of probabilistic diagnosis, which can
be formulated as a Maximum A Posteriori (MAP) probability problem: given a set of observations, find the most
likely states of unobserved variables.
Although probabilistic inference can be intractable in general, there exists a simple linear-time approximate inference algorithm known as belief propagation (BP) [ 16]. BP
is provably correct on polytrees (i.e. Bayesian networks
with no undirected cycles), and can be used as an approximation on general networks. In belief propagation, probabilistic messages are iterated between the nodes. The

c∈N (i)\a

and the message from factor a to node i is defined as


fa (xa )
nj→a (xj ).
ma→i (xi ) :=
xa \xi

(4)

j∈N (a)\i

Based on these messages, we can compute the beliefs for
each node and the probability potential for each factor:

ma→i (xi ),
(5)
bi (xi ) ∝
a∈N (i)

ba (xa ) ∝ fa (xa )



ni→a (xi ).

(6)

i∈N (a)

Observations are incorporated into the process via δfunctions as local potentials for the evidence nodes. In
that case, bi (xi ) becomes the approximation of the posterior probability P (x i | e).

3 The Active Test Selection Problem
In many diagnosis problems, the user has an opportunity
to actively select tests in order to improve the accuracy of
diagnosis. For example, in medical diagnosis, doctors face
the experiment design problem of choosing which medical
tests to perform next.
Let S = {S1 , S2 , . . . , SN } denote a set of unobserved
random variables we wish to diagnose, and let T =
{T1 , T2 , . . . , TM } denote the available set of tests. Our
objective is to maximize diagnostic quality while minimizing the cost of testing. The diagnostic quality of a
subset of tests T∗ can be measured by the amount of uncertainty about S that remains after observing T ∗ . From
the information-theoretic perspective, a natural measurement of uncertainty is the conditional entropy H(S | T ∗ ).
Clearly, H(S | T) ≤ H(S | T∗ ) for all T∗ ⊆ T. Thus
the problem is to find T ∗ ⊆ T which minimizes both
H(S | T∗ ) and the cost of testing. When all tests have
equal cost, this is equivalent to minimizing the number of
tests.
This problem is known to be NP-hard [19]. A simple
greedy approximation is to choose the next test to be T ∗ =
arg minT H(S | T, T ), where T is the currently selected

test set. The expected number of tests produced by the
greedy strategy is known to be within a O(log N ) factor
from optimal (see Appendix). The same result holds for
approximations (within a constant multiplicative factor) to
the greedy approach. Furthermore, our empirical results
show that the approach works well in practice.
We make a distinction between off-line test selection and
online test selection. In online selection, previous test outcomes are available when selecting the next test. Off-line
test selection attempts to plan a suite of tests before any
observations have been made. We will focus on the online approach, sometimes called active diagnosis, which is
typically much more efficient in practice than its off-line
counterpart [19].
Active Test Selection Problem: Given the observed outcome t of previously selected sequence of tests T  , select
the next test to be arg minT H(S | T, t ).
In a Bayesian network, the joint entropy H(X) can be decomposed into sum of entropies over the families and thus
can be easily computed using the input potential functions.
Conditional marginal entropies, on the other hand, do not
generally have this property. Under certain independence
conditions they decompose into functions over the families. But computing those functions will require inference.
(See Appendix for proofs.)
Lemma 1. Given a Bayesian network representing a joint
PDF P (X), the joint entropy H(X) can be decomposed
into the sum of entropies over the families: H(X) =
N
i=1 H(Xi | Pai ).
Lemma 2. Given a Bayesian network representing a joint
PDF P (S, T), where ∀i : paTi ⊆ S (i.e. tests Ti and Tj
are independent given a subset of S), the observation t  of
previously selected test set, and a candidate test T , the conditional marginal entropy H(S | T, t  ) can be written as

H(S | T, t ) = −
P (spaT , t | t ) log P (t | spaT )
t,spaT

+



P (t | t ) log P (t | t ) + const, (7)

t

where const is a constant expression.

4 BP for Entropy Approximation
Let us consider the problem of computing the conditional
marginal entropy

P (xa | e) log P (xa | e),
(8)
H(Xa | e) = −
xa



where P (xa | e) =
x\xa P (x | e), x\xa representing
variable nodes not in x a . The trick is to replace the marginal
posterior P (xa | e) with its factorized BP approximation,
and make use of the BP message passing mechanism to
perform the summation over x a . We call this process Belief
Propagation for Entropy Approximation (BPEA).
Pick any node X 0 from Xa and designate it as the root
node. We modify the final message passed to X 0 as follows:

ma→0 (x0 ) := −
(9)
b̃a (xa ) log b̃a (xa ).
xa \x0

Here, b̃a (xa ) is the unnormalized
belief of X a (i.e.,

b̃a (xa ) = σba (xa ), where σ = xa b̃a (xa )).
Plugging in b̃a (xa ) in place of P (x a | e) in Eqn. 8, we
see that it only remains to sum over the root node X 0 and
normalize properly.

h̃(Xa | e) :=
ma→0 (x0 ),
(10)
x0

h(Xa | e)

:=

h̃(Xa | e)
+ log σ.
σ

(11)

It follows immediately that BPEA is exact whenever BP is
exact.
The normalization constant σ is already computed during
normal BP iterations. The computation of b̃a (·), ma→i , and
h̃(·) can all be piggy-backed onto the same BP infrastructure, and therefore does not impact its overall complexity.
Furthermore, due to the local and parallel message update
procedure in BP, we can compute the marginal posterior
entropies of multiple families in one single sweep. This is
an important advantage for the active probing setup.

Minimizing conditional entropy is a particular instance of
value-of-information (VOI) analysis [9], where tests are selected to minimize the expected value of a certain cost function c(s, t, t ). The result of Lemma 2 can be generalized
to this case if the cost function is decomposable over the
families. See Lemma 3 in the Appendix for details.

It is also easy to show that the approach is extendible beyond the entropy computation, to an arbitrary cost function decomposable over families (see Lemma 3 in the Appendix). The cost function replaces the negative logarithm
in Eqns. (8) and (9).

Since observations of test outcome correlate the parent
nodes, the exact computation of all the posterior probabilities in Eqn. (7) is intractable. We can certainly use an existing approximation method to compute P (s paT , t | t ) and
P (t | t ). But a more efficient approach is possible if we
exploit the belief propagation infrastructure.

5 Application: Fault Diagnosis in Computer
Networks
Suppose we wish to monitor a system of networked computers. Let S represent the binary state of N network elements. Si = 0 indicates that the element is in normal

is the cross entropy between the posterior probability of T
and its parents, and the conditional probability of T given
its parents. The second term in Eqn. (7) is simply the negative conditional entropy −H(T | t  ).

···
S1

S1

S3

T1

T1

···

SM

···

TN

Figure 1: Factor graph of the fault diagnostic Bayes net.
operation mode, and S i = 1 indicates that the element is
faulty. We can take S i to be any system component whose
state can be measured using a suite of tests. If the system
is large, it is often impossible to test each individual component directly. A common solution is to test a subset of
components with a single test probe. If all the test components are okay, the test would return a 0. Otherwise the test
would return 1, but it does not reveal which components
are faulty.

We deal with the two entropy terms separately. For H(T |
t ), we may use approximation methods such as BP or GBP
to calculate the belief b(t | t ), which can then be used
to directly compute H(T | t  ). (Note that the summation over values of T is simple since T is binary-valued.)
To calculate A(T, SpaT | t ), we use the entropy approximation method BPEA, as described in Section 4. Because BP message updates are done locally, we can compute A(T, SpaT | t ) for all unobserved T nodes during a
single application of BP. Thus, picking the next probe requires only one run of the BPEA approximation algorithm.
For each candidate probe, we designate the probe node T
itself as the root node. The unnormalized belief has the
form

b̃t (t, spaT ) := P (t | spaT )
nj→t (sj ).
(15)
j∈paT

We assume there are machines designated as probe stations, which are instrumented to send out probes to test the
response of the network elements represented by S. Let T
denote the available set of probes. A probe can be as simple as a ping request, which detects network availability. A
more sophisticated probe might be an e-mail message or a
webpage-access request. In the absence of noise a probe
is a disjunctive test: it fails if an only if there is at least
one failed node on its path. More generally, it is a noisyOR test [16]. The joint PDF of all tests and network nodes
forms the well-known QMR-DT model [13]:

This is used to calculate the modified message m a→t (t)
(cf. Eqn. (9)). However, since A(T, S paT | t ) is a cross
entropy term, we do not take the log of b̃, but rather take
the logarithm of the known probabilities P (t | s paT ). This
simplifies the normalization step described in Eqn. (11)
to A(T, SpaT | t ) = Ã(T, SpaT | t )/σ, where σ =

t,spa (T ) b̃t (t, spaT ).

P (sj ) = (αj )sj (1 − αj )(1−sj ) ,
 s
P (ti = 0 | spai ) = ρi0
ρijj ,

We conduct experiments on network topologies built by
the INET generator [20], which simulates an Internet-like
topology at the Autonomous Systems level. Our dataset includes a set of networks of 485 nodes, where the number
of probe stations varies from 1 to 50.

P (s, t) =


i

(12)
(13)

j∈pai

P (ti | spai )



P (sj ).

(14)

j

Here, αj := P (sj = 1) is the prior fault probability, ρ ij is
the so-called inhibition probability, and (1−ρ i0 ) is the leak
probability of an omitted faulty element. The inhibition
probability is a measurement of the amount of noise in the
network. Fig. 1 shows a factor graph representation of our
model.
As discussed in Section 3, we adopt the active probing framework for fault diagnosis, sequentially selecting
probes to minimize the conditional entropy. Our previous
work [17] makes the single-fault assumption, which effectively reduces S to one random variable with N +1 possible
states. In general, however, multiple faults could exist in
the system simultaneously, which requires the more complicated conditional entropy given in Eqn. ( 7).
Let A(T, SpaT | t ) denote the first term in Eqn. (7). This

6 Empirical Results

The connections between probe nodes and network nodes
are generated with two goals in mind: detection and diagnosis. A detection probe set needs to cover all network
components, so that at least one probe has a positive probability of returning 1 when a component fails. A diagnosis
probe set needs to further distinguish between faulty components. Optimal probe set design is NP-hard for either
detection or diagnosis. For the datasets used here, we first
use a greedy approach to obtain a probe set that covers all
network components, then augment this set with additional
probes in order to guarantee single-fault diagnosis. Interested readers may find detailed discussions of probe set design for diagnostic Bayesian networks in [11, 18].
In our experiments, we measure the effects of prior fault
probability α and inhibition probability ρ on approximation and diagnostic quality. We compare the approximate
entropy values and the quality of the selected probe set

0.02

0

//

//

600

0.01

0.01
0.05
0.1
0.3

100
0

0

//

0.1
0.05

0

0.01

0.05 0.1 0.2 0.4

inhibition prob

//

//

800

400

200

0.15

0

0.05 0.1 0.2 0.4

0.01
0.05
0.1
0.3

0.2

inhibition prob
(c) Test set entropy

500

300

//

0.01

0.05 0.1 0.2 0.4

inhibition prob
(d) Test set size

0.01
0.05
0.1
0.3
0

//

0.01

0.05 0.1 0.2 0.4

inhibition prob

Figure 2: Approximation errors and diagnostic quality for
an augmented detection network. Each curve represents a
different prior fault probability.
against the ground truth, which is obtained via the junction tree exact inference algorithm. In subsection 6.3, we
also summarize how the type of network may effect computational efficiency. Since all measurements depend on the
particular set of probe outcomes, we repeat all experiments
on 10 different samples of the Bayes net.
We use the diagnostic quality of the probe set to determine
when to stop the probe selection process: when the reduction in entropy for the past 5 iterations is no more than
0.00001, the selection process is deemed to converge. Otherwise we continue until all probes have been picked.
6.1 Approximation accuracy
First, we look at approximation accuracy. Recall that at
each time step of the active probing process, we obtain a
vector of approximate entropy values, one for each candidate probe T . We average the relative error between the
approximate values and the exact values for all candidate
probes, and further average over all time steps and samples. Let M denote the total number of probes, n the number of selected probes, h ij the approximate value for probe
j at the ith time step of probe selection, and H ij the corresponding exact value. We compute
R(h, H) :=

200
150
100
50
diag 1 10 20 30 40 50
Network type

1
0.8
0.6
0.4

diag
1
10
20
30
40
50

0.2
0

−9 −7 −5 −3 −1 1 3
# seconds saved

5

Figure 3: Efficiency of approximate method. (a) Average number of BP iterations saved by re-using messages;
(b) CDF of speed-up (in CPU seconds) compared to exact
method.

400

0

250

0

600

200

(b)

300
cumulative distribution

0.04

0

reduction in bit entropy

ave relative abs error

0.06

size of final probe set

ave relative abs error

0.08

(a)

(b) Second term approx errors
0.25

0.01
0.05
0.1
0.3

# iters saved per node

(a) First term approx errors

//

0.1

n−1
M−i
1  1  |hij − Hij |
.
n i=0 M − i j=1
|Hij |

(16)

We conduct this experiment on the detection network with
10 probe stations, augmented with single-node probes.
Fig. 2(a-b) contains plots of the average, the minimum, and
the maximum approximation errors, taken over 10 samples
of probe outcomes. Relative error values are shown separately for the first term, A(T, S paT | t ), and the second
term, H(T | t ). For both terms, the approximation errors

are generally lower at lower α values. The average errors
do not exceed 2%, with the only exception being the BP
error for term two at α = 0.3 and ρ = 0, which reaches up
to 10%. BP approximaton errors of the second term seem
to be generally higher than BPEA approximations of the
first term. At the maximum, the approximation error never
exceeds 10% for term one, and 20% for term two. BP errors for term two does not seem to contain any linear trends
with respect to ρ. However, BPEA’s approximation quality
of term one does seem to become slightly worse at higher
levels of the inhibition probability.
6.2 Diagnostic quality
The quality of diagnosis is taken to be the reduction in conditional bit entropy of the hidden states. If t  represents
the observed outcomes of the final set
of selected probes,

we
measure
H(S)
−
H(S
|
t
)
=
−
s P (s) log2 P (s) +



P
(s
|
t
)
log
P
(s
|
t
).
2
s
Fig. 2(c) compares the diagnostic quality of approximate
and exact algorithms on the augmented detection network
with 10 probe stations. Overall, the reduction in bit entropy
is larger for higher values of α. This is due to the fact that
H(S) is higher when α is larger. The quality of the exact
algorithm is almost identical to that of the approximate algorithm. The two are virtually indistinguishable, except at
α = 0.1 and ρ = 0.3. There is an outlier at this combination. For one of the samples, the value of the entropy
H(S | t ) plateaued unusually early during the active probing process, fooling the algorithm into believing that it had
converged, even though the amount of reduction in entropy
is still very small. Fig. 2(d) shows that the process terminated after selecting only a small set of tests. This outlier is
an artifact of our convergence criterion, not of the approximate algorithm itself.
Fig. 2(d) looks at the size of the final selected probe set
when active probing converges. Here again, the two algorithms have almost identical behavior. The value of α does
not have much impact on the number of selected tests, except when ρ = 0 (i.e., no noise in the tests), in which case

fewer tests are needed for diagnosis at lower levels of α.
These results demonstrate that, while the approximated entropy values may deviate from the truth, the diagnostic
quality of the approximate method is virtually identical to
that obtained using the exact method. Combined with its
speed advantages as described in the next section, these results make a strong case for why the approximate method
is preferable over the exact one.
6.3 Implementation and speed
We use the junction tree inference engine in Kevin Murphy’s Bayes Net Toolbox [15] for Matlab to obtain exact
singleton posterior probabilities. The approximate method
is implemented on top of the belief propagation C++/mex
code developed by Yair Weiss and Talya Meltzer. Additionally, we speed up the approximate active probing process by re-using BP messages at the start of each round of
test selection, thereby maintaining BP’s state from the end
of the selection round. We find that BP converges in substantially fewer iterations this way.
Fig. 3(a) plots the average, maximum, and minimum number of BP iterations that we save by re-using BP messages.
The results are aggregated over 5 samples of the Bayes net.
The x-axis denotes the type of network used. The label
diag represents the diagnosis network with 1 probe station, and the rest are detection networks with various numbers of probe stations. In the detection network with 50
probe stations, we save up to 269 iterations per test node
at the maximum. On average, re-using messages shortens
the BP convergence time by 40-50 iterations per test. If
active probing selected 100 tests, say, then re-using messages would require 4000 to 5000 fewer iterations of belief
propagation.
Fig. 3(b) is a plot of the empirical cumulative distribution
of the speed-up using the approximate method. For all of
the detection networks, the approximate method is at least
1 CPU second faster than the exact method for 75% of the
test nodes. The speed-up is even higher for the diagnostic network, where for 78% of all test nodes the approximate method saves at least 2 CPU seconds per node. This
amounts to substantial savings over the entire active probing process. Also keep in mind that, for networks with large
tree-width, the exact method is not even computationally
feasible. Hence, approximation may be the only realistic
option.

7 Related Work
The problem of most-informative test selection was previously addressed in various areas including diagnosis, decision analysis, and feature selection in machine
learning. Given a cost function, a common decisiontheoretic approach is to compute the expected value-of-

information [10] of a candidate test, i.e., the expected cost
of making a decision after observing the test outcome.
When entropy is used as the cost function, the approach is
called most-informative test selection. In particular, mostinformative test selection was considered in the context of
model-based diagnosis [4] and probabilistic diagnosis [16].
Previous research [9, 8] on VOI analysis has made various simplifying assumptions such as binary hypothesis
and direct observations. An interesting but tangential approach was taken in [9], which proposes to select a set of
tests based on a law-of-large-numbers approximation of the
VOI. Up to now, however, no one seems to have addressed
the efficiency of computing single-test information gain in
a generic Bayesian network.
Most-informative test selection is quite similar to the optimal coding problem [2]. Namely, the hidden state vector
S is the input message, and the test outcomes T the output message from some noisy channel. The goal of mostinformative test selection is to minimize the number of bits
sent through the channel while still accurately decoding the
input message. There is, however, an important difference
between the two. In the coding domain, one may separate
source coding from channel coding. Fault diagnosis, on
the other hand, has to deal with a combination of the two,
represented by the conditional probability P (T i | Spai ).
We may have no control over the source coding function,
but we can still aim to select the smallest, most informative
subset of tests.
In the context of probing, optimal test selection is very similar to the group testing problem [5]. Given a set of Boolean
variables, the objective of group testing is to find all ’failed’
objects by using a sequence of disjunctive tests. Particularly, sequential test selection is known as adaptive group
testing [5]. (There is also a direct connection between
adaptive group testing and Golomb codes [ 7].) Note that
group testing assumes no constraints on the tests (i.e., any
subset of objects can be tested together), while in Bayesian
networks the tests can be only selected from a fixed set.
Even in a less restrictive case of probe selection, we are still
constrained by the network topology. Theoretical analysis
of constrained group testing is difficult.

8 Conclusions
We propose an entropy approximation method based on
loopy belief propagation, and examine its behavior on the
application of active probing for fault diagnosis in a networked computer system. The level of approximation error
varies slightly with the level of noise. But even so, the diagnosis quality is practically identical to that of the exact
method. Furthermore, the approximate method can handle
larger networks than the exact method, and is almost always faster on the smaller ones. This highlights a promising direction for active probing and fault diagnosis, as well

as for entropy approximation on Bayesian networks in general.




Keywords

In evaluating prediction markets (and other crowd-prediction
mechanisms), investigators have repeatedly observed a socalled wisdom of crowds effect, which can be roughly summarized as follows: the average of participants performs
much better than the average participant. The market price—
an average or at least aggregate of traders’ beliefs—offers a
better estimate than most any individual trader’s opinion.
In this paper, we ask a stronger question: how does the
market price compare to the best trader’s belief, not just
the average trader. We measure the market’s worst-case log
regret, a notion common in machine learning theory. To arrive at a meaningful answer, we need to assume something
about how traders behave. We suppose that every trader
optimizes according to the Kelly criteria, a strategy that
provably maximizes the compound growth of wealth over
an (infinite) sequence of market interactions. We show several consequences. First, the market prediction is a wealthweighted average of the individual participants’ beliefs. Second, the market learns at the optimal rate, the market price
reacts exactly as if updating according to Bayes’ Law, and
the market prediction has low worst-case log regret to the
best individual participant. We simulate a sequence of markets where an underlying true probability exists, showing
that the market converges to the true objective frequency
as if updating a Beta distribution, as the theory predicts. If
agents adopt a fractional Kelly criteria, a common practical
variant, we show that agents behave like full-Kelly agents
with beliefs weighted between their own and the market’s,
and that the market price converges to a time-discounted
frequency. Our analysis provides a new justification for fractional Kelly betting, a strategy widely used in practice for
ad-hoc reasons. Finally, we propose a method for an agent
to learn her own optimal Kelly fraction.

Auction and mechanism design, electronic markets, economically motivated agents, multiagent learning

Categories and Subject Descriptors
I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence—Intelligent agents, Multiagent systems

General Terms
Economics

Short Version Appears in: Proceedings of the 11th International Conference on Autonomous Agents and Multiagent
Systems (AAMAS 2012), Conitzer, Winikoff, Padgham, and van der
Hoek (eds.), June, 4–8, 2012, Valencia, Spain.

1.

INTRODUCTION

Consider a gamble on a binary event, say, that Obama
will win the 2012 US Presidential election, where every x
dollars risked earns xb dollars in net profit if the gamble
pays off. How many dollars x of your wealth should you risk
if you believe the probability is p? The gamble is favorable
if bp−(1−p) > 0, in which case betting your entire wealth w
will maximize your expected profit. However, that’s extraordinarily risky: a single stroke of bad luck loses everything.
Over the course of many such gambles, the probability of
bankruptcy approaches 1. On the other hand, betting a
small fixed amount avoids bankruptcy but cannot take advantage of compounding growth.
The Kelly criteria prescribes choosing x to maximize the
expected compounding growth rate of wealth, or equivalently to maximize the expected logarithm of wealth. Kelly
betting is asymptotically optimal, meaning that in the limit
over many gambles, a Kelly bettor will grow wealthier than
an otherwise identical non-Kelly bettor with probability 1
[1, 3, 7, 16, 17].
Assume all agents in a market optimize according to the
Kelly principle, where b is selected to clear the market. We
consider the implications for the market as a whole and
properties of the market odds b or, equivalently, the market probability pm = 1/(1 + b). We show that the market
prediction pm is a wealth-weighted average of the agents’
predictions pi . Over time, the market itself—by reallocating wealth among participants—adapts at the optimal rate
with bounded log regret to the best individual agent. When
a true objective probability exists, the market converges to
it as if properly updating a Beta distribution according to
Bayes’ rule. These results illustrate that there is no “price
of anarchy” associated with well-run prediction markets.
We also consider fractional Kelly betting, a lower-risk variant of Kelly betting that is popular in practice but has less
theoretical grounding. We provide a new justification for
fractional Kelly based on agent’s confidence. In this case,
the market prediction is a confidence-and-wealth-weighted
average that empirically converges to a time-discounted version of objective frequency. Finally, we propose a method
for agents to learn their optimal fraction over time.

2.

KELLY BETTING

When offered b-to-1 odds on an event with probability p,
the Kelly-optimal amount to bet is f ∗ w, where
bp − (1 − p)
b
is the optimal fixed fraction of total wealth w to commit to
the gamble.
If f ∗ is negative, Kelly says to avoid betting: expected
profit is negative. If f ∗ is positive, you have an information
edge; Kelly says to invest a fraction of your wealth proportional to how advantageous the bet is. In addition to maximizing the growth rate of wealth, Kelly betting maximizes
the geometric mean of wealth and asymptotically minimizes
the mean time to reach a given aspiration level of wealth
[17].
Suppose fair odds of 1/b are simultaneously offered on the
opposite outcome (e.g., Obama will not win the election). If
bp − (1 − p) < 0, then betting on this opposite outcome is
favorable; substituting 1/b for b and 1 − p for p, the optimal
fraction of wealth to bet becomes 1 − p − bp.
An equivalent way to think of a gamble with odds b is as
a prediction market with price pm = 1/(1 + b). The volume
of bet is specified by choosing a quantity q of shares, where
each share is worth $1 if the outcome occurs and nothing
otherwise. The price represents the cost of one share: the
amount needed to pay for a chance to win back $1. In this
interpretation, the Kelly formula becomes
f∗ =

f∗ =

p − pm
.
1 − pm

The optimal action for the agent is to trade q ∗ = f ∗ w/pm
shares, where q ∗ > 0 is a buy order and q ∗ < 0 is a sell
order, or a bet against the outcome.
Note that q ∗ is the optimum of expected log utility
p ln((1 − pm )q + w) + (1 − p) ln(−pm q + w).

odds reached when all agents are optimizing, and supply
and demand are precisely balanced. Recall that the market’s
probability implied by
Pthe odds of b is pm = 1/(1 + b). We
will show that pm is i wi pi .

4.1

Payout balance

The first approach we’ll use is payout balance: the amount
of money at risk must be the same as the amount paid out.
Theorem 1. (Market Pricing) For all normalized agent
wealths wi and agent beliefs pi ,
X
pi wi
pm =
i

Proof. To see this, recall that fi∗ = (pi − pm )/(1 − pm ) for
pi > pm . For pi < pm , Kelly betting prescribes taking the
other side of the bet, with fraction
(1 − pi ) − (1 − pm )
pm − pi
=
.
1 − (1 − pm )
pm
So the market equilibrium occurs at the point pm where the
payout is equal to the payin. If the event occurs, the payin
is
X pi − pm
X pi − pm
1
wi =
wi .
(1 + b)
1 − pm
pm i:p >p 1 − pm
i:p >p
Thus we want
X
1
pm i:p >p
i

1 − pm
pm

X pi − pm
pi − pm
wi =
wi +
1 − pm
1 − pm
i:pi >pm
X pm − pi
wi ,
pm
i:p <p

i:pi <pm

X

pi w i =

i

X

pm wi .

i

Using

P

4.2

Log utility maximization

wi = 1, we get the theorem.

An alternate derivation of the market prediction utilizes
the fact that Kelly betting is equivalent to maximizing expected log utility. Let q = x(b + 1) be the gross profit of an
agent who risks x dollars, or in prediction market language
the number of shares purchased. Then expected log utility
is
E[U (q)] = p ln((1 − pm )q + w) + (1 − p) ln(−pm q + w).
The optimal q that maximizes E[U (q)] is
q(pm ) =

w p − pm
·
.
pm 1 − pm

(1)

Proposition 2. In a market of agents each with log utility
and initial wealth w, the competitive equilibrium price is
X
pm =
wi pi
(2)

MARKET PREDICTION

In order to define the prediction market’s performance,
we must define its prediction b, or the equilibrium payoff

or

m

X pm − pi
pi − pm
wi =
wi , or
1 − pm
pm
i:pi >pm
i:pi <pm
X
X
(pi − pm )wi =
(pm − pi )wi , or

i:pi >pm

i

m

X

MARKET MODEL

Suppose that we have a prediction market,
P where participant i has a starting wealth wi with
i wi = 1. Each
participant i uses Kelly betting to determine the fraction fi∗
of their wealth bet, depending on their predicted probability
pi .
We model the market as an auctioneer matching supply and demand, taking no profit and absorbing no loss.
We adopt a competitive equilibrium concept, meaning that
agents are ”price takers”, or do not consider their own effect
on prices if any. Agents optimize according to the current
price and do not reason further about what the price might
reveal about the other agents’ information. An exception
of sorts is the fractional Kelly setting, where agents do consider the market price as information and weigh it along
with their own.
A market is in competitive
at price pm if all
P equilibrium
∗
agents are optimizing and
q
=
0,
or
every buy order
i
i
and sell order are matched. We discuss next what the value
of pm is.

4.

m

i

i

This is not a coincidence: Kelly betting is identical to maximizing expected log utility.

3.

m

i

i

where we assume
absolute wealth.

P

i wi = 1, or w is normalized wealth not

P
Proof. These prices satisfy i qi = 0, the condition for
competitive equilibrium (supply equals demand), by substitution. 2
This result can be seen as a simplified derivation of that
by Rubinstein [13, 14, 15] and is also discussed by Pennock
and Wellman [11, 10] and Wolfers and Zitzewitz [18].

5.

as
L≡

5.1

Wealth redistributed according to Bayes’
Law

In an individual round, if an agent’s belief is pi > pm ,
i −pm
wi and have a total wealth afterward
then they bet p1−p
m
dependent on y according to:

I(yt = 1) log

t=1

1
1
+ I(yt = 0) log
.
pt
1 − pt

Similarly, we measure the quality of market participant making prediction pit as
Li ≡

LEARNING PREDICTION MARKETS

Individual participants may have varying prediction qualities and individual markets may have varying odds of payoff. What happens to the wealth distribution and hence the
quality of the market prediction over time? We show next
that the market learns optimally for two well understood
senses of optimal.

T
X

T
X

I(yt = 1) log

t=1

1
1
+ I(yt = 0) log
.
pit
1 − pit

So after T rounds, the total wealth of player i is
y 
1−yt
T 
Y
pit t 1 − pit
wi
,
pt
1 − pt
t=1
where wi is the starting wealth. We next prove a well-known
theorem for learning in the present context (see for example [4]).
Theorem 3. For all sequences of participant predictions pit
and all sequences of revealed outcomes yt ,
L ≤ min Li + ln
i



If

y = 1,

If

y = 0,


1
pi − pm
pi
−1
wi + wi =
wi
pm
1 − pm
pm
pi − pm
1 − pi
(−1)
wi + wi =
wi
1 − pm
1 − pm

Similarly if pi < pm , we get:
If

y = 1,

If

y = 0,

pm − pi
pi
wi + wi =
wi
pm
pm

1
pm − pi
1 − pi
−1
wi + wi =
wi ,
1 − pm
pm
1 − pm

(−1)


This theorem is extraordinarily general, as it applies to all
market participants and all outcome sequences, even when
these are chosen adversarially. It states that even in this
worst-case situation, the market performs only ln 1/wi worse
than the best market participant i.
P
Proof. Initially, we have that i wi = 1. After T rounds,
the total wealth of any participant i is given by
y 
1−yt
T 
Y
pit t 1 − pit
wi
= wi eL−Li ≤ 1,
p
1
−
p
t
t
t=1
where the last inequality follows from wealth being conserved. Thus ln wi + L − Li ≤ 0, yielding

which is identical.
If we treat the prior probability that agent i is correct
as wi , Bayes’ law states that the posterior probability of
choosing agent i is
P (i | y = 1) =

P (y = 1 | i)P (i)
p i wi
pi w i
,
=
= P
P (y = 1)
pm
i pi wi

which is precisely the wealth computed above for the y = 1
outcome. The same holds when y = 0, and so Kelly bettors
redistribute wealth according to Bayes’ law.

5.2

Market Sequences

It is well known that Bayes’ law is the correct approach for
integrating evidence into a belief distribution, which shows
that Kelly betting agents optimally summarize all past information if the true behavior of the world was drawn from
the prior distribution of wealth.
Often these assumptions are too strong—the world does
not behave according to the prior on wealth, and it may act
in a manner completely different from any one single expert.
In that case, a standard analysis from learning theory shows
that the market has low regret, performing almost as well as
the best market participant.
For any particular sequence of markets we have a sequence
pt of market predictions and yt ∈ {0, 1} of market outcomes.
We measure the accuracy of a market according to log loss

1
.
wi

L ≤ Li + ln

6.

1
.
wi

FRACTIONAL KELLY BETTING

Fractional Kelly betting says to invest a smaller fraction
λf ∗ of wealth for λ < 1. Fractional Kelly is usually justified
on an ad-hoc basis as either (1) a risk-reduction strategy,
since practitioners often view full Kelly as too volatile, or
(2) a way to protect against an inaccurate belief p, or both
[17]. Here we derive an alternate interpretation of fractional
Kelly. In prediction market terms, the fractional Kelly formula is
p − pm
.
λ
1 − pm
With some algebra, fractional Kelly can be rewritten as
p0 − pm
1 − pm
where
p0 = λp + (1 − λ)pm .

(3)

In other words, λ-fractional Kelly is precisely equivalent to
full Kelly with revised belief λp+(1−λ)pm , or a weighted average of the agent’s original belief and the market’s belief. In

this light, fractional Kelly is a form of confidence weighting
where the agent mixes between remaining steadfast with its
own belief (λ = 1) and acceding to the crowd and taking the
market price as the true probability (λ = 0). The weighted
average form has a Bayesian justification if the agent has a
Beta prior over p and has seen t independent Bernoulli trials
to arrive at its current belief. If the agent envisions that the
market has seen t0 trials, then she will update her belief to
λp + (1 − λ)pm , where λ = t/(t + t0 ) [9, 10, 12]. The agent’s
posterior probability given the price is a weighted average of
its prior and the price, where the weighting term captures
her perception of her own confidence, expressed in terms of
the independent observation count seen as compared to the
market.

1.0

7.

MARKET PREDICTION WITH FRACTIONAL
0.9
KELLY

When agents play fractional Kelly, the competitive equilibrium price naturally changes. The resulting market price
is easily compute, as for fully Kelly agents.
Theorem 4. (Fractional Kelly Market Pricing) For all agent
beliefs pi , normalized wealths wi and fractions λi
P
λi wi pi
.
(4)
pm = Pi
l λl wl
Prices retain the form of a weighted average, but with
weights proportional to the product of wealth and self-assessed
confidence.
Proof. The proof is a straightforward corollary of Theorem 1.
In particular, we note that a λ-fractional Kelly agent of
wealth w bets precisely as a full-Kelly agent of wealth λw.
Consequently, we can apply theorem 1 with wi0 = Pλiλwiiwi
i
and p0i = pi unchanged.

8.

MARKET DYNAMICS WITH STATIONARY OBJECTIVE FREQUENCY

The worst-case bounds above hold even if event outcomes
are chosen by a malicious adversary. In this section, we
examine how the market performs when the objective frequency of outcomes is unknown though stationary.
The market consists of a single bet repeated over the
course of T periods. Unbeknown to the agents, each event
unfolds as an independent Bernoulli trial with probability of
success π. At the beginning of time period t, the realization
of event Et is unknown and agents trade until equilibrium.
Then the outcome is revealed, and the agents’ holdings pay
off accordingly. As time period t + 1 begins, the outcome of
Et+1 is uncertain. Agents bet on the t + 1 period event until
equilibrium, the outcome is revealed, payoffs are collected,
and the process repeats.
In an economy of Kelly bettors, the equilibrium price is
a wealth-weighted average (2). Thus, as an agent accrues
relatively more earnings than the others, its influence on
price increases. In the next two subsections, we examine how
this adaptive process unfolds; first, with full-Kelly agents
and second, with fractional Kelly agents. In the former case,
prices react exactly as if the market were a single agent
updating a Beta distribution according to Bayes’ rule.

0.8
0.7
0.6
20

(a)

0.035
0.030
0.025
0.020
0.015
0.010
0.005
0.000
(b)

40

0.2

60

0.4

80

100

0.6

120

0.8

140

1.0

Figure 1: (a) Price (black line) versus the observed
frequency (gray line) of the event over 150 time periods. The market consists of 100 full-Kelly agents
with initial wealth wi = 1/100. (b) Wealth after
15 time periods versus belief for 100 Kelly agents.
The event has occurred in 10 of the 15 trials. The
solid line is the posterior Beta distribution consistent with observing 10 successes in 15 independent
Bernoulli trials.

8.1

Market dynamics with full-Kelly agents

Figure 1.a plots the price over 150 time periods, in a
market composed of 100 Kelly agents with initial wealth
wi = 1/100, and pi generated randomly and uniformly on
(0, 1). In this simulation the true probability of success π
is 0.5. For comparison, the figure also shows the observed
frequency, or the number of times that E has occurred divided by the number of periods. The market price tracks
the observed frequency extremely closely. Note that price
changes are due entirely to a transfer of wealth from inaccurate agents to accurate agents, who then wield more power
in the market; individual beliefs remain fixed.
Figure 1.b illustrates the nature of this wealth transfer.
The graph provides a snapshot of agents’ wealth versus their
belief pi after period 15. In this run, E has occurred in 10
out of the 15 trials. The maximum in wealth is near 10/15 or
2/3. The solid line in the figure is a Beta distribution with
parameters 10 + 1 and 5 + 1. This distribution is precisely
the posterior probability of success that results from the
observation of 10 successes out of 15 independent Bernoulli
trials, when the prior probability of success is uniform on
(0,1). The fit is essentially perfect, and can be proved in the
limit since the Beta distribution is conjugate to the Binomial
distribution under Bayes’ Law.
Although individual agents are not adaptive, the market’s composite agent computes a proper Bayesian update.
Specifically, wealth is reallocated proportionally to a Beta
distribution corresponding to the observed number of successes and trials, and price is approximately the expected
value of this Beta distribution.1 Moreover, this correspondence holds regardless of the number of successes or failures,
or the temporal order of their occurrence. A kind of collective Bayesianity emerges from the interactions of the group.
We also find empirically that, even if not all agents are
Kelly bettors, among those that are, wealth is still redistributed according to Bayes’ rule.

8.2

0.5
0.4
0.3
0.2
0.1

where 1E(t) is the indicator function for the event at period
t, and γ is the discount factor. Note that γ = 1 recovers the
standard observed frequency.
1
As t grows, this expected value rapidly approaches the observed frequency plotted in Figure 1.

40

60

80

100

120

140

0.5
0.4
0.3
0.2
0.1

Market dynamics with fractional Kelly agents

In this section, we consider fractional Kelly agents who,
as we saw in Section 2, behave like full Kelly agents with
belief λp + (1 − λ)pm . Figure 2.a graphs the dynamics of
price in an economy of 100 such agents, along with the observed frequency. Over time, the price remains significantly
more volatile than the frequency, which converges toward
π = 0.5. Below, we characterize the transfer of wealth that
precipitates this added volatility; for now concentrate on the
price signal itself. Inspecting Figure 2.a, price changes still
exhibit a marked dependence on event outcomes, though at
any given period the effect of recent history appears magnified, and the past discounted, as compared with the observed
frequency. Working from this intuition, we attempt to fit
the data to an appropriately modified measure of frequency.
Define the discounted frequency at period n as
Pn
n−t
(1
)
t=1 γ
P E(t) n−t
,
(5)
dn = Pn
n−t
(1E(t) ) + n
(1E(t) )
t=1 γ
t=1 γ

20

(a)

(b)

20

40

60

80

100

120

140

Figure 2: (a) Price (black line) versus observed frequency (gray line) over 150 time periods for 100
agents with Kelly fraction λ = 0.2. As the frequency
converges to π = 0.5, the price remains volatile. (b)
Price (black line) versus discounted frequency (gray
line), with discount factor γ = 0.96, for the same
experiment as (a).

For example, if you allocate an initial weight of 0.5 to your
predictions and 0.5 to the market’s prediction, then the regret guarantee of section 5.2 implies that at most half of all
wealth is lost.

0.020
0.015
0.010

10.

0.005
0.000

0.2

0.4

0.6

0.8

1.0

Figure 3: (a) Wealth wi versus belief pi at period 150
of the same experiment as Figure 2 with 100 agents
with Kelly fraction λ = 0.2. The observed frequency
is 69/150 and the solid line is Beta(69 + 1, 81 + 1).
The wealth distribution is significantly more evenly
dispersed than the corresponding Beta distribution.

Figure 2.b illustrates a very close correlation between discounted frequency, with γ = 0.96 (hand tuned), and the
same price curve of Figure 2.a. While standard frequency
provides a provably good model of price dynamics in an
economy of full-Kelly agents, discounted frequency (5) appears a better model for fractional Kelly agents.
To explain the close fit to discounted frequency, one might
expect that wealth remains dispersed—as if the market’s
composite agent witnesses fewer trials than actually occur.
That’s true to an extent. Figure 3 shows the distribution of
wealth after 69 successes have occurred in 150 trials. Wealth
is significantly more evenly distributed than a Beta distribution with parameters 69+1 and 81+1, also shown. However,
the stretched distribution can’t be modeled precisely as another, less-informed Beta distribution.

9.

LEARNING THE KELLY FRACTION

In theory, a rational agent playing against rational opponents should set their Kelly fraction to λ = 0, since, in a
rational expectations equilibrium [6], the market price is by
definition at least as informative as any agent’s belief. This
is the crux of the no-trade theorems [8]. Despite the theory
[5], people do agree to disagree in practice and, simply put,
trade happens. Still, placing substantial weight on the market price is often prudent. For example, in an online prediction contest called ProbabilitySports, 99.7% of participants
were outperformed by the unweighted average predictor, a
typical result.2
In this light, fractional Kelly can be seen as an experts
algorithm [2] with two experts: yourself and the market.
We propose dynamically updating λ according to standard
experts algorithm logic: When you’re right, you increase
λ appropriately; when you’re wrong, you decrease λ. This
gives a long-term procedure for updating λ that guarantees:
• You won’t do too much worse than the market (which
by definition earns 0)
• You won’t do too much worse than Kelly betting using
your original prior p
2

http://www.overcomingbias.com/2007/02/how_and_when_to.html

DISCUSSION

We’ve shown something intuitively appealing here: selfinterested agents with log wealth utility create markets which
learn to have small regret according to log loss. There are
two distinct “log”s in this statement, and it’s appealing to
consider what happens when we vary these. When agents
have some utility other than log wealth utility, can we alter
the structure of a market so that the market dynamics make
the market price have low log loss regret? And similarly if
we care about some other loss—such as squared loss, 0/1
loss, or a quantile loss, can we craft a marketplace such that
log wealth utility agents achieve small regret with respect to
these other losses?
What happens in a market without Kelly bettors? This
can’t be described in general, although a couple special cases
are relevant. When all agents have constant absolute risk
aversion, the market computes a weighted geometric average of beliefs [10, 11, 13]. When one of the bettors acts
according to Kelly and the others in some more irrational
fashion. In this case, the basic Kelly guarantee implies that
the Kelly bettor will come to dominate non-Kelly bettors
with equivalent or worse log loss. If non-Kelly agents have
a better log loss, the behavior can vary, possibly imposing
greater regret on the marketplace if the Kelly bettor accrues
the wealth despite a worse prediction record. For this reason, it may be desirable to make Kelly betting an explicit
option in prediction markets.

11.


