
Bayesian reinforcement learning (BRL) encodes
prior knowledge of the world in a model and represents uncertainty in model parameters by maintaining a probability distribution over them. This
paper presents Monte Carlo BRL (MC-BRL), a
simple and general approach to BRL. MC-BRL
samples a priori a finite set of hypotheses for
the model parameter values and forms a discrete partially observable Markov decision process (POMDP) whose state space is a cross product of the state space for the reinforcement learning task and the sampled model parameter space.
The POMDP does not require conjugate distributions for belief representation, as earlier works
do, and can be solved relatively easily with pointbased approximation algorithms. MC-BRL naturally handles both fully and partially observable worlds. Theoretical and experimental results show that the discrete POMDP approximates the underlying BRL task well with guaranteed performance.

1. Introduction
A major obstacle in reinforcement learning is slow convergence, requiring many trials to learn an effective policy.
Model-based Bayesian reinforcement learning (BRL) provides a principled framework to tackle this difficulty. To
speed up convergence, BRL encodes prior knowledge of
the world in a model. It explicitly represents uncertainty in
model parameters by maintaining a probability distribution
over them and chooses actions that maximize the expected
long-term reward with respect to this distribution. One approach to BRL is to cast it as a partially observable Markov
decision process (POMDP) P (Duff, 2002). The state of P
Appearing in Proceedings of the 29 th International Conference
on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright
2012 by the author(s)/owner(s).

is a pair (s, θ), where s is the discrete world state for the reinforcement learning task and θ is the unknown continuous
model parameter. POMDP policy computation automatically analyzes both aspects of each action: its reward and
its contribution towards inferring unknown model parameters, thus achieving optimal trade-off between exploration
and exploitation.
Despite its elegance, this approach is not easy to use in
practice. Since model parameters are continuous in general, P has a hybrid state space and requires the restrictive
assumption of conjugate distributions to represent beliefs
during the policy computation (Duff, 2002; Poupart et al.,
2006; Ross et al., 2007; Poupart & Vlassis, 2008).
We propose Monte Carlo Bayesian Reinforcement Learning (MC-BRL), a simpler and more general approach to
BRL, based on the following observation: although there
are infinitely many parameter values, it may be possible to
compute an approximately optimal policy without considering all of them, if the objective is good average performance with respect to a prior distribution b0P of model parameters. We sample a finite set of values from b0P and form
a discrete POMDP P̂ whose state is (s, θ̂), with θ̂ taking
values from the sampled set only. This discrete POMDP
P̂ approximates the hybrid POMDP P. P̂ does not require
conjugate distributions for belief representation and can be
solved much more easily with existing point-based approximation algorithms, e.g., (Kurniawati et al., 2008). MCBRL also naturally handles both fully and partially observable worlds.
We show that MC-BRL is approximately Bayes-optimal
with a bounded error in the average case. The outputsensitive bound indicates that if a small approximately optimal policy exists, then a small number of samples is sufficient for P̂ to approximate P well. In other words, if we
treat P as a generalization of P̂ with a richer model parameter space, a small policy results in better generalization.
This nicely mirrors similar results in learning theory. We
also provide experimental results evaluating MC-BRL on
four distinct domains, including one from an application in

Monte Carlo Bayesian Reinforcement Learning

autonomous vehicle navigation.

2. Background
2.1. MDP and POMDP
An MDP is a tuple hS, A, T, R, γi, where S is a set of world
states, A is a set of actions, T (s, a, s0 ) specifies the transition probability of reaching state s0 when taking action a
in state s, R(s, a, s0 ) specifies the reward received when
taking action a in state s and reaching state s0 , and γ is a
discount factor.
A policy π : S → A for an MDP is a function that specifies
which action to take in each state s ∈ S. The value of a
policy π is defined as the expected cumulative discounted
reward
!
∞
X
t
E
γ R (st , π(st ), st+1 ) ,
t=0

where the expectation is with respect to the random variable st , the state at step t. The aim of the MDP is to find an
optimal policy π ? with maximum value.
MDPs assume that the agent can directly observe the
world state. POMDPs generalize MDPs by allowing partially observable states. Formally, a POMDP is a tuple
hS, A, O, T, Z, R, γi, where S, A, T , R, γ are as defined in
the case of MDP, O is a set of observations, and Z(s0 , a, o)
is the observation function that specifies the probability of
observing o when action a was taken in the previous step
and the current state is s0 .
In a POMDP, the agent does not know for sure its state.
Instead, it maintains a probability distribution or belief b(s)
over the state space S. A policy π : B → A for a POMDP
is a mapping from the belief space to actions. The value of
π at a belief b is defined as
!
∞
X
t
Vπ (b) = E
γ R (bt , π(bt ), bt+1 ) | b0 = b ,
t=0

where the expectation is with respect to the random variable bt , the belief at step t. Given an initial belief b0 , the
aim of the POMDP is to find an optimal policy π ? with
maximum value at b0 .
2.2. Related Works
One common approach to BRL adopts the proposal in
(Duff, 2002) and casts BRL as a POMDP P with a hybrid state space (Wang et al., 2005; Poupart et al., 2006;
Ross et al., 2007; Castro & Precup, 2007; Poupart & Vlassis, 2008; Ross & Pineau, 2008). To maintain the posterior
belief of continuous model parameters, it requires either a
closed-form representation or effective approximate inference techniques. Instead of solving P directly, MC-BRL

approximates it with a discrete POMDP P̂ by sampling
from the prior distribution and takes advantage of the recent advances in point-based discrete POMDP algorithms.
This way, we avoid the restrictive assumption of close-form
belief representation and obtain a simpler and more general
approach.
Sampling has been used extensively in BRL (Castro & Precup, 2007; Ross et al., 2007; Poupart & Vlassis, 2008; Ross
& Pineau, 2008; Asmuth et al., 2009). However, the earlier works draw samples from the posterior distributions to
speed up planning for P or to maintain beliefs efficiently.
This is conceptually different from our approach, which
samples hypotheses from the model parameter space a priori to form P̂ and works exclusively with the sampled hypotheses afterwards.
Our theoretical result shares a similar idea with that for
the (PO)MDP algorithm PEGASUS. (Ng & Jordan, 2000).
The PEGASUS analysis bounds the number of samples required to find a good policy in a policy class with finite
VC-dimension. Our result does not assume such a policy
class. It provides an output-sensitive bound that depends
on the size of the policy actually computed, instead of a
worst-case bound for all policies in a class.

3. Monte Carlo BRL
3.1. BRL as POMDP
To simplify the presentation, let us first consider BRL of
an MDP. Given an MDP hS, A, T, R, γi, the task of BRL
is to find an optimal policy when the transition function
T is unknown. Let θ = {θsas0 |s, s0 ∈ S, a ∈ A} denote the collection of unknown parameters of the MDP,
where θsas0 = T (s, a, s0 ). It has been shown that the
BRL problem can be formulated as a POMDP P =
hSP , AP , OP , TP , ZP , RP , γ, b0P i (Duff, 2002). The state
space SP = S × Θ is the cross product of the MDP states
S and the parameter space Θ. A state (s, θ) consists of a
world state s of the MDP and a hypothesized value θ of the
unknown parameter. The actions AP are identical to the
actions A in the MDP. Assuming the parameter θ does not
change over time, the transition function is defined as
TP (s, θ, a, s0 , θ0 )

= Pr(s0 , θ0 |s, θ, a)
= Pr(s0 |s, θ, a, θ0 )Pr(θ0 |s, θ, a)
= θsas0 δθθ0 ,

where δθθ0 is the Kronecker delta that takes value 1 if
θ = θ0 and value 0 otherwise. The observation of the
POMDP P indicates the current MDP state. Therefore,
we define OP = S and ZP (s0 , θ0 , a, o) = δs0 o . The reward does not depend on the parameter θ, so we have
RP (s, θ, a, s0 , θ0 ) = R(s, a, s0 ). Finally, we put a prior
distribution b0P (θ) over θ, which reflects our initial belief

Monte Carlo Bayesian Reinforcement Learning

of the unknown parameter.
This formulation explicitly represents the uncertainty in
the unknown parameter. The parameter θ forms a component of the POMDP state, which is partially observable
and can be inferred based on the history of the observed
MDP state/action pairs. By solving the POMDP P, one
plans against both the uncertainty in the dynamics and the
uncertainty in the model parameter. An optimal policy for
P thus yields an optimal strategy for action selection that
balances exploration with exploitation.
Since the parameter θsas0 takes continuous value, P has a
hybrid state space. Two difficulties arise as a result. The
first is how to efficiently maintain a belief for the continuous state variable. In order to attain a closed-form representation, most existing work assumes a conjugate prior
b0P over the parameter θ, such as the Dirichlet distribution
(Dearden et al., 1999; Duff, 2002; Poupart et al., 2006;
Ross et al., 2007; Poupart & Vlassis, 2008). The second difficulty is how to solve the hybrid POMDP P efficiently. Although several approximate algorithms based on
function approximation and online planning have been proposed (Duff, 2002; Poupart et al., 2006; Ross et al., 2007),
there is no satisfactory answer in general.
3.2. Algorithm
MC-BRL is motivated by the following observation. Although there are infinitely many possible values for the parameter θ, it may be possible to compute an approximately
optimal policy without considering all of them. MC-BRL
consists of two phases, offline and online. Given a prior
distribution b0 (θ) and a sample size K, the offline phase of
the algorithm works in three steps.


1. Sample K hypotheses θ̂1 , θ̂2 , . . . , θ̂K independently from b0 (θ).
2. Form
a
discrete
POMDP
P̂
=
hSP̂ , AP̂ , OP̂ , TP̂ , ZP̂ , RP̂ , γ, b0P̂ i. The state space is
the cross product SP̂ = S × {1, 2, . . . , K}. A state
(s, k) consists of an MDP state s and an indicator
k of the sampled hypotheses for θ. The actions
AP̂ = A and observations OP̂ = S are defined
in the same way as in Section 3.1. The transition,
observation, and reward functions are defined as
k
0 0
TP̂ (s, k, a, s0 , k 0 ) = θ̂sas
=
0 δkk 0 , ZP̂ (s , k , a, o)
0 0
0
δs o , and RP̂ (s, k, a, s , k ) = R(s, a, s0 ), respectively. Finally, the initial belief b0P̂ (k) is defined as
the uniform distribution over {1, 2, . . . , K}.
3. Solve the POMDP P̂ and output a policy π̂.
In the online phase, the agent then follows the policy π̂ to
select actions.

MC-BRL sidesteps the two technical obstacles of the existing approach based on the hybrid POMDP P. The discrete
POMDP P̂ can be readily solved with point-based approximation algorithms (Pineau et al., 2003; Smith & Simmons,
2005; Kurniawati et al., 2008). There is also no restrictive
assumption on the form of the prior distribution b0 (θ). The
only requirement is that it is easy to sample from.
We further note that P̂ falls into the class of mixed observability MDPs (MOMDPs). Its state (s, k) has mixed observability. While the second component k is hidden, the
first component s is fully observable. It has been shown
that MOMDPs admit a compact factored representation of
the state space, which can be exploited to speed up POMDP
planning (Ong et al., 2010). In this paper, we use SARSOP
(Ong et al., 2010) to solve P̂ which readily takes advantage
of the MOMDP representation.
MC-BRL takes a prior distribution b0 (θ) as input. In practice, if we know nothing about the true parameter, we use a
non-informative prior such as uniform distribution. When
there is prior knowledge about the true parameter, more informative prior can be used to bias the hypotheses towards
the ground truth.
3.3. Generalization to Partially Observable
Environments
MC-BRL can be readily generalized to BRL problems under partially observable environments. Suppose we are
given a POMDP hS, A, O, T, Z, R, γi, and we aim to find
an optimal policy when both the transition function T and
the observation function Z are unknown. The unknown
parameters can be denoted as a pair (θ, ψ), where θ is as
defined before, while ψ = {ψs0 ao |s0 ∈ S, a ∈ A, o ∈ O}
denotes the observation function and ψs0 ao = Z(s0 , a, o).
MC-BRL can be naturally adapted to address this problem
with two modifications to the offline phase. First, it samples the hypotheses from a joint prior distribution b0 (θ, ψ)
instead of b0 (θ). Second, the POMDP P̂ is modified by set0
ting OP̂ = O and ZP̂ (s0 , k 0 , a, o) = ψ̂sk0 ao . The modified
observation function ZP̂ now incorporates the uncertainty
in the unknown parameter ψ of the underlying POMDP.

4. Theoretical Analysis
MC-BRL uses the discrete POMDP P̂ to approximate the
hybrid POMDP P. To analyze the quality of this approximation, we derive a probably approximately correct (PAC)
bound on the regret of MC-BRL’s solution, compared with
the optimal solution to P.
We assume that a POMDP policy π is represented as a policy graph G, which is a directed graph with labeled nodes
and edges. Each node of G is labeled with an action a ∈ A

Monte Carlo Bayesian Reinforcement Learning

and has |O| outgoing edges, each labeled with a distinct
observation o ∈ O. The size of the policy π, denoted as
|π|, is the number of nodes in G. To execute the policy, the
agent first picks a node in G according to the initial belief.
It then takes the action associated with the node, receives
an observation, and transits to the next node by following
the edge labeled with that observation. The process then
repeats.
The policy graph representation allows us to establish the
correspondence between policies for P and P̂. If π is a policy for P, then it is also a valid policy for P̂, and vice versa,
as P and P̂ share the same action space A and observation
space O.
Suppose that MC-BRL forms the discrete POMDP P̂ by
taking K samples from the initial belief b0P of P. There are
three policies of interest: an optimal policy π ? for P, an
optimal policy π̂ ? for P̂, and the policy π̂ that MC-BRL actually computes. We want to bound the regret of π̂ against
π ? . Define Vπ as the value of a policy π for P with initial
belief b0P , and V̂π as the value of π for P̂ with initial belief b0P̂ . The following theorem states our main theoretical
result. The proof is given in the supplementary material1 .
Theorem 1. Suppose that π ? is an optimal policy for P
and π̂ is the policy that MC-BRL computes by taking K
samples to form a discrete POMDP P̂. Let Rmax =
maxs,s0 ∈S,a∈A |R(s, a, s0 )|. If V̂π̂? − V̂π̂ ≤ δ, then for any
τ ∈ (0, 1),
q
2((|π̂||O|+2) ln |π̂|+|π̂| ln |A|+ln(4/τ ))
max
Vπ? − Vπ̂ ≤ 2R
1−γ
K
+δ
with probability at least 1 − τ .
The theorem says that MC-BRL with a small set of samples
produces a good approximate solution π̂ to P with high
probability, provided that there exists a simple approximate
solution π̂ to P̂. It is interesting to observe that although we
formulate and solve the underlying reinforcement learning
task as a planning problem, this analysis closely mirrors
similar results in learning: if we think of P as a generalization of P̂ with a richer model parameter space, then the
theorem implies that a small policy results in better generalization.
The error bound consists
√ of two terms. The first term decays at the rate O(1/ K). We can reduce it by sampling
more hypotheses from the prior, but at the cost of potentially increasing the complexity of the discrete POMDP P̂
and the resulting policy π̂. The second term δ bounds the
error in the approximate solution to the discrete POMDP
1
Available
at
http://www.comp.nus.edu.sg/
leews/publications/icml2012-supp.pdf.
˜

Figure 1. Chain problem.

P̂. Algorithms such as HSVI (Smith & Simmons, 2005)
and SARSOP (Kurniawati et al., 2008) output such bounds
as a by-product of POMDP policy computation. We can
reduce δ by running these algorithms longer towards convergence.
It is also important to observe that the approximate Bayesoptimality of π̂, quantified by Vπ̂ , guarantees the average
performance of π̂ with respect to the prior distribution b0P
of models. It does not guarantee the performance of π̂ on
any particular model.
Our analysis assumes a policy graph representation of
POMDP policies.
In practice, point-based discrete
POMDP algorithms, such as HSVI and SARSOP, typically
output policies represented as a set of α-vectors, which in
principle can be converted to policy graphs.

5. Experiments
We now experiment with MC-BRL on both fully observable and partially observable reinforcement learning tasks.
First, we evaluate MC-BRL on two small synthetic domains widely used in the existing work on BRL (Sections 5.1 and 5.2). Here the standard setup requires us
to measure the performance of an algorithm on particular model parameter values rather than the average performance with respect to a prior distribution of model parameters. Therefore the bound in Theorem 1 is not applicable
here. Next, we test MC-BRL on two more realistic domains
(Sections 5.3 and 5.4), where we measure the average performance of MC-BRL and show that it performs well in
this sense, as our theoretical result guarantees.
All the experiments are conducted on a 16-core Intel Xeon
2.4GHz server.
5.1. Chain
We start with the Chain problem used in (Dearden et al.,
1998; Poupart et al., 2006). This problem consists of a
chain of 5 states and 2 actions {a, b}. The actions cause
the transitions between states and receive corresponding rewards, as shown in Figure 1. The actions are noisy. They
slip with probability 0.2 and cause the opposite effect. The
optimal policy of this problem is to always perform action
a.
We consider two versions of the Chain problem. In the

Monte Carlo Bayesian Reinforcement Learning
Table 1. Average total rewards (reported with two standard errors)
for the Chain problem. The results for Beetle and Exploit are from
(Poupart et al., 2006).
Semi-Tied

Full

MC-BRL (K = 10)
MC-BRL (K = 100)
MC-BRL (K = 1000)

3216±64
3603±32
3618±29

1661±27
1630±25
1646±32

Upper Bound
Beetle
Exploit
Q-Learning

3677
3648±41
3257±124
1560±18

3677
1754±42
3078±49
1560±18

MC-BRL+ (K = 10)
MC-BRL+ (K = 100)
MC-BRL+ (K = 1000)

−
−
−

3655±24
3644±24
3638±24

semi-tied version, we assume that the structure of transitions between states in Figure 1 are given. The only unknown parameters are the 2 slipping probabilities, one for
each action. In the full version, we assume that the transition function T (s, a, s0 ) is completely unspecified. This
leads to 40 unknown parameters.
We evaluate MC-BRL algorithm using 500 simulations
with 1000 steps in each simulation. We test K = 10, 100,
and 1000, and use the uniform prior to sample hypotheses.
Since it is a stochastic algorithm, we rerun the offline phase
of MC-BRL before each simulation, obtain a policy, and
then execute that policy online. We run the offline phase up
to 180 seconds. The online time is negligible.
Table 1 reports the average (undiscounted) total rewards of
MC-BRL. For comparison, we also report an upper bound
on the reward that could be achieved only if we had known
the true model parameters, as well as the rewards of three
alternatives: the Beetle algorithm (Poupart et al., 2006), the
Exploit heuristic, which never explores but takes the optimal action with respect to the expected MDP under the current belief, and Q-learning with -greedy exploration and
linear learning rate. For Q-learning, we test a wide range
of  values from 0 to 0.5. The reward for the optimal value
is reported.
MC-BRL achieves good performance in the semi-tied version. It obtains near-optimal reward with 1000 samples
and is comparable to Beetle. It outperforms Exploit and
Q-learning. In the full version, MC-BRL is still better than
Q-learning. However, it performs slightly worse than Beetle and is unable to improve the performance substantially
with increased number of samples. Exploit performs much
better than both MC-BRL and Beetle. However, Exploit
relies on a myopic heuristic and does not explore well in
general. For example, it performs much more poorly than
MC-BRL and Beetle in the semi-tied version.
MC-BRL’s performance degrades in the full version, be-

Table 2. Average total rewards for the Tiger problem.
Total Reward
MC-BRL (K = 10)
MC-BRL (K = 100)

68.63±13.45
113.36±2.38

Upper Bound
Prior Model

126.34±3.73
7.12±0.16

cause the sample size is too small to cover the neighborhood of the true parameters within the 40-dimensional parameter space using the uniform prior. To verify this, we
conduct another experiment by inserting the true parameter values as one of the samples of MC-BRL. The results, denoted as MC-BRL+ in Table 1, show that MCBRL achieves good performance in this case. Constructing
effective sampling strategies is an important direction for
future research.
5.2. Tiger
We next test MC-BRL on the Tiger problem (Kaelbling
et al., 1998) with partial observability. In this problem,
the agent must decide whether to open one of two doors
or to listen for the position of the tiger at each time step.
Opening the wrong door will cause the agent to be eaten by
a tiger with a penalty of −100, while opening the correct
door will give a reward of 10. Listening costs −1 and gives
the true position of the tiger with 15% error. We assume
that the transition and reward functions are given, but the
observation error rates are unknown.
We evaluate MC-BRL using 1000 simulations. Each simulation consists of 100 episodes. In each episode, the
agent takes actions and receives observation sequentially.
The episode ends when the agent opens a door and the
position of the tiger is reset. We test MC-BRL with
K = 10 and 100. Following (Ross et al., 2007), we use
Dirichlet(3, 5) as the prior distribution to sample the
unknown parameters. This prior corresponds to an expected error rate 37.5%. We run the offline phase of MCBRL up to 300 seconds.
Table 2 shows the total reward gained by MC-BRL in 100
episodes, averaged over the 1000 simulations. For reference, we also include the upper bound induced by the true
model, and the reward of the prior model in which the observation error rate is set to the prior expectation 37.5%.
With K = 100, MC-BRL achieves performance close to
the upper bound, and is far better than the prior model.
We further look into the evolution of the reward over
episodes. Figure 2 shows the reward gained by MC-BRL
per episode, averaged over the 1000 simulations. As we
do not have the exact settings used in (Ross et al., 2007),
we cannot directly compare with their experimental results.
However, we can see that MC-BRL quickly learns the un-

Monte Carlo Bayesian Reinforcement Learning
Table 3. Average total rewards over 1000 random opponents for
the IPD problem.
Total Reward

Figure 2. Average reward evolving over episodes for the Tiger
problem.

known parameters and improves over the prior model. It
achieves near-optimal performance after about 20 episodes.
5.3. Iterated Prisoner’s Dilemma
The Prisoner’s Dilemma (Poundstone, 1992) is a well
known one-shot two-player game in which each player tries
to maximize his own reward by cooperating with or betraying the other. In this section, we studied its repeated
version, the Iterated Prisoner’s Dilemma (IPD) (Axelrod,
1984), and show that MC-BRL can achieve excellent performance on this problem.
In IPD, the game is played repeatedly and each player
knows the history of his opponents moves. A key factor for
an agent to gain high reward is the capability to model the
opponent’s behaviour based on history. It has been shown
that any memoryless and one-stage memory opponent can
be modeled using 4 parameters hPS , PT , PR , PP i, which
are the probabilities that the opponent will cooperate in the
next step, given the 4 possible situations of the current step:
(1) the agent cooperates while the opponent defects (denoted by S); (2) the agent defects while the opponent cooperates (T ); (3) mutual cooperation (R); and (4) mutual
defection (P ) (Kraines & Kraines, 1995).
Suppose the agent knows the parameters of its opponent.
Then the IPD can be naturally formulated as an MDP. The
state of the MDP is the current move of the two players,
which takes values from {S, T, R, P }. The agent needs to
select between cooperating or defecting for the next move.
The transition function is defined based on the parameters
of the opponent. The reward depends on the next state, and
is set to 0, 5, 3, 1 for S, T, R, P respectively, following the
setting commonly used in IPD tournaments.
In reality, the parameters of the opponent are unknown.
The agent needs to explore the opponent’s strategy and at
the same time maximize its reward. This leads to a RL
problem and we apply MC-BRL to solve it.
We are interested in the average performance of MC-BRL
when facing various opponents. Therefore, we randomly

MC-BRL (K = 250)
MC-BRL (K = 1000)

917.92±15.97
928.03±15.70

Upper Bound
OTFT
Q-Learning
Pavlov
TFT
AP

942.75±15.74
935.80±15.60
841.61±13.86
742.15±15.49
661.24±7.98
520.13±14.87

select 1000 test opponents by uniformly sampling their parameters. For each opponent, we run the offline phase of
MC-BRL for 180 seconds and obtain a policy. We then use
the policy to play against the opponent for 300 steps and
collect the total reward. This is repeated for 20 times to
account for the stochastic behaviour of the opponent. For
MC-BRL, we test K = 250 and 1000, and use the uniform
prior to sample the parameters. We set the discount factor
γ = 0.95.
Table 3 shows the total rewards averaged over the 1000 opponents. With K = 250, MC-BRL already achieves good
rewards. With K = 1000, it approaches the upper bound,
which is achieved by solving the underlying MDP with the
true parameters of the opponents.
For reference, we also compare MC-BRL with two classic
hand-crafted strategies, Tit-for-Tat (TFT) (Axelrod, 1984)
and Pavlov (Nowak & Sigmund, 1993), and the two winning entries of the 2005 IPD tournament, Adaptive Pavlov
(AP) (Li, 2007) and Omega Tit-for-Tat (OTFT) (Slany &
Kienreich, 2007). These four strategies are used to play
against the same 1000 test opponents under the same setting as MC-BRL. The results are summarized in Table 3.
MC-BRL achieves comparable reward to OTFT, and significantly outperforms all the others. It is interesting to note
that AP, the tournament winner, performs very poorly.
TFT, Pavlov, AP, and OTFT are all specially designed to
win the IPD tournaments, while MC-BRL is a general algorithm for BRL and is not optimized for competitions. On
the other hand, one should not directly translate the good
performance of MC-BRL here to the IPD tournaments, as it
is unlikely to face random opponents. However, MC-BRL
can use more informative priors to exploit domain knowledge on the opponents, as the other algorithms do.
We further compare MC-BRL with Q-learning. We follow the setting suggested by (Littman & Stone, 2001). The
result is shown in Table 3. We can see that MC-BRL significantly outperforms Q-learning on this task.
While MC-BRL achieves good average performance,

STOP

Monte Carlo Bayesian Reinforcement Learning

A

R

Figure 3. A near-miss accident during the 2007 DARPA Urban
Challenge.

as our theorem guarantees, it can perform worse than
other algorithms when faced with particular opponents. For instance, for the opponent parameterized
by h0.806, 0.108, 0.596, 0.185i, MC-BRL obtains a much
lower reward than that of Q-learning: 596.05 versus 659.1.
5.4. Intersection Navigation
This problem is motivated by an accident in the 2007
DARPA Urban Challenge (Leonard et al., 2008). In that
event, two autonomous vehicles, R and A, approached
an uncontrolled traffic intersection as shown in Figure 3.
R had the right-of-way and proceeded. However, possibly due to sensor failure or imperfect driving strategy, A
did not yield to R and caused a near-miss. This situation is quite common and occurs frequently even with human drivers. Crossing the intersection safely and efficiently
without knowing the driving strategy of A poses a significant challenge.
We formulate the problem as a RL problem. The underlying model is a POMDP. The state consists of the positions
and velocities of R and A. For simplicity, we discretize the
environment into a uniform grid. In each step, the agent
R can take three actions: accelerate, maintain speed, and
decelerate. It then receives an observation on its own state
and the state of A. Both actions and observations are noisy.
The transition function is defined based on the driving strategy of A, which is unknown to the agent R. The agent receives a reward for crossing the intersection safely, and a
large penalty for collision with A. A small penalty is given
in each step to expedite the agent to cross the intersection
faster. Due to space limitation, we give the detailed settings
in the supplementary material.
The driving strategy of A is unknown to the agent. We
parameterize the driving strategy using 4 parameters: (1)
driver imperfection, σ ∈ [0, 1], (2) driver reaction time,
τ ∈ [0.5, 2] s, (3) acceleration, a ∈ [0.5, 3] m/s2 , and (4)
deceleration, d ∈ [−3, −0.5] m/s2 . A preliminary study
shows that this parameterization can cover a variety of
drivers such as a reckless driver who never slows down
at the intersection and an impatient driver who performs a

Figure 4. Average discounted total reward for the Intersection
Navigation problem versus sample size K, reported with two
standard error bar.

rolling stop near the intersection. The agent needs to learn
the parameters of A and cross the intersection at the same
time.
We test MC-BRL on this RL problem. We test a range
of K values and sample the parameters from the uniform
distribution. Similar to the IPD problem, we are interested
in the average performance of MC-BRL with respect to different drivers A. Therefore, we uniformly sampled 250 test
drivers. For each driver, we run the offline phase of MCBRL for 1.5 hours and obtain a policy. We then evaluate the
policy against that test driver using 200 simulations with 40
steps in each simulation.
Figure 4 shows the average discounted total rewards with
discount factor γ = 0.99. We can see that, as the sample
size K increases, the performance of MC-BRL improves
quickly. With K = 300, it gets close to the upper bound,
which is achieved when the true parameters of the driver A
are known.
We also compare MC-BRL to a hand-crafted intersection
policy that is commonly used in the traffic modeling community (Liu & Ozguner, 2007). With K = 150 and above,
MC-BRL significantly outperforms that policy. While the
hand-crafted policy is not designed to handle noisy observations, we think that the performance gap between
the hand-crafted policy and MC-BRL is more likely to be
caused by insufficient adaptivity of the hand-crafted policy
in learning the driving strategy of A.
As a final remark, this problem gives an example where it is
more natural to define the prior over the physical properties
of the environment. MC-BRL handles such priors easily,
although they are challenging to specify using methods that
rely on conjugate distributions.

6. Conclusion
We have presented MC-BRL, a simple and general approach to Bayesian reinforcement learning. We prove that
by sampling a finite set of hypotheses from the model

Monte Carlo Bayesian Reinforcement Learning

parameter space, MC-BRL generates a discrete POMDP
that approximates the underlying BRL problem well with
guaranteed performance. We provide experimental results
demonstrating strong performance of the approach in practice. Furthermore, MC-BRL naturally handles both fully
and partially observable worlds.

Kraines, D. and Kraines, V. Evolution of learning among pavlov
strategies in a competitive environment with noise. Journal of
Conflict Resolution, 39:439–466, 1995.

One important issue for MC-BRL is to sample the model
parameter space effectively. A naive method is to discretize the parameter space uniformly and treat the fixed
grid points as samples. This method, however, suffers from
the “curse of dimensionality” and is difficult to scale up as
the number of parameters increases (Poupart et al., 2006).
MC-BRL takes one step further and samples a set of hypotheses independently from a given prior distribution. The
promising results obtained in this work open up many possibilities for future investigation, e.g., constructing better
informed prior distributions by exploiting domain knowledge and adaptive sampling.

Leonard, J., How, J., and Teller, S. A perception driven autonomous urban vehicle. Journal of Field Robotics, 25(10):
727–774, 2008.

Acknowledgments

Kurniawati, H., Hsu, D., and Lee, W. S. SARSOP: Efficient pointbased POMDP planning by approximating optimally reachable
belief spaces. In RSS, 2008.

Li, J. How to design a strategy to win an IPD tournament. In The
Iterated Prisoners’ Dilemma: 20 Years On, 2007.
Littman, M. L. and Stone, P. Leading best-response strategies
in repeated games. In IJCAI Workshop on Economic Agents,
Models, and Mechanisms, 2001.
Liu, Y. and Ozguner, U. Human driver model and driver decision making for intersection driving. IEEE Intelligent Vehicles
Symposium, pp. 642–647, 2007.
Ng, A. and Jordan, M. PEGASUS: A policy search method for
large MDPs and POMDPs. In UAI, pp. 406–415, 2000.
Nowak, M. and Sigmund, K. A strategy of win-stay, lose-shift that
outperforms tit-for-tat in the prisoner’s dilemma game. Nature,
364, 1993.

Y. Wang and D. Hsu are supported in part by MoE AcRF
grant 2010-T2-2-071 and MDA GAMBIT grant R-252000-398-490. K.S. Won is supported by an NUS President’s Fellowship. W.S. Lee is supported in part by the
Air Force Research Laboratory, under agreement number
FA2386-12-1-4031. The views and conclusions contained
herein are those of the authors and should not be interpreted
as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory or the U.S. Government.

Poundstone, W. Prisoner’s Dilemma. Doubleday, New York, NY,
1992.



We apply decision theoretic techniques to construct nonplayer characters that are able to assist a human player in collaborative games. The method is based on solving Markov
decision processes, which can be difficult when the game
state is described by many variables. To scale to more complex games, the method allows decomposition of a game task
into subtasks, each of which can be modelled by a Markov
decision process. Intention recognition is used to infer the
subtask that the human is currently performing, allowing the
helper to assist the human in performing the correct task. Experiments show that the method can be effective, giving nearhuman level performance in helping a human in a collaborative game.

Introduction
Traditionally, the behaviour of Non-Player Characters
(NPCs) in games is hand-crafted by programmers using
techniques such as Hierarchical Finite State Machines (HFSMs) and Behavior Trees (Champandard 2007). These techniques sometimes suffer from poor behavior in scenarios
that have not been anticipated by the programmer during
game construction. In contrast, techniques such as Hierarchical Task Networks (HTNs) or Goal-Oriented Action
Planner (GOAP) (Orkin 2004) specify goals for the NPCs
and use planning techniques to search for appropriate actions, alleviating some of the difficulties of having to anticipate all possible scenarios.
In this paper, we study the problem of creating NPCs that
are able to help players play collaborative games. The main
difficulties in creating NPC helpers are to understand the intention of the human player and to work out how to assist
the player. Given the successes of planning approaches to
simplifying game creation, we examine the application of
planning techniques to the collaborative NPC creation problem. In particular, we extend a decision-theoretic framework
Copyright c 2011, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

for assistance used in (Fern and Tadepalli 2010) to make it
appropriate for game construction.
The framework in (Fern and Tadepalli 2010) assumes that
the computer agent needs to help the human complete an unknown task, where the task is modeled as a Markov decision
process (MDP) (Bellman 1957). The use of MDPs provide
several advantages such as the ability to model noisy human
actions and stochastic environments. Furthermore, it allows
the human player to be modelled as a noisy utility maximization agent where the player is more likely to select actions
that has high utility for successfully completing the task. Finally, the formulation allows the use of Bayesian inference
for intention recognition and expected utility maximization
in order to select the best assistive action.
Unfortunately, direct application of this approach to
games is limited by the size of the MDP model, which grows
exponentially with the number of characters in a game. To
deal with this problem, we extend the framework to allow
decomposition of a task into subtasks, where each subtask
has manageable complexity. Instead of inferring the task
that the human is trying to achieve, we use intention recognition to infer the current subtask and track the player’s intention as the intended subtask changes through time.
For games that can be decomposed into sufficiently small
subtasks, the resulting system can be run very efficiently in
real time. We perform experiments on a simple collaborative
game and demonstrate that the technique gives competitive
performance compared to an expert human playing as the
assistant.

Scalable Decision Theoretic Framework
We will use the following simple game as a running example, as well as for the experiments on the effectiveness of
the framework. In this game, called Collaborative Ghostbuster, the assistant (illustrated as a dog) has to help the
human kill several ghosts in a maze-like environment. A
ghost will run away from the human or assistant when they
are within its vision limit, otherwise it will move randomly.
Since ghosts can only be shot by the human player, the dog’s

role is strictly to round them up. The game is shown in Figure 1. Note that collaboration is often truly required in this
game - without surrounding a ghost with both players in order to cut off its escape paths, ghost capturing can be quite
difficult.

This algorithm is guaranteed to converge to the optimal
value function V ∗ (s), which gives the expected cumulative
reward of running the optimal policy from state s.
The optimal value function V ∗ can be used to construct
the optimal actionsPby taking action a∗ in state s such that
a∗ = argmaxa { s0 Ta (s, s0 )V ∗ (s0 )}. The optimal Qfunction is constructed from V ∗ as follows:
X
Q∗ (s, a) =
Ta (s, s0 )(Ra (s, s0 ) + γV ∗ (s0 )).
s0

The function Q∗ (s, a) denotes the maximum expected longterm reward of an action a when executed in state s instead
of just telling how valuable a state is, as does V ∗ .

Figure 1: A typical level of Collaborative Ghostbuster. The
protagonists, Shepherd and Dog in the bottom right corner,
need to kill all three ghosts to pass the level.

Markov Decision Processes
We first describe a Markov decision process and illustrate
it with a Collaborative Ghostbuster game that has a single
ghost.
A Markov decision process is described by a tuple
(S, A, T, R) in which
• S is a finite set of game states. In single ghost Collaborative Ghostbuster, the state consists of the positions of the
human player, the assistant and the ghost.
• A is a finite set of actions available to the players; each
action a ∈ A could be a compound action of both players.
If each of the human player and the assistant has 4 moves
(north, south, east and west), A would consist of the 16
possible combination of both players’ moves.
• Ta (s, s0 ) = P (st+1 = s0 |st = s, at = a) is the probability that action a in state s at time t will lead to state s0
at time t + 1. The human and assistant move deterministically in Collaborative Ghostbuster but the ghost may
move to a random position if there are no agents near it.
• Ra (s, s0 ) is the immediate reward received after the state
transition from s to s0 triggered by action a. In Collaborative Ghostbuster, a non-zero reward is given only if the
ghost is killed in that move.
The aim of solving an MDP is to obtain a policy
maximizes the expected cumulative reward
P∞π that
t
t=0 γ Rπ(st ) (st , st+1 ) where 0 < γ < 1 is the discount
factor.
Value Iteration. An MDP can be effectively solved using a simple algorithm proposed by Bellman in 1957 (Bellman 1957). The algorithm maintains a value function V (s),
where s is a state, and iteratively updates the value function
using the equation
!
X
0
0
0
Vt+1 (s) = max
Ta (s, s )(Ra (s, s ) + γVt (s )) .
a

s0

Intractability. One key issue that hinders MDPs from being widely used in real-life planning tasks is the large state
space size (usually exponential in the number of state variables) that is often required to model realistic problems.
Typically in game domains, a state needs to capture all essential aspects of the current configuration and may contain
a large number of state variables. For instance, in a Collaborative Ghostbuster game with a maze of size m (number
of valid positions) consisting of a player, an assistant and n
ghosts, the set of states is of size O(mn+2 ), which grows
exponentially with the number of ghosts.

Subtasks
To handle the exponentially large state space, we decompose
a task into smaller subtasks and use intention recognition to
track the current subtask that the player is trying to complete.

Figure 2: Task decomposition in Collaborative Ghostbuster.
In Collaborative Ghostbuster, each subtask is the task of
catching a single ghost, as shown in Figure 2. The MDP for
a subtask consists of only two players and a ghost and hence
has manageable complexity.

Human Model of Action Selection In order to assist effectively, the AI agent must know how the human is going to
act. Without this knowledge, it is almost impossible for the
AI to provide any help. We assume that the human is mostly
rational and use the Q-function to model the likely human
actions.
Specifically, we assume
maxaAI Q∗
i (si ,ahuman ,aAI )

P (ahuman |wi , si ) = α.e
(1)
where α is the normalizing constant, wi represents subtask i
and si is the state in subtask i. Note that we assume that the
human player knows the best response from the AI sidekick
and plays his part in choosing the action that matches the
most valued action pair. However, the human action selection can be noisy, as modelled by Equation (1).

Intention Recognition and Tracking
We use a probabilistic state machine to model the subtasks
for intention recognition and tracking. At each time instance, the player is likely to continue on the subtask that
he or she is currently pursuing. However, there is a small
probability that the player may decide to switch subtasks.
This is illustrated in Figure 3, where we model a human
player who tends to stick to his chosen sub-goal, choosing
to solve the current subtask 80% of the times and switching
to other sub-tasks 20% of the times. The transition probability distributions of the nodes need not be homogeneous, as
the human player could be more interested in solving some
specific subtask right after another subtask. For example, if
the ghosts need to be captured in a particular order, this constraint can be encoded in the state machine. The model also
allows the human to switch back and forth from one subtask
to another during the course of the game, modelling change
of mind.

where T (wj → wi ) is the switching probability from subtask j to subtask i.
Next, we compute the posterior belief distribution using
Bayesian update, after observing the human action a and
subtask state si,t at time t, as follows:
Bt (wi |at = a, st , θt−1 ) = α.Bt (wi |θt−1 ).P (at = a|wi , si,t )
(3)
where α is a normalizing constant. Absorbing current human action a and current state into θt−1 gives us the game
history θt at time t.
Complexity This component is run in real time, and thus
its complexity dictates how responsive our AI is. We are
going to show that it is at most O(k 2 ), with k being the
number of subtasks.
The first update step as depicted in Equation 2 is executed
for all subtasks, thus of complexity O(k 2 ).
The second update step as of Equation 3 requires the computation of P (at = a|wi , si ) (Equation 1), which takes
O(|A|) with A being the set of compound actions. Since
Equation 3 is applied for all subtasks, that sums up to
O(k|A|) for this second step.
In total, the complexity of our real-time Intention Recognition component is O(k 2 + k|A|), which will be dominated
by the first term O(k 2 ) if the action set is fixed.

Decision-theoretic Action Selection
Given a belief distribution on the players targeted subtasks
as well as knowledge to act collaboratively optimally on
each of the subtasks, the agent chooses the action that maximizes its expected reward.
(
)
X
i
∗
Bt (wi |θt )Qi (st , a)
a = argmaxa
i

CAPIR: Collaborative Action Planner with
Intention Recognition
We implement the scalable decision theoretic framework
as a toolkit for implementing collaborative games, called
Collaborative Action Planner with Intention Recognition
(CAPIR).
Figure 3: A probabilistic state machine, modeling the transitions between subtasks.
Belief Representation and Update The belief at time t,
denoted Bt (wi |θt ), where θt is the game history, is the conditional probability of that the human is performing subtask
i. The belief update operator takes Bt−1 (wi |θt−1 ) as input
and carries out two updating steps.
First, we obtain the next subtask belief distribution, taking into account the probabilistic state machine model for
subtask transition T (wk → wi )
X
Bt (wi |θt−1 ) =
T (wj → wi )Bt−1 (wj |θt−1 )
(2)
j

CAPIR’s Architecture
Each game level in CAPIR is represented by a GameWorld
object, which consists of two Players and multiple SubWorld
objects, each of which contains only the elements required
for a subtask (Figure 4). The game objective is typically to
interact with these NPCs in such a way that gives the players the most points in the shortest given time. The players
are given points in major events such as successfully killing
a monster-type NPC or saving a civilian-type NPC – these
typically form the subtasks.
Each character in the game, be it the NPC or the protagonist, is defined in a class of its own, capable of executing
multiple actions and possessing none or many properties.
Besides movable NPCs, immobile items, such as doors or

Figure 4: GameWorld’s components.
shovels, are specified by the class SpecialLocation. GameWorld maintains and updates an internal game state that captures the properties of all objects.
At the planning stage, for each SubWorld, an MDP is generated and a collaboratively optimal action policy is accordingly computed (Figure 5). These policies are used by the AI
assistant at runtime to determine the most appropriate action
to carry out, from a decision-theoretic viewpoint.







   
  

   

   





  
   

 

   

!  "#

$ $     

"#   

(
'


%

&

Figure 5: CAPIR’s action planning process. (a) Offline subtask Planning, (b) in-game action selection using Intention
Recognition.

busters. We chose five levels (see Appendix) with roughly
increasing state space size and game play complexity to assess how the technique can scale with respect to these dimensions.
The participants were requested to play five levels of the
game as Shepherd twice, each time with a helping Dog controlled by either AI or a member of our team, the so-called
human expert in playing the game. The identity of the dog’s
controller was randomized and hidden from the participants.
After each level, the participants were asked to compare
the assistant’s performance between two trials in terms of
usefulness, without knowing who controlled the assistant at
which turn.
In this set of experiments, the player’s aim is to kill three
ghosts in a maze, with the help of the assistant dog. The
ghosts stochastically1 run away from any protagonists if they
are 4 steps away. At any point of time, the protagonists could
move to an adjacent free grid square or shoot; however, the
ghosts only take damage from the ghost-buster if he is 3
steps away. This condition forces the players to collaborate
in order to win the game. In fact, when we try the game with
non-collaborative dog models such as random movement,
the result purely relies on chance and could go on until the
time limit (300 steps) runs out, as the human player hopelessly chases ghosts around obstacles while the dog is doing
some nonsense at a corner. Oftentimes the game ends when
ghosts walk themselves into dead-end corners.
The twenty participants are all graduate students at our
school, seven of whom rarely play games, ten once to twice
a week, and three more often.
When we match the answers back to respective controllers, the comparison results take on one of three possible
values, being AI assistant performing “better”, “worse” or
“indistinguishable” to the human counterpart. The AI assistant is given a score of 1 for a “better”, 0 for an “indistinguishable” and -1 for a “worse” evaluation.
Qualitative evaluation For simpler levels 1, 2 and 3, our
AI was rated to be better or equally good more than 50%
the times. For level 4, our AI rarely got the rating of being
indistinguishable, though still managed to get a fairly competitive performance. Subsequently, we realized that in this
particular level, the map layout is confusing for the dog to
infer the human’s intention; there is a trajectory along which
the human player’s movement could appear to aim at any
one of three ghosts. In that case, the dog’s initial subtask belief plays a crucial role in determining which ghost it thinks
the human is targeting. Since the dog’s belief is always initialized to a uniform distribution, that causes the confusion.
If the human player decides to move on a different path, the
AI dog is able to efficiently assist him, thus getting good ratings instead. In level 5, our AI gets good ratings only for
less than one third of the times, but if we count “indistinguishable” ratings as satisfactory, the overall percentage of
positive ratings exceeds 50%.

Experiment and Analysis
In order to evaluate the performance of our AI system, we
conducted a human experiment using Collaborative Ghost-

1
The ghosts run away 90% of the times and perform some random actions in the remaining 10%.

12	
  
10	
  
8	
  
-­‐1	
  

6	
  

0	
  
1	
  

4	
  
2	
  
0	
  
1	
  

2	
  

3	
  

4	
  

5	
  

Figure 6: Qualitative comparison between CAPIR’s AI assistant and human expert. The y-axis denotes the number of
ratings.
120	
  
100	
  
80	
  

AI	
  

60	
  

Human	
  

40	
  
20	
  
0	
  
1	
  

2	
  

3	
  

4	
  

5	
  

Figure 7: Average time, with standard error of the mean as
error bars, taken to finish each level when the partner is AI
or human. The y-axis denotes the number of game turns.
Quantitative evaluation Besides qualitative evaluation,
we also recorded the time taken for participants to finish
each level (Figure 7). Intuitively, a well-cooperative pair
of players should be able to complete Collaborative Ghostbuster’s levels in shorter time.
Similar to our qualitative result, in levels 1, 2 and 3, the
AI controlled dog is able to perform at near-human levels in
terms of game completion time. Level 4, which takes the
AI dog and human player more time on average and with
higher fluctuation, is known to cause confusion to the AI
assistant’s initial inference of the human’s intention and it
takes a number of game turns before the AI realizes the true
target, whereas our human expert is quicker in closing down
on the intended ghost. Level 5, larger and with more escape
points for the ghosts but less ambiguous, takes the protagonist pair (AI, human) only 4.3% more on average completion
time.

Related Work
Since plan recognition was identified as a problem on its
own right in 1978 (Schmidt, Sridharan, and Goodson 1978),
there have been various efforts to solve its variant in different domains. In the context of modern game AI research,
Bayesian-based plan recognition has been inspected using

different techniques such as Input Output Hidden Markov
Models (Gold 2010), Plan Networks (Orkin and Roy 2007),
text pattern-matching (Mateas and Stern 2007), n-gram and
Bayesian networks (Mott, Lee, and Lester 2006) and dynamic Bayesian networks (Albrecht, Zukerman, and Nicholson 1998). As far as we know, our work is the first to use
a combination of precomputed MDP action policies and online Bayesian belief update to solve the same problem in a
collaborative game setting.
Related to our work in the collaborative setting is the work
reported by Fern and Tadepalli (Fern and Tadepalli 2010)
who proposed a decision-theoretic framework of assistance.
There are however several fundamental differences between
their targeted problem and ours. Firstly, they assume the task
can be finished by the main subject without any help from
the AI assistant. This is not the case in our game, which
presents many scenarios in which the effort from one lone
player would amount to nothing and a good collaboration
is necessary to close down on the enemies. Secondly, they
assume a stationary human intention model, i.e. the human
only has one goal in mind from the start to the end of one
episode, and it is the assistant’s task to identify this sole intention. In contrary, our engine allows for a more dynamic
human intention model and does not impose a restriction on
the freedom of the human player to change his mind mid
way through the game. This helps ensure our AI’s robustness when inferring the human partner’s intention.
In a separate effort that also uses MDP as the game AI
backbone, Tan and Cheng (Tan and Cheng 2010) model the
game experience as an abstracted MDP - POMDP couple.
The MDP models the game world’s dynamics; its solution
establishes the optimal action policy that is used as the AI
agent’s base behaviors. The POMDP models the human play
style; its solution provides the best abstract action policy
given the human play style. The actions resulting from the
two components are then merged; reinforcement learning is
applied to choose an integrated action that has performed
best thus far. This approach attempts to adapt to different
human play styles to improve the AI agent’s performance. In
contrast, our work introduces the multi-subtask model with
intention recognition to directly tackle the intractability issue of the game world’s dynamics.

Conclusions
We describe a scalable decision theoretic approach for constructing collaborative games, using MDPs as subtasks and
intention recognition to infer the subtask that the player is
targeting at any time. Experiments show that the method is
effective, giving near human-level performance.
In the future, we also plan to evaluate the system in more
familiar commercial settings, using state-of-the-art game
platforms such as UDK or Unity. These full-fledged systems offer development of more realistic games but at the
same time introduce game environments that are much more
complex to plan. While experimenting with Collaborative
Ghostbuster, we have observed that even though Value Iteration is a simple naive approach, in most cases, it suffices,
converging in reasonable time. The more serious issue is the

state space size, as tabular representation of the states, reward and transition matrices takes much longer to construct.
We plan to tackle this limitation in future by using function
approximators in place of tabular representation.

Appendix
Game levels used for our experiments.

Acknowledgments
This work was supported in part by MDA GAMBIT grant R252-000-398-490 and AcRF grant T1-251RES0920 in Singapore. The authors would like to thank Qiao Li (NUS),
Shari Haynes and Shawn Conrad (MIT) for their valuable
feedbacks in improving the CAPIR engine, and the reviewers for their constructive criticism on the paper.



