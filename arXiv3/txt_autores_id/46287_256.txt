
The paper describes aHUGIN, a tool for cre­
ating adaptive systems. aHUGIN is an exten­
sion of the HUG IN shell, and is based on t he
methods repo r ted by Spiegelhalter and Lau­
r itzen {1990a). The adaptive systems resu lt­
ing from aHUGIN are able to adj u s t the con­
ditional probabi lities in the modeL A short
analysis of the adap tation task is gi ven and
the fe atures of aHUGIN are des cribed. Fi­
nally a sess i on with experiments is reported
and the results are discussed.

1

Introduction

With the revival of Bayesian methods in decision sup­

port systems ( Shachter 1986; Pearl 1 988; Shafer and
Pearl 1990; Andreassen ei al. 1991b) mainly due to

the construction of e ffi cien t methods for belief re vi­

sion in causal probabilistic networks (Pearl 1988; L au­
ritzen and Spiegelhalter 1988; Andersen d a/. 1989;

et al. 1990; Shenoy and Shafer 1990), the
process of knowledge acq uisit ion under the Bayesi an
Jensen

paradigm has become increasingly important. Wh en
construc ti ng causal probabilistic network models, var­
ious sources may be used, ranging from ignorance over
experts' subjective assessments to well established sci­
entific theories and statistical models based on large
database s . Very often a mod el is a mixtu re of contri­
butions from sources of different epistemological char­
acter.

Sometimes these contributions do not coin ci de , and

the model is a mediation between them; sometimes the
(ignorance has, for ex­
ample, forced crude 'guesses' on certain distributions);
sometimes the model must vary with contexts which
cannot be specified beforehand; sometimes the domain
is drifting ov er time - requiring the mo del to drift

r esu lt in g model is incomplete

along with i t, and sometimes the model quite s i mply
does not reflect the real world pr op erly.
All the proble ms listed above call for pr ocedures which
enable the sy stem to mod ify the mo d el t h ro ugh ex p eri-

Finn V . .Tens en

ence. We call such an a ctiv ity adaptation, and sy stems

p erfo rm in g automatic adaptation
tems.
we

Note that

we

call adaptwe

sys­

have chosen t.o distinguish adaptation

which we usc to describe the activity
or creat in g models by hatch-pmccssing of large data.
from training,

( 10D2)

bases. In Spiegelhalter el rd

called

leaming and they

arc

both activities are

dist-inguished as sequent-i a l

learning and batch learning.

W h en using adaptat.ion

we <He us i ng the analogy to tlw n ot i on

of adaptive
reg u lato rs in control t h eo r y . llopefully, t h is abundance
of te rm i n o logy will not. confuse the reader completely.
The present paper descri hes all UG IN, a tool for cre­
ating adaptive systems. Tlw system, which is an ex­
tension of

IJUGIN ( Andersen

ct a/.

198\J), is based

on methods reported in Spiegellta.lt.er <Hid Lau ntzen
(l!.l!.lOa), see also Spiegelhaltcr and Lamit.zcn (1090b),

and th e adapt ive s y s te ms result in g from aliUGIN arc
able to adjust the con d i t i on a l probabilities in
mo d el .

sented

the

In a l l UGIN the mo d el is compactly repre­

by

a

contingency

table of i m a gi nary counts,

the adaptation p roced u re is
counts in this table.

a

and

proccs� of 111odifying

the

In se ction 2 we give a short analysis of the adaptation
discuss various simple adapt.ation methods
lead ing up to a dcscri ption of the one used in all U GIN.

task and

In section 3

we

d cscri h e the f"caturcs ofalllJCJN, a11d

in section '1 a session with t·xpcrillwnts
the

results

ar<'

disciiSSt'd.

i� r···portr·d

and

( 1 \JU2), r�nd Cowell ( I\J\J2) clt'­
difl'erent
ma.in difl'ercnre bctii'('CII t. h is sys­

Spiegclhalt.cr and Cc)\\"cll

scribe

a similar systt'lll a.nd rt'sult.s of s l ig h tly

experiments.

The

tem and t he i r system, is that. we allow an extra facilit.y,
\ailed fadwy, that. makf's the systcrn forget. t.h<' past at.
an ex p onenti a l rate, t.herel>y 111aking them 11101-e prone

to adapt in changing cnvir-onmcnt.s.

2

Analysis of adaptation

CPN m o dels have both a quantit.at.ive a n d a. qualita­
tive aspect. Througl1 t.hc eli reeled aTcs, the networ k
reflects the only ways in which variables mny have im-

224

Olesen, Lauritzen, and Jensen
pact on each other. The st.rengt.h of the impact is mod­
e ll ed through conditional probability tables. We shall
here describe how the probabil ity tables are modifi ed
in the adaptation process.
So, consider a causal probabilistic network into which
information on the state of the variables can be en­
t ered
If the state of only some of the variables is
known, then the probability distributions for the re­
maining variables are calculated. Each time this is
done, we have described a case. Now, a large number
of cases is at hand, an d we want to improve the model
.

by adjusting the conditional probability tables to the
set of cases.
2.1

Direct modelling of table uncertRinty

In Fig u re 1 ( a ) , the state of the variable A is influenced
direct ly by the states of B and C, and the strength

of this influence is modelled by P(AIB, C). If the
this may, for exam­
stren gth is subject to doubt
ple, be due to d ifferent estimates from experts, or it
may be due to a context influence not mod elled (like
soil quality of corn fields or ge ne t i c disposition for a
disease)
then this doubt may be modelled directly
by introducing an extra parent, T, for A ( F igu re 1
( b)) . This variable can be c onsi d e r ed as a ty pe vari­
able modelling, for example, types of context or differ­
ent experts' assessments. To reflect credibility of the
experts or frequencies of the co ntext types, a prior dis­
tribution forT could be given. When a case is entered
-

-

ma.y be neces,;ar·y. These <lrffi•renl. t.ypes rnay !w lw:n·­
ily interdependent (for example, assessments from Vilr­
ious intersecting sets of experts) and it. may he neces­
sar y to construct a COirlplcx t.ype nel.work with t.he risk

A simplifying assump­
ti o n would be global independence: the context depen­
dence for the conditional probabilities are mutually in­
dependent. In that case, each variable can he given its
own parent of ty p es and retrieval and dissemination
are completely lo c al (perfonned as above). However,
the pro ce d u re is still v u l nera b l e t.o combinatorial PX­
plosion. Take for instance the variahlc A in Figure I.
For each parent configurat.ion a type dcpcnclcncc on
the distribu tion for A shall be described. These de­
pe nden c ies may vary a lot with tile p a rticu la r parent
con figu rations and all kinds of intcr-dcpcndrncics rnay

of a combinatorial explosion.

,

be prcsellt.

Tlw1·d'or<'

WI'

('orc1�d

""'Y lw

in<'l'\';1"''

to

the p mbabil i ty tables by a. factor which is the prodtrct
of the number of states in l11c parents. So, a further

simplification wo uld he lorn/ independence: tlw ron­
text dependence for the various p<HCII1 coni\gtrratinns
ar e mutually independent.

Indirect moddliug of table 111H:1�rtaint.y

2.2

If nothing is known

a p ossibl e

011

t.!w structure of llw

inadequacy of the rnodello

the

GlliSI�S

case

for

set, the

uncertainty can not he represented directly through
a

network of discr e t e types,

and we rnust. leave

roorn

for all kinds of types witlt <dl kinds of distrdartions.

The learning process h e re is

as

everywhere else in t.he

B ayesi a n p ar a d igm : Specif"y a prior distribution

of the

ty pes and calculate the posteri or given the case ob­
served . It remai ns to find a natural way of spccifyi11g
,

and

such a p robabil i ty model. Sricgelh a ltc r

(Hl90a)

give

a

Lauritu·n

range of pos;,ihi\ities, incl11ding normal­

l og istic models.

The simplest probability n1odel which is convenient. f"or
cue I

(a)

case 2
(b)

Figure 1: Ad apt ation th ro ugh a type variable.
to the CPN, the calculation of updated probabilities
will yield a new distribution on T, an d we may say that
the change of these probabi litie s reflects what we have
learnt from the case. This process is called retrieval
of experience. The new dist ri bution may now be used
as prior probabilities for the next case and its impact
on the conditional probabilities found by summing out
the type variable. This process is called dzssemination
of experience. The technique has, for example, been
used in Andreassen et al. (199la), where the system
con tain s a model for metab ol i sm in patients suffering
from diabetes. Through a type variable, the syst em
adapts to the characteristics of the individual patient.
Several conditional probabilities in the CPN may be
context dependent, and a whole set of type variables

th is

situation assu rncs that each set of entries i 11 the

conditional probability tahlcs for

configuration follows

( .Johnson

and Kotz

distribution has �:
sit y

f(pt,····Pk-ll
f•or

p; > ()

I

atl<

The simplicity is
p ro p er )

a

so-called

a

part.icular parent

/Jmchlr!-rhslnh·ulnm

l!li2). A k:-dilll<"nsiOJd Dirichl,'t­
paJ<It\\cV•rs, (n1, . . . , rq.) <�ttd d<'ll­

(X

(

k-J

1-

LJil

)"k-1

•=I

k-1

II;;;··-!.
•=1

(I)

"\'"'k- I
L,=1 p, <I.
in

the

distribution with

itt\.crprcl.a.tion�
tri

=

0 for all

lf

i

the (im­

is

consid­

specified
may be interpreted a.� representing past exper ie nce as a
coni.ingency iable (cl'J, .. . . nk) of counl.� of pas! casrs.

ered

a

noninformativc prior, the distribution

s = L; rt; i:; thcr·eforc refet·red to as 1/ic
equivalent sample size. The updating p roced ure con­

The quanti ty

sists of modifying the cou nt.s as new cases
observed.

We shall

not give details,

hut. just

state

arc

that

bci 11g

the frac-

aHUGIN: A System Creating Adaptive Causal Probabilistic Networks

tion a;/ s = m; is t he mean for the ith outcome, and
for each i the variance of the probability for the ith
outcome is

m;(l- m;)

v;= --'---.:...

(2)

s +l

Hence v; is a measure of the uncertainty of the prob­
ability m;.
Using this interpretation we also have a tool to model
expert opinion s of the type "the probability is some­
where between p and q, but I believe it is about 1·" . In
the case of two states a and b, consider, for example,
the statement that the probability of a is between 0.3
an d 0.4 and that it is about .35. If we, as in Spiegel­
halter et al. ( 1990), interpret the statement so that
the mean is 0.35 and the standard deviation is 0.05,
then it can be modelled by a 2-dimensional Dirichlet­
distribution (which is called a Beta-distribution). We
then have to determine two counts na an d Ob whi ch
satisfy the equations
Oa
--- =

aa+ab

0.35 and

0.35

·

0.65

aa+ab+l

=

0.0025

(3)

which we solve to get Oa = 31.5 and Ob = 58.5. This
can be an attractive alternative to modelling second
order u n certainty by intervals of lower and upper prob­
abilities.
learning: Let (m1, . . . , mn) and a sample size
s be a given specification of the conditional probabil­
ity table P(Aib, c) . We can then act as if we had a
contingency table of counts (sm1, .. . , smn)· lf we ge t
a case in the configuration (b, c ) and a;, then retrieval
qu ite simply consist in adding 1 to the count for a;,
and dissemination is just to calculate the new frequen­
cies. If global and local independence can be assumed
the scheme is applicable to all tables.
Back to

The scheme only works if both the states of A an d its
parents are known. In general we may anticipate that
the provided evidence, E, may leave uncertainty on
both the states of A and of its parents.
A nai ve approach in the general case could be to add a
count of P( a;, b, cl E) to the counts for a;. This scheme
is known as fractional updating (Titterington 1976).
However, the scheme has several drawbacks. For ex­
ample, if P(A I b, c) = P(A I E) then the scheme may
give unjustified counts yielding a false accuracy. If, for
example, E = (b, c), then nothing can be learned on
the distribution of A, but nevertheless the sample size
will be increased by one. See further discussion of this
issue in Spiegelhalter and Lauritzen (1990a) as well as
in Spiegelhalter and Cowell (1992).
A mathematically correct updating of the distribu­
tions under our interpretation results in a mixture of
Dirichlet-distributions rather than in a si ngl e one (a
mixture is a linear combination with non-negative co­
efficients summing to 1). This complicates the calcu­
lations intractably - in particular when adapting from
the next case where mixtures of Dirichlet-distributions
are to be updated. Eventually the process will yet

agam result in a combinatorial explosion. Instead,
the correct distribution is approximated by a single
Dirichlet-distribution (keeping the approach of modi­
fying counts). First of all, we want the approximated
distribution to have the correct means, and the new set
of probabilities (mj, ... , m;) is set to be the means of
the correct distribution. Secondly, it would be prefer­
able also to give the distrihu t.ion t.he c orrect varian cPs.
However, this is not possible since only one free parant­
eter is left, namely the equivalent sam ple size. Instead,
the equivalent sample size is give n a value such t.h<-tt.
t he

'average v<�riancc'

v =

L

111.1: v;

i=l

(" )

ts correct. The r csu lt.i ng »chcrnc, wh ich is used i11
aHUGIN, is t.hc followittg: 1'- in;t. the ltwans art' chang('d
as if a full count W<\.� oht.aitwd:

mi

=

m;s

P(a;, b,c II,· ')+ w;{ I- P(b, c I!�')}

+

s+l

The last term

(rl)

be undcrst.oocl so that it clistr ibut .cs
that. fJ and C arc not in st.ates (b, c)
according to their present p ro babili t ies .
may

the probability

over the

a;

's

Next, the sam pie size is determined:
s

•

=

L�-l rn;-" (1- mi) - 1
""k
. • •
L..- i=l 7ll.; V;

((i)

where v; is the variance of l'i in t.he mixture (the for­
mulae may be found in Spiegelhalt.er and Lauritzen
(19!)0a)). The new r.ounts iii'C s"mi.
3

Features of aHUGIN

The program a ii U G IN , wltich is currently 1111der int­
plementation, is an ex t.cnsion of II U GIN (A nders<'n
et at. 1989). JIUGIN is a shell which allows the user
to edit CPNs over finit.e st.at.c variahlcs, and wh en t.ll<'
CPN is specified, TIUG!N creates <1. runtime sysl<'llt
for entering findings and ttpd<�t.inr; prohahilit.i<�s of tlw
variables in the network.
In aiJUGIN each variable lllilY be declared t.o be in
adaptation mode. If, for cx<�mplc, the variab le A with
states a1, . . . , an has parents B, . . . , C, th e n the con·
ditiona! probability table P( A I B, . .. , C) is modified
by declaring A of ndapt.at.ion t.y p e . The t.ahle is i n ­
terpreted as a contingency table such that for ead1
parent configuration b, .. . , c, the set P(Ajb, ... , c) is
interpreted as a set of frequencies based on a sampl e
of cases. Therefore the user will for each p arent config­
uration be prompted t.o specify F:QUIVALENT SAMPLE
SIZE. The l arger the ESS, t.hc more conservative t.hc
adaptation will be. The default val l !!! of ESS is Gk·,
where k is the number of states in 11.

Alternatively t h e ttscr will he M>ked t.o specify an in­
terval for cac\1 of the prohahililics in t ���� conditional

225

226

Olesen, Lauritzen, and Jensen

probability tables. These in terva ls will th en be trans­
lated to sample sizes using the equivalent of (3). The
ESS used for the given parent configur at io n will now
be chosen as the minimum of the t r anslate d sam p le
sizes for the individual entries .
3.1

Fading

Variables in ada p tati on mode have an extra feature,
fading, which makes them tend to ignore th in gs they
have learnt a long time ago, considering them as less
relevant. Each time a new case is taken into account,
the equivalent sample si ze is discounted by a fading
factor q, which is a re al number less than one but typ­
ically close to one. From the expression ( 1) for the
Dirichlet density, it is seen that the fading scheme es­
sentially corresponds to flattening the density by r ais­
ing it to the power q, known as power-steady d ynamic
modelling (Smith 1979; Smith 1981).

SIZE. In the case o f a change from accumulating to
fading the EQUIVALENT SAMPLE SlZE is kept but the
MAXIMAL SAMPLE SIZE provided by the user

ually claim its influence.

4

Experiments with aHUGIN

To

in v es tigate

the

strengths

and

will grad­

limitations

of

se r ies of experiments were carried out..
The investigation was designed as a complete fa<"lo­
rial simulation experilllent 011 t.IH' now classical "Chest

aiiUGIN, a

clinic" example (Figure 2) originat.ing frorn

and Spiegelhalt.er

( 1088).

Laurit.�•�n

Each experiment simulates

If s is the initial ESS, then the maximal ESS after
adaptation from a case is qs + 1. Running n cases will
result in a maximal ESS of
1- q"
q"s + ---

1- q

This gives that 1/(1 - q) is the maximal sam ple size
in the long run.
Therefore the user is given the choice between ACCU­
MULATING (fading factor 1) and FADING. If fa din g is
selected, the user is prompted for MAXIMAL SAMPLE
SIZE, MSS, and the fading factor is then computed as
(MSS- 1)/MSS. Defau lt value is lOOm, w h e re m is the
number of entries in the table.
Note: The result of fad in g is not only that the sample

size is reduced. Consider namely an entry with count
a- and with samp le size s, an d suppose that ret r ie va l of
a case results in an increase of the sample size by 1 and
o f the count by x. Without fading the ratio between
counts from present and past is x ( a-, but with fading
the ratio is xjqa-. This tells us that with fading the

present counts are given more weight. This can also be
seen by assuming that the entry wilt never receive more
counts. Without fadi n g the p roba bili ty will vanis h at
the speed of a-f(s + n ) while wi th fadi ng, the speed of
vanishing is in the order of a-f(s + q-").
3.2

Runtime mode

Figure 2:
10,000

cases,

The

"Ciwst clinic" •'X<llllpk.

and fo ur factors, dc110I.l'd ll, 0, P

are considered. Tine•� r<�ndom

ated from

Rl: Probabilities

close to the

R2: Probabilities v<�ry

san1pks (R)

o rigi n a l

are

and

L.

�enn­

ones.

difk.renL from the migina! o1ws.

R3: Probabilities "drifting over time", starting as the
original ones.

To control difTerences due t.o chance va riat ions , the
samples are reused . Thus, for example, all experimcTit.s
with probabilities as in Rl are based on identical dat.a.
Two different observational schemes ( 0) are investi­
gated, the first one is lll<lillly included for control pur­
poses.

01: Complete observations.

The ad a pta tion starts with the CPN in the initial con­
figuration. Findings are entered, and wh en all infor­
mation on the case has been entered , the adaptation
takes place changing the tables for the variables of

02: Data observed only on the variables "Visit. t.o
Asia?", "Smoker?", "Positive X - r ay ? " and " Dys­

adaptation type.

The P factor des c ribes difl'erent weights on the prior
distributions, expressed as v a ryi n g eq11ivalcnt. sR.rnpk
sizes. Two cases are consid•�rcd

At any time between two cases the user can choose
to change the adaptation type of any variable. When
the adaptation type of a variable has been chan ge d ,
the user is prompt ed for p ossibl e mi s s i n g information
on EQUIVALENT SAMPLE SIZE and MAXIMAL SAMPLE

pnoea?".

PI: Low precision,
P2: ll igh pn�cision,

ESS
ESS

=

=

10.
I Oll

aHUGIN: A System Creating Adaptive Causal Probabilistic Networks

Finally, three different learning schemes (L) are inves­

the random s amp le .

tigated

11: All variables except
ac c u mu latin g mode .

"Th ber c ulos i

s or cancer" in

Experiment ,22�

a

12: As 11 for the first 1000 cases, then t he mod e is

postet ior probability intervals �or p(bls)

"'\...:··----.--- ..

1000).

�

13: A s 12, b ut with short memory (MSS = 100).
"Tuberculosis or can ce r " is always in fixed mode as
it is a pure logical tran sit ion . As can be seen, the
whole in ves tigati on consists of 3 x 2 x 2 x 3 = 36
experiments . For each experiment a plot is generate d ,
sho w in g the current value of th e conditional probabil­
ities after each case has been processed, t ogether with
ap proximate 95% p oste ri o r probability intervals.

Results in accumulating mode

95%

!\

change d to fading, with long memory (MSS

4.1

·

· •'--__ -..---'--'---"- �. :: :_::. : :d:._

2000

•ooo

Experiment 1222

b

6000

.

sooo

I

,0000

95% posterior probability intervals tor p(bls)

These experiments are very similar to those performed

by Spie gelhalter and Cowell (1992). However, we al­
low uncertainty on all conditional probability tables.
In general our results show the same pattern. For com­

plete data the correct values are obtained quite fast,
and the influence of the initial specifications vanishes
after a f e w hundred cases.
Figures 3 {a) and 4 (a) show an interesting phe­

nomenon w h en learning from incomplete data ( 02).
In these experiments, it can only be observed from t he
given data that a maj ori ty of smokers suffer f rom dys­
pnoea (shortness of breath). It can not be inferred
f rom the data whether this correlation is due to the
presence or absence of bronchitis. In the fi rst exper­
iment, where all variables are in accumulating mode,
th e frequency of bronchitis is overestimated (Figure 3
(a)). To compensate for th is , the condition al pr oba­
bility for dy spn oe a given bronchitis and none of the
other diseases, is underestimated ( F igure 4(a)). Thus
t he correlation b etw een what can a ct u ally be observed
in the data is determined correctly, but the intermedi­
ate expl an ation is slightly incorrect.
From these experiments we conclude, not su rpri singly ,
that the method has difficulties learning about con­

ce pts on which dat a are indirect. In such situations the
system rel ies str ongly on p ri or k nowledge . This con­
clusion was also reached by Spiegelhalter and Cowell

(1992).

4.2

Results in fading mode

Figures 3 (b)-(c) and 4 (b)-(c) dis p l ay the results for
the same exp er i m ent s as in F ig ures 3 (a) and 4 (a), but
with the variab l es ch anged to fading with l ong memory
after the first 1000 cases. The same effect on esti mat­
ing intermediate variables can be observed. Note also,
that the two curves vary syn ch rono u sly. Most proba­
bly this is a r e s u l t of variations in freq uenc ies due to

" -------r--�--�
,.,.,
0000

Experimenl I 223

c

·

95% poslerior probabilily

intervals tor p(bjs)

� -

..

i

"

.

0

.
0

;: ·_�---r-----�----.-------,�· · ,__._j
8000

'0000

Figure 3: Exp erimen ts with in c ompl ete data. The con­
diti on al probability of bt·onchitis given the p a t.ient. is a
smo k e r is learnt in (a) <lccurnulating mode; (b) fading
m ode with long memory: (c) fading mode with short.
memory.
ln the

third experiment

(Figures 3

(c) and

4 (c))

the

maximal samplesizes are reduced to 100. ThisexJwri­
ment reveals the
mode. Figure 4

limit of the applicability of the frtdin)!;
(c) shows t.hat t.lw dat.<1 <He lwst. l'X­
ass u ming t.hat. <�.II pa.tients wi t. h bronchit.is

plained by
suffer from dyspnoea. To 1naint.ain t.he consistP.ncy
with the d ata , the frequency of s1nokcrs sufT'ering from
b ronchitis is

underestimated acc o r dingly. This pat.t.ern

227

22H

Olesen,

Lauritzen, and

Jensen

is general for fading with short memory for high and
low probabilities. We conclude that special attention
must be directed towards systematically m issi ng data
and the choice of MSS if such variables are fading.
Figure 5 shows a series of experiments with a declin­

Experiment 3221

a

��

ing probability of being a smoker. The first 1000 cases
are identical for the three plots, the variable being in

-

95% posterior probability intervals tor p(s)

accumulating mode. In Figure 5 (a) the variable re­

mains in this mode and it is seen how the probability
is becomin g increasingly conservative as the ESS m­
creases.

Experiment 1221

a

. 95% posterior probabili!y 1ntervals for p(djnot e.b)

2000

b

Experiment 3222- 956/o posterJor probabilrty interva!s for p(s}

c

Exp&rimen\ '3223- 9-5°/.g poslerior probability ln•ervals tor p(s)

eooo

Experiment 1222 · 95% posterio r probability intervals lor p(djnot e.b)

b

I

�-��!�'""""'�-��

(

r

6

gl

Q �-----,----�--,---�--�
2000

c

Experiment 1223

10000

·

95% posterior

probabili!y intervals for p(dlnot e,b)

10000

Figure 5: Learning about.
bei ng a smoker.

. ��

J .

a

declining probability of

·0 ;

I

;:I
� �-----,----.--,---,�
10000

Figure 4: The same experiment as in Figure 3 but for
"Dyspnoea" given the patient has bronchitis but none
of the other diseases.

In Fig u re 5 (b) the variable is changed to fading with
lo ng memory (MSS = lOOO) after t.he first 1000 ca�es.
This i n creases the dynami<: behaviour of the system
an d an almost correct adaptation is obtained. De­

creasing the MSS to 100 (Figure 5 (c)) increases the
dynamic behaviour further, re!>ult.ing in stronger fluc­
tuations around the correct value. The general expe­
r i e nc e is that the !V1SS shonld not he sd too low, nnd

that the experiments confirm th<.:
of aiiUGIN.

expected behaviour

aHUGIN: A System Creating Adaptive Causal Probabilistic Networks

To s u mmarize, aH U G I N seems to be able to adapt
to chang i n g environments, thereby extending H U G I N
with a valuable fun ctional i ty . Howeve r , special atten­
tion must be directed to the choice of M SS and to
var i ables with systematically missing d ata.

Andersen, S . K . , O lesen , K. G . , Jensen , F . V . , and
J ensen, F. ( 1 989). H U G I N - A she l l for building
Bayesian belief u ni verses for expert systems. In
Proceedings of t h e 1 1 t h int ern ational joint confe r­
ence o n artificial intellig e n ce , p p .

reprinted i n S hafer and Pearl

1080-5.
( 1 990).

Also

Andreassen , S., Benn , J . J . , Hovorka, R . , Olesen ,
K . G . , and Carso n , E . R. ( 19 9 1 a ) . A probabilis­
tic approach to glucose prediction and i nsulin dose
adj us t ment . Techn i c al report , Inst i t u te for Elec­
tronic Sys tems , A a l b org U n i versity .

A n d reassen, S . , J ensen, F. V . , and O lesen , K . G .
( 1991 b) . Medical expert systems b ased on causal
probabilistic networks. In ternational Journ al of
Bi omedical Computation, 2 8 ,

1 -30 .

Cowell, R. G . ( 1992). BAlES - a probab i l i s t i c ex­
pert system shell with qual i tative and quantita­
tive learning . In Bayesian st a tistics 4, ( ed . J . M.
Bern ardo , J . 0. Berger, A . P. D awi d , and A . F. M .
Smith) , p . i n press. Clarendon Press, Oxford , UK .
Jensen, F . V . , Lauri tzen , S . L . , and Olese n , K . G .
( 1990) . Bayesian up dat i n g i n causal probabil istic
networks by local c o mpu tations . Co m p u t a t ion a l
St atistics Q u a rt e rly, 4,

Johnson , N . L . and Kotz ,

269-82.
S . ( 1972).

Distri b u t ions i n

statistics. Co ntinu ous multivariate dis t ri b u ti o n s .

J oh n Wiley and Sons, New York .
Lauritzen, S . L . and S p iegel h a l t er , D. J . ( 1988). Lo­
cal computations with probabilities on graphical
st r u ctures and their appli cation to expert systems
( with discussi on ) . Journ al of the Royal Sta tistical
Society, Series B, 50,

( 1988 ) .

1 57-224 .

Probab ilis t i c inference

m

i n t ellig e n t

syste ms. Morgan Kaufmann , San Mateo.

Shachter, R. D. ( 1986). Eval u at i n g influence d i agrams.
Opera t i o ns Research, 34,

87 1 -82.
( 1990).

Sh afer , G . R. and Pearl , J . (ed . )

Rea dmg s

in u n certain reas on ing. Morgan Kaufm an n , San

M ateo , Califor n i a.
Shenoy, P. P. and Shafer , G . R. ( 1 990 ) . Axioms for
probab i l i ty and belief-fu nction propagation . In
Uncert a inty in artificial intelligence I V, ( ed . R. D .
Shachter, T . S . Le v i t t , L. N . K an a! , an d J . F . Lem­
mer) , pp . 1 69-98. North - Hol l and , Amsterdam .
Smith, J . Q. ( 1979 ) . A general ization of the Bayesian
steady forecasting mo d el . Journal of t h e Royal
Statistical So ci e t y, Series B, 4 1 ,

Smith, J .
model .

Q. ( 1 98 1 ) .

3 75-87.

The m u l ti parameter st ead y

Journal of t h e Ro yal St atistical Society,

Series B, 4 3 ,

Spiegel halter, D . and Lau r i tzen , S . L. ( 1 9906 ) . Tech­
n i ques fo r Bayesi a n a n alysis i n e x p e r t. syste m s .
A n n als of




As Bayesian networks are applied to larger
and more complex problem domains, search
for flexible modeling and more efficient in­
ference methods is an ongoing effort. Mul­
tiply sectioned Bayesian networks (MSBNs)
extend the HUGIN inference for Bayesian
networks into a coherent framework for
flexible modeling and distributed inference.
Lazy propagation extends the Shafer-Shenoy
and HUGIN inference methods with reduced
space complexity.
We apply the Shafer-Shenoy and lazy propa­
gation to inference in MSBNs. The combina­
tion of the MSBN framework and lazy propa­
gation provides a better framework for mod­
eling and inference in very large domains. It
retains the modeling flexibility of MSBNs and
reduces the runtime space complexity, allow­
ing exact inference in much larger domains
given the same computational resources.
1

Introduction

Bayesian networks (BNs) provide a coherent frame­
work for inference with uncertain knowledge, and as
more complex domains are being tackled, search for
flexible modeling and more efficient inference meth­
ods is an ongoing effort. Multiply Sectioned Bayesian
Network s (MSBNs) [11] extend the HUGIN inference
method (2]. The framework allows a large domain to
be modeled modularly and inference to be performed
distributedly. It supports object-oriented modeling (3]
and multi-agent paradigm (10]. Lazy propagation (5]
extends the Shafer-Shenoy (S-S) (9] and the HUGIN
methods, resulting in much reduced runtime space
complexity.
We extend the lazy propagation to inference in an
MSBN. The contribution is an inference scheme for

MSBNs that has much reduced space complexity com­
pared to the S-S and HUGIN-based scheme. The new
scheme allows coherent inference in much larger MS­
BNs given the same computational resources.
We extract common aspects of tree-based inference in
Section 2. We review the S-S and lazy propagation
in Section 3. A distributed triangulation for MSBN
compilation is presented in Section 4. We overview
MSBNs in Sections 5. In Section 6, we present a new
MSBN compilation. We extend the S-S and lazy prop­
agations for inference with MSBNs in Sections 7 and
8. We compare alternative MSBN inference methods
in Section 9.
We focus on the new methods without detailing most
formal properties. A few necessary formal results are
included with the proofs omitted due to space limit.
These proofs will be included in a longer version.
2

Communication in trees

Consider a connected tree T where each node has
its (internal) state and can receive/send a message
from/to a neighbor. The exchange follows the con­
straints:
1. Each node sends one message to each neighbor.
2. Each node can send a message to a neighbor after
it has received a message from each other neigh­
bor.
A message sent by a node is prepared on the basis
of the messages received and its internal state. If the
state may change as a result of messages received, then
the message passing is called dynamic (see Fig. 1 and
Section 4), otherwise called static (see 3.2 and 3.3).
We shall refer to all the processing (outgoing message
preparation and state change) taking place between re­
ceiving messages and sending a message to a particular
neighbor as a generic operation called SetMsgState.

Inference in MSBNs with Extended S-S and Lazy Propagation

We refer to the combined activity of nodes according
to the constraints as (message) propagation. Based on
the constraints, initially only leaves can send and at
any time there is a subset of nodes ready to send a mes­
sage. Depending on the sending order of nodes, two
regimes of propagation can be identified, asynchronous
and rooted.
In asynchronous propagation, no additional rules gov­
ern the sending order. In rooted propagation, a node
r is arbitrarily chosen as the root, and T is directed
from r to the leaves. All nodes except r has exactly
one parent. First a recursive operation CollectMessage
is called in r. For each node x, when CollectMessage
is called in x, x calls CollectMessage in all children.
When each child has finished with a message sent to
x, x sends a message to its parent (if any). We shall
refer to this stage of rooted propagation as a (rooted)
collect propagation.
After CollectMessage has terminated in r, another re­
cursive operation DistributeMessage is called in r. For
each node x in T, when DistributeMessage is called
in x, x sends a message to each child and calls Dis­
tributeMessage in the child. We shall refer to this stage
as a (rooted) distribute propagation. It is easy to show
that each asynchronous propagation corresponds to a
rooted propagation.

Figure 1: Dynamic propagation in a tree.

Consider Figure 1 (a). Each node stores a pair (x, y),
where x is a local constant and y is a sum initialized to
x. To sum x at all nodes, we call CollectMessage from
any root (b). SetMsgState consists of adding incoming
numbers toy, and setting the message to a neighbor V
as the sum of x and all incoming numbers except that
from V. The sum can now be retrieved from the root.
Next, we call DistributeMessage at the same root (c).
The sum can now be retrieved from any node.
3

Probability propagation in JTs

Various methods for inference in BNs have been con­
structed [6, 1, 4, 8, 9, 2]. Several [4, 9, 2] use ajunction
tree (JT) as mntime structure. We review how to con­
vert a BN into a JT and then consider two of them.
3.1

Conversion of a BN into a JT

A BN S is a triplet (N, D, P) where N is a set of
variables, D is a DAG whose nodes are labeled by el­
ements of N, and P is a joint probability distribution

681

(jpd) over N. D encodes independence inN through
d-separation [6], and hence P(N) = DxEN P(xj1r(x)),
where 1r(x) is the parents of x in D.
Conversion of a BN starts with moralization. It con­
verts a DAG into an undirected graph by completing
the parents o f each node and dropping direction of
links. The result is called a moral graph. Then trian­
gulation (see Section 4) converts the moral graph into
a chordal graph [7].
A JT over N is a tree where each node is labeled by a
subset (called a cluster) of N and each link is labeled
by the intersection (called a sepset) of its incident clus­
ters, such that the intersection of any two clusters is
contained in every sepset on the path between them1.

A maximal complete set of nodes in a graph is called
a clique. After the triangulation step, a JT for a BN
is created with nodes labeled by cliques of the chordal
graph. Such a JT exists iff the graph is chordal.
After a JT is created, distributions in the BN are as­
signed to the clusters. For each x E N, P(xj1r(x)) is
assigned to a cluster containing x and 1r(x).
3.2

Shafer-Shenoy propagation

S-S propagation [9] is static, where each cluster holds
a belief table over its variables, defined as the product
of all distributions assigned to it. Hence the product
of the belief tables in all clusters is the jpd.
During propagation, each message sent over a sepset is
a belief table over the variables in the sepset. SetMs­
gState consists of multiplying the local table with in­
coming tables from other neighbors and marginalizing
the product down to the corresponding sepset. For
each cluster, after the propagation, the product of the
local tables and all incoming tables is the marginal
probability distribution over the variables of the clus­
ter.
3.3

Lazy propagation

Lazy propagation [5] is also static, where each cluster
C holds the assigned distributions as a set rather than
as a product. The belief table of C is defined the same
as above but the product is not explicitly computed
(hence the reduced space complexity over the S-S and
HUGIN methods).
Each message sent over a sepset is a set of tables each
of which is over a subset of the sepset. SetMsgState to
a given neighbor consists of taking the union of local
tables and incoming tables from other neighbors, and
then marginalizing out each variable not in the sepset.
1 The

property is a.lso known

as

running intersection.

682

Xiang and Jensen

Figure 3: (a) G is the union of the graphs in (b) . (b) G is sectioned into four subgraphs. (c) A hypertree over G.

.....____

�
B(d,g},B(e)

{c.e}

B'(c,e)�
�

B(a,d)�

/{a,d}

a,c,d,e,g
B(a,g),B(c,e)

{d,e,g}

Figure 2: Message passing in lazy propagation.
Figure 2 illustrates lazy propagation. The cluster
{a,c,d,e,g} has sepsets {a,d}, {c,e} and {d,e,g}.
It has local tables {B(a,g),B(c,e)} and receives the
tables B'(c,e) and B(a,d). It sends out B(d,g) ==
I:.B(a,d)B(a,g) and B(e) == LcB(c,e)B'(c,e).
Triangulation as tree propagation

4

We consider triangulating an undirected graph orga­
nized as a (hyper) tree.

Let G; == (N;, E;) (i == 0, . . .,n-1) be n
graphs. The graph G == (U;N;, U;E;) is the union of
G;s, denoted by G == U;G;.
Definition 1

If for each i and j, l;j == N; n Nj spans identical sub­
graphs in G; and Gj, then G is sectioned into G;s.
l;j is the separator between G; and Gj.
The graph in Figure 3 (a) is sectioned in (b) . Each
node in a separator is highlighted by a dashed circle.

Let G == (N, E) be a connected graph
sectioned into {G; == (N;, E;)}. Let the G;s be orga­
nized as a connected tree H where each node is labeled
by a G; and each link is labeled by a separator such
that for each i and j, N; n Ni is contained in each
subgraph on the path between G; and Gj in H 2. Then
H is a hypertree overG. Each G; is a hypernode
and each separator is a hyperlink.

some N;. Then the triangulation is constrained by
H.
A node x in an undirected graph is eliminated by
adding links such that all of its neighbors are pair­
wise linked and then removing x together with links
incident to x. The added links are called fill-ins.
Theorem 4

([7]) A graph is chordal iff all its nodes
can be eliminated one by one without adding fill-ins.

Let a hypertree H overG be rooted at a given hyper­
node G;. An elimination order p of G is constrained
by H if p consists of recursively eliminating nodes that
are only contained in a single leaf hypernode of H.
Proposition 5 An elimination order of G con­
strained by a hypertree H over G produces a trian­
gulation of G constrained by H.

Triangulation constrained by H can be performed as a
(dynamic) rooted collect propagation of fill-ins: LetG;
be the child of Gj in H with separator I;j == N; n Nj.
The message sent from G; toGj is a set of fill-ins over
l;j. SetMsgState consists of the following:
Algorithm 1 (SetMsgState for propagating fill-ins)

add to G; fill-ins received from each neighbor except G1;
eliminate N; \ N1 and add fill-ins to G;;
set message to G1 as all fill-ins over l;j obtained above;

Definition 2

Figure 3 (c) shows a hypertree H over G in (a) . Note
that the above concepts are applicable to both directed
and undirected graphs.
Definition 3 Let H be a hypertree over a graph G
sectioned into { G;}. Let G' be a graph from a trian­
gulation of G such that each clique in G' is a subset of
2

Note the similarity to JTs.

{f.g,h)
{(f,h))

'�J
.

g

k

h

•I

u�

�

{G,k),G,I))

�)
�
{(fj))

Figure 4: Hypernode G; (i == 2) receives fill-ins from
two hyperlinks {!,i,j} and {j,k,l}. After SetMs­
gState, fill-ins (dashed lines) are added toG; and the
message { (!, h)} is sent to the parent over the hyper­
link {f,g,h}.
Suppose H is rooted at G1. ForG1, SetMsgState is
simplified ( Gj == null, Nj == ¢ and the last step is not
applicable) . Figure 4 illustrates the collect propaga­
tion of fill-ins.

Inference in MSBNs with Extended S-S and Lazy Propagation

(a) (b)

j�
.

k

'

:

I' '

',

:

:

•

rn

Gj_2

i

0�
f

___ ____ !.

G6� 2

. '

G*0

&

g

a.�
G!- 2

h

'

:

�

�
,

- _______:
�

,

G! - 3

\

-

,

' ,/

�;',
' ,'

:
./ : 'k
�

',

I

'

·��l
:··

G!� I

'�' :�,
c

��

g• :

h

(e)

(c) (d)

,�, ':=J'
'.

'

j

683

�:

-

-

,I

G2*

&

-

-

:

'

c

••

.

G!- o

'

'

,

g

h

(h)

'',,, : \

-- - -- - -

b

G*I

:

t

'tp
�

.·:'

rn

G*3

Figure 5: Illustration of propagation of fill-ins.

It can be shown that fill-ins sent during collect propa­
gation of fill-ins is independent of the elimination order
used by SetMsgState in each hypernode and are deter­
mined uniquely by the chosen root. Hence if H has n
hypernodes, potentially n different triangulations of G
(assuming each local elimination is optimized without
ties) can be obtained each from a collect propagation
at a distinct root. To obtain then triangulations, how­
ever, we do not have to perform collect propagationn
times. Instead, a full propagation in H is sufficient:
CollectMessage will be performed as above. Dis­
tributeMessage will be performed with the same
SetMsgState (Algorithm 1). Finally, each non-root
performs SetMsgState as if it is a root,
Figure 5 illustrates the full propagation with H in Fig­
ure 3. The root is G1. During CollectMessage, SetMs­
gState is first performed in Go and G�. Suppose the
elimination order in G3 is (n, m ) . The fill-ins produced
are { {j, k}, {j, /}} as shown in (a) with dashed links.
The resultant chordal graph is labeled Gij_. 2• Ga sends
the above fill-ins to G2. Similar operations then occur
in Go (b) and G2 (c).
Since G1 is the root, it performs a simplified SetMs­
gState. After adding the fill-in {!, h}, the resultant
graph Gi is chordal as shown in (d) . CollectMessage
now terminates. DistributeMessage follows as shown
in (e) to (g). Each non-root hypernode performs one
more SetMsgState as if it is a root with the results
shown in (b) , (c) and (h). Note that in (h) , since the
received fill-in is {j, k} and the elimination can be per­
formed in any order, G3 is simpler than G 3 _. 2 .
5

Overview of MSBNs

An MSBN M is a collection of Bayesian subnets that
together defines a BN [11, 10]. M represents proba­
bilistic dependence of a total universe partitioned into

multiple subdomains each of which is represented by a
subnet.
Just as the structure of a BN is a DAG, the structure
of an MSBN is a multiply sectioned DAG (MSDAG)
with a hypertree organization:

A hypertree MSDAG 1J = U D; 1 where
each D; is a DAG, is a connected DAG such that {1}
there exists a hypertree over 1), and {2} each hyper/ink
d-separates {6] the two subtrees that it connects.

Definition 6

The second condition requires that nodes shared by
two subnets form a d-sepset:

7 Let D; = (N;,E;) (i = 0,1) be two
DAGs such that D = Do U D1 is a DAG. The in­
tersection I = No n N1 is a d-sepset for Do and D1
if for every x E I with its parents 7r(x) in D 1 either
1r(x) <; N0 or 7r(x) <; N1. Each x E I is called a

Definition

d-sepnode.

This is established as follows:
Proposition 8 Let D; = (N;, E;) (i = 01 1) be two
DAGs such that D = Do U D1 is a DAG. No \ N1
and N1 \ N0 are d-separated by I = No (l N1 iff I is a
d-sepset.

It can be shown that the above definition of MSDAG
is equivalent to the constructive definition in [11]. An
MSBN is defined as follows:
Definition 9

An MSBN M is a triplet M =
(N, 1J, 'P). N = U; N; is the total universe where
each N; is a set of variables. 1) = U; D; (a hypertree
MSDAG) is the structure where nodes of each DAG
D; are labeled by elements of N;. Let x be a variable
and 11'( x) be all parents of x in 1). For each x 1 exactly
one of its occurrences {in a D; containing {x} U 7r(x))
is assigned P(x\1r(x))1 and each occurrence in other

684

Xiang and Jensen

DAGs is assigned a constant table. P = Il; Pn, is
the jpd, where each Pn, is the product of the prob­
ability tables associated with nodes in D;. A triplet
S; = (N;, D;, Pn.) is called a subnet of M.
An example MSBN is shown in Figure 6.

j g·V�o
P(oll)

o�So
�li,p)

P(
d �·l�
P(�s>
e
c

r

P(M

P(glh)

P(bjb)

a;�

f.

•

P(ilf.g)

i

..

•

h,

P(h)

52

.

j

P(mti)
k· P(·Jk.l�--

.1

.

�
rz

·

but incomplete in Gi ....2 . By using Gi-tj• the message
from sl to s2 can be decomposed into two submes­
sages, one over {!,g} and the other over {g, h}. This
results in a more compact message representation. For
each Gi-+i' we organize its cliques into a set of JTs (a
JF) so that each submessage can be obtained directly
from one cluster of each JT. Without formally pre­
senting the general algorithm, we illustrate using the
example in Figure 5.

n

,_

P(l�>

m

s3

Figure 6: An MSBN.
6

Compilation of MSBNs

So far, inference in MSBNs [11, 10] has been an exten­
sion to the HUG IN method [2]3, which works with one
triangulation and one decomposition of messages for
the entire propagation. As demonstrated in Section 4
and below, it is possible to let the triangulation and
decomposition depend on the direction of messages.
The resultant clusters can be smaller than obtained
by the HUGIN method. Below we explore this idea
for inference in MSBNs using the S-S and lazy propa­
gation.
6.1

Local structure for message/inference

First moralization is performed as a full dynamic prop­
agation on the hypertree. A message sent from a hy­
pernode to another consists of (moral) links over their
d-sepset. During CollectMessage, SetMsgState con­
sists of the following: (1) For each hypernode, parents
of each node in D; are completed and directions of
links are dropped. (2) Moral links from each child
hypernode are then added. (3) Set the message to
the parent hypernode as the moral links over their d­
sepset. For DistributeMessage, SetMsgState consists
of (2) and (3). Figure 3 (b) is the moralization of the
MSBN in Figure 6.
Next triangulation is performed as in Section 4. Then
we convert each Gi into a JT for local inference (as
in Section 3.1) and convert each Gi'-tj into a junction
forest (JF) for computing messages from subnet S; to
Sj for inter-subnet belief propagation. We present the
conversion of Gi-tj into a message JF below:
To see the need of multiple structures for each subnet,
observe that Gi is generally more densely connected
than Gi-tj. In Figure 5, the d-sepset is complete in Gi,
3The HUGIN propagation is dynamic whereas S-S as
well as lazy propagation are static.

Figure 7: Junction forests for message computation.
First, consider G3....2• Since the d-sepset is complete
(no opportunity for message decomposition) , we or­
ganize the cliques of G3.... 2 into a JT Ta-t2 shown in
Figure 7 (1). During inference, the message from Sa to
s2 can then be obtained from the cluster {j' k' l' m}.
Similarly, JTs To-t2, T2-tl and T2-to can be obtained.
Next, consider G't ....2• Since the d-sepset is incomplete
(the message is decomposable) , we create a JF con­
sisting of two JTs as in (2). During inference, the
submessage over {f,g} can be computed using the up­
per JT from the cluster {e,f,g}. The submessage over
{g,h} can be obtained from the cluster {g, h} of the
lower JT.
The JF is constructed as follows: For each clique in the
subgraph of Gi .... 2 spanned by the d-sepset, create an
isolated node labeled by the clique. Hence we obtain
the two clusters at the bottom of (3). They are the
candidate clusters from which the submessages will be
obtained. We then complete the d-sepset in Gi.... 2 and
create a JT out of it as shown in the top of (3). We
split this JT into two and merge each with one of the
candidate clusters as follows:
We delete the d-sepset cluster {f,g, h }, breaking the
JT into two subtrees. For one subtree, the cluster
{b, h} was adjacent to {f,g,h}. Since the candidate
cluster {g,h} satisfies {g, h}n{b, h} = {f,g, h}n{b,h},
we connect {g, h} with {b, h}. For the other subtree,
the cluster {e,f,g} was adjacent to {f,g, h}. Since
the candidate cluster {f, g} is a subset of {e, f,g}, we
remove the candidate cluster {!,g}. The resultant JF
is the one in (2). Similarly, JF T2-t3 can be obtained.
Without confusion, we refer to message JFs and in­
ference JTs collectively as JFs. In the next section,
we define a data structure to guide message passing
between local JFs at adjacent subnets.

Inference in MSBNs with Extended S-S and Lazy Propagation

6.2

Linking message

JFs and

inference

JTs

Inference in an MSBN can be performed as a full prop­
agation in the hypertree consisting of message passing
among JFs (SetMsgState will be detailed later). When
a message is to be sent from S; to Sj, it is computed
using T;_.j. When Sj receives the message, it will be
processed by Tj and each Tj_.k (k =/; i). Figure 8 (1)
illustrates directions of messages during collect propa­
gation with root sl' and (2) illustrates distribute prop­
agation.

Figure 8: Directions of messages during propagation.
As each submessage is obtained from a cluster of the
sending JF and absorbed into a cluster of the receiving
JF, we create a linkage that links the pair of clusters.
{'}

;;I
··

g ---

••

e.f,g

•

.>::--:-:---�

•
•
•
•

•

•
•

--

�.:.

c,d

d,e

Figure 9: Linkages between two message JFs.
Figure 9 shows the two linkages from Tl-+2 to T2-+o
used during distribute propagation. It reflects the
fact that the d-sepset {!, g, h} can be decomposed into
two independent subsets {f, g } and {g, h} conditioned
on their intersection {g}. Each linkage (shown as a
dashed arc) is labeled by the intersection of the two
end clusters. We shall call the two clusters the hosts
of the linkage. Once linkages are determined, the set
of all JFs forms a linked junction forest (LJF).
6.3

Belief assignment

Next, we assign conditional probability tables (CPTs)
in the MSBN to clusters in the LJF. For each JF of
each subnet, the assignment is performed as follows:
For each variable .r, if a CPT is associated with it, then
assign the CPT to a cluster in the JF that contains .r
and its parents.
The joint system belief of the LJF is then defined as
B(.N) = fl; flj flk f3i,j,k, where i is the index of infer­
ence JTs, j is the index of clusters in a given JT, f3i,j
denotes the set of CPTs assigned to the jth cluster in
the ith JT, and f3i,j,k is the kth CPT in the set. It
is easy to see that B(N) is identical to the jpd of the
MSBN.

685

Since CPTs are assigned in the same way in inference
JTs and message JFs, the belief of all JFs from the
same subnet are identical.
Although each subnet is associated with multiple JFs,
only one copy of each CPT needs to be physically
stored. For each CPT, it suffices to store a pointer
at the assigned cluster in each JF.
7

Shafer-Shenoy propagation in LJF

We extend the S-S propagation (Section 3.2) for infer­
ence in a linked junction forest.
For each cluster in each JF of each subnet, a belief ta­
ble is created by multiplying the CPTs assigned to the
cluster. Inference is performed as a full propagation
over the hypertree during which messages are sent be­
tween JFs in adjacent subnets. When a message JF
has multiple linkages to an adjacent JF, the message
consists of multiple submessages (otherwise the mes­
sage consists of a single submessage) each of which is
sent across a distinct linkage. Each linkage is used for
message passing in a unique direction.
Each submessage is prepared at a distinct JT in a mes­
sage JF. A local collect S-S propagation is started at
the linkage host and the submessage is then obtained
at the host. The propagation involves incoming link­
ages and their hosts in the adjacent JFs, as illustrated
in Figure 10.

Figure 10: To compute the submessage from T2-+1 to
is extended (dotted box) to include link­
age hosts {j, k , l , m } from T3-+2 and {!, i,j,p} from
To-+2. The collect propagation starts at linkage host
{f,g, h, i}.
Now we define SetMsgState for preparing the message
from S; to Sj sent by message JF T;-+j:

T1, T2-+1

Algorithm 2 (SetMsgState for S-S propagation in LJF)

for each junction tree of Ti-tj
start collect S-S propagation at the host of linkage to S1;
set submessage as marginal of host belief to the linkage;

To analyze the effect of the propagation, we define the
belief tables associated with different identities in an
LJF: For each cluster C with a local belief table (3
and incoming messages (3; (i = 1, . . . ), the belief table
Be (C) is the product (3* f]; (3;. Note that the messages

686

Xiang and Jensen

include messages from sepsets as well as submessages
from linkages. For each inference JT T over N, the be­
lief table BT(N) is the product BT(N) = IJ; Be, (C;),
where i indexes clusters ofT. It can be shown that
the extended S-S propagation is coherent.
After the extended S-S propagation in the LJF, a S-S
propagation needs be performed at an inference JT to
answer local queries. Note that the collect stage of the
propagation should be performed on the extended JT
to count the incoming messages from adjacent message
JFs. Also note that when evidence is available on a
variable in a subnet, it should be entered to a relevant
cluster in each JF of the subnet.
8

Lazy propagation in LJF

The extended S-S propagation can be directly modified
into extended lazy propagation in LJFs as follows:
For each cluster in each JF of each subnet, its belief
table is defined in the same way as the extended S­
S propagation, but multiplication of assigned CPTs
is not performed explicitly. The S-S propagation per­
formed in each JF is replaced by lazy propagation (Sec­
tion 3.3). Each message over a sepset and each sub­
message over a linkage will in general be a set of belief
tables over a subset of variables of the sepset/linkage
without being multiplied together. Theorem 10 shows
that the extended lazy propagation ensures coherent
inference.
Theorem 10

After a full extended lazy propagation
in an LJF, for each subnet S; over N;, its inference
JT 11 satisfies BT, (N;) = I:.N"\N; Ilj BT; (Nj ) , where
j indexes inference JTs.
As for normal BNs, the main advantage of lazy
propagation is its decomposed representation of be­
lief tables/messages. The decomposition leads to re­
duced space complexity, which is particularly signifi­
cant when the problem domain is very large.
9

Conclusion

We presented how to construct a linked junction for­
est (LJF) from a multiply sectioned Bayesian network
(MSBN) , and how to extend Shafer-Shenoy and lazy
propagation for inference in such an LJF. It is worth­
while to compare the new methods with earlier work
on the construction of LJF and the HUG IN based in­
ference method [11, 10].
First of all, the new method constructs multiple JFs
for each subnet, one for local inference and the oth­
ers for inter-subnet message computation. The previ­
ous method, on the other hand, creates a single JT at

each subnet for both local inference and inter-subnet
message computation. With the new method, since
each message JF is dedicated to the computation of
messages to a particular subnet, its structure is less
constrained and is generally more sparse. With the
previous method, a JT must function correctly at all
conditions (send and absorb messages to/from each
adjacent subnet) and it is thus more constrained, re­
sulting in generally more densely connected JT struc­
tures.
Although we have extended the S-S and lazy propaga­
tions in the LJF constructed by the new method, they
can be modified to perform in an LJF constructed by
the previous method as well. G iven what we have pre­
sented, the modification is straightforward. To the S-S
propagation, the benefit of using the new construction
is more compact belief representation and more effi­
cient inference computation due directly to the sparser
JF structure. To lazy propagation, the benefit is that
the sparser structures provide better guidance to the
propagation. To see this, imagine that if an entire mes­
sage JF is a single cluster, the burden of finding an
effective marginalization order for computing a mes­
sage will be placed entirely at runtime. Hence, each
message JF in the new construction can be viewed as
a concise recording of a set of effective marginalization
orders ready for runtime exploitation. 4 On the other
hand, the LJF by the previous method needs not to
maintain multiple JFs at each subnet. Inter-subnet
message computation and local inference computation
can then be completed by just one propagation in the
only JT at a subnet (instead of several propagations
one at each JF) .
This observation suggests a tradeoff between using an
LJF constructed by the new method and that by the
previous method. One factor in making a choice is
the relative sparseness of the LJF obtained by each
method, which depends on the topology of the MSBN
in question. Another factor in practice is the empha­
sis placed on simplicity in control (which translates to
development time) and efficiency in runtime computa­
tion.
Secondly, the extended lazy propagation has much
lower space complexity than the previous HUGIN
based inference for MSBNs due to the factorized stor­
age of belief. With the lazy propagation, for each CPT
in the MSBN, only one copy needs to be stored in the
LJF. Hence the total number of independent param­
eters stored in the LJF is 46 for the example MSBN.
If full CPTs are stored to save the on-line derivation,
4A

marginalization order specifies the order in which

each variable is to be marginalized out. Two such orders
are equally effective if their computational complexity are
the same.

Inference in MSBNs with Extended S-S and Lazy Propagation

92 values should be stored. With the HUGIN based
method, the total storage of all belief tables for all
clusters in the sparsest LJF has a size of 140. As the
MSBN grows in size and connectivity, the sizes of clus­
ters of the LJF grow. The belief storage per cluster in
an LJF grows exponentially with the cluster size with
the HUGIN based method, while with the extended
lazy propagation it grows only linearly. Therefore, the
extended lazy propagation will allow much larger MS­
BNs to be constructed and used than possible with the
HUGIN based inference, given one's computational re­
source.
Acknowledgement

We thank the anonymous reviewers for helpful com­
ments. The support of Research Grant OGP0155425
to the first author from NSERC of Canada is acknowl­
edged.



We present a method for calculation of my­
opic value of information in influence dia­
grams (Howard & Matheson, 1981) based on
the strong junction tree framework (Jensen
et al., 1994).
An influence diagram specifies a certain or­
der of observations and decisions through its
structure. This order is reflected in the corre­
sponding junction trees by the order in which
the nodes are marginalized. This order of
marginalization can be changed by table ex­
pansion and use of control structures, and
this facilitates for calculating the expected
value of information for different information
scenarios within the same junction tree. In
effect, a strong junction tree with expanded
tables may be used for calculating the value
of information between several scenarios with
different observation-decision order.
We compare our method to other methods
for calculating the value of information in in­
fluence diagrams.
Influence diagrams, value of in­
formation, strong junction tree, table expan­
sion, dynamic programming.

Keywords:

1

INTRODUCTION

Influence diagrams were introduced by Howard &
Matheson ( 1981) as a formalism to model decision
problems with uncertainty for a single decision maker.
An influence diagram can be considered a Bayesian
network augmented with decision variables and a util­
ity function. The decision variables, D1, . . . , Dn, in
the influence diagrams are partially ordered and the
chance variables are divided into information sets, !0 ,

... , In· The information set !;_1 is observed immedi­
ately before decision D; is made, and the information
set In consists of the chance variables that are observed
later than the n'th decision is made, if ever.

Let v; be the set of variables preceding D;, t hat is,
v; contains the past relevant for D;. The solution
of a decision problem modeled by an influence dia­
gram is a sequence of decisions that maximizes the
expected utility. Shachter (1986) describes a method
to solve an influence diagram without unfolding it
into a decision tree; rather, the influence diagram
is transformed through a series of node-removal and
arc-reversal operations. Shenoy (1992) describes an­
other approach to the problem of solving influence dia­
grams by conversion into valuation networks. This ap­
proach is slightly more efficient than that of (Shachter,
1986). (Shachter & Ndilikilikesha, 1993) and (Ndiliki­
likesha, 1994) modified the node-removal/arc-reversal
algorithm and achieved a method that is equivalent to
the algorithm presented in (Shenoy, 1992) with respect
to computational efficiency.
Jensen et al. (1994) describes an efficient method for
solving influence diagrams using strong junction trees.
This is an extension to the junction trees used for com­
putation in pure Bayesian decision analysis. It is on
this framework we base the present work.
We are about to choose among a set of k options.
These options are packed into the decision node D. We
have already received some information e, and now we
can either choose among the options or we can look for
more information. The 'looking for more information'
is to consult some source which will provide the state
of a chance variable. Let the chance variables in ques­
tion be the set r = {AlI ... 'Am}. We want to calcu­
late what we can expect to gain from consulting the
information source. For all the considerations in this
paper we deal with the myopic value-of-information
question: At any time, we can ask for the state of at
most one of the variables in r.

Myopic Value of Information in Influence Diagrams

As basis for the considerations we have
expected utilities for

D

EU(Die),

143

the

given the evidence e, and the

decision d of maximal expected utility is chosen. If
A; E r is observed to be in state a, then EU(Die, A; =
a ) is the new basis. Now, before observing A; we have
probabilities P(A; le), and the expected utilities of the
optimal action after having observed A; is

EUO(A;, Die)= LP(A;]e)· mgxEU(Die, A;)

F igure

The

value of observing A;

1:

The scenario with one non-intervening deci­

sion node.

A;

is the difference

VOI(A;, Die)= EUO(A;, Die)- maxEU(Die)

For this scenario we have

D,

Value of information is a core element in decision anal­
ysis, and a method for efficient calculation of myopic
value of information in Bayesian networks (augmented

VOI(A;, Die)

(L:H P(HIA;, e)· U(D, H))
-mgx(� P(Hie) · U(D,H))

with a utility function) is described by {Jensen &
Jiangmin 1.,

1995).

Also, (Beckerman et al.,

= LP(Ade)

1992)

·

max

A,

describes a method for calculating the utility-based
myopic value of information.
Methods for computing the value of information in
influence diagrams have been described by {Ezawa,

1994)

based on the arc-reversal/node-removal meth­

ods. (Poh & Horvitz,

1996) approach a notion of qual­

itative value of information through graph-theoretic
considerations yielding a partial order of the chance
nodes in the model.

For

the

P(HIA;, e)

calculation

of

for all variables

VOI(A;, Die)
A; in r. These

The value of information can be viewed as the dif­
differing in the observation-decision sequence in the
influence diagram. We present a single-model frame­
work for calculating the exact value of information of
a chance node.

need

probabilities can be achieved through entering and
propagating each state of

A;.

Using Bayes' rule, the

requirement is transformed to a need for

ference in expected value between two models only

we

conditional

all

A;

P(A; IH, e) for

in r. They can be achieved all by entering and

propagating the states of

H.

So, the number of propa­

gations necessary for solving the value-of-information
task for this scenario is the minimum of the number of
states of
in r.

H and

the sum of the states of the variables

For the considerations in this paper, the network is of
considerable size so that a propagation in the network
is a heavy (but feasible) task.

2.2

THE NUMBER OF H IS LARGE

This means that the

methods presented shall be evaluated in the light of

The assumptions in Section

their propagation demand.

we would like to relax them. Often

2.1

are very crude and

D has an impact
P(H/D). Also,

on H and in that case we will need

2

the number of states of

SIMPLE SCENARIOS

We shall first describe a couple of simple scenar­
ios which have efficient solutions.

The first scenario

is standard and has been treated more detailed by
(Jensen,

H

as well as the sum of all

states of r may be very large

1996).

(H may be

propagations (see Figure

2).

The following method reduces the number of propa­
gations to the number of states in

D.

is a modification of a trick by Cooper
2.1

ONE NON-INTERVENING DECISION

There is one decision node

D

which has no impact

on any of the chance nodes in the model. The utility
function

U is a function of D

and the chance variable H

which may actually be a set of variables (see Figure

1).

a large set of

variables), and we will look for methods requiring less

The method

(1988).

The

utility function is transformed to a normalized util­
ity NV through a linear transformation such that

0 S NU S 1. NU

is represented in the influence dia­

gram by a binary node NU with the argument vari­

ables H (which might include D) as parents
P(NU = yiH)= NU(H) (see Figure 3).

and with

144

Dittmer and Jensen

ENU(Die) can be
NU = y.

calculated by entering and propa­

gating

Now, let A be a variable in r. Assume that A is ob­

served to be in the state

a.

Then we have

ENU(Dia, e)
)
= P(NU:::: YID, a, e
) P(aiNU = y, D, e)
= P(NU = y ID'e .
P(aJD, e)
P(aiNU = y, D, e)
= ENU(Die).
P(aiD, e)
Figure

2:

A scenario where the method of Section

2.1

is inadequate.

and the expected normalized utility after observing A

is

2::A (maxn ENU(DIA, e))· P(AID, e ) .

The required probabilities

P(AID, e)

P(AINU = y, D, e)

and

can be achieved by entering and propagat­

ing the states of Din a network conditioned one and in

one conditioned on

(e, NU = y).

Hence, the number of

propagations required for this calculation is twice the
number of states in

D,

that is, with

2k

propagations

we can calculate the value of observation for all vari­

ables. It should be noted that there were no structural
assumptions for this result.

In most cases the information e as well as the variables

D are not descendants
P(AID, e) = P(Aie) and the

which may be observed prior to

Figure

3:

in Figure

The Cooper transformation of the scenario

2.

of

D. In

method only requires

3
The normalized value of information is defined as

NVOI(A;, Die)
=

A,

- �x
and

(L::H P(HIA;, D, e)· NU(D))
(L:: P(HID,e) NU(H) )

L P(A;Ie)

VOl

· mtx

·

H

can be calculated from

NVOI

transformation.

by the inverse

The expected normalized utility of a decision d, given

the evidence

e

can be calculated as

ENU LNU(H)
=

·

P(HJd,e)

H
=

L P (NU = yiH) · P(Hid,e)
H

=

L P(NU

=

y, Hid, e)

H

=

P(NU

=

D

The

propagations.

k

A SEQUENCE OF DECISIONS
next

scenario

to

consider

is

the

following:

We have a sequence of decisions and observations
Io, D1,

I1, . . . , Dn, In where each I; is a set of chance

variables

(In

observed).

is the set of variables which are never

The variables are structured in an influ­

ence diagram (see Figure

4

for an example). We are

in the middle of this sequence, we have observed /;-1

and are about to decide on

ther option of observing
Let

VOI(X, Di, ,jiVi)

one

(where

D;

but we have a fur­

variable of the set r.

(i

<

j)

denote the dif­

ference in maximal expected utility for

D;

between

observing chance node X immediately before deciding

D; and immediately before deciding on Dj. That is
VOI(X, Di,jJVi) denotes the difference between hav­
ing X in /;_1 and in Ij-1 at the time of deciding on
D;.
on

The standard dynamic programming technique for
solving an influence diagram is to perform a sequence

of marginalizations in reverse order (Shenoy,

Shachter & Peat,

yid, e)

Using Bayes' rule and giving

these cases

1992).

1992;

Chance nodes are marginal­

ized through a summation and decision nodes are

the even distribution,

maximized.

Since summation and maximization do

not commute, the order of marginalization is impor-

145

Myopic Value of Information in Influence Diagrams

Figure 5: A strong junction tree for the influence dia­
gram in Figure 4. The strong root is the c lique G0 at
the far left.
the chance variable B. The model for this observation­
decision sequence is shown in Figure 6.
Figure 4: An influence diagram with the observation­
decision sequence Dt, C, D2, {A,E}, Da, B. Note
that A and E may be observed in mutually arbitrary
order but both will be observed.

B r-------�

tant and it is performed in the following order: First
marginalize In (in any order), then D... , then In-1 (in
any order) , etc. When /; has been marginalized, we
have a representation of the expected utility of the
various options of D; given the past.
It is tempting to use this technique to condense the
future into a utility function over a subsei; of the cur­
rently unknown variables and the decision node D;
and to use this condensed future for the calculation of
value of information. However, the condensed future
contains max-expected-utility decisions, and observing
a variable from r may affect these decisions. This can
be avoided by assuming that the future is independent
of r given D; (and the past) . Such an assumption will
rarely hold, and instead we will introduce a technique
which does not have that kind of assumption.
In (Jensen et al., 1994) the junction tree technique is
used to solve influence diagrams. A so-called strong
junction tree is constructed with a so-called strong
root. This means that there is a clique Co such that
when a collect-operation to Co is performed, then all
marginalizations can be performed in the proper order
(see Figure 5). Note that the strong junction tree in
itself does not ensure that marginalizations are per­
formed in a proper order. When marginalizing in a
clique we need a control structure giving the order of
rnarginalizations. The " proper order" need not be the
reversed temporal order. It is sufficient that each vari­
able is eliminated in reverse temporal order with re­
spect to its Markov blanket. The Markov blanket of a
node X is the minimal set of nodes covering X from
influence from other nodes, that is, the Markov blan­
ket for node X consists of X 's parents, children, and
children's parents.
In Figures 4 and 5, B is not observed (or rather: B
is not observed until after the last decision is made).
Now, assume that before deciding on D1, we observe

Figure 6: An influence diagram with the observation­
decision sequence B, D1, C, D2, {A,E}, D3.
The difference in expected utility when solving the two
influence diagrams is VOI(B, D1, oolv;), that is, the
value of observing B before D1 rather than never ob­
serving B. The difference between the two scenarios
can be seen on the strong junction trees in Figures 7a
and 7b.

a

b

c

Figure 7: Strong junction trees for the two scenarios
of Figures 4 and 6, and a junction tree adequate for
both scenarios.
It is possible to construct a junction tree capable of

146

Dittmer and Jensen

solving both scenarios and in effect calculate the value
of information between the two information scenarios.
The crucial thing about a strong junction tree is that
it allows marginalization in a proper (reverse) tem­
poral order and this can be done for both temporal
orders in the strong junction tree shown in Figure 7c.
This strong junction tree is obtained from the junction
tree in Figure 7a by adding B to the cliques down to
(D1, C).
This observation can be used in general: To obtain a
strong junction tree with strong root Co for calculat­
ing VOI(A, D;, jiVi), construct a strong junction tree
for the scenario with A in Ij -1 · Then Co imposes a
(partial) order < for the cliques, such that C < C' if
and only if C is on the path from C' to C0. Identify
the cliques C; and CA . C; is the clique closest to the
Co containing D;, and CA is the clique closest to Co
containing D;. Let C;A be the "greatest lower bound"
of C; and CA . That is, C;A is the clique furthest away
from Co such that C;A < C; and C;A < CA (when the
temporal order is strict, then C;A = C;). Finally, ex­
tend all cliques on the path between C;A and CA with
the variable A.
As mentioned earlier, a control structure is associated
with the (strong) junction tree. This structure handles
the order of marginalization, and therefore we can use
the expanded junction tree (and the associated con­
trol structure) in Figure 7c to marginalize B from any
clique of our chaise. After B has been marginalized
from a clique, the table space reserved for B in cliques
closer to the strong root is obsolete. Clever use of
the control structures will prevent calculations to take
place in the remaining table expansions, and the num­
ber of table operations in the remaining subtree equals
that of an ordinary strong junction tree.
3.1

Figure 8: An influence diagram with temporal order
from left to right (no-forgetting arcs are not included) .
It discloses temporal independence between D3 and
{D2, D4}. (From (Jensen et al., 1994))

a

NON-STRICT TEMPORAL ORDERS

As mentioned previously, a proper elimination order
of an influence diagram is an order where the elimi­
nation order of each node and its Markov blanket is a
reverse temporal order. This means that although the
influence diagram in the offset requires a linear tem­
poral order of the decisions, then the actual diagram
may disclose temporal independencies which can be
exploited when solving it.
The influence diagram in Figure 8 has a temporal order
of the decision nodes with increasing index. However,
when f has been observed, then Da can be decided at
any time independently of the observations and deci­
sions on e, g, D2, and D4. This is also reflected in
the strong junction tree in Figure 9a where the branch
containing D3 can be marginalized independently of
the other branches.

b

Figure 9: Strong junction trees (derived from Figure 8
illustrating the difference between never observing h
(a) and observing h immediately before decision D2
(b) when decisions are not strictly ordered.

Myopic Value of Information in Influence Diagrams

The value of information technique is illustrated on
the influence diagram in Figure 8 through Figures 9a
and 9b. The strong junction tree in Figure 9a can also
be used to solve an influence diagram with h observed
before deciding on D3. The difference between the
two scenaria is reflected in the control structure for
the collect operation rather than in the junction tree.
A strong junction tree also being able to handle the
situation where h is observed before deciding on D2 is
shown in Figure 9b.
3.2

NOTATION

In Figure 10, we present an extended version of the in­
fluence diagram from (Jensen et al., 1994). The origi­
nal influence diagram notation has been extended with
triangular nodes, observation nodes. An observation
node designates that the chance node associated with
it will be observed within some interval of information
sets.

g

147

would have the observation interval [ h; Is].

3.3

ALTERNATIVE METHODS

There are other methods for calculating the value of
information in influence diagrams. These can be sep­
arated into multiple-model methods and single-model
methods.
The value of information in influence diagrams can be
viewed as the difference in expected utility between a
set of influence diagrams each implementing a specific
scenario of the desired observation-decision sequences.
In that view Ezawa (1994) creates and solves multi­
ple models for calculating the value of information in
influence diagrams. However, as the construction of
strong junction trees is a complex task it is preferable
to reduce the number of different junction trees. Also,
to cover all desired observation-decision sequences the
decision analyst may be facing a considerable task in
constructing the needed influence dia grams .
Instead, the different decision models in Figure 4 and
Figure 6 can be combined into a single influence dia­
gram which gives us the power to calculate whether or
not to observe B. Such a model is shown in Figure 11.

Figure 10: Influence diagram from (Jensen et al., 1994)
with extended notation.
Though there may not be any computational difficul­
ties associated with observing variables at an earlier
time than modeled, there may be some conceptual
problems. It does not make sense to observe, say, the
state of a fungus attack on your crop in May before
deciding whether or not to apply fungicide in April.
In other words: We cannot observe a variable prior to
making a decision that influences it.
Hence, a variable is modeled in the influence dia­
gram as belonging to the last information set possible,
and the observation node is associated with a "lower
boundary" for the observation. For node c in Figure 10
the lower boundary is !0, yielding the observation in­
terval to be [!0; !4] whereas the lower boundary for
node j is h and hence the observation interval for j is
[!1;!4]. If associated with an observation node, node

Figure 11: General model capable of handling the sce­
narios of Figures 4 and 6.

The resulting model consists of the original model
without observation on B (f ro m Figure 4) with an ad­
ditional two nodes; a decision node, Do and the chance
node B'.
Do will consist of the decisions (B) and (--,B) and the

observed node, B' will have the same states as its un­
observed counterpart, B, plus an additional state, (No
observation). If the optimal decision, d0, is (B), then
B ' is observed and set to the true state of B; if the op­
timal decision is (-.B), B' is set to (No observation).
The probability table for B is equal to the one specified
in Figures 4 and 6 and the behavior of B' is specified

148

Dittmer and Jensen

as
B'
B'

=

No observation

=

B otherwise

for

d0

=

(-,B)

This type of modeling cannot be called neither simple
nor intuitive. Furthermore, as can be seen from Figure
12, the junction tree for the general model in Figure 11
is larger than the junction tree produced by expansion
(Figure 7c).

Figure 12: Strong junction tree constructed from the
general model of Figure 11.
It is also worth noting that the model in Figure 11
and its corresponding junction tree in Figure 12 are
made for the case where B is either unobserved or ob­
served before D1. The junction tree in Figure 7c is
capable of calculating the expected utility for the de­
cision problem with B belonging to any information
set. Should the model in Figure 11 be extended to the
same flexibility, we are facing a larger and consider­
ably less intuitive model with little resemblance to the
original decision problem.
4

CONCLUSION

For specific influence diagrams, such as scenarios with
non-intervening decisions, we have presented a simple
method for calculating the value of information. This
method is simple in construction and cheap in terms
of time and space requirements, but is restricted in
the structure of the influence diagram. It is based
on methods developed by (Cooper, 1988) and ( Jensen
& Jiangmin L., 1995). For certain, well-defined tasks
there may be advantages in using this method but in
the general case we propose to use the method pre­
sented for influence diagrams with sequences of inter­
vening decisions.
In strong junction trees constructed for decision prob­
lems formulated as general influence diagrams we are
able to calculate the value of information for a given
chance node, that is, the gain in expected utility
from observing variable X before making a decision
In other words, we can calculate the differ­
Di.
ence in expected utility between models that differ in
observation-decision sequence, using the same junction
tree structure with only a number of tables expanded
but not recalculated. We find this method far more in­
tuitive than modeling all possible outcomes in a gen­
eral influence diagram as the structure of the model

will not change even when chance nodes (within lim­
its) are observed prior to the latest possible observa­
tion time. Also, modeling observations as intervening
decisions may seem unappealing to decision analysts.
In addition to this, we experienced that the junction
trees produced from the general models are larger than
those produced by table expansion.
Using our method is not for free as in its worst case
(modeling a chance node as never observed and ob­
serving it before the first decision D1) all tables in the
junction tree will be expanded (assuming that the de­
cisions are strictly ordered). This means that with a
states in the node in question, the resulting junction
tree will be almost a: times larger than the original
junction tree. This corresponds to performing a: prop­
agations in the strong junction tree and the gain is
therefore minimal.
However, the method presented will only expand the
tables needed, that is, only part of the junction tree
becomes larger (by a factor of a: ) which consequently
reduces the number of operations performed during
a propagation. Also, clever use of the control struc­
tures associated with the strong junction tree will pre­
vent excess operations in the expanded tables after
marginalization of the node in question. Still, if for
example r is very large and if all A E r are placed in
In, we may very well face an intractable problem as we
expand the cliques beyond the capacity of computers.
Topics for further research include the possibility for
utilizing independence assumptions in order to further
reduce complexity.



When using Bayesian networks for modelling
the behavior of man-made machinery, it usu­
ally happens that a large part of the model
is deterministic. For such Bayesian networks
the deterministic part of the model can be
represented as a Boolean function, and a cen­
tral part of belief updating reduces to the
task of calculating the number of satisfying
configurations in a Boolean function. In this
paper we explore how advances in the calcu­
lation of Boolean functions can be adopted
for belief updating, in particular within the
context of troubleshooting. We present ex­
perimental results indicating a substantial
speed-up compared to traditional junction
tree propagation.
1

INTRODUCTION

When building a Bayesian network model it frequently
happens that a large part of the model is determinis­
tic. This happens particularly when modelling the be­
havior of man-made machinery. Then the situation is
that we have a deterministic kernel with surrounding
chance variables, and it seems excessive to use stan­
dard junction tree algorithms for belief updating. First
of all, the calculations in the deterministic kernel are
integer calculations and double precision calculations
are unnecessary complex. However, there may be room
for further improvements. If the deterministic part of
the model is represented as a Boolean function, we
may exploit contemporary advances in calculation of
Boolean functions.
A major advance in Boolean calculation is Binary
Decision Diagrams, particularly Reduced Ordered Bi­
nary Decision Diagrams, ROBDDs[Bryant, 1986]. An
ROBDD is a DAG representation of a Boolean func­
tion. The representation is tailored for fast calculation

Uffe Kjcerulff

of values, but the representation can also be used for
fast calculation of the number of satisfying configura­
tions given an instantiation of a subset of the variables.
To be more precise: let B(X) be a Boolean function
over the Boolean variables X, and let Y � X with
X\Y. Define Cards (1)) on a configuration iJ
Z
of y as the number of configurations z over Z such
that B (1j, z)
true( 1). It turns out that given iJ an
ROBDD representation of B can be constructed such
that Cards can be calculated in time linear in the num­
ber of nodes in the ROBDD. However, the number of
nodes in an ROBDD may be exponential in the num­
ber of variables in the domain of the Boolean function.
=

=

In this paper we exploit the ROBDD representation for
propagation through a Boolean kernel in a Bayesian
network, and we illustrate that a central part of this
propagation is to calculate Cards (i)). We use the tech­
nique on models for troubleshooting. These models
are particularly well suited for ROBDD calculation as
the size of the ROBDD is quadratic in the size of the
domain.
In section 2 we illustrate the use of Cards for prob­
·
ability updating in Bayesian networks. Section 3 is
a brief introduction to ROBDDs and in section 4 we
show how to calculate Cards in an ROBDD. Section
5 introduces the troubleshooting task and the type of
Bayesian network models used. In section 6 the de­
terministic kernel of these models is represented as an
ROBDD and it is shown that the size of this represen­
tation is quadratic in the number of Boolean variables.
In section 7 we outline the propagation algorithms for
various troubleshooting tasks, and in section 8 we re­
port on empirical results indicating a substantial speed
up compared to traditional junction tree propagation.
2

TWO MOTIVATING EXAMPLES

To illustrate the special considerations in connection
with Boolean kernels we shall treat a couple of exam-

427

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

3

ples. First consider the situation in Figure 1.

BOOLEAN FUNCTIONS AND
ROBDDS

This section is a survey of classical logic in the context
of binary decision diagrams.
Figure 1: The Boolean variable A has a parent net­
work of proper chance variables and a child network
representing a Boolean function B.
For the situation in Figure 1 we have (U
P (U)

=

=

WUVU{A}):

Q (W,A)I.t B (V,A),

where 1-l = 1/ Lvu{A} B (V,A) is a normalization con­
stant. Assume we have evidence e ew u ev , where
ev is a configuration y of the variables Y � V, then:
=

P(A,e)

1-l

1-l

L
L
w
w

Q(W,ew,A) L B(Z,y,A)
z

If we extend the example s.t. a Boolean variable C E V
has a child network R (T, C) of proper chance variables,
we get ( the normalization constant is omitted) :
P (U) = Q (W,A)B (V,A)R (T,C)
Assume we have evidence e = ew u ev u er, where ev
is a configuration y of the variables Y � V. If er is
empty then R does not contribute, and the calculations
are as for Figure 1. If not, we have:

L
L (L
w

Q(W,ew,A)

·

B (Z,y, A, C)

Z

C

L

)

T

w

(L
L

B (Z,y,A,C

=

y)

Z

+

T

B (Z,y,A, C =n )

Z
=

L Q(W,ew,A)

R (T,e7,C =n )

)

T
·

w

(

CardB(y,A, C = y)

+

Card8(Y,A, C = n

� R (T,er, C
) � R (T,er, C

=

y)

=n )

All operators in propositional logic can be expressed
using only this operator and this can be done s.t. tests
are only performed on unnegated variables.
Definition 1. An If-then-else Normal Form (INF) is
a Boolean function built entirely from the if-then-else
operator and the constants 0 and 1 s.t. all tests are
performed only on variables.

B

R (T,er,C = y )

L
L

Let X --7 Y1, Y2 denote the if-then-else operator. Then
X --7 Y1 , Y2 is true if either X and Y, are true or X is
false and Y2 is true; the variable X is said to be the
test expression. More formally we have:

Consider the Boolean function B and let B[X H 0]
denote the Boolean function produced by substituting
0 for X in B. The Shannon expansion of B w.r.t. X is
defined as:

R (T,e7, C)

L Q (W,ew,A)·

=

A truth assignment to a Boolean function B is the
same as fixing a set of variables in the domain of B,
i.e., if X is a Boolean variable in the domain of B, then
X can be assigned either 0 or 1 ( denoted [X H 0] and
[X H 1], respectively) .
A Boolean function is said to be a tautology if it yields
true for all truth assignments, and it is satisfiable if it
yields true for at least one truth assignment.

Q(W,ew,A)CardB (y,A),

As the example illustrates, an efficient procedure for
calculating CardB is central for probability updating.

P(A,e) =

The classical calculus for dealing with truth assign­
ments consists of Boolean variables, the constants true
(1) and false (0) and the operators 1\ (conjunction),
V (disjunction), -. (negation), =} (implication) and {:::}
(bi-implication). A combination of these entities form
a Boolean function and the set of all Boolean functions
is known as propositional logic.

)

Again, calculation of Card8 is part of belief updating.

=:

X -t B [(<

H

1], B [X H 0]

From the Shannon expansion we get that any Boolean
function can be expressed in INF by iteratively using
the above substitution scheme on B.
By applying the Shannon expansion to a Boolean func­
tion B w.r.t. an ordering of all the variables in the do­
main of B we get a set of if-then-else expressions which
can be represented as a binary decision tree. The de­
cision tree may contain identical substructures and by
"collapsing" such substructures we get a binary deci­
sion diagram ( BDD) which is a directed acyclic graph.
The ordering of the variables, corresponding to the
order in which the Shannon expansion is performed,

428

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

is encoded in the BDD hence, we say that the BDD
is an ordered binary decision diagram (OBDD); the
variables occur in the same order on all paths from the
root. If all redundant tests are removed in an OBDD
it is said to be reduced and we have a reduced ordered
binary decision diagram (ROBDD).
Definition 2. A reduced ordered binary decision di­
agram {ROBDD) is a rooted, directed acyclic graph
with
•

•

•

•

ROBDD. The algorithm basically propagates a num­
ber (2 n , where n is the number of distinct variables
in the corresponding Boolean function) from the root
of the ROBDD to the terminal node. The value sent
from a node (including the root) to one of its children
is the value associated with that node divided by 2.
The value associated with a node (except the root) is
the sum of the values sent from its parents (see Fig­
ure 2).

one or two terminal nodes labeled 0 and 1 respec­
tively.
a set of non-terminal nodes of out-degree two with
one outgoing arc labeled 0 and the other 1.
a variable name attached to each non-terminal
node s.t. on all paths from the root to the ter­
minal nodes the variables respect a given linear
ordering.
no two nodes have isomorphic subgraphs.

We will use Eo to denote the set of 0-arcs (drawn as
dashed arcs) and £ 1 to denote the set of l-ares (drawn
as solid arcs).
Theorem 1 ([Bryant, 1986]). For any Boolean
function f : {0,l}n --7 {0,1} there is exactly one
ROBDD B with variables X, < X2 <
< Xn
s.t. B[X, H b1 ,X2 H b2, ... ,Xn H bnJ
f (b1,b2,... ,bn), \i (b1,b2,... ,bn) E {0,l}n.
·

Figure 2: There are 3 satisfying configurations for
the Boolean function "Exactly one variable among
A 1, A 2, A 3 is true" represented by this ROBDD.
Definition 3. Let B = (U,£) be an ROBDD. Propa­
gation in B is the computation of v (u), where u E U
and v : U --7 lR is defined as:

· ·

From Theorem 1 we have that in order to calculate
the number of satisfying configurations in a Boolean
function B we can produce an ROBDD equivalent to
B and then count in this structure.
In the remainder of this paper we assume that an
ROBDD has exactly one terminal node labeled 1, as we
are only interested in the number of satisfying configu­
rations; in this situation we allow non-terminal nodes
with out-degree one. Additionally, we will use the term
"nodes" in the context of ROBDDs and "variables"
when referring to a Boolean function or a Bayesian
network(BN); nodes and variables will be denoted with
lower case letters and upper case letters, respectively
(the nodes representing a variable Xi will each be de­
noted Xi if this does not cause any confusion).
4

2000

CALCULATION OF CARDB
USING ROBDDS

Given an ROBDD representation of a Boolean func­
tion B, the number of satisfying configurations can be
calculated in time linear in the number of nodes in the

•

•

v (r) 2 n , where r is the root in B and n is the
number of distinct variables in B.
LpEnd' v(p) , where nu repre\iu E U\{r}: v (u)
=

=

sents the set of parents for u in B.

So, in order to determine Cards for some Boolean
function B (U) we only need one propagation in the
corresponding ROBDD since Cards
v (l). In case
evidence y has been received on the variables Y � U
we simply modify the algorithm s.t. configurations, in­
consistent with y, does not contribute to the propaga­
tion, i.e., given a configuration y the function v (u)y is
defined as:
=

\iu E U\{r}.. v (u)y

_
-

LvEna v(p)y ,
2

where nR = {p E nul[p (j. Yl or [y(p) = i and (p, u) E
£i]}; y(p) is the state ofp E Y under y and v (r) 2n,
n being the number of distinct variables in B including
those on which evidence has been received. In partic­
ular we have that Cards (i:i) = v (1)y. Notice, that the
structure of the ROBDD is not changed when evidence
is received.
=

The size of the ROBDD has a significant impact on
the performance of the algorithm and the problem of

429

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

identifying a minimal sized ROBDD is NP-complete.
Thus, in the remainder of this paper we shall mainly
focus on troubleshooting models as it turns out that
the structure of such a model ensures that the size of
the corresponding ROBDD is at most quadratic in the
size of the domain.
5

TROUBLESHOOTING

Definition 4. A troubleshooting model is a con­
nected BN T5 =((U =Us U Uc U UA,£), P), where:
•

•

The set Us contains a distinct variable 5 with no
successors, and for each 51 E Us\{5} there exists
a directed path from 51 to 5.
For each variable C EUc there exists an 51 EUs
s.t. C Ens, and nc 0.
=

When troubleshooting a device which is not working
properly we wish to determine the cause of the problem
or find an action sequence repairing the device. At any
time during the process there may be numerous differ­
ent operations that can be performed e.g. a component
can be repaired/replaced or the status of a component
can be investigated. Because such operations can be
expensive and may not result in a functioning device,
it is expedient to determine a sequence of operations
that minimizes the expected cost and (eventually) re­
pairs the device.
[Breese and Heckerman, 1996] presents a method to
myopicly determine such a sequence. The method as­
sumes a BN representing the device in question, and
the BN is assumed to satisfy the following properties.
1. There is only one problem defining variable in the
BN and this variable represents the functional sta­
tus of the device.

2. The device is initially faulty.
3. Exactly one component is malfunctioning causing

the device to be faulty (single fault).
A central task of troubleshooting, within the frame­
work of [Breese and Heckerman, 1996], is the calcula­
tion of Pi =P( C i =faulty!e) which denotes the prob­
ability that component ci is the cause of the problem
given evidence e. So we are looking for a way to exploit
the logical structure of the model when calculating the
probabilities Pi· As such a scheme is strongly depen­
dent on the structure of the troubleshooting model we
give a syntactical definition of this concept. The def­
inition is based on BNs: a BN consists of a directed
acyclic graph G =(U, £) and a joint probability dis­
tribution P(U), where U is a set of variables and £ is
a set of edges connecting the variables in U; we use
sp(X) to denote the state space for a variable X E U.
The joint probability distribution P(U) factorizes over
U s.t. :

P(U)

=

•

•

•

For each variable A EUA there does not exist an
X EU s.t. A E nx.
sp(X) ={ok, �ok}, VX EUs U Uc.
For each X E Us: P(xly) =1 or P(xly)
sp(X) and Vy E sp(nx).

=

0, Vx E

The variable 5 is termed the problem defining variable
and the variables Us are termed system variables. The
variables Uc (termed cause variables) represent the set
of components which can be repaired, and the vari­
ables in UA (termed action variables) represent user
performable operations such as observations and sys­
tem repairing actions; notice that UA is not part of
the actual system specification. In the remainder of
this paper we shall extend the single fault assump­
tion to include the system variables also. That is, if
a system variable 5i is faulty, then there exists ex­
actly one variable X E ns, which is faulty also (see
[Skaanning et al., 1999] for further discussion of this
extension and how the single fault assumption can be
enforced using so-called "constraint variables" ).
Figure 3 depicts a troubleshooting model, where A is
an action variable and 5 represents the problem defin­
ing variable. The variables 51 ,5z,53 and 54 repre­
sent subsystems, which should be read as: the sys­
tem 5 can be decomposed into two subsystems 51 and
5z, and subsystem 51 can be decomposed into 53 and
54. Component C 1 can cause either 53 or 54 to fail,
whereas C z can cause either 5z or 54 to fail (neither
C 1 nor C z can cause two subsystems to fail simultane­
ously). Notice that A is not part of the actual system
model.

IJ P(XInx),

XEU

where nx is the parents of X in G. The set of con­
ditional probability distributions factorizing P(U) ac­
cording to G is denoted P.

Figure 3: A troubleshooting model with five system
variables, two cause variables and one action variable.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

430

From assumption (2) and (3) we have that:

P(C1 =y, e)
=P(C1 =1J, Cz =n, . . . , Cm =n, e)
m
= P(C1 =y)
P(Ci n)
i=2
�-tB(C1 = y, Cz n, .. . , Cm =n, Us, e)

II

=

.L_

=

Us

II P(Ci

=

i=2

n ) �-tCard s (C1 = y, e),
•

where B(Us, Uc) is a Boolean function (specified in the
following section) and J.t is a normalization constant.
Now, P(C11el = P(C1, e)/P(e) and P(e) is given by:

m

.[_ IJ P(Cd!-!B(C1, ··· , Cm, S, S, . . . , Sn, el
u

i=1

In the remainder of this paper we omit the normaliza­
tion constant.
6

•

m

= P(C1 =y)

P(e) =

(and only one) of its subsystems (Sc) is faulty (if a
cause is not present we can not say anything about its
subsystems). M says that there can be either zero or
at most one cause present (consistent with the system
state). B(U) is the Boolean function representing the
system as a whole. Note that:

ROBDDS AS
TROUBLESHOOTING MODELS

•

The Boolean function is a list of expressions for
local constraints and it can therefore be built in
an incremental fashion.
The Boolean function can easily be modified to
represent any logical relation between the compo­
nents.
The expression ensures the single fault assump­
tion based on the structure of the model, i.e., it is
not necessary to introduce "constraint variables" .

Example 1. The Boolean function representing the
troubleshooting model depicted in Figure 3 is specified
by B:

((SA(S1 l3l Sz)) V (�SA--,51--,Sz))
B1A((S1A(S3 l3l S4)) V (�S1A�s3�S4))

In what follows we shall assume single fault and use
the truth values 1 and 0 to denote the state of a com­
ponent/subsystem (1 indicates a fault).

B3A(Cz:::} (Sz!Zl S4))
84A((SA(C1 IZl Cz)) V (�SA�C1A�Cz))

Now, let nsi be the subsystems which immediately
compose Si E Us and let Sc � Us be the subsys­
tems that component C E Uc can cause to fail; Sc
is the immediate successors of C. The Boolean func­
tion representing the logical kernel of a troubleshoot­
ing model TS =((Us UUc UUA, £), P) is then given by
B(U' =Us UUc):

Given the ordering S, S1, Sz, S3, S4, C1, C2, the
ROBDD corresponding to B is depicted in Figure 4.
Note that all paths from the root S to the terminal
node are consistent with the ordering above.1
D

F(T)

G(C)

(

®

TA

) (

s' v �rA

S'EnT

Q9 T

C=}

1\

�s'

S'EnT

TESc

M

( ® ) (
( f\ ) ( f\
c

sA

v

�sA

CEUc

B(U')

F(T) A

TEUs

1\
CEUc

�c

)

)

)

G(C) AM,

CEUc

where ® �=1 xi denotes an exclusive-or between the
variables {X1, . . . , Xn}. F(T) specifies that if the sys­
tem Tis malfunctioning then one (and only) of its sub­
systems is faulty, and if the system is functioning prop­
erly then all of its subsystems are functioning properly
also. G(C) states that if a cause is present then one

Figure 4: An ROBDD representation of the trou­
bleshooting model depicted in Figure 3.
1The ROBDD was generated by the software tool
http:/ fwww.cs.auc.dk/�behrmann/iben/.

iben,

431

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Now, as indicated in Section 3, the size of the ROBDD
is dependent on the ordering of the variables. So we
are looking for a general "rule of ordering" producing
ROBDDs of "small" size.
Consider an ordering of the variables where each sys­
tem variable occurs before all the variables repre­
senting its subsystems, and where all the cause vari­
ables occur last in the ordering. By constructing the
ROBDD according to this ordering we get the node
representing the problem defining variable as root and
the nodes representing the cause variables at the bot­
tom (see Figure 4). Moreover, we get an upper bound
on the size of the ROBDD as stated in the following
theorem; note that the action variables are not part of
the logical kernel.
Theorem 2. Let TS=((U =UAUUsUUc,£), P) be a
troubleshooting model. Then the size of the ROBDD,
representing the Boolean function B(Us U Uc), is
O(IUsi2+1Ucl2), if the ordering a: UsUUc H IUsUUcl
satisfies:
•

•

VX E Us: a(X) < a(Y) for each Y E nx.
VZ E Uc there does not exist an X
a(Z) < a(X).

E

Us s.t.

Proof. Assume an indexing of the layers in the
ROBDD s.t. the layers containing the root node and
the terminal node have index 1 and IUs U Ucl + 1, re­
spectively; a layer is the set of nodes representing a
distinct variable.
Now, consider the layers consisting of system nodes
but no cause nodes. The number of nodes in the i'th
layer either equals the number of nodes in the i'th - 1
layer or it has exactly one more node than the i'th- 1
layer. This is the same as saying that at most one
node in the i'th- 1 layer branches in two; if two differ­
ent nodes in a layer branched into two we would have
two distinct paths from a node at a higher level to
these nodes however, this contradicts the single-fault
assumption due to the ordering of the nodes. Thus,
the number of nodes in the layers containing system
1
nodes is at most L. �Z::11 i= IUs l{l�s I+ 1.
For the cause nodes, there can be at most one distinct
path for each of their possible configurations. This
means that the number of nodes in the layers contain­
ing cause nodes is at most IUcl('�cl) = 1Ucl2. Hence,
D
the size of the ROBDD is O(IUsl2 + IUcl2).
In the ROBDDs, we have an all-false path from the
root to the terminal node. Indeed the Boolean function
is true when the model has no fault. However, we can
force S to be true (faulty) to avoid this path.

7

PROPAG ATION USING ROBDDS

For our context, we need to compute the number of
satisfying configurations for each instantiation of the
cause variables (see Section 5). Now, if we order the
variables as described in Theorem 2 we get an ROBDD
where the nodes representing the cause variables are
the nodes closest to the terminal node. This means
that after one propagation we can determine all the
values needed, i.e., the number of configurations con­
sistent with ci=1J and evidence e is given by:

CardB(C=y,e)=

c· )
" v(#\ie'
­

L

CtECt

2

where Ci is the set of nodes Ci with an outgoing 1-arc
and # li is the number of arcs on the path li from the
Ci in question to the terminal node; the single-fault
assumption ensures that there exist exactly one path
from each Ci to the terminal node which include the
1-arc emanating from Ci.
However, this scheme does not take user performable
operations (i.e. UA) into account, and in the follow­
ing section we extend the algorithm to include such
scenarios.
7.1

Inserting evidence

After an action has been performed we may gain new
knowledge about the system. This knowledge is incor­
porated into the model by instantiating the appropri­
ate variable. If either a system variable or a cause vari­
able is instantiated we can use the method described in
Section 4. So, let A E UA be a binary variable associ­
ated with a proper conditional probability distribution
P(AISi) and assume that A= y is observed. In order
to take the state of A into consideration we get:

P(C1 =y, A=y)
=P(Cl =1J,Cz=n, ... ,Cm=n, A=y)
=P(C1 =y) IT P(Ci=n) L (P(A=yiSd
Us
B(C1 =y, Cz=n, ... , Cm =n,Us))
By expanding the sum in the above equation we get:

L P(A=yiSi)B(Cl =y, Cz=n, ... , Cm= n,Us)
Us
=P(A=yiSi=y)CardB(C1 =y, Si=y)
+P(A=yiSi=n)CardB(Cl =1J,Si=n)
(1)

Thus, with one piece of evidence we can retrieve the
probabilities with two propagations. However, if we
have a set of actions u;,._ � UA with parents u;, �Us

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

432

we need to count the number of satisfying configura­
tions consistent with each configuration of U$. So, by
using the above approach, the number of times we need
to count is exponential in the number of variables on
which evidence has been received.
In what follows we will consider a different algorithm,
where all values can be found after one propagation.
Initially we assume that evidence has been received on
exactly one variable, but the algorithm can easily be
generalized to any number of variables.
In order to prove the soundness of the algorithm
we will use the following notation. If B is an
ROBDD with root r, then l;_
{ v i r = v1, . . . , V;_ =
vis a directed path in B} is termed the i'th layer of
B; the layers l1 and ln+ 1 contain the root node and
the terminal node, respectively. So, given a Boolean
function over the variables U = {X1 , Xz, . . . , X n} (or­
dered by index), the corresponding ROBDD can be
1
specified as B =(Us = U��1 lk, £ = £1 U £o); assum­
ing that the variable X;_ is represented by the layer
l_; . Now, let f : sp(W) -t IR be a function where
W ={Xi, . . . , Xi} � U, and assume that the variables
are ordered by index. We define the following parti­
1
tioning of B =(Us = u�;;1 lk, £) w.r.t. f:
=

•

•

•

The root part of B w.r.t. f is given by BT =
1
(Uf3, £f3), where Uf3 = u�: 1lk.
The conditioning part of B w.r.t. f is given by
lk.
Be =(U� , £�), where U� =

ul=i

The terminal part of B w.r.t. f is given by Bt =
1
(U�, £�), where U� = u�,:i +1lk.

For ease of exposition, we shall in the remainder of this
section assume that no evidence has been received on
any variable in Us UUc; the results presented can easily
be generalized to this situation also.
Algorithm 1. Let B =(U = U�11li, £ £1 U £o) be
an ROBDD corresponding to a Boolean function over
the variables U = {X1 , Xz, . . . , Xn}, and assume that
the variables are ordered by index. Let f : sp(W) -t IR
be a function with W � U and let Q =W\{Xj}, where
X i E W is the variable with highest index.
=

i) Propagate from the root to the terminal nodes in
the root part of B.
ii) Use the values obtained in step (i) to perform a
propagation in the conditioning part of B, i. e. , for
each q E sp( Q) :
a) Propagate to layer li.
b) If there exists an arc (p, u) E Ci from a node
p E lj to a node u E li +1 add the value
to the value ofu.

c(ii.X;=;i)v(p)q)

iii) Use the values obtained in step (ii) to propagate
in st.

Note, that the number of variables in the domain off
determines the number of iterations performed by the
algorithm. In particular, if IWI = 1 we only need one
iteration.
Theorem 3. Let B = (U = U �1 li, £ = £1 U£o) be
an ROBDD and let f : sp(W) -t IR be a function where
W � U. If Algorithm 1 is invoked on B, then:

1

v(l)

=

L f(w)Cards(w)

wEsp(W)

Proof. Let Q =W\{Xj}, where X i E W is the variable
i
with highest index. Let q E sp( Q) and let n�q, ) =
{p E nuiP f/. W or (p, u) E £;_}. Then 'v'u E li+ we
have:

1

LqEsp(Q)

v(u)

LqEsp(Q)

(L.bE{0,1},pEn�<i.b l v(p)qf(q, bl)
2

(L.bE{O,l},pEn�'l.bl v(p) (q,b f(q, bl)
)

2
LwEsp(W) (L.vEn� v(p)wf(wl)
2
f
)
(
LwEsp{Wl w LvEn� v(p)w
2
L f(w)v(u)w
wEsp(W)

=
=

t 1 :
Let u E lt, for l > j + 1. Suppose that 'v'p E lv(p) = LwEsp(W) f(w)v(P lw · Then:
v(p) LvEnu LwEsp(W) f(w)v(p)w
=
v(u) LvEnu
2
2
=

In particular we have that for l =n + 1:

v( 1)

LwEsp(W) f(w) LvEnu v(p)w
2

L f(w)Card8(w)
wEsp(W)
Thereby completing the proof.

D

By performing induction in the number of operations
the algorithm can easily be extended to handle multi­
ple functions, assuming that the variables in the do­
main of the functions do not overlap; the variables in
the domain of two functions f and g are said to over­
lap w.r.t. the ordering a if a(Xd < a(X k ) < a(Xj ),
where xk is a variable in the domain of g, and xi and
Xi are the variables in the domain of f with lowest
and highest index, respectively. If the variables of two
functions overlap we can multiply these functions and
consider the resulting function.

433

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Example 2. Consider the troubleshooting model de­
picted in Figure 3, and assume that action A E UA
is associated with the conditional probability distribu­
tion specified in Table 1; nA = {5z, 54}.

B'

B'

C0

(0.

'•

...

,,

Table 1:
The conditional probability function
P(AI5z, 54).
Sz
Sz

=
=

0.6
0.4

0.3
0.2

1
0

�&
0.4. 2

The ROBDD corresponding to this specification is de­
picted in Figure 4. In the naive approach, if A = y is
observed, we perform three propagations to the termi­
nal node (one propagation for each configuration of 5z
and 54 except for (5z
1 , 54
1 ) due to the single
fault assumption). The resulting counts are weighted
with the appropriate values and then added (see equa­
tion 1): 0 0.3 + 4 0.2 + 1 0.4 + 1 0.6 = 1.8.
=

·

·

=

·

·

When using algorithm 1 we start off by propagating to
the layer l3 (the nodes representing 5z); after propa­
gation, each node in l3 is associated with 25. We then
perform two propagations to the layer ls (the nodes
representing 54); each propagation is conditioned on
the state of 5z, i.e., 1 and 0, respectively. After each
propagation, the resulting value is multiplied with the
appropriate value from the conditional probability ta­
ble and then added to the value associated with its
child. So, the final value can be found with less than
two full propagations (see Figure 5); note that we only
D
perform one propagation in W and in Bt.
Step (ii) of Algorithm 1 can be optimized by start­
ing the iteration with the variable with highest in­
dex, and then iterate in reverse order of their in­
dex. That is, when iterating over the variables
{X1, Xz, . . . , Xt-1, Xt} we can start off by propagat­
ing to the layer containing Xt, for some configuration
of {X1, Xz, . . . , Xt-1}. The values associated with the
nodes Xt-1 can then be used when propagating from
the nodes Xt, for each instance of Xt. The same ap­
plies when considering variables of lower index, i.e.,
we can reuse previous computations. For instance, in
Figure 5 we can use the value from the first iteration
when computing the value 0.2 22 associated with c1
(consistent with (5z = 0, 54 1 ) ).
·

=

8

RESULTS

We have measured the performance of the ROBDD
algorithm by comparing it to the Shafer-Shenoy algo­
rithm [Shafer and Shenoy, 1990] and the Hugin algo­
rithm [Jensen et al., 1990] w.r.t. the number of opera­
tions performed during inference; the number of opera-

2

�

.. ()_
B'

:· .
.

0 4• 2

2

�

_':_.. __

0

·

,.···

- --�r
{a)

8

'Q_......':... �.::. ..... 0..
.

..

.

0 6•2 2

··8

i

.8"

•..··

(bl

Figure 5: Figure (a) depicts the ROBDD after propa­
gation w.r.t. the configuration (5z = 0, 54 = 0). Fig­
ure (b) depicts the ROBDD after the full propagation;
no propagation is performed w.r.t. (Sz
1, 54
1)
due to the single fault assumption.
=

=

tions refers to the number of additions, multiplications
and divisions.
The tests were performed on 225 randomly generated
troubleshooting models (see Definition 4) which dif­
fered in the number of system variables, cause vari­
ables and action variables; the total number of vari­
ables varied from 21 to 322 and for a fixed set of vari­
ables 15 different troubleshooting models were gener­
ated. As the single fault assumption is not ensured in
the troubleshooting models we augmented these mod­
els with constraint variables when using the Hugin al­
gorithm and the Shafer-Shenoy algorithm (the single
fault assumption is naturally ensured in the ROBDD
architecture). Finally, evidence were inserted on the
problem defining variable and on the constraint vari­
ables.
Figure 6 show plots of the number of operations per­
formed as a function of the number of variables in the
models. Note that we use a logarithmic scale on the
y-axis and that the numbers on the x-axis do not rep­
resent the actual number of variables in the models.
The plots show that, w.r.t. the number of operations,
propagation using ROBDDs is considerably more ef­
ficient than both Shafer-Shenoy and Hugin propaga­
tion. Moreover, as indicated in Section 6, the tradi­
tional tradeoff between time and space is less apparent
in the ROBDD architecture, as the space complexity
is O(IUcl2 + IUsl2 ).
It should be noted that the tests were designed to

434

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

""�

20000

I

�
..

ll�

I

•

I

I

I

I

I

1

I

I

1

�

K

10000

I

1000

�

100

•
20

100

I
..
Variable�

1<0

Variable•
Sluorer-Sheuoy

Figure 6: A plot of the number of operations per­
formed by Hugin, Shafer-Shenoy and ROBDD prop­
agation as a function of the number of variables in
randomly generated troubleshooting models (logarith­
mic scale).
compare ROBDD propagation with Shafer-Shenoy and
Hugin propagation, and they should not be seen as a
comparison of Shafer-Shenoy propagation and Hugin
propagation. In particular, we have only considered
troubleshooting models and not Bayesian networks in
general.
The efficiency of the ROBDD architecture is partly
based on the single fault assumption. However, this
assumption can also be exploited in certain trou­
bleshooting models by compiling the original model
TS=((Us UUc UUA,£), P) into a secondary Bayesian
network BN ((UA U{CU
} {S,} £'),P'), where Cis a
variable having a state for each cause variable in the
original model together with a state representing the
situation where no fault is present. S is a problem
defining variable having Cas parent, and UA is the set
of action variables in the original model each having C
as parent. We have compared the ROBDD architec­
ture with this approach using the randomly generated
troubleshooting models from the previous tests (see
Figure 7).
=

By using this secondary representation the speed-up
is less apparent. However, if we allow multiple faults
then this representation can not be used. Moreover,
a troubleshooting model allowing multiple faults will
in general not be simpler than a model with no con­
straints on the number of faults. In the case of ROB­
DDs, assume that the single fault assumption still ap­
plies to the system variables and consider the case
where exactly m components can fail simultaneously;
m is generally "small" . In this situation the number
of nodes in the layers containing system nodes does
not change but the number of nodes in the layers con­
taining cause nodes do: there can be a distinct path
for each configuration of the cause nodes so the num-

Figure 7: A plot of the number of operations per­
formed by ROBDD propagation and Hugin propaga­
tion with a single cause node.
ber of nodes in the layers containing cause nodes is
at most IUcl(l�1). Hence the size of the ROBBD is
O(IUsl2 + IUcl(l�1)); note that in an ROBDD there
does not exist two nodes having isomorphic subgraphs
so the size of the ROBDD is usually much smaller.
Now, as the complexity of propagation in an ROBDD
is linear in its size, the maximum number of operations
performed for m = 2 increases by a factor of n2l ; with
m faults the maximum number of operations increases
i. This corresponds to adding a
by a factor of
constant value to the ROBDD plots in Figure 6 since
we use a logarithmic scale on the y-axis.

n��!=

Furthermore, if we redefine the m-faults assump­
tion to cover at most m faults then the number of
nodes in the layers containing cause nodes is at most
IUcl L�l euicl). Again, it should be noticed that the
actual number of nodes is usually significantly smaller
as isomorphic subgraphs are collapsed.
In case m-faults is extended to include system vari­
ables also, it can be shown that the variables can be
ordered s.t. the number of nodes in the layers contain­
ing system nodes is exponential in m but quadratic in
the number of system variables if m � maxsEUs Ins/
(see Figure 8).
Finally, as the single fault assumption no longer ap­
plies, the number of configurations consistent with
Ct =y and evidence y is given by:
Cards(Ct =y , y)

=

v(c· )-

L 2#\iy,
Ci ECi li E.Ci
�
L

where Ct is the set of nodes Ct with an outgoing 1-arc,
Lt is the set of distinct paths from the Ct in question
to the terminal node and # lt is the number of arcs on
such a path.
Having multiple faults also supports other frame-

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

435

be ordered s.t. the size of the ROBDD is quadratic in
the size of the domain.

20

40
.so
60
70
Numb:rof•y..emvuiable•inlhed.omaoll

Figure 8: The number of nodes in the layers containing
system nodes as a function of the number of system
variables.
works
like
[de Kleer and Williams, 1987]
and
[Williams and Nayak, 1996]. For instance, in cir­
cuit diagnosis [de Kleer and Williams, 1987] uses a
logical model of the system to be diagnosed and
determines the next action based on expected Shan­
non entropy. To calculate the expected Shannon
entropy they require the conditional probability of
a set of failed components (termed a candidate in
[de Kleer and Williams, 1987]) given some observa­
tion. As their framework does not yield an easy way to
obtain this probability they use an approximation. In
our framework the logical circuits can be represented
as ROBDDs which makes the necessary probabilities
easily available.
So far we have not established a practical upper
bound on the size of ROBDDs with m faults, but
all the examples we have worked with until now have
been of a "small" size. Moreover, several heuristic
methods have been devised for finding a good order­
ing of the variables (see e.g. [Malik et al., 1988] and
(Fujita et al., 1988]).
9

CONCLUSION

When modelling the behavior of man-made machinery
using Bayesian networks it frequently happens that a
large part of the model is deterministic. In this pa­
per we have reduced the task of belief updating in
the deterministic part of such models to the task of
calculating the number of configurations satisfying a
Boolean function. In particular, we have exploited
that a Boolean function can be represented by an
ROBDD, and in this particular framework the number
of satisfying configurations can be calculated in time
linear in the size of the ROBDD.
The use of ROBDDs for belief updating was exempli­
fied in the context of troubleshooting, which is partic­
ular well-suited as it was shown that the variables can

The performance of ROBDD propagation was com­
pared with Shafer-Shenoy and Hugin propagation us­
ing randomly generated troubleshooting models. The
results showed a substantial speed-up and it was
argued that the single-fault assumption, underlying
troubleshooting models, can be weakened without sig­
nificantly affecting the performance of the algorithm
in case the number of faults is "small" .



This paper deals with the representation
and solution of asymmetric Bayesian decision
problems. We present a formal framework,
termed asymmetric influence diagrams, that
is based on the influence diagram and al­
lows an efficient representation of asymmet­
ric decision problems. As opposed to exist­
ing frameworks, the asymmetric influence di­
agram primarily encodes asymmetry at the
qualitative level and it can therefore be read
directly from the model.
We give an algorithm for solving asymmetric
influence diagrams. The algorithm initially
decomposes the asymmetric decision problem
into a structure of symmetric subproblems
organized as a tree. A solution to the de­
cision problem can then be found by propa­
gating from the leaves towards the root using
existing evaluation methods to solve the sub­
problems.

1

INTRODUCTION

The power of an influence diagram, both as an analysis
tool and a communication tool, lies in its ability to con­
cisely and precisely describe the structure of a decision
problem[Smith et al., 1993]. However, influence dia­
grams can not efficiently represent the so-called asym­
metric decision problems; decision problems are usu­
ally asymmetric in the sense that the set of possible
outcomes of a chance variable may vary depending on
the conditioning states, and the set of legitimate deci­
sion options of a decision variable may vary depending
on the different information states [Qi et al., 1994].
Various frameworks have been proposed as alterna­
tives to the influence diagram when dealing with asym­
metric decision problems. [Covaliu and Oliver, 1995]

extends the influence diagram with another diagram,
termed a sequential decision diagram, which describes
the asymmetric structure of the problem as comple­
mentary to the influence diagram which is used for
specifying the probability model. [Smith et al., 1993]
introduces the notion of distribution trees within the
framework of influence diagrams. The use of distribu­
tion trees allows the possible outcomes of an observa­
tion to be specified, as well as the legitimate decision
options of a decision variable. However, as the distri­
bution trees are not part of the influence diagram, the
structure of the decision problem can not be deduced
directly from the model. Moreover, the sequence of
decisions and observations is predetermined, i.e., pre­
vious observations and decisions can not influence the
temporal order of future observations and decisions.
Finally, distribution trees have a tendency of creating
large conditionals during the evaluation since they en­
code both numeric information and information about
asymmetry. To overcome this problem [Shenoy, 2000]
presents the asymmetric valuation network as an ex­
tension of the valuation network for modelling sym­
metric decision problems[Shenoy, 1992]. The asym­
metric valuation network uses indicator functions to
encode asymmetry, thereby separating it from the
numeric information. However, asymmetry is still
not represented directly in the model and, as in
[Smith et al., 1993], the sequence of observations and
decisions is predetermined.1
In this paper we present the asymmetric influ­
ence diagram which is a framework for represent­
ing asymmetric decision problems. The asymmet­
ric influence diagram is based on the partial influ­
ence diagram[Nielsen and Jensen, 1999b], and encodes
structural asymmetry at the qualitative level; struc­
tural asymmetry has to do with the occurrence of vari­
ables in different scenarios as opposed to functional
asymmetry which has to do with the possible out1Further details and comparisons of these methods can
be found in [Bielza and Shenoy, 1999].

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

comes/decision options of the variables. As a mod­
elling language, the syntactical rules of the asymmet­
ric influence diagram allow decision problems to be
described in an easy and concise manner. Further­
more, its semantic specification supports an efficient
evaluation algorithm.
An outline of this paper is as follows. In Section 2
we describe the partial influence diagram, together
with the terms and notation used throughout this pa­
per. In Section 3 we formally introduce the asym­
metric influence diagram and illustrate this framework
by modelling a highly asymmetric decision problem
termed "the dating problem". Finally, in Section 4 we
present an algorithm for solving asymmetric influence
diagrams.
2

PRELIMINARIES

The partial influence diagram (PID) was defined in
[Nielsen and Jensen, 1999b] as an influence diagram
(ID) with only a partial temporal order over the deci­
sion nodes. That is, a PID is a directed acyclic graph
I = (U, £), where the nodes U can be partitioned into
three disjoint subsets; chance nodes Uc, decision nodes
U0 and value nodes Uv. The chance nodes (drawn as
circles) correspond to chance variables, and represent
events which are not under the direct control of the de­
cision maker. The decision nodes (drawn as squares)
correspond to decision variables and represent actions
under the direct control of the decision maker. We will
use the concept of node and variable interchangeably
if this does not introduce any inconsistency, and we
assume that no barren nodes are specified by the PID
since they have no impact on the decisions.2
With each chance variable and decision variable X we
associate a finite discrete state space W x which de­
notes the set of possible outcomes/decision options for
X. For a set U' of variables we define the state space
as Wu, = x{WxiXE U'}.
The set of value nodes (drawn as diamonds) defines a
set of utility potentials with the restriction that value
nodes have no descendants. Each utility potential in­
dicates the local utility for a given configuration of the
variables in its domain; the domain of a utility poten­
tialljJx, for a value nodeX, is denoted dom(ljJx) =nx,
where nx is the immediate predecessors of X. The to­
tal utility is the sum or the product of the local utilities
(see [Tatman and Shachter, 1990]); in the remainder
of this paper we assume that the total utility is the
sum of the local utilities.
2 A chance node or a decision node is said to be barren if
it does not precede any other node, or if all its descendants
are barren.

417

The uncertainty associated with a variable X E Uc
is represented by a conditional probability potential
<Px = P(XInx) : W xunx --t [0; 1]. The domain
of a conditional probability potential <Px is denoted
dom(<J:>xl ={X}unx.
The arcs in a PID can be partitioned into three dis­
joint subsets, corresponding to the type of node they
go into. Arcs into value nodes represent functional
dependencies by indicating the domain of the associ­
ated utility potential. Arcs into chance nodes, denoted
dependency arcs, represent probabilistic dependencies,
whereas arcs into decision nodes, denoted informa­
tional arcs, imply information precedence; if DE U0
and (X ,D) E £ then the state of X is known when
decision D is made.
The set of informational arcs induces a partial order
-< on Uc U U0 as defined by the transitive closure of
the following relation:
•

Y-< Di, if (Y,Dd is a directed arc in I (DiE Uo ).

•

Di -< Y, if (Di,X,,Xz, . . . ,Xm,Y) is a directed
path in I (Y E Uc UU0 and DiE Uo).

•

Di -< A, if A -/< Dj for all Di E U0 (AE Uc and
DiE Uo).

•

Di -< A, if A -/< Di and :3Di E U0 s.t. Di -< Di
and A-< Di (AE Uc and DiE Uo).

In what follows we say that two different nodes X and
Y are incompatible if X -/< Y and Y -/< X.
We define a realization of a PID I as an attachment
of potentials to the appropriate variables in I, i.e., the
chance nodes are associated with conditional probabil­
ity potentials and the value nodes are associated with
utility potentials. So, a realization specifies the quan­
titative part of the model whereas the PID constitutes
the qualitative part.
Evaluating a PID amounts to computing a strategy
for the decisions involved. A strategy can be seen
as a prescription of responses to earlier observations
and decisions, and it is usually performed according
to the maximum expected utility principle; the max­
imum expected utility principle states that we shall
always choose a decision option that maximizes the
expected utility. However, the strategy for a deci­
sion variable may depend on the variables observed
thus, we define an admissible total order for a PID I
to describe the relative temporal order of incompati­
ble variables: an admissible total order is a bijection
13 : Uo UUc H {1, 2, ... , IUo U Ucl} s.t. if X -< Y then
I3(X)< /3(Y), where -< is the partial order induced by
I. In what follows < will be used to denote the total
ordering 13 s.t. X< Y if P.,(X)< P.,(Y). Notice, that an

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

418

admissible total ordering of a PID I implies that I can
be seen as an ID.
Given an admissible total ordering <, we define an
admissible strategy relative to< as a set of functions
�<
{&[310 E Uo }, where &[3 is a decision function
given by:
=

and pred(D)< ={XIX< D} (the index< in pred(D)<
will be omitted if this does not introduce any confu­
sion). Given a realization of a PID I, we term an ad­
missible strategy relative to<, an admissible optimal
strategy relative to< if the strategy maximizes the ex­
pected utility for I; two admissible optimal strategies
are said to be identical if they yield the same expected
utility. A decision function &[3, contained in an ad­
missible optimal strategy relative to <, is said to be
an optimal strategy for D relative to <. Note that
an optimal strategy for a decision variable D relative
to< does not necessarily depend on all the variables
observed. Hence, we say that an observed variable
X is required for D w.r.t. < if there is a realization
of I s.t. the optimal strategy for D relative to< is a
non-constant function over X. By this we mean that
there exists a configuration y over dom(S[3)\{X} and
two states x1 and xz of X s.t. S[3(x1, y) -1- S[3(xz, y).

Definition 1 . A realization of a PID I is said to define
a decision problem if all admissible optimal strategies
for I are identical. A PID is said to define a decision
problem if all its realizations define a decision problem.
The above definition characterizes the class of PIDs
which can be considered welldefined since the set of
admissible total orderings for a PID I corresponds to
the set of legal elimination sequences for I. However,
it also conveys the problem of only having a partial
temporal ordering of the decision variables; the rela­
tive temporal order of a chance variable (eliminated
by summation) and a decision variable (eliminated by
maximization) may vary under different admissible or­
derings and summation and maximization does not in
general commute. So, in order to determine whether
or not a PID defines a decision problem we introduce
the notion of a significant chance variable.

Definition 2. Let I be a PID and let A be a chance
variable incompatible with a decision variable D in I.
Then A is said to be significant for D if there is a
realization and an admissible total order< for I s.t. :
•

•

A occurs immediately before D under<.
The optimal strategy for D relative to< is differ­
ent from the one achieved by permuting A and D
in<.

Based on the above definition we have the following
theorem which characterizes the constraints necessary
and sufficient for a PID to define a decision problem.

Theorem 1 ([Nielsen and Jensen, 1999b]). The
PID I defines a decision problem if and only if for
each decision variable D there does not exist a chance
variable A significant for D.
See [Nielsen and Jensen, 1999b] for a structural char­
acterization of the chance variables being significant
for a given decision variable.
3

ASYMMETRIC INFLUENCE
DIAGRAMS

[Qi et a!., 1994] states that decision problems are usu­
ally asymmetric in the sense that the set of possible
outcomes of a chance variable may vary depending
on the conditioning states, and the set of legitimate
decision options of a decision variable may vary de­
pending on the different information states. Equiva­
lently, [Bielza and Shenoy, 1999] characterizes a deci­
sion problem as being asymmetric if, in its decision tree
representation, the number of scenarios is less than
the cardinality of the Cartesian product of the state
spaces of all chance and decision variables. However,
both of these characterizations fail to recognize deci­
sion problems in which the relative temporal order of
two variables vary w.r.t. to previous observations and
decisions; this is for example very common for trou­
bleshooting problems. Thus, we define an asymmetric
decision problem as follows:

Definition 3. A decision problem is said to be asym­
metric if, in its decision tree representation, either:
•

the number of scenarios is less than the cardinality
of the Cartesian product of the state spaces of all
chance and decision variables or

•

there exists two scenarios in which the relative
temporal order of two variables differ.

In order to deal with such asymmetric decision prob­
lems we introduce the asymmetric influence diagram
(AID). An AID is a labeled directed graph I
(U, £,F), where the nodes U can be partitioned into
four disjoint subsets; test-decision nodes (UT), action­
decision nodes (UA), chance nodes (Uc) and value
nodes (Uv) ; we will sometimes omit the distinction
between test-decisions and action-decisions by simply
referring to a node in Uo = UT UUA as a decision node.
=

The chance nodes and value nodes are similar to the
chance nodes and value nodes in a PID. The deci­
sion nodes correspond to decision variables and rep­
resent actions under the direct control of the decision

419

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

maker. A test-decision (drawn as a triangle) is a de­
cision to look for more evidence, whereas an action­
decision (drawn as a rectangle) is a decision to change
the state of the world.
The arcs £ in an AID can be partitioned into four dis­
joint subsets. An arc into a value node or a chance
node is semanticly defined as in the PID framework
if, in case of the latter, it does not emanate from a
test-decision node. Arcs into decision nodes, termed
informational arcs, imply a possible information prece­
dence; if there is an arc from a node X to a decision
node 0 then the state of X may be known when de­
cision 0 is made. This redefinition is needed since
we deal with asymmetric decision problems, i.e., the
set of variables observed immediately before decision
0 is taken may dependent on previous decisions and
observations.
If there exists an arc, termed a test arc, from a test­
decision node 0 to a chance node X, then the state of
0 determines whether or not X is eventually observed;
having an arc from a test-decision node to a chance
node represents a logical relation and does not imply
probabilistic dependence. Note that in the trivial case,
where X is observed no matter the state of 0, the arc
(0, X) implies information precedence only. If there
exists an arc (D, 0 1) from a test-decision node 0 to
another decision node 01, then (0, 01) implies infor­
mation precedence ((0 , 0 1) is an informational arc)
however, ( 0, 0 1) is termed a test arc if the state of
0 determines whether or not 0 1 is eventually decided
upon; whether we are referring to an informational arc
or a test arc is conveyed by the label associated with
0 1; in the remainder of this paper we let I denote the
graph obtained from I by removing all test arcs and
informational arcs.
The asymmetry of a decision problem is graphically
represented in the AID by a set of restriction arcs and
by a set of labels :F. The set of restriction arcs (drawn
as dashed arcs) is a subset of the informational arcs.
A restriction arc (X, 0) indicates that the set of legit­
imate decision options for 0 may vary depending on
the state of X, in which case we say that X is restrictive
w.r.t. 0 (or X is restricting 0). The set of labels :F
is associated with a subset of the nodes and informa­
tional arcs. A label specifies under which conditions
the associated node or informational arc occurs in the
decision problem. The following rules informally sum­
marize the semantics of labels when specifying asym­
metry in the AID; in the remainder of this section they
will be referred to as rule (i)-(iii), respectively:
i) Let X be a node labelled with fx and let Y be
the variables observed before X is observed (or
decided upon). If fx is unsatisfied w.r.t. the state
configuration Y
y observed, then X is not in=

eluded in the scenario.
ii) Let (X, 0) be an informational arc labeled with
f(X,DJ and let Y be the variables observed be­
fore X is observed (or decided upon). If f(x,o) is
unsatisfied w.r.t. the state configuration Y
y
observed, then (X, D) is not included in the sce­
nario.
=

iii) If there exists a directed path from a node X to
a node Y in I, then whenever X is not included
in the scenario Y is not included in the scenario
either.
Based on the rules above we require that if the label
of a node Z is a function of a node X, then there must
exist an arc from X to Z (See Figure 1).

0--0-0
fz(X)

��
�
fz(X)

Figure 1: There must exist a directed arc from X to Z
since fz is a function of X.
1. [The dating problem] Joe has to de­
cide whether or not to ask a girl he has recently met
on a date. If Joe decides not to ask her out he can
choose either to stay at home and watch TV or visit
a night club; before taking that decision Joe observes
what programs will be on TV that night. The plea­
sure of staying at home is influenced by his liking of
the program watched, whereas the pleasure of going to
a night club is dependent on the comfort of going to
that night club and the entrance fee; comfort is depen­
dent on whether Joe likes the night club and whether
he meets any friends there.

Example

If Joe decides to ask her out her response will depend
on her feelings towards him. If she declines the date,
Joe can decide to go to a night club or stay at home
and watch TV; we assume that the two "staying at
home scenarios" are the same. If she accepts to go on
a date with him, Joe will ask her whether she wants
to go to a restaurant or to the movies. The choice of
movie (decided by Joe) may influence her mood which
in turn may influence Joe's satisfaction concerning the
evening. Similarly, the choice of menu (decided by
Joe) might influence Joe's satisfaction.
This decision problem is represented by the AID de­
picted in Figure 2. The variable Date? is represented
as a test-decision since it has no impact on the value
of Accept?. In the evaluation of whether or not to ask
for a date, the distribution of Accept? is relevant and
Accept? is therefore always part of the entire decision
problem.
The decision Night Club? is decided upon if Joe ini-

420

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Figure 3: The dating problem if Date?=n.

Figure 2: An AID representing "the dating problem".
tially chooses not to ask the girl for a date; the label as­
sociated with Night Club? specifies a logical-or and is
therefore satisfied by the state configuration Date?=n.
If Joe chooses to go to a night club the chance vari­
ables Meet Friends and Likes Club influence Joe's com­
fort which in turn influences the pleasure of going to
that club; the chance variable Liking is excluded from
the decision problem since Liking is only included if
Club?=n (rule (i)). Notice that this property could
also be modelled by introducing a redundant state in
the variable Liking. However, having redundant states
tends to obscure the asymmetric structure of the de­
cision problem, and is in general computationally de­
manding.

Satisfaction which has the mutually exclusive variables
Menu and Movie as predecessors. As no descendant of
an excluded variable can be included (rule (iii)), we
would unintentionally exclude Satisfaction whenever
Menu or Movie are included. This problem can be
solved by duplicating Satisfaction and its descendants
or by adding an extra state (no-decision) to the vari­
ables Menu and Movie. In order to minimize redun­
dancy in the representation we have chosen the latter.
D

3.1

THE QUALITATIVE LEVEL

Now, from rule (iii) we can infer the following syn­
tactical simplification: If there exists a directed path
from a node X to a node Y in I, then Y "inherits"
the label associated with X, i.e., Y is "effectively" la­
beled with fx I\ fv, where fx and fv are the labels
explicitly associated with X and Y, respectively. This
means that we need not explicitly associate Y with
the label fx I\ fv (see Figure 4a). The set of vari­
ables from which a chance variable X "inherits" labels
is given by dep(X)
Y, where Y is the set of vari­
ables from which there exists a directed path to X in
I; to ensure consistency it should be noted that de­
cision nodes, value nodes and informational arcs "in­
herit" labels from the empty set. E.g. in Figure 2
the variable Meet Friends is "effectively" conditioned
on (Night club?=y I\ (Accept?=n V Date?=n)) since
dep (Meet Friends) Night club?.
=

Since Date?=n the informational arcs from Accept?
are excluded (rule (ii)) meaning that her potential re­
sponse is never observed; as previously mentioned, Ac­
cept? is still part of the decision problem as opposed
to e.g. Liking if Club?=y. Now, as Accept? is never
observed the variables only labeled by the state of Ac­
cept? are removed (rule (i)), together with all their
successors (rule (iii)). The resulting decision problem
is depicted in Figure 3; the variables Accept and Likes
me? are included in the figure for the purpose of the
AID being a tool for communication.
If Joe on the other hand chooses to ask the girl for
a date he will observe her response (Accept?) . If she
declines the invitation he can choose either to go to
a night club or stay at home. If she accepts the in­
vitation the variable To do? will be observed which
can restrict the possible decision options for Menu and
Movie.
There is a small technical problem with the variable

=

0--8
fx
fx /\fy

fx

fy

Figure 4: The figure illustrates the use of labels when
specifying asymmetry.
The AID allows the specification of directed cycles
with the restriction that before any of the nodes in
a cycle are observed the cycle must be "broken", i.e.,
no matter the variables observed there must exist at
least one unsatisfied label associated with a node or an

v"vt:K 1 AINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

informational arc in the cycle. This syntactical con­
straint compensates for the traditional constraint that
the graph should be acyclic. Having cycles in an AID
supports the specification of decisions for which the
temporal order is dependent on previous observations
and decisions. For instance, in Figure 5 decisionDz is
taken beforeD 1 if X = x butD 1 is taken beforeD2 if
X =f. x.

Xfx

Figure 5: The figure illustrates the use of cycles in the
AID.

Formally, a label is a Boolean function defined as a
combination of Boolean variables, the constants true
( 1) and false (0) and the operators 1\ (conjunction),
V (disjunction),--. (negation),=} (implication) and�
(bi-implication).
The Boolean variables are used to represent the con­
ditioning on states e.g. if a node Y is conditioned on
X = x then X = x should be represented as a Boolean
variable in the label associated with Y. However, for
ease of notation we shall use e.g. X = x directly in the
label (without creating an actual Boolean variable); a
Boolean variable in the context of labels must there­
fore denote a state configuration of some node in the
AID.
A truth assignment to a Boolean function f is the same
as fixing a set of variables in the domain of f, i.e., if
X = x represents a Boolean variable in the domain
of f, then X = x can be assigned either true or false
by associating X with some state x' E Wx (denoted
f[X H x']). E.g. (X = x)[X H x'] = 1 if x = x' and
(X =x) [X H x'] = 0 otherwise.
[n the remainder of this paper, we assume that each
10de X is associated with a label fx, i.e., if X is not
.ssociated with a label in I = (U, £,F), then we extend
=-with the label fx = 1. Moreover, we will use dom(f)
>denote the domain of the label f; the domain of a
bel is the set of nodes referenced by the label.
chance variable X is said to be present in I given
c:onfiguration c over a set of variables C, if ( fx 1\
'Edep(X) fy )[C H c] = 1 in I given c. A chance vari­
e X is said to be unresolved in I given a configura1 c over a set of variables C if dom(fx [C H c]) =f. 0
fx[C H c] '/:. i(Vi E {0, 1}), or 3Y E dep(X) s.t.
l(fv[C H c]) =f. 0 and fv[C H c] t i(Vi E {0, 1}).

2000

421

The concepts present and unresolved are similarly de­
fined for value nodes, decision nodes and informational
arcs.

3.2

THE QUANTITATIVE LEVEL

A decision variable D is associated with a set of re­
strictive functions; a restrictive function is given by
y0 : Wn0 y 2Wo and specifies the legitimate deci­
sion options forD given a configuration of n0 � n�,
where n� denotes the set of variables which can re­
strict the legitimate decision options forD. In Figure 2
the restrictive function associated with Movie specifies
that the state no-decision is the only legitimate deci­
sion option if To do?=restaurant (similar for Menu if
To do?=movie).
The uncertainty associated with a chance variable X
is represented by a partial conditional probability po­
tential tPx = P(XIInx) : Wxun� Y [0; 1], where
nx = nx \UT; by definition, a test-decision variable
has no probabilistic influence on a chance variable. A
partial probability potential can specify that given a
configuration of the conditioning set for a chance vari­
able X, some states of X are impossible (denoted .L);
we make a conceptual distinction between an impos­
sible state and a state having zero probability. Note
that if all the states of X are impossible for some con­
figuration of the conditioning set, then this must be
reflected in the labeling of X e.g. if P(XIY) is only de­
fined for Y = 1J, then Y = 1J must occur in the label of
X.
A value node X is associated with a partial utility po­
tential1\>x : Wnx Y JR+ U {0}; requiring that the par­
tial utility potential does not take on negative values
is not an actual restriction as any utility potential can
be transformed s.t. it adheres to this assumption. Fur­
thermore, as for the PID we assume that the total
utility is the sum of the local utilities.
The combination of partial potentials (addition, multi­
plication and division) is defined similarly to the com­
bination of total functions by treating the undefined
value (.L) as an additive identity and a multiplica­
tive zero. In particular, this ensures consistency when
defining the total utility as being the sum of the local
utilities.
As for the PID we define a realization of an AID as
an attachment of probability and utility potentials to
the appropriate variables. The probability and utility
potentials associated with an AID I is denoted ll>r and
'l'r, respectively. Given a realization ll>r U'Jlr of an AID
I the set of probability potentials with X E X in the
domain will be denoted ll>x, i.e., ll>x = {<P E ll>ri3X E
X: X E dom(<jl)}.

422

3.3

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

SPLIT CONFIGURATIONS AND
DECISIONS IN CONTEXT

iii)

Definition 4. Let I be an AID and let :F be the set

of labels associated with I. A variable X is said to be
a split variable in I if there exists a label f E :F s.t.
X E dom(f). The set of split variables in I is denoted

Sr.

Now, the partial order--( induced by an AID I is found
by initially treating I as a PID (ignoring any labels)
and then refining the partial order --( 1 induced by the
PID s.t. for any pair of variables X andY, where X f. 1Y
andY f.' X we have X--(Y if X E Sr (see Figure 6); if
{X, Y} £; Sr we have X f.Y andY f. X.

(b)

Figure 6: In figure (a) the AID induces the partial
order X --( Y --( D, and in figure (b) the AID induces
the partial order X --(Y --( D --( 0 1•
In what follows we assume that an AID has exactly
one split variableS � (termed the initial split variable)
satisfying that WE Sr\{S�}: S� --( Y; Date? is the
initial split variable in the AID depicted in Figure 2.
Obviously, the set of variables succeeding the initial
split variable s� is dependent of the state of s� hence,
we define the concept of a missing variable.

Definition 5. Let I be an AID and let S� be the
initial split variable in I. The chance variable X is said
to be missing in I given s� S] if:
=

i)

fx[S� H s,] = 0 or

ii) 3Y E dep(X) s.t. y is missing givens �

=

S]

:

X

--(

S

and X is unresolved given

=

When taking a decision in an asymmetric decision
problem, previous observations and decisions may de­
termine the variables observed before the decision in
question. For both semantic and computational rea­
sons it is important to identify the variables actually
observed before taking a particular decision. For in­
stance, in the AID depicted in Figure 2 it would not be
meaningful to have an optimal strategy for Menu con­
ditioned on both Date?=n and Accept?=y since this
is an impossible state configuration. So, in order to
reason about the different informational states when
taking a decision D we must associate D with a con­
text describing the variables observed. That is, we
need to identify the possible temporal orderings of the
variables and in particular, the variables which influ­
ence the occurrence of future variables.

<•l

VS E Sr \{S�}
s� s,.

or

The above definition is easily adopted to value nodes,
decision nodes and informational arcs and will there­
fore not be described further.
The following definition specifies the AID obtained
from another AID I by instantiating the initial split
variable in I.
6. Let I
(U, £,:F) be an AID and let
be the initial split variable in I. The AID I I
(U1, £',:F1) is said to be myopicly reduced from I given
s� = s1 if:

Definition

=

s�

=

•

U' ={X E UIX is not missing givenS � = sJ}.

•

£'

•

{(X,Y)
E
£1{X, Y}
(X,Y) is not missing in I given s�

C

=

S] }.

U' 1\

:F' = {fx[S� H s1]1fx E :F and X E U'}
{f(x,v)(S� H SJ]if(X,Yl E :F and (X,Y) E £'}.

U

That is, we myopicly reduce an AID I by removing
the missing nodes and the missing arcs. However, the
removal of arcs might render additional nodes missing
thus, for all missing nodes and arcs to be removed we
need to remove them iteratively. The AID I' obtained
from I by iteratively removing missing nodes and arcs
is said to be reduced from I given s� = S] and is
denoted I[S� H s,].
Figure 3 illustrates the AID which has been reduced
from the AID in Figure 2 given Date?=n. Notice that
reducing an AID w.r.t. its initial split variable S� is
the same as instantiating s� hence, s� is not a split
variable in I[S� H s,].
In the remainder of this paper we restrict our atten­
tion to AIDs having exactly one initial split variable.
This restriction also applies to AIDs which have been
reduced from other AIDs that is, we do not consider
AIDs which can be reduced to an AID that does not
adhere to this restriction; having a unique initial split
variable ensures that the reduction is unambiguous
and it does not seem to exclude any natural decision
problems; actually, decision trees have the same prop­
erty. Furthermore, for ease of notation we shall treaJ
restrictive variables as split variables unless stated oth
erwise (Sf denotes the union of Sr and the set of H
strictive variables in 1). However, we do not requir
the occurrence of a unique restrictive variable as tf
order in which the restrictive variables are instantiat(
is of no importance, i.e., the syntactical constraint 1
the split variables does not extend to the restricti
variables.
Now, based on the requirement about a unique SJ
variable in an AID I we can identify the possible st

423

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

configurations in I. These configurations are found by
going in the temporal order specified by I. That is, we
iteratively identify the initial split variable in the AID
reduced from the AID in the previous step and assign
this split variable a configuration consistent with the
previous one.

The set of contexts in which a decision variable 0 can
occur is denoted Oo.

The initial split variable S� in I is identified as de­
scribed previously. In general the k 'th split variable in
I w.r.t. the configurations= (s1,sz,... ,sk-d, de­
noted S�k-1, is the initial split variable in I[S� H
s1](S�1 H Sz]·· · (S�;_\ H Sk-1], where S�i-1 is
the initial split variable in I w.r.t. the configuration
Si-1 (s1,sz, ... ,S-t-1 ) . Obviously, if I ' has been re­
duced from I w.r.t. s� =51 then 51 must be a possible
state for S�, i.e., each time an AID is reduced the pos­
sible outcomes/decision options for the split variables
are "updated" . In what follows we let I[S�i-1 H sd
.
1
denote the AID I[SV' H s1][S521 H sz]· ·· [S1i_1 H sd.

Solving an AID is the same as determining an optimal
strategy for the decisions involved. However, as op­
posed to the PID we can not restrict our attention to
the variables being required for the decision variable in
question; the variables being required for 0 may vary
depending on the context in which 0 appears. Thus
we define a strategy as follows:

=

Definition 7. Let

C
Sf and S{'
subsets of the split vari­
ables contained in the AID I.
A configuration
s = (s1,sz,... ,st) over the variables S{' is said to be
a split configuration for S{ over S{' if:

{S1,Sz, ... ,St}

�

S{
S{ be

•

Si is the i'th split variable in I w.r.t. the configu­
ration S-t-1 = (s1,sz, ... ,S-t-11 and

•

S is not
S{\Sf'

a split variable in

1[5�1_1

H

sr], 'r/5 E

If S{ = Sf thens is said to be an exhaustive split
configuration for I over S{'.
For notational convenience, we will sometimes use
to denote the AID reduced from the AID I
w.r.t. the split configuration s over the variables S{'.

I[S{' H s]

Example 2. Consider the AID depicted in Figure 2.
The configuration (Date?=y) is a split configura­
tion, whereas (Date?=y,Accept?=y, To do ?=movie) is
an exhaustive split configuration. The configuration
(Date?=n,Club?=n) is an exhaustive split configura­
tion also since Date?=n implies that Accept? is never
observed, thereby rendering the variables Movie, Menu
D
and To Do? missing.
Based on the notion of a split configuration we can de­
termine the contexts in which a decision variable can
occur. A context for a decision variable 0 is a config­
urations
( w, x), where:
=

•

s is a split configuration for S{ over the variables
S{' satisfying that there does not exist a split vari­
able S E Sr[S!'Hs] s.t. S -< 0 in I [S{' H s].

•

x is a configuration over the restrictive variables
for 0 in I [S{' Hs].

4

SOLVING ASYMMETRIC
INFLUENCE DIAGRAMS

Definition 8. A strategy for an AID I is a set of func­
tions .1 ={&olD E Uo}, where bo is a decision func­
tion given by:
bo : Wpred(D)s

......j

Wo, Vs E Oo,

where pred(D ls is the set of variables preceding 0 un­
der the partial order induced by the AID which has
been reduced from I w.r.t.s.
A strategy that maximizes the expected utility is
termed an optimal strategy, and a decision function
60 that maximizes the expected utility for decision
0 w.r.t. each context s E Oo is termed an optimal
strategy for 0.
A well-defined AID (specified in the following section)
can in principle be solved by unfolding it into a de­
cision tree, and then use the "average-out and fold­
back" algorithm on that tree; the partial probability
potentials specified by the realization of the AID can
be seen as a model of the uncertainty associated with
the chance variables (this is somewhat similar to the
approach found in [Call and Miller, 1990]). However,
this "brute force" approach would create an unneces­
sary large decision tree in case the original decision
problem specifies symmetric subproblems.

4.1

DECOMPOSING ASYMMETRIC
INFLUENCE DIAGRAMS

In this section we present an algorithm for solving
AIDs. The main idea underlying the algorithm is
to decompose the decision problem into a collection
of symmetric subproblems organized in a tree struc­
ture, and then propagate from the leaves towards the
root using existing evaluation methods to solve the
"smaller" symmetric subproblems.
The decomposition is performed by reducing the AID
w.r.t. the possible states of its initial split variable.
This reduction is then applied iteratively to the AIDs
produced in the previous step until no split variables
remain. For instance, Date? is the initial split variable

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

424

in "the dating problem" depicted in Figure 2 so by
decomposing the problem w.r.t. Date?=no we obtain
the AID depicted in Figure 3, where the initial split
variable is Night Club?. An optimal strategy can then
be found by iteratively eliminating the so-called free
variables in each of the subproblems:

Definition 9. Let I[S�

H s,] be the AID reduced
I. The variable X is said to be free in
S� -< X and VS E Sr[S��-+s!l : X -< S,
where-< is the partial order induced by I[S� H s,]. If
X is not free in I[S� H s,] then X is said to be bound
in I[S� H s1].

from the AID
I[S� H s,] if

The evaluation of an AID I is initiated by invoking
the algorithm Evaluation on I; note that in the follow­
ing algorithms we exploit that instantiating the initial
split variable in an AID produces another AID with a
unique initial split variable.

Algorithm 1 (Evaluation). Let I be an AID and let

S�

be the initial split variable in
invoked on I, then
i) Invoke Evaluation on

I[S�

ii) Absorb the potentials from

W51 .

H

I.

If Evaluation is

s,], Vs1 E W51

I[S�

v

H

s,]

to

of probability potentials and utility potentials ob­
tained.

E /\, remove i\ from I[S�. H Sj] and
associate <Dj[S�-+s] U'l'j[So-+s] with I[S�; 1 Hsj],for

iii) For each i\

J

1

alll � j � i.

The algorithm below describes the elimination of
variables, and is inspired by the lazy evaluation
architecture [Madsen and Jensen, 1999] . However, any
elimination algorithm can in principle be used.

Algorithm 3 (Elimination). Let I be an AID and
let <D I and '¥I be the sets of probability and utility po­
tentials associated with I. If Elimination of variable X
is invoked on <D I U 'l'I, then:

<Dx {cl:> E <l>IIX E dom(cl:>)}
'l'IIX E dom(1!>)}.

i) Set

=

and

'l'x

=

{1!>

E

ii) Calculate:

•

I, Vs1 E

v

where M is a marginalization operator depending
on the type of X, i.e., M denotes a summation if
X is a chance variable and a maximization if X is
a decision variable.

iii) Let 11> be a utility potential absorbed (Algorithm 2}
from I[S� H s�] s.t. S� rf. dom(1!>). If :Jj -# i
s. t 11> is not absorbed from I [S� H S�] to I then
condition 11> on S s� (see Figure 7).
=

iii) Return <l>i
}
{

*f

Figure 7: The occurrence of 0 ' is dependent on
state of S thus, the utility potential produced by
elimination of 0 , A and 0 ' is dependent on S;
relative temporal order of A and 0 vary w.r.t.
state of S.

the
the
the
the

Algorithm 2 (Absorption). Let I[S1t_1

H Si] be
an AID and let S be the initial split variable in
I[S1i-1 H sil· If Absorption is invoked on I[S1i _1 H
Si][S H s] from I[S1t_1 H si], then:

X be the free variables I[S1i_1 H Si][S H
s] and set 1\
{i\ E <Drrs�i-1 �-+sd[S�-+sl U
'l'rrs�i-1�-+stHS�-+sli:JX EX s.t. X E dom(i\)}.

i) Let

=

ii) Eliminate the variables X from 1\ w.r.t. the par­
tial order induced by I[St_1 H si][S H s] {Al­
gorithm 3}. Let <I>i[So-+s] and '1'irs�-+sl be the sets

=

<l>r\<l>x U {cl:>x}

and

'l'i

=

'l'I\'l'x U

During the evaluation, the decision option maximizing
the utility potential from which a decision variable 0 is
eliminated should be recorded as the optimal strategy
for 0 w.r.t. to the context in question.
Now, based on the algorithms above we define the con­
cept of a well-defined AID. The definition is based
on the notion of a significant chance variable, which
can be adopted from the PID framework by consider­
ing the admissible total orderings of the free variables
in each of the AIDs produced during the decompo­
sition; the structural characterization of the chance
variables being significant for a given decision variable
( see [Nielsen and Jensen, 1999b]) can also be adopted
to an AID I, except that we have to investigate each
of the AIDs reduced from I w.r.t. the exhaustive split
configurations for I.

Definition 10 (Well-defined). An AID I is said to
define a decision scenario if:
•

for all split configurations s, there does not exist a
free chance variable A and a free decision variable

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

0 in I [S{' H s] s.t. A is significant for 0; s is a
configuration over the variables S{' � Sf
•

for any decision variable 0 and for each context
s ( w, x) for 0 there does not exist two restric­
tive functions Y b and Yb s.t. dom(y b ) � n�ls
=

and dom(yb ) � n�ls' where n�ls is the set of
restrictive variables which are present in I given

s.

Theorem 2 (Sound). If Algorithm 1 is invoked on
an AID I which define a decision scenario, then Algo­
rithm 1 computes an optimal strategy for each decision
variable in I .
Proof. The idea of the proof is t o initially treat non­
split variables as split variables, thereby obtaining a
decision tree representation of the decision problem
when reducing the AID; each subproblem contains ex­
actly one free variable which corresponds to a node in
the decision tree. Note that from Algorithm 1 we have
that treating non-split variables as split variables has
no impact on the evaluation.
Finally, we exploit that the set of partial probability
potentials constitutes a model of the uncertainty as­
sociated with the chance variables, and from this it
can be shown that the calculations performed by Al­
gorithm 1 are equivalent to the calculations peformed
when solving the corresponding decision tree. For fur­
D
ther detail see [Nielsen and Jensen, 1999a].
5

CONCLUSION

In this paper we have presented a framework, termed
asymmetric influence diagrams, for representing asym­
metric decision problems. The asymmetric influence
diagram is based on the partial influence diagram and
uses labels, associated with nodes and informational
arcs, to encode structural asymmetry at the qualita­
tive level. Asymmetry which deals with the possible
outcomes of an observation or the legitimate decision
options of a decision variable is represented in partial
probability potentials and restrictive functions, respec­
tively.
We have presented an algorithm for solving asymmet­
ric influence diagrams. The algorithm decomposes the
asymmetric decision problem into a collection of sym­
metric subproblems which can be solved using existing
methods for solving influence diagrams.
As part of the future work, the class of asymmetric de­
cision problems which can be modeled effectively using
AIDs needs to be determined. We claim that the lan­
guage of AIDs is as strong as that of decision trees,
but the amount of redundancy in the models should
be determined.

425



Influence diagrams serve as a powerful tool
for modelling symmetric decision problems.
When solving an influence diagram we de­
termine a set of strategies for the decisions
involved. A strategy for a decision variable
is in principle a function over its past. How­
ever, some of the past may be irrelevant for
the decision, and for computational reasons it
is important not to deal with redundant vari­
ables in the strategies. We show that current
methods (e.g. the Decision Bayes-ball algo­
rithm [Shachter, 1998]) do not determine the
relevant past, and we present a complete al­
gorithm.
Actually, this paper takes a more general out­
set: When formulating a decision scenario as
an influence diagram, a linear temporal or­
dering of the decisions variables is required.
This constraint ensures that the decision sce­
nario is welldefined. However, the structure
of a decision scenario often yields certain de­
cisions conditionally independent, and it is
therefore unnecessary to impose a linear tem­
poral ordering on the decisions. In this paper
we deal with partial influence diagrams i.e.
influence diagrams with only a partial tem­
poral ordering specified. We present a set of
conditions which are necessary and sufficient
to ensure that a partial influence diagram is
welldefined. These conditions are used as a
basis for the construction of an algorithm for
determining whether or not a partial influ­
ence diagram is welldefined.
1

INTRODUCTION

Graphical modelling for decision support systems is
getting more and more widespread. F irst of all, graph-

ical modelling is an appealing way to think of and com­
municate on the underlying structure of the domain in
question, but it also helps the modeller to focus on
structure rather than calculations. I nfluence diagmms
(ID) serve as a powerful modelling tool for symmet­
ric decision problems with several decisions. However,
IDs require a linear temporal ordering of the decisions,
and this is often felt as an unnecessary constraint.
E.g. if no information is gathered between two deci­
sions, then these decisions can be taken independently
of each other. This type of very obvious temporal in­
dependence can be handled by a computer system, but
temporal independence may be more complicated. For
example, observing a variable A immediately before
taking a decision D need not have any impact on this
particular decision. Hence, we look for an operational
characterization of temporal independence in IDs.
The advantages of having an operational charac­
terization of temporal independence are twofold.
To take the most obvious advantage first. When
a computer system solves an ID it basicly elimi­
nates the variables in reverse temporal order (see
[Shachter, 1986], [Shenoy, 1992], [Jensen et al., 1994]
and [Zhang, 1998]); eliminating a variable produces
a table (function) over all non-eliminated neighbours.
However, the reverse temporal order of elimination has
a tendency to create very large tables (usually much
larger than for Bayesian networks of the same com­
plexity). Thus, if we could relax the temporal order
to a partial order, we would have more freedom when
looking for a good elimination sequence. To say it an­
other way. When a decision variable D is eliminated
we create a strategy for D given its past. If we can
reduce the past to contain only the variables required
for taking that decision, we have reduced the domain
for the strategy function.
The second advantage has to do with the modelling
process. That is, will we allow two decisions to be
taken independently of each other? Or, do we allow
an observation to be made independently of a certain

Welldefined Decision Scenarios

decision? Hence, we work with partial specifications of
IDs, and would therefore like to know whether or not
this partial structure is ambiguous, and if it is ambigu­
ous we would like the system to give suggestions for
further specification of the temporal order. An unam­
biguous partial influence diagram is said to represent
a welldefined decision scenario.
·
In this paper we give a set of operational rules for
determining whether or not a partial influence diagram
represents a welldefined decision scenario. These rules
are used as a basis for an algorithm for answering this
question. The algorithm can furthermore be used in a
dialogue between computer and user to pinpoint how
to change the model in order to make it unambiguous.
In section 2 we formally introduce IDs, and the terms
and notations used throughout this paper. In section 3
we define a partial ID as a generalization of the tradi­
tional ID, and we give a semantic as well as a syntactic
characterization of conditions ensuring that a partial
influence diagram is unambiguous.

2

INFLUENCE DIAGRAMS

An ID can be seen as a belief network augmented
with decision variables and utility functions. Thus,
the nodes in the ID can be partitioned into three dis­
joint subsets; chance nodes, decision nodes and value
nodes.
The chance nodes (drawn as circles) correspond to
chance variables, and represent events which are not
under the direct control of the decision maker. The
decision nodes (drawn as squares) correspond to deci­
sion variables and represent actions under the direct
control of the decision maker. In the remainder of
this section we assume a total ordering of the decision
nodes, indicating the order in which the decisions are
made. 1 Furthermore, we will use the concept of node
and variable interchangeably if this does not introduce
any inconsistency, and we assume that no barren nodes
are specified by the ID since they have no impact on
the decisions.2
The set of value nodes (drawn as diamonds) defines a
set of utility functions, indicating the local utility for
a given configuration of the variables in their domain.
The total utility is the sum or the product of the local
utilities; in the remainder of this paper we assume that
the total utility is the sum of the local utilities.
1 The ordering of the decision nodes is traditionally rep­
resented by a directed path which includes all decision
nodes.

2 A chance node or a decision node is said to be barren if
it does not precede any other node, or if all its descendants
are barren.

503

With each chance variable and decision variable X we
associate a state space Wx which denotes the set of
possible outcomes/decision alternatives for X. For a
set U' of variables we define the state space as Wu' =
x{WxiX E U'}.
The uncertainty associated with each chance variable
A is represented by a conditional probability function
P(A IPA) : WAuPA -t [0; 1], where PA denotes the
immediate predecessors of A.
The arcs in an ID can be partitioned into three dis­
joint subsets, corresponding to the type of node they
go into. Arcs into value nodes represent functional
dependencies by indicating the domain of the associ­
ated utility function. Arcs into chance nodes, denoted
dependency arcs, represent probabilistic dependencies,
whereas arcs into decision nodes, denoted informa­
tional arcs, imply information precedence; if there is
an arc from a node X to a decision node D then the
state of X is known when decision D is made.
Let Uc be the set of chance variables and let Un =
{D1,D2,. . . ,Dn} be the set of decision variables. As­
suming that the decision variables are ordered by
index, the set of informational arcs induces a par­
titioning of Uc into a collection of disjoint subsets
C0, C1, . . . ,Cn. The set Ci denotes the chance vari­
ables observed between decision Dj and Dj+! thus,
the variables in Cj occur as immediate predecessors of
Dj+!· This induces a partial order-< on U = Uc UUn
i.e. Co -< D1-< C1 -< · · -< Dn -< Cn
·

The set of variables known to the decision maker when
deciding on D j is called the informational predeces­
sors of Dj and is denoted pred(Dj). Assuming "no­
forgetting" the set pred(Dj) corresponds to the set of
variables that occur before Di under -<. Moreover,
based on the "no-forgetting" assumption we can as­
sume that an ID does not specify any redundant no­
forgetting arcs i.e. a chance node can be an immediate
predecessor of at most one decision node.
2.1

EVALUATION

When evaluating an ID we identify a strategy for the
decision variables; a strategy can be seen as a prescrip­
tion of responses to earlier observations and decisions.
The evaluation is usually performed according to the
maximum expected utility principle, which states that
we should always choose an alternative that maximizes
the expected utility.
'I be an ID and let Un denote the
decision variables in 'I. A strategy is a set of functions
� = {6nD
i E Un}, where 6n is a decision function
given by:

Definition 1. Let

tln: Wpred(D)

-t

Wn,

504

Nielsen and Jensen

A strategy that maximizes the expected utility is
termed an optimal strategy.
In general, the optimal strategy for a decision variable
Dk in an ID I is given by:

8v.(Co,D1,... ,Ck-1)
v. �
c.

3.1

=

arg max "' P(CkiCo,D1,... ,Ck-1,Dk)Pv•+•

(1)

where pv.+t is the maximum expected utility function
for decision Dk+l:

PD•+•(Co,D1,··· ,Ck) =
max L P(Ck+!ICo,D1,... , ck,Dk+l)P Dk+,
D1o+1
C�o+l

By continuously expanding Equation 1, we get the fol­
lowing expression for the optimal strategy for Dk:

where 1/J1,. . . 1/Jl are the utility functions specified by

I.

The expression above conveys that the variables are
to be eliminated w.r.t. an elimination sequence which
is consistent with the partial order, and in what fol­
lows we define a legal elimination sequence as a bijec­
tion a : U B {1, 2, . . . , lUI}, where X -< Y implies
a(X) < a(Y). Note that a legal elimination sequence
is not necessarily unique, since the chance variables
in the sets Ci can be commuted. Even so, any two
legal elimination sequences result in the same optimal
strategy since the decision variables are ordered totally
and 'I:' operations commute; the total ordering of the
decision variables ensures that the relative elimination
order for any pair of variables of opposite type is in­
variant under the legal elimination sequences (this is
needed since a 'max' operation and a 'I:' operation
do not commute in general).
3

seen from the optimal strategy for a decision variable
(equation 1), where the elimination order for any two
adjacent variables of opposite type may be permuted
if the variables do not occur in the same function.

REPRESENTING DECISION
PROBLEMS UNAMBIGUOUSLY

In the section above we described the evaluation of
an ID w.r.t. the maximum expected utility principle.
The underlying assumption was a total ordering of the
decision variables ensuring that the optimal strategy
is independent of the legal elimination sequences.
However, it is in general not necessary to have a to­
tal ordering of the decision variables (if ck = 0 then
Dk and Dk+1 can be commuted). This can also be

PARTIAL INFLUENCE DIAGRAMS
AND DECISION SCENARIOS

Given that a total ordering of the decision variables
may not be needed, we define a partial influence dia­
gram (PID) as a directed acyclic graph consisting of
decision nodes, chance nodes and value nodes, assum­
ing that value nodes have no children. Notice, that
chance nodes may have several decision nodes as im­
mediate successors, and that no ordering is imposed on
the decisions. Additionally, we define a realization of a
PID as an attachment of functions to the appropriate
variables i.e. the chance variables are associated with
conditional probability functions and the value nodes
are associated with utility functions.
Since the semantics of a PID correspond to the seman­
tics of an ID, a PID induces a partial order -< on the
nodes Uc U Uv, as defined by the transitive closure of
the following relation (see figure 1):
•

Y-< D;, if (Y, D;) is a directed arc ini (D; E Uv).

•

D;

•

D;
D;

•

-< Y, if (D;,X 1,X2,... ,Xm, Y) is a directed
path in I (Y E Uc U Uv and D; E Uv).
-<

A, if A f< Dj for all Di E Uv (A E Uc and

E

Uv).

D; -< A, if A f< D; and �Di E Uv s.t. D; -< Di
and A-< Di (A E Uc and D; E Uv).

In what follows we say that two different nodes X and
Y in a PID I are incompatible if X f< Y and Y f< X.
Note that a chance node A is incompatible with a de­
cision node D if there exists a decision node D' s.t.
D and D' are incompatible and (A, D') is an informa­
tional arc in I and A¢ pred(D)(see figure 1).
As for the traditional ID we seek to identify an opti­
mal strategy when evaluating a PID. Since the optimal
strategy for a decision variable may be dependent on
variables observed, we define a total order< for a PID.
Definition 2. Let I be a PID and let U = Uv U Uc
denote the set of decision variables and chance vari­
ables contained in I. A total ordering of I is a bijec­
tion j3 : U B {1, 2, . . . , lUI}. A total ordering of I is
said to be an admissible total ordering if X -< Y im­
plies that j3(X) < j3(Y), where -< is the partial order
induced by I.

In what follows <(J will denote the total ordering j3
s.t. X <(! Y if j3(X) < j3(Y) (the index j3 will be

Welldefined Decision Scenarios

505

The above definition characterizes the class of PIDs
which can be considered welldefined, since the set of
admissible total orderings for an PID I can be seen
as the legal elimination sequences for I (Note that the
traditional ID defines a decision scenario). Moreover,
in correspondence with the permutations of chance
variables in any legal elimination sequence for an ID,
we define the following relation for any admissible total
order.
Figure 1: The figure represents a PID which specifies
the partial order {B}-<D1-< {E, F,G,D2,D4}-< C4,
{B} -< D1-< {E,F} -<D2-< C4, {B} -< D1 -< {G} -<
D4 -< C4 and D3 -< C4 (C4 denotes the chance vari­
ables observed (possibly never) after deciding on all
the decisions). Thus, D2, D3 and D4 are pairwise in­
compatible, whereasD1 andD4 are not. Furthermore,
it can be seen that F is incompatible withD4.
omitted if this does not introduce any confusion). E.g.
E < F < D2 < B < D1 < G < D4 < D3 < C4 is
a total ordering of the PID in figure 1, but it is not
admissible since it contradictsD1 -<D2.
Notice, that an admissible total ordering of a PID I
implies that I can be seen as an ID (assuming that
redundant no-forgetting arcs have been removed).
Based on the informational predecessors for a decision
variable, we define a strategy relative to a total order
<, as a set of functions �< = {6.l3!D E Un}, where 613
is a decision function given by:

813: Wpred(D)<

--+

Wn,

where pred( D)< = {XIX < D} ( the index < in
pred(D)< will be omitted if this does not introduce
any confusion). Given a realization of a PID I, we
term a strategy relative to<, an optimal strategy rela­
tive to< if the strategy maximizes the expected utility.
Likewise we term a decision function 8.!3 contained in
an optimal strategy relative to <, an optimal strategy
forD relative to<. Note that an optimal strategy for
a decision variable D relative to < does not necessar­
ily depend on all the variables observed. Hence we say
that an observed variable X is required for D w.r.t.<
if there is a realization of I s.t. the optimal strategy
forD relative to < is dependent on the state of X.
Since a total order for a PID need not be admissible,
we define an admissible optimal strategy for a realiza­
tion of a PID as an optimal strategy relative to an
admissible total order.
Definition 3. A realization of a PID I is said to de­

fine a decision scenario if all admissible optimal strate­
gies for I are identical. A PID is said to define a de­
cision scenario if all its realizations define a decision
scenario.

Definition 4. Let< be an admissible total order, and

let X and Y be two neighbouring variables under <,
fulfilling one of the following three conditions:
•

X and Y are both chance variables.

•

X and Y are both decision variables.

•

X and Y are incompatible.

The ordering <1 obtained from < by permuting X
and Y according to the rules above is said to be C­
equivalent with<, denoted< =c <1•
1. The transitive closure of
equivalence relation.

Proposition

=c

is an

Theorem 1. All admissible orderings of a partial or­

der -< are C-equivalent.

Proof. It is sufficient to prove the following claim: Let
< be an admissible total ordering, and let X and Y be
incompatible s.t. X< Y. Then the ordering obtained
from < by permuting X and Y is C-equivalent with
<.
Assume the claim not to be true. Then there exists an
admissible total ordering< and a pair of incompatible
variables X,Y which can not be permuted. Let X
and Y be such that the segment between X and Y
under< is minimal. That is, it is not possible to find
any other admissible total order with an incompatible
non-permutable pair of variables closer than X and Y
under<:
X< X1 < < Xn < Y
·

·

·

Now, start with X and follow< until we reach an in­
compatible variable X;; we know that at least when
we reach Y we will meet an incompatible variable. If
X; = Y then Y and X;-1 can be permuted, and we
have an admissible ordering with an incompatible non­
permutable pair closer than the closest. If i :S n then
X and X; are incompatible. If they can not be per­
muted we have a pair of incompatible non-permutable
variables closer than the closest. If they can be per­
0
muted we also obtain a closer pair.
So, we are looking for a set of necessary and sufficient
conditions ensuring that all admissible orderings yield

506

Nielsen and Jensen

the same set of strategies. Actually, we will look for
conditions ensuring that orderings, C-equivalent with
an admissible ordering, yield the same strategies; this
is a bit broader as we allow permutation of two neigh­
bouring decision variables. From Theorem 1 we in­
fer that we can narrow down the scope to neighbour­
ing variables of opposite type (in general neighbour­
ing variables of opposite type can not be permuted
without affecting the strategies). Hence, we look for a
necessary and sufficient set of conditions granting com­
mutation of two incompatible neighbouring variables
of opposite type.
Definition 5. Let I be a PID and let A be a chance
variable incompatible with a decision variable D in I.
Then A is said to be significant for D if there is a
realization and an admissible total order< for I s.t.
•
•

A occurs immediately before D under<.
The optimal strategy for D relative to< is differ­
ent from the one achieved by permuting A and D
in<.

A chance variable is said to be significant for D relative
to< if the above conditions are satisfied w.r.t.<.
Notice that if a chance variable A is significant for D
w.r.t.< then A is required for D w.r.t.<.
Based on the above definitions we present the following
theorem which characterizes the constraints necessary
and sufficient for a PID to define a decision scenario.
Theorem 2. The PID I defines a decision scenario if
and only if for each decision variable D there does not
exist a chance variable A significant for D.

Proof. Follows immediately from Theorem 1 and Def­
inition 5.
0
So, we have reduced the task to the following: Let I
be a PID, and let A be a chance variable incompatible
with a decision variable D. Is A significant for D?
[Shachter, 1998) presents an algorithm for determining
the so called requisite information for a decision vari­
able in an ID. Unfortunately, the algorithm does not
meet our needs as shown by the following example.
Example 1. When running the algorithm Decision

Bayes-ball[Shachter, 1998) on the ID depicted in fig­
ure 2, the chance variable B is marked as requisite for
decision D1. However, B is not relevant for the opti­
mal strategy for D1, i.e. the elimination order of B
relative to D1 is of no importance when considering
the optimal strategy for D1.
0

Figure 2: The algorithm Decision Bayes-ball marks
the chance variable B as requisite for D1.
The following method, which corresponds to itera,.
tively replacing decisions by their strategies, has the
same drawback. For an ID we start off with the moral
graph i.e. informational arcs are removed, undirected
arcs are added between nodes with a common child
and finally, value nodes are removed together with the
directions on the arcs. When eliminating a decision
variable D the resulting set of neighbours N(D) is a
subset of pred(D). This set of neighbours is invariant
w.r.t. the legal elimination sequences, and it is charac­
terized as the set of variables connected in the moral
graph to D through a path with no intermediate vari­
able in pred(D). As N(D) contains all the information
relevant for determining the optimal strategy for D, it
is a candidate for the relevant past. However N(D)
may contain variables insignificant for D as can be
seen from the ID depicted in Figure 2; B is contained
in the neighbouring set for D1 as the elimination of A
produces a fill-in between B and D1.
So, neither Decision Bayes-ball nor the elimination
method presented above is fine-grained enough to de­
tect all independencies. The problem is that N(D)
may contain variables relevant for the maximum ex­
pected utility for D; the maximum expected utility
for D may cover utility functions having no influence
on the optimal strategy for D. This means that we
need to characterize and identify the utility functions
on which the optimal strategy for D depends.
Definition 6. The utility function '1/J is relevant for
D w.r.t. the admissible total order < for I, if there
exists two realizations R1 and R2 of I who only differ
on '1/J s.t. the optimal strategies for D relative to< are
different in R1 and R2.
We need to determine the structural constraints neces­
sary and sufficient for a utility function to be relevant
for a decision variable, and based on this character­
ization we shall define the constraints necessary and
sufficient for a chance variable to be significant for a
given decision variable.
3.2

RELEVANT UTILITY FUNCTIONS­
EXAMPLES AND RULES

The optimal strategy for a decision variable D is based
on the assumption that we always adhere to the max­
imum expected utility principle. Hence, if deciding on
D can influence a future decision D' then the utility

Welldefined Decision Scenarios

functions relevant for D' may be relevant for D also.
From this observation together with the expression for
the optimal strategy for D (see equation 1), we present
the following metarules. For notational convenience
we shall sometimes treat uninstantiated decision nodes
as chance nodes with an even prior distribution. More­
over, since a utility function is termed relevant w.r.t.
an admissible total order we will mainly consider IDs
in the section.
Metarule 1. 1/J is relevant for D in I if there is a real­
ization of I s.t. D has an impact on the expected
utility for 7/J.
Metarule 2. 1/J is relevant for D in I if there is a

realization of I and a future decision D' s.t. D
has an impact on D', for which 1/J is relevant.

Metarule 3. If none of the metarules above can be

applied then 1/J is not relevant for D.
The following examples present a set of IDs where we
identify the utility functions relevant for a given de­
cision variable. The properties relating to these ex­
amples will be generalized to arbitrary IDs, which will
serve as a basis for determining the structural con­
straints necessary for a utility function to be relevant
for a given decision variable.
Example 2. Consider the ID depicted in figure 3 and
assume that the conditional probability functions are
specified s.t. the state of a variable corresponds to the
state of its parent.

Figure 3: The figure represents an ID, where the utility
function 1/J is relevant for the decision variable D.
It is easy to specify two realizations of 1/J s.t. the op­
timal strategies relative to those realizations differ i.e.
1/J is relevant for D1.
0
Now, assume an arbitrary ID I in which there exists
a directed path from a decision node D to a value
node 1/J (excluding informational arcs), and assume a
realization R of I. Since the conditional probability
functions associated with the variables on the path
from D to 1/J can be specified s.t. deciding on D has an
impact on the expected utility for 1/J, it follows that 1/J
is relevant for D.
Rule 1. Let I be a PID, and let f denote I without
informational arcs. The utility function 1/J is relevant
for the decision variable D if there exists a directed
path from D to 1/J in I.
This rule is equivalent to Metarule 1, as can be
seen from the mathematical expression correspond­
ing to this metarule: 1/J is relevant for D if

507

Table 1: The utility function 7j;'(C,D4).
10
0

0
9

P(dom(7/J)ID, pred(D)) is a function of D, where
dom( 1/J) is the chance variables in the domain of
'1/J (uninstantiated decision variables are treated as
chance variables). The conditional probability func­
tion P(dom(7/J)Ipred(D), D) is a function of D if D is
d-connected to a variable A E dom(¢) given pred(D).
However, this implies that there exists a directed path
from D to 7/J in f. Conversely, if there exists a di­
rected path from D to 1/J in f, then D is d-connected
to a variable A E dom('I/J) given pred(D).
The following examples illustrate, that in order to
identify all the utility functions relevant for a given
decision variable D, it is in general not sufficient only
to consider those utility functions to which there exist
a directed path from D.
Example 3. When deciding on D4 in the ID depicted

in Figure 4 we want to maximize 7/J'. Now, as the deci­
sion variable D2 is d-connected to C given pred(D4),
it follows that the decision made w.r.t. D2 may change
our belief in C (when deciding on D4) and thereby in­
fluence D4 through 7/J' (D2 is required for D4). Hence
'1/J' is relevant for D2, which is also true for 1/J as
Dz E dom(7j;). Moreover, knowledge of A may like­
wise change our belief in C when deciding on D4, and
since A is influenced by D1 it follows that D 1 has an
impact on D2 since knowledge of D1 can be taken into
account when deciding on D2 (D1 is required for D2).
Thus, 7/J' is relevant for both D1 and D2 conveying
that '1/J is relevant for D1 also.

Figure 4: The figure represents an ID where both 1/J
and '1/J' are relevant for D1.
This can also be seen by considering the utility func­
tion specified by table 1 together with the functions
P(C) = (0, 5; 0, 5), 'I/J1(D2) = (0; 3) and 7f;2(D2) =
(3; 0) (we assume that the state of A corresponds to
the state of D1 and that P(BID2, A, C) is specified s.t.
the state of C is revealed if the state of A, B and D2
are the same, and no knowledge is gained on C if this
is not the case).

508

Nielsen and Jensen

These functions define two realizations of I who only
differ on '1/J, and when evaluating I w.r.t. these real­
izations we obtain two different optimal strategies for
D1 i.e. 8n, = d1 if '1/J = '1/JJ and 8n, = d2 if 'ljJ = 'I/J2·
From these strategies it can be seen that the utility
function 'ljJ influences the optimal strategy for D1 i.e.
0
'1/J is relevant forD1 .
The example above can be seen as an instance of
Metarule 2. Assume an arbitrary ID I and a realiza­
tion of I in which the conditional probability function
for any intermediate variable in a converging connec­
tion is as specified in the example above. From this
structure we may deduce that, if '1/J' is relevant for a
future decision D' andD is required forD', then the
utility function '1/J' is relevant forD; D has an impact
anD'.
Example 4. When deciding onD4 in the ID depicted
in figure 5, we seek to maximize '1/J'. By the arguments
given in the example above, it follows that both '1/J and
'1/J' are relevant for D2. Additionally, knowledge of B
may change our belief in E when deciding onD4, and
since B is influenced by A, which in turn is influenced
by D1 , it follows that D1 has an impact on D2 (A is
required forD2). Thus, both 'ljJ and '1/J' are relevant for
DJ.

Figure 5: The figure represents a ID,in whichD1 may
influence the chance variable A required forD2, indi­
cating that both '1/J and '1/J' are relevant forD1 .
This can also be seen by assuming the two realizations
consisting of the utility functions 'I/J1 D
( 2) = (0; 3) and
?jJ2(D2) = (3; 0) together with functions corresponding
to the ones specified in Example 2 and Example 3.
When evaluating I w.r.t. these realizations we obtain
two optimal strategies which differ on D1 i.e. 8n, =
d1 if '1/J = ?/J1 and 8n, = d2 if 'ljJ = ?/J2- From these
strategies it can be seen that the utility function 'ljJ
may influence the optimal strategy for D1 i.e. 'ljJ is
0
relevant forD1 .
The structural properties relating to the example
above can, as for the previous example, be general­
ized to an arbitrary ID. Thus, based on the example
above and the deductions made w.r.t. Example 3, we
present the following rule.
Rule 2. Let I be a P D
I and let < be an admissible
total order for I. The utility function 'ljJ is relevant

for the decision variable D w. r.t. < if there exists a
decision variableD' s.t.
i) D<D' and 'ljJ is relevant forD' w.r.t. <.
ii) either
a) D is required forD' w.r.t. < or
b) there exists a directed path in f from D to
a chance variable X E pred(D'), and X is
required forD' w.r.t.<.
This rule is equivalent to Metarule 2, since the rule
covers all the cases where D can have an impact on
a future decision D'; D has an impact on D' if and
only ifD is required forD' orD influences a variable
required forD'.
Note that Rule 2 is not a complete structural rule as
it refers to the term "required", which has not yet
been characterized structurally. This is done in the
following section.
3.3

REQUIRED VARIABLES­
EXAMPLES AND RULES

Having established a method to identify the utility
functions relevant for a decision variable, one might
think that the required variables could be identified
in the following way: Before constructing the moral
graph, remove all utility functions not relevant for
D and then eliminate the variables as described in
Section 3.1. However, the resulting neighbouring set
ND
( ) may still contain variables which are not re­
quired as can be seen from Figure 8: if we add the
arc (A,D) and remove the arc (D"','I/J) then A is not
required forD but A E N(D) when D is eliminated.
A variable X is required for a decision variableD if X
is observed beforeD and the state of X may influence
the optimal strategy forD. Since the optimal strategy
forD is dependent on the assumption that we always
adhere to the maximum expected utility principle it
follows that X is required forD if X has an impact on
D or X has an impact on a future decision variableD',
on which D also has an impact. Hence, analogously
to the metarules specifying the utility functions rele­
vant for a decision variable, we present three metarules
concerning the variables required for a given decision
variable; according to the definition of a required vari­
able we assume an admissible total order < where a
variable X occurs before a decision variable D under
<.
Metarule 4. X is required for D if there is a real­

ization s.t. when deciding on D the state of X
has an impact on the expected utility for a utility
function 'ljJ relevant for D w.r.t. <.

Welldefined Decision Scenarios

Metarule 5. X is required for D if there is a realiza­

tion and a future decisionD' s.t.X has an impact
on D', and there exists a utility function 1/J rele­
vant for bothD andD' w.r.t.<.
6.

If none of the metarules above can be
applied then X is not required forD.

Metarule

The following examples present a set of PIDs in which
some of the required variables are identified. The prop­
erties described by these examples will be generalized
to arbitrary PIDs, and they will serve as a basis for
a theorem describing the structural constraints nec­
essary and sufficient for a variable to be required for
a given decision variable. In the examples we assume
that chance variables, with no immediate predecessors,
are given an even prior distribution.

509

in general not sufficient only to consider the variables
which directly influence the decision made w.r.t. D
(see Metarule 5).
Example 6. In the PID I depicted in figure 7 the
chance variable A may be observed before D. More­
over, A is required for D" since 1/J is relevant for D"
and A is d-connected to 1/J given pred(D"). Now, since
1/J is relevant for D also, it follows that ifD<D" then
A andD may both have an impact on D". Addition­
ally, if A is observed prior to D then the state of A
can be taken into account when deciding onD i.e. A
is required forD.

Example 5. Consider the PID I depicted in figure 6.

The utility function 1/J is relevant for D for any admis­
sible total ordering of I, and since 1/J is functionally
dependent on the chance variable A it follows that A
is required forD; as A is incompatible withD there is
an admissible total order< with A<D.

Figure 7: The figure represents a PID in which the
variable A is required forD.
This can also be seen by considering the utility func­
tion specified by Table 2, assuming that P(B[A, C)
has the properties of P(C[D2 , B , E) specified in Ex­
ample 3.

Figure 6: The figure represents a PID in which the
chance variable A is required forD.

Table 2: The utility function 1/J( D , C, D").

0

The example above can be generalized to an arbitrary
PID I, assuming that I contains a decision variableD
and a variable X. If there exists an admissible total
order< s.t. X occurs before D under < and X is d­
connected to a utility function 1/J, relevant for D w.r.t.
<, given pred(D), then X is required for D; we can
specify a realization of I s.t. the state of X has an
impact on the expected utility for 1/J.
Rule 3. Let I be a PD
I and letD be a decision vari­

able in I. The variable X is required for D if there
exists a utility function 1/J relevant forD w.r.t. an ad­
missible total order< s.t. X occurs beforeD under<
and X is d-connected to 1/J given pred(D).
This rule is equivalent to Metarule 4, as can be seen by
expressing the metarule mathematically: X is required
for D if P(dom( 1/J) [D, pred(D)) is a function ofX, and
1/J is a utility function relevant for D. The probability
function P(dom('I/J)[D,pred(D)) is a function of X if
and only if X is d-connected to 1/J given pred(D).
The following examples show, that in order to identify
all the variables required for a decision variable D, it is

The optimal strategy
for D relative to
A,D,D',B,D",C is given by 8n(a1) = d2 and
8n(a2) = d1 which indicate, that D is dependent on
0
the state of A (A is required forD).
In the example above, the conditional probability func­
tion for B is specified s.t. the state of C is revealed if
the state of A corresponds to the state of B, and no
knowledge is gained on C if the state of A does not
correspond to the state of B. Furthermore, the utility
function 1/J relevant for bothD andD" is specified s.t.
the state of C and the decision made w.r.t. D influ­
ences D". Thus, A is required for D" and therefore
required forD also.
Analogously to the previous examples, we may gener­
alize this example by considering an arbitrary PID I
and an admissible total order for I, where a variableX
occurs before a decision variableD and X is required
for a future decision D', which has a relevant utility
function in common with D. By specifying the real­
ization of I according to the example above, it follows

510

Nielsen and Jensen

that the state of X may influence the decision made
w.r.t. D'. Moreover, since D and D' have a relevant
utility function in common and X is required for D'
we have that X is required for D also; the state of X
can be taken into account when deciding on D.
Example 7. Consider the PID I depicted in figure 8.
Observing the chance variable A may reveal the state
of X, and when deciding on D'" the observation of X
may change our belief in the state of C. Now, since
'1/J is relevant for both D and Dm the decision made
w.r.t. D has an impact on D'", and since the state
of A may influence the decision made w.r.t. Dm, and
thereby the optimal strategy for D,it follows that the
state of A is relevant when deciding on D. That is, A
is required for D.

i) there exists a decision variable D'(D< D') s.t. '1/J
is relevant for D' w.r.t. <.
ii) X is required for D' or X is d-connected to a
chance variable Y E pred( D') given pred( D) and
Y is required for D'.
This rule is equivalent to Metarule 5, since the obser­
vation of X can have an impact on a future decision
D' if and only if X is required for D' or X influences
a variable required for D'.
The rules 2 and 4 represent a set of simultaneous recur­
sive structural constraints. The recursion terminates
because it moves forward in the temporal ordering for
each "call".
Based on the rules above we present the following theo­
rem which defines the structural constraints necessary
and sufficient for a variable to be required for a given
decision variable.
Theorem 3. Let I be an PID and let D be a decision

Figure 8: The figure represents a PID in which the
chance variable A is required for D.
This can also be seen by considering a realiza­
tion of I, where P(X)A) is a deterministic func­
tion and '1/J(D,C,Dm) and P(B)X, C) correspond to
'1/J( D , C,D") and P(A) D1 ,B) respectively (see Ex­
ample 3 and Table 2). When evaluating I w.r.t.
A,D,D',X, D",B,Dm,c the optimal strategy for D
is given by <lv( ai) = d2 and <lv( a2) = d1 i.e. A is
D
required for D.
In the example above, the state of X is determined by
the state of A, whereas the state of C is determined by
the state of X and B i.e. as for the previous examples
the state of C is revealed if the state of X corresponds
to the state of B,whereas no knowledge is gained on C
if the state ofX does not correspond to the state of B.
Now, consider an arbitrary PID I, and let X denote
a variable which occurs before the decision variable
D under <, and assume that X is d-connected to a
chance variable Y E pred(D') given pred(D). If Y is
required for D'(D< D') and D' has a relevant utility
function relevant in common with D w.r.t.<,then we
can specify a realization, as described in the example
above, s.t. the optimal strategy for D is dependent on
the state of X. That is, X is required for D.
Rule 4. Let I be a PD
I and let D be a decision vari­
able in I. Then the variable X is required for D if
there exists a utility function '1/J relevant for D w.r.t.
an admissible total order<, whereX occurs before D
and:

variable in I. Then the variableX is required for D if
and only if Rule 3 or Rule 4 (and Rule 1 and Rule 2)
can be applied.

Proof. The "if" part of the proof is apparent from the
examples above. A mathematial proof of the "only if"
part can be performed by closely following the elimi­
nation process when solving an ID. The basic idea is
to postpone the calculations until a maximization is
performed in order to calculate a strategy for a deci­
sion variable. That is, instead of marginalizing out a
chance variable a script is produced, and when maxi­
mizing, the relevant scripts are identified. The details
D
may be found in [Nielsen and Jensen, 1999].
Based on the previous rules we present the following
rule characterizing the chance variables significant for
a given decision variable; this rule is apparent from
Rule 3 and Rule 4.
Rule 5. Let I be an PID and let D be a decision vari­

able in I incompatibel with a chance variable A. Then
A is significant for D if there exists a utility function
'1/J relevant forD w.r.t. an admissible total ordering<,
whereA occurs immediately before D s.t. :
i) A is d-connected to '1/J given pred(D) or
ii) there exists a decision variable D'(D < D') s.t. '1/J
is relevant for D' and:
a) A is required for D' or
b) A is d-connected to a chance variable X E
pred(D') given pred(D) andX is required for
D'.

Welldefined Decision Scenarios

Additionally we have the folllowing corollaries as a
consequence of theorem 3.

Let I be a PID and let D be a decision
variable in I. Then the utility function '1/J is relevant
for D if and only if Rule 1 or Rule 2 can be applied.

Corollary

1.

Corollary 2. Let I be a PID and let D be a decision
variable in I incompatible with the chance variableA.
Then A is significant for D if and only if Rule 5 can
be applied.
Corollary 3. Let I be an ID and let D be a decision

variable in I. Then X is required for D if and only if
Rule 3 or Rule 4 can be applied.

4

ALGORITHMS

511




quacy of the model and the reliability of data used.

After a brief introduction to causal proba­

system comes up with. At least there will be kept

Therefore, no expert will blindly accept what the
bilistic networks and the HUG IN approach,

a critical eye on the data, and mainly one will

I

the problem of conflicting data is discussed.

look for conflicts in the data or conflicts with the

A measure of conflict is defined,

model.

I

MUNIN. Finally it is discussed how to dis­

I
I
I
I
I
I
I
I
I
I

and it

is used in the medical diagnostic system
tinguish between conflicting data and a rare
case.

In this paper we present

a

way of building such

a critical eye into a system with a CPN model.
Our suggestion requires an easy way of calculating
probabilities for specific configurations. We start
with a brief introduction to the HUGIN approach.

1

In section 3 we discuss CPN's and data conflict.

Introduction

In section

4

a measure of conflict is defined, and

it is shown that this measure is easy to calculate
It has for many years been widely recognized that

in HUGIN and that it supports a decomposition

causal probabilistic networks (CPN's), have many

of global conflict into local conflicts.. Section

virtues with respect to expert systems mainly due

reports on experience with a large CPN, and in

5

to the transparency of the knowledge embedded

section 6 we discuss how to distinguish between

and their ability to unify almost all domain knowl­

conflicts in data and data originating from a rare

edge relevant for an expert system (Pearl 1988).

case.

However, the calculation of revised probability dis­
tributions after the arrival of new evidence was
for a long period intractable and therefore an ob-

2

Causal probabilistic Networks

stacle for pursuing these virtues. Theoretical de­

and the HUGIN approach

velopments in the 80ies have overcome this diffi­
culty (Kim and Pearl1983, Lauritzen and Spiegel­

A causal probabilistic network (CPN) is con­

halter 1988, Schachter 1988, Cooper 1984, Shafer

structed over a

universe,

consisting of a set of

With the

states. The
variables. The universe is or­
ganized as a directed acyclic graph. The set of
parents of A is denoted by pa(A) . To each vari­

HUGIN approach efficient methods have been im­

able is attached a conditional probability table for

plemented for calculation of revised probability
distributions for variables in a CPN without di­

P(Ajpa(A)) .
Let V be a set of variables.

rected cycles (Andersen et a.l. 1989).

Cartesian product of the state sets of the elements

the results infered from the model rely on the ade-

tables are considered as functions and they are de-

and Shenoy 1989).

The Lauritzen and Spiegel­

halter method has been further developed to the
HUGIN approach (Andersen et al. 1987, Jensen
et al.

1990a, Jensen et al. 1990b)

.

As always when modelling real world domains,

nodes each node having a finite set of
nodes are called

in

V

and is denoted by

The

Sp(V).

space of V

is the

The probabilitie

547

noted by greek letters </> and 1/J. If A is a variable,
then ¢>A= P(A!pa(A)) maps Sp(pa(A)U{A}) into
the unit interval [0, 1]. It is convenient to consider
functions which are not normalized and take arbi­
trary non-negative values. So in the sequel, <P and
1/J denote such functions.
Evidence can by entered to a CPN in the form
of findings. Usually a finding is a statement, that
a certain variable is in a particular state.
After evidence has been entered to the CPN one
should update the probabilities for the variables in
the CPN. It would be preferable to have a local
method sending messages to neighbours in the net­
work. However, such methods do not exist when
there are multiple paths in the network.
The HUGIN approach which is an extension of
the work of Lauritzen and Spiegelhalter ( 1988)
(Jensen et al 1990a; Jensen et al 1990b) repre­
sents one way of achieving a local propagation
method also for CPN's with multiple paths. This
is done by constructing a so-called junction tree
which represents the same joint probability distri­
bution as the CPN.
The nodes in a junction tree are sets of variables
rather than single variables. Each node V has a
belief table <Pv : Sp(V) - Ro attached to it. The
pair ( V, <Pv) is called a belief universe.
The crucial property of junction trees is that
for any pair ( U, V) of nodes, all nodes on the path
between U and V contain U n V.
A belief table is a (non-normalized) assessment
of joint probabilities for a node. If S C V, then an
(non-normalized) assessment of joint probabilities
for Sp(S) can be obtained from <Pv by marginal­
ization: <Ps = E V\S <Pv
Evidence can be transmitted between belief uni­
verses through the absorption operation: ( U, <Pu)
absorbs from (V, <Pv ) , ... , (W, <Pw) by modifying
<Pu with the functions L: V\S <Pv, . .. , LW\U <Pw.
Actually, the new belief function <Pu is defined as
•

<Pu =

<P'u *

:Evw <Pv

LU\V </>u

*

w
. .. * E ww <P
LU\w <Pu

where the product ¢> * 1/J is defined as
(¢> * 1/J)(x)= ¢>(x),P(x)

Based on the local operation of absorption the two
propagation operations CollectEvidence and Dis­
tributeEvidence are constructed. When CollectEv­
idence in Vis called (from a neighbour W) then V
calls CollectEvidence in all its neighbours (except
W), and when they have finished their CollectEv­
idence, V absorbs from them (see figure 1).

I
I
I
I
I
I

- Direction of �bsorption

� Ca.ll of COLLECT EVIDENCE

Figure 1: The calls and evidence passing in Col­
lectEvidence
When DistributeEvidence is called in V from a
neighbour W then V absorbs from W and calls
DistributeEvidence in all its other neighbours.
Having constructed a junction tree, we need not
be as restrictive with findings as in the case of
CPN's:
Let V be a belief universe in the junction tree.
A finding on V is a function
Fv

:

Actually, the more general notion of likelihood can be

entered: Evidence is a function Ev :
not pursue this in the present paper.

Sp(V)

I
I
I
I
I
I
I

Sp(V)- {0, 1}

So, a finding is a statement that some configu­
rations of Sp(V) are impossible. Note that the
product of two findings f : Sp(V) - {0, 1} and
9 : Sp(W) - {0, 1} is a finding f * 9 : Sp(V U
W) - { 0, 1}, and f* 9 corresponds to the conjun­
cion f 1\ g.
Using the HUGIN approach, it is possible to en­
ter findings to the CPN (or the junction tree)1,
update the probabilities for all variables, and to
1

with ¢> and 1/J extended to the relevant space (if
necessary).

I

- &.

We will

I
I
I
I
I

548

I
I
I
I
I
I
I
I
I
I
I

I
I
I
I

I
I
I
I

Following the tradition in probabilistic reason­
achieve joint probability tables for all sets of vari­
ables which are subsets of nodes in the junction ing to take examples from California, where bur­
tree. The method has proved itself very efficient glary and earthquake are everyday experiences, we
even for fairly large CPN's like MUNIN (see Ole­ have constructed the following example:
sen et al. 1989, Andersen et al. 1989).
When Mr. Holmes is at his office he fre­
The main theorem behind the method is the fol­
quently gets phone calls from his neigh­
lowing.
bour Dr. Watson telling him that his
burglar alarm has gone off, and Mr.
Theorem 1
Holmes rushing home hears on the ra­
dio that there has been an earthquake
Let T be any junction tree over the universe U,
nearby. Knowing that earthquakes have
and let <Pu be the joint probability table for U.
a tendency to cause false alarm, he then
has
returned to his office leaving his
(a) If CollectEvidence is evoked in any node
neighbours
with the pleasure of the noise
V and <Pv is the resulting belief table,
from the alarm. Mr. Holmes has now in­
then <Pv is proportional to LU\ v <Pu.
stalled a seismometer in his house with a
(b) If further, DistributeEvidence is evoked
direct line to the office. The seismometer
in V, then for any node W the result­
has three states:
ing belief table <Pw is proportional to
LU\W ¢U·
0 for no vibrations
0

Before we proceed with data conflict, we will state
an observation proved in Jensen et al. (1990b),
but first noted by Lauritzen and Spiegelhalter
(1988) in their reply to the discussion.

1 for small vibrations (caused by
earthquakes or passing cars.)
2

for larger vibrations (caused by ma­
jor earthquakes or persons walking
around in the house.)

The CPN for this alarm system is shown
in figure 2:

Theorem 2

One afternoon Dr. Watson calls again
Let T be a junction tree with all belief tables nor­
and tells that the alarm has gone off. Mr.
malized, and let x, . . , y be findings with prior
Holmes
checks the seismometer, it is in
joint probability P(x * ... * y). Enter x, . . . , y to
state
0!
T and activate CollectEvidence in any belief uni­
verse for V. Let <l>v be the resulting belief universe
From our knowledge of the CPN, we would say
for V.
that
the two findings are in conflict. Performing
0
Then :Z::::v <l>v = P(x * .. . * y).
an evidence propagation does not disclose that.
The posterior probabilities are given in figure 3.
Only
in the rare situations of inconsistent data, an
CPN's and data conflict
3
evidence propagation will show that something is
A CPN represents a closed world with a finite set wrong. The problem for Mr. Holmes is whether
of variables and causal relations between them. he should believe that the data originate from a
These causal relations are not universal, but re­ rare case covered by the model, or he should reject
flect relations under certain constraints. Take for that.
From a CPN m_pdel's point of view there is no
example a diagnostic system which on the basis of
blood analysis monitors pregnancy. Only diseases difference between a case not covered by the model
relevant for pregnant women are represented in and flawed data. So what we can hope for to pro­
the model. If the blood originates from a man, the vide Mr. Holmes with is a measure indicating pos­
constraints are not satisfied, and the case is not sible conflicts in the data given the CPN.
In MUNIN (Olesen et al. 1989) an attempt to
covered by the model . A similar situation appears
incorporate conflict analysis in the CPN is made.
if the test results are flawed (e. g. red herrings).
.

549

This is done by introducing 'other'-states and
'other'-variables. In the example of Mr. Holmes'
alarm system, an 'other'-variable covering lighten­
ing, flood, baseballs breaking windows etc. could
be introduced to represent unknown causes for the
alarm to go off, and the Burglar variable could
have an 'other'-state covering Mr. Holmes' mis­
tress having forgotten the code for switching off
the burglary alarm.
Though this approach is claimed to be fairly
successful, it raises several problems. First of all
there is a modelling problem. The effect of an
'other'-statement is hard to model without know­
ing what 'other' actually stands for . What should
the conditional probabilities be? In fact, these
Burglary: <I>B : (50, 50); Earthquake: ¢E(90, 10)
probabilities were in MUNIN constructed by feed­
ing the network with conflicting data and thereby
tuning the tables as to make 'other' light up ap­
E
<l>s
propriately.
y
N
A second problem is that conflict in data is a
N (97, 2, 1) ( 1, 97, 2)
global property, and the introduction of 'other'­
B
statements in the CPN gives only a possibility of
y (1, 2, 97) (0, 3, 97)
evaluating
evidence locally. In order to combine
Seismometer
the local 'other' statements to a global one, the
CPN has to be extended drastically.
E
<I>A
This leads to the third major problem, which
N
y
is more of a technical kind. The introduction of
N (99, 1) (1, 99)
'other'-statements to the CPN can cause a dra­
B
matic increase in the size of the junction tree. Be­
y (1, 99) (0, 100)
Alarm
sides, the technique with 'other'-states is hard to
use if the variables are not discrete.
Figure 2: Mr. Holmes' Alarm system with seisAnother approach has been suggested by
mometer.
Habbena ( 1976). It consists of calculating a sur­
prise index for the set of findings. Essentially, the
surprise index off: V --. {0, 1} is the sum of the
probabilities of all findings on V with probabilities
no higher thanf's.
Habbena suggests that a threshold between 1%
and 10% should be realistic. In the seismometer
E
<I>E,B
case, the surprise index for (a, s) is 3%. However,
N
y
the calculation of a surprise index is exponential in
N .47 .05
the number variables in V and must be considered
B
as intractable in general.
y .48 0
Figure 3: Joint probabilities for earthquake and 4
The conflict measure conf
burglary posterior to a : 'alarm = Y' and s : 'Seis­
Our approach to the problem is that correct find­
mometer = 0'.
ings originating from a coherent case covered by
the model should conform to certain expected pat­
terns. If x, · · ·, y are the findings, we therefore

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

550

I
I
I
I
I
I

I
I

should expect:
P(x * ···* y)

>

P(x)

x

·· ·

x

P(y)

Hence we define the conflict measure conf as:

conf(x, ···,y) =log

P(x) X··· XP(y)
P( X*. * y)
•

.

(where log is with base 2).
This means that a positive conf(x, · ··,y) is an in­
dicator of a possible conflict.
For the data in section 3 we have conf(a, s ) =
4.5.
Using theorem 2, conf(x, · ··,y) is very easy
to calculate in HUGIN. The prior probabilities
P(x),···,P(y) are available before the findings
are entered, and P(x,· ··,y) is the ratio between
the prior and the posterior normalizing constant
for any belief universe.

I

P(x. y,

z. u.

consists of two sets of findings, namely {x,y,z}
and { u,v}. Since the product of findings is also a
finding, we can say that the two findings x * y * z
and u * v meet in V.
The conflict in the data meeting in V is therefore
composed of the conflict between x*y*z and u*v
, the conflict inside {x,y,z} and inside {u,v}. It
is easy to show that:
conf(x, y,z,u,v) =conf(x * y * z, u *z)
+conf(x, y,z) + conf(u,v)
Furthermore, as indicated at figure 4, P( x *y * z)
and P(u * v) can be calculated as ratios between
prior and posterior normalizing constants, and
therefore conf(x,y,z) and conf(u,v) as well as
conf(x * y * z, u * z) are easy to calculate.
In general: If evidence is propagated to any belief
universe U from neighbours V, ···, W originating
from findings (v, .. ·v') . . . ( w,·· ·, w') respectively,
then

vl

conf(v,· · ·,v',··· , W , ··· W 1) =

I
I

conf(v * ···* v',· ·, w * · · ·* w')
·

+conf(v,···, v') + · ·· + conf( w, ···, w')
All terms are in HUGIN easy to calculate by use
of Theorem 2.
We call conf(v,· ··, v',···, w, ·· ·, w') the global
conflict and conf(v * ·· ·* v',· ··, w * · · ·* w') the
local conflict.
The calculation of conf has been implemented
in HUGIN to follow the calls of CollectEvidence.
The overhead to the propagation methods m
terms of time and space is neglectable.

P(I. y.z)

I
I
t

I

I
I
I
I

t

y

t
l

t
u

t

v

Figure 4: A junction tree with findings x, y, z, u,v
entered. Theorem 2 provides the joint probabili­
ties indicated at nodes V, U, W' and W".
The conflict analysis can be further refined. In
figure 4 is shown a junction tree with findings
x,y, z, u, v entered. If CollectEvidence is evoked
in the node V, then the evidence flowing to V

5

Example: APB-MUNIN

The conflict measure has been tested on small
fictions examples and on a large subnetwork of
MUNIN, namely the network for the muscle Ab­
ductor Pollicis Brevis (APB). The network is
shown in figure 5.
The rightmost variables in figure 5 are finding
variables. This means that evidence is entered at
the right hand side of the CPN and propagates to
the left. However, as described in section 2, the
propagation takes place in a junction tree of belief
universes. In the test, CollectEvidence was called

551

I
I
I
I
I
I
I

5: The DAG in
ductor Pollicis Brevis.

Figure

MUNIN for

I

Medianus Ab­

The attached numbers in­

entered (see figure 6.)

I

in universe number 59, and the call propagates

I

dicate the belief universe to which the finding is

recursively down the junction tree. In figure 6 is
shown the junction tree.

I

(Only belief universes

where evidence meet are shown).
First we asked the model builder, Steen An­

I

dreassen, to provide us with a complete set of nor­
mal findings. They were entered, and global and
universal conflict values were calculated. The re­
sults are shown in figure
a global conflict of

7.

23.3 for

I

Surprisingly we got
the entire set of find­

ings and apparently the conflict can be traced to

I

belief universe no. 45. Further, the evidence from
15 and 17 looks conflicting.

Returning to Steen

Andreassen with our surprise, he recognized that

I

he had given us a wrong value for the finding
qual.mup.amp. which was entered to belief uni­
verse 15. It should have been

540 J.LV

rather than

200 pV.

We entered the corrected finding and got a
global conflict value

-1.5

for the entire set of find­

ings with local and subglobal values ranging be­
tween

0

and

-1.4.

Then typical findings for a patient suffering
from moderate proximal myopathy were entered.
As can be seen in figure 8, this resulted in large

Figure 6: The part of the MUNIN junction tree
for APB where evidence meet. The numbers are
labels of belief universes. Bold numbers indicate
entrance of findings.

I
I
I
I'

552

I
I

negative conflict values confirming the coherence
of the findings.

I
I
I
I
I
I
I
I
I

Figure

I

I
I
I
I
I

Typical findings for a patient s uffer ing

Finally, we simulated hypothesizing.

We en­

tered a set of findings originating from a healthy
patient, and we also entered the disease state

I
I

8:

from moderate proximal myopathy entered.

'moderate proximal myopathy'.
shown in figure
Figure 7: The conflict measures from the first test
example. The italiced values are local conflict val­

ues and the bold figures are the global ones.

9.

The result is

The disease finding is entered to

belief universe 58, and it can be seen that the dis­
ease does not contradict a couple of normal find­
ings, but indeed the whole set.

6

Conflict or rare case?

It can happen that typical data from a very rare
case might cause a high value of conf. In the case
of Mr. Holmes' alarm system a flood (with proba­

10-3 could be entered to the CPN explaining
the data (see figure 10).
For this system we get conf(a, s) = 4.5. It is

bility

still indicating a possible conflict. The reason is
that though P(a, s) is possible, it is under the

553

rare

Mr.

condition of flood.

of the window.

Holmes looks out

It rains cats and dogs, and he

has resolved the problem; the model gives a

P(Flood )

I

n ew

0.84.

=

The problem above call s for more than a pos­
analysis .

refined conflict

sibility for

We need a

method to point out whether a conflict

can

be ex­

plained away through a rare cause.

(x,.. .,y) be findings with a positive conflict
H be a hypothesis which could
explain the findings: conf ( x, . . . , y, H) < 0
Let

measure, and let

We have

con1'r( x, . . . , y, H) =

log

P(x) x . . . x P(y) x P(H)
P( X* .. ·*Y*H )

=

conf(x, ..., y)+ log

P(H )
P(HIx,...,y )

og

then

P(Hjx,.. . ,y)
P(H)

H can

> conf( x,

explain away the

variables (in

conflict.

t he flood example the

value is 5.6). This means that there is no need
Figure

9:

Findings for a healthy patient, and

the hypothesis 'moderate proximal myopathy' en­
t er ed.

for manually to formulate explaining hy pot hesis
in terms of states of variables. More complex hy

­

pot hesis can also be monitored if they can be ex­

pressed as findings.

7

Conclusion

· · ·

, y)

=

log

P(;�

P( Y)
X* ... * y)
x

·

·

·

x

has many promising properties. It is easy to cal­

culate in HUGIN, it is independent of the order in

which fi nd ings are entered , it can be used for both
global and loc al analysis of conflicts in data, and

it has a natural interpretation which supports the

usual mental way of inspecting data for flaws or
for originating from sources outside the scope of
the current investigation.
Figure 10: Mr. Holmes' revised CPN.

I
I
I

I
I
I
I

The measure o f confli ct

con f ( x,

I

I

. . . , y)

The left-hand ratio can be monitored automat­
ic ally for all

I

I

This means that i f

1

I

However, still some practical and theoretical
work is needed in order to understand the signifi­
cance of specific positive conflict values. Also, the

I
I
I
I
I
I

554

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

detailed conflict analysis a it is nowconnected to Kim, J. H. and Pearl, J. (1983). A computational
the structure of the junction tree rather than to model for causal and diagnostic reasoning in infer­
the CPN itself. This should be relaxed.
ence systems. In Proceedings of the 8th Interna­
tional Joint Conference on Artificial Intelligence.,

8

Acknowledgements

190-193.

Lauritzen, S. L. and Spiegelhalter, D. J. (1988) P.
We thank Steffen Lauritzen for many valuable dis­ Local computations with probabilities on graphi­
cussions on the subjects of this paper, and Steen cal structures and their applications to expert sys­
Andreassen for helping with the MUNIN experi­ tems (with discussion). J. Roy. Statis. Soc. B.
ment.
50, 157-224.
Olesen, K. G., Kjrerulff, U., Jensen, F., Jensen, F.
V., Falck, B., Andreassen, S. and Andersen, S. K.
(1989). A MUNIN network for the median nerve
Andersen, S. K., Jensen, F. V. and Olesen, K. G. - a case study on loops. Applied Artificial Intel­
(1987). The HUGIN core- preliminary consider­
ligence 3, 385-404. Special issue: Towards Causal
ations on systems for fast manipulations of prob­ AI
Models in Practice.
abilities. In Proceedings of Workshop on Induc­

9




•

The paper deals with optimality issues in con­

these labels are called separators (see Figure la);

nection with updating beliefs in networks. We
address two processes: triangulation and con­
struction of junction trees. In the first part,

•

network.

In the second part, we argue that

any exact method based on local calculations
must either be less efficient than the junction
tree method, or it has an optimality problem
equivalent to that of triangulation.

1

attaching a pote ntial to all separators (initially

the neutral potential consisting of ones);

we give a simple algorithm for constructing
an optimal junction tree from a triangulated

giving all links in the junction tree a label con­
sisting of the intersection of the adjacent nodes;

•

letting the nodes communicate via the separa­
tors:

a message from U to V with separator S

has the form that <Pu is marginalized down to S,
resulting in ¢5; <Psis placed on the separator and
¢'(S)/¢(S) is multiplied on ¢v (see Figure 1b).

INTRODUCTION

The junction tree propagation method (Jensen et al.,
1990; Lauritzen and Spiegelhalter, 1988) is designed
for propagation in Markov networks:
•

cjl'(S)!cjl(S)

cjl'(S)

an undirected graph with discrete variables as
nodes;

•

for each clique

U

in the graph there is a poten­

tial <Pu, which is a non-vanishing function from
the set of configurations of

U

to the set of non­

negative reals.

{a)

The compilation part of the method is to
•

FIGURE 1.

triangulate the graph (i.e., add extra links such
that every cycle of length greater than three has
a chord);

•

construct

(a) A junction tree. (b) Message passing in junction
trees.
It is so, that after a finite number of message passes

form a potential <Pu for each c lique

U

of the tri­

angulated graph;
•

(b)

a

junction tree over the cliques.

A junction tree over the cliques is characterized by the

so-called junction tree property: For each pair U, V
of cliques with intersection S, all cliques on the path
between U and

V containS.

The propagation part of the method consists of

between neighbours in the junction tree, each po­
tential in the junction tree holds the (possibly non­
normalized) marginal of the joint probability distribu­
tion for the entire set of variables. In fact, the message
passing can be organized so that it is sufficient with
exactly one pass in each direction of the links in the
junction tree. Therefore, in complexity considerations
for propagation in junction trees, one can associate a
local measure

C(U, V) to

links

(U, V),

where

C(U, V)

indicates time/space consumption for the two passes.

361

Optimal Junction Trees

The compilation is not deterministic. Markov net­
works may have several different triangulations yield­
ing different sets of cliques, and a triangulated network
may have several different junction trees. We therefore
would like to have algorithms yielding optimal trian­
gulations and optimal junction trees with respect to
complexity. However, the optimality problem for tri­
angulations isN'J'-complete (Arnborg et al., 1987).
In the first part of the paper, we address the optimal­
ity problem for junction trees given the triangulated
graph, and we present a simple algorithm which is
quadratic in the number of cliques.
In the last section, we address the triangulation pro­
cess and ask the question whether it may be possible
to come up with a propagation method which does not
contain anN'J'-hard optimality problem. The answer
is discouraging. We show that any local calculation
method must involve a hidden triangulation, and we
use this to conclude that the method is either less ef­
ficient than the junction tree method, or it has an
N'J'-hard optimality problem.
2

JUNCTION TREES AND
MAXIMAL SPANNING TREES

Throughout the remainder of the paper, we consider a

triangulated connected graph G with clique set e. The
cliques of G are denoted b.I the letters U, V, W, ll1,
etc. We shall not distinguish between a clique and
its set of variables. So we talk of the intersection of
cliques meaning the set of variables common to the
cliques. Intersections are denoted by letters R,S, R1,
etc.
Definition 1 The junction graph for G has e as nodes,
and for each pair U, V of cliques with nonempty inter­
section R there is a link with label R. Each link has a
weight which is the number of variables in the label.
Theorem 1 A spanning tree for the junction graph
of G is a junction tree if �nd only if it is a spanning
tree of maximal weight.

Theorem 1 has been proved independently by Shibata
{1988) and Jensen (1988). Here we will give a proof
much simpler than the o:tiginal ones. Before giving the
proof, we shall recall two algorithms for the construc­
tion of maximal spanning trees.

.

.

®-0--cb
FIGURE 2.

Paths in T and T1•

Prim's algorithm constructs a sequence To �
� Tn.
of maximal spanning trees for the subgraph deter­
mined byN.
· ·

·

Algorithm 2 (Kruskal)

Choose successively a link of maximal weight not pro­
ducing a cycle.
Kruskal's algorithm works with a forest of partial max­
imal weight spanning trees. Whenever a link is cho­
sen, two partial trees are connected into a new partial
spanning tree of maximal weight.
Both algorithms result in maximal weight spanning
trees, and each maximal weight spanning tree can
be constructed through any of the two algorithms.
[Proofs can be found in many textbooks on graph
algorithms, e.g., (Goudran and Minoux, 1984) and
(McHugh, 1990)].
Proof of Theorem 1:
Let T be a spanning tree of
maximal weight. Let it be constructed by Prim's al­
gorithm such that T1 �
� Tn =T is a sequence of
partial maximal weight spanning trees.
· ·

·

Assume that T is not a junction tree. Then, at some
stage m, we have that Tm. can be extended to a junc­
tion tree T1 while Tm.+1 cannot. Let (U, V) with la­
belS be the link chosen at this stage; V E Tm.+ 1 (see
Figure 2).
Since Tm.+ 1 cannot be extended to a junction tree, the
link (U, V) is not a link in T1• So, there is a path in T1
between U and V not containing (U, V). This path
must contain a link (U1, V') with labelS' such that
U 1 E T m. and V1 (j_ Tm. (see Figure 2).
Since T 1 is a junction tree, we must haveS �S1, and
sinceS was chosen through Prim's algorithm at this
stage, we also have S
l I;::: 1511. Hence, S =51•

Algorithm 1 (Prim)
(1) Put N {U}, where U is an arbitrary node.

Now, remove the link (U1, V1) from T' and add the
link (U, V). The result is a junction tree extending
Tm.+ 1, contradicting the assumption that it cannot be
extended to a junction tree.

(2) Choose successively a link (W, V) of maximal
weight such that W E N and V (j_ N, and add V
toN.

Next, let T be any non maximal spanning tree. We
shall prove that T is not a junction tree. Again, let
T1 �
� T 1 be a sequence of maximal trees con-

=

-

•

·

·

362

Jensen and Jensen

FIGURE 3. The thinning task at stage

i+ 1

in Kruskal's algorithm.

structed through Prim's algorithm. Let the construc­

trees of maximal weight. Note that any thinning at a

tion be so that a link from

given stage will result in the same connected compo­

ble. Let

m

T is chosen whenever possi­

be the first stage where this is not possible,

and let

(U, V} with separator S be the link actually
chosen (U E Tm• V � Tm). In T there is a path be­
tween U and V. As in the first part of the proof, we
have that this path contains a link [U 1, V 1) with la­
belS' such that U' E Tm and V' � Tm (see Figure 2).
Since (U' , V') could not be chosen, we have S
I 'I < S
l I,
and thereforeS contains variables not inS '. Hence, T
does not satisfy the junction tree condition.

3

1

OPTIMAl JUNCTION TREES

W henever the junction graph has several spanning
trees of maximal weight, there are accordingly several
junction trees. Assume that there is a real-valued mea­

sure on junction trees yielding a priority among them,
and assume that this measure can be decomposed to
a local measure

C(U, V}

call the measure a

cost.

attached to the links.

We

We may also assume that

the entire measure is strictly increasing in the local
measures, and that an optimal junction tree is one of
minimal cost.

nents, and therefore the thinning chosen has no impact
on the next stage.

Hence, if we in the construction

have a secondary priority (cost, say), we can perform
the thinning by using Kruskal's algorithm according
to cost. In this way we will end up with a maximal
weight spanning tree of minimal cost (see Figure

3).

We conclude these considerations with

Theorem

2 Any minimal cost juncti on tree can be
constructed by successively choosing a link of max­
imal weight not introducing cycles, and if several
links may be chosen then a link of minimal cost is
selected.

A proof of Theorem
stages.

2

is an induction proof over the

The induction hypothesis is that at the end

of each stage, the forest consists of partial maximal
distance junction trees.

Remark 1

An analoguous algorithm based on Prim's

algorithm will also construct minimal cost junction
trees.

Let us take a closer look at the construction of junction
trees through Kruskal's algorithm. Let w, , ... , Wn be
the different weights of

G in

decreasing order. The al­

gorithm can be considered as running through n stages

Corollary 1 All juncti on trees over the same triangu­
lated graph have the same separators (also counting
multiplicity).

characterized by the weight of the links chosen. At the
end of stage

i,

all links possible of weight w,

have been chosen, and a forest

T},

. .

.

, T�,

,

... , Wi

of partial

Proof:

Consider stage i+ 1 (Figure

3).

A cycle can be

broken by removing any link of weight Wi+ 1· If

(U, V)

maximal weight spanning trees has been constructed.

with separatorS is removed, then all separators in the

Now, the task at stage

remaining paths between

i+ 1

can be considered in the fol­

lowing way. Add all links of weight Wi+ 1 to the forest,

and break the cycles by removing links of weight Wi+ 1 .

Any thinning will result in a forest of partial spanning

U

and

V

must contain S.

This means that any separator of weight Wi+ 1 on these
paths must equalS. By thinning we therefore remove
the same separators.

1

Optimal Junction Trees

•

363

For each separator, establish links to all cliques
and separators containing it.

•

For each separator (with multiplicity n), choose
n+ 1 links to supersets without introducing cycles.

Theorem 3 Any minimal cost Almond tree can be
constructed by successively choosing links for sepa­
rators of maximal weight, and if several links may
be chosen, take one of minimal complexity.

A proof of Theorem

(b)

(a)

3 is

an induction proof along the

same line as a proof of Theorem

2.

FIGURE 4.

(a) Contraction of the junction tree from Figure 1.

THE NECESSITY OF TRIANGULATION

(b) An Almond tree.

5

4

for constructing optimal junction trees given the tri­

In the former sections we gave an efficient algorithm

ALMOND TREES

angulated graph. Thereby all steps from DAG to junc­

Almond and Kong

(1993) suggest another type of junc­

tion tree. Compared to the junction trees in (Jensen
et al.,

1990),

they give some reduction in computa­

tional complexity.

tion tree is covered by efficient algorithms yielding an
optimal output-except for the triangulation. Since
this problem is N:P-complete, we cannot hope for an
efficient algorithm yielding an optimal triangulation.
It appears that a one-step look-ahead heuristic pro­

Observation 1 If n links have the same separator, the

vides the best triangulations. An alternative propaga­

communication scheme can be contracted (Figure 4a).

tion scheme is conditioning (Pearl,

1988).

The N:P­

complete part of conditioning is the determination of
In junction trees, each separator holds exactly one po­

a cut set for the DAG, and Becker and Geiger

tential table where the marginal last communicated

have given an algorithm which guarantees a cut set

is stored.

space no larger than the square of the space for an

In contracted junction trees, a separator

with n neighbours must hold at least n

-

1

potential

optimal cut set. Other schemes exist, like, e.g., arc­

1990); however, as has been shown
(1991), all known methods do in fact

tables to store marginals communicated from neigh­

reversal (Shachter,

bours. This means that there is no saving in space.

by Shachter et al.

There is, however, a saving in time, since a number of

contain a hidden triangulation.

marginalizations are avoided.

(1994)

Since belief updating in Bayesian networks is N'.P-hard

Observation 2 If a separator is a subset of another sep­

arator, they can be linked (Figure

4b).

(Cooper,

1990),

there is not much hope of finding a

scheme avoiding an N::P-hard step. However, Cooper's
result does not yield that any scheme will contain such
a step.

Cooper showed that through belief updat­

The type of calculations are the same for links between

ing, the satisfiability problem for propositional calcu­

separators as for links between separators and cliques.

lus

can

be solved, but it may still be so that a search

S,

for an optimal structure for belief updating is poly­

the number of supersets to which it shall be linked,
and for each link (S, S'}, we can associate a local cost

nomially solvable. Note namely that the space of the

Due to the corollary, we know for each separator

C(S, S').

Also, new schemes are proposed (Zhang and Poole,

Junction trees simplified through these two observa­
tions we call

cliques are exponential in their presentation.

Almond trees.

The construction of an

Almond tree may go as follows:

1992)

which may seem as if they avoid the triangula­

tion problem. We will in this section argue that

any

scheme for belief updating- meeting certain require­
ments- will contain a hidden triangulation. Then, if

•

From the triangulated graph, the set of cliques

the complexity ordering of the hidden triangulations

and the set of separators (including multiplicity)

follows the ordering in the original scheme, we can con­

This can be done through elim­

dude that if the scheme has a polynomially solvable

ination in the triangulated graph, but it is not

optimality problem, then the junction tree method ei­

important for our considerations.

ther provides more efficient solutions or '.P

is established.

=

N::P.

364

Jensen and Jensen

The considerations to come are somewhat specula­
tive and at places they need further precision. Hence,
we call the results 'statements' rather than theorems.
However, a reader looking for alternative propagation
methods can use them as guidelines preventing inves­
tigations of several alternatives.
FIGURE 5.

Specifications

A graph representing a general propagation task.

U {A, . . . , B} is a universe consisting of a finite set
of discrete variables. The joint probability P(U} is a
distribution over the configurations Xu =Ax · · · x B.
=

A local representation of P(U) consists of a set
{P(U,), ... , P(Un)}, where U, . . . , lin is a covering
of U, and P(U;,} is the marginal distribution of Ui.
A local representation can be visualized by a graph G
with the variables as nodes and with a link between
two variables if there is a Ui containing both; G is
called the representing graph.
The propagation task can be formulated as follows.
Let P 1(Ui) be substituted forP(Uil; ifP1(U} P(U} x
P1(Ui)/P(Ui) is well-defined, then calculate the new
marginals P1(U,), ... , P'(Unl·
=

By a scene for a propagation task, we understand a
universe U together with a covering U1, ... , lin such
that the covering equals the cliques in the representing
graphs. An instance of a propagation task is a pair
(G, P), where G is an undirected graph, and P is a set
of marginals of a joint distribution P(U) to the cliques
of G.
Let U be a universe. By a local method on U, we un­
derstand an algorithm working only on subsets of U.
More precisely: The algorithm consists of a control
structure and a fixed set Pr1, ... , Pr = of proce­
dures such that each Pri only processes information
on Vi c; U. We call Vi the scope of Pri. The repre­
senting graph G1 for a local method is defined as the
graph with U as nodes, and with links between vari­
ables if there is a scope containing them. Notice that
the cliques of G 1 need not be scopes.
We have defined a local method such that the control
structure mainly consists of controlling message pass­
ing between procedures. Note that between Pr;. and
Pri only information on Vi n Vi is worth passing.

First, we shall transform the problem to propositional
calculus.
Lemma 1 Let P(U,), ... , P(Uml be projections of
the joint probability table P(U). Let Pos(U} be the
table of possible configurations of U:

Pos(u} =

{�

if P(u) > 0
otherwise

{�

if P (ud > 0
otherwise

Define Pos(U;.) as:
Pos(u;,}

=

Then Pos(Ui} 1 if and only if Ui is a projection
of a possible configuration.
=

Proof: Since P(U;.) is the marginal of P(U), we have
that P(u;,} > 0 if and only if ut is the projection of at
least one configuration with positive probability.
1
The lemma shows that any scheme for belief updating
has the calculus of possible configurations in proposi­
tional calculus as a special case. So, if we can prove
Statement 1 for this calculus, we are done.
We shall start with an example which is the corner­
stone of the proof.
Example 1 Let the graph in Figure 5 represent a gen­
eral propagation task over the propositional calculus,
and let Pas be the potential giving 1 for possible con­
figurations and 0 for impossible ones.

Let PrAs, PrAc, Prsn, Prnc be procedures for solv­
ing the task {the index indicates the scope, see Fig­
ure 6).

A general local belief updating method for a scene
represented by G is a local method solving the propa­
gation task for each instance (G, P}.

We shall construct an instance which cannot be solved
by the procedures. For each variable we only use the
first two states. This means that all other states are
impossible.

We aim at the following:

Initially, we have for i, j ::; 2

Statement 1

Let G represent a scene, and let a gen­
eral local belief updating method be represented by
the graph G 1• Then G1 contains a triangulation
of G.

Pas( at, bj) 1
Pas(Ut, Cj ) 1
Pos(bi, dj) 1
Pos(ci,dj)=l
=

=

=

for all i, j
if and only if i
if and only if i
for all i, j

=

=

j
j

Optimal Junction Trees

365

Proof of Statement 1:
Assume that G 1 does not
contain a triangulation of G. Then there is a cycle C
in G such that the subgraph of G 1 consisting of the
nodes in C is not triangulated. Let C' be a chordless
cycle of length greater than three in that subgraph.
Let A 1 , ... , An be the nodes of C'.

FIGURE 6.

The scopes for the procedures and the communication
channels.
That is, A and C as well as B and D are forced into
the same state, and everything else is possible. Note
that the Pas-relations above are projections of the Pas­
relation over the universe:

if and only if
Pos ( U>. bi}

=

Pos ( ai, ck}
=

Pos(bj, df)

=

Pos(ck, de)

=

1

Now, assume we get the information that the config­
urations (a1, b2) and (a2, b1) are impossible. This is
equivalent to replacing the relation Pos(ai, bj) by
Pos1(ai, bj)

=

1

if and only if

i

=

j

(and i,j � 2).

Now, the propagation task is to determine Pos' (A, C),
Pos ' (B , D), and Pos ' (C, D) such that these local rela­
tions are projections of the unique universal relation
Pos'(A, B, C, D), satisfying the relations Pos'(A, B),
Pos(A, C), Pos(B, D), and Pos(C, D).
Clearly, Pos'(ai,bi>ck,dt)
l if and only if i
j
e, and therefore Pos'(ck.de)
l if and only
=

k

=

k=€.

=

=

=

if

The tool for achieving this result is the set PrA s ,
PrAc, Prso, and Prco of procedures. Since PrAB can
only process information on the variables A and B, and
PrAc can only process information on A and C, then
the only valuable information to communicate be­
tween the two procedures is information on A (see Fig­
ure 6). That is, between Pr1 and Prz with scopes V1
and V2, respectively, only information on V 1 n V2 need
to be communicated. The new relation Pos'(A, B) in­
troduces a constraint between the state of A and the
state of B, but since only information on A alone and
B alone can be communicated, the constraint cannot
be communicated to Prc0.
Note that if a cycle contains more than 4 variables, the
construction can be extended by clamping the states
of further intermediate variables.

We now can construct an instantiation, which cannot
be propagated correctly: (1) Let a configuration be
possible if and only if its projection to A1 x · · · x An
is possible. (2) Perform the construction as shown in
the example.
1
By the proof of Statement 1, we see that it can be
generalized to systems with other uncertainty calculi
like, e.g., Dempster-Shafer belief functions or fuzzy
systems. In fact, the reasoning can be applied to any
calculus having propositional calculus as a special case.
An axiomatization of these possible calculi is outside
the scope of this paper, but the axioms in (Shenoy and
Shafer, 1990) form a good starting point.
Concerning complexity we still have a couple of loose
ends. Although a general scheme involves a hidden tri­
angulation, the computational complexity needs not
be of the same kind as for the junction tree scheme.
In the junction tree scheme the complexity is propor­
tional to the number of configurations in the cliques.
Therefore a general local scheme has an equivalent
computational complexity if it is proportional to the
number of configurations in the scopes. This is the
case if each configuration has an impact on the mes­
sages sent in the algorithm. In this paper we shall not
give sufficient conditions for this to hold.
The second loose end has to do with optimality. A gen­
eral scheme is, e.g., to work with P(U) only. This cor­
responds to working with the complete graph over U.
This scheme has a trivial optimality problem, but the
junction tree method can do much better even for sub­
optimal triangulations. Therefore we conclude:

Statement 2 If a general local propagation scheme
has a complexity at least proportional to the num­
ber of configurations in the scopes, and its opti­
mality problem can be solved in polynomial time,
then either the junction tree scheme can do better
or 'J' = N'J'.

Acknowledgements
The work is part of the ODIN-project at Aalborg Uni­
versity, and we thank our colleagues in the group for
inspiring discussions.
The work is partially funded by the Danish Research
Councils through the PIFT-programme.

366

Jensen and Jensen



Decision theoretical troubleshooting is about
minimizing the expected cost of solving a
certain problem like repairing a complicated
man-made device. In this paper we consider
situations where you have to take apart some
of the device to get access to certain clusters and actions. Specifically, we investigate
troubleshooting with independent actions in
a tree of clusters where actions inside a cluster cannot be performed before the cluster is
opened. The problem is non-trivial because
there is a cost associated with opening and
closing a cluster. Troubleshooting with independent actions and no clusters can be solved
in O(n · lg n) time (n being the number of
actions) by the well-known ”P-over-C” algorithm due to Kadane and Simon, but an efficient and optimal algorithm for a tree cluster model has not yet been found. In this
paper we describe a ”bottom-up P-over-C”
O(n · lg n) time algorithm and show that it is
optimal when the clusters do not need to be
closed to test whether the actions solved the
problem.

1

INTRODUCTION

In decision theoretical troubleshooting we are faced
with a problem that needs to be solved by applying
solution actions and by posing questions that gather
information about the problem. The premise is that
after each action we can cost free observe whether the
problem was solved. The domain is assumed to be uncertain, that is, solution actions may be imperfect and
information might be non-conclusive. Given a model
that describes the uncertainty and the cost of actions
and questions, the goal is to compute a strategy for
solving the problem with the lowest expected cost.

If the model has the following assumptions:
(a)
(b)
(c)
(d)

the problem is due to a single fault,
different actions address different faults,
costs do not depend on the previous history, and
there are no questions,

then the problem is solvable in O(n · lg n) time where
n is the number of actions. This algorithm is the wellknown ”P-over-C” algorithm by (Kadane and Simon,
1977) which was first brought into a troubleshooting
context by (Kalagnanam and Henrion, 1990). Furthermore, if any of the above assumptions are relaxed
without restrictions, the problem becomes NP-hard
(Vomlelová, 2003). If assumption (a) is replaced with
an assumption about multiple independent faults, an
O(n·lg n) P-over-C-like algorithm also exists (Srinivas,
1995). Troubleshooting without assumption (b) can
also be somewhat simplified due to the dependency
set algorithm of (Koca and Bilgic, 2004).
(Langseth and Jensen, 2001) proposed to relax assumption (c) slightly by considering a model where
the actions can be partitioned into a flat set of socalled cost clusters (see Figure 1). The idea is that in
order to access an action in a bottom level cluster Ki ,
you need to pay an additional cost Coi and to close the
cluster you have to pay an additional cost Cci . Thereby
it is possible to model e.g. the repair of complex manmade devices where you need to take apart some of the
equipment to perform certain actions. If we can determine whether an action has solved the problem without assembling the cluster first, Langseth and Jensen
said that the cluster has inside information; otherwise the cluster is without inside information. They
furthermore describe heuristics for both problems. In
this paper we present a proof of correctness of their
algorithm for the problem with inside information. We
furthermore extend the model to a tree of clusters and
give an O(n · lg n) time algorithm that is proved optimal. (Warnquist et al., 2008) describe a slightly more
general cost cluster framework, but they do not address the issue of finding an efficient algorithm.

conditional cost Cα (ε) of an action α given evidence
ε is given by Cα + CK(α) if α ∈ CA(ε) and by Cα if
α ∈ FA(ε).

Cluster 0
α1
Co1

Cc1

Cluster 1
α3

α2

α4

Co2

Cc2

Cluster 2
α5

α6

Co3

Cc3

Cluster 3
α7

α8

Figure 1: Example of the flat cost cluster model. To
open a cluster Ki we pay the cost Coi , and to close a
cluster we pay the cost Cci .

2

PRELIMINARIES

In this paper we shall examine troubleshooting problems where the following model parameters are given.
F = {f1 , . . . , fm } is a set of faults describing the
possible causes to the problem. For each fault f ∈
F, we have a probability P(f) describing how likely
it is that f is present when troubleshooting begins.
A = {α1 , . . . , αn } is a set of actions that can potentially solve the problem. Each action α has two
possible outcomes, namely ”α = yes” (the problem
was fixed) and ”α = no” (the action failed to fix the
problem). Each action α has a positive cost Cα describing the resources required to perform the action.
Finally, each action has an associated success probability P(α = yes | f), the probability of solving the
problem by performing the action when f is present.
The set of actions A can be partitioned into ` + 1 clusters K, K1 , . . . , K` where cluster K is the top-level cluster and the remaining are bottom-level clusters. The
cost of opening a cluster Ki is Coi and the cost of closing it again is Cci . We define CKi = Coi +Cci . An action
α belongs to cluster K(α).
During the course of troubleshooting we gather evidence εi meaning that the first i actions failed tosolve
the problem, and we have by assumption P ε0 = 1
x:y
because the device
as
Sy is faulty. We also write ε
shorthand for i=x {αi = no}. FA(ε) is the set of
free actions consisting of all actions (excluding those
already performed) from open clusters given evidence
ε. CA(ε) is the set of confined actions consisting of
all actions from closed clusters. Note that we have
FA(ε) ∪ CA(ε) ⊆ A and FA(ε) ∩ CA(ε) = ∅ for all
evidence ε. By performing an action α from CA(ε) we
pay the cost CK(α) because at this point we are certain
that we must both open and close the cluster. In that
case α is called an opening action (for K(α)), and all
remaining actions of K(α) are released by removing
them from CA(ε) and adding them to FA(ε). The

Throughout this paper we uphold the following simplifying assumptions about the model:
1 (The single fault assumption). Initially the problem
is known to exist and it is due to the presence of a
single fault from F.
2 (The idempotent action assumption). Repeating a
failed action will not fix the problem.
3 (The carefulness assumption). By performing an action or testing the system, we never introduce new
faults.
4 (The independent actions assumption). Different
actions address different faults.
5 (The costless system test assumption). Checking
whether the problem still exists after performing an action can be done at a negligible cost.
6 (The inside information assumption). All clusters
have inside information.
Due to the single-fault assumption we may compute
the repair probabilityPof an action given evidence ε
as P(α = yes | ε) =
f∈F P(α = yes | f) · P(f | ε). In
a few places we shall abbreviate P(α = yes | ε) with
P(α | ε). Due to the independent actions assumption
P(α) /P(β) = P(α | ε) /P(β | ε) for all evidence ε not
involving α or β. We shall therefore abbreviate the
initial repair probability P(α = yes) as Pα .
A troubleshooting sequence is a sequence of actions
s = hα1 , . . . , αn i prescribing the process of repeatedly
performing the next action until the problem is fixed
or the last action has been performed. We shall write
s[k, m] for the subsequence hαk , . . . , αm i and s(k, m)
for the subsequence hαk+1 , . . . , αm−1 i. The index of
an opening action in a troubleshooting sequence s is
called an opening index, and the set of all opening indices for s is denoted Z with Z ⊆ {1, . . . , n}, |Z| = `.
To measure the quality of a given sequence we use the
following definition.
Definition 1. Let s = hα1 , . . . , αn i be a troubleshooting sequence. Then the expected cost of repair (ECR)
of s is given by
ECR (s) =

n
X


P εi−1 · Cαi (εi−1 ) .

i=1

Formally, our optimization problem is to find a troubleshooting sequence with minimal ECR. Without
cost clusters, the problem is easily solved due to the
theorem below.

Theorem 1 (Kadane and Simon (1977)). Let s =
hα1 , . . . , αn i be a troubleshooting sequences in a model
without cost clusters. Then s is optimal if and only if
Pαi
Pαi
≥
Cαi
Cα i

for i ∈ {1, . . . , n − 1} .

If costs are not conditional and actions are independent, the lemma below leads directly to the ”P-over-C”
algorithm.
Lemma 1 (Jensen et al. (2001)). Let s be a troubleshooting sequence and let αx and αx+1 be two adjacent actions in s. If s is optimal then

Cαx (εx−1 ) + 1 − P αx | εx−1 · Cαx+1 (εx ) ≤
Cαx+1 (εx−1 ) +
1 − P αx+1 | εx−1



· Cαx (εx−1 , αa+1 = no) .

With assumption 1, 3, 4, and 5 we may simplify computations and notation somewhat because of the following result.
Proposition 1. Let s = hα1 , . . . , αn i be a troubleshooting sequence. Then the ECR of s may be computed as


i−1
n
X
X
Pαj  ,
Cαi (εi−1 ) · 1 −
ECR (s) =
j=1

i=1

where 1 −

Pi−1

j=1

Pαj


= P εi−1 .

This easy computation of the probabilities can be
dated back to (Kalagnanam and Henrion, 1990) and
(Heckerman et al., 1995).
Thus, due to our assumptions, we may completely
ignore F, P(f), and P(α = yes|f) once the repair probabilities have been computed. Therefore, we mainly
use Pα in the rest of this paper.
Using the set of opening indices Z, we can rewrite the
definition of ECR of a sequence s to


i−1
X
X
ECR (s) =
Cαi · 1 −
Pαj 
i=1

j=1


+

X
z∈Z

CK(αz ) · 1 −

z−1
X


Pαj  (1)

j=1

where we have decomposed the terms into those that
rely on the cost of performing actions and those that
rely on the cost of opening and closing a cluster. We
define the efficiency of an action α given evidence ε
as ef(α | ε) = Pα /Cα (ε), and we write ef(α) for the
unconditional efficiency Pα /Cα . Finally, the cluster
P
efficiency of an opening action is cef(α) = Cα +CαK(α) .

Lemma 2. Let s = hα1 , . . . , αn i be an optimal troubleshooting sequence with opening indices
zi ∈ Z. Then the ` + 1 subsequences s[α1 , αz1 ),
s[αzi , αzi+1 ) ∀i ∈ {1, . . . , ` − 1}, and s[αz` , αn ] are
ordered with respect to descending efficiency.
Proof. Between opening indices the costs are not conditional, and so we must sort by descending ef(·) to be
optimal.
We have now established that given the opening index
for each cluster, it is a simple task of merging ordered
sequences to establish an optimal sequence. The difficult part is to determine the opening indices.

3

THE EXTENDED P-OVER-C
ALGORITHM

The standard ”P-over-C” algorithm works by sorting
the actions based on descending efficiency. The extended algorithm works in a similar manner, but it
also considers the efficiency of a cluster: if a cluster is
more efficient than all remaining actions and clusters,
we should perform some actions from that cluster first.
Definition 2. The efficiency of a cluster K is defined
as
P
α∈M Pα
P
ef(K) = max
M⊆K CK +
α∈M Cα
and the largest set M ⊆ K that maximizes the
ficiency is called the maximizing set of K. The
quence of actions found by sorting the actions of
maximizing set by descending efficiency is called
maximizing sequence of K.

efsethe
the

It turns out that it is quite easy to calculate the efficiency of a cluster. The following result is a slightly
more informative version of the one from (Langseth
and Jensen, 2001):
Lemma 3. Let K be a cluster. Then the maximizing set M can be found by including the most efficient
actions of K until ef(K) starts decreasing. Furthermore, all actions α in the maximizing set M have
ef(α) ≥ ef(K) and all actions β ∈ K \ M have
ef(β) < ef(K).
The algorithm is described in Algorithm 1. If n denotes the total number of actions, we can see that line
2 takes at most O(n · lg n) time. Once the actions have
been sorted, line 3-6 takes at most O(n) time. The
loop in line 7-20 can be implemented to run in at most
O(n · lg(` + 1)) time by using a priority queue for the
most efficient element of A and the most efficient element of each cluster. Thus the algorithm has O(n·lg n)
worst case running time. In the next section we prove
that the algorithm returns an optimal sequence.

Algorithm 1 The extended P-over-C algorithm
(Langseth and Jensen, 2001)
1: function ExtendedPOverC(K, K1 , . . . , K` )
2:
Sort actions of K and all Ki by descending ef(·)
3:
Calculate ef(Ki ) and maximizing sets Mi
4:
for all i ∈ {1, . . . , `}
5:
Let Kclosed = {Ki | i ∈ {1, . . . , `}}
6:
Let A = {α | α ∈ K or α ∈ Ki \ Mi for some i}
7:
Let s = hi
8:
repeat
9:
Let β be the most efficient action in A
10:
or cluster in Kclosed
11:
if β is an action then
12:
Add action β to s
13:
Set A = A \ {β}
14:
else
15:
Add all actions of the maximizing set
16:
of cluster β to s in order of
17:
descending efficiency
18:
Set Kclosed = Kclosed \ {β}
19:
end if
20:
until Kclosed = ∅ and A = ∅
21:
Return s
22: end function

Proof. We shall use the fact that for positive reals we
have
b
a
a+b a
⊗ ⇔ ⊗
c+d
c
d
c
for any weak order ⊗ (e.g. ≥ and ≤). Let M consist
of actions in K such that ef(M) is maximized. Then
ef(M) equals
P
P
α∈M\{β} Pα + Pβ
α∈M Pα
P
P
=
CK + α∈M Cα
CK + α∈M\{β} Cα + Cβ
=

SP + Pβ
P
=
SC + Cβ
C

where β is chosen arbitrarily. Let furthermore γ ∈
K \ M. We shall prove
Pβ
P
Pγ
>
≥
Cβ
C
Cγ
which implies the theorem. We first prove the leftmost inequality. Because ef(M) is maximal we have
SP + Pβ
SP
Pβ
SP
≥
which is equivalent to
≥
SC + Cβ
SC
Cβ
SC
which again is equivalent to

Example 1. We consider a model with three clusters,
where Kα is the root cluster and Kβ and Kγ are the
bottom-level clusters. We have CKβ = 2 and CKγ = 1,
and the following model parameters:

α1
α2
β1
β2
γ1
γ2

P
0.14
0.11
0.20
0.10
0.25
0.20

C
1
1
1
1
1
1

ef(·)
0.14
0.11
0.067
0.033
0.125
0.10

cluster
Kα
Kα
Kβ
Kβ
Kγ
Kγ

ef(K)

0.075
0.15

The maximizing set for Kβ is {β1 , β2 } and for Kγ it
is {γ1 , γ2 }, and from this the cluster efficiencies have
been calculated. Algorithm 1 returns the sequence s =
hγ1 , γ2 , α1 , α2 , , β1 , β2 i which has ECR
ECR (s) = 2+0.75+0.55+0.41+0.30·3+0.10 = 4.71 .
If we followed the simple P-over-C algorithm we would
get the sequence s2 = hα1 , γ1 , α2 , γ2 , β1 , β2 i with ECR

ECR s2 = 1+0.86·2+0.61+0.50+0.30·3+0.10 = 4.83 .

SP + Pβ
Pβ
≥
.
Cβ
SC + Cβ
The second inequality is proved similarly.
When we look at opening indices we get the following
result.
Lemma 4. Let s = h. . . , αx , αx+1 , . . .i be an optimal
troubleshooting sequence, and let Z be the opening indices of s. Then
cef(αx ) ≥ ef(αx+1 ) if x ∈ Z, αx+1 ∈ FA(εx−1 )
ef(αx ) ≥ cef(αx+1 ) if αx ∈ FA(εx−1 ), x + 1 ∈ Z
cef(αx ) ≥ cef(αx+1 ) if x ∈ Z, x + 1 ∈ Z
Proof. Apply Lemma 1 and do some pencil pushing.
For example, case 1: x ∈ Z and αx+1 ∈ FA(εx−1 ). In
this case we have

Cαx + CK(αx ) + 1 − P αx | εx−1 · Cαx+1 ≤


Cαx+1 + 1 − P αx+1 | εx−1 · Cαx + CK(αx )
m
P αx+1 | εx−1




Cαx + CK(αx ) ≤ P αx | εx−1 Cαx+1

m

4

CORRECTNESS OF THE
ALGORITHM

We start with a proof of Lemma 3:

ef(αx+1 ) ≤ cef(αx )
because P(αx ) ≥ P(αx+1 ) ⇔ P(αx | ε) ≥ P(αx+1 | ε)
for independent actions.

Definition 3. Let s[x, y] be a subsequence of a troubleshooting sequence s. Then the efficiency of s[x, y] is
given by
Py
Pαi
ef(s[x, y]) = Py i=x i−1
C
)
i=x αi (ε
Definition 4. Let s = h. . . , αx , . . . , αy , . . .i be a troubleshooting sequence. If all actions of the subsequence
s[x, y] belong to the same cluster, we say that the subsequence is regular. If furthermore s[x, y] is as long as
possible while not breaking regularity, we say that the
subsequence is a maximal regular subsequence.
Remark. Any troubleshooting sequence can be partitioned into a sequence of regular subsequences, and
if all the subsequences are maximal, this partition is
unique.
Lemma 5. Let s be an optimal troubleshooting sequence, and let s[x, x + k] and s[y, y + `] (with y =
x + k + 1) be two adjacent regular subsequences such
that K(αx ) 6= K(αy ) or such that neither x nor y is an
opening index. Then
ef(s[x, x + k]) ≥

ef(s[y, y + `])


So ECR (s) − ECR s2 ≤ 0 is equivalent to


"x+k
# y+`
y+`
x+k
X
X
X
X
Cαi (εi−1 ) ·
Pαj ≤ 
Cαi (εi−1 ) ·
Pαj
i=x

j=y

i=y

j=x

which yields the result.
Lemma 6. There exists an optimal troubleshooting
sequence s where for each opening index x ∈ Z, there
is a maximal regular subsequence s[x, x+j] (j ≥ 0) that
contains the maximizing sequence for cluster K(αx ).
Proof. Let s be an optimal troubleshooting sequence,
and let x be an opening index. Let s[x, x + j] be a
maximal regular subsequence and assume that it does
not contain the maximizing set. Then there exists
αy ∈ K(αx ) with y > x + j + 1 such that
ef(αy ) > ef(s[x, x + j])
Observe that the subsequence s[x, y − 1] can be partitioned into m > 1, say, maximal regular subsequences
s 1 , . . . , s m with s 1 = s[x, x + j]. By Lemma 5 we have
ef(αy ) > ef(s 1 ) ≥ ef(s 2 ) ≥ · · · ≥ ef(s m ) ≥ ef(αy )

Proof. We consider the sequence
s2 = h. . . , αx−1 , αy , . . . , αy+` , αx , . . . , αx+k , . . .i
which is equal to s except that the two regular sequences have been swapped.
Since s is optimal we

have ECR (s) − ECR s2 ≤ 0. Because the subsequences are regular and belong to different clusters or
do not contain opening indices, the costs are the same
in the two sequences in both s and s2 . Therefore,
we

get that the terms of ECR (s) − ECR s2 equal



Cαx (εx−1 ) · P εx−1 − P εx−1 , εy:y+`
..
.



x+k−1
x+k−1
Cαx+k (ε
)· P ε
− P εx+k−1 , εy:y+`



Cαy (εy−1 ) · P εy−1 − P εx−1
..
.



y+`−1
y+`−1
− P εx−1 , εy:y+`−1
Cαy+` (ε
)· P ε
since the remaining terms cancel out. Now observe
that


P εx+i−1 − P εx+i−1 , εy:y+` =


y+`
y+`
x+i−1
x+i−1
X
X
X
X
1−
Pαj − 1 −
Pαj −
Pαj  =
Pαj
j=1

j=1

j=y

j=y

and, similarly,


P εy+i−1 − P εx−1 , εy:y+i−1 =


y+i−1
y+i−1
x−1
x+k
X
X
X
X
1−
Pαj − 1 −
Pαj −
Pαj  = −
Pαj
j=1

j=1

j=y

j=x

where the last inequality follows by the fact that αy is
not an opening action (so we avoid ≥ cef(αy )). This
situation is clearly impossible. Therefore s[x, x + j]
must contain the maximizing set. By Lemma 2, it
must also contain a maximizing sequence.
Remark. In the above proof there is a technicality that
we did not consider: there might be equality between
the efficiency of an action in the maximizing sequence,
the efficiency of the maximizing sequence, and one or
more free actions. This problem can always be solved
by rearranging the actions, and so for all proofs we
shall ignore such details for the sake of clarity.
Finally, we have the following theorem:
Theorem 2. Algorithm 1 returns an optimal troubleshooting sequence.
Proof. By Lemma 5 we know that an optimal sequence
can be partitioned into a sequence of maximal regular
subsequences which is sorted by descending efficiency.
If we consider Lemma 6 too, then we know that we
should open the clusters in order of highest efficiency
and perform at least all actions in their maximizing
sequences as computed by Lemma 3. By Lemma 2
we know that the order of actions in the maximizing
sequences is the optimal one. By Lemma 5 we also
know that all free actions α with ef(α) > ef(K) must
be performed before opening the cluster, and all free
actions with ef(α) < ef(K) must be performed after
opening the cluster and performing all the actions in
its maximizing sequence.

Cluster 0
α1
Co1

α2
Cc1

Co2

Cluster 1
α3

Cc2

Cluster 2

α4
Co3

α5

α6

Cc3

Cluster 3
α7
Co4

α8
Cc4

Cluster 4
α10

α9

Co5

Cc5

Cluster 5
α12

α11

Figure 2: Example of a tree cost cluster model. To
open a cluster Ki when the parent cluster is open we
pay the cost Coi and to close a cluster given that all
children clusters are closed we pay the cost Cci .

5

THE TREE CLUSTER MODEL

In this section we shall investigate an extension of the
flat cluster model where the clusters can be arranged
as a tree. We call such a model for a tree cluster model,
and an example is given in Figure 2. In the tree cluster
model, the ECR does not admit the simple decomposition of Equation 1. The complication is that several
clusters might need to be opened before performing an
action in a deeply nested cluster. We therefore call
troubleshooting sequences in the tree cluster model
for tree troubleshooting sequences. Unfortunately, it is
easy to construct examples that show that Algorithm 1
will not yield optimal tree troubleshooting sequences.
Therefore, we shall present a new algorithm that solves
the tree cluster model in O(n · lg n) time.
First we need some additional definitions. The conditional cost Cα (ε) of α ∈ Ki will now depend on how
many clusters that have been opened on the path from
the root K to Ki . We therefore let AK(Ki | ε) denote
the set of ancestor clusters that needs to be opened on
the path from the root K to Ki given evidence ε. We
then define
X
CK ,
Cα (ε) = Cα + CK(α) (ε)
CKi (ε) =
K∈AK(Ki | ε)

Given this, Definition 1 is still valid for tree troubleshooting sequences.

A single action is called an atomic action. A compound
action consists of opening a cluster K and a sequence
of actions in which each action may be either atomic
or compound. Note that we shall usually not distinguish syntactically between atomic and compound actions. Also note that a compound action corresponds
to a subsequence where the first action is an opening action, and the efficiency of a compound action is
simply defined as the efficiency of the corresponding
subsequence. If T is a tree cluster model and K is an
arbitrary cluster in T , then the subtree model induced
by K, denoted TK , is a new tree cluster model containing exactly the clusters in the subtree rooted at
K, and with K as the open root cluster. If the induced
subtree model is a flat cluster model, we call it a flat
subtree model.
Definition 5. Let TK = {K, K1 , . . . , K` } be a flat subtree model. Then the absorbtion of K1 , . . . , K` into K
is a new cluster K↑ containing
1. for each child cluster Ki , a compound action induced by the maximizing sequence for Ki , and
2. all remaining actions from K,K1 ,. . . ,K` .
Note that in K↑ all the actions in a child cluster Ki that
are not contained in the newly generated compound
action will have a lower efficiency than the compound
action for Ki .
Definition 6. Let T be a tree cluster model, and let K
be any cluster in T . Then TK may be transformed into
a single cluster K↑ by repeated absorbtion into the root
cluster of flat subtree models. The resulting cluster K↑
is called the model induced by absorbtion into K.
Remark. By construction, the compound actions in a
model induced by absorbtion into the root cluster K
will only contain actions from the subtrees rooted at
a child of K.
With these definitions we can now present Algorithm
2. The algorithm works in a bottom-up fashion, basically merging leaf clusters into their parents (absorbtion) until the tree is reduced to a single cluster. Then
an optimal sequence is constructed by unfolding compound actions when they are most efficient.
The algorithm can be made to run in O(n · lg n) time
by the following argument. Sort the actions of all clusters in the tree T —this takes at most O(n · lg n) time.
During absorbtion, it is important to avoid merging
all actions of the child clusters into the parent cluster. Instead, we merge only the compound actions
into the parent cluster (takes O(` · lg n) time overall),
and create a priority queue holding the most efficient
remaining action of each child cluster. When creating a compound action for a parent cluster, we then

Algorithm 2 The bottom-up P-over-C algorithm
function BottomUpPOverC(T )
Input: a cluster tree T with root K
Compute the model K↑ induced by absorbtion
into K (see Definition 6)
Let s = hi
while K↑ 6= ∅ do
Let β be the most efficient action in K↑
if β is an atomic action then
Add action β to s
else
Add all actions of β to s in the order
prescribed by β
end if
Set K↑ = K↑ \ {β}
end while
Return s
end function
use actions from the priority queue as needed, and update the priority queue whenever an action is taken
out. Therefore, creating all the compound actions can
never take more than O(n · lg `) time. As the absorbtion process moves towards the root, we are forced to
merge priority queues from different subtrees. A simple induction argument can establish that it takes at
most O(` · lg `) time to merge all these priority queues.
In the following we shall prove that Algorithm 2 computes an optimal tree troubleshooting sequence. The
first two lemmas are minor generalizations of previous
lemmas, and the proofs are almost identical.
Lemma 7. Lemma 2 generalizes to tree troubleshooting sequences.
Lemma 8. Lemma 5 generalizes to subsequences of
actions that consists of (i) only free actions, or (ii)
actions from the same subtree.
Next we shall investigate the special properties of the
compound actions generated by the absorbtion process.
Definition 7. Let T be a tree cluster model, and let K
be any non-leaf cluster in T . A maximizing compound
action α̂ for K in T is defined as any most efficient
compound action in the model induced by absorbtion
into K.
Lemma 9. Let T be a tree cluster model, and let K be
any non-leaf cluster in T . Let TK be the subtree model
induced by K, and let α̂ be a maximizing compound
action for K in T . Then
ef(α̂) ≥ ef(β)
where β is any possible compound action in TK not
including actions from K.

Proof. We proceed by induction. Basis is a flat cluster model T = {K, K1 , . . . , K` } with compound actions βˆ1 , . . . , βˆ` of K and α̂ = maxi β̂i . Let β be any
compound action including actions from clusters in
T \ {K}, and assume that ef(β) > ef(α̂). We shall
use the fact
Pn
n Pi
Pi
n Pi
≤ max
min
≤ Pni
(2)
i
i Ci
Ci
C
i
i
(which is also known as Cauchy’s third inequality).
Then by Equation 2, β cannot be formed by any combination of the β̂i ’s as this would not increase the efficiency. Therefore β must be formed by either a strict
subset or a strict superset of one of the β̂i ’s. If β is a
subset of any β̂i , then the maximality of β̂i leads to a
contradiction. If β is a superset of any β̂i , then it will
include subsets of actions from a set of clusters with
subscripts I ⊆ {1, . . . , `}. Let us denote the subsets
from each Ki as βi . We then have
P
Pβ̂i
Pβ
Pβ
= ef(α̂)
ef(β) = Pi∈I i ≤ max i ≤ max
i∈I
C
C
C
i∈{1,...,`}
βi
i∈I βi
β̂i
where the first inequality follows by Equation 2, the
second follows by the definition of compound actions
formed during absorbtion, and the last equality is by
definition of a maximizing compound action. Since the
sets βi were chosen arbitrarily, we get a contradiction.
Hence in all cases ef(α̂) ≥ ef(β).
Induction step: we assume the Lemma is true for
all children Ki , . . . , K` of an arbitrary cluster K
where the children have maximizing compound actions
βˆ1 , . . . , βˆ` . A similar argument as above then shows
that the lemma is true for K as well.
Lemma 10. Let T be a tree cluster model with root
cluster K. Then there exists an optimal tree troubleshooting sequence s that contains (as subsequences) all
the compound actions of the model induced by absorbtion into K. Furthermore, the compound actions in s
are ordered by descending efficiency.
Proof. Let s = hα1 , . . . , αx , . . . , αx+k , . . .i be an optimal tree troubleshooting sequence and let αx be an
opening action, and let s[x, x + k], k ≥ x be the sequence of maximal length of actions from the same
subtree. Let furthermore s[x, x + k] be the first subsequence that contradicts the lemma, that is, s[x, x + k]
does not contain the compound action α̂ for the cluster K(αx ). Then there exists an atomic action αy ∈ α̂
(with y > x + k + 1) such that αy 6∈ s[x, x + k]. We
then have
ef(αy ) > ef(α̂) > ef(s[x, x + k])
because all atomic actions in a compound action are
more efficient than the compound action itself, and because α̂ is the most efficient compound action in the

subtree rooted at K(αx ) (Lemma 9). We can then partition the actions between αx+k and αy into m > 1,
say, subsequences (of maximal length) s 1 , . . . , s m . If
one (or more) of these subsequence is more efficient
than αy , we immediately get a contradiction to optimality of s because such a subsequence can be moved
before s[x, x + k] (Lemma 8). So we can assume that
all the m subsequences are less efficient than αy . Then
by successive application of Lemma 8 we can decrease
the ECR by moving αy to position x + k + 1. However, this again contradicts that s was optimal. Hence
s[x, x + k] must contain α̂.
By Lemma 8 it follows that the order of the compound
actions must be by descending efficiency.
Theorem 3. Algorithm 5 returns an optimal troubleshooting sequence.
Proof. By Lemma 10 we only need to establish the
order of the free actions between compound actions.
By Lemma 8 it follows that any compound action is
preceeded by more efficient free actions and followed
by less efficient free actions.

6

CONCLUSION

We have presented an algorithm, which in O(n · lg n)
time (n being the number of actions) provides an optimal troubleshooting sequence for scenarios where the
cost clusters form a tree and have inside information.
This is a useful result on its own, but there is more to
it.
When evaluating algorithms for troubleshooting, you
must distinguish between off-line and on-line activity.
If your task is off-line, the time complexity of your
algorithm may not be particularly important as long
as the result can be stored easily (like for example
an optimal action sequence). However, if the decision
support system is flexible, it must allow the user to interact with the recommendations and have the system
calculate an optimal next action based on alternative
information.
Furthermore, for many scenarios you will request online calculation of an optimal sequence; for example
when the model includes questions and tests. For this
kind of scenario, a direct representation of an optimal strategy may require too much space. Therefore,
a myopic question heuristic usually relies on optimal
sequences of actions calculated on-line.
Finally, our results imply a major improvement for offline methods like AO∗ because the search tree can now
be extensively pruned. This is because all subtrees
that consist entirely of actions can be replaced with a
single sequence of actions.

7

ACKNOWLEDGEMENTS

We would like to thank the three anonymous reviewers
for their excellent feedback. Thanks also go to Sven
Skyum for help with Lemma 3.



We present an approach to the solution of de­
cision problems formulated as influence dia­
grams.

This approach involves a special tri­

angulation of the underlying graph, the con­
struction of a junction tree with special prop­

tials normalized. Ndilikilikesha (Shachter and Ndiliki­
likesha, 1993; Ndilikilikesha, 1994) modified the node­
removal/ arc-reversal algorithm to avoid these extra di­
visions; the result is an algorithm that is equivalent to
Shenoys algorithm with respect to computational effi­
ciency.

erties, and a message passing algorithm op­

Our work builds primarily on the work of Shenoy

erating on the junction tree for computa­

(1992) and Shachter and Peat {1992), in addition

tion of expected utilities and optimal decision

to our previous work on propagation algorithms for

policies.

the expert system shell Hugin (Andersen et al., 1989;
Jensen et al., 1990) .

1

INTRODUCTION

Influence diagrams were introduced by Howard and
Matheson (1981) as a formalism to model decision

2

INFLUENCE DIAGRAMS

An

influence diagram is a belief network augmented

problems with uncertainty for a single decision maker.

with decision variables and a utility function.

The original way to evaluate such problems involved

The structure of a decision problem is determined by

unfolding the influence diagram into a decision tree
and using the "average-out and fold-back" algorithm
on that tree. Shachter (1986) describes a way to eval­

uate an influence diagram without tranforming it into
a decision tree. The method operates directly on the
influence diagram by means of the node-removal and
arc-reversal operations. These operations successively
transform the diagram, ending with a diagram with
only one utility node that holds the utility of the op­
timal decision policy; the policies for the individual
decisions are computed during the operation of the
algorithm (when decision nodes are removed).
Shenoy (1992) describes another approach to the eval­
uation of influence diagrams: the influence diagram is
converted to a valuation network, and the nodes are
removed from this network by fusing the valuations
bearing on the node (variable) to be removed. Shenoys
algorithm is slightly more efficient than Shachters al­
gorithm in that it maintains a system of valuations,

an acyclic directed graph G. The vertices of G repre­
sent either random variables (also known as chance or
probabilistic variables) or decision variables, and the
edges represent probabilistic dependencies between
variables. Decision variables represent actions that are
under the full control of the decision maker; hence, we
do not allow decision variables to have parents in the
graph.
Let

UR

be the set of random variables, and let the

set of decision variables be

Uo

=

{0 1, .

.

.

, Dn},

with

the decisions to be made in the order of their index.
Let the universe of all variables be denoted by U

UR U Uo.

sets lo, . ..

We partition

, In;

for

0<

UR

=

into a collection of disjoint

Ik is the set of variables
Dk and Dk+ 1;
variables, and In is the set

k <

n,

that will be observed1 between decision

Io is the initial evidence

of variables that will never be observed (or will be
observed after the last decision). This induces a partial
order..:: on

U:

whereas Shachters algorithm maintains a system of
conditional probability functions (in addition to the
utility functions), and some extra work (some division
op erations) is required to keep the probability paten-

1 By

'observed,'

will be revealed.

we

mean that the true state of the variable

368

Jensen, Jensen, and Dittmer

We associate with each random variable A a condi­
tional probability function ¢A=P(AI:PA), where 'J'A
denotes the set of parents of A in G .
The state space Xv for V � U i s defined a s the Carte­
sian product of the sets of possible outcomes/decision
alternatives for the individual variables in V. A po­
tential ¢v for a set V of variables is a function from
Xv to the set of real numbers.
The potential <Pv can be extended to a potential

¢w (V � W) by simply ignoring the extra variables:
¢w(w) =¢v(v) ifv is the projection ofw on V.
Given two potentials, <P and tf!. The product ¢ * tV
and the quotient ¢/tf! are defined in the natural way,
except that 0/0 is defined to be 0 (x/0 for x -1= 0 is
undefined) .
The (a priori) joint probability function <Pu is defined
as
¢A·
<Pu
=

IT

AEUR

For each instance of Uo (i.e., each element of
<Pu defines a joint probability function on UR.

Xu0 ),

A solution to the decision problem consists of a series
of decisions that maximizes some objective function.
Such a function is called a utility function. Without
loss of generality, we may assume that the utility func­
tion tfJ is a potential that may be written as a sum of
(possibly) simpler potentials:

The independence restriction imposed on the decision
problem can be verified by checking that, in the influ­
ence diagram, there is no directed path from a deci­
sion Ok to a decision Di (i < k).
3

DECISION MAKING

Assume we have to choose an alternative for deci­
sion On (i.e., the last decision). We have already
observed the random variables 10, . . . , In-1, and we
have chosen alternatives for decisions D1 , ..., 0 n-1.
The maximum expected utility principle2 says that
we should choose the alternative that maximizes the
expected utility. The maximum expected utility for
decision Dn is given by

Pn =max L P(Inllo, · · ·, In-1, 01, ..., Dn) * tf!.
On

I,

Obviously, Pn is a function of previous observations
and decisions. We calculate the maximum expected
utility for decision Dk (k < n) in a similar way:

Pk =Df,ax L P(Ikllo, ... , Ik-1, o, . . . , Dk) *Pk+1·
k

I<

We note that Pk is well-defined because of ( 1).
By expansion of ( 2 ) , we get

Pk =max L P(Ikllo,... , Ik-1, D 1, ... , Dk)
Dk

m

Ik

*max
Dk+l

=max
We need to impose a restriction on the decision prob­
lem, namely that a decision cannot have an impact on
a variable already observed. This translates into the
property

P(Ikllo, ... , Ik-1, D1, ... , On)
=P(Ikllo, ... ,Ik-1.01, ... ,0kl·

(1)

In words: we can calculate the joint distribution for I k
without knowledge of the states of Dk+ 1, ... , Dn (i.e.,
the future decisions).
2.1

GRAPHICAL REPRESENTATION

In Figure 1, an example of an influence diagram is
shown. Random variables are depicted as circles, and
decision variables are depicted as squares. Moreover,
each term of the utility function is depicted as a dia­
mond, and the domain of the term is indicated by its
parent set. The partial order-< is indicated by making
I k-1 the parent set of D k, and we shall use the con­
vention that the temporal order of the decisions are
read from left to right.

(2 )

o.

L
h+•

P(Ik+11Io, ... ,Ik,
D1, ... ,0k+1 ) *Pk+2

L max L P(Ik, Ik+11Io, ... , Ik-1,
l•

Dk+l

Ik+l

D1, ... , 0k+1) *Pk+2·

The last step follows from ( 1) and the chain rule of
probability theory: P(AjB,C)P(BIC) =P(A,BIC). By
further expansion, we get

Pk =max L ···max L P(Ik,..., Inllo, ... , Ik-1,
Dk h
Dn
DnJ * t!J.
D1,
In
•

• •

1

From this formula, we see that in order to calculate
the maximum expected utility for a decision, we have
to perform a series of marginalizations (alternately
sum- and max-marginalizations), thereby eliminating
the variables.
When we eliminate a variable A from a function ¢,
expressible as a product of simpler functions, we par­
tition the factors into two groups: the factors that
involve A, and the factors that do not; call (the prod­
uct of) these factors ¢ :t_ and ¢A, respectively. The
marginal LA¢ is then equal to <PA: *LA¢:t_; LA <Pt
2There are good arguments for adhering to this principle.
See, e.g.,

(Pearl, 1988).

From Influence Diagrams to Junction Trees

369

FIGURE 1.
An influence diagram for a decision problem with four decisions. The set of variables is partitioned into the

sets:

I0

=

{b}, I 1

=

{ e, f}, Iz

=

0,

I3 = {g}, and I4 =

{a, c, d, h, i, j, k, e}.

four local utilities, three of which are associated with single variables (D 1 ,
the pair

(j, k).

then becomes a new factor that replaces the prod­
uct

cj:J:t_ in the expression for ¢1.

This also holds true for

max-marginalizations, provided

cPA.

does not assume

negative values.

The utility function is a sum of

D3, and €), and one associated with

marginalizations; but- in general- we cannot inter­
change the order of a max- and a sum-marginalization;
this fact imposes some restrictions on the elimination
order.

The product cjJ may be represented by an undirected
graph, where each maximal complete set (clique) of
nodes (the nodes being variables) corresponds to a fac­

4

INFLUENCE DIAGRAMS

tor (or a group of factors) of cjJ with that set as its do­
main. Marginalizing a variable

A out of ¢1 then corre­

COMPILATION OF

We first form the moral graph of G. This means adding

sponds to the following operation on the graph: the set

(undirected) edges between vertices with a common

A is

child. We also complete the vertex sets corresponding

of neighbors of A in the graph is completed, and

removed. It is a well-known result that all variables

to the domains of the utility potentials. Finally, we

can be eliminated in this manner without adding edges

drop directions on all edges.

if and only if the graph is triangulated (Rose, 1970).

Next, we triangulate the moral graph in such a way

Obviously, it is desirable to eliminate all variables

that it facilitates the computation of the maximum

without adding extra edges to the graph since this

expected utility. This is equivalent to the selection of

means that we do not create new factors with a larger

a

domain than the original factors (the complexity of

verse of the elimination order must be some extension

representing and manipulating a factor is exponen­

of -< to a total order.

tial in the number of variables comprising its domain).
However, in most cases, this is not possible: we have
to add some edges, and the elimination order chosen

special elimination order for the moral graph: the re­

Finally, we organize the cliques of the triangulated
graph in a strong junction tree:

A tree of cliques

optimal elimination order for all reasonable criteria of
optimality.

( C 1 , C2l of
C2 is contained in every clique on the
path connecting C1 and C2. For two adjacent cliques,
C1 and C2, the intersection C, n Cz is called a sepa­

W hen we perform inference in a belief network (i.e.,

least one distinguished clique R, called a strong root,

will determine how many and hence also the size of
the cliques.

Unfortunately, it is :N'Jl-hard to find an

is called a junction tree if for each pair

cliques,

C1

n

rator. A junction tree is said to be strong if it has at

(C1, Cz) of adjacent cliques in
C1 closer to R than C2, there exists
an ordering of C2 that respects -< and with the ver­
tices of the separator C1 n Cz preceding the vertices
of C2\C1 . This property ensures that the computation

calculation of the marginal probability of some vari­

such that for each pair

able given evidence on other variables), the computa­

the tree, with

tion only involves sum-marginalizations. In this case,
we can eliminate the variables in any order, since the
order of two marginalizations of the same kind can be
interchanged.

However, the calculation of the max­

imum expected utility involves both max- and sum-

of the maximum expected utility can be done by local
message passing in the junction tree (see Section 5).

370

Jensen, Jensen, and Dittmer

that no two cliques have the same index. Moreover,
unless index( C) = 1, the set

{ vE C I !X(v) <index( C)}

will be a proper subset of some other clique with a

lower index than

C.

Let the collection of cliques of the triangulated graph
be

C1, ... , Cm, ordered in

increasing order according

to their index. As a consequence of the above construc­
tion, this ordering will have the running intersection
property (Beeri et

al., 1983),

meaning that

k-1
for all

FIGURE 2.
The moral graph for the decision problem in Figure 1.
Edges added by the moralization process are indicated
by dashed lines.

k

>

1: Sl<

=

C 1< n

U

Ci � C;

for some

i=1

j < k.

It is now easy to construct a strong junction tree: we
start with
each clique

C1 (the root); then we successively attach
Ck to some clique C; that contains Sl<.

Consider the decision problem in Figure 1. Figure

2

shows the moral graph for this problem: edges have

been added between vertices with a common child (in­
cluding utility vertices), utility vertices have been re­
moved, and directions on all edges have been dropped.
Note that the time precedence edges leading into de­
cision vertices are not part of the graph and are thus
not shown.
Figure

shows the strong triangulation of the graph

3

2 generated by the elimination sequence e,
j, k, i (fill-ins: D2
04 and g
04), lt (fill-in:
f
03), a, c (fill-in: b
e), d (fill-ins: 01
e,
01 - f, b f, and e
f), 04, g (fill-in: e --. Oz),
03, 02, f, e, 01, and b. This graph has the fol­
lowing cliques: C16
{04,i,e}, C,s = {lt,k,j},
C14 = {03,lt,k}, C11
{b,c,a}, C10
{b,e,d,c},
Cs
{Oz, g, 04, i}, C6
{f, 03, lt}, Cs
{e, 02, g},
and C1 = {b,O,e,f,d}. Using the above algorithm,
in Figure

�

�

�

�

�

FIGURE 3.

�

�

=

The triangulated graph of the moral graph in Figure 2.
Fill-in edges added during triangulation are indicated
by dashed lines.

=

=

=

=

=

we get the strong junction tree shown in Figure

4

for

this collection of cliques. (There exists another strong

4.1
Let

CONSTRUCTION OF STRONG JUNCTION TREES

!X be

a numbering of

{1, ... , lUI})

U

such that for all

plies !X(u) <

!X(v).

!X: U H
U, u -< v im­

(i.e., a bijection

u,v

E

We assume that !X is the elimi­

nation order used to produce the triangulated graph

of G: vertices with higher numbers are eliminated be­
fore vertices with lower numbers.
Let C
vE C

the edge

Cs

--1

C1

by the edge

Cs

--1

C10.

This tree is

computationally slightly more efficient, but- unfor­
tunately- it cannot be constructed by the algorithm
given in this paper.)
In general, previous observations and decisions will be
relevant when making a decision. However, sometimes
only a subset of these observations and decisions are

be a clique of the triangulated graph, and let
be the highest-numbered vertex such that the

{wE C I !X(w) < !X(v)} have a common neigh­
bor u (j. C with !X(u) < !X(v). If such a vertex v exists,
we define the index for Cas index( C)
!X(v); other­
wise, we define index( C)
1. Intuitively, the index for
a clique C identifies the step in the elimination process
that causes C to "disappear" from the graph. It is easy
to see that the index for a clique C is well-defined, and
vertices

=

=

junction tree for this collection, obtained by replacing

needed to make an optimal decision. For example, for
the decision problem in Figure 1, the variable

e

sum­

marizes all relevant information available when deci­
sion

02

has to be made: although

before decision

0 2,

f

is observed just

it has no relevance for that deci­

sion (it does, however, have relevance for decision 03).
This fact is detected by the compilation algorithm: the
only link from 02 to past observations and decisions
goes to

e.

371

From Influence Diagrams to Junction Trees

FIGURE 4. A strong junction tree for the cliques of the graph in Figure 3.
5

USING THE STRONG JUNCTION TREE

Now, letT be a strong junction tree, and let C1 and C2

FOR COMPUTATIONS

be adjacent cliques with separator S inT. We say that

We perform computations in the junction tree as a spe­
cial'collect' operation from the leaves of the junction

C1 absorbs from Cz if
and tVc as follows:

Q:Jc,

and tVc, change to

,

cPc,

tree to some strong root of the tree.
To each clique

C in the junction tree, we associate a

probability potential QJc and a utility potential tVc
defined on

Xc.

Let e be the set of cliques. We define

where

the joint potentials q, and tV for the junction tree as

tVs

=

M

Cz\S

cPCz *tV c z

·

Note that this definition of absorption is 'asy mmetric'
We initialize the junction tree as follows: each variable

A E U R is assigned to a clique that contains A U PA.
T he probability potential for

a

clique is the product of

the conditional probability functions for the variables
assigned to it. For cliques with no variables assigned
to them, the probability potentials are unit functions.
In this way, the joint probability potential for the junc­
tion tree becomes equal to the joint probability func­
tion for the influence diagram. Similarly, each utility

in the sense that information only flows in the direc­
tion permitted by the partial order -<:. It is possible
to generalize this definition of absorption to a sym­
metric definition similar to the one given in (Jensen
et

al.,

1990} for the case of pure probabilistic influence

diagrams.
Clearly, the complexity of an absorption operation is

O{I Xc,l + IXsl + 1Xc21).

Note in particular that the
contribution from the division operation plays a much
smaller role than in (Shenoy, 1992), since division op­

function tVk is assigned to some clique that can ac­
commodate it. The utility potential for a clique is the

erations are performed on separators only.

sum of the utility functions assigned to it; for cliques

We will need the following lemma, which we shall state

with no utility functions assigned to them, the utility

without proof.

potential is a null function.
We need a generalized marginalization operation that
acts differently on random and decision variables. We
denote the operation by

'M'

.

For random variable

A

and decision variable D, we define

M q,
D

For a set

V of variables,

we define

=

maxQ:l.
D

Mv q,

as a series of

single-variable marginalizations, in the inverse order
as determined by the relation -<:. Note that although
-<: is only a partial order,

Mv q,

is well-defined.

Lemma 1 Let D be a decision variable, and let

V

a set of variables that includes all descendants of
in G .
of

D

Then

Mv\{ D} Q:lu,

be

0

considered as a function

alone, is a non-negative constant.

Let T be a strong junction tree with at least two
cliques; let

cPT

be the joint probability potential and

tVT the joint utility potential on T. Choose a strong
root R for T and some leaf l (=I R); let T \ l denote
the strong junction tree obtained by absorbing l into

its neighbor N and removing l; denote the separator
between N and l by S.

372

Jensen, Jensen, and Dittmer

Theorem 1 After absorption of l into T, we have

M <Pr

tl>r = <PT\L

*

*

(2) Xk

is a decision variable. By induction, we get

tVT\L·

L\S
Proof: Let

<iJL

IJ

=

1
Because of Lemma 1, <P ( k+ l, considered as a

<Pc;

function of xk alone, is a non-negative constant,

CEe\{L}

and we get
Since <PL does not assume negative values, we get

(mxax<P(k+1l)

*

,

L\S

L\S

(

maxx,

tj>lk+1l

maxx. <j)lk+ 1 J

- .+,(k)
-�

*

We have to show that

M <PL
L\S

*

(ti>L + lj}L)

=

<Ps

*

(�5�S

+ 'iJL

),

+\i))

( tj>(k)
<tJ(k)

L

+

::1:

�L

)

·

I

By successively absorbing leaves into a strong junction
tree, we obtain probability and utility potentials on
the intermediate strong junction trees that are equal
to the marginals of the original potentials with respect

where

to the universes of these intermediate trees.

This is

ensured by the construction of the junction tree in
which variables to be marginalized out early are lo­
We shall prove this by induction. Let X1

,

... , Xt be

some ordering of l \ S that respects -<. Now, consider

the equation:

cated farther away from the root than variables to be
marginalized out later.

The optimal p olicy for a decision variable can be deter­

mined from the potentials on the clique that is closest

to the strong root and contains the decision variable
(that clique may be the root itself), since all variables
that the decision variable may depend on will also be
members of that clique.

where

For our example decision problem (Figure 4), we can
determine the optimal policy for 01 from {the poten­
tials on) clique C 1 (the root), and the optimal policies
for the remaining decisions can be determined from

(For k = 1, (3) is equivalent to the desired result.) For
k > e, (3) is clearly true; for 1 :s k :s e, we have two

cases:

(1) Xk

cliques Cs (decision 02), C6 (decision 03), and Cs

(decision 0 4).

If only the maximum expected utility is desired, it
is a random variable. By induction, we get

should be noted that only storage for the 'active' part
of the junction tree during the collect operation needs
to be reserved; this means that storage for at most
two adjacent cliques and each clique that corresponds
to a branch point on the currently active path from

the root to a leaf

must be reserved. Since elimination

of a group of variables can be implemented more effi­

ciently than the corresponding series of single-variable
eliminations, it is still useful to organize the computa­
tions according to the structure of the strong junction
tree as compared to (Shenoy,

The correctness of the last step follows from the
fact that

<P(kl (x)

=

0 implies tj>lkl (x)

=

0 (so that

our division-by-zero convention applies).

6

1992).

CONCLUSION

We have described an algorithm to transform a deci­
sion problem formulated as an influence diagram into a

From Influence Diagrams to Junction Trees

secondary structure, a strong junction tree, that is par­
ticularly well-suited for efficient computation of max­

networks by local computations.
4:269-282.

373

Computational

Statistics Quarterly,

imum expected utilities and optimal decision policies.
The algorithm is a refinement of the work by Shenoy
{1992) and Shachter and Peot {1992); in particular,
the construction of the strong junction tree and its
use for computations has been elaborated upon.

Kjcerulff, U. {1990). Triangulation of graphs-algo­
rithms giving small total state space. Research
Report R-90-09, Department of Mathematics and
Computer Science, Aalborg University, Denmark.

T he present work forms the basis for an efficient com­
puter implementation of Bayesian decision analysis in
the expert system shell Hugin (Andersen et al., 1989).

Lauritzen, S. L. and Spiegelhalter, D. J. (1988). Lo­
cal computations with probabilities on graphical
structures and their application to expert systems.

We have not given an algorithm to construct the elim­
ination sequence that generates the strong triangula­
tion. However, the triangulation problem is simpler
than for ordinary probability propagation, since the
set of admissible elimination sequences is smaller; at
this stage, it appears that simple adaptations of the
heuristic algorithms described by Kjcerulff (1990) work
very well. Moreover, even given a triangulation, there
might exist several strong junction trees for the collec­
tion of diques.
Besides the use of the strong junction tree for compu­
tation of expected utilities and optimal decision poli­
cies, it should be possible to exploit the junction tree
for computation of probabilities for random variables
that only depend on decisions that have already been
made. Ideally, this should be done through a 'dis­
tribute' operation from the root towards the leaves of
the junction tree.
Work regarding these problems is in progress.
Acknowledgements

This work has been partially funded by the Danish
research councils through the PIFT programme.

