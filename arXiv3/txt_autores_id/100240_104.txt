

the

Most successful Bayesian network (BN) ap­
plications to date have been built through
knowledge elicitation from experts.

This is

difficult and time consuming, which has lead
to recent interest in automated methods for
learning BNs from data. We present a case

study in the construction of a BN in an intel­
ligent tutoring application, specifically dec­
imal misconceptions.

We describe the BN

construction using expert elicitation and then

domain,

Liz Sonenberg1 Vicki Steinlell

common human difficulties

ifying

and

being

unable to identify the

influences
been

combining
between

much

(e.g. ,

for

in

Hence

recent

time

constructing BNs

[Spirtes et al. , 1993,

in

and

spec­

experts

causal direction of

variables.

interest

mated methods

probabilities,

there

has

in

auto­

from

data

Wallace and Korb, 1999,

Beckerman and Geiger, 1995]).

Most evaluation of

these automated methods is done by taking an exist­
ing BN model, generating data from it that is given to
the automated learner; the learned BN is compared to
the original.

investigate how certain existing automated

While there have been attempts to combine knowl­

knowledge discovery methods might support

edge elicitation from experts and automated knowl­

the BN knowledge engineering process.

edge discovery methods (e.g. [Heckerman et a!., 1994,
Onisko et al., 2000]), there is as yet no established
methodology [Kennett et al . , 2001]. Appropriate eval­

1

INTRODUCTION

Bayesian networks have become a popular AI represen­
tation for reasoning under uncertainty, with successful
applications in (medical) diagnosis, planning, moni­
toring, vision, information retrieval and intelligent tu­
toring [Conati et al., 1997, Mayo and Mitrovic, 2001,
VanLehn and Niu, 2001]. Most successful applications
to date have been built through knowledge elicita­
tion from experts.

time consuming

In general, this is difficult and

[Druzdzel and van der Gaag, 200 1],

with problems involving incomplete knowledge of
'School of Computer Sci. and Soft. Eng., Monash Uni­
versity, VIC 3800, Australia. annnfllcsse.monash.edu.au.
sity

t Department of Computer Science, The Univer­
of Melbourne, Parkville, VIC 3052, Australia.

bonehfllstudents.cs.mu.oz.au

tAs for A. Nicholson. tawll!csse.monash.edu.au
§ Department of Science and Mathematics Educa­
tion, The University of Melbourne, VIC 3010, A ustralia.

uation, in particular, is an open question; most auto­
mated methods use some sort of statistical measure of
how well the BN model fits the data whereas elicited
models are assessed in part by how well their predic­
tions on particular test scenarios meet expert expecta­
tions. When both data and expert knowledge about a
domain is available, it is not simply a question of using
the automated methods to validate the expert elicited
BN, or using the expert to choose between learned
networks or complete those not fully specified.

Net­

works built using the different methods may be very
different. The question then becomes how to resolve
such differences such that the resultant BN model is
acceptable to the expert/client and hence deployable.
In this paper, we present a case study in the construc­
tion of a BN model in the intelligent tutoring system
(ITS) (Section 2). We describe the initial network con­
struction using expert elicitation, together with a pre­
liminary evaluation (Section 3).

We then apply au­

k.staceyfllunimelb.edu.au.

tomated knowledge discovery methods to each main

, Department of Information Systems, The Uni­
versity of Melbourne, Parkville, VIC 3052, Australia.

task in the construction process: (Section 4): ( 1 ) we

LizS�staff.dis.unimelb.edu.au.

we perform simple parameter learning based on fre-

liAs for K. Stacey.

v. steinleflledfac.unimelb.edu. au

apply a classification method to student test data; (2)

NICHOLSON ET AL.

UA1 2001

quency counts to the expert BN structures and (3) we
apply an existing BN learning program. In each case

387

for that item type by that student class other than the

we compare the performance of the resultant network

combinations seen above. We note that the fine mis­
conception classifications have been "grouped" by the

with the expert elicited networks, providing an insight

experts into a coarse classification

into how elicitation and knowledge discovery might b e

decimals are larger numbers), S (shorter is larger), A

combined in the B N knowledge engineering process.

-

L (think longer

(apparent expert) and UN (other). The LU, SU and AU
fine classifications correspond to students who on their
answers on Type 1 and 2 items behave like others in

THE ITS DOMAIN

2

their coarse classification, however they don't b ehave

D ecimal notation is widely used in our society.

Our

testing [Stacey and Steinle, 1999] of 5383 students has
indicated that less 70% of Year 10 students (age
about 15 years) understand the notation well enough
to reliably judge the relative size of decimals.

(age about 10 years) have mastered this important
Expertise grows only very slowly through­

out the intervening years under normal instruction
in our schools, and so an intelligent tutoring ap­
proach to this important topic is of interest.

Stu­

dents' understanding of decimal notation has been
mapped using a short test, the Decimal Comparison
Test (DCT), where the student is asked to choose
the larger number from each of 24 pairs of decimals
[Stacey and Steinle, 1 999]. The pairs of decimals are
carefully chosen so that from the patterns of responses,
students' (mis)understanding can be diagnosed as be­
longing to one of a number of categories. These cate­
gories have been identified manually, based on exten­
sive research [Stacey and Steinle,

1999].

may be students behaving consistently according to
an unknown misconception, or students who are not
following any consistent interpretation.

On

the other hand, more than 30% o f Grade 5 students
concept.

like them on the other item types. These and the UNs

For most stu­

Table 1: Responses experts expect from students with
different misconceptions
Expert
Class

ATE
AMO
MIS
AU
LWH

LZE
LRV

LU

SDF
SRN

su
UN

1
0.4
0.35
H
H
L
H
L

L
L
L
H

H

H

2
5.736
5.62
H
H

L
H
H
H
H
H

Item type
3
4
4.7
0.452
4.08
0.45
H
H
H
L

L

L

L

H
H

H

L
L
L

L

H

H

L

H

L

5
0.4
0.3

6
0.42
0.35

H
H

H
H
L

H

H
H

H
H
L

H

H

L

L

L

dents, there is consistency in their responses to similar
test items and some children display the same miscon­
ception over long periods of time.
Most are based on

false analogies, which are sometimes embellished by
isolated learned facts (such as a zero in the tenths
column makes a number small). For example, many
younger students think 0.4 is smaller than 0.35 be­
cause there are 4 parts (of unspecified size, for these
students) in the first number and 35 parts (also of
unspecified size) in the second. However, these stu­
dents (LWH in Table 1) get many items right, e.g.
5.736 compared with 5.62, with the same erroneous
thinking. Students in the SRN class (Table 1) choose
0 .4 as greater than 0.35 but for the wrong reason, as
they draw an analogy between fractions and decimals
and use knowledge that 1/4 is greater than 1/35 . See
[Stacey and Steinle, 1999] for a detailed discussion of
these responses and categories of students.

Table 1

shows the rules the experts originally used to classify
students based on their response to

6

types of DCT

test items: H = High correctness (e.g. 4 or 5 out of 5),
L

=

have

developed

an

i ntelligent

tutoring

sys­

tem based on computer games for this decimals

A bout a dozen misconceptions have been identified
using the DCT and interviews.

We

Low number correct (e.g. 0 or

1

out of 5), with

'.' indicating that any performance level is observable

domain[Mclntosh et al., 2000].

The overall architec­

ture of the system is shown in Figure 1.

The com­

puter game genre was chosen to provide children with
an experience different from, but complementary to,
normal classroom instruction and to appeal across the
target age range (Grade 5 and above).

Each game

focuses on one aspect of decimal numeration, thinly
disguised by a story line.

It is possible for a stu­

dent to be good at one game or the diagnostic test,
but not good at another; emerging knowledge is often
compartmentalised.1
1In the "Hidden Numbers" game students are con­
fronted with two decimal numbers with digits hidden be­
hind closed doors; the task is to find which number is the
larger by opening as few doors as possible. The game "Fly­
ing Photographer" requires students to place a number on a
number line, prompting students to think differently about
decimal numbers. The "Number Between" game is also
played on a number line, but particularly focuses on the
density of the decimal numbers; students have to type in
a number between a given pair. "Decimaliens" is a classic
shooting game, designed to link various representations of
the value of digits in a decimal number.

NICHOLSON ET AL.

388

UAI2001

The simple expert rules classification described above

the conditional probability tables associated with each

makes quite arbitrary decisions about borderline cases.

network node. For our purposes we consider there to

The use of a BN to model the uncertainty allows it to

be an additional task (4) the evaluation of the network.

make more informed decisions in these cases. Using a

While in theory these tasks can be performed sequen­

BN also provides a framework for integrating student

tially, in practice the knowledge engineering process

responses from the computer games with DCT infor­

iterates over these task until the resultant network is

mation. The BN is initialised with a generic student

considered "acceptable". In this section we describe

model, with the options of individualising with class­

the elicitation of the decimal misconception BN from

room or online DCT results. The BN is used to update

the education domain experts.

an ongoing assessment of the student's understanding,
to predict which item types that student might be ex­
pected to get right or wrong, and, using sensitivity
analysis, to identify which evidence would most im­
prove the misconception diagnosis. The development
of the BN is described below.
The system controller module uses the information

3.1

BN VARIABLES

Student misconceptions are represented on two levels,
by two variables. The coarseClass node can take the
values L, S, A, and UN, whereas the f ineClass node,
incorporating all the misconception types identified by
the experts, can take the 12 values shown in column

provided by the BN, together with the student's previ­

1 of Table 1. Note that the experts consider the clas­

ous responses, to select which item type to present to

sifications to be mutually exclusive. If that were not

the student next, and to decide when to present help

the case, then two variables would not be sufficient;

or change to a new game.

This architecture allows

flexibility in combining the teaching "sequencing tac­
tic" (that is, whether easy items are presented before
harder ones, harder first, or alternating easy /hard),
coverage of all item types, and items which will most
improve the diagnosis.

More detailed descriptions of

both the architecture and the item selection algorithm
are given in [Stacey et al., 2001]. The ITS shown in

rather we would require a Boolean variable for each of
the classifications.
Each DCT item type is made a variable in the BN, rep­
resenting student performance on test items of those
types; student test answers are entered as evidence for
these nodes. The following alternatives were consid­
ered for the possible values of the item type nodes.

Figure 1 (including the four computer games) has

1. Suppose the test contains N items of a given type.

been fully implemented. The game interfaces are cur­

One possible set of values for the BN item type node is

dren, with deployment and assessment in the class­

{0, 1, ... , N}, representing the nu m ber of the items the
student answered correctly. The number of items may

room environment to take place over the next year.

vary for the different types, and for the particular test

rently being assessed for usability on individual chil­

set given to the students, but it is not difficult to adapt
the BN. Note that the more values for each node, the
more complex the overall model; if N were large (e.g.
> 20), this model may lead to complexity problems.

2. The item type node may be given the values {High,
Medium, Low}, reflecting an aggregated assessment of
the student's answers for that item type. For example,
if 5 such items were presented, 0 or 1 correct would
be considered low, 2 or 3 would be medium, while 4
or

5

would be High. For types with 4 items, medium

encompasses only 2 correct, while for types with only
3 itemsl the medium value is omitted completely. This
Figure 1: Intelligent Tutoring System Architecture

3

EXPERT ELICITATION

It is generally accepted that building a BN for a
particular application domain involves three tasks
[Druzdzel and van der Gaag, 2001]: ( 1) identification
of the important variables, and their values; (2) iden­

reflects the expert rules classification described above.
3.2

BN STRUCTURE

The experts considered the coarse classification to be
a strictly deterministic combination of the fine classifi­
cation, hence the coarseClass node was made a child
of the fineClass node, For example, a student was
considered an L if and only if it was one of an LWH,

tification and representation of the relationships be­
tween variables in the network structure; and (3) pa­

LZE, LRV or LU.

rameterisation of the network, that is determining

The type nodes are observation nodes, where entering

UA1 2001

N ICHOLSON ET AL.

389

evidence for a type node should update the posterior

hundred students from Grades

probability of a student having a particular misconcep­

then pre-processed to give each student's results in

tion.

This diagnostic reasoning is typically reflected

terms of the

6

5

test item types;

and

6.

These were
were the

5,5,4,4,3,3

in a BN structure where the class, or "cause" is the

number of items of these type 1 to 6 respectively. The

parent of the "effect" (i.e. evidence) node. Therefore

particular form of the pre-processing depends on the

an arc was added from the subclass node to each of

item type values used: with the

the type nodes. No connections were added between

ues, a student's results might be 541233, whereas with

0

-

N type node val­

any of the type nodes, reflecting the experts' intuition

the H/M/L values, the same students results would be

that a student's answers for different item types are

represented as HHLMHH.

independent, given the subclassification.

The expert rule classifications were used to generate

A part of the expert elicited BN structure implemented

the priors for the sub-classifications.

in the ITS is shown in Figure 2 . This network fragment

of the test item types take the form of· P(Type

shows the coarseClass node (values L,S,A,UN), the

ValuejClassification

detailed misconception fineClass node (12 values),

the domain description, the experts expect particu­

X).

=

All the CPTs

=

As we have seen from

the item type nodes used for the DCT, plus additional

lar classes of students to get certain item types cor­

nodes for some games. These additional nodes are not

rect, and others wrong.

described in this paper but are included to illustrate

model the natural deviations from such "rules", where

the complexity of the full network.2 The balded nodes

students make a careless error, that is, they apply

are those in the restricted network used subsequently

their own logic but do not carry it through.

in this paper for evaluation and experimentation.

ample, students who are thinking according to the

However we do need to

For ex­

LWH misconception are predicted to get all
of Type 2 correct.
ability of

0.1

5

items

If however that there is a prob­

of a careless mistake on any one item,

the probability of a score of 5 is

(0.9)5,

and the prob­

ability of other scores follows the binomial distribu­

tion; the full vector for P(Type2JSubclass=LWH) is

(0.59,0.33,0.07,0.01,0.00,0.00)
When

the

item

type

(to two decimal places).

values

H/M/L

are

used,

the numbers are accumulated to give the vector
(0.92,0.08,0.00) for H, M and L. We note that the ex­
perts consider that this mistake probability is consid­
erably less than

0.1, say

of the order of 1-2%.

Much more difficult than handling the careless errors
in the well understood behaviour of the specific known
misconceptions, is to model situations where the ex­
perts do not know how a student will behave. This was
the case where the experts specified '.' for the classifi­
cations LU, SU, AU and UN in Table 1. We modelled the
expert not knowing what such a student would do on
the particular item type in the BN by using
Figure 2: Fragment of the expert elicited BN currently
implemented. Bold nodes are those discussed here.

50/50

0.5

(i.e.

that a student will get each item correct) with

the binomial distribution to produce the CPTs.
We ran experiments with different probabilities for

3.3

BN PARAMETERS

The education experts had collected data that con­
sisted of the test results and the expert rule classifi­
cation on a 24 item DCT for over two thousand five
2 An indication as to the meaning of these additional
nodes is as follows. The "HN" nodes relate to the Hidden
Numbers game, with evidence entered for the number of
doors opened before an answer was g iven, and a measure
of the "goodness of order" in opening doors. The root
node for the Hidden Number game subnet reflects a player's
game ability - in this case door opening "efficiency".

a single careless mistake

(pcm=0.03, 0.11

and

0.22),

with the CPTs calculated in this manner, to investi­
gate the effect of this parameter on the behaviour of
the system. These number were chosen to give a com­
bined probability for HIGH (for 5 items) of

0.99, 0.9

and 0.7 respectively, numbers that our experts thought
were reasonable. Results are described in Section
3.4

3.5.

BN EVALUATION P ROCESS

During the expert elicitation process we performed the
following three basic types of evaluation.

First was

NICHOLSON ET AL.

390

Case-based evaluation, where the experts "play" with
the net, imitating the response of a student with cer­
tain misconceptions and review the posterior distribu­
tions on the net.

Depending on the BN parameters,

it was often the case that while the incorporation of
the evidence for the

6

item types from the DCT test

data greatly increased the BN's belief for a particu­
lar misconception, the expert classification was not the
BN classification with the highest posterior, because it
started with a low prior. We found that it was useful to
the experts if we also provided the ratio by which each
classification belief had changed (although the highest
posterior is used in all evaluations).
During the use of the BN in the full ITS (see Figure 1),
each

Table

2:

UAI2001

Expert rule vs expert elicited BN classifica­

tion. Type node states 0-N, pcm=O.Il.
lwh lze lrv lu sdf srn su ate amo mis au un
lwh 386 0 0 0 0
0 0 0 0
0
0 0
lze
0 98 0 0 0 0 0
0 0 0 0
0
lrv
10 0 0 0 0 0 0
0 0 0 0
0
lu
6 9 0 54 0 0 0
0
0 0 0 6
sdf
0 0 0 0 83
0
0 0 0 0
0 4
srn
0 0 0 0 0 159 0
0 0 0 0
0
su
0 0 0 0 :2 22 40
3
0 0 0 2
ate
0 0 0 0 0 0 0 1050
0 0 0 0
amo
0 0 0 0 0 0 0
0
79 0 0 0
mis
0 0 0 0 0 0 0
0 6 0 0
0
au
9 0 0 0 0
0 0 1
8
0 0 63
un
43 6 0 15 35 14 11 119 26 2 0 66

time student answers are entered, the posteriors

for the fine classification are updated and in turn be­
come the new priors for that node; in this way, the
network adapts to the individual student over a range
of games and item types over time. This adaptive as­
pect allows the system to identify students with mis­
conceptions that are fairly infrequent in the overall
population. This motivated our Adaptiveness evalua­

tion, where the experts imitate repeated responses of
a student, update the priors after every test and enter
another expected test result. This detection of classi­
fications over repetitive testing built up the confidence
of the experts in the adaptive use of the BN.
Next, we undertook Comparison evaluation between
th e classifications of th e BN compared to the expert

rules on the DCT data.3

As well as a comparison

grid (see next subsection), we provided the experts
with details of the records where the BN classification
differed from that of the expert rules.

This output

proved to be very useful for the expert in order to
understand the way the net was working and to build
their confidence in the net.

have had on the overall behaviour. The iterative pro­
cess halts when the exp erts feel the behaviour of the
BN is satisfactory.

3.5

RESULTS

Table 2 is an example of the comparison grids for the
fine classification that were produced during the com­
parison evaluation phase. Similar grids were produced
for the coarse classification. Each row corresponds to
the expert rules classification, while each column cor­
responds to the BN classification, using the highest
posterior; each entry in the grid shows how many stu­
dents had a particular combination of classifications
from the two methods. The grid diagonals show those
students for whom the two classifications are in agree­
ment, while the "desirable" changes are shown in ital­

ics, and undesirable changes are shown in bold. Note

that we use the term "match" , rather than saying that
the BN classification was "correct", because the expert

Finally, we undertook a Prediction evaluation which
considers the prediction of student performance on in­
dividual item type nodes rather than direct miscon­
ception diagnosis. We enter a student's answers for

5

of the 6 item type nodes, then predict their answer for
the remruning one; this is repeated for each item type.
The number of correct predictions gives a meas ure of
the accuracy of each model, using a score of 1 for a
correct prediction (using the highest posterior) and 0
for an incorrect prediction. We also look at the pre­
dicted probability for the actual student answer. Both
measures are averaged over all students.

we modified the BN structure or the CPTs. This way
we were aware of the implications this change may

4

We performed these four types of evaluation every time
3We note that this evaluation is similar to the compar­
ison used in [van der Gaag et al., 2000].

4This evaluation method was suggested by an anony­
mous reviewer; the analysis of results using these predic­
tion measures is preliminary due to time constraints.

ule classification is not necessarily ideal.

r

Further assessment of these results by the experts re­
vealed that when the BN classification does not match
the expert rules classification, the misconception with
the second highest posterior often did match. The ex­
perts then assessed whether differences in the BN's

classification from the expert rules classification were
in some way desirable or undesirable, depending on
how the BN classification would be used. They came
up with the following general principles which provided
some general comparison measures: (1) it is desirable
for expert rule classified LUs to be re-classified as an­
other ofthe specific Ls, similarly for A Us and SUs, and
it was desirable for Us to be re-classified as an ything

else; (because this is dealing with borderline cases that

the expert rule really can't say much about); (2) it
is undesirable for

(a)

specific classifications (i.e. not

those involving any kind of

"U")

to change, because

UAI2001

Table

NICHOLSON ET AL.

Coarse classification grid, expert rules vs ex­

3:

pert elicited BN, varying
item type values

pcm (0.22, 0.11, 0.03),
(H/M/L and 0- N ) .

0-N
s

0.22
A

s

L
UN
0.11
A
s
L
UN
0.03
A
s
L
UN

A
L
UN
86.95% 12.56% 0.49%
1207
0
9
0
3
0
312
0
0
0
0
569
157
71
31
78
88.02% 11.12% 0.86%
1206
1
0
9
2
3
310
0
0
6
563
0
147
66
64
60

89.29% 9.48% 1.23'fo
1202
0
6
8
4
308
3
0
0
9
0
560
102
106
80
49

and

H[MJL
L
A
UN
S
87.61% 11.98% 0.41'fa
1213
0
3
0
4
1
0
310
557
0
2
0
45
150
60
82
87.28% 10.71% 2.01%
1184
23
9
0
1
0
4
310
5
7
557
0
49
139
76
73

91.63% 5.25% 3.12%
43
0
1173
0
0
11
304
0
22
547
0
0
209
36
83
9

391

Table 4: Fine classification summary

com p arison

ious models compared to the expert rules
Method

Type
values

Expert
BN

0-N

0-N
H/M/L

Avg
Avg
Avg
Avg

86.51
83.48

0-N
H/M/L

Avg
Avg

EBN
learned

24 DCT
0-N
H/M/L
0-N
H/M/L

CaMML
constr.

CaMML
uncons.

0.22
0.11
0.03
0.22
0.11
0.03

77.88
82.93
84.37
80.47
83.91
90.40
79.81
72.06
72.51
95.97
97.63

HfMJL
SNOB

Match

86.15
92.63

Des.
change
20.39
15.63
11.86
18.71
13.66
6.48
17.60
16.00
17.03
2.36
1.61
5.08
8.12

5.87
4.61

var­

Uncles.
change
1.72
1.44
3.78
0.82
2.42
3.12
2.49
11.94
10.46
1.66
0.75
8.41
8.34
7.92
2.76

ond, the expert elicited BN structure and parameters
the experts are confident about these classifications,

reflects both the experts' good understanding for the

and (b) for any classification to change to UN, be­

known fine classifications, and their poor understand­

cause this is in some sense throwing away information

ing of the behaviour of "U" students (LU, SU, AU,

(e.g. LU to UN loses information about the "1-like"

and UN). Finally, as discussed earlier, some classes are

behaviour of the students).

broken down into fine classifications more than others,

Table

3

shows the coarse classification comparison

grids obtained when varying the probability of a care­

resulting in lower priors, so the more common classifi­
cations (such as ATE and UN) tend to draw in others.

less mistake

(pcm=0.22, 0.11 and 0.03) and the item
(O-N vs H/M/L). Each grid is accompanied

Closer inspection also shows that some "undesirable"

type values

changes are reasonable. For example a student answer­

by the percentages for match, desirable and undesir­

ing 443322 is classified as an ATE by both the expert

able change.

rule and the H/M/L BN, since one mistake on any

As the probability decreases, the total

number of matches with expert classifications goes up,

item is considered "high". However the 0-N BN (for

due to more UN students being in agreement, how­
ever more L,S and A students no longer match, which

pcm=0.03) classifies the student as UN, since the com­
bined probability of 5 careless mistakes (one on each

is considered "undesirable" (see above). In effect, the

item type) is very low.

definition of A,S, and L becomes more stringent as the
probability of a careless error decreases, so more move
out of A, Land S into UN, and less move from UN into
A, L and S. T here are also shifts between undesirable
classification differences, for example the 8 A students
who the BN classifies as L (values

0-N, pcm=0.22)

(in

fact, highly offensive to our experts!), shift to the also
generally undesirable UN

(pcm=0.03).

It is not possible for reasons of space to present the
full set of results for the fine classifications. Table

4

(Set 1) shows a summary of the expert BN fine clas­
sification, varying the type values and probability of
careless mistake, in terms of percentage of matches
(i.e. on the grid diagonal), desirable changes and un­
desirable changes. We can see that matches are higher
for H/M/L than the corresponding 0-N run, desirable

We believe that the differences between the BN and

changes are lower, while there is no consistent trend

the expert rule classifications are due to the follow­

for undesirable changes. The undesirable change per­

ing factors. First, the expert rules give priority to the

centages are quite low, especially considering that we

type 1 and type

2 results, whereas the BN model gives
6 item types. An example of
a student with answers 450433, who the expert

know some of these can be considered quite justified.

equal weighting to all

Table 5 (Set

this is

Section

1) shows the two prediction measures (see
3.4), averaged over all predicted item types, for

rule classifies as AU due to the "High" result for item

all students. Both measures show the H/M/L model

with a fairly high chance of a careless mistake

(0.22)

ing O-N run. Both measures show the probability of

says this student looks like an LHW

as the

a careless error effects the results for the 0-N models,

only difference is the answer for item type 1, while for

but only the predicted probability shows an effect on

types 1 and 2 (ignoring the other answers ) . The BN

pcm=0.03,

(050433},

the BN classifies the student as UN. Sec-

giving better prediction results than the correspond­

the H/M/1 model results.

392

NICHOLSON ET AL.

Table 5: Accuracy of various models predicting stu­
dent item type answers.
Method

Expert
BN

Type
values

0-N
H/M/L

0.22
0.11
0.03
0.22
0.11
0.03
Avg
Avg

0.34
0.83
0.82
0.89
0.89
0.88
0.83
0.89

O-N
H/M/L

Avg

0.83
0.88

0-N

H/M/L
EBN
learned
CaMML
constr.
CaMML

uncons.

Avg Pred.
Accuracy

0-N

H/M/L

Avg
Avg

Avg

Avg Pred.

Prob.
0.34
0.53
0.70
0.69
0.80
0.83
0.74
0.83

0.83
0.89

0.72
0.79
0.74
0.83

Overall it is clear that the expert elicited network per­
forms a good classification of students misconceptions,
and captures well the different uncertainties in the ex­
perts domain knowledge. In addition, its performance
is quite robust to changes in parameters such as the
probability of careless mistakes or the granularity of
the evidence nodes.
4

KNOWLEDGE DISCOVERY

The next stage of the project involved the application
of certain automated methods for knowledge discovery
to the domain data.
4.1

CLASSIFICATION

The first aspect investigated was the classification of
decimal misconceptions. We applied the SNOB clas­
sification program[Wallace and Dowe, 2000], based on
the information theoretic Minimum Message Length
(MML). SNOB was run on the data from 2437 stu­
dents on 24 DCT items, each being a binary value as
to whether the student got the item correct or incor­
rect, with a variety of initial guesses for the number
of classes (5,10,15,20,30). All five classifications were
very similar; we present here results from the model
with the lowest MML estimate (5 initial classes). Us­
ing the most probable class for each member, we con­
structed a grid comparing the SNOB classification
with the expert rule classification. Of the 12 classes
produced by SNOB, we were able to identify 8 that
corresponded closely to the expert classifications (i.e.
had most members on the grid diagonal). Two classes
were not found (LRV and SU). Of the other 4 classes,
2 were mainly combinations of the AU and UN classi­
fications, while the other 2 were mainly UNs. SNOB
was unable to classify 15 students (0.6%). The per­
centages of match, desirable and undesirable change
are shown in Table 4 (set 2, row 1). They are compa­
rable with the expert BN 0-N and only slightly worse
than the expert BN H/M/L results.

UAI2001

SNOB was then run on the pre-processed data consist­
ing of student answers on the 6 item types (values 0-N
and H/M/1). The comparison results for this run were
not particularly good. For O,N type values, SNOB
found only 5 classes (32 students
1.3% not classi­
fied), corresponding roughly to some of the most pop­
ulous expert classes� 1WH, SDF, SRN, ATE and UN,
and subsuming the other expert classes. For H/M/1
type values, SNOB found 6 classes (33 students= 1.4%
not classified), corresponding roughly to 5 of the most
populous expert classes (1WH, SDF, SRN, ATE, UN),
plus a class that combined MIS with UN. In this case
1ZEs were all grouped with ATEs, as were AMOs. The
match results are shown in Table 4 (set 2, rows 2 and
3). Clearly, summarising the results of 24 DCT into
types gives relatively poor performance; it is proposed
that this is because many pairs of the classes are dis­
tinguished by student behaviour on just one item type,
and SNOB might consider these differences to be noise
within one class.
=

The overall good performance of the classification
method shows that automated knowledge discovery
methods may be useful in assisting expert identify suit­
able values for classification type variables.
4.2

PARAMETER S

Our next investigation was to learn the parameters for
the expert elicited network structure. The data was
randomly divided into five 80%-20% splits for training
and testing; the training data was used to parame­
terise the expert BN structures using the Netica BN
software's parameter learning feature5, while the test
data was given to the resultant BN for classification.
The match results (averaged over the 5 splits) for the
fine classification comparison of the expert BN struc­
tures (with the different type values, 0-N and H/M/L)
with learned parameters are shown in Table 4 (set 3),
with corresponding prediction results (also averaged
over the 5 splits) given shown in Table 5 (set 2).
The average prediction probabilities for the BN with
learned parameters are better than for the expert BNs
for the 0-N type values {0. 74 compared to 0. 70); the
other prediction results show no significant difference.
The match percentages for both BNs with learned pa­
rameters are significantly higher than for all the ex­
pert BNs with elicited parameters (with varying pcm),
while both the desirable and undesirable changes are
lower; most of the difference is due to the reduction in
desirable changes. Within the learned parameter re­
sults, the match percentage is significantly higher for
H/M/1 than 0-N, while the changes are lower. In both
cases, the percentage of undesirable changes is lower
than the desirable change. Clearly, learning the pa5ww.norsys.com

NICHOLSON ET AL.

UAI2001

393

rameters from data, if it is available, gives results that

The undesirable changes include quite a few shifts from

are much closer to the expert rule classification. The

one specific classification to another, which is particu­

trade-off is that the network no longer makes changes

larly bad as far as our experts are concerned; for exam­

to the various "U" classifications, i.e. it doesn't shift

ple, several of the networks do not identify the SDF

LUs, SUs, AUs and UNs into other classifications that

and MIS classifications, instead grouping them with

may be more useful in a teaching context. However it

ATE. We also note that the variation between the re­

does mean that expert input into the knowledge en­

sults for each data set 1-5 was much higher than for

gineering process can be reduced, with the parameter

the variation when learning parameters for the expert

learning on an elicited structure giving a BN model

BN structure. This no doubt reflects the difference be­

that can be used in the ITS.

tween the network structure learned for the different

4.3

tween the complexity of the learned network structures

splits. However we did not find a clear correlation be­
STRUCTURE

and their classification performance.
Our third investigation involved the application of
Causal MM1 (CaMM1) [Wallace and Korb,
learn network structure.

1999]

to

In order to compare the

learned structure with that of the expert elicited BN,
we decided to use the pre-processed

6

type data; each

program was given the student data for 7 variables
(the fine classification variable and the

6

item types),

with both the 0-N values and the H/M/1 values. The
same

5

random 80%-20% splits of the data were used

for training and testing. The training data was given
as input to the structural learning algorithm, and then

used to parameterise the result networks using Netica's
parameter learning method.

In seeking to improve automated discovery of struc­
ture by exploiting expert domain knowledge, experts
could provide constraints to guide the search and could
manually select for further investigation those alterna­
tive structures which were best interpretable in terms
of the domain concepts.

5

CON CLUSION S

This work began with the recognition that we had ac­
cess to a novel combination of data and information
which could enable the developments and compara­
tive studies reported above: a domain where student

We ran CaMM1 once for each split (a) without any

misconceptions abound; involvement of experts with a

constraints and (b) with the ordering constraint that

detailed understanding of the basis of the misconcep­

the classification should be an ancestor of each of

tions, and how they relate to domain specific activities;

This constraint reflects the gen­

and an extensi ve data set of student behaviour on test

the type nodes.

eral known causal structure.

Each run produced a

items in the domain.

The work reported here falls

slightly different network structure, with some hav­

into three components: (a) the development, by expert

ing the fineClass node as a root, some not.

elicitation, of a Bayesian network designed to be the

One

fairly typical network with the ordering constraint con­

"engine" of an adaptive tutoring system; an elicitation

tained 4 arcs from the class node to type nodes, with

strongly informed by the experts' detailed understand­

one type node also being a root node, only two type

ing of patterns in the data set; (b) a study of learn­

nodes leaves, and 10 arcs between type nodes.

ing techniques applied to the same data set - looking

The

arcs/nodes ratio of the learned structures varies from

at learning of classification, structure, and parameters

1.4 to 2.2, while the number of parameters varies from

- which could be compared against the experts' net­

about 700 to 144,000; the structures produced for the

work; and (c) some reflection, based on the experience

H/M/L data seem simpler using these measures, but

of working with the experts and the automated tools,

this is not statistically significant.

as to how elicitation and knowledge discovery might be

The percentage match results comparing the CaMML
BN classifications (constrained and unconstrained, O­

N and H/M/L) are also shown in Table 4 (sets 4 and

5), with the prediction results shown in Table 5 (sets
3 and 4). The prediction results for both 0-N and

combined in the BN knowledge engineering process.
A first, important, observation is that the automated
techniques were able to yield networks which gave
quantitative results comparable to the results from the
BN elicited from the experts. This level of matching

H/M/1 are similar to those of the fully elicited expert

provides a form of 'validation' of the learning tech­

BNs.

niques and suggests that automated methods can re­

The match percentages are similar to those of

the fully elicited expert BN for the 0-N, however the

duce the input required from domain experts. It also

undesirable change percentages are higher, while the

supports the reciprocal conclusion regarding the valid­

desirable change percentages are lower. For H/M/1,

ity of manual construction when there is enough expert

the match results are higher than for the expert BN

knowledge but no available data set. In addition, we

compared to the highest of 90.40), with fewer

have seen that the use of automated techniques can

(92.63

desirable and undesirable changes.

provide opportunities to explore the implications of

NI CHOLSON ET AL.

394

mo delli n g choices and to get a feel for design tradeoffs
- some examples of this were reported above in both

the initial eli cit ation stage, and the discovery stage
(e.g. 0-N vs. H/M/L) .

UA1 2001

networks and decision theory. Int. Journal of AI in

Education (to appear), 1 2 .

[Mcintosh et

al. ,

2000]

J.,

Mcintosh ,

S t acey,

De s ig ning

Tromp, C . , and Lightfoot , D . (2000) .

G iven that elicited BN was based on the expert knowl­

K.,

constructivist computer games for teaching about

.

J. B .

edge that had been accumulated over a period of time

decimal numbers .

through much analysis and investigation, how useful

t or , Mathematics Education Beyond 2000. (Proc. of

is an automated approach in domains where such de­

the 23rd A nnual Conf. of the Mathematics Educa­

t ailed (validation) knowledge is not available?

Our

experience suggests that a hybrid of expert and au­

tomated approaches is feas ib le.
these methods in a situation

We plan to apply

( st u d ent

work on alge­

bra) where we have data on student behaviour, but do
not have detailed prior exp ert analysis of the data.

knowledge the preliminary work undertaken by Elise
Dettman, an d thank Brent Boerlage for his assistance
with Netica and the anonymous reviewers for their
helpful s ugg e s t io ns .

modeling

C.,

Gertn er , A . , Van­
On-line stu­

for coached problem solving using

B ayesian Networks.

In UM97 - Proc. of the 6th

Int. Conf. on User Modeling, p ages 231-242.
[Druzdzel and van der Gaag, 200 1] Druzdzel,

M.

and

van der Gaag, L. (200 1 ) . Building probabilistic net­
works: Where do the numbers come from? G u est

editors int ro duct ion .

IEEE Trans. on Knowledge

and Data Engineering, 1 2 (4) :481-486.
[He cker man and G ei ger , 1995] Heckerman,

and

( 1 995) . Learning B ayesian networks: A

P.

In

and Hanks, S . , ed i t ors , UAI95 - Proc.

of the 1 1th Conf. on Uncertainty in A rtificial Intel­
ligence, pages 274-284, San Francisco.

and Chickering, D .

(1994) . L earning

Bayesian net­

works: the combination of knowledge and statisti­
In de Mantras, L. and Poole, D . , edi­

tors, UAI94 - Proc. of the 1 0th Conf. on Uncer­
tainty in Artificial Intelligence, pages 293-301 , San
Francisco.
[K en ne t t et al. , 2001] Ken n e tt ,
N ichol son , A. (200 1 ) .

Bayesian networks.

R.,

Korb,

K . , and

Seabreeze prediction using

In PAKDD '01 - Proc. of the

4th Pacific-Asia Conf. on Knowledge Discovery and
Data Mining, p ages
(M ayo and
(200 1 ) .

Dr uzd zel , M., and

rameters from small data sets: application of Noisy­

Working Notes of the Workshop on

"Bayesian and Causal networks: from inference to
data mining. " 12th European Conf. on Artificial in­
telligence

(ECAI-2000).

[S pir t e s et al. , 1993] Spirtes,

(1 993) .

P. ,

G lymo ur,

C . , and

Causation, Prediction and

148-153,

Mitrovic, 2001]

2001]

[St acey et al . ,

Stacey,

K . , Sonenberg, L . , Nicholson, A . , Boneh, T . , and
Steinle, V. (200 1 ) . Modelling teaching for concep­

tual change using a Bayesian network.

Submitted

to Int. Journal of AI in Education.
[Stacey and Steinle, 1 999] Stacey, K. and Steinle,
( 1 999 ) .

V.

A longitudinal study of childen's thin k­

ing about decimals: a preliminary analy s i s . In Za­
slavsky, 0 . ,

editor, Proc. of the 23rd Conf. of the

Int. Group for the Psychology of Mathematics Edu­
[van der Gaag et al. , 2000] van der Gaag, L., Renooij ,

S . , Ale m an ,

B. ,

and Taal, B . (2000) . Evaluation of

a probabilistic model for staging of oesophageal car­
cinoma. In Medical Infobahn for Europe: Proc. of
MIE2000 and GMDS2000, pages 772-776, Amster­

dam. !OS Press.

[Heckerman et al. , 1994] Hecker man , D., Geiger, D . ,

cal data.

O ni sko , A.,

cation, volume 4, pages 233-241 , Haifa. PME.

D.

unification for discret e and gaussian domains.
Besnard,

al. , 2000]

Wasyluk, H. (2000) . Learning Bayesian network p a­

Search. Number 8 1 in Lecture Notes in Statistics.

Lehn, K . , and Druzdzel, M . ( 1997) .

D.

[Onisko et

S pringe r Verlag.

[Conati et al. , 1997] Conati,

Geiger,

tion Research Group of Australasia), pages 409-4 1 6 .

Scheines, R.




the expeeted number of actions required to reach the
goal.

We describe a method for time-critical de­
cision making involving sequential tasks and
stochastic processes. The method employs
several iterative refinement routines for solv­
ing different aspects of the decision mak­
ing problem. This paper concentrates on
the meta-level control problem of delibera­
tion scheduling, allocating computational re­
sources to these routines. We provide dif­
ferent models corresponding to optimization
problems that capture the different circum­
stances and computational strategies for de­
cision making under time constraints. We
consider precursor models in which all deci­
sion making is performed prior to execution
and recurrent models in which decision mak­
ing is performed in parallel with execution,
accounting for the states observed during ex­
ecution and anticipating future states. We
describe algorithms for precursor and recur­
rent models and provide the results of our
empirical investigations to date.

We represent goals of aehievement in terms of an opti­
mal sequential decision making problem in which there
is a reward function specially formulated for a partie­
ular goal. For the goal of achieving p as quiekly as
possible, the reward is 0 for all states satisfying p and
-1 otherwise. The optimization problem is to find a
policy (a mapping from states to actions) maximiz­
ing the expected discounted cumulative reward with
respect to the underlying stochastic process and the
specially formulated reward function. In our formula­
tion, a policy is nothing more than a conditional plan
for achieving goals quickly on average.

Introduction

We are interested in solving sequential decision making
problems given a model of the underlying dynamical
system specified as a stochastic automaton (i.e., a set
of states, actions, and a transition matrix which we
assume is sparse ) . In the following, we refer to the
specified automaton as the system automaton. Our
approach builds on the theoretical work in operations
research and the decision sciences for posing and solv­
ing sequential decision making problems, but it draws
its power from the goal-directed perspective of artifi­
cial intelligence. Achieving a goal corresponds to per­
forming a sequence of actions in order to reach a state
satisfying a given proposition. In general, the shorter
the sequence of actions the better. Because the state
transitions are governed by a stochastic proeess, we
cannot guarantee the length of a sequenee achieving a
given goal. Instead, we are interested in minimizing

Instead of generating an optimal policy for the sys­
tem automaton, which would be impractical for an
automaton with a large state space, we formulate a
simpler or restrictert stochastic automaton and then
search for an optimal policy in this restricted automa­
ton. At all times, the system maintains a restricted au­
tomaton. The restricted automaton and correspond­
ing policy are improved as time permits by successive
refinement. This approach was inspired by the work
of Drummond and Bresina [Drummond and Bresina,
1990] on anytime synthetic projeC-tion.
The state space for the restricted automaton corre­
sponds to a subset ·of the states of the system au­
tomaton (this subset is called the envelope of the re­
stricted automaton) and a special state OUT that rep­
resents being in some state outside of the envelope.
For states in the envelope, the transition funetion of
the restricted automaton is the same as in the system
automaton. The pseudo state OUT is a sink (i.e., all
actions result in transitions back to OUT) and, for a
given action and state in the envelope, the probability
of making a transition to OUT is one minus the sum
of the probabilities of making a transition to the same
or some other state in the envelope.
There are two basic types of operations on the re­
stricted automaton. The first is called envelope al­
teration and serves to increase or decrease the num­
ber of states in the restricted automaton. The second
is called policy generation and determines a policy for

310

Dean et al.

b

i.

ii.

Figure 1: Stochastic process and a restricted version

the system automaton using the restricted automaton.
Note that, while the policy is constructed using the re­
stricted automaton, it is a complete policy and applies
to all of the states in the system automaton. For states
outside of the envelope, the policy is defined by a set of
reflexes that implement some default behavior for the
agent. In this paper, deliberation scheduling refers to
the problem of allocating processor ·time to envelope
alteration and policy generation.
There are several different methods for envelope al­
teration. In the first method, we simply search for
a (new) path or trajectory from the initial state to a
state satisfying the goal and add the states traversed in
this path to the state space for the restricted automa­
ton. This method need not make use of the current
restricted automaton. A second class of methods op­
erates by finding the first state outside the envelope
that the agent is most likely to transition to using its
current policy, given that it leaves the set of states
corresponding the current envelope. There are several
variations on this: add the state, add the state and the
n next most likely states, add all of the states in a path
from the state to a state satisfying the goal, add all of
the states in a path from the state to a state back in
the current envelope. Finally, there are methods that
prune states from the current envelope on the grounds
that the agent is unlikely to end up in those states
and therefore need not consider them in formulating a
policy.
Figure 1.i shows an example system automaton con­
sisting of five states. Suppose that the initial state is
1, and state 4 satisfies the goal. The path 1 � 2 � 4
goes from the initial state to a state satisfying the
goal and the corresponding envelope is {1, 2, 4}. Fig­
ure 1.ii shows the restricted automaton for that en­
velope. Let 1r( x) be the action specified by the pol­
icy 1r to be taken in state x; the optimal policy for
the restricted automaton shown in Figure l.ii is de­
fined by 1r ( 1) = 1r (2) = 1r (4) = a on the states of
the envelope and the reflexes by 1r(OUT)
b (i.e.,
'ifX (/_ {1 1 2 1 4} 1 1r( X) = b)·
=

All of our current policy generation techniques are
based on iterative algorithms such as value iteration
[Bellman, 1957] and policy iteration [Howard, 1960].
In this paper, we use the latter. These techniques can
be interrupted at any point to return a policy whose

value improves in expectation on each iteration. Each
iteration of policy iteration takes 0( IE13) where E is
the envelope or set of states for the restricted automa­
ton. The total number of iterations until no further
improvement is possible varies but is guaranteed to be
polynomial in lEI. This paper is primarily concerned
with how to allocate computational resources to enve­
lope alteration and policy generation. In the following,
we consider several different models.
In the simpler models called precursor-deliberation
models, we assume that the agent has one opportu­
nity to generate a policy and that, having generated
a policy, the agent must use that policy thereafter.
Precursor-deliberation models include
1. a deadline is given in advance, specifying when
to stop deliberating and start acting according to
the generated policy
2. the agent is given an unlimited amount of time to
respond, with a _linear cost of delay
There are also more complicated precursor­
deliberation models, which we do not address in this
paper, such as the following two models, in which a
trigger event occurs, indicating that the agent must
begin following its policy immediately with no further
refinement.
3. the trigger event can occur at any time in a fixed
interval with a uniform distribution
4. the trigger event is governed by a more compli­
cated distribution, e.g., a normal distribution cen­
tered on an expected time
In more complicated models, called recurrent­
deliberation models, we assume that the agent period­
ically replans. Recurrent-deliberation models include
1. the agent performs further envelope alteration
and policy generation if and only if it 'falls out'
of the envelope _defined by the current restricted
automaton
2. the agent performs further envelope alteration
and policy generation periodically, tailoring the
restricted automaton and its corresponding pol­
icy to states expected to occur in the near future
The rest of this paper assumes some familiarity
with basic methods for sequential decision making in
stochastic domains. A companion paper [Dean et
al., 1993] provides additional details regarding algo­
rithms for precursor-deliberation models. In this pa­
per, we dispense with the mathematical preliminaries,
and concentrate on conveying basic ideas and empir­
ical results. A complete description of our approach
including relevant background material is available in
a forthcoming technical report.
2

Deliberation Scheduling

In the previous section, we sketched an algorithm that
generates policies. Each policy 1r has some value with

Deliberation Scheduling for Time-Critical Sequential Decision Making

311

respect to an initial state x0; this value is denoted
V,.(x0) and corresponds to the expected cumulative
reward that results from executing the policy starting
in x0• Given a stochastic process and reward function,
V,.(x0) is well defined for any policy 1r and state x0.
We are assuming that, in time critical applications,
it is impractical to compute V,. (x0) for a given policy
and initial state and, more importantly, that it is im­
practical to compute the optimal policy for the entire
system automaton.
In order to control complexity, in generating a pol­
icy, our algorithm considers only a subset of the state
space of the stochastic process. The algorithm starts
with an initial policy and a restricted state space (or
envelope), extends that envelope, and then computes
a new policy. We would like it to be the case that the
new policy 1r1 is an improvement over (or at the very
least no worse than ) the old policy 1r in the sense that
V1r' (xo)- V ,.(xo) 2: 0 .
In general, however, we cannot guarantee that the pol­
icy will improve without extending the state space to
be the entire space of the system automaton, which
results in computational problems. The best that we
can hope for is that the algorithm improves in expecta­
tion. Suppose that the initial envelope is just the ini­
tial state and the initial policy is determined entirely
by the reflexes. The difference Vrr'(xo)- V1r(xo) is a
random variable, where 1r is the reflex policy and 1r' is
the computed policy. We would like it to be the case
that E[V1r'(x0)- V1r(x0)] > 0, where the expectation
is taken over start states and goals drawn from some
fixed distribution. Although it is possible to construct
system automata for which even this improvement in
expectation is impossible, we believe most moderately
benign navigational environments, for instance, are
well-behaved in this respect.
Our algorithm computes its own estimate of the value
of policies by using a smaller and computationally
more tractable stochastic process. Ideally, we wo'uld
like to show that there is a strong correllation be­
tween the estimate that our algorithm uses and the
value of the policy as defined above with respect to
the complete stochastic process, but for the time be­
ing we show empirically that our algorithm provides
policies whose values increase over time.
Our basic algorithm consists of two stages: envelope
alteration (EA) followed by policy generation (PG).
The algorithm takes as input an envelope and a policy
and generates as output a new envelope and policy.
We also assume that the algorithm has access to the
state transition matrix for the stochastic process. In
general, we assume that the algorithm is applied in
the manner of iterative refinement, with more than
one invocation of the algorithm. We will also treat en­
velope alteration and policy generation as separate, so
we east the overall process of poliey formation in terms
of some number of rounds of envelope alteration fol­
lowed by poliey generation, resulting in a sequenee of

Figure 2: Sequenee of restrieted automata and associ­
ated paths through state space
polieies. Figure 2 depicts a sequenee of automata gen­
erated by iterative refinement along with the associ­
ated paths through state spaee traversed in extending
the envelope.
Envelope alteration can be further classified in terms
of three basic operations on the envelope: trajectory
planning, envelope extension, and envelope pruning.
Trajectory planning eonsists of searching for some path
from an initial state to a state satisfying the goal. En­
velope extension consists of adding states to the enve­
lope. Envelope pruning involves removing states from
the envelope and is generally used only in recurrent­
deliberation models.
Let

1r;

represent the policy after the ith round and let

tEA; be the time spent in the ith round of envelope
alteration. We say that poliey generation is inflexi­
ble if the ith round of poliey generation is always run
to completion on IEil . Policy generation is itself an

iterative algorithm that improves an initial policy by
estimating the value of policies with respect to the re­
stricted stochastic. process mentioned earlier. W hen
run to eompletion, policy generation continues to iter­
ate until it finds a policy that it cannot improve with
respect to its estimate of value. The time spent on the
ith round of policy generation tpa, depends on the
size of the state space IEil .
In the following, we present a number of deeision mod­
els. Note that for each instance of the problems that
we eonsider, there is a large number of possible deci­
sion models. Our seleetion of which decision models to
investigate is guided by our interest in providing some
insight into the problems of time-critical deeision mak­
ing and our antieipation of the combinatorial problems
involved in deliberation scheduling.
3

Precursor Deliberation

In this section we consider the first precursor­
deliberation model, in which there is a fixed deadline
known in advance. It is straightforward to extend this
to model 2, where the agent is given an unlimited re­
sponse time with a Linear eost of delay; models :3 and
4 are more eomplicated and and are not eonsidered in
this paper.

312

3.1

Dean et al.

The Model

Let troT be the total amount of time from the current
time until the deadline. If there are k rounds of enve­
lope alteration and policy generation, then we have
tEA1 + tpa, +···+ tEAk+ tpak =trOT·
Case 1: Single round; inflexible policy genera­
tion In the simplest case, policy generation does not

inform envelope alteration and so we might as well do
all of the envelope alteration before policy generation,
and tEA, + tpa, = troT· Here is what we need in
order to schedule time for EA1 and PG1:
1. the expected value, taken over randomly-chosen
pairs of initial states and goals, of the improve­
ment of the value of the initial state, given a fixed
amount of time allocated to envelope alteration,
E[V1r, (xo)- V1ro(xo)itEA.Ji
2. the expected size of the envelope given the time
allocated to the first round of envelope alteration,
E[IE1IItEA,]; and
3. the expected time required for policy generation,
given the size of the envelope after the first round
of envelope alteration, E [tpa,IIE1I].
Note that, because policy generation is itself an
iterative refinement algorithm, we can interrupt
it at any point and obtain a policy, for instance,
when policy generation takes longer than pre­
dicted by the above expectation.
Each of (1), (2) and (3) can be determined empm­
cally, and, at least in principle, the optimal allocation
to envelope alteration and policy generation can be
determined.
Case II: Multiple rounds; inflexible policy gen­
eration Assume that policy generation can prof­

itably inform envelope alteration, i.e., the policy after
round i provides guidance in extending the environ­
ment during round i +1. In this case, we also have k
rounds and tEA, +tpa, +···+tEAk+ tpak =troT·

Informally, let the fringe states for a given envelope
and policy correspond to those states outside the enve­
lope- that can be reached with some probability greater
than zero in a single step by following the policy start­
ing from some state within the envelope. Let the most
likely falling-out state with respect to a given envelope
and policy correspond to that fringe state that is most
likely to be reached by following the policy starting
in the initial state. We might consider a very simple
method of envelope alteration in which we just add the
most likely falling-out state and then the next most
likely and so on. Suppose that adding each additional
state takes a fixed amount of time. Let

E[V1r; (xo)- V1r;_, (xo)IIE;-11 = m, IE;I = m + n]
denote the expected improvement after the ith round
of envelope alteration and policy generation given that

there are n states added to the m states already in the
envelope after round i - 1.
Again, the expectations described above can be ob­
tained empirically. Coupled with the sort of expecta­
tions described for Case I (e.g., E [tPa;IIE;I] ) , one
could (in principle) determine the optimal number
of rounds k and the allocation to tEA; and tpa; for
1 � j � k . In practice, we use slightly different statis­
tics and heuristic methods for deliberation scheduling
to avoid the combinq.torics.
Case III: Single round: flexible policy genera­
tion Actually, this case is simpler in concept than

Case I, assuming that we can compile the following
statistics.
Case IV: Multiple round: flexible policy gener­
ation Again, with ;tdditional statistics, e.g.,

E[V1r;(xo)-V1r;_, (xo)IIE;-11 = m, IE;I = m+n, tpa;_.],
this case is not much more difficult than Case II.
3.2

Algorithms and Experimental Results

Our initial.experiments are based on stochastic au­
tomata with up to several thousand states; automata
were chosen to be small enough that we can still
compute the optimal policy using exact techniques
for comparison, but large enough to exercise our ap­
proach. The domain, mobile-robot path planning, was
chosen so that it would be easy to understand the poli­
cies generated by our algorithms. For the experiments
reported here, there were 166 locations that the robot
might find itself in and four possible orientations re­
sulting in 664 states. These locations are arranged on
a grid representing the layout of the fourth floor of the
Brown University Computer Science department. The
robot is given a tasK to navigate from some starting
location to some target location. The robot has five ac­
tions: stay, go forward, turn right, turn left, and turn
about. The stay action succeeds with probability one,
the other actions succeed with probability 0.8, except
in the case of sinks corresponding to locations that
are difficult or impossible to get out of. In the mobile­
robot domain, a sink might correspond to a stairwell
that the robot could fall into. The reward function
for the sequential des_:ision problem associated with a
given initial and target location assigns 0 to the four
states corresponding to the target location and -1 to
all other states.
We gathered a variety of statistics on how extend­
ing the envelope increases value. The statistics that
proved most useful corresponded to the expected im­
provement in value for different numbers of states
added t"o the envelope. Instead of conditioning just on
the size of the envelope prior to alteration we found it
necessary to condition on both the size of the envelope
and the estimated value of the current policy (i.e., the

Deliberation Scheduling for Time-Critical Sequential Decision Making

value of the optimal policy computed by policy itera­
tion on the restricted automaton). At run time, we use
the size of the automaton and the estimated value of
the current policy to index into a table of performance
profiles giving expected improvement as a function of
number of states added to the envelope. Figure 3 de­
picts some representative functions for different ranges
of the value of the current policy.

313

Value
10.00
s.oo
6.00

-+--r
--·--=s=·-:t""'"'--+-...,---+---r - hCXi6ie-----·
=--··
-+--+---n------t----1---t---1
---t
..

.J
'

-+-1--i+----+----l--t-- -----t--

r'
-+----t----'j-+-,1---++---+1
2.00 -+-1---i-+---+--+----t----'j­
..�
/
0,00 -fl--L--1----+--'--t------t--

4.00

0.00

2.00

4.00

6.00

8.00

Time (secoo.ds}

Figure 4: Comparison of the greedy algorithm with
standard (inflexible) policy iteration and interruptable
(flexible) policy iteration

10.00

20.00

30.00

40.00

Figure 3: Expected improvement as a function of the
number of states n added to initial envelope of size m
In general, computing the optimal deliberation sched­
ule for the multiple-round precursor-deliberation mod­
els described above is computationally complex. We
have experimented with a number of simple, greedy
and myopic scheduling strategies; we report on one
such strategy here.
Using the mobile-robot domain, we generated 380,000
data points to compute statistics of the sort shown in
Figure 3 plus estimates of the time required for one
round of envelope alteration followed by policy gen­
eration given the size of the envelope, the number of
states added, and value of the current policy. We use
the following simple greedy strategy for choosing the
number of states to add to the envelope on each round.
For each round of envelope alteration followed by pol­
icy generation, we use the statistics to determine the
number of states which, added to the envelope, max­
imizes the ratio of performance improvement to the
time required for computation. Figure 4 compares the
greedy algorithm with the standard (inflexible) pol­
icy iteration on the complete automaton and with an
interruptable (flexible) version of policy iteration on
the complete automaton. The data for Figure 4 was
determined from one representative run of the three
algorithms on a particular initial state and goal. In
another paper [ Dean et al., 1993] we present results
for the average improvement of the start state under
the policy available at time t as a function of time.
4
4.1

Recurrent Deliberation
The Model

In recurrent-deliberation models, the agent has to re­
peatedly decide how to allocate time to deliberation,
taking into account new information obtained during
execution. In this section, we consider a particular

model for recurrent deliberation in which the agent al­
locates time to deliberation only at prescribed times.
We assume that the agent has separate deliberation
and execution modules that run in parallel and com­
municate by message passing; the deliberation module
sends policies to the execution module and the execu­
tion module sends observed states to the deliberation
module. We also assume that the agent correctly iden­
tifies its current state; in the extended version of this
paper, we consider the case in which there is uncer­
tainty in observation.
We call the model considered in this section the dis­
crete, weakly-coupled, recurrent deliberation model. It
is discrete because each tick of the clock corresponds to
exactly one state transition; recurrent because the exe­
cution module gets a new policy from the deliberation
module periodically; weakly coupled in that the two
modules communicate by having the execution mod­
ule send the deliberation module the current state and
the deliberation module send the execution module the
latest policy. In this section, we consider the case in
which communication between the two modules occurs
exactly once every n ticks; at times n, 2n, 3n, . . ., the
deliberation module sends off the policy generated in
the last n ticks, recei�es the current state from the ex­
ecution module, and begins deliberating on the next
policy. In the next section, we present an algorithm for
the case where the interval between communications is
allowed to vary.
In the recurrent models, it is often necessary to remove
states from the envelope in order to lower the compu­
tational costs of generating policies from the restricted
automata. For instance, in the mobile-robot domain,
it may be appropriafe to remove states corresponding
to portions of a path the robot has already traversed
if there is little chance of returning to those states. In
general, there are many more possible strategies for
deploying envelope alteration and policy generation in
recurrent models than in the case of precursor mod­
els. Figure 5 shows a typical sequence of changes to
the envelope corresponding to the state space for the
restricted automaton. The current state is indicated

314

Dean et al.

Find path to the goal

�
Extend the envelope

Extend and then prune the envelope

�·
0

Find path back to the

interval, the execution module is given a new policy 11"1,
and the deliberation module is given the current state
x'. It is possible that x' is not included in the enve­
lope for 11"1; if the reflexes do not drive the robot inside
the envelope then the agent's behavior throughout the
next n-tick interval will be determined entirely by the
reflexes. Figure 6 shows a possible run depicting inter­
vals in which the system is executing reflexively and
intervals in which it is using the c.urrent policy; for this
example, we assume_reflexes that enable an agent to
remain in the same state indefinitely.

Let 8n (x,1r, x') be the probability of ending up in x'
starting from x and following 1r for n steps. Suppose
that we are given a set of strategies {F1,F2, }. As
Extend and then prune the envelope
is usual in such combinatorial problems with indefi­
nite horizons, we adopt a myopic decision model. In
particular, we assume that, at the beginning of each
n-tick interval, we are planning to follow the current
policy 1r for n steps, .follow the policy F(1r) generated
Figure 5: Typical sequence of changes to the envelope
by some strategy F attempting to improve on 1r for the
next n steps, and thereafter follow the optimal policy
intervals during which the system is executing reflexively
7r*. If we assume that it is impossible to get to a goal
0
3n
4n
n
2n
state in the next 2n steps, the expected value of using
strategy F is given by
falls out�ofthe envelo
2n-l
n
curre nt state happens tt be contaied in the new envelo
Z:-l+i2 L 8n (x,7r,x1) L8n (x',F (7r),x")V.,.. (x") ,
.

�

I

falls out of the envelope again

+

r

ctuTent state is not in the new envelope

'

current state is in the new envelope

Figure 6: Recurrent-deliberation
by + and the goal state is indicated by D.
To cope with the attendant combinatorics, we raise
the level of abstraction and assume that we are given
a small set of strategies that have been determined
empirically to improve policies significantly in vari­
ous circumstances. Each strategy corresponds to some
fixed schedule for allocating processor time to envelope
alteration and policy generation routines. Strategies
would be tuned to a particular n-tick deliberation cy­
cle. One strategy might be to use a particular pruning
algorithm to remove a specified number of states and
then use whatever remains of the n ticks to generate
a new policy. In this regime, deliberation scheduling
consists of choosing which strategy to use at the begin­
ning of each n-tick interval. In this section, we ignore
the time spent in deliberation scheduling; in the next
section, we will arrange it so that the time spent in
deliberation scheduling is negligible.
Before we get into the details of our decision model,
consider some complications that arise in recurrent­
deliberation problems. At any given moment, the
agent is exec.uting a polic.y, call it 1r, defined on the cur­
rent envelope and augmented with a set of reflexes for
states falling outside the envelope. The agent begins
exec.uting 1r in state x. At the end of the c.urrent n-tick

x'EX

i=O

.

•

[

x"EX

l

where 0 <= 1 < 1 i& a discounting factor, controlling
the degree of influence of future results on the current
decision.
Extending the above model to account for the possi­
bility of getting to the goal state in the next 2n steps
is straightforward; computing a good estimate of v.,..
is not, however. We might use the value of some pol­
icy other than 7r*, but then we risk choosing strategies
that are optimized to support a particular suboptimal
policy when in fact. the agent should be able to do
much better. In general, it is difficult to estimate the
value of prospects beyond any given limited horizon
for sequential decision problems of indefinite duration.
In the next section, we consider one possible practical
expedient that appears to have heuristic merit.
4.2

Algorithms and Experimental Results

In this section, we present a method for solving
recurrent-deliberation problems of indefinite duration
using statistical estimates of the value of a variety of
deliberation strategies. We deviate from the decision
model described in the previous sec.tion in one addi­
tional important way; we allow variable-length inter­
vals for deliberation. Although fixed-length facilitate
exposition, it is much easier to collect useful statistical
estimates of the utility of deliberation strategies if the
deliberation interval is allowed to vary.
For the remainder of-this section, a deliberation strat­
egy is just a particular sequence of invocations of enve­
lope alteration and policy generation routines. Delib-

Deliberation Scheduling for Time-Critical Sequential Decision Making

eration strategies are parameterized according to at­
tributes of the policy such as the estimated value of
policies and the size of the envelopes. The function
EIV (F,V11, IE11l) provides an estimate of the expected
improvement from using the strategy F assuming that
the estimated value of the current policy and the size
of the corresponding envelope fall within the speci­
fied ranges. This function is implemented as a table in
which each entry is indexed by a strategy F and a set of
ranges, e.g., { [minV11,maxV,.], [miniE,.I,maxiE111]}.

315

online deliberation scheduling.

We determine the EIV function off line by gathering
statistics for F running on a wide variety of policies.
The ranges are established so that, for values within
the specified ranges the expected improvements have
low variance. At run time, the deliberation scheduler
computes an estimate of the current policy V,., deter­
mines the size IE,.. I of the corresponding envelope and
chooses the strategy F maximizing EIV (F, V,., IE111).

To avoid complicating the online decision making, we
have adopted the following expedient which allows us
to keep our one-step-lookahead model. We modify the
transition probabilities for the restricted automaton so
that there is always a non-zero probability of getting
back into the envelope having fallen out of it. Exactly
what this probability should be is somewhat eompli­
cated. The particular value chosen will determine just
how concerned the agent will be with the prospect of
falling out of the envelope. In fact, the value is depen­
dent on the actual strategies chosen by deliberation
scheduling which, in our particular case, depends on
EIV and this value of falling back in. We might pos­
sibly resolve the circularity by solving a large and very
complicated set of simultaneous equations; instead, we
have found that in practice it is not difficult to find a
value that works reasonably well.

To build a table of estimates of function EIV off line,
we begin by gathering data on the performance of
strategies ranging over possible initial states, goals,
and policies. For a particular strategy F , initial state
x, and policy 1r, we run F on 1r, determine the elapsed
number of steps k, and compute estimated improve­
ment in value,

The experimental results for the recurrent model were
obtained on the mobile-robot domain with 1422 possi­
ble locations and hence 5688 states. The actions avail­
able to the agent were the same as those used to obtain
the precursor-model-results. The transition probabil­
ities were also the same, except that the domain no
longer contained sinks.

[-�

'Yi

+ 'Yk

x� 8k(x,

71",

]

x')VF(1r)(x') - V,.(x),

where the first term corresponds to the value of using
1r for the first k steps and F (1r) there after and the
second term corresponds to the case in which we do
no deliberation whatsoever and use 1r forever. As in
the model described in the previous section, we assume
that the goal cannot be reached in the next k steps;
again it is simple to extend the analysis to the case in
which the goal may be reached in less than k steps.
Given data of the sort described above, we build the
table for EIV (F,V,., IE,..I ) by appropriately dividing
the data into subsets with low variance.
One unresolved problem with this approach is exactly
how we are to compute V11 ( x). Recall that 1r is only
a partial policy defined on a subset of X augmented
with a set of reflexes to handle states outside the cur­
rent envelope. In estimating the value of a policy, we
are really interested in estimating the value of the aug­
mented partial policy. If the reflexes kept the agent in
the same place indefinitely, then as long as there was
some nonzero probability of falling out of the envelope
with a given policy starting in a given state the actual
value of the policy in that state would be 1/ ( 1 1).
Of course, this is an extremely pessimistic estimate for
the long term value of a particular policy since in the
recurrent model the agent will periodically compute a
new policy based on where it is in the state space. The
problem is that we cannot directly account for these
subsequent policies without extending the horizon of
the myopic decision model and absorbing the associ­
ated computational costs in offline data gathering and
-

-

We used a set of 24 hand-crafted strategies, which were
combinations of envelope optimization (a) and the
following types of envelope alteration;
1. findpath (FP): if the agent's current state X cur
is not in the envelope, find a path from Xcur to a
goal state, and add this path to the envelope
2. robustify (R[N]): we used the following heuris­
tic to extend the envelope: find the N most l�kely
fringe states that the agent would fall out of the
envelope into, and add them to the envelope
3. prune (P[N]): of the states that have a worse
value than the current state, remove the N least
likely to be reached using the current policy.
Each of the strategies used began with findpath and
ended with optimization. Between the first and last
phases, robustification, pruning and optimization were
used in different combinations, with the number of
states to be added or deleted E {10, 20, 50, 100}; exam­
ples of the strategies we used are {FP R[10] a}, {FP
P[20] a}, {FP P[20] R[50] o}, {FP R[100] P[50]
0}, {FP R[50] a P[50] 0}.
We collected statistics over about 4000 runs generat­
ing 100,000 data points for strategy execution. The
start/goal pairs were, chosen uniformly at random and
we ran the simulated robot in parallel with the plan­
ner until the goal was reached. The planner executed
the following loop: choose one of the 24 strategies uni­
formly .at random, execute that strategy, and then pass
the new policy to the simulated robot. We found the
following conditioning variables to be significant: the
envelope size, lEI, the value of the current state V,.,
the "fatness" of the envelope (the ratio of envelope

316

Dean et al.

size to fringe size), and the Manhattan distance, M,
between the start and goal locations. We then build
a lookup table of expected improvement in value over
the time the strategy takes to compute, 8V11' / k, as a
function of E, V11', the fatness, M and the strategy s.
To test our algorithm, we took 25 pairs of start and
goal states, chosen uniformly at random from pairs of
Manhattan distance less than one third of the diameter
of the world. For each pair we ran the simulated robot
in parallel with the following deliberation mechanisms:
• recurrent-deliberation with strategies chosen us­
ing statistical estimates of EIV (LOOKUP)
• dynamic programming policy iteration over the
entire domain, with a new policy given to the
robot after each iteration (ITER) and only after
it has been optimized ( wHOLE )
The average number of steps taken by LOOKUP,
and WHOLE were 71, 87 and 246 respectively

ITER

W hile the improvement obtained using the recurrent­
deliberation algorithm is only small it is statistically
significant. These preliminary results were obtained
when there were still bugs in the implementation, how­
ever, since we have determined that the strategies are
in fact being pessimistic, we expect to obtain further
performance improvement using LOOKUP. Recall also
that we are still working in the comparatively small
domain necessary to be able to compute the optimal
policy over the whole domain; for larger domains, ITER
and WHOLE are computationally infeasible.
5

Related Work and Conclusions

Our primary interest is in applying the sequential de­
cision making techniques of Bellman [Bellman, 1957]
and Howard [Howard, 1960] in time-critical applica­
tions. Our initial motivation for this research arose
in attempting to put the anytime synthetic projec' ­
tion work of Drummond and Bresina [Drummond and
Bresina, 1990] on more secure theoretical foundations.
The approach described in this paper represents a
particular instance of time-dependent planning [Dean
and Boddy, 1988] and borrows from, among others,
Horvitz' [Horvitz, 1988] approach to flexible compu�
tation. Hansson and Mayer's BPS (Bayesian Problem
Solver) [Hansson and Mayer, 1989] supports general
state space search with decision theoretic control of in­
ference; it may be that BPS could be used as the basis
for envelope alteration. Boddy [Boddy, 1991] describes
solutions to related problems involving dynamic pro­
gramming. For an overview of resource-bounded de­
cision making methods, see chapter 8 of the text by
Dean and Wellman [Dean and Wellman, 1991].
We have presented an approach to coping with un­
certainty and time pressure in decision making. The
approach lends itself to a variety of online computa­
tional strategies, a few of which are described in this
paper. Our algorithms exploit both the goal-directed,

state-space search methods of artificial intelligence and
the dynamic programming, stochastic decision making
methods of operations research. Our empirical results
demonstrate that it is possible to obtain high perfor­
mance policies for large stochastic processes in a man­
ner suitable for time critical decision making.
Acknowledgements

Tom Dean's work was supported in part by a Na­
tional Science Foundation Presidential Young Investi­
gator Award IRI-8957601, by the Advanced Research
Projects Agency of the DoD monitored by the Air
Force under Contract No. F30602-91-C-0041, and by
the National Science foundation in conjunction with
the Advanced Research Projects Agency of the DoD
under Contract No. IRI-8905436. Leslie Kaelbling's
work was supported· in part by a National Science
Foundation National Young Investigator Award IRI9257592 and in part by ONR Contract N00014-914052, ARPA Order 8225. Thanks also to Moises Lejter
for his input during the development and implementa­
tion of the recurrent deliberation model.



The trajectory of a robot is monitored in a
restricted dynamic environment using light
beam sensor data. We have a Dynamic Belief
Network (DBN), based on a discrete model
of the domain, which provides discrete mon­
itoring analogous to conventional quantita­
tive filter techniques. Sensor observations are
added to the basic DBN in the form of specific
evidence. However, sensor data is often par­
tially or totally incorrect. We show how the
basic DBN, which infers only an impossible
combination of evidence, may be modified to
handle specific types of incorrect data which
may occur in the domain. We then present
an extension to the DBN, the addition of an
invalidating node, which models the status of
the sensor as working or defective. This node
provides a qualitative explanation of incon­
sistent data: it is caused by a defective sen­
sor. The connection of successive instances of
the invalidating node models the status of a
sensor over time, allowing the DBN to handle
both persistent and intermittent faults.

1

IN T RODUCTION

A robot vehicle is to be monitored as it executes a se­
quence of tasks against a schedule. People and other
robots cross its path, so the schedule is not strictly
adhered to. On occasions the robot fails; it is late
arriving at its next port of call, or it turns left in­
stead of right. In the current application we use data
from a simple sensor, a light beam sensor, which sig­
nals when some object crosses it. Other sensors are
available also for conventional (quantitative) control
and will be incorporated later into our framework for
discrete probabilistic monitoring.
The conventional q uantitative approach to such a
tracking problem is to use a controller such as a
Kalman Filter (Bar-Shalom and Fortmann, 1988),
which is based on the cycle: predict state, measure

(i.e. sense), update state estimate. Such quantitative
methods are inadequate for handling the gross changes
that are the focus of our work, as they are restricted
to reporting ever larger covariances. Light beam sen­
sors provide coarse, comparatively sparse data about
movement in an environment, which are not suited to a
conventional quantitative treatment. A symbolic rep­
resentation of change is more informative, as we apply
probabilistic reasoning techniques to monitoring gross
changes.
Belief Networks (Pearl, 1988) integrate a mechanism
for inference under uncertainty with a secure Bayesian
foundation. Belief networks have been been used
in various applications, such as medical diagnosis
(Spiegelhalter et al., 1989) and model-based vision
(Levitt et al., 1989), which initially were more static,
i.e. essentially the nodes and links do not change over
time. Such approaches involve determining the struc­
ture of the network; supplying the prior probabilities
for root nodes and conditional probabilities for other
nodes; adding or retracting evidence about nodes; re­
peating the inference algorithm for each change in evi­
dence. There has also been work on the dynamic con­
struction of belief networks (Breese, 1989) (C harniak
and Goldman, 1989), but the desired output is still a
single static network. Only recently have a few re­
searchers used belief networks in dynamic domains,
where the world changes and the focus is reasoning
over time. Such dynamic applications include robot
navigation and map learning based on temporal belief
networks (Dean and Wellman, 1991) and monitoring
diabetes (Andreassen et al., 1991). For such applica­
tions the network grows over time, as the state of each
domain variable at different times is represented by a
series of nodes. These dynamic networks are Marko­
vian, which constrains the state space to some extent,
however it is also crucial to limit the history being
maintained in the network. We have developed such
a dynamic belief network for discrete monitoring us­
ing light beam sensor data (Nicholson, 1992) which we
briefly describe in Section 2.
Sensor data may be noisy or incorrect. In Section 3
we review how conventional quantitative methods val­
idate sensor data and reject incorrect data, then de-

208

Nicholson and Brady

scribe the types of incorrect data which may occur
in the domain. In Section 4 we show how the basic
DBN, which infers only an impossible combination of
evidence, may be modified to handle (and implicitly re­
ject) specific types of incorrect data. We then present
an extension to the DBN in Section 5 which provides
a qualitative explanation of inconsistent data being
caused by a defective sensor, allowing us to model ei­
ther intermittent or persistent faults.

2
2.1

THE DOMAIN
THE DISCRETE SPATIAL AND
TEMPORAL MODEL

The environment (a laboratory in which a robot ve­
hicle roams) is divided into regions by the light beam
sensors. Without significant loss of generality, we re­
strict attention initially to rectangular regions. We
monitor moving objects, which may be robots or peo­
ple. An object's position is given by the region it is
believed to be in. Each light beam sensor provides data
about a light beam sensor crossing (BC): the direction
of the crossing, and the hegin and tend time points for
the time interval over which it occurred. The tempo­
ral representation is a time line divided at irregular
intervals by the tbegin/tend time points. The time in­
tervals between observation data, during which there
is no change in the world state, are labelled T0, T1,
... T;, T;+l, etc. (We also refer to successive intervals
as T and T+l.) The discrete trajectory for an object
is a sequence of region/time interval pairs (R, T). The
heading of an obj ect in a given region indicates from
which direction it entered (i.e. one of N, S, E, W).
We also assume that we have a model of the object's
mobility, the tendency of an object to move. This is
a function over time of the speed of the object, the
spatial layout, the type of object, and so on, and gives
us the probability that it will move at time instant t,
which can be projected onto the time interval T.
2.2

THE DBN MODEL

This discrete spatial and temporal model of the do­
main may be represented by the discrete valued nodes
in the DBN. For now we make the reasonable prac­
tical assumption that the environment is closed and
that the number of objects, N, in the environment,
is known and fixed; the more general case is not sig­
nificantly more complex but involves dynamic mod­
ification of the network structure. The spatial lay­
out of regions and sensors is fixed and known; let us
suppose that we have M regions and P sensors. Ta­
ble 2.1 gives a summary of the types of nodes, their
states, and their function in the network. The world
nodes are those that represent the world state space:
object position (OBJ), heading (HEAD) and mobility
(MOTION}, and region occupancy information (#R).
Nodes are time-stamped with the time interval, T;,
over which they apply and during which the world

Table
Node
UHJ;fl')

HEAD;(T)
MOTION;(T)
#RAT)

1:

DBN Node Types

:States and l''unctton
r1 .r2

rM

• . , .

Position of object i at timeT
u,hN,hs,hE,hw
Heading of object i: u = unknown;

stat, aoye

Mobility of object i;
for stationary

stat short
0, 1, ...

n

Region occupancy - number only:
for i, region j contains i objects

HC-Otl::i;

BC-ACT;

nc,dir1,dir2

crossing data from sensor i:
nc indicates no crossing,
dir1, dir2: the possible directions

nc, dir1 ,dir2,both
Actual crossings of LB i;
both represents objects crossing
in both directions

does not change. Observation nodes are those rep­
resenting the sensor crossings: a node for the crossing
data provided by the sensor (BC-OBS), and a node
representing the actual physical crossing of the whole
sensor which occurred {BC-ACT). We make this dis­
tinction between actual and observed because objects
may cross a sensor in both directions during the ob­
servation data time interval, T BC, however the sensors
only detect a single directional crossing. The proba­
bility distribution (PD) for BC-OBS., is:

P(BC-OBS;=dirl I BC-ACT; = dirl)
P(BC-OBS;=dir2 IBC-ACT; :::: dir2)
P(BC-OBS ;=dir2 IBC-ACT; = both)
P(BC-OBS;=dir1 IBC-ACT; =both)=
P(BC-OBS;=nc IBC-ACT; nc)
1
=

=
=

=

1
1
0.5
0.5

=

During any given time interval T when nothing has
changed, there will be N object position, heading, and
mobility nodes, and M region nodes. If there are P
light beam sensors, a sensor crossing will generate P
actual crossing (BC-ACT) and observed signal (BC­
OBS) nodes.

.

..._,

ITTTl
L::.J.:.L.LJ

Figure 1: (a) General expansion of the network; (b)
Example scenario used throughout this paper.
The dynamic construction of a network combines the
world model (movement of objects between regions)
and the observation model (the light beam sensor data
which is generated) as the network grows over time (see
Figure l(a)). The network expansion and inference

Sensor Validation Using Dynamic Belief Networks

algorithm is:
1.

2.

3.
4.
5.
6.

since they are non-linear.

Make new instiUlces of world nodes
(OBJ, HEAD, MOTION, R) for the T+l interV&l.
Connect old {T) &nd new (T+l) world nodes.
Create new observation nodes (BC-ACT, BC-OBS).
Connect world iUld observation nodes.
Add da.ta. as evidence for obs. nodes (BC-OBS).
Run inference algorithm to update beliefs.

If step 5 is omitted then the predictions made by the
network corresponds to a prediction of the position
of an object dependent only on its previous position
and its mobility. If the sensor crossing data is added 88
evidence, then the inference gives an updated estimate
of the object position at time interval T+1, and may
also change beliefs about any node in the network,
including those before time intervalT.
The DBN is multiply-connected, requiring compli­
cated inference algorithms, such as conditioning or
clustering (Pearl, 1988). The problem of inference for
such networks is NP-hard (Cooper, 1990), however im­
proved algorithms such as (Jensen et al., 1990) have
made inference in carefully structured networks feasi­
ble (Andreassen et al., 1987). The DBN as described
gives us an inference engine which infers alternative
world models (the position of object in regions) with
associated probabilities, from both the model of object
motion (the prior probabilities for the objects' mobil­
ity) and the the sensor crossing data (the observation
model). (Nicholson, 1992) provides more details.
The example scenario, shown in Figure 1(b) used
throughout this paper is a linear arrangement of 4 re­
gions, 3 light beam sensors containing one object. The
methods described in this paper also apply to multiple
objects and other divisions of the environment, includ­
ing a grid of sensors.

A previous paper (Nicholson and Brady, 1992) shows
how the DBN may be extended to maintain a limited
history of the movement of the object. This provides a
solution to the data association problem (DAP), that
of deciding which object has given rise to which ob­
servation. Quantitative solutions to the DAP include
certain techniques for handling observations which do
not fall within the validation regions. One method is
to discard them as "clutter", which is sometime called
a false alarm (Bar-Shalom and Fortmann, 1988). An
alternative is to initiate a new track (and hence filter)
for such an observation and discontinue it after a cer­
tain time if no further data supports this hypothesis
of a new object.
In some quantitative methods track continuation (Bar­
Shalom and Fortmann, 1988p. 255} is done to handle
missing data. If the validation region is empty, the
track is extrapolated. If a predetermined number of
subsequent validation regions in a row are also empty,
the track is dropped.
3.2

Incorrect light beam sensor data may be classified
fol1ows:

3.1

a sensor crossing is signaled but in
fact never took place, a fa]se positive. This corre­
sponds to clutter, noise or general false alarms in
quantitative methods.

2.

Wrong Direction Data: a beam is broken, but the
signaled direction of crossing is incorrect. The
sensor data is inaccurate, rather than completely
wrong; the sensor is certainly malfunctioning.

3.

Missing Data: an object moves from one region
to another but no sensor crossing data is regis­
tered, a false negative. This corresponds directly
to missed detection in quantitative methods.

4.

Wrong Time Data: a sensor crossing does oc­
cur, and the direction is correct, however the time
stamp is incorrect.

INCORRECT DATA
Quantitative Methods

Quantitative approaches to tracking and sensor vali­
dation involve a noise model; random perturbations
which usually have only a small effect. For the usual
case of unbiased estimators, a Gaussian model is ad­
equate and is optimal for white noise. (If the estima­
tors are biased, there are models for "coloured" noise).
These are used for continuous variables. If variables
values are discrete a Poisson distribution is used in­
stead of a Gaussian. To handle gross errors of the sort
that are the focus here, a number of different tech­
niques have been proposed. A threshold called a vali­
dation gate may be applied to the Gaussian (for exam­
ple, 2 standard deviations, corresponding to 97.7%).
Alternatively robust statistics (Huber, 1981; Durrant­
W hyte, 1987) may be used, where, for example, the
error is a linear combination of two Gaussians. Fi­
nally non-parametric statistics have been developed;
but they are more difficult to compute and analyse

as

1. Ghost Data:

,

3

Incorrect Data for the Domain

Suppose we know that the object is in region R1 at
timeT. The observation BC-OBS3 of either direction
of crossing must be ghost data. However if we know
the object is in region Rt and the next data received
is BC-OBS1 = dir2, then this may be either ghost
data or wrong direction data. Obviously we are not
always able to determine immediately that data is in­
correct: this may depend on the combination of data
received. Suppose that we do not know the where­
abouts of the only object in the environment and that
we receive two pieces of data: BC-OBS1 = dirt and
BC-OBS3 = dir2. Received together, the two obser­
vations are not mutually compatible, they are incon­
sistent; one must be a ghost crossing (or they both
might be). If they are observed sequentially, there

209

210

Nicholson and Brady

may have been some missing crossings, or again one
or both are ghost data. We want to represent these
as possible but competing alternatives, and to allow
subsequent data to support a particular alternative.
In this paper we do not deal with the possibility that
both ghost and wrong direction data could be caused
by an object which the system does not know about;
we assume that all initialisation information is correct
and that no new objects appear. The main point to
be noted for both ghost and wrong direction data is
that there is an observation node with evidence in the
DBN which directly represents the incorrect data.
We have based the DBN on the assumption that the
time frames are determined by the sensor data which
corresponds to a change of state, i.e. an object has
moved between regions. Missing data means that an
abject has moved undetected to another region. In
some situations we can model this missing data within
the existing DBN expansion and inference algorithm.
Suppose, for example, there is a missing crossing for
sensor LB1, and an observation is received for another
sensor, LB2. While adding the received observation,
BC-OBS2 = ciir1, we create a negative data node far
the first sensor, BC-OBS1 = DC, which represents the
missing crossing (although with incorrect time stamp).
However if nothing has changed, the network has not
been expanded, and there is not even an incorrect DC
signal recorded. If the object that made the unde­
tected movement generates the next positive observa­
tion, then there will never be a BC-OBS added with
evidence nc that actually represents the wrong read­
ing. If the region the object has moved into undetected
is otherwise unoccupied this may cause a subsequent
detected sensor signal that would be considered ghost
data, or wrong direction data. The higher level rea­
soning and additional expansion of the DBN which is
required to handle this missing data is given in (Nichol­
son, 1992).
If the time stamp is incorrect but the temporal order
of the observation data nodes added to the network
is correct, then wrong time data will only affect the
system's temporal reasoning, for example comparing
against schedules and predictions. If the error in the
time stamp is wrong to the extent the order of the BC
nodes is wrong, this will generate problems of missing
data and ghost crossings. Such incorrect ordering of
data cannot be handled within the network and is not
considered in this paper.

4

HANDLING INCORRECT DATA
WI THIN THE BASIC DBN

The basic DBN does not handle inconsistent data; it
finds the evidence impossible and rejects it. We can
modify the existing DBN to provide a mechanism for
handling certain kinds of inconsistent data.

4.1

MODIFYING THE PD FOR BC-OBS

The first three types of incorrect data which we iden­
tified above involve a discrepancy between the sensor
crossing data received by the DBN controller, and the
crossing which actually took place. We have already
modeled the distinction between the crossing which
took place and the data received by creating the two
types of sensor crossing node, BC-ACT and BC-OBS.
The modification to the existing DBN involves chang­
ing the probability distribution for the BC-OBS node.
Instead of using binary values, we represent the uncer­
tainty in the network itself, as the PD entries for each
BC-OBSr become:
P(BC-OBS=dir1IBC-ACT=dir1)=conft
P(BC-OBS=dir2IBC-ACT=dir1)=(1-con/J)/2
P(BC-OBS=nciBC-ACT=dir1)=(1-conh)/2

ok
wrong
miss.

P(BC-OBS=dir1IBC-ACT=dir2)=(1-conft)/2
P(BC-OBS=dir2IBC-ACT=dir2)=con/J
P(BC-OBS=nciBC-ACT=dir2)=(1-con/t)/2

wrong
ok
miss.

P(BC-OBS=dir1IBC-ACT=nc)=(1-con/2)/2
P(BC-OBS=dir2jBC-ACT=nc)=(l-conh)/2
P(BC-OBS=ncjBC-ACT=nc)=con/2

ghost
ghost
ok

The confidence in the observation is given by some
value based on a model of the sensor's performance
and is empirically obtainable; conft is the confidence
in the positive sensor data, con/2 is the confidence in
the negative sensor data (or, 1-con/2 is the probability
of ghost data). We have modeled positive data being
ghost or wrong direction data as being equiprobable
- this need not be the case and can be replaced by
any alternative plausible values. Likewise for negative
data, although the equiprobable direction of the actual
crossing seems intuitively reasonable.
4.2

RESULTS FOR UNINITIALISED
EXAMPLE

We now show the results from the modified DBN for
the example environment, with the position of the ob­
ject at To unknown. The sensor observations made
are as follows.
Crossing

To
Tt

to
to
T:a to
T3 to

Tt
T2
T3
T4

BC-OBS2
nc

BC-OBSt
dir1
nc

nc

DC

nc

nc

nc

BC-OBS3
nc
dir2
dir1
dir2

Table 2 and Figure 2 shows the beliefs inferred by
the DBN after each new observation is received and
the network expanded. Each row of example diagrams
shows the updated beliefs for the position of the object
at some timeT. The observations are shown between
the appropriate rows. Each column corresponds to the
belief at some timeT for the position of the object over
time, i.e. shows the inferred trajectory. We make the

Sensor Validation Using Dynamic Belief Networks

Table 2: Beliefs inferred by the modified DBN for
inconsistent observations. Initial position unknown.

conf

=

0.99.
0.8827
0.0405
0.0384
0.0384
0.0405
0.8827
0.0384
0.0384

.,.

.,

0.4 81
0.02195
0.0221
0.4779
0.0219
0.4781
0.0220
0.4780
1
0.4779
0.4781
0.0220

.,

3

0.0 66
0.0035
0.0403
0.8796
0.0035
0.0766
0.0401
0.8798
0.0035
0.0764
0.9162
0.0039
0.0035
0.0763
0.0768
0.8434

4
0.0002
0.0436

0.911:110

0.0002
0.0042
0.0433
0.9523
0.0002
0.0039
0.9918
0.0041
0.002
0.0039
0.0041
0.9918
0.0002
0.0039
0.9526
0.0433

.,

BC-OBSt{Tt)
BC-OBSa(T2)
correct
ghost
ghost
correct
wrong direction ghost
ghost
wrong direction
ghost
ghost
The first two alternatives have the same probabilities
and are considered the most likely (i.e. approaching
0.5 probability).

Beliefs during Ta: The additional BC-OBSa(Ta)
crossing (from Ra to �) acts as support for the
BC-OBS3(T2 ) observation being correct; belief(BC­
ACT3(T2 ) = dir2) = 0.8761 and belief(BC­
ACTt(Tl) = nc) = 0.9265, i.e. BC-OBSt(To) was
probably ghost data.
Beliefs during T4 : The additional BC-OBSa(T4)
crossing is further support for the alternative that
the first observation was ghost data and second
correct; belief(BC-ACT1{Tt)=nc) = 0.996 and
beliet(BC-ACTa(T2 )=nc) = 0.9484. The DBN has
inferred that the object is probably initially in Rt;
belief(OBJt(To)=r 4) = 0.9520 .
4.3

RESULTS FOR INITIALISED
EXAMPLE

DATA:N.,.U
... _
DAT'-U•l4

A

�• I I
...

_

•

c

• I

• I

• I I I

I�!! I I�!! I I !!� I

.........
...,.

Figure 2: Object position beliefs for inconsistent ob­
servations from Table 2. The belief that the object is
in a region is indicated by the intensity of shading.

following observations on these results.
Beliefs during T0: 4 alternatives are being main­
tained explicitly, all equally probable.
Beliefs during T1: The DBN is nearly certain that
the OBJ moved R1 to R2. The initial beliefs (i.e. the
Oth instance) have been revised, indicating that the
OBJ was very likely to have been in R1. If the data
was ghost data (considered unlikely), there is a small
chance that the object started in R 2, Ra or�- There
is also the alternative that the crossing occurred but in
the opposite direction. Hence the belief for R2 (ghost
plus wrong direction alternatives) is larger than R3
and R4 (ghost only).
Beliefs during T 2: The system now maintains the
alternatives:

J)J

Figure

,01

3:

.01

.....

0

0

I

0

0

-

·

0

'

Object position beliefs with successive BC­

INV nodes unconnected.

Figure 3 shows the beliefs inferred with the object
initially in R1, with no observations added (column
1) , then for the 3 alternative observations shown. We
make the following observations on these results.
No observations: The object may stay in R1 or move
into Rz. Sensors LB 2 and LB3 should generate a no­
crossing signal, because there are no object in the re­
gions they separate, however the DBN infers a small
probability of a ghost crossing signal. The beliefs in­
ferred for the signal BC-OBSt are a combination of the
possible nc or dir2, plus possible incorrect data from
the sensor, hence the predicted observation probabili­
ties differ from the actual crossings predicted.
Observation A: For the BC-OBS1 = dir1 crossing
data, the DBN correctly infers that this might be cor­
rect data (i.e. BC-ACTt = dirt) or ghost data (i.e.
BC-ACT1 = nc).

211

212

Nicholson and Brady
Because there was no object in R2 at
To, the BC-ACT1 = dir2 crossing must be i�corr�ct
data; it may be either ghost data, or wrong duectlon
data.

Observation B:

BC-ACT(f)

BC-INV(T)

The DBN infers that BC-ACTa must
be nc, implicitly rejecting the observation as incorrect
data.

Observation C:

4.4

Our model includes observations as specific evidence
for a variable, the BC-OBS node. One possible al­
ternative would be to model the uncertainty in the
accuracy of the observation by using virtual evidence
{Pearl, 1988), which is given as a likelihood ratio of
the states of the BC-OBS node. If the data was
for a dir1 crossing of sensor LB.,, then the spe­
cific evidence using the existing scheme would be
..t-evidenc:e(BC-OBS., = dir1). The corresponding
virtual evidence for takes the form dir1 :DIR2: IC, i.e.
conf: (1-con/)/2: {1-con/)/2. This use of virtual
evidence provides the same results as modifying the
PD for BC-OBS. Since the BC-OBS evidence is the
physical output of a sensor, we prefer to enter it as
specific evidence and model the difference between � he
observation from the sensor and the actual crossmg
within the DBN itself.
5

EXPLAINING BAD DATA AS A
DEFECTIVE SENSOR

The modification to the DBN described in the pre­
vious section provides a mechanism for handling (by
implicitly rejecting) certain inconsistent data. It rep­
resents adequately the underlying assumptions about
the data uncertainty, which are that the observed sen­
sor crossing might not match the actual sensor crossing
that took place. However it does not provide an ex­
planation of why the observed sensor data might not
reflect the actual crossing. We want to represent the
most usual source of incorrect data, namely a defective
sensor.
5.1

BC-OBS(T)

Using Virtual Evidence

THE INVALIDATING NODE

We adapt an idea that has been used in other re­
search areas, that of a moderating or invalidating con­
dition. In the social sciences and psychology, the term
"moderator" is used for an alternative variable that
"messes up" or "moderates" the relationship between
other variables (Zedeck, 1971; Wermuth, 1987; Wer­
muth, 1989). A similar idea has been used in expert
system research; in (Andersen et al., 1989) such nodes
are called "invalidators". Of course, this idea is also fa­
miliar to the AI community; Winston (Winston, 1977)
described the notion of a censor, which acts as an "un­
less" condition: if a BC-ACT occurs, then BC-OBS
will be generated unless the sensor is defective.

Figure 4: Adding the invalidating node,
the DBN

BC-INV,

to

We add a node, BC-INV, the invalidating node, which
has two states, [work • det] short for "working" and
"defective". It is connected as a predecessor of BC­
OBS (see Figure 4). The PD for BC-OBS for ghost
data, wrong direction data, and missing data is given
by:
,

P(BC-OBS=dir1 IBC-ACT=dir1 BC-INV=vort) = 1
P(BC-OBS=dir2l BC-ACT=dir2 BC-INV=vort) 1
P(BC-OBS=nc I BC-ACT =nc BC-INV=vort) = 1
=

P(BC-OBS=dir2 I BC-ACT=dirl BC-INV=def) = 0.5
P(BC-OBS=nc I BC-ACT=dir1 BC-INV=def) = 0.5
P(BC-OBS=dir1 I BC-ACT=dir2 BC-INV=def) = 0.5
P(BC-OBS=nc I BC-ACT=dir2 BC-INV=def) = 0.5
P(BC-OBS=dirl I BC-ACT=nc BC-INV=def) = 0.5
P(BC-OBS=dir2 I BC-ACT=nc BC-INV=def) = 0.5

The question then arises: what are the prior probabil­
ities for BC-INV? We explicitly represent how likely
it is that the sensor is working correctly by the prior
probabilities for BC-INV, which can be obtained from
empirical data; con/ is now explicitly the confidence
that the sensor is working.
P(BC-INV=vort I ) = conf
P(BC-INV=def I) = 1-conf
5.2

RESULTS FOR SENSOR STATUS

The inference algorithm was run for the same set of al­
ternative observations (A, B and C ) on the DBN with
the BC-INV nodes added; again the object is initially
in R1 and con/ = 0.99. The additional beliefs inferr�d
for the BC-INV nodes having state def are shown m
Figure 3 under the appropriate sensor {in row labelled
P(def)). For cases A and B, the DBN infers that all
nc observations for sensors LB2 and LBa are correct
(i.e. BC-INV =wort) because there were no objects in
adjacent regions to move across these sensors. In case
C the DBN infers correctly that sensor LBa must be
d�fective (i.e. BC-INVa = def); there is a small pos­
sibility that the nc observation for sensor LBt may be
incorrect, if there is missing data.
5.3

MODELING SENSOR STATU S OVER
TIME

Sensor Validation Using Dynamic Belief Networks

P(BC-OBS:::::dir1 IBC-ACT=dir1 BC-INV=def)=x
P(BC-OBS=dir2IBC-ACT=dir2 BC-INV=de:f)=x
P(BC-OBS=nciBC-ACT=nc BC-INV�def)=x > 0

BC.OOS(T+I)

Figure

5:

Modeling sensor status over time.

The invalidating node provides the explicit representa­
tion of the cause of incorrect data - a defective sensor.
However, there is no connection between successive
BC-INV nodes, which means no correlation between
the working status of a sensor at different times. If
the DBN infers that a sensor is defective at some time
T because the dat a received has been wrong, it should
also effect the interpretation put on subsequent (and
possibly earlier) data from that sensor. To provide
such a model of the sensor, we assume that at the ini­
tial time T0 all BC-I NV ;(T0 ) nodes have some prior
such as described above. At each time step, a copy
is made of all the BC-INV nodes (whether or not any
data is received for that sensor), and each is connected
to its success or (see Figure 5). The PO for each BC­
INV(T+l) is then given by:

A persistent fault may be modeled by X equals 0, but
without the need to change the probability distribu­
tion for BC-OBS. An example of a persistent fault is
the incorrect wiring of the sensor so that the crossing
direction is wrong each signal. In practice, a controller
will request confirmation of the status of the sensor,
or receive information that it has been repaired. In
this case, BC-INV(time-of-report) will h av e no prede­
cessors and the prior will reflect the confidence in the
status report. Results from the DBN with the BC­
INV{T) to BC-INV (T+l) connection for the same set
of observations are shown in Figure 6, conf = 0.99, d
= 0.01, X = 0. Since X = 0, once sensors LB1 (case
B) and LB3 (case C) has been identified as definitely
defective, the DBN infers that the probability it was
defective initially (BC-INV3(To) = def) is 0.5025.

.....

... _
..

�

..

·-

�iI1 I
I0 I •lffl-1
I I •(JI
.0031
1
0
.0011 0
0

0

d

where d is a degradation factor and X is related to the
consistency of the fault.
The degradation factor dis the probability that a sen­
sor which has been working during the previous time
interval has begun to respond defectively. It is based
on a model of the expected degradation of the sensor
and is a function of the time between sensor readings.

5.4

.Ot99 .0199 ..,191

0

I � i: 1 I I ;; f ! I I ! f-+ I
.916)

Tt•�+�l I
l'(do!)

P(BC-INV(T+I)==vork IBC-INV(T)::::: vork)::::: IP(BC-INV(T+I)==def I BC-INV(T)= vork)::::: d
P(BC-INV(T+l)==def I BC-INV{T} = def) = 1-X
P(BC-INV(T+l)==vork IBC-INV(T)::::: def)=X

> 0
> 0

PERSISTENT AND INTERMITTENT
FAULTS

There are two general models for a defective sensor: an
intermittent fault, which means that not every signal
from the sensor is incorrect; a persistent fault, that
manifests itself for each observation.
One method for modeling an intermittent fault is to
make the variable X strictly positive. However if the
DBN infers from the data that (i) BC-INV(T;) = def,
and (ii) BC-INV(Ti+I) =work then the fault detected
during T; cannot be passed on to T;+2· An alterna­
tive is to have X = 0 all the time (i.e. once a sensor is
known to be defective it remains defective) and change
the PD for BC-OBS so that if a defective sensor can
still produce correct data:

Figure 6: Object position beliefs inferred by DBN with
successive invalidating nodes connected.
5.5

MODELING DIFFERENT DEFECTS

The current BC -INV node, with only two states,
does not allow the explanation to distinguish between
types of defects. We can increase the BC-INV states
to [work, def-ghost, def-dir, def-miaa], for ghost
data, wrong direction data and missing data respec­
Details of and results for these additional
tively.
states, as well as results for various combinations of
con f , d and X may be found in (Nicholson, 1992).

6

CONCLUSIONS

The basic DBN provides discrete tracking of objects
based on light beam sensor data, in a method which
is analogous to quantitative filter techniques. In this
paper we have described a solution to the problem of
incorrect or noisy data. By changing the PD for the
BC-OBS node to contain values other than 1 or 0,
the DBN is able to handle inconsistent data, rather
than simply inferring a contradiction in the evidence.
The addition of a node which models the status of
the sensor as working or defective, as another parent
of the BC-OBS node, provides an explanation of the
incorrect data as being caused by a defective sensor.

213

214

Nicholson and Brady

The connection of the successi ve instances of the inval­
idating node models the status of a sensor over time,
allowing the DBN to handle both persistent and in­
termittent faults. We have shown that a combination
of AI techniques - discrete representation and reason­
ing with uncertainty - can provide a solution to a real
world problem, i.e incorrect sensor data. Moreover,
the solution is in some ways more intuitive than equiv­
alent conventional quantitative methods.
Acknowledgements


