
Qualitative possibilistic networks, also
known as min-based possibilistic networks,
are important tools for handling uncertain
information in the possibility theory framework. Despite their importance, only the
junction tree adaptation has been proposed
for exact reasoning with such networks.
This paper explores alternative algorithms
using compilation techniques.
We first
propose possibilistic adaptations of standard
compilation-based probabilistic methods.
Then, we develop a new, purely possibilistic,
method based on the transformation of the
initial network into a possibilistic base. A
comparative study shows that this latter
performs better than the possibilistic adaptations of probabilistic methods. This result
is also confirmed by experimental results.

1

INTRODUCTION

In possibility theory there are two different ways to
define the counterpart of Bayesian networks. This is
due to the existence of two definitions of possibilistic
conditioning: product-based and min-based conditioning (Dubois and Prade, 1988). When we use the product form of conditioning, we get a possibilistic network
close to the probabilistic one sharing the same features
and having the same theoretical and practical results.
However, this is not the case with min-based networks.
In this paper, we are interested in the inference problem in multiply connected networks, which is known
as a hard problem (Cooper, 1990). More precisely,
we propose three compilation methods for min-based
possibilistic networks.
The compilation of Bayesian networks is always considered as an important area. Recently, researchers

Salem Benferhat
CRIL-CNRS
University of Artois
France, 62307
benferhat@cril.univ-artois.fr

Rolf Haenni
RISIS
Bern University
Switzerland, CH-2501
rolf.haenni@bfh.ch

have been interested in various kinds of exact and approximate Bayesian networks inference algorithms using compilation techniques (Darwiche, 2003) (Chavira
and Darwiche, 2005) (Wachter and Haenni, 2007), etc.
Despite the importance of possibility theory, there is
no compilation that has been proposed for possibilistic networks. This paper analyzes this issue by first
adapting well-known compilation-based probabilistic
inference approaches, namely the arithmetic circuit
method (Darwiche, 2003) and the logical compilation
of Bayesian Networks (Wachter and Haenni, 2007).
Both of them are based on a network’s encoding into a
logical representation and a compilation into a target
compilation language, namely Π-DNNF. From there,
all possible queries are answered in polynomial time.
The third method exploits results obtained on one
hand in (Benferhat et al., 2002) that transforms a minbased possibilistic network into a possibilistic knowledge base, and on the other hand results obtained regarding compilation of possibilistic bases (Benferhat
et al., 2007) in order to assure inference in polytime.
This method that is purely possibilistic is flexible since
it permits to exploit efficiently all the existing propositional compilers.
The rest of this paper is organized as follows: Section 2 gives a briefly background on possibility theory, possibilistic logic, possibilistic networks and introduces some compilation concepts. Section 3 is dedicated to possibilistic adaptations of compilation-based
probabilistic inference methods. Section 4 presents a
new inference method in possibilistic networks using
compiled possibilistic knowledge bases. Experimental
study is presented in Section 5.

2
2.1

BASIC CONCEPTS
POSSIBILITY THEORY

This subsection briefly recalls some elements of possibility theory, for more details we refer to (Dubois and
Prade, 1988). Let V = {X1 , X2 , ..., XN } be a set of

variables. We denote by DXi = {x1 , .., xn } the domain
associated with the variable Xi . By xi we denote any
instance of Xi . Ω denotes the universe of discourse,
which is the Cartesian product of all variable domains
in V . Each element ω ∈ Ω is called a state of Ω.
The notion of possibility distribution denoted by π is
a mapping from the universe of discourse to the unit
interval [0, 1]. To this scale, two interpretations can be
attributed, a quantitative one when values have a real
sense and a qualitative one when values reflect only an
order between the different states of the world. This
paper focuses on the qualitative interpretation of possibility theory.
Given a possibility distribution π, we can define a mapping grading the possibility measure of an event φ ⊆ Ω
by Π(φ) = maxω∈φ π(ω). Π has a dual measure which
is the necessity measure N (φ) = 1 − Π(¬φ).
Conditioning consists in modifying our initial knowledge, encoded by a possibility distribution π, by the
arrival of a new certain piece of information φ ⊆ Ω.
The qualitative interpretation of the scale [0, 1] leads
to the well known definition of min-conditioning (Hisdal, 1978), (Dubois and Prade, 1988):

Π(ψ ∧ φ) if Π(ψ ∧ φ) < Π(φ)
Π(ψ | φ) =
(1)
1
otherwise
2.2

POSSIBILISTIC LOGIC

Possibilistic logic (Dubois et al., 1994) handles qualitative uncertainty in a logical setting. A possibilistic
logic formula is a pair (p, a) where p is a propositional
formula and a its uncertainty degree which estimates
to what extent it is certain that p is true. The higher
is the weight, the more certain is the formula. A possibilistic knowledge base Σ is made up of a finite set
of weighted formulas, i.e.,
Σ = {(pi , ai ), i = 1, .., n}

(2)

where ai is the lower bound on N (pi ).
Each possibilistic knowledge base induces a unique
possibility distribution such that ∀ ω ∈ Ω and ∀
(pi , ai ) ∈ Σ:

1
if ω |= pi
πΣ (ω) =
(3)
1 − max {ai : ω 2 pi } otherwise
where |= is propositional logic entailment.
2.3

POSSIBILISTIC NETWORKS

A min-based possibilistic network over a set of variables V , denoted by ΠGmin is composed of:
- a graphical component that is a DAG (Directed

Acyclic Graph) where nodes represent variables and
edges encode the links between the variables. The
parent set of a node Xi is denoted by Ui =
{Ui1 , Ui2 , ..., Uim }. For any ui of Ui we have ui =
{ui1 , ui2 , ..., uim } where m is the number of parents of
Xi . In what follows, we use xi , ui , uij to denote, respectively, possible instances of Xi , Ui and Uij .
- a numerical component that quantifies different links.
For every root node Xi (Ui = ∅), uncertainty is represented by the a priori possibility degree Π(xi ) of each
instance xi ∈ DXi , such that maxxi Π(xi ) = 1. For the
rest of the nodes (Ui 6= ∅) uncertainty is represented
by the conditional possibility degree Π(xi |ui ) of each
instances xi ∈ DXi and ui ∈ DUi . These conditional
distributions satisfy the following normalization condition: maxxi Π(xi |ui ) = 1, for any ui .
The set of a priori and conditional possibility degrees
in a min-based possibilistic network induce a unique
joint possibility distribution defined by the following
chain rule:
πmin (X1 , .., XN ) = min

i=1..N

2.4

Π(Xi | Ui )

(4)

COMPILATION CONCEPTS

A target compilation language is a class of formulas
which is tractable for a set of transformations and
queries. Compilation languages are compared in terms
of their spatial efficiency via the succinctness criteria and also in terms of the set of logical queries and
transformations they support in polynomial time (see
(Darwiche and Marquis, 2002) for more details).
Within the most effective target compilation languages, we cite the Decomposable Negation Normal
Form (DNNF) (Darwiche, 2001). This language is universal and presents a number of properties (determinism, smoothness, etc.) that makes it of a great interest.
It supports a rich set of polynomial-time logical operations. To define DNNF, the starting point is Negation
Normal Form (NNF) which is a set of propositional
formulas where possible connectives are conjunctions,
disjunctions and negations. A set of important properties may be imposed to NNF, such that:
- Decomposability: the conjuncts of any conjunction in
NNF do not share variables.
- Determinism: two disjuncts of any disjunction in
NNF are logically contradictory.
- Smoothness: the disjunct of any disjunction in NNF
mentions the same variables.
These properties lead to a number of interesting subsets of NNF. Within these subsets, the language
DNNF (Darwiche, 2001) is one of the most effective
target compilation languages that supports the decomposability. We can also mention, the d-DNNF sat-

isfying determinism, sd-DNNF satisfying smoothness
and determinism, etc. Each compilation language supports some queries and transformations in polynomial
time. In what follows we are in particular interested
by conditioning and forgetting transformations (Darwiche and Marquis, 2002).

∨ and ∧ as max and min operators, respectively). A
sentence in Π-sd-DNNF is a sentence in Π-DNNF satisfying decomposability, determinism and smoothness.

3

In (Darwiche, 2003), authors have focused on inference in compiled Bayesian networks. The main idea is
based on representing the network using a polynomial
and then retrieving answers to probabilistic queries by
evaluating and differentiating the polynomial. This
latter itself is exponential in size, so it has been represented efficiently using an arithmetic circuit that can
be evaluated and differentiated in time and space linear in the circuit size. In what follows, we propose
a direct adaptation of this method in the possibilistic setting. Given a min-based possibilistic network,
we first encode it using a possibilistic function fmin
defined by two types of variables:

POSSIBILISTIC ADAPTATIONS
OF COMPILATION-BASED
PROBABILISTIC INFERENCE
METHODS

There are several compilation methods which handle
the inference problem in probabilistic graphical models. In this section, we first propose an adaptation
of the arithmetic circuit method of (Darwiche, 2003).
Then we will study one of its variants proposed in
(Wachter and Haenni, 2007), namely the logical compilation of Bayesian Networks.
DNNF has been introduced for propositional language.
Recall that in qualitative possibility theory, we basically manipulate two main operators Max and Min.
These operators fully make sense when we deal with
qualitative plausibility ordering. Therefore, we propose to define concepts of Π-DNNF (resp. Π-d-DNNF,
Π-sd-DNNF) as adaptations of the DNNF language
(resp. d-DNNF, sd-DNNF) (Darwiche, 2001) in the
possibilistic setting (definition 1).
Definition 1. A sentence in Π-DNNF is a rooted
DAG where each leaf node is labeled with true, false
or variable’s instances and each internal node is labeled with max or min operators and can have arbitrarily several children. Roughly speaking, Π-DNNF is
the same as the classical DNNF although its operators
are max and min instead of ∨ and ∧, respectively.
Example 1. Figure 1 depicts a sentence in Π-DNNF.
Consider the Min-node (root) in this figure. This node has
two children, the first contains variables A, B while the
second contains variables C, D. This node is decomposable
since its two children do not share variables.

3.1

INFERENCE USING POSSIBILISTIC
CIRCUITS

• Evidence indicators: for each variable Xi in the
network , we have a variable λxi for each instance
xi ∈ DXi .
• Network parameters: for each variable Xi and its
parents Ui in the network, we have a variable
θxi |ui for each instance xi ∈ DXi and ui ∈ DUi .
fmin = max
x

min
(xi ,ui )∼x

λxi θxi |ui

(5)

where x represents instantiations of all network variables and ui ∼ x denotes the compatibility relationship among ui and x. The possibilistic function fmin
of a possibilistic network represents the possibility distribution and allows to compute possibility degrees of
variables of interest. Namely, for any piece of evidence
e which is an instantiation of some variables E in the
network, we can instantiate fmin as it returns the possibility of e, Π(e) (Definition 2 and Proposition 1).
Definition 2. The value of the possibilistic function
fmin at evidence e, denoted by fmin (e), is the result of
replacing each evidence indicator λxi in fmin with 1 if
xi is consistent with e, and with 0 otherwise.
Proposition 1. Let ΠGmin be a possibilistic network
representing the possibility distribution π and having
the possibilistic function fmin . For any evidence e, we
have fmin (e) = π(e).

Figure 1: A sentence in Π-DNNF.

A sentence in Π-d-DNNF is a sentence in Π-DNNF
satisfying decomposability and determinism (viewing

Let figure 2 be the min-based possibilistic network
used throughout the paper.
The possibilistic function of the network in figure 2
has 8 terms corresponding to the 8 instantiations of
variables F, B, D. Two of these terms are as follows:

is outlined by algorithm 1. Note that the suffix P F is
added to signify that this method uses a possibilistic
function (fmin ) before ensuring the CNF encoding.
Algorithm 1: Inference using Π-DNNF (Π-DNNFP F )

Figure 2: Example of ΠGmin .

fmin = max(min(λd1 , λf1 , λb1 , θd1 |f1 ,b1 , θf1 , θb1 ); min
(λd1 , λf2 , λb1 , θd1 |f2 ,b1 , θf2 , θb1 ); · · · )
If the evidence e = (d1 , b1 ) then fmin (d1 , b1 ) is obtained by applying the following substitutions to fmin :
λd1 = 1, λd2 = 0, λb1 = 1, λb2 = 0, λf1 = λf2 = 1. This
leads to Π(e) = 0.7.
The possibilistic function fmin is then encoded on a
propositional theory (CNF) using λxi and θxi |ui . For
each network variable Xi , the encoding contains the
following clauses:
λxi ∨ λxj
(6)
¬λxi ∨ ¬λxj , i 6= j

(7)

Moreover, for each propositional variable θxi |ui , the
encoding contains the clause:
λxi ∧ λui1 ∧ . . . ∧ λuim ↔ θxi |ui

(8)

The CNF encoding, denoted by Kfmin recovers the
min-joint possibility distribution (proposition 2).
Proposition 2. The CNF encoding Kfmin of a possibilistic network encodes the joint distribution of given
network.
Once the CNF encoding is accomplished, it is then
compiled into a Π-DNNF, from which we extract the
possibilistic circuit ζp (definition 3) that implements
the encoded fmin .
Definition 3. A possibilistic circuit ζp encoded by a
Π-DNNF sentence ξ c is a DAG in which leaf nodes
correspond to circuit inputs, internal nodes correspond
to max and min operators, and the root corresponds to
the circuit output.
As in the probabilistic case (Darwiche, 2003), this circuit can be used for linear-time inference. More precisely, computing the possibility degree of an event
consists on evaluating ζp by setting each evidence indicator λx to 1 if the event is consistent with x, to 0
otherwise and applying operators in a bottom-up way.
This possibility degree corresponds exactly to the one
computed from the min-joint possibility distribution
(proposition 3). This method referred to Π-DNNFP F

Data: ΠGmin , instance of interest x, evidence e
Result: Π(x|e)
begin
Compilation into Π-DNNF
Encode ΠGmin into fmin using equation 5
EncodeCNF of ΠGmin into ξ using equations 6, 7, 8
Compile ξ into ξ c
ζp ← Possibilistic Circuit of ξ c
Inference
Applying Operators on ζp
Π(x, e) ← Root Value (ζp ; (x,e))
Π(e) ← Root Value (ζp ; e)
if Π(x, e) ≺ Π(e) then Π(x|e) ← Π(x, e)
else Π(x|e) ← 1
return Π(x|e)
end

Proposition 3. Let ΠGmin be a possibilistic network.
Let πmin be a joint distribution obtained by chain rule.
Then for any a ∈ Da and e ∈ DE , we have Π(A =
a|E = e) = Πmin (A = a|E = e) where Πmin (A =
a|E = e) is obtained from πmin using equation 1 and
Π(A = a|E = e) is obtained from algorithm 1.
The key point to observe here is that this approach
can handle possibilistic circuits of manageable size as
in the probabilistic case since some possibility values
may have some specific values; for instance, whether
they are equal to 0 or 1, and whether some possibilities are equal. In this case, we can say that the
network exhibit some local structure. By exploiting
it, the produced circuits can be smaller. In fact, the
normalization constraint relative to the initial network
will mean that we will have several values equal to 1.
Thus the idea is to make an advantage from such a
local structure which has a particular behavior with
the max operator in order to construct more compact
possibilistic circuits w.r.t. standard ones as stated by
the following proposition:
Proposition 4. Let N bposs and N bproba be the number of clauses in the possibilistic and probabilistic
cases, respectively. Then N bposs ≤ N bproba .
Note that for particular situations where probability
values are 1 or 0, we have N bposs = N bproba , otherwise
N bposs ≺ N bproba .
Example 2. To illustrate algorithm 1 we will consider
the min-based possibilistic network represented in figure 2.
We are looking for Π(f2 |d1 ) with f2 as instance of interest and d1 as evidence. First, we encode the network as
a possibilistic function and encode it on CNF. This latter is then compiled into Π-DNNF from which a possibilistic circuit is extracted. The possibility degree Π(f2 |d1 ) is
computed using this circuit in polynomial time. For instance, Π(f2 , d1 ) is computed using ζp by just replacing

λf2 = λd1 = λb1 = λb2 = 1 and applying possibilistic
operators in a bottom-up way as shown in figure 3. Hence,
Π(f2 |d1 ) = Π(f2 , d1 ) = 0.4 since Π(f2 , d1 ) = 0.4 ≺ 1.

from a function fψ encoding the CNF. Then, we have
πmin (xi , ..., xj ) = Π(xi , ..., xj ), i.e. fψ recovers the
min-joint possibility distribution πmin .
Comparing theoretically the probabilistic and the possibilistic case allows us to deduce the following proposition:
Proposition 6. The possibilistic encoding of a possibilistic network given by Kψ (equation 10) is more
compact than the probabilistic encoding given in
(Wachter and Haenni, 2007).
In fact, the number of variables used in Kψ is less than
the one used in (Wachter and Haenni, 2007). In particular for parameters, our approach uses one variable
per different weight, while in the probabilistic encoding
one variable per parameter. For each clause in Kψ
there exists a clause of the same size in the probabilistic encoding. The converse is false.

Figure 3: Inference using the possibilistic circuit (ζp ).

3.2

INFERENCE USING POSSIBILISTIC
COMPILED REPRESENTATIONS

DNNF plays an interesting role in compiling propositional knowledge bases. It has been used to compile
probabilistic networks. More precisely in (Wachter
and Haenni, 2007), authors have been interested in
performing a CNF logical encoding of the probability distribution induced by a bayesian network, then
a compilation phase from CNF to d-DNNF. In this
section, we propose to adapt this encoding in the possibilistic setting by taking into consideration the local
structure aspect. This allows to reduce the number
of additional variables comparing to the probabilistic
encoding. Let ∆ be propositions linked to network’s
variables and let θ be propositions linked to the possibility distribution entries (equal to 1). We start by
looking at the possibility distribution encoding. The
logical representation of a network variable Xi is defined by: ψXi =

^
^

ui1 ∧ · · · ∧ uim ∧ θxi |ui → xi
(9)
ui

θxi |ui ∈ΩθX

i

|ui

By taking the conjunction of all logical representations
of variables, we obtain the network’s representation ψ
as follows:
^
ψ=
ψXi
(10)
Xi ∈∆

The CNF encoding, denoted by Kψ indeed recovers
the min-joint possibility distribution (proposition 5).
Proposition 5. Let πmin be the joint possibility distribution obtained using the chain rule with the minimum operator and Π be the possibility degree computed

Once the qualitative network is encoded by Kψ , it is
compiled into a compilation language that supports
the transformations conditioning and forgetting and
the query possibilistic computation. This language is
Π-DNNF (proposition 7). Therefore, the CNF encoding is first compiled, and the resulting Π-DNNF is then
used to compute efficiently, i.e. in polynomial time
a-posteriori possibility degrees (proposition 8). This
method referred to Π-DNNF is outlined by algo. 2.
Proposition 7. Π-DNNF supports conditioning, forgetting and possibilistic computation.
Algorithm 2: Inference using Π-DNNF
Data: ΠGmin , instance of interest x, evidence e
Result: Π(x|e)
begin
Compilation into Π-DNNF
EncodeCNF of ΠGmin into ψ using equation 10
Compile ψ into ψpc
Inference
v1 ← Explore Π-DNNF(x ∧ e, ψpc )
v2 ← Explore Π-DNNF(e, ψpc )
if v1 ≺ v2 then Π(x|e) ← v1 else Π(x|e) ← 1
return Π(x|e)
end

Proposition 8. Let ΠGmin be a possibilistic network.
Let πmin be a joint distribution obtained by chain rule.
Then for any a ∈ Da and e ∈ DE , we have Π(A =
a|E = e) = Πmin (A = a|E = e) where Πmin (A =
a|E = e) is obtained from πmin using equation 1 and
Π(A = a|E = e) is obtained from algorithm 2.
Example 3. Let us illustrate algorithm 2. In fact, ψ of
the network of figure 2 is : ψ = ψF ∧ ψB ∧ ψD = {(θ1 ∨
f2 ) ∧ (θ2 ∨ b1 ) ∧ (f2 ∨ b2 ∨ θ2 ∨ d1 ) ∧ (f2 ∨ b1 ∨ θ1 ∨ d2 ) ∧
(f1 ∨ b2 ∨ θ3 ∨ d2 ) ∧ (f1 ∨ b1 ∨ θ4 ∨ d2 )} such as θ1 , θ2 , θ3
and θ4 correspond respectively to 0.8, 0.7, 0.4 and 0.2.
To compute Π(f2 |d1 ), we should first compute Π(f2 , d1 ) using algorithm 3. The first step is to check if we have at least

Algorithm 3: Explore Π-DNNF
Data: a set of instances x, compiled representation ψpc
Result: Π(x)
begin
if ∀ xi ∈ x, θxi |Ui is not a leaf node then
Π(x) ← 1
else
y= {xi | ∀, θxi |Ui is a leaf node ∀ Ui ⊆ x}
c
ψp|y
← Condition ψpc on y
c
ψpc ↓|y ← Forget ∆ from ψp|y
c
Applying Operators on ψp ↓|y
Π(x) ← Root Value of ψpc ↓|y
return Π(x)
end

one θ as a leaf node. In this example, we have θd1 |f2 ,b1
and θd1 |f2 ,b2 as leaf nodes, hence conditioning should be
performed. Then, a computation step is required by applying in a bottom-up way Min and Max operators on the
forgotten Π-DNNF. Therefore, Π(f2 |d1 ) = Π(f2 , d1 ) = 0.4.

4

NEW POSSIBILISTIC
INFERENCE ALGORITHM

In (Benferhat et al., 2002), authors have been interested in the transition of possibilistic networks into
possibilistic logic bases. The starting point is that the
possibilistic base associated to a possibilistic network
is the result of the fusion of elementary bases. Definition 4 presents the transformation of a min-based possibilistic network into a possibilistic knowledge base.
Definition 4. A binary variable Xi of a possibilistic network can be expressed by a local possibilistic knowledge base as follows:
ΣXi
=
{(¬xi ∨ ¬ui , αi ) : αi = 1 − π(xi |ui ) 6= 0}. The possibilistic knowledge base of the whole network is: Σmin =
ΣX1 ∪ ΣX2 ∪ · · · ∪ ΣXn .
In another angle, researchers in (Benferhat et al., 2007)
have focused on the compilation of bases under the
possibilistic logic policy in order to be able to process
inference from it in a polynomial time. The combination of these methods allows us to propose a new
alternative approach to possibilistic inference. This is
justified by the fact that the possibilistic logic reasoning machinery can be applied to directed possibilistic
networks (Benferhat et al., 2002).
The idea is to encode the possibilistic knowledge base
Σmin into a classical propositional base (CNF). Let
A = {a1 , ..., an } with a1  ...  an the different
weights used in Σmin . A set of additional propositional variables, denoted by Ai , which correspond exactly to the number of different weights, are incorporated and for each formula φi , ai will correspond the
propositional formula φi ∨Ai . Hence, the propositional

encoding of Σmin , denoted by KΣ is defined by:
KΣ = {φi ∨ Ai : (φi , ai ) ∈ Σmin }

(11)

The following proposition shows that the CNF encoding KΣ recovers the min-joint possibility distribution.
Proposition 9. Let πmin be the joint possibility
distribution obtained using the chain rule with the
minimum-based conditioning and let KΣ be the propositional base associated with the possibilistic network
given by equation 11. Let φi be a propositional formula associated with a degree ai . Then ∀ω ∈ Ω,
Π(ω) = 1 iff {¬A1 , ..., ¬An } ∧ ω ∧ KΣ is consistent.
Π(ω) = ai iff {¬A1 , ..., ¬Ai } ∧ ω ∧ KΣ is inconsistent
and {¬A1 , ..., ¬Ai−1 } ∧ ω ∧ KΣ is consistent.
The CNF encoding KΣ is then compiled into a target
compilation language in order to compute a-posteriori
possibility degrees in an efficient way. Here, we are
interested in a particular query useful for possibilistic networks, namely what is the possibility degree of
an event A = a given an evidence E = e? Therefore, we propose to adapt the algorithm given in (Benferhat et al., 2007) in order to respond to this query
as shown by algorithm 4. Proposition 10 shows that
the possibility degree computed using algorithm 4 and
the one computed using the min-based joint possibility distribution are equal. Note that this approach is
qualified to be flexible since it takes advantage of existing propositional knowledge bases compilation methods (Benferhat et al., 2007). This method referred to
DNNF-PKB is outlined by algorithm 4.
Algorithm 4: Inference using DNNF
Data: ΠGmin , instance of interest x, evidence e
Result: Π(x|e)
begin
Transformation into KΣ
Transform ΠGmin into Σmin using definition 4
Transform Σmin into KΣ using equation 11
Inference
c
KΣ
← T arget(KΣ )
c
K ← KΣ
StopCompute ← false
i←1
Π(x|e) ← 1
while (K 2 Ai ∨ ¬e) and (i ≤ k) and (StopCompute=false) do
K ← condition (K, ¬Ai )
if K ¬x then
StopCompute← true
Π(x|e) ← 1-degree(i)
else
i←i+1
return Π(x|e)
end

Proposition 10. Let ΠGmin be a possibilistic network. Let πmin be a joint distribution obtained by

chain rule. Then for any a ∈ Da and e ∈ DE , we
have Π(A = a|E = e) = Πmin (A = a|E = e) where
Πmin (A = a|E = e) is obtained from πmin using equation 1 and Π(A = a|E = e) is obtained from Algo. 4.

Indeed, in Π-DNNFP F , we associate propositional
variables not only to possibility degrees (parameters),
but also to each value xi of Xi . While in DNNF-PKB
only m new variables are added (one variable per different degree).

Example 4. To illustrate algorithm 4 we will consider

Let us now analyze these three approaches from experimental points of view. Our experimentation is
performed on random possibilistic networks. More
precisely, we have compared DNNF-PKB and ΠDNNFP F on 100 possibilistic networks having from 10
to 50 nodes. As mentioned that the approaches focus
mainly on encoding the possibilistic network as a CNF
then compile it into the appropriate language, hence,
it should be interesting to compare the CNF parameters (the number of variables and clauses) and the
DNNF parameters (the number of nodes and edges)
for the two methods.

the min-based possibilistic network represented in figure 2.
The CNF encoding is as follows :
KΣ
=
(d2 ∨ f1 ∨ b2 ∨ A1 ) , (b1 ∨ A2 ) , (d1 ∨ f2 ∨ b2 ∨ A2 ) ,
(f2 ∨ A3 ) , (d2 ∨ f2 ∨ b1 ∨ A3 ) , (d2 ∨ f1 ∨ b1 ∨ A4 )
such
as A1 (0.8), A2 (0.6), A3 (0.3) and A4 (0.2) are
propositional variables followed by their weights under
c
brackets. Compiling KΣ into DNNF results in: KΣ
=
((b2 ∧ A2 ) ∧ [(A3 ∧ f1 ) ∨ (f2 ∧ [d2 ∨ (A4 ∧ d1 )])]) ∨ (b1 ∧
[[f2 ∧ (d2 ∨ (A1 ∨ d1 ))] ∨ [(f 1 ∧ A3 ) ∧ (d1 ∨ (A2 ∧ d2 ))]]).
c
The computation of Π(f2 |d1 ) using KΣ
requires two
iterations. Therefore, Π(f2 |d1 ) = 1 − degree(2) = 0.4.

Due to the compilation step, this algorithm runs in
polynomial time. Moreover, the number of additional
variables is low since it corresponds exactly to the
number of priority levels existing in the base.

5

COMPARATIVE AND
EXPERIMENTAL STUDIES

The paper analyzes three compilation-based methods,
namely DNNF-PKB, Π-DNNF and Π-DNNFP F . The
first dimension that differentiates the three approaches
proposed in this paper is the CNF encoding. It consists
of specifying the number of variables and clauses per
approach.
The CNF of DNNF-PKB is based on encoding ¬x
where x is an instance of interest having a possibility degree different from 1. In Π-DNNF, we write
implications relative to instances having 1 as possibility degree. We can notice that the local structure
in both methods is exploited in semantically different
ways. In DNNF-PKB, the encoding uses the number
of different weights as the number of additional variables while the Π-DNNF encoding uses the number of
the non-redundant possibility degrees different from 1
in the distributions. Regarding the number of clauses,
both methods handle possibility degrees different from
1. This leads us to the following proposition:

5.1

CNF PARAMETERS

First we propose to test the CNF encodings characterized by the number of variables and the number of
clauses. Regarding DNNF-PKB, the number of additional variables correspond to the number of weights
which are different. While in Π-DNNFP F , variables
are both those associated to the possibility degrees
of each distribution and those to variable’s instances.
The number of clauses for each method is related to
the CNF encoding itself. Figure 4 shows the results of
this experimentation. Each approach is characterized
by a curve for the average number of variables and a
curve for the average number of clauses. It is clear
that the higher the number of nodes considered in the
possibilistic network, the higher the number of variables and clauses. Figure 4 shows that DNNF-PKB
has the lower number of variables and clauses comparing to Π-DNNFP F , which confirms the theoretical
results detailed above.

Proposition 11. The CNF encodings of DNNF-PKB
and Π-DNNF have the same number of variables and
clauses.
The CNF encoding of Π-DNNFP F is different from
the ones of DNNF-PKB and Π-DNNF. Proposition 12
shows the difference between Π-DNNFP F and DNNFPKB in terms of number of variables and clauses.
Proposition 12. The number of variables and clauses
in Π-DNNFP F is more important than those in
DNNF-PKB.

Figure 4: CNF parameters.

5.2

DNNF PARAMETERS

Once we obtain the CNF encodings, it is important
to compare the number of nodes and edges for each
compiled base. Figure 5 represents the average size of
the compiled bases for the two methods in terms of
nodes and edges numbers. We remark that the number of nodes and edges depends deeply on CNF parameters. More precisely, the number of nodes and
edges in DNNF-PKB is considered narrow comparing
to Π-DNNFP F . This can be explained by the lower
number of variables and clauses on CNFs and the local structure which shrinks the sizes of compiled bases.
Comparing DNNF-PKB to Π-DNNFP F , the behavior
of DNNF-PKB is important.

(Pearl, 2000) (Benferhat and Smaoui, 2007).
Acknowledgements
We thank the anonymous reviewers for many interesting
comments and suggestions. Also, we wish to thank Mark
Chavira for our valuable discussions on this subject. The
third author would like to thank the project ANR Placid.


