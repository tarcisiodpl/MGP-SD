
We investigate an approach to reasoning about
causes through argumentation. We consider a causal
model for a physical system, and look for arguments
about facts. Some arguments are meant to provide
explanations of facts whereas some challenge these
explanations and so on. At the root of argumentation
here, are causal links ({A1 , Â· Â· Â· , An } causes B) and
ontological links (o1 is_a o2 ). We present a system
that provides a candidate explanation ({A1 , Â· Â· Â· , An }
explains {B1 , Â· Â· Â· , Bm }) by resorting to an underlying
causal link substantiated with appropriate ontological
links. Argumentation is then at work from these various
explaining links. A case study is developed : a severe
storm Xynthia that devastated part of France in 2010,
with an unaccountably high number of casualties.

1

Introduction and Motivation

Looking for explanations is a frequent operation, in various domains, from judiciary to mechanical fields. We

consider the case where we have some precise (not necessarily exhaustive) description of some mechanism, or situation, and we are looking for explanations of some facts.
The description contains logical formulas, plus some causal and ontological formulas (or links). Indeed, it is known
that, while there are similarity between causation and implication, causation cannot be rendered by a simple logical
implication. Also, confusing causation and co-occurrence
could lead to undesirable relationships. This is why we
use here a causal formalism where some causal links and
ontological links are added to classical logical formulas.
Then the causal formalism will produce various explanation links [1]. However, if the situation described is complex enough, this will result in a great number of possible
explanations, and some argumentation is involved in order
to get some reasons to choose between all these candidate
explanations.
In this text, we will consider as an example a severe
storm, called Xynthia, which made 26 deaths in a single
group of houses in La Faute sur Mer, a village in VendÃ©e during a night in February 2010. This was a severe
storm, with strong winds, low pressure, but it had been
forecast. Since the casualties were excessive with respect
to the strength of the meteorological phenomenon, various
investigations have been ordered. This showed that various
factors combined their effects. The weather had its role, however, other factors had been involved : recent houses and a
fire station had been constructed in an area known as being
susceptible of getting submerged. Also, the state authorities did not realize that asking people to stay at home was
inappropriate in case of flooding given the traditionally low
VendÃ©e houses.
In this paper, we define in section 2 an enriched causal

model, built from a causal model and an ontological model.
We then show in section 4 how explanations can be derived
from this enriched causal model. We explain in section 5
the use of argumentation in order to deal with the great
number of possible explanations obtained and conclude in
section 6. The Xynthia example, introduced in section 3, is
used as illustration throughout the article.

2.3

2

simply means that Î² can be deduced from Î± due to specialiis-a
zation/generalization links, the âˆ’â†’ links in the ontological
model, that relate the classes of objects mentioned in Î± and
ded ont
Î². Note that the relation âˆ’â†’ is transitive and reflexive.
Here is an easy illustration. A sedan is a kind of car,
is-a
which is represented by sedan âˆ’â†’ car in the ontological
ded ont
model. Then, Iown(sedan) âˆ’â†’ Iown(car) is an ontological deduction link in the enriched model.
ded ont
Technically, the âˆ’â†’ links among literals are generated
by means of a single principle as follows. The predicates
used in the causal model are annotated so that each of their
parameters is sorted either as universal or as existential.
A universal parameter of a predicate â€œinheritsâ€ by specialization, meaning that if the predicate is true on this parameter then the predicate is also true for specializations
of this parameter. The existential parameters of a predicate
â€œinheritsâ€ by generalization, meaning that if a the predicate
is true on this parameter, then the predicate is also true for
generalizations of this parameter (cf above example of our
owner of a sedan).
As another example, consider the unary predicate
Flooded, where Flooded(o) means that class o (an
area or a group of buildings) is submerged. Its unique
parameter is taken to be â€œuniversalâ€ so that if the literal
Flooded(o) is true, then the literal Flooded(s) is also
true whenever s is a specialization of o. The causal model
ded ont
is enriched by adding Flooded(o) âˆ’â†’ Flooded(s) for
is-a
each class s satisfying s âˆ’â†’ o.
Let us now consider the unary predicate Occurs so
that Occurs(Hurri) intuitively means : some hurricane
occurs. Exactly as above â€œI ownâ€ predicate, this predicate is existential on its unique parameter. By means of
is-a
is-a
the âˆ’â†’ link Hurri âˆ’â†’ SWind, we obtain the following
ded ont
ded ont
âˆ’â†’ link : Occurs(Hurri) âˆ’â†’ Occurs(SWind).
ded ont
Let us provide the formal definition of âˆ’â†’ links in the
case of unary predicates. Then, the general case is a natural,
but intricate and thus ommitted here, generalization of the
unary case.

Enriched causal model = Causal model + ontological model

The model that is used to build tentative explanations
and support argumentation, called the enriched causal model, is built from a causal model relating literals in causal links, and from an ontological model where classes
of objects are related through specialization/generalization
links.
2.1

The causal model

By a causal model [8], we mean a representation of a
body of causal relationships to be used to generate arguments that display explanations for a given set of facts.
The basic case is that of a causal link â€œÎ± causes Î²â€ where
Î± and Î² are literals. In this basic case, Î± stands for the
singleton {Î±} as the general case of a causal link is the
form
{Î±1 , Î±2 , Â· Â· Â· , Î±n } causes Î²
where {Î±1 , Î±2 , Â· Â· Â· Î±n } is a set (interpreted conjunctively)
of literals.
Part of the causal model for our Xynthia example is given in Fig. 2 (each plain black arrow represents a causal
link).
2.2

The ontological model

The literals P (o1 , o2 , Â· Â· Â· , ok ) occurring in the causal
model use some predicates P applied to classes of objects oi . The ontological model consists of specialization/generalization links between classes of objects
is-a

o1 âˆ’â†’ o2 ,
is-a

where âˆ’â†’ denotes the usual specialization link between
is-a
classes. E.g., we have Hurri âˆ’â†’ SWind, House1FPA
is-a
is-a
âˆ’â†’ HouseFPA and HouseFPA âˆ’â†’ BFPA : a â€œhurricaneâ€ (Hurri) is a specialization of a â€œstrong wingâ€
(SWind), and the class of â€œlow houses with one level
only in the flood-prone areaâ€ (House1FPA) is a specialization of the class of â€œhouses in the flood-prone areaâ€
(HouseFPA), which itself is a specialization of the class
of â€œbuildings in this areaâ€ (BFPA). A part of the ontological model for our Xynthia example is given in Fig. 1
(each white-headed arrow labelled with is-a represents an
is-a
âˆ’â†’ link).

The enriched causal model

The causal model is extended by resorting to the ontological model, and the result is called the enriched causal
model. The enrichment lies in the so-called ontological deded ont
duction links (denoted âˆ’â†’ ) between literals. Such a link
ded ont

Î± âˆ’â†’ Î²

Definition 1 Let us suppose that P rop1âˆƒ and P rop2âˆ€ are
two unary predicates, of the â€œexistential kindâ€ for the first
one, and of the â€œuniversal kindâ€ for the second one. If in the
is-a
ontology is the link class1 âˆ’â†’ class2 , then the following
two links are added to the enriched model :
ded ont
P rop1âˆƒ (class1 ) âˆ’â†’ P rop1âˆƒ (class2 ) and
ded ont
P rop2âˆ€ (class2 ) âˆ’â†’ P rop2âˆ€ (class1 ).

ded ont

In our formalism, causal and ontological links âˆ’â†’ entail
classical implication :
Vn
{Î±1 , Â· Â· Â· Î±n } causes Î± entails ( i=1 Î±i ) â†’ Î±.
(1)
ded ont
Î± âˆ’â†’ Î² entails Î± â†’ Î².
Ordinary logical formulas are also allowed (for example
for describing exclusions), which are added to the formulas
coming from (1).
When resorting to explanations (see section 4 below),
ded ont
these âˆ’â†’ links are extended to sets of literals (links denoont
ted by â†’ded
) as follows :
set
Definition 2 Let Î¦ and Î¨ be two sets of literals, we define
ont
Î¦ â†’ded
Î¨, if for each Ïˆ âˆˆ Î¨, there exists Ï• âˆˆ Î¦ such
set
ded ont
ded ont
that Ï• âˆ’â†’ Ïˆ (remind that Ïˆ âˆ’â†’ Ïˆ).

The following predicates are introduced : (Flooded)
and (Victims_I) applied to a group of building respectively meaning that â€œfloodingâ€ occurs over this group, and
that there were â€œvictimsâ€ in this group (I âˆˆ {1, 2, 3} is
a degree of gravity, e.g. Victims_1, Victims_2 and
Victims_3 respectively mean, in % of the population
of the group : â€œa small numberâ€, â€œa significant numberâ€
and â€œa large numberâ€ of victims). OK means that its
unique parameter is in a normal state. Occurs means
that some fact has occurred (a strong wind, a disaster,
. . .), Expected means that some fact is expected to occur.
All these predicates are â€œuniversalâ€ on their unique parameter, except for the predicates Occurs and Expected
which are â€œexistentialâ€.

is-a

Notice that if a sedan is a kind of car (sedan âˆ’â†’ car),
and Own a predicate existential on its unique argument,
ont
then we get
{Iown(sedan), IamHappy} â†’ded
set
{Iown(car), IamHappy},
and
ont
{Iown(car)}.
{Iown(sedan), IamHappy} â†’ded
set

Note that negation of a literal is expressed by Neg
as in Neg OK(Anemo) meaning that the anemometer is not in its normal behaviour state (the formula
Â¬(OK(Anemo) âˆ§ (NEG OK(anemo)) is thus added).

3

The classes and the ontological model are given in Fiis-a
gure 1 with the âˆ’â†’ links represented as white-headed arrows labelled with (is-a).

The Xynthia example

From various enquiries, including one from the French
parliament 1 and one from the Cours des Comptes 2 and
many articles on the subject, we have many information
about the phenomenon and its dramatic consequences. We
have extracted a small part from all the information as an
illustration of our approach.
The classes we consider in the ontological model and/or
in the causal model are the following ones : Hurri,
SWind, BFPA, House1FPA, HouseFPA, and BFPA
have already been introduced in Â§2.2, together with a
is-a
few âˆ’â†’ links. Among the buildings in the flood-prone
area, there is also a fire station FireSt (remind that
we have also a group of houses HouseFPA, including a
group of typical VendÃ©e low houses with one level only
House1FPA). We consider also three kinds of natural
disasters (NatDis) : Hurri, together with â€œtsunamiâ€
(Tsun) and â€œfloodingâ€ (Flooding). As far as meteorological phenomena are concerned, we restrict ourselves
to â€œVery low pressureâ€ (VLPress), together with already
seen Hurri and SWind, and finally we add â€˜high spring
tideâ€ (HSTide) to our list of classes.
Two kinds of alerts (Alert) may be given by the
authorities, â€œAlert-Evacuateâ€ (AlertEvac) and â€œAlertStayAtHomeâ€ (AlertStay). We consider also an anemoter (Anemo) able to measure the wind strength and a fact
asserting that â€œpeople stay at homeâ€ (PeopleStay).

NatDis
(isâˆ’a)

SWind
(isâˆ’a)

(isâˆ’a)

Alert
(isâˆ’a)

(isâˆ’a)

Tsun Flooding Hurri
BFPA

(isâˆ’a)

AlertEvac

AlertStay

VLPress

(isâˆ’a)

(isâˆ’a)

FireSt

HouseFPA

Anemo

(isâˆ’a)

HSTide
PeopleStay

House1FPA

F IGURE 1 â€“ Ontological model for Xynthia
Part of the causal model is given in Figure 2 (remember,
black-headed arrows represent causal links). It represents
causal relations between [sets of] literals. It expresses that
an alert occurs when a natural disaster is expected, or when
a natural disaster occurs. Also, people stay at home if
alerted to stay at home, and having one level home flooded
results in many victims, and even more victims if the fire
station itself is flooded,...

From the ontological model and the causal model, the
ded ont
enriched causal model can be build, adding âˆ’â†’ links between literals when possible.
For instance, for our â€œexistentialâ€ predicates, from
is-a
Hurri âˆ’â†’ SWind, the links
ded ont
Occurs(Hurri) âˆ’â†’ Occurs(SWind), and
ded ont
Expected(Hurri) âˆ’â†’ Expected(SWind) are ad1. http://www.assemblee-nationale.fr/13/rap-info/ ded.
i2697.asp
In the same way, for a universal predicate,
2. www.ccomptes.fr/Publications/Publications/
is-a
from House1FPA âˆ’â†’ BFPA, is added the link
Les-enseignements-des-inondations-de-2010-sur
ded ont
-le-littoral-atlantique-Xynthia-et-dans-le-Var
Flooded(BFPA) âˆ’â†’ Flooded(House1FPA)

Expected(VLPress)

We require also that the origin of the explanation is
possible, thus the full set of justifications is here the set
Î¦ âˆª {Î´}, while in case of (2), this full set is Î¦, even if
no set is given explicitly. Indeed, it is always understood
that the starting point of the explanation link must be
possible,Vthus we sometimes omit it. Remind that from (1)
we get ( Ï•âˆˆÎ¦ Ï•) â†’ Î² and Î´ â†’ Î², thus adding Î² to the
justification set is useless.

Expected(SWind)

Occurs(SWind)

Red(Anemo)

Occurs(Hurri)

Â¬ OK(Anemo)

OK(Anemo)
Expected(NatDis)
Occurs(Alert)
Occurs(NatDis)
Occurs(VLP)
Occurs(SWind)
Occurs(HSTide)

Occurs(Flooding)

Occurs(AlertStay)

Flooded(BFPA)

Occurs(PeopleStay)

Victims_1(BFPA)

Victims_2(House1FPA)

Flooded(House1FPA)
Victims_2(House1FPA)
22

3
Victims_3(House1FPA)

Flooded(FireSt)

F IGURE 2 â€“ A part of the causal model for Xynthia
(cf Fig. 6).
Figure 3 represents a part of the enriched causal model,
where the (unlabelled) white-headed arrows represent the
ded ont
âˆ’â†’ links and each black-headed arrow represents a causal
link (from a literal or, in case of forked entry, form a set of
literals).
Occurs(VLPress)
Occurs(SWind)

Occurs
(Flooding)

Flooded
(BFPA)

Victims_1
(BFPA)

Occurs(HSTide)

An explaining link
Î¦ can explain Î± provided Î¨ is possible
where Î¨ is a set of literals means that
Î± can be explained by Î¦ provided the set Î¦ âˆª Î¨ is possible :
if adding Î¦ âˆª Î¨ to the available data leads to inconsistency,
then the explanation link cannot be used to explain Î´ by Î¦.
In the figures, white-headed arrows represent â€œontologided ont
is-a
cal linksâ€ : âˆ’â†’ links for bare arrows, and âˆ’â†’ links for
arrows with (is-a) mentioned, while dotted arrows represent explanation links (to be read can explain) produced
by these rules, these arrows being sometimes labelled with
the corresponding justification set.

Flooded
(House1FPA)
Occurs
(AlertStay)

Occurs
(PeopleStay)

Phi

Victims_2
(House1FPA)
Flooded
(FireSt)

delta

4.1

Phi

beta
{delta}

delta

Victims_3
(House1FPA)

F IGURE 4 â€“ The explanation link from (3) when Î¦ = {Ï•}

F IGURE 3 â€“ Numerous victims in low and flood-prone
houses

4

beta

Explanations
Explaining a literal from a [set of] literal[s]

Causal and ontological links allow us to infer explanation links. We want to exhibit candidate reasons that can
explain a fact by means of at least one causal link. We disregard explanations that involve only links of the implicational kind. Here is how causal and ontological links are
used in our formalism :
Let Î¦ denote a set of literals and Î² be a literal.
The basic case is that Î¦ causes Î²
yields that
Î² can be explained by Î¦.

We want also that our explanation links can follow
ded ont
âˆ’â†’ links as follows, in the case where Î¦ = {Ï•} is a singleton (thus we note sometimes Ï• instead of {Ï•}) :


Ï• can explain Î´ provided Î¨ is possible
ded ont
ded ont
Ï•0 âˆ’â†’ Ï•
Î´ âˆ’â†’ Î´1
(4)
yields that Ï•0 can explain Î´1 provided Î¨ is possible
{Ï•0 } âˆª Î¨ is the full justification set for this explanation of
Î´1 by Ï•0 . Again we get Ï•0 â†’ Ï• by (1), so we need not to
mention Ï• here.
Phi

The general initial case involves two literals Î² =
P rop(cl2 ) and Î´ = P rop(cl1 ) built on the same predicate P rop (eventual other parameters equal in these two
literals) :


Î¦ causes Î²
Î´ can be explained by Î¦,
yields that
(3)
ded ont
provided Î´ is possible.
Î´ âˆ’â†’ Î²
{Î´} is the set of justifications for this explanation of Î´ by Î¦.

Phi {delta}

delta
Phi0

(2)

delta1

{delta}

delta1
delta

Phi0
ded ont

F IGURE 5 â€“ Explanation links follow âˆ’â†’ links [cf (4)]
Generalizing the case of (4) when Î¦ is a set of literals
ont
needs the use of â†’ded
:
set


Î¦ can explain Î´ provided Î¨ is possible
ded ont
ont
Î¦0 â†’ded
Î¦
Î´ âˆ’â†’ Î´1
(5)
set
yields that Î¦0 can explain Î´1 provided Î¨ is possible
Now, we also want that our explanation links are transitive, and this necessitates to be able to explain not only

literals, but sets of literals. Thus we introduce explanation
links among sets of literals, which extend our explanation
links from sets of literals towards literals (since it is an extension, we can keep the same name explanation link) :
4.2

Explaining a set of literals from a set of literals

Definition 3 Let n be some natural integer, and, for
i âˆˆ {1, 2, Â· Â· Â· , n} Î¦, Î¨i be sets of literals and Î´i
be literals. If, for each i âˆˆ {1, 2, Â· Â· Â· , n}, we have
Î¦ can explain Î´i provided Î¨i is possible,
then we define the following [set] explanation link
Î¦ can explain {Î´S
i /i âˆˆ {1, 2, Â· Â· Â· , n}}
n
provided i=1 Î¨i is possible.
Again, such an explanation link
Sn applies only when its
(full) justification set, here Î¦ âˆª i=1 Î¨i , is possible (not
contradicted by the data).
Notice that we do not want to explain Î¦ by Î¦ itself, and
ont
links :
we extend this restriction to â†’ded
set
We do not want to explain Î¦ by Î¦0 is all we know is
ont
Î¦. Indeed, this seems to be cheating about what
Î¦0 â†’ded
set
an explanation is (we want some causal information to play
a role).
However, in the line of (5), we want that explanations
ont
links, thus we introduce the following definifollow â†’ded
set
tion :
Definition 4 If we have


Î¦ can explain âˆ† provided Î¨ is possible
ont
ont
âˆ†1
Î¦
âˆ† â†’ded
Î¦0 â†’ded
set
set
then we have
Î¦0 can explain âˆ†1 provided Î¨ is possible
Again, the full justification set of the resulting explaining
link is Î¦0 âˆª Î¨.
Our last definition of [set] explanation links concerns
transitivity of explanations. This is a â€œweakâ€ transitivity
since the justifications are gathered, however, we will call
this property â€œtransitivityâ€. We need to be able to omit in
the resulting link the part of the intermediate set which is
already explained in the first explanation giving rise to a
transitive link :
Definition 5 If


Î¦ can explain âˆ†1 âˆª âˆ†2 provided Î¨1 is possible and
Î“ âˆª âˆ†2 can explain Î˜ provided Î¨2 is possible,
then Î¦ âˆª Î“ can explain âˆ†1 âˆª âˆ†2 âˆª Î˜
provided Î¨1 âˆª Î¨2 is possible.
Again, the full justification set is Î¦ âˆª Î“ âˆª Î¨1 âˆª Î¨2 .

4.3

About explanation links and arguments

As a small example, let us suppose that in the causal
model is the link
reason causes P ropâˆƒ (class2 )
and that in the ontology are the links :
is-a
class1 âˆ’â†’ class2
is-a
class1 âˆ’â†’ class3
Resulting are the next two links in the enriched model :
ded ont
P ropâˆƒ (class1 ) âˆ’â†’ P ropâˆƒ (class2 )
ded ont
P ropâˆƒ (class1 ) âˆ’â†’ P ropâˆƒ (class3 )
By pattern matching over the diagrams in Fig. 4 and 5
[cf patterns (3) and (5], we get
reason can explain P ropâˆƒ (class3 ) through P ropâˆƒ (class1 )
(â€œjustificationâ€).
Importantly, it is assumed that the causal link is expressed
on the appropriate level : in other words, should there be
some doubts about the kind of objects (here class2 ) that
enjoy P ropâˆƒ due to reason, the causal link would be about
another class.
The proviso accompanying the explanation takes place
at the level of justifications : the candidate explanation is
worth inasmuch as P ropâˆƒ (class1 ) is not contradicted. In
particular it must be consistent with the reason causing
P ropâˆƒ (class3 ) [remind (1)].
Here is an example from Xynthia data. Consider the causal link
Expected(V LP ress) causes Expected(SW ind)
together
with the following ontological
links
(
)
is-a
Hurri âˆ’â†’ SW ind
is-a

Hurri âˆ’â†’

N atDis.

The links below can be obtained in the enriched model
(
)
ded ont
Expected(Hurri) âˆ’â†’ Expected(SW ind)
ded ont
Expected(Hurri) âˆ’â†’ Expected(N atDis)
We get an argument to the effect that from the set of
data
ï£± Î˜=
ï£¼
ï£²Expected(V LP ress) causes Expected(SW ind)ï£½
ded ont
Expected(Hurri) âˆ’â†’ Expected(StrW )
ï£³
ï£¾
ded ont
Expected(Hurri) âˆ’â†’ Expected(N atDis)
we obtain :

Î˜ yields that

Expected(V LP ress) can explain
Expected(N atDis) provided Expected(Hurri) is
possible.
The intuition is that, from these data, it is reasonable
to explain Expected(N atDis), by Expected(V LP ress),

provided Expected(Hurri) is possible (not contradicted
by other data).
4.4

An example of compound explanations

Figure 6 displays an example from Xynthia (cf also
Fig. 3) of a few possible explanations, represented by
dotted lines, with their label such as 1 or 1a. The sets
of literals, from which the explaining links start, are
framed and numbered from (1) to (5). This shows transitivity of explanations at work : e.g. set 1 can explain
Victims_1(BFPA) (explanation link labelled 1+1a+1b)
uses explanation links 1, 1a and 1b.
Another example is
set 5 can explain Victims_3(House1FPA) (explanation link 1+1a+2+3) :
explanations 1, 1a, 2 and 3 are at work here, and link 2 uses
links 1 and 1a together with
ded ont
Flooded(BFPA) âˆ’â†’ Flooded(House1FPA)
while explanation 3 comes from links 1+1a together with
ded ont
Flooded(BFPA) âˆ’â†’ Flooded(FireSt).

1

Occurs(VLPress)
(1) Occurs(SWind)

Occurs(AlertStay)
(2)

1b

1a

Occurs
(Flooding)

Occurs(HSTide)

1+1a+1b

1+1a
Flooded
(BFPA)

Victims_1
(BFPA)

Argument : That
â€œÎ¦ can explain Î³ in view of Î˜, provided Î´ is possibleâ€
is formalized here as an argument whenever (3) holds, that
is :
Î˜ yields that Î³ can be explained by Î¦, provided Î´ is
possible.
The components of an argument consist of :
â€“ Î¦, the explanation, a set of literals.
â€“ Î³, the statement being explained, a literal.
â€“ âˆ†, the justification of the explanation (see Section 4),
a set of literals.
â€“ Î˜,
 evidence, comprised of propositions (e.g.,
V the
Î¦ â†’ Î³), causal links
(e.g., Î¦ causes Î²), and ontological deduction links
ded ont
(e.g., Î´ âˆ’â†’ Î²).

Occurs(PeopleStay)
Victims_2
(House1FPA)

Flooded(House1FPA)

Victims_2(House1FPA)
(3)
Flooded(FireSt)

2
Victims_3(House1FPA)

Here is an illustration from the Xynthia event. From
Fig. 2, that the BF P A buildings are flooded can be
explained via the set of two causal links

3
1+1a+2+3

Occurs(VLPress)
(4)

get any alarm. However, this counter-argument may itself
be attacked by remarking that, in the case of a hurricane,
that is a kind of strong wind, an anemometer is no longer
operating, which can explain that a red alarm cannot be observed.
Let us see how to consider formally argumentation when
relying on an enriched causal model and explanations as
described in sections 2 and 4. Of course, we begin with
introducing arguments, as follows.

1+1a+2

Occurs(StWind)
Occurs(HSTide)

Î˜ï£±
= { Occurs(F looding)
ï£¼ causes F looded(BF P A),
Occurs(V
LP
ress)
ï£²
ï£½
Occurs(SW ind)
causes Occurs(F looding) }
ï£³
ï£¾
Occurs(HST ide)

Occurs(AlertStay)
(5)

Flooded(FireSt)

(bis)

F IGURE 6 â€“ A few explanations for victims

5

Argumentation

As just seen, the enriched causal graph allows us to infer
explanations for assertions and these explanations might be
used in an argumentative context [2, 3]. Let us first provide
some motivation from our example.
A possible set of explanations for the flooded buildings
is constituted by the bad weather conditions (â€œvery low
pressureâ€ and â€œstrong windâ€) together with â€œhigh spring
tideâ€ (see Fig. 2). Given this explanation (argument), it is
possible to attack it by noticing : a strong wind is supposed
to trigger the red alarm of my anemometer and I did not

More precisely, using the basic case (2) twice, we obtain that F looded(BF P A) can be explained by the set of
literals
ï£±
ï£¼
ï£² Occurs(V LP ress) ï£½
Occurs(SW ind)
Î¦=
ï£³
ï£¾
Occurs(HST ide)
The corresponding argument is
Î˜ yields that Î³ can be explained by Î¦, provided âˆ† is
possible
where
â€“ The explanation is Î¦.
â€“ The statement being explained is Î³
F looded(BF P A).
â€“ The justification of the explanation is empty.
â€“ The evidence is Î˜.

=

5.1

Counter-arguments

Generally speaking, an argument â€œÎ¦ can explain Î³ in
view of Î˜, provided âˆ† is possibleâ€ is challenged by any
statement which questions
1. either Î¦ (e.g., an argument
exhibiting an explanation
V
for the negation of Î¦)
2. or Î³ (e.g., an argument exhibiting an explanation for
the negation of Î³)
3. or âˆ† (e.g., an argument
exhibiting an explanation for
V
the negation of âˆ†)
4. or any item in Î˜ (e.g., an argument exhibiting an explanation for the negation of Î˜ for some Î¸ occurring
in Î˜)
5. or does so by refutation : using any of Î¦, Î˜, âˆ† and Î³
to explain some falsehood.
Such objections are counter-arguments (they have the form
of an argument : they explain something â€“but what they
explain contradicts something in the challenged argument).
Dispute. Let us consider the illustration at the start
of this section : The argument (that the buildings in the
flood-prone area are flooded can be explained, partly, by
a strong wind) is under attack on the grounds that my
anemometer did not turn red â€“indicating that no strong
wind occurred. The latter is a counter-argument of type
5 in above list. Indeed, the statement to be explained
by the counter-argument is the falsehood Red(anemo)
(e.g., Green(Anemo) has been observed), using SW ind,
i.e., an item in the explanation in the attacked argument.
Remember, the attacked argument is
ï£¼ P A) can be explained by
ï£± Î˜ yields that F looded(BF
ï£² Occurs(V LP ress) ï£½
Occurs(SW ind)
ï£¾
ï£³
Occurs(HST ide)
Taking Red(Anemo) to be a falsehood, the counterargument at hand is
can be explained by
 Î˜â€™ yields that Red(anemo)

Occurs(SW ind)
OK(Anemo)
where
â€“ The explanation is
Î¦0 = {Occurs(SW ind), OK(Anemo)}.
â€“ The statement being explained is Î³ 0 = Red(Anemo).
â€“ The evidence is
Î˜0 =



Occurs(SW ind)
causes Red(anemo)
OK(Anemo)
In our illustration, this counter-argument has in turn a
counter-argument (of type 1.) explaining why the anemometer did not get red : i.e., explaining the negation of an

item (OK(Anemo) is the item in question) in the explanation in the counter-argument. So, the counter-counterargument here is :
Î˜â€ yields that

Â¬OK(Anemo) can be explained by Occurs(Hurri)
where
â€“ The explanation is Î¦00 = {Occurs(Hurri)}.
â€“ The statement being explained is
Î³ 00 = Â¬OK(Anemo).
â€“ The evidence is
Î˜00 = {Occurs(Hurri) causes Â¬OK(Anemo)}
The dispute can extend to a counter-counter-counterargument and so on as the process iterates.

6

Conclusion

The aim of this work is to study the link between causes
and explanations [5, 6], and to rely on explanations in an
argumentative context [2].
In a first part, we define explanations as resulting from
both causal and ontological links. An enriched causal model is built from a causal model and an ontology, from
which the explaining links are derived (cf e.g. Fig4 and
(2). Our work differs from other approaches in the literature in that it strictly separates causality, ontology and explanations, while considering that ontology is key in generating sensible explanations from causal statements. Note
however that some authors have already introduced ontology to be used for problem solving tasks as planning [7,
Chapter 2] and more recently for diagnosis and repair [9].
We then argue that these causal explanations are interesting
building blocks to be used in an argumentative context.
Although explanation and argumentation have long been
identified as distinct processes [10], it is also recognized
that the distinction is a matter of context, hence they both
play a role [4] when it comes to eliciting an answer to a
â€œwhyâ€ question. This is exactly what is attempted in this
paper, as we are providing â€œpossibleâ€ explanations, that
thus can be turned into arguments. The argument format
has some advantages inasmuch as its uniformity allows us
to express objection in an iterated way : â€œpossibleâ€ explanations are challenged by counter-arguments that happen
to represent rival, or incompatible, â€œpossibleâ€ explanations.
However, a lot remains to be done. Among others, comparing competing explanations according to minimality, preferences, and generally a host of criteria.
We have designed a system in answer set programming
that implements most of the explicative proposal introduced above. We plan to include it in an argumentative framework and think it will a good basis for a really practical
system, able to manage with a as rich and tricky example
as the full Xinthia example.

RÃ©fÃ©rences
[1] Philippe Besnard, Marie-Odile Cordier, and Yves
Moinard. Ontology-based inference for causal explanation. Integrated Computer-Aided Engineering,
15 :351â€“367, 2008.
[2] Philippe Besnard and Anthony Hunter. Elements of
Argumentation. MIT Press, Cambridge, 2008.
[3] Phan Minh Dung. On the acceptability of arguments
and its fundamental role in nonmonotonic reasoning,
logic programming and n-person games. Artificial Intelligence, 77 :321â€“357, 1995.
[4] Justin Scott Giboney, Susan Brown, and Jay F. Nunamaker Jr. User acceptance of knowledge-based system recommendations : Explanations, arguments, and
fit. In 45th Annual Hawaii International Conference
on System Sciences (HICSSâ€™45), pages 3719â€“3727.
IEEE Computer Society, 2012.
[5] Joseph Halpern and Judea Pearl. Causes and Explanations : A Structural-Model Approach. Part I : Causes.
In Jack S. Breese and Daphne Koller, editors, 17th
Conference in Uncertainty in Artificial Intelligence
(UAIâ€™01), pages 194â€“202. Morgan Kaufmann, 2001.
[6] Joseph Y. Halpern and Judea Pearl. Causes and Explanations : A Structural-Model Approach. Part II :
Explanations. In Bernhard Nebel, editor, 17th International Joint Conference on Artificial Intelligence
(IJCAIâ€™01), pages 27â€“34. Morgan Kaufmann, 2001.
[7] Henry Kautz. Reasoning about plans. In James F.
Allen, Henry A. Kautz, Richard N. Pelavin, and Joshua D. Tennenberg, editors, Reasoning About Plans,
chapter A Formal Theory of Plan Recognition and its
Implementation, pages 69â€“126. Morgan Kaufmann,
1991.
[8] Dov Hugh Mellor. The Facts of Causation. Routledge, London, 1995.
[9] Gianluca Torta, Daniele Theseider DuprÃ©, and Luca
Anselma. Hypothesis discrimination with abstractions based on observation and action costs. In Alban
Grastien and Markus Strumpter, editors, 19th Workshop on Principles of Diagnosis (DXâ€™08), pages 189â€“
196, Blue Mountains, NSW, Australia, 2008.
[10] Douglas Walton. Explanations and arguments based
on practical reasoning. In Thomas Roth-Berghofer,
Nava Tintarev, and David B. Leake, editors, Workshop on Explanation-Aware Computing at IJCAIâ€™09,
pages 72â€“83, Pasadena, CA, U.S.A., July 2009.





We propose an integration of possibility theÂ­
ory into non-classical logics. We obtain many
formal results that generalize the case where
possibility and necessity functions are based
on classical logic. We show how useful such
an approach is by applying it to reasoning unÂ­
der uncertain and inconsistent information.
1

IRIT
Paul Sabatier
118 route de Narbonne
3 1062 Toulouse Cedex
France

U niversite

Now, possibility theory brings in something more that
should be fruitfully exploited as complementary to
such aspects of reasoning. Hence, we study how to
integrate possibility theory with non-classical logics.
Our work comes from the following two facts:
â€¢

even when the involved uncertainty has a possiÂ­
bilistic nature, "classical" possibility theory may
not be well-suited to the addressed problem, due
to shortcomings, not of possibility theory itself
but of classical logic, on which possibility theory
is defined. For example, some problems require a
formalization with a local view of inconsistency:
this is impossible with classical possibility theory
(we need a paraconsistent approach, cfSection 3).

â€¢

on the ot.her hand, non-standard logics su ch as
intuitionistic logic, paraconsistent logics,. . . are
not expressive enough to express uncertainty in
a gradual way.

Introduction

Possibility theory has been widely used in Artificial InÂ­
telligence to represent uncertain knowledge in a more
qualitative way than, for example, probability theory:
indeed, it is equivalent to work with "quantitative"
possibility theory (which means using possibility and
necessity measures and possibility distributions, which
map formulas or worlds to [0, 1]) or with its qualitative
counterpart (where qualitative necessity and possibilÂ­
ity relations are preorders on the logical language and
qualitative possibility distributions are just preorders
on the set of worlds). Besides, its connection to variÂ­
ous qualitative formalisms in logic and Artificial IntelÂ­
ligence has been established, notably with epistemic
entrenchment relations in [DP 9la], conditional logics
in [Bou 92] [F HL 94], System Z in [BDP 92]. The use of
possibility theory in Artificial Intelligence covers nonÂ­
monotonic reasoning [DP 91b], belief revision, inconÂ­
sistency handling, inheritance and default rules hanÂ­
dling, temporal reasoning, constraint satisfaction, . . .
In Knowledge Representation, many non-classical logÂ­
ics have been used (note that in this paper we consider
only non-classical logics sharing the same language as
classical logic). Each of them was intended for some
particular focus, a specific aspect of reasoning: E.g.
paraconsistent logics have been used to deal with conÂ­
tradictory knowledge bases. Or, intuitionistic logic has
been used to take into account some subtle distinctions
between statements involving double negation for exÂ­
ample. Or, Kleene's 3-valued logic {and other manyÂ­
valued logics) has been used to cope with statements
for which neither truth nor falsity make sense.

These arguments show that it is generally valuable to
integrate non-classical logics with a numerical theory
of uncertainty. Now, the reason why we focus in this
paper on possibility theory rather than another theory
of uncertainty, is its qualitative nature (as it amounts
to a "numerical account" of preordering relations over
formulas or worlds), which should make it a priori simÂ­
pler to generalize than more quantitative approaches
such as probability theory or belief functions.
The methodology we follow in this paper consists of
going from the general case to the particular case:
â€¢

in Section 2, we investigate whether, and under
which conditions, important properties of possiÂ­
bility theory remain valid when generalized. We
state the results in the most general case to make
the study "reusable", though the applications deÂ­
veloped in Section 3 focus on paraconsistency.

â€¢

in Section 3, we take a case study, that is, we
choose a paraconsistent logic (namely Ct) and disÂ­
cuss more practical applications to reasoning with
uncertain and inconsistent information.

*Research supported by CNRS in proj ect "Gestion de
l'evolutif et l'incertain dans une ba.$e de connaissances".

Besnard and Lang

70

Non-classical necessity and

2

possibility functions
2.1

Necessity /possibility functions

The natural presentation of necessity and possibility
functions (see [Zad 78] for instance) shows that posÂ­
sibility theory consists in meta-level definitions over
classical logic, which respect completely the structure
of classical logic.
This suggests that similar functions could be defined
on other logics than classical logic; so, replacing (.C,I-)
by (.C, f---L) where L is a given non-classical logic, we can
look for a definition of possibility/necessity functions
on the logic L. We deal with classical propositional1
languages, built from a list of propositional variables
- sometimes required to be finite -, and the connecÂ­
tives -., 1\, V, ï¿½, <-> (where 1-L 9 <--> t/; is a shorthand
for 1-L 9 -+ t/; and 1-L t/; -+ Â¥') . The only varyÂ­
ing parameter is then the consequence relation f-L.
We now give a generic definition of non-classical neÂ­
cessity /possibility functions, of which the usual neÂ­
cessity /possibility functions correspond to the special
case where L is classical logic (Section 3 deals with the
special case where L is the paraconsistent logic Cr).

Definition: let .C be a classical propositional language
and 1-L a consequence relation, L being a given (maybe
non-classical) logic. A L-necessity function is a mapÂ­
ping N from .C to [0, 1] satisfying the following axioms2
(Taut) if

f---L 9

N (9)

then

=

1

The dual functions of necessity functions are called
possibility functions. They can be defined by 3 axioms
about contradiction, equivalence and disjunction:

Definition: A L-possibility
from C to [0, 1] such that

function is a mapping II

(Contr) if 1-L â€¢({) then II(Â¥=')= 0
(Eqn) if l- L 9 +-+ t/; then II(If') = II(t/;)
(Disj) II(9 V Â¢) = max(II(If'), II(t/;))
Whatever the logic L, the next property entails (Eq n):
(Domn) iff-L

9--+

t/; then II('P)::; II(t/;)

Proposition 2: (Domrr) is entailed by (Eqrr) and
(Disj) on condition that f- L satisfies:
1-L

2.2

(/)-+

Some properties of non-classical
necessity /possibility functions

W hen L is classical logic, (L-)possibility functions can
be defined from (L-)necessity functions by means of
\lip E Â£, ll(lf') = 1- N(...,r.p) and (L-)necessity funcÂ­
tions can be defined from (L-)possibility functions by
V'(J E .C,N(Â¥') = 1-II(-.Â¥')Â· That is, "classical" necesÂ­
sity and possibility functions enjoy the (double) dualÂ­
ity property:
(Dl} II is a possibility function iff d n : Â£ -+ [0, 1]
defined by drr (r.p) 1 -II( -,'P) is a necessity funcÂ­
tion.
(D2) N is a necessity function iff dN : C -+ [0, 1] deÂ­
fined by dN(9) = 1- N(-.9) is a possibility funcÂ­
tion.
=

(Eq) if 1-L 9 <--> t/; then N('P) = N(t/;)
(Conj) N(Â¥' 1\ t/;) = min(N(Â¥'), N(t/;))
When L is classical logic, we recover the classical neÂ­
cessity functions. Whatever the logic L is, the followÂ­
ing property always entails (Eq):
(Dom) if f---L 'P-+ t/; then N('P)::; N (t/;)

Some questions we may ask are: how can (D1) and
(D2) carry over to L-necessity and L-possibility funcÂ­
tions? When are (Dl) and (D2) equivalent?

Proposition 1: (Dom) is entailed by (Eq) and (Conj)

Proposition 3: if 1-L

on condition that l-L satisfies:

Proposition 4: if L satisfies

f-L\;'ï¿½1/J
Hence, for all logics

L

fulfilling the latter condition, a
as a funcÂ­
tion N: .C----> [0, 1] satisfying (Taut), (Conj), (Dom).

necessity function can then be characterized

1 For

the sake of simplicity, we

consid er

( <p)

tra axiom (Contr) if f- â€¢<p then N
for example, in
the quantity

0 but not all:
N{) > 0, reflects
=

a degree of (partial) inconsistency.
Note that requiring
(Contr) or not -and the same for (Taut)- does not make
much difference since (Dam) ensures that contradictions
(resp. tautologies) have anyway the lowest (resp. highest)
necessity degree. Now, the reason why we require (Taut)
and not (Contr) concerns the characterization of necessity
functions in terms of possibility distributions.

-.-.r.p

<-->Â¥' then (D1)

{::}

(D2).

1. 1-L r.p +-+ ...,-,'(J
2. 1- L -.(91\ t/;) ï¿½ ( -.9 V -.'ljJ)
3 . f-L (tp V '1/J) ï¿½ (-.r.p 1\ -,.,p)
-.

and the following inference rules

1-L(/)

only the propoÂ­

sitional level throughout the paper.
2
Many definitions of necessity functions include the exÂ­

[DLP 94]

'1/J

1-Llp-+tj;
'1/J

j- L

f---L({)--"1/J
1-L -.'ljJ -+ -.r.p

then (D1) and (D2) hold.
Note that among non-classical logics admitting (1)-(3)
and the above two inference rules (modus ponens and
contraposition), there are various relevant logics such
as the logic E [AB 75]. Let us now have a look on
necessary conditions for having (D1) (or (D2)).

Proposition 5: if there exists 9 such that f-L r.p and
r.p then (Dl) does not hold.
IfL
-.-.

Possibility and Necessity Functions over Non-Classical Logics

Proposition

and lfL

6: if there exists I{) such that rL
(D2) does not hold.

1p then

--,---,1{)

Next, we investigate a few issues related to the conÂ­
dition under which a function from C to [0, 1] can be
both a necessity and a possibility function.
a truth-functional valuation is a fun cÂ­
tion f from C to [0, 1] such that there exist two nonÂ­

Definition:

decreasing operators EB andÂ® from (0, 1]2 to (0, 1] such
that 'V1p, t/J, f('P V t/J) = f(cp) ffi f(t/J) and /('P 1\ t/J) =
f(cp) 0 f(,P).
Definition: a logic L is said to admit trivialisation
of truth-functional valuations iff any truth-functional
valuation f satisfying (Dom), i.e. f-L 1p -+ t/J implies
f( 1p) ï¿½ f( t/J) (we will also say that f is monotonic
w.r.t. 1-L) is a classical valuation, i.e. there are two
values O" and 1* such that Vip,/(IP) E {oâ€¢,l*} and
f(-.IP) f= f(IP)It is well-know n that trivialisation of truth-functional
va luations holds in the case of classical propositional
logic ([Wes 87] (DP 88] - see also [DP 94] for a discusÂ­
sion on the implications of this result). To study the
condition under which this property also holds in the
case of non-classical logics, Je t us consider the followÂ­
ing assumptions:
1. 'r-Lip-+ipVt/J
2.

f-.LI{)I\1/J-1{)

3.

f-L

'P

1\

2.3

Semantics of L-necessity /possibility
functions

With the assumption that C is built from a finite
number of propositional variables, "classic al " necesÂ­
sity /possibility functions can be semantically defined
by means of possibility distributions: a possibility disÂ­
tribution 7r is simply a fun cti on from the set n of all inÂ­

terpretations for L to (0, 1]. The necessity function inÂ­
duced by 1r is defined by N ( 'P) = inf{1-7r(w) /w f= ''P}
(with the convention inf 0 = 1 th at we take in all the
paper as well as sup 0 = 0). It can then be proved
that N is a necessity measure, and that any necessity
measure is induced by a possibility distribution.
We now turn to the general case of a logic L f or which
the class of L-m ode ls is wri tten nL .
a L-possibility distribution is a mapping
from ï¿½h to [0, 1 ] . It is said to be normalized iff
sup v E: i1L 7r(v) = 1.
Definition:
1r

In classical logic, due to the equivalence between v li: 'P
-.cp, the two following defini ti ons for inducing
a C-necessity function from a C-possibility distribution
a re e qui valent:

and v f=

N(cp)
N(cp)

V cp +->I{)

5. 'rL

'P

1\ '1j;

+-+

'1j; 1\ 'P

6. f-. L

cp V 1/J

+->

t/J V cp

II(cp)
ll(cp)

let L be a logic satisfying (1) to (8)
and f a truth-functional valuation mon otonic w .r .t.
f-L. Then we have ffi = max and Â® = min.

Proposition 7:

8: let L be a logic satisfying (1) to (8)
and excluded middle, and f a truth-functional valuaÂ­
tion on L mo notonic w.r.t. f-.LÂ· Then, Vcp,f(<p) = 1*
or f(-,cp) = 1â€¢, where 1â€¢ = sup{ /('P) , cp E C}.
Proposition

Proposition 9: let L be a logic satisfying (1)-(8) and
non-contradiction and f a truth-functional valuation
monotonic w.r.t. 1-Â£. Then Vcp,/(cp) = 0* or f(-,IP) =
oâ€¢ where 0* = inf{f(cp),ip E Â£}.
Corollary 10: any logic satisfying (1) to {8), excluded
middle and non-contradi c tion admits trivialisation of
truth-functional valuations.

=

sup{7r(v)/v ï¿½ 'P}
sup{7r(v)/v f= ''P}

JI{1r), /2{1r), /a(7r), /4(1r) are the mapÂ­
pings from ,C to (0, 1] induced from 1r by:

Definition:

7. 'r-L (cpl\t/J)I\ ï¿½ +-+cpi\(1/J/\0
8. 'rL('P Vt/J)Yï¿½+-+cp V(t/JYï¿½)
9. f-.L cp V -.cp (exc luded middle)
10. f-.L -.(cp/\-.cp) (non-contradiction)
Again, an example of a logic satisfying these properties
is the logic E [AB 75]. On the other hand, intuitionistic
logic and paraconsistent logics do not.

1- sup{1r(v)/v li: cp}
1- sup{7r(v)/v F ''P}

Analogously, for p ossibitity functions:

'P +->I{)

4. 'r-L I{)

71

ft(7r)(cp)
f2(7r)(cp)
/a(7r)('P)
/4(7r)(cp)

=

1- sup{7r(v)/v
1- sup{7r(v)lv
sup{1r(v)/v FL
::: sup { 1r( v ) I v ï¿½ L

ï¿½L cp}
FL â€¢'P}
cp}
-.cp}

It is straightforward from these definitions that the
following duality properties hold:
â€¢
â€¢

/4 ( 7r)('P) = 1- ft(7r)(-,cp)
f2(7r) {'P) = 1- fa(7r)(â€¢cp)

Proposition 11: if L is such that v !i=L cp =? v FL
-.cp (or equivalent1y, v ï¿½L ''P =? v f= L 'P )3 then
/2(1r)(cp) ï¿½ fl(7r)(cp), and /4(1r)(cp) ï¿½ /3(1r)(cp).
Proposition 12: ft is a L-necessity function, proÂ­
vided that the following conditions hold:
â€¢

if

â€¢

V

â€¢

V

then v FL cp for all v (Soundness)
FL 'P +-+ t/J iff ( V F L 'P) {::> ( V F tj;)
F L 'P 1\ 1/J iff V FL I{) and V F L 1/J
f-L cp

3Either (v ï¿½L cp =? v I=L -.cp) or (v ï¿½L â€¢cp =? v FL cp)
basically amounts to the validity of cp V â€¢cp in the logic L.

72

Besnard and Lang

Proposition 13: h is a 1-necessity function, proÂ­
vided that the following conditions hold:

rp then v ï¿½L -.rp for all v
v FL rp <c-t 1/; iff (v FL -.rp) <:? (v FL -.'lj;)
â€¢ v FL -.( rp 1\ '1/J) i ff v FL -.rp or v FL -.tf
Proposition 14: h is a 1-possibility function,
â€¢

if I-L

â€¢

proÂ­

vided that the following conditions hold:
â€¢
â€¢
â€¢

v ï¿½L -.rp for all v
v FL rp <c-t 1/; iff (v FL 'P) <:? (v FL '1/J)
v F=L rp V '1/J iff v F= L cp or v F= L tP
if I- L cp then

Proposition 15: J4 is a 1-possibility function, proÂ­
vided that the following conditions hold:
â€¢
â€¢
â€¢

f= L 'P for all v (Soundness)
v FL 'P <c-t 1/; iff (v FL -.'P) <:? (v FL -,'lj;)
v FL --.(cp V tf) iff v FL ''P and v FL â€¢1/;
if I-L

2.4

rp

then

v

L-necessity orderings

It has been shown [Dub 86] that necessity and possiÂ­
bility functions can be equivalently expressed in purely
qualitative terms, with preordering relations.
We
briefly give a generalization of this result, for the case
of necessities (the case for possibilities is similar).
Definition: A 1-necessity ordering is a relation on .C
satisfying the following properties:

rp 2:: 1/;

1/; 2:: ï¿½ then rp 2:: ï¿½ (transitivity)
1/; 2:: rp (dominance)
or rp 1\ tjJ ï¿½ '1/J (conjunctiveness)

â€¢

if

â€¢

if I- L 'P -+ '1/J then

and

â€¢

rp 1\ tP

ï¿½ rp

[Dub 86]: a relation 2:: on C is said to
agree strictly with a mapping f from C to [0, 1] iff
Vrp, 1/; E .C, we have 'P 2:: tP <:? !(rp) 2:: f('1/J ).
Definition

Proposition 16 (correspondence between 1-necessity
functions and 1-necessity orderings): the only mapÂ­
pings from .C to [0, 1] agreeing strictly with 1-necessity
orderings and also satisfying (Taut) are 1-necessity
functions.

3

Application to reasoning with
uncertain and inconsistent
information

3.1

Motivations

Possibility theory, as well as its qualitative counterÂ­
parts such as epistemic entrenchment relations [GM
88], ranked knowledge bases [Pea 90] or rational cloÂ­
sure [1eh 89] provide a relativized treatment of inconÂ­
sistency, since the latter becomes a gradual notion.
I.e., a possibilistic knowledge base [D1P 94] consists
of a set of constraints KB = {(cp; ai),i = l..n}, where
(cp; a;) is a syntactic notation for the semantical conÂ­
straint N(rp;) 2:: a;.

A possibilistic knowledge base is partially inconsisÂ­
tent if it leads to enforce N(.l) > 0; stated othÂ­
erwise, the inconsistency degree of K B is defined
by Incons(KB) = maxsÂ£;KB,Sï¿½l_ min(<p;a,)es a; =
min{N(.l), N satisfies KB}. Any formula below this
level, i.e. any rp; where a;:::; Incons(KB), is then inÂ­
hibited (it is "drown" by the inconsistency [BCD1P
93]). This shows that the notion of inconsistency
in possibilistic logic and its qualitative counterpart is
gradual but global. The inconsistency level measures
to what extent the knowledge base is inconsistent, but
do not locate the inconsistency. The aforementioned
"drowning effect" is a consequence of this global treatÂ­
ment of inconsistency. One way to cope with it is
to consider the knowledge base syntactically [Bre 89]
[Neb 91] [BCD1P 93], by selecting among maximal
sub-bases of KB using a criterion involving the a;'s.
However, these syntactical approaches do not have
(yet) any semantics in terms of uncertainty measures.
Now, using paraconsistent logics for handling inconÂ­
sistent knowledge bases enables a local treatment of
inconsistency, by locating the inconsistency on some
formulas. Yet, these paraconsistent approaches do not
allow for any graduality in the inconsistency, which imÂ­
plies some loss of information if the initial knowledge
was pertained with uncertainty.
While possibilistic logic allows for a gradual but global
treatment of inconsistency, where conflicts are solved
only by comparing the uncertainty level of the pieces of
information with the inconsistency level of the knowlÂ­
edge base, the pure paraconsistent approach localizes
inconsistency, but conflicts cannot be ranked accordÂ­
ing to uncertainty, importance, priority, normality as
done in rank-based systems. Thus paraconsistency is
not able to "solve" the conflicts. What we propose
here is to apply the results of Section 2 to a given
paraconsistent logic, namely C1 [daC 74], to handle
both uncertain and inconsistent knowledge, and with
a local treatment of inconsistency. We now give two
motivating examples, one about fusion of uncertain
information (multi-source reasoning) and one about
reasoning with default rules.
Example 1 (multiï¿½source reasoning)
This example is borrowed from [Cho 94].
Two witnesses report their observations about a murÂ­
derer. Witness 1 (noted W1) is certain that the murÂ­
derer was a woman with blond hair, and believes (with
some uncertainty) that she was wearing a Chanel suit,
glasses, and was driving a BMW. Witness 2 (noted
W2) is certain that the murderer was a woman with
brown hair and that she was not wearing glasses, and
believes (with some uncertainty) that she was driving
a Fiat.

W1 female (sure), blond-hair (sure), drives-BMW
(unsure), wear-glasses (unsure), wear-ChanelÂ­
sui t (unsure)
W2 female (sure), brown-hair (sure), drives-Fiat
(unsure), â€¢wear-glasses (sure)

Possibility and Necessity Functions over Non-Classical Logics

What would we like to conclude about the following
statements?
â€¢

Both witnesses agree that the murderer was feÂ­
male and are completely sure; so we want to conÂ­
clude the murderer was female.

â€¢

No contradiction either about wear-Chanel-suit
since witness 2 does not know anything.

â€¢

Strong contradiction about the colour of the murÂ­
derer's hair; we wish to conclude neither blond
nor brown but we want to keep in mind that
these literals are "strongly subject to inconsisÂ­
tency" (knowing the constraint â€¢(blond-hair 1\
brown-hair)).

â€¢

â€¢

Contradiction about wear-glasses: the contradicÂ­
tion is weaker than the one above since witness 1
is unsure; moreover, since witness 2's information
is prioritary to witness 1 's we would like to solve
the conflict (by concluding â€¢wear-glasses).
Weak contradiction again, about the car; however,
since both witnesses are equally certain, we do not
want to conclude anything.

Example 2

(drowning effect)
Here, applying Pearl's ranking procedure of default
rules to
.6.

=

{penguin ......,. bird, penguin
bird fly, b ird ___. wings}

---->

â€¢fly,

___.

3.2

A case study: C1-necessity functions

3.2.1

The paraconsistent logic C1

C1 [daC 7 4] is a paraconsistent logic, that is, a logic
in which a contradiction <p 1\ -,IP fails to entail other
arbitrary contradictions 1/J 1\ -,'ljJ cl retains all inferÂ­
ence patterns of classical logic that are not based on
negation. For instance,
,

IP

1/J

<pi\Â¢
is valid in C1. By contrast, some inference patterns
of classical logic that do appeal to negation are not
preserved. For instance,
â€¢1/J
'IP
-.(<p V 'I/J)
is not valid in C1. The idea is that positive informaÂ­
tion is fundamental: positive formulas and inferences
contribute to state what the facts are whereas negative
formulas and inferences are merely constraints (in the
sense of integrity constraints for databases). AccordÂ­
ingly, cl allows us to elicit all and only the formulas
responsible for a given contradiction ([CL 92] (BL 941).
A valuation-based semantics for C1 (Alv 84] is given
in Section 3.2.3 as we now reproduce the original axÂ­
iomatic presentation of C1 that consists of the next
ten axioms
1. <p---+(1/J----><p)
2. (<p----> 1/J)----> [(10---+ (1/J--->

g1ves
.6. 1

=

.6.0

=

{penguin ......,. bird, penguin ---> â€¢fly} ;
{bird ---. fly, bird ---+ wings}.

Adding the fact penguin to .6. enables us to infer â€¢fly
but wings is not deduced (it is "drown" by the inconÂ­
sistency appearing at rank 1). This particular case of
the drowning effect is known as the property of "inÂ­
heritance blocking".
Considering .6. as a set of formulas for the logic C1, we
obtain
AU

{penguin} f-c, {fly, â€¢fly, wings}.

Thus, we avoid the drowning effect but we do not take
into account priorities (induced by specificity) such as
penguin ......,. â€¢fly over bird ......,. fly and we conclude
that fly is not well-behaved.
What we would like is to take advantage of the loÂ­
calisation of inconsistency, as done by paraconsistent
entailment, and the priority between formulas, which
would lead us to infer {fly, wings} but not â€¢fly.
Note that prioritized syntax-based approaches based
on the selection of maximal consistent subsets of the
knowledge base guided by the priorities solve the
drowning effect but do not tell anything about where
the contradictions are localized; so, for instance, the
conclusion â€¢wear-glasses is not relativised by the
fact that it is (weakly) subject to inconsistency.

73

u

))

---->

(10---+ u)]

3.
4.

\0

5.

<p-->(1/J----><p/\tf;)
<p ---> <p V 7./J
<p---->1/JV<p
(IP----> CT)--> [(1/J----> 0')......,. (<p V tf;---+ 0')]

6.

7.

8.

1\ 1/J ......,.

\0 1\

'P

1/J -1/J

9. 'P v --,'P
10. --,--,1{)-+<p

together with the single inference rule

'f r..!f

C1 has the following basic features.

First, the conÂ­
nectives are not interdefinable. For instance, <p V 7./J
cannot be defined as â€¢ ( â€¢<p 1\ â€¢7./J). Second, the reÂ­
placement of equivalent formulas does not hold. For
instance, (<p v 7./J) +--+ [(<p ---+ 1jJ) ......,. 7./J] is valid in C1 but
â€¢(<p V 1/J) ......, -.[('P--> 1/J) ---+ 1/J] is not. Third , neither
modus tollens
â€¢<p
nor disjunctive syllogism
\0

are valid in C1.

--,\0

v 1/J
1/J

Regarding notation, we use <p0 as an abbreviation for
--, (<p 1\ â€¢<p) . In the next two sections, we also use # to
denote any of 1\, V, ---+.

74

Besnard and Lang

3.2.2

C1 Mnecessity functions: definition and

basic properties
Definition: like for L-necessity functions, replacing
1--L by 1--c,.
Some properties enjoyed by C1-necessity functions are:
Dam: If

1--c, t.p -+

(P1) N(t.p)

=

.,P then N(t.p) ï¿½ N(.,P)

N(â€¢t.p0) or N(â€¢t.p)

=

3.3

N(â€¢ t.p0 )

(P2) N(.,P);::: min(N(t.p), N(t.p-+ .,P))

3.3.1

::::} N(1/J) ;::: min;=l..n N('Pi)
(P4) N(-.t.p);::: min(N(t.p0), N(t.p--+ '1/J), N(t.p- + â€¢if))
(P5) N(t.p v '1/J);::: max(N(t.p), N(.,P))
(P6) N(-,-,t.p) = N('P)

(P3) 'Pl,... , 'Pn 1--c, 1/J

(P7) N{t.p00) = 1
(P8) N(('P# 1/J)0) ï¿½ min(N(t.p0), N(.,P0))
(P9) N is a classical necessity function if and only if
N(t.p0) = 1 for all t.p
min(N('P) ,N(-.t.p)) is the necessity of t.p
N(-,t.p0)
"behaving badly"; it can be seen as a measure of the
inconsistency inherent in t.p. C1-necessity functions enÂ­
able us to rank the formulas not only with respect to
their certainty, but also with respect to their inherent
inconsistency: N( ''Po) gives a notion of inconsistency
which is both local and gradual. We recover of course
as particular cases:
=

â€¢

â€¢

Classical necessity functions, so that N (''Po) =
N(.l) for all t.p. The notion of inconsistency is
still gradual but global.
Classical C1-valuations, which verify N( -.t.p0) = 0
or N(...,t.p") = 1, for all t.p. The notion of inconsisÂ­
tency is still local but not gradual.

3.2.3

C1-necessity functions: semantics

At first, a (paraconsistent) Ct-valuation [Alv 84] is a
mapping from .C to {0, 1} such that:
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢

0::::} v(..,IP) 1
v(-.-,'P) = 1 {:} v(t.p) = 1
v('l/!0) = v(t.p---+ '1/J) = v(tp- + â€¢1/J) = I::::} v(IP)
v(lf' -+ '1/J) = 1 {:} v(lf') = 0 or v(.,P) = 1
v(t.p 1\ '1/!) = 1 {::} v(tp) = 1 and v('I/J) ::::= 1
v(tpV.,P) = 1 {:} v(lf') = 1 or v('!j!) = 1
v(tp0) = v('l/!0) = 1::::} v((!f'#'l/!)0) = 1
v(t.p)

=

=

=

0

from the set of all C1-valuations to [0, 1].

Due to Proposition 12, the function fi (1r) defined

fl(7r)(1;?)

=

1- sup{11'(v)\v(1;?)

=

Reasoning with C1 Mnecessity functions
Generalizing the principle of minimum
specificity

The principle of minimum specificity [DP 86] or equivÂ­
alently,minimum compact ranking [Pea 90] and ratioÂ­
nal closure [Leh 8 9] (all these being equivalent, up to
the language on which they are defined) induces, from
a possibilistic knowledge base, a particular necessity
function i.e. the smallest among all necessity functions
satisfying the knowledge base. Thanks to the propÂ­
erty (P3), we are able to generalize the principle of
minimum specificity to C1-necessity functions:

Definition: a C1-possibilistic knowledge base is a fiÂ­
nite set KB = {(tp; a; ) , 1 ï¿½ i ï¿½ n} where if'; E .C and
a; E [0, l]. A C1-necessity function N is said to satisfy
KB iff Vi
l..n, N(lf'i) ;::: a;.
=

Definition: the minimum specificity closure NKB of a

C1-possibilistic knowledge base KB is the C1-necessity
function defined by

Vtj;
where KB13

E

=

.C, Nxs (,P)
{lf';\(!f'; ai)

=

E

sup{,BIKB11I--c1 '1/J}

KB and a;;:=: ,8}.

Proposition 17 (principle of minimum specificity for
C1-necessities): For any Ct-necessity function N, N
satisfies KB iff N;::: NxsMore generally, the minimum specificity closure could
be extended to any logic L satisfying the property
(P3). Applying the principle of minimum specificity
enables us to draw conclusions that taking into account
the uncertainty and the inconsistency of the knowledge
base. We propose the following definition of a conseÂ­
quence relation:

Definition: KB f--- '1/J iff Nxs ( .,P)

>

Nxs(-.'1/!0).

Proposition 18: KB f--- 'rP iff Nxs(t/;)

Definition: a C1-possibility distribution is a mapping
1r

We could have also defined C1-possibility functions,
C1-necessity and possibility orderings, that we do not
discuss for the sake of brevity. C1-necessity functions
are sufficient to deal with the next section, devoted to
the application to reasoning with uncertain and inconÂ­
sistent knowledge.

as

0}

is a Ct-necessity function (since Ct obeys the condiÂ­
tion stated in Proposition 12 - the soundness of the
semantics coming from the soundness and completeÂ­
ness of C1 established in [Alv 84]).

>

Nxs(-,'!j!).

Intuitively, we deduce '1/J from KB iff the certainty of
'1/J is higher than the inconsistency inherent to '1/J, or
equivalently, iff the certainty of 'lj; is higher than the
certainty of â€¢Â¢. The binary version of f--- would be
defined by if' f'--KB '1/J iff Nxs(t.p -+ Â¢) > Nxs(l;? -+
â€¢if0), or equivalently iff Nxs(t.p--+ Â¢) > Nxs(IP -+
-,'CjJ). Note that f-- is nonmonotonic; a more complete
study of the properties of f--- a Ia Kraus, Lehmann
and Magidor [KLM 90], is possible with respect to the
(monotonic) logic cl instead of classical logic.
Note that when N collapses to a classical necessity
measure, we haveVt/;Nx8(-.,P0) = N(..L) and f--- is the
classi cal possibilistic consequence relation [DP 91 b].

Possibility and Necessity Functions over Non-Classical Logics

(multi-source reasoning): let us return to
the example of Section 3.1. Taking some a E (0, 1 ) :

Example

â€¢

â€¢

W1 (witness 1):
N(female) = 1; N(brown) = 1; N(BMW) = a;
N(Chanel) = a ; N(glasses) = u:.
W2 (witness 2):
N(female) = 1; N(-.brown) = 1; N(-.BMW) =a;
N(-. glasses)= 1.

The fusion K B of these two knowledge bases gives the
following minimum specificity closure:
â€¢

NKB(female) = 1; NKB(-.female) = 0;
NxB(-.female0) = 0;

â€¢

NxB(brown) = 1; NKs(-.brown)= 1;
NxB(-.brownÂ°) = 1;
NxB(BMW) u:; NxB(-.BMW) =a;
NxB(Chanel) =a; NKB(â€¢Chanel) = 0;
NxB(â€¢Chanel0) = 0;
NxB(glasses) = a; NKB(-.glasses)
1;
NxB(-.glasses0) = a.

â€¢
â€¢

â€¢

=

=

Therefore, we have K B f--- female, K B f--- Chanel,
KB f--- -.glasses; however, KB lt- BMW, KB lt- â€¢BMW,
K B lt- brown, K B lt- -,brown.

75

Therefore, we have
K B f--- â€¢fly (which is intended);
K B f--- -,wings (which is intended);
f--- avoids the drowning effect, contrarily to
the classical minimum specificity closure,
System Z, and similar systems.
but also
K B f--- â€¢live-in-Antarctica

which is not intended! (Due to NKB(fly) = /3, the
rule fly--+ â€¢live-in-Antarctica applies).
Here is a revised definition, more suited to handling
default rules:
Definition: Let K B = FULl, where F is a set of
facts and Ll = {'Pi --+ 1/;i, i = l..n} a set of default
rules, where each rule is assigned a necessity degree
corresponding to its Z-ranking. We define

G0(Ll) =FuLl
and Vk 2': 0,

ck+l(Ll)
F u {.:p;--+ 1/;; E Gk(Ll) I NGk(f!.)('Pi) > NGk(f!.)(â€¢.:pi)}
F u {'Pi--+ 1/;; E Gk{Ll) I Gk(Ll ) r- <:;';}.
k(Ll). Then Ll f---â€¢ 1/; iff
Lastly, let G00(Ll) = n >oG
kG00(Ll ) r- 1/;.
=

=

3.3.2

Handling default rules

Example:

Consider the fact penguin and the rules

ï¿½ = {penguin---+ bird, penguin---+ â€¢fly,
bird ---+ fly, bird ---+ wings,
fly--+ -.live-in-Antarctica}.
Applying the Z ranking procedure to Ll (written with
the possibilistic ranking convention) gives the ranking:
(for any a:, j3 such that 0 < j3 < a < 1)
{penguin--+ bird, penguin---+ --,fly,
fly---+ -.live-in-Antarctica};
ï¿½f3 = {bird ---+ fly, bird---+ wings}.

Lla

=

Then, taking the C1-minimum specificity closure of
K B = {penguin} U ï¿½ leads to
â€¢
â€¢

â€¢

NxB(penguin) = 1;
NxB(bird) u:;
NxB(â€¢fly) =a;
NKB(fly--+ â€¢live-in-Antarctica) =a;
NxB(fly) = /3;
NxB(â€¢fly0) = /3;
NxB(wings) j3;
NxB(-.live-in-Antarctica) = /3;
NKB(-.bird)= 0;
NxB(-.bird0) = 0;
NxB(-.penguin) = 0;
NxB(--,penguinÂ°) = 0;
NxB(-.wings) 0;
NxB(â€¢wings0) = 0;
NxB(live-in-Antarctica)= 0;
NKB(--,live-in-Antarctica0) = 0.
=

=

â€¢

=

Example:

We apply the usual ranking procedure:

ï¿½ = {bird --+ fly, bird ---+ wings,
penguin ---+ bird, penguin ---+ -.fly,
fly---+ â€¢live-in-Antarctica}.

G 1(Ll) = {penguin, bird---+ fly, bird--+ wings,
penguin ---+ bird, penguin --+ --,fly}.
Clearly, G00(Ll) = G1(6.). Therefore, Ll f---â€¢ â€¢fly.
Also, Ll r-â€¢ bird and 6. f---1 wings. Contrastedly,
Ll ï¿½Â· â€¢live-in-Antarctica.
4

Conclusion

We have given some basic results describing what reÂ­
mains and what changes when switching from classiÂ­
cal possibility theory to possibility theory over a nonÂ­
classical logic. We have then focused on a case study,
namely the paraconsistent logic C1, and showed how to
use it to reason with inconsistent and uncertain inforÂ­
mation. W hat has been left aside in this paper is the
other possible applications of possibility theory over
non-classical logics: first, one could think of applying
the general results of Section 2 to other non-classical
logics: for instance, introducing possibility and neÂ­
cessity valuations into intuitionistic logic could model
gradual strengths of proofs; or, introducing them to
Kleene's logic (or more generally to a multi-valued
logic) would enable us to handle both uncertainty and
partial truth.

76

Besnard and Lang

Another topic for further research would be a parallel
study for other numerical theories of uncertainty. For
instance, paraconsistent probabilities would lead to a
more quantitative framework for reasoning with unÂ­
certain and conflictual information; in this framework,
noticing that Prob( <p) + Prob( -.ï¿½.p) = Pro b( <p v -,<p) +
Prob( <pi\ -.ï¿½.p) = 1 + Prob( -.ï¿½.p0),relaxing the constraint
Prob( -.ï¿½.p0) = 0 would make P r ob( <p) + Prob( -.ï¿½.p) > 1
possible for some formulas; then one could think of
searching for the "least inconsistent" probability disÂ­
tribution satisfying a set of constraints, which could be
useful for instance when rectifying a set of inconsistent
probabilistic data.
5


