
The paper introduces a generalization for
known probabilistic models such as log-linear
and graphical models, called here multiplicative models. These models, that express
probabilities via product of parameters are
shown to capture multiple forms of contextual independence between variables, including decision graphs and noisy-OR functions. An inference algorithm for multiplicative models is provided and its correctness is
proved. The complexity analysis of the inference algorithm uses a more refined parameter
than the tree-width of the underlying graph,
and shows the computational cost does not
exceed that of the variable elimination algorithm in graphical models. The paper ends
with examples where using the new models
and algorithm is computationally beneficial.

1

Introduction

Probabilistic models that represent associations
and/or interactions among random variables have been
heavily applied in the past century in various fields of
science and engineering. The statistical methods originating with the work of Fisher (1925, 1956) [6, 7] culminated in the log-linear models which describe the association patterns among a set of categorical variables
without specifying any variable as a response (dependent) variable [1].
A specific type of probabilistic models, probabilistic
graphical models, can be visually described as an interaction graph, and embody independence assumptions
in the domain of interest [15]. Their main attraction
is that the independences encoded in the structure of
the model allow to indirectly specify the join distribution as a product of functions ψi (Di ), each depends
only on a limited set of variables Di . Algorithms that
compute the posterior distribution conditioned on ev-

idence, called inference algorithms, exploit this structure, avoiding a direct computation of the join probabilities [5, 19]. The complexity of such algorithms
depends on the topology of the model, and is exponential in the tree-width of the underlying graph.
The common distinction within graphical models is
between undirected graphical models [15], a subset of
log-linear models, where there are no restrictions on
the functions ψ, and Bayesian networks (BNs) [19]
in which every function is a conditional distribution
ψi (Di ) = P (Xi |Πi ) where Πi is the set of parent variables of Xi in the model. Another type of probabilistic
models that can be represented visually, called factor
graphs, extends undirected graphical models and incorporates many of the desired properties of graphical
modes [14].
Aside of the independences that are imposed by the
model’s structure, often there exist additional independences stemming from the specific values of the
functions. These independences are not systematically exploited by the traditional inference algorithms,
resulting in an unnecessary computational cost. For
such non-structural independences we use the name
context-specific independence (CSI), which was suggested in previous studies [2, 20]. We note that the
term CSI takes here a more general meaning as it is
not restricted to any specific type of independence.
Several studies have suggested changes in the traditional representation of graphical models in order
to capture context-specific independences. These include similarity networks suggested by Heckerman
(1991) [12], multinets (Geiger & Heckerman 1996) [9],
asymmetric influence diagrams (Fung and Shachter
1990) [8], and structured representations of the functions ψ based on decision trees (Boutilier et al.
1996 [2], Poole& Zhang 2003 [20]). Other studies resorted to revised representations for specific functions
(e.g. Quickscore algorithm by Heckerman 1989 for
noisy-OR functions [11]).

Although the new representations proved useful from
an empirical view point, they lack the ability to encompass a wide variety of CSI. In addition, the theoretical
complexity of inference using these representation remained a function only of the topology of the graph
underlying the model.
In this paper we approach the problem of inference
from a more general perspective. We introduce a set of
models called multiplicative models in which the functions ψ that account for the dependency of variables
are in a multiplicative representation, where a value of
an instance is a product over a set of parameters. We
show that multiplicative models generalize over loglinear models, factor graphs, and graphical models. In
addition, we show that multiplicative models can capture multiple forms of CSI, including CSIs captured
via decision trees, decision graphs, and via noisy-OR
functions. This leads to the question whether an inference algorithm that takes advantage of these independences can be constructed without additional cost.
We provide such an algorithm, and show how different
types of independences are utilized in this procedure to
reduce the needed computations. The inference algorithm provided herein simplifies over the inference algorithm suggested by Poole & Zhang (2003) [20] when
applied to Bayesian networks, by avoiding the use of
tables and tables splitting operations. The more general nature of the algorithm also enables it to deal
with different representations, and thus account for
CSI that can not be represented by decision trees and
decision graphs.
We prove the correctness of the inference procedure
and give a new notion of complexity instead of the
exponent of the tree-width which is commonly used
to describe the complexity of inference in graphical
models. The new time complexity is shown to be less
than or equal to the standard complexity.

2

Multiplicative models

We propose a generalization of graphical models, factor graphs and log-linear models which represents the
dependency of variables in the model via the notion
of multiplicative models. In these models a value of
an instance in the dependency function is a product
over a specific set of parameters. The definition relies
on the concept of a lattice. A lattice (L, E, ∩, ∪) is
a partially ordered set (poset) with respect to some
relation E, in which for every two elements l1 , l2 ∈ L
their least upper bound is denoted as l1 ∩ l2 and their
greatest lower bound is denoted as l1 ∪ l2 .
We usually use upper case letters to denote random
variables and sets of random variables, and lower case
letters to denote their values. For a variable V we

denote its domain, or the set of possible values it can
get, by dom(V ). For a set of variables D = {Vi }ni=1 ,
the notation dom(D) corresponds to the cross product
of the domains dom(Vi ), i = 1, . . . , n.
Let D = {Vi }ni=1 be a set of n multivalued variables,
and let the function ψ(D) : dom(D) → R specify the
values in a full table for the set D, then the following
is a definition for a mapping function of D.
Definition 1 (Mapping function) A function f is
called a mapping function of D with respect to the lattice L, if it is defined as f : dom(Z) → L for every
Z ⊆ D, and maps partial instances Z = z onto L.
We use this definition to define a lattice multiplicative
model of ψ(D).
Definition 2 (Lattice multiplicative model)
A model ρ = {Sρ , Γρ } of a function ψ(D) is
called a lattice multiplicative model with respect
to a lattice (L, E, ∩, ∪) and a mapping function
f , if Sρ ⊆ L, QΓρ = {γs ∈ R : s ∈ Sρ } and
ψ(D = d) =
γs .
sEf (d),s∈Sρ

The set Sρ is called the structure of the model, and
the set Γρ is called the parameters of the model. In
multiplicative models elements s ∈ S for which γs = 1
can be removed from S.
Here we focus on a lattice L which is a set of propositional clauses over the variables and their values, and
call this model a propositional multiplicative model,
or simply a multiplicative model. In this model, the
operators on the lattice are ∧ and ∨. The mapping
function used for this model is called the propositional
mapping function, and is defined as follows.
Definition 3 (Propositional mapping function)
A mapping function f is called a propositional mapping function of D with respect to the lattice L, if
for every set Z ⊆ D the function mapsVevery partial
instance Z = z into the conjunction
(Vi = vi ),
Vi ∈Z

where vi is the projection of z onto the variable Vi .
Definition 4 (Propositional multiplicative model)
A lattice multiplicative model ρ = {Sρ , Γρ } of a function ψ(D) is called a propositional multiplicative
model with respect to a lattice (L, , ∧, ∨) and a
propositional mapping function f , if the elements of
L are propositional clauses over the variables in D
and for two clauses c and c0 we denote c  c0 if c is
implied by c0 .
Example 1 Consider a set D which contains two
ternary variables A and B. The corresponding lattice

contains propositional clauses over A and B, and for
the two clauses c = (A = 0) and c0 = (A = 0)∧(B = 2)
we denote c  c0 . The corresponding mapping function
maps the instance A = 0, B = 2 into the propositional
clause (A = 0) ∧ (B = 2), and the partial instance
A = 0 into the clause (A = 0).
In this definition, the standard model which uses fulltable representations of the functions ψ(D), such as
graphical models, and handles each instance separately, is also a multiplicative model with the set S
containing all mapping f (d) of instances D = d, and
with values γd = ψ(d).
Another well-known model that falls into Definition 2
is the log-linear model.
2.1

Log-linear models

Log-linear models are usually used to analyze categorical data, and are a direct generalization of undirected
graphical models. These models that have been heavily used for statistical analysis for the past four decades
describe the association patterns among a set of categorical variables without specifying any variable as
a response (dependent) variable, treating all variables
symmetrically [1].
Formally, a log-linear model specifies the natural log
of the expected frequency of values d for a set of variables D as a linear combination of the main effect λVvii
of every variable Vi ∈ D, and if |D| > 1 interaction effects λSs of every subset of variables S ⊆ D, where the
instances s are consistent with d. For example, suppose that we want to investigate relationships between
three categorical variables, A, B and C, then the full
log-linear model is
B
C
AB
AC
BC
ABC
ln(Fa,b,c ) = µ+λA
a +λb +λc +λab +λac +λbc +λabc

where µ is the overall mean of the natural log of the
expected frequencies.
Clearly in the log-linear models instances are partially
ordered by inclusion of their sets and by consistency
of instantiations. To formalize log-linear models as a
multiplicative models, for every subset Z ⊆ D and for
Z
every instantiation Z = z such that
V λz 6= 0, the set S
contains all clauses of the form
(V = v), where v

els that can potentially reduce the amount of work
needed for inference [12, 9]. The notion of ContextSpecific Independence (CSI) was then introduced by
Smith et al. (1993) [23] and Boutilier et al. (1996) [2].
Context-specific independence corresponds to regularities within probabilistic models based on the values
assigned in the model.
Formally, we say that the sets of variables X and Y
are contextually independent in the context of C = c
given Z if
(1)
P (X, Y |Z = z, C = c) =
P (X|Z = z, C = c) · P (Y |Z = z, C = c)
for every value Z = z. One aspect of this equation is
that if X and Y are contextually independent given
Z, then
P (X|Y = y1 , Z = z, C = c) = P (X|Y = y2 , Z = z, C = c)
(2)
for any two values y1 , y2 of Y , which appear as repetitive values in conditional probability tables, such as
those used in BNs. These repetition which are the basis of compact representations like decision trees and
graphs were exploited for inference in BN [2, 20].
Another kind of CSI which was exploited for enhanced
inference in BNs is the independence in noisy-OR functions. A noisy-OR function is a conditional probability function of a binary effect variable E given a set
of m binary cause variables C = {C1 , . . . , Cm }. The
conditional probabilities
Q of the function are P (E =
0|C1 , . . . , Cm ) = c0
P (E = 0|Ci ), where c0 is a
i:Ci =1

constant, and the values P (E = 0|Ci ) are some real
numbers.
For any particular CSI of the sets of variables X and Y
in the context C = c given the set Z, as in Eq. 1, there
exists a multiplicative model that captures this independence. Such a model is any multiplicative model
where the structure does not contain elements s that
involve variables from X and Y , such that there exists
an instance Z = z for which s ∧ (Z = z) = ⊥ and
s ∧ (C = c) 6= ⊥.
We now define two types of multiplicative models that
capture two different types of common CSIs.

V ∈Z

is the projection of z onto the variable V . In addition,
we set the parameters of the model to γ> = eµ and
S
γf (s) = eλs .
2.2

Context-specific independence

With the introduction of graphical models and in particular Bayesian Networks (BNs), and the proof that
inference in these models is NP-hard [4], several studies looked for further independences encoded in mod-

2.2.1

Positive models

Representing the dependency of variables using loglinear models has some desirable properties, such as
being general while ensuring the existence of a maximum likelihood without enforcing dependencies to be
strictly positive. However, in the representation discussed in Section 2.1 the log-linear models use more
parameters than necessary [3, 13]. Take for example
the log-linear model for two binary variables A and B.

Assuming all possible effects exist, the corresponding
log-linear model uses eight parameters rather than the
four parameters in a standard representation as a full
A
B
B
AB
AB
AB
AB
table: λA
0 , λ1 , λ0 , λ1 , λ00 , λ01 , λ10 , λ11 .
Another representation of the log-linear models that
accounts for these redundancies uses only parameters
which involve non-zero instantiations of variables [10].
In the above example the only parameters used in
B
AB
this representation are: λA
1 , λ1 , λ11 . We describe this
representation of log-linear models as a multiplicative
model, which we call here the positive model.
Definition 5 (Positive model) A positive model ρ
of a function ψ(D) is a multiplicative model wrt to
the lattice (L, , ∧, ∨) and a (propositional) mapping
function f in which Sρ contains only elements s = f (z)
where Z ⊆ D and no variable in Z = z is set to zero.
Log-linear models, and thus positive models, are
known to capture conditional and contextual independences [16].
Example 2 An example is a function ψ over two
binary variables A and B where ψ(0, 0) · ψ(1, 1) =
ψ(0, 1) · ψ(1, 0). This implies that A is independent
of B and the function can be written as ψ(A, B) =
ψ(A) · ψ(B). In the corresponding positive model the
parameter γ(A=1)∧(B=1) = ψ(0,0)·ψ(1,1)
ψ(0,1)·ψ(1,0) = 1. Thus, this
independence is captured in the model.

of ψ(d = v1 v2 · · · vm vm+1 · · · vn ), where Vj = vj for
m < j ≤ n is any possible value of Vj . We note that
in a decision tree every instance D = d is mapped to a
single path in the tree. An example of a decision tree
that encodes a function over the variables A, B, C, D
is shown in Figure 1.
One can choose to use decision graphs [18] instead of
decision trees. These are more compact structures that
can encode for more distributions. For a function ψ(D)
over a set of variables D, a decision graph G that represents ψ(D) is a directed graph with sets of variables
from D at internal nodes and values from ψ(D) at the
leaves. Similar to decision trees, every edge from a set
of variables W to a child Z corresponds to a different
set of values H ⊆ dom(W
), and can be represented
W
as a set of clauses
(W = w). A value at the end
w∈H

of a path p equals to the value of ψ(d), where d is an
instance of D consistence with the sets of values encoded by p. Again, as in decision trees, we note that
in a decision graph every instance D = d is mapped to
a single path in the graph.
Definition 6 (Decision-graph model) A decision
graph model ρ of a function ψ(D) is a multiplicative
model wrt to the lattice (L, , ∧, ∨) and a mapping
function f where everyWtwo elements s1 , s2 ∈ Sρ satisfy s1 ∧ s2 = ⊥, and
s = >, where ⊥ = f alse and
s∈S

> = true.
Example 3 In a more complex function with three binary variables A, B and C, every pair of variables is
independent whenever the third variable is set to zero.
For this function the corresponding positive model assigns γ(V =1)∧(U =1) = 1 for every pair of variables
V, U ∈ {A, B, C} and where V 6= U .
2.2.2

Decision trees and graphs as
multiplicative models

Common structures for representing functions with
contextual independence are decision trees (DTs) and
decision graphs (DGs) [22, 18]. These structures capture contextual independences that are the result of
repetitive values, as specified in Eq. 2. Several studies
have used decision trees to enhance inference in graphical models [2, 20]. We show how DTs and DGs fall
into the category of multiplicative models.
For a function ψ(D) over a set of variables D, a decision tree T that represents ψ(D) is a tree with variables
from D at internal nodes and values from ψ(D) at the
leaves. Every edge from a variable V to a child in T
corresponds to a different set of values H W
⊆ dom(V ),
and can be represented as a set of clauses
(V = v).
v∈H

A value at the end of a path p = v1 → v2 → · · · → vm ,
where vi is some value of Vi , equals to the value

For a specific decision graph G that represents ψ(D),
the decision graph model of G is ρ(G) in which the
structure contains one clause for every path from the
root to a leaf in G, which is a conjunction of the clauses
on the edges. For every such path s, we set γs to
the value at the end of the path. We note that in
this model for every instance D = d there is only one
element s ∈ Sρ such that s  f (d).

3

Inference for multiplicative models

Consider a model that Q
encodes for the probability distribution P (x) =
i ψi (di ), with sets Di =
{Xi1 , . . . Ximi }, and multiplicative models ρi =
{Si , Γi } over all the functions ψi (Di ) wrt a lattice
(L, , ∧, ∨). We first show how to perform inference,
and compute a probability of a set of query variables Q
using a multiplicative model. In particular we perform
inference for a multiplicative model via the variable
elimination scheme (Zhang & Poole 1996 [24], Dechter
1999 [5]) which was originally suggested for inference
in BNs. Then, we prove the correctness of the algorithm and analyze its time complexity.
We define an operation M (V, {ρi }), which given a variable V ∈ X and a set of models {ρi }, i = 1, . . . , m over
X returns a model ρ0 over the variables X \ V . This

A
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1

B
0
0
0
0
1
1
1
1
0
0
0
0
1
1
1
1

C
0
0
1
1
0
0
1
1
0
0
1
1
0
0
1
1

D
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1

ψ(A, B, C, D)
0.4
0.4
0.4
0.4
0.8
0.8
0.8
0.8
0.1
0.1
0.032
0.08
0.1
0.1
0.65
0.08

Figure 1: (left) A full-table over the binary variables A, B, C, D that specifies the value of the function ψ for each instance.
(right) A decision tree corresponding to the function ψ on left. Under every node in the tree appears the corresponding
proposition in the decision-tree representation, and below the corresponding proposition in a positive representation of
the propositions in the decision tree.
Algorithm 1: VE for multiplicative models
Input: A model with n variables Xi (i = 1, . . . , n)
and m functions ψi (Di ⊆ X), that encodes for
the distribution P (X). A set of multiplicative
models ρi = {Si , Γi } wrt a mapping function
f , where ρi model ψi (Di ), and a set of k
query variables Q = {Xi : i ≤ k}
Output: The distribution P (Q).

operator is analogous to marginalization in standard
inference algorithms. In addition, for a model ρ we define a relevance indicator Is (V ) for each element s ∈ Sρ
and each variable V in D, which is set to 1 if there exists a pair of instances d1 , d2 of D that differ only by
the value of V and for which s  f (d1 ) but s  f (d2 ).
Otherwise, Is (V ) is set to 0.
These operations allow us to write an inference procedure which computes the probability of a set of query
variables in a multiplicative model as in Algorithm 1.
The algorithm operates like the bucket-elimination algorithm [5], where given an order on the variables we
iterate over them (Line 2), and marginalize out one
variable at a time (Line 9). Only elements that include
terms that involve the current variable are considered
in the marginalization.

1
2
3
4
5
6
7

Note that for graphical models, in which the elements 8
of Si are a mapping of instances of the functions Di , 9
this algorithm is exactly the known variable elimi- 10
nation algorithm, in its implementation as bucket- 11
elimination [5], where the sets Si [j] are the tables in
the bucket of the variable Xj .
12

t = m + 1;
for j = k + 1 to n do
for i = 1 to t − 1 do
Si [j] ← {s : s ∈ Si , Is (Xj ) = 1};
Γi [j] ← {γs : s ∈ Si [j], γs ∈ Γi };
Si ← Si \ Si [j];
Γi ← Γi \ Γi [j];
end for;
{St , Γt } ← M (V, {Si [j], Γi [j]});
t = t + 1;
end for; (
)
Q
P (Q) ← P (q) =
γsi : Q = q, si ∈ Si ;
si f (q)

A general algorithm for computing M (V, {ρi }) is given 13 return P (Q);
as Algorithm 2. We use there the notation s ∨ V
W for an
element s ∈ S and a variable V to denote s
(V =
Figure 2: Algorithm for variable elimination with a mulV =v
tiplicative model
v). This operation removes all terms that specify a
value for V . For example, if s = (V = 0) ∧ (U = 0)
then s ∨ V = (U = 0).
of R is chosen, and selects those elements r with paThe algorithm has two main parts: upto Line 5 the alrameters γr 6= 1.
gorithm generates the set R of possible new elements
To compute the possible new elements, Lines 2 and 3
in the model. From Line 6 it computes the new paramfirst create a closure under the operator ∧ of each
eters γr , where at each iteration a “minimal” element
structure Si . Then, in Line 5 all conjunctions of terms

Algorithm 2: M (V, {ρi })
Input: A variable V and a set of representations
ρi = {Si , Γi }, i = 1, . . . , t, wrt a lattice
(L, , ∧, ∨), where Isi (V ) = 1 for every i and
si ∈ Si .
Output: A representation ρ0 = {S 0 , Γ0 }.
1
2

0

0

S ← ∅; Γ ← ∅;
for i = 1 to t do
V 0
Ri = {
s : Si0 ⊆ Si };

3

s0 ∈Si0

4

end for; V
R←{
ri : ri ∈ Ri };

5
6

Assume that after removing the set of variable U we
are left with the set X 0 = X \ U , and now wish to
eliminate a variable V ∈ X 0 . We write the probability
of an instance x0v of X 0 \ V which is the projection of
an instance X 0 = x0 onto X 0 \ V via the parameters γ:
P (x0v ) =

X

P (x0 ) =

XY

Y

γi s

V =v i sf (x0 ),s∈Si

V =v

We can decompose the product into terms that involve theQ
variable V Q
and those which do not. Denoting
α(x0v ) =
γi s , we get
i sf (x0 ),s∈Si ,Is (V )=0

1≤i≤t

P (x0v ) = α(x0v ) ·

while R 6= ∅ do

XY

Y

γi s .

(3)

V =v i sf (x0 ),s∈Si ,Is (V )=1
7
8

* r is a minimal element in R *
0
: r0 ∈ R , @r00 ∈ R s.t. r00 ≺ r0 };
r ∈ MP
in(R)
= {rQ
Q
γs

γr =

V =v i s(r∧(V =v)),s∈Si

9
10
11
12
13
14
15

Q
r 0 ∈S 0 ,r 0 r

γr 0

;

R ← R \ {r};
if γr 6= 1 then
S 0 ← S 0 ∪ {r};
Γ0 ← Γ0 ∪ {γr };
end while;
return {S 0 , Γ0 };
Figure 3:

Now, lets examine what the algorithm encodes for
after removing variable V , and show that it equals
Eq. 3. While the elements that do not involve variable V are not changed, the elements that do involve
V are removed and the elements st ∈ St are added.
Therefore, after applying Algorithm 2 for V the remaining sets encode
for P̂ (x0v ) = α(x0v ) · β(x0v ) where
Q
β(x0v ) =
γst . To express β(x0v ) in the
st f (x0 ),st ∈St

terms of Algorithm 2, recall that St ⊆ R and if an
element s ∈ R and s ∈
/ St then γs = 1. Thus, we can
rewrite β(x0v ) using elements of R as

Algorithm for computing the operation

M (V, {ρi }).

β(x0v ) =

Y

γr .

rf (x0 ),r∈R

from the different closures consist of the set of possible new elements. In analogy to inference in graphical
models, this operation is equivalent to the operation
of tables’ multiplication, often denoted as ⊗. In these
models the set R is the set of instances in the table
after marginalization.
We note that for some models, like graphical models,
lines 2-5 are trivial, and are executed implicitly, since
the elements in R are known
S to be all instances of a
full-table over variables in Si .
3.1

Correctness of the inference procedure

We prove the correctness of Algorithm 1 by showing
that the algorithm maintains the property that after
iterating over the set of variables U , the models ρi =
{Si , Γi } encode to the probability distribution P (X \
U ).
At the beginning of the algorithm every model ρi represents the corresponding function ψi (Di ). Thus,
P (X = x) =

Y
i

ψi (Di = di ) =

Y

Y

i sf (di ),s∈Si

γi s .

From lines 2-5 in Algorithm 2, there is one element
r∗  f (x0 ) in R for which ∀r ∈ R such that r  f (x0 )
also satisfies r  r∗ . First, to show there is such an
element r∗ we recall from Line
V 5 that all elements in
R can be written as r =
ri , where ri ∈ Ri , and
1≤i≤t

Ri is the closure of Si under the operator ∧. Consider
the set of elements ri∗  f (x0 ), i = 1, . . . , t, for which
0
all other elements ri ∈ Ri such that rV
i  f (x ) satisfy
∗
ri  ri . Then, every element r =
ri such that
1≤i≤t

r  f (x0 ) also satisfies r  r∗ .
Now, assume by contradiction that there were two such
elements, r1∗ , r2∗ ∈ R. Then from the definition of r1∗
and r2∗ we get r1∗  r2∗ and r2∗  r1∗ , yielding r1∗ = r2∗ .
Thus, from line 9 in Algorithm 2
Y
XY
Y
β(x0v ) = γr∗ ·
γr =
γs
rr ∗ ,r∈R

V =v i s(r ∗ ∧(V =v)),s∈Si

where the last equality is due to the fact that
Q the denominator in the computation of γr∗ is
γr . In
rr ∗ ,r∈R

the terms of Algorithm 1 the set {s : s  (r∗ ∧ (V =

v)), s ∈ Si } can be rewritten as {s : s  f (x0 ), s ∈
Si , Is (V ) = 1}. Thus, we can write
Y
Y
γs
β(x0v ) =
i sf (x0 ),s∈Si ,Is (V )=1

and from Eq. 3 we get P̂ (x0v ) = P (x0v ). Namely, the
new models encode for P (X 0 \ V ).
3.2

Incorporating evidence

In many practical scenarios we observe the value of
some of the variables in the model, and wish to incorporate this evidence. The multiplicative models allow
us to do so in a most natural way. Consider a set
E of evidence nodes for which we observed the values
E = e, and a multiplicative model ρ = {Sρ , Γρ }. Then,
in order to incorporate the evidence
into ρ, we adjust
V
(V = v), where v is the
the elements in Sρ by s = s
V ∈E

projection of e onto the variable V ∈ E. Then, we remove every element not consistent with the evidence,
s = ⊥.
3.3

Complexity of inference

It is well known that the complexity of inference in
graphical models is NP-hard and its cost exponential
in the tree-width of the underlying graph [4].
We analyze the time complexity of the inference procedure for multiplicative models given in Algorithm 1.
As a by-product we refine the standard complexity and
provide a new complexity bound which is based on
the representation used. One can then say that the
complexity of the problem is the minimum complexity
among all possible representations.
3.3.1

Diameter of multiplicative models

The structure of a multiplicative model determines the
amount of computations needed to obtain the value
ψ(d) of a single instantiation of values to variables in
a set D. Although at first glance it seems that for
a model ρ = {S, Γ} of a function ψ(D) the number
of operations needed to obtain
Pvalues of all instances
D = d amounts to a total of
|{s : s  d}|, the real
D=d

number of operations can be dramatically lower and we
denote it by δ(ρ). For hierarchical models, in which
if an element s is not in the structure of the model
then all elements s  s0 are also not in the model,
Good (1963) provides a method that computes all such
values in time |S| log |S| [10]. We denote the ratio
between the number of computations and the number
of elements in S, which is the size of the model, by
diam(ρ) = δ(ρ)
|S| and name it the diameter of ρ.
From a computational perspective, it is clearly beneficial to use models with a small diameter, as this

directly leads to fewer operations whenever we want
to either obtain a value of ψ or update the values γs .
Examples of models with a diameter of 1 are graphical
models and decision graph models, in which for every
element s ∈ S, the only element s0 such that s0  s,
is s itself. On the other hand, the diameter of a positive model can be as high as log2|S| . This maximum
is achieved for a positive model of m binary variables,
when all 2m parameters do not equal one, and hence
all possible elements are in S. In this scenario the
diameter is exactly m
2.
Although in the worst scenario the diameter of a positive model can be large, often this is not the case, and
the diameter is typically bounded to be very small.
Example 4 Consider as an example the Potts
model [21] in which a function ψ(D) over a set
DQ
= {Vi }ni=1 decomposes according to ψ(D = d) =
c0 ψ(vi , vj ), where vi and vj are projections of d
i,j

onto the variables Vi and Vj respectively, and c0 is a
constant. Although in general a positive model over n
binary variables has a diameter of n2 , in this example,
the structure of the positive model includes only elements that involve at most two variables. Therefore,
the diameter of the model is bounded by two.
Similarly, in a more complex scenario where the function ψ decomposes to functions of k-tuples of variables,
the diameter will be bounded by k.
Consider a tree decomposition of the graph in which
there is an edge between a pair of variables V, U if
there exists an element s in one of the models for which
Is (V ) · Is (U ) = 1. We denote by S(W ) = {s ∨ (X \ Z) :
s ∈ Si } the set of parts of elements in the models ρi
that involve variables from the set of graph vertices
Z which is mapped onto the tree node W . Further
denoting as S − (W ) the closure of S(W ) under the
operator ∧, we say that complexity of the algorithm for
this tree decomposition is the maximum over the nodes
W in the tree of |S − (W )| · diam(S − (W )), as described
in Section 3.3.1. Then, the overall complexity of the
algorithm is the complexity for the tree decomposition
that yields the minimum for this term.
To see that this is indeed the time complexity of the
algorithm, consider the elements in a set R in Algorithm 2. The number of elements there does not exceed the number of elements in S − (W ) for the corresponding tree decomposition and where W maps onto
the variables that appear in R. Most of the computation stems from computing the products in Line 9,
and these can be done for the entire set of elements
of R in time proportional to |R| · diam(R). Therefore,
having the ability to choose an elimination order, the
complexity of the algorithm is |S − (W )|·diam(S − (W ))

maximized over all nodes W in a tree decomposition
and minimized over all possible such decompositions.
3.4

Benefits of inference for multiplicative
models

Different multiplicative models capture different contextual independences, hence specifying different number of parameters. Take for example the function over
four binary variables A, B, C, D with values according
to the table in Figure 1. The structure of the corresponding decision-tree model contains six elements
while the structure of the corresponding positive model
contains eight elements. In this latter model, the CSI
captured in the decision tree, yielding the value of ψ
to be independent of B given that A, C and D are set
to one, does not have any effect. This variation and
the structure of the model affect the run time of the
inference algorithm.

[2] C. Boutilier and et al. Context-specific independence
in Bayesian networks. In Uncertainty in Artificial Intelligence, pages 115–123, 1996.
[3] R. Christensen. Log-Linear Models and Logistic Regression. Springer, 1997.
[4] G. Cooper. The computational complexity of probabilistic inference using bayesian belief networks. Artificial Intelligence, 42:393–405, 1990.
[5] R. Dechter. Bucket elimination: A unifying framework
for reasoning. Artificial Intelligence, 113:41–85, 1999.

[6] R. Fisher. Statistical Methods for Research Workers.
Macmillan Pub Co, 1925.
[7] R. Fisher. Statistical Methods and Scientific Inference.
Oliver and Boyd, 1956.
[8] R. Fung and R. Shachter. Contingent influence
diagrams. Working Paper, Dept. of EngineeringEconomic Systems, Stanford University, 1990.
[9] D. Geiger and D. Heckerman. Knowledge representation and inference in similarity networks and bayesian
multinets. Artificial Intelligence, 82:45–74, 1996.
[10] I. Good. Maximum entropy for hypothesis formulation, especially for multidimensional contingency taAn example where there are substantial computational
bles. The Annals of Math. Stat., 34:911–934, 1963.
savings when using the inference algorithm proposed
[11]
D. Heckerman. A tractable inference algorithm for
can be found in a model such as the QMR-DT netdiagnosing multiple diseases. UAI, 230:362–367, 1989.
work [17], which is comprised of noisy-OR functions,
[12] D. Heckerman. Probabilistic Similarity Networks.
mentioned in Section 2.2. The QMR-DT network is
MIT Press, 1991.
a two-level or bipartite BN where all variables are bi[13] D. Knoke and P. Burke. Log-Linear Models. Sage
nary. The top level of the graph contains nodes for
Publications Inc, 1980.
[14] F. R. Kschischang, B. Frey, and H. Loeliger. Factor
the diseases C, and the bottom level contains nodes
graphs and the sum-product algorithm. IEEE Trans.
for the findings E. The conditional probabilities in
Inform. Theory, 47(2):498–519, 2001.
the network P (Ei = ei |Πi ), where Πi are the parents
[15] S. Lauritzen. Graphical Models. Oxford University
of finding Ei in the network, are represented by noisyPress, 1996.
OR functions.
[16] J. Lindsey. Conditional independence and log linear models for multi-dimensional contingency tables.
Heckerman (1989) has developed an algorithm, called
Quality and Quantity, 8(4):377–390, 1974.
Quickscore, which takes advantage of the indepen[17] B. Middleton and et al. Probabilistic diagnosis using a
dence of the cause variables in the context of a negative
reformulation of the INTERNIST-1/QMR knowledge
finding Ei = 0 and uses it to speed up inference in the
base: Part II. Evaluation of diagnostic performance.
SIAM Journal on Computing, 30:256–267, 1991.
QMR-DT network [11].
[18] J. J. Oliver. Decision graphs - an extension of decision
For every noisy-OR function P (E|C1 , . . . , Cm ) a structrees. Proceedings of the Fourth International Workture of a multiplicative model that captures the inshop on Artificial Intelligence and Statistics, pages
343–350, 1993.
dependence does not contain elements s such that
[19] J. Pearl. Probabilistic Reasoning in Intelligent Syss ∧ (E = 0) 6= ⊥ for which Is (Ci ) = 1 and Is (Cj ) = 1,
tems: Networks of Plausible Inference. Morgan Kauffor all i, j ≤ m.
mann, 1988.
[20] D. Poole and N. Zhang. Exploiting contextual indeIn addition, running Algorithm 1 using multiplicative
pendence in probabilistic inference. JAIR, 18:263–313,
models with structures
^
2003.
Si = {(Ei = 1)
(Ci = ci ) : ∀Ci = ci }∨
[21] R. Potts. Some generalized order-disorder transforCi ∈Πi
mations. Proceedings of the Cambridge Philosophical
^
Society, 48:106–109, 1952.
{(Ei = 0)∧(Ci = 1) : Ci ∈ Πi }∨((Ei = 0)
(Ci = 0)) [22] S. Safavian and D. Landgrebe. A survey of decision
Ci ∈Πi
tree classifier methodology. IEEE transactions on systems, man, and cybernetics, 21(3):660–674, 1991.
is identical to the Quickscore algorithm and gains the
[23] J. Smith, S. Holtzman, and J. Matheson. Structuring
same savings automatically.
conditional relationships in influence diagrams. Oper.
Res., 41(2):280–297, 1993.



Recently several researchers have investigated techniques for using data to learn
Bayesian networks containing compact representations for the conditional probability
distributions (CPDs) stored at each node.
The majority of this work has concentrated
on using decision-tree representations for
the CPDs. In addition, researchers typically apply non-Bayesian (or asymptotically
Bayesian) scoring functions such as MDL to
evaluate the goodness-of-fit of networks to
the data.
In this paper we investigate a Bayesian approach to learning Bayesian networks that
contain the more general decision-graph representations of the CPDs. First, we describe
how to evaluate the posterior probability—
that is, the Bayesian score—of such a network, given a database of observed cases.
Second, we describe various search spaces
that can be used, in conjunction with a scoring function and a search procedure, to identify one or more high-scoring networks. Finally, we present an experimental evaluation
of the search spaces, using a greedy algorithm
and a Bayesian scoring function.

1

INTRODUCTION

Given a set of observations in some domain, a common problem that a data analyst faces is to build one
or more models of the process that generated the data.
In the last few years, researchers in the UAI community have contributed an enormous body of work to
this problem, using Bayesian networks as the model of
choice. Recent works include Cooper and Herskovits

(1992), Buntine (1991), Spiegelhalter et. al (1993),
and Heckerman et al. (1995).
A substantial amount of the early work on learning Bayesian networks has used observed data to infer global independence constraints that hold in the
domain of interest. Global independences are precisely those that follow from the missing edges within
a Bayesian-network structure. More recently, researchers (including Boutilier et al., 1995 and Friedman and Goldszmidt, 1996) have extended the “classical” definition of a Bayesian network to include efficient representations of local constraints that can hold
among the parameters stored in the nodes of the network. Two notable features about the this recent work
are (1) the majority of effort has concentrated on inferring decision trees, which are structures that can explicitly represent some parameter equality constraints
and (2) researchers typically apply non-Bayesian (or
asymptotically Bayesian) scoring functions such as
MDL as to evaluate the goodness-of-fit of networks
to the data.
In this paper, we apply a Bayesian approach to learning Bayesian networks that contain decision-graphs—
generalizations of decision trees that can encode arbitrary equality constraints—to represent the conditional probability distributions in the nodes.
In Section 2, we introduce notation and previous relevant work. In Section 3 we describe how to evaluate
the Bayesian score of a Bayesian network that contains
decision graphs. In Section 4, we investigate how a
search algorithm can be used, in conjunction with a
scoring function, to identify these networks from data.
In Section 5, we use data from various domains to
evaluate the learning accuracy of a greedy search algorithm applied the search spaces defined in Section 4.
Finally, in Section 6, we conclude with a discussion of
future extensions to this work.

2

BACKGROUND

In this section, we describe our notation and discuss
previous relevant work. Throughout the remainder of
this paper, we use lower-case letters to refer to variables, and upper-case letters to refer to sets of variables. We write xi = k when we observe that variable
xi is in state k. When we observe the state of every variable in a set X, we call the set of observations
a state of X. Although arguably an abuse of notation, we find it convenient to index the states of a
set of variables with a single integer. For example, if
X = {x1 , x2 } is a set containing two binary variables,
we may write X = 2 to denote {x1 = 1, x2 = 0}.
In Section 2.1, we define a Bayesian network. In Section 2.2 we describe decision trees and how they can be
used to represent the probabilities within a Bayesian
network. In Section 2.3, we describe decision graphs,
which are generalizations of decision trees.
2.1

BAYESIAN NETWORKS

Consider a domain U of n discrete variables x1 , . . . , xn ,
where each xi has a finite number of states. A Bayesian
network for U represents a joint probability distribution over U by encoding (1) assertions of conditional
independence and (2) a collection of probability distributions. Specifically, a Bayesian network B is the pair
(BS , Θ), where BS is the structure of the network, and
Θ is a set of parameters that encode local probability
distributions.
The structure BS has two components: the global
structure G and a set of local structures M . G is an
acyclic, directed graph—dag for short—that contains
a node for each variable xi ∈ U . The edges in G denote probabilistic dependences among the variables in
U . We use P ar(xi ) to denote the set of parent nodes
of xi in G. We use xi to refer to both the variable in
U and the corresponding node in G. The set of local
structures M = {M1 , . . . , Mn } is a set of n mappings,
one for each variable xi , such that Mi maps each value
of {xi , P ar(xi )} to a parameter in Θ.
The assertions of conditional independence implied by
the global structure G in a Bayesian network B impose
the following decomposition of the joint probability
distribution over U :
Y
p(xi |P ar(xi ), Θ, Mi , G)
(1)
p(U |B) =
i

The set of parameters Θ contains—for each node xi ,
for each state k of xi , and for each parent state j—
a single parameter1 Θ(i, j, k) that encodes the condiP
1
Because the sum

k

p(xi = k|P ar(xi ), Θ, Mi , G) must

x

y
z

Figure 1: Bayesian network for U = {x, y, z}
tional probabilities given in Equation 1. That is,
p(xi = k|P ar(xi ) = j, Θ, Mi , G) = Θ(i, j, k)

(2)

Note that the function Θ(i, j, k) depends on both Mi
and G. For notational simplicity we leave this dependency implicit.
Let ri denote the number of states of variable xi , and
let qi denote the number of states of the set P ar(xi ).
We use Θij to denote the set of parameters characterizing the distribution p(xi |P ar(xi ) = j, Θ, Mi , G):
i
Θij = ∪rk=1
Θ(i, j, k)

We use Θi to denote the set of parameters
characterizing all of the conditional distributions
p(xi |P ar(xi ), Θ, Mi , G):
i
Θij
Θi = ∪qj=1

In the “classical” implementation of a Bayesian network, each node xi stores (ri − 1) · qi distinct parameters in a large table. That is, Mi is simply a lookup
into a table. Note that the size of this table grows
exponentially with the number of parents qi .
2.2

DECISION TREES

There are often equality constraints that hold among
the parameters in Θi , and researchers have used mappings other than complete tables to more efficiently
represent these parameters. For example, consider the
global structure G depicted in Figure 1, and assume
that all nodes are binary. Furthermore, assume that if
x = 1, then the value of z does not depend on y. That
is,
p(z|x = 1, y = 0, Θ, Mz , G) = p(z|x = 1, y = 1, Θ, Mz , G)

Using the decision tree shown in Figure 2 to implement the mapping Mz , we can represent p(z|x =
1, y, Θ, MZ ) using a single distribution for both
p(z|x = 1, y = 0, Θ, Mz , G) and p(z|x = 1, y =
1, Θ, Mz , G).
be one, Θ will actually only contain ri − 1 distinct parameters for this distribution. For simplicity, we leave this
implicit for the remainder of the paper.

ditional distributions. Furthermore, many researchers
have developed methods for learning these local structures from data.

x
1

0

y
0
p(z|x=0, y=0)

1

p(z|x=1, y=0)
=
p(z|x=1, y=1)

p(z|x=0, y=1)

Figure 2: Decision tree for node z

Decision trees, described in detail by Breiman (1984),
can be used to represent sets of parameters in a
Bayesian network. Each tree is a dag containing exactly one root node, and every node other than the
root node has exactly one parent. Each leaf node contains a table of k − 1 distinct parameters that collectively define a conditional probability distribution
p(xi |P ar(xi ), Θ, Mi , D). Each non-leaf node in the
tree is annotated with the name of one of the parent
variables π ∈ P ar(xi ). Out-going edges from a node π
in the tree are annotated with mutually exclusive and
collectively exhaustive sets of values for the variable
π.
When a node v in a decision tree is annotated with
the name π, we say that v splits π. If the edge from v1
to child v2 is annotated with the value k, we say that
v2 is the child of v1 corresponding to k. Note that by
definition of the edge annotations, the child of a node
corresponding to any value is unique.
We traverse the decision tree to find the parameter
Θ(i, j, k) as follows. First, initialize v to be the root
node in the decision tree. Then, as long as v is not a
leaf, let π be the node in P ar(xi ) that v splits, and
reset v to be the child of v corresponding to the value
of π—determined by P ar(xi ) = j—and repeat. If v is
a leaf, we we return the parameter in the table corresponding to state k of xi .
Decision tree are more expressive mappings than complete tables, as we can represent all of the parameters
from a complete table using a complete decision tree. A
complete decision tree Ti for a node xi is a tree of depth
|P ar(xi )|, such that every node vl at level l in Ti splits
on the lth parent πl ∈ P ar(xi ) and has exactly rπl
children, one for each value of π. It follows by this definition that if Ti is a complete tree, then Θ(i, j, k) will
map to a distinct parameter for each distinct {i, j},
which is precisely the behavior of a complete table.
Researchers have found that decision trees are useful
for eliciting probability distributions, as experts often have extensive knowledge about equality of con-

2.3

DECISION GRAPHS

In this section we describe a generalization of the decision tree, known as a decision graph, that can represent a much richer set of equality constraints among
the local parameters. A decision graph is identical to a
decision tree except that, in a decision graph, the nonroot nodes can have more than one parent. Consider,
for example, the decision graph depicted in Figure 3.
This decision graph represents a conditional probability distribution p(z|x, y, Θ) for the node z in Figure
1 that has different equality constraints than the tree
shown in Figure 2. Specifically, the decision graph encodes the equality
p(z|x = 0, y = 1, Θ) = p(z|x = 1, y = 0, Θ)

x
1

0

y
0

p(z|x=0, y=0)

y
1

0

p(z|x=0, y=1)
=
p(z|x=1, y=0)

1
p(z|x=1, y=1)

Figure 3: Decision graph for node z
We use Di to denote a decision graph for node xi .
If the mapping in a node xi is implemented with Di ,
we use Di instead of Mi to denote the mapping. A
decision-graph Di can explicitly represent an arbitrary
set of equality constraints of the form
Θij = Θij 0

(3)

for j 6= j 0 . To demonstrate this, consider a complete
tree Ti for node xi . We can transform Ti into a decision
graph that represents all of the desired constraints by
simply merging together any leaf nodes that contain
sets that are equal.
It is interesting to note that any equality constraint of
the form given in Equation 3 can also be interpreted
as the following independence constraint:
xi ⊥⊥ P ar(xi ) | P ar(xi ) = j or P ar(xi ) = j 0
If we allow nodes in a decision graph Di to split on
node xi as well as the nodes in P ar(xi ), we can represent an arbitrary set of equality constraints among

the parameters Θi . We return to this issue in Section
6, and assume for now that nodes in Di do not split
on xi .

3

LEARNING DECISION GRAPHS

As we showed in Section 2, if the local structure for a
node xi is a decision graph Di , then sets of parameters Θij and Θij 0 can be identical for j 6= j 0 . For the
derivations to follow, we find it useful to enumerate the
distinct parameter sets in Θi . Equivalently, we find it
useful to enumerate the leaves in a decision graph.

Many researchers have derived the Bayesian measureof-fit—herein called the Bayesian score—for a network,
assuming that there are no equalities among the parameters. Friedman and Goldszmidt (1996) derive
the Bayesian score for a structure containing decision trees. In this section, we show how to evaluate
the Bayesian score for a structure containing decision
graphs.

For the remainder of this section, we adopt the following syntactic convention. When referring to a parameter set stored in the leaf of a decision graph, we
use a to denote the node index, and b to denote the
parent-state index. When referring to a parameter set
in the context of a specific parent state of a node, we
use i to denote the node index and j to denote the
parent-state index.

To derive the Bayesian score, we first need to make
an assumption about the process that generated the
database D. In particular, we assume that the
database D is a random (exchangeable) sample from
some unknown distribution ΘU , and that all of the
constraints in ΘU can be represented using a network
structure BS containing decision graphs.

To enumerate the set of leaves in a decision graph D a ,
we define a set of leaf-set indices La . The idea is that
La contains exactly one parent-state index for each leaf
in the graph. More precisely, let l denote the number
of leaves in Da . Then La = {b1 , . . . , bl } is defined as a
set with the following properties:

As we saw in the previous section, the structure
BS = {G, M } imposes a set of independence constraints that must hold in any distribution represented
using a Bayesian network with that structure. We define BSh to be the hypothesis that (1) the independence
constraints imposed by structure BS hold in the joint
distribution ΘU from which the database D was generated, and (2) ΘU contains no other independence
constraints. We refer the reader to Heckerman et al.
(1994) for a more detailed discussion of structure hypotheses.
The Bayesian score for a structure BS is the posterior
probability of BSh , given the observed database D:
p(BSh |D) = c · p(D|BSh )p(BSh )
1
. If we are only concerned with the relwhere c = p(D)
ative scores of various structures, as is almost always
the case, then the constant c can be ignored. Consequently, we extend our definition of the Bayesian score
to be any function proportional to p(D|BSh )p(BSh ).

For now, we assume that there is an efficient method
for assessing p(BSh ) (assuming this distribution is uniform, for example), and concentrate on how to derive
the marginal likelihood term p(D|BSh ). By integrating
over all of the unknown parameters Θ we have:
Z
h
p(Θ|BSh )p(D|Θ, BSh )
(4)
p(D|BS ) =
Θ

Researchers typically make a number of simplifying
assumptions that collectively allow Equation 4 to be
expressed in closed form. Before introducing these assumptions, we need the following notation.

1. For all {b, b0 } ⊆ La , b 6= b0 ⇒ Θa,b 6= Θa,b0
2. ∪b∈La Θa,b = Θa
The first property ensures that each index in L corresponds to a different leaf, and the second property
ensures that every leaf is included.
One assumption used to derive Equation 4 in closed
form is the parameter independence assumption. Simply stated, this assumption says that given the hypothesis BSh , knowledge about any distinct parameter set
Θab does not give us any information about any other
distinct parameter set.
Assumption 1 (Parameter Independence)
p(Θ|BSh ) =

n Y
Y

p(Θab |BSh )

a=1 b∈La

Another assumption that researchers make is the
Dirichlet assumption. This assumption restricts the
prior distributions over the distinct parameter sets to
be Dirichlet.
Assumption 2 (Dirichlet)
For all a and for all b ∈ La ,
p(Θab |BSh ) ∝

ra
Y

abc −1
Θα
abc

c=1

where αabc > 0 for 1 ≤ c ≤ ra
Recall that ra denotes the number of states for node
xa . The hyperparameters αabc characterize our prior

knowledge about the parameters in Θ. Heckerman et
al. (1995) describe how to derive these exponents from
a prior Bayesian network. We return to this issue later.
Using these assumptions, we can derive the Bayesian
score for a structure that contains decision graphs by
following a completely analogous method as Heckerman et al. (1995). Before showing the result, we must
define the inverse function of Θ(i, j, k). Let θ denote
an arbitrary parameter in Θ. The function Θ−1 (θ) denotes the set of index triples that Θ() maps into θ.
That is,
Θ−1 (θ) = {i, j, k|Θ(i, j, k) = θ}
Let Dijk denote the number of cases in D for which
xi = k and P ar(xi ) = j. We define Nabc as follows:
X
Dijk
Nabc =
ijk∈Θ−1 (θabc )

Intuitively, Nabc is the number of cases in D that provide information
about thePparameter θabc . Letting
P
Nab = c Nabc and αab = c αabc , we can write the
Bayesian score as follows:
p(D, BSh )

= p(BSh )

n Y
Y

a=1 b∈La

Γ(αab )
Γ(Nab + αab )

Y Γ(Nabc + αabc )
Γ(αabc )
c=1
|ra |

·

(5)

We can determine all of the counts Nabc for each node
xa as follows. First, initialize all the counts Nabc to
zero. Then, for each case C in the database, let kC
and jC denote the value for xi and P ar(xi ) in the
case, respectively, and increment by one the count
Nabc corresponding to the parameter θabc = p(xi =
kC |P ar(xi ) = jC , Θ, Da ). Each such parameter can
be found efficiently by traversing Da from the root.
We say a scoring function is node decomposable if it
can be factored into a product of functions that depend only a node and its parents. Node decomposability is useful for efficiently searching through the
space of global-network structures. Note that Equation 5 is node decomposable as long as p(BSh ) is node
decomposable.
We now consider some node-decomposable distributions for p(BSh ). Perhaps the simplest distribution is
to assume a uniform prior over network structures.
That is, we set p(BSh ) to a constant in Equation 5.
We use this simple prior for the experiments described
in Section 5. Another approach is to (a-priori) favor
networks with fewer parameters. For example, we can
use
n
Y
p(BSh ) ∝ κ|Θ| =
κ|Θa |
(6)
a=1

where 0 < κ <= 1. Note that κ = 1 corresponds to
the uniform prior over all structure hypotheses.
A simple prior for the parameters in Θ is to assume
αabc = 1 for all a, b, c. This choice of values corresponds to a uniform prior over the parameters, and
was explored by Cooper and Herskovits (1992) in the
context of Bayesian networks containing complete tables. We call the Bayesian scoring function the uniform scoring function if all the hyperparameters are
set to one. We have found that this prior works well
in practice and is easy to implement.
Using two additional assumptions, Heckerman et al.
(1995) show that each αabc can be derived from a prior
Bayesian network. The idea is that αabc is proportional to the prior probability, obtained from the prior
network, of all states of {xi = k, P ar(xi ) = j} that
map to the parameter θabc . Specifically, if B P is our
prior Bayesian network, we set
X
αabc = α
p(xi = k, P ar(xi ) = j|B P )
ijk∈Θ−1 (θabc )

where α is a single equivalent sample size used to asses
all of the exponents, and P ar(xi ) denotes the parents
of xi in G (as opposed to the parents in the prior network). α can be understood as a measure of confidence that we have for the parameters in B P . We call
the Bayesian scoring function the PN scoring function
(P rior N etwork scoring function) if the exponents are
assessed this way. Heckerman et al. (1995) derive
these constraints in the context of Bayesian networks
with complete tables. In the full version of this paper,
we show that these constraints follow when using decision graphs as well, with only slight modifications to
the additional assumptions.
Although we do not provide the details here, we can
use the decision-graph structure to efficiently compute
the exponents αabc from the prior network in much
the same way we computed the Nabc values from the
database.

4

SEARCH

Given a scoring function that evaluates the merit of
a Bayesian-network structure BS , learning Bayesian
networks from data reduces to a search for one or more
structures that have a high score. Chickering (1995)
shows that finding the optimal structure containing
complete tables for the mappings M is NP-hard when
using a Bayesian scoring function. Given this result,
it seems reasonable to assume that by allowing (the
more general) decision-graph mappings, the problem
remains hard, and consequently it is appropriate to
apply heuristic search techniques.

In Section 4.1, we define a search space over decisiongraph structures within a single node xi , assuming
that the parent set P ar(xi ) is fixed. Once such a space
is defined, we can apply to that space any number of
well-known search algorithms. For the experiments
described in Section 5, for example, we apply greedy
search.

0
v1

0

There are three operators we define, and each operator is a modification to the current set of leaves in a
decision graph.
Definition (Complete Split)
Let v be a leaf node in the decision graph, and let π ∈
P ar(xi ) be a parent of xi . A complete split C(v, π)
adds ri new leaf nodes as children to v, where each
child of v corresponds to a distinct value of π.
Definition (Binary Split)
Let v be a leaf node in the decision graph, and let π ∈
P ar(xi ) be a parent of xi . A binary split B(v, π, k)
adds 2 new leaf nodes as children to v, where the first
child corresponds to state k of π, and the other child
corresponds to all other states of π.
Definition (Merge)
Let v1 and v2 be two distinct leaf nodes in the decision
graph. A Merge M (v1 , v2 ) merges the v1 and v2 into
a single node. That is, the resulting node inherits all
parents from both v1 and v2 .
In Figure 4, we show the result of each type of operator
to a decision graph for a node z with parents x and y,
where x and y both have three states.
We add the pre-condition that the operator must
change the parameter constraints implied by the decision graph. We would not allow, for example, a
complete split C(v1 , y) in Figure 4a: two of v1 ’s new
children would correspond to impossible states of y
({y = 0 and y = 1} and {y = 0 and y = 2}), and
the third child would correspond to the original constraints at v1 ({y = 0 and y = 0}).
Note that starting from a decision graph containing a

y
1
0

DECISION-GRAPH SEARCH

In this section, we assume that the states of our search
space correspond to all of the possible decision graphs
for some node xi . In order for a search algorithm to
traverse this space, we must define a set of operators
that transform one state into another.

2

v2

v3

(a)

In Section 4.2 we describe a greedy algorithm that
combines local-structure search over all the decision
graphs in the nodes with a global-structure search over
the edges in G.
4.1

y
1

(b)

0

2
x
1

y
1

y

2
0

x
2

0

(c)

1,2

1,2

(d)

Figure 4: Example of the application of each type of
operator: (a) the original decision graph, (b) the result of applying C(v3 , x), (c) the result of applying
B(v3 , x, 0), and (d) the result of applying M (v2 , v3 )
single node (both the root and a leaf node), we can
generate a complete decision tree by repeatedly applying complete splits. As discussed in the previous
section, we can represent any parameter-set equalities
by merging the leaves of a complete decision tree. Consequently, starting from a graph containing one node
there exists a series of operators that result in any set
of possible parameter-set equalities. Note also that if
we repeatedly merge the leaves of a decision graph until there is a single parameter set, the resulting graph
is equivalent (in terms of parameter equalities) to the
graph containing a single node. Therefore, our operators are sufficient for moving from any set of parameter constraints to any other set of parameter constraints. Although we do not discuss them here, there
are methods that can simplify (in terms of the number
of nodes) some decision graphs such that they represent the same set of parameter constraints.
The complete-split operator is actually not needed to
ensure that all parameter equalities can be reached:
any complete split can be replaced by a series of binary
splits such that the resulting parameter-set constraints
are identical. We included the complete-split operator
in the hopes that it would help lead the search algorithm to better structures. In Section 5, we compare
greedy search performance in various search spaces defined by including only subsets of the above operators.
4.2

COMBINING GLOBAL AND LOCAL
SEARCH

In this section we describe a greedy algorithm that
combines global-structure search over the edges in G

with local-structure search over the decision graphs in
all of the nodes of G.
Suppose that in the decision-graph Di for node xi ,
there is no non-leaf node annotated with some parent
π ∈ P ar(xi ). In this case, xi is independent of π
given its other parents, and we can remove π from
P ar(xi ) without violating the decomposition given in
Equation 1. Thus given a fixed structure, we can learn
all the local decision graphs for all of the nodes, and
then delete those parents that are independent. We
can also consider adding edges as follows. For each
node xi , add to P ar(xi ) all non-descendants of xi in
G, learn a decision graph for xi , and then delete all
parents that are not contained in the decision graph.
Figure 5 shows a greedy algorithm that uses combines
these two ideas. In our experiments, we started the
algorithm with a structure for which G contains no
edges, and each graph Di consists of a single root node.
1.

Score the current network structure BS

2.

For each node xi in G

3.
4.
5.

Add every non-descendant that is not a parent
of xi to P ar(xi )
For every possible operator O to the decision
graph Di
Apply O to BS

6.

Score the resulting structure

7.

Unapply O

8.
9.
10.
11.

12.
13.
14.

Remove any parent that was added to xi in
step 3
If the best score from step 6 is better than the
current score
Let O be the operator that resulted in the best
score
If O is a split operator (either complete or binary) on a node xj that is not in P ar(xi ), then
add xj to P ar(xi )
Apply O to BS
Goto 1
Otherwise, return BS

Figure 5: Greedy algorithm that combines local and
global structure search
Note that as a result of a merge operator in a decision
graph D i , xi may be rendered independent from one
of its parents π ∈ P ar(xi ), even if D i contains a node
annotated with π. For a simple example, we could
repeatedly merge all leaves into a single leaf node,
and the resulting graph implies that xi does not depend on any of its parents. We found experimentally
that—when using the algorithm from Figure 5—this
phenomenon is rare. Because testing for these parent
deletions is expensive, we chose to not check for them

in the experiments described in Section 5.
Another greedy approach for learning structures containing decision trees has been explored by Friedman
and Goldszmidt (1996). The idea is to score edge operations in G (adding, deleting, or reversing edges) by
applying the operation and then greedily learning the
local decision trees for any nodes who’s parents have
changed as a result of the operation. In the full version
of the paper, we compare our approach to theirs.

5

EXPERIMENTAL RESULTS

In this section we investigate how varying the set of
allowed operators affects the performance of greedy
search. By disallowing the merge operator, the search
algorithms will identify decision-tree local structures in
the Bayesian network. Consequently, we can see how
learning accuracy changes, in the context of greedy
search, when we generalize the local structures from
decision trees to decision graphs.
In all of the experiments described in this section, we
measure learning accuracy by the posterior probability
of the identified structure hypotheses. Researchers often use other criteria, such as predictive accuracy on a
holdout set or structural difference from some generative model. The reason that we do not use any of these
criteria is that we are evaluating how well the search
algorithm performs in various search spaces, and the
goal of the search algorithm is to maximize the scoring
function. We are not evaluating how well the Bayesian
scoring functions approximate some other criteria.
In our first experiment, we consider the Promoter
Gene Sequences database from the UC Irvine collection, consisting of 106 cases. There are 58 variables
in this domain. 57 of these variables, {x1 , . . . , x57 }
represent the “base-pair” values in a DNA sequence,
and each has four possible values. The other variable,
promoter, is binary and indicates whether or not the
sequence has promoter activity. The goal of learning in
this domain is to build an accurate model of the distribution p(promoter|x1 , . . . , x57 ), and consequently it is
reasonable to consider a static graphical structure for
which P ar(promoter) = {x1 , . . . , x57 }, and search for
a decision graph in node promoter.
Table 1 shows the relative Bayesian scores for the best
decision graph learned, using a greedy search with various parameter priors and search spaces. All searches
started with a decision graph containing a single node,
and the current best operator was applied at each step
until no operator increased the score of the current
state. Each column corresponds to a different restriction of the search space described in Section 4.1: the
labels indicate what operators the greedy search was

Table 1: Greedy search performance for various
Bayesian scoring functions, using different sets of operators, in the P romoter domain.

uniform
U-PN 10
U-PN 20
U-PN 30
U-PN 40
U-PN 40

C
0
0
0
0
0
0

B
13.62
6.12
5.09
4.62
3.14
2.99

CB
6.07
4.21
3.34
2.97
1.27
1.12

CM
22.13
9.5
14.11
10.93
16.3
15.76

BM
26.11
10.82
12.11
12.98
13.54
15.54

CBM
26.11
12.93
14.12
16.65
16.02
17.54

allowed to use, where C denotes complete splits, B
denotes binary splits, and M denotes merges. The column labeled BM, for example, shows the results when
a greedy search used binary splits and merges, but
not complete splits. Each row corresponds to a different parameter-prior for the Bayesian scoring function.
The U-PN scoring function is a special case of the PN
scoring function for which the prior network imposes
a uniform distribution over all variables. The number following the U-PN in the row labels indicates the
equivalent-sample size α. All results use a uniform
prior over structure hypotheses. A value of zero in a
row of the table denotes the hypothesis with lowest
probability out of all those identified using the given
parameter prior. All other values denote the natural
logarithm of how many times more likely the identified
hypothesis is than the one with lowest probability.
By comparing the relative values between searches
that use merges and searches that don’t use merges,
we see that without exception, adding the merge operator results in a significantly more probable structure
hypothesis. We can therefore conclude that a greedy
search over decision graphs results in better solutions
than a greedy search over decision trees. An interesting observation is that complete-split operator actually
reduces solution quality when we restrict the search to
decision trees.
We performed an identical experiment to another classification problem, but for simplicity we only present
the results for the uniform scoring function. Recall
from Section 3 that the uniform scoring function has
all of the hyperparameters αabc set to one. This second
experiment was run with the Splice-junction Gene Sequences database, again from the UC Irvine repository.
This database also contains a DNA sequence, and the
problem is to predict whether the position in the middle of the sequence is an “intron-exon” boundary, an
“exon-intron” boundary, or neither. The results are
given in Table 2. We used the same uniform prior for
structure hypotheses.

Table 2: Greedy search performance for the uniform
scoring function, using different sets of operators, in
the Splice domain.
C
0

B
383

CB
363

CM
464

BM
655

CBM
687

Table 3: Greedy search performance for the uniform
scoring function for each node in the ALARM network.
Also included is the uniform score for the completetable model
COMP
C
B
CB CM BM CBM
0
134 186 165 257 270
270

Table 2 again supports the claim that we get a significant improvement by using decision graphs instead of
decision trees.
Our final set of experiments were done in the ALARM
domain, a well-known benchmark for Bayesiannetwork learning algorithms. The ALARM network,
described by Beinlich et al. (1989), is a handconstructed Bayesian network used for diagnosis in a
medical domain. The parameters of this network are
stored using complete tables.
In the first experiment for the ALARM domain, we
demonstrate that for a fixed global structure G, the
hypothesis identified by searching for local decision
graphs in all the nodes can be significantly better than
the hypothesis corresponding to complete tables in the
nodes. We first generated 1000 cases from the ALARM
network, and then computed the uniform Bayesian
score for the ALARM network, assuming that the parameter mappings M are complete tables. We expect
the posterior of this model to be quite good, because
we’re evaluating the generative model structure. Next,
using the uniform scoring function, we applied the six
greedy searches as in the previous experiments to identify good decision graphs for all of the nodes in the
network. We kept the global structure G fixed to be
identical to the global structure of the ALARM network. The results are shown in Table 3, and the values
have the same semantics as in the previous two tables.
The score given in the first column labeled COMP is
the score for the complete-table model.
Table 3 demonstrates that search performance using
decision graphs can identify significantly better models than when just using decision trees. The fact that
the complete-table model attains such a low score (the
best hypothesis we found is e270 times more probable
than the complete-table hypothesis!) is not surprising upon examination of the probability tables stored

Table 4: Performance of greedy algorithm that combines local and global structure search, using different
sets of operators, in the ALARM domain. Also included is the result of a greedy algorithm that searches
for global structure assuming complete tables.
COMP
255

C
0

B
256

CB
241

CM
869

BM
977

CBM
1136

in the ALARM network: most of the tables contain
parameter-set equalities.
In the next experiment, we used the ALARM domain
to test the structure-learning algorithm given in Section 4.2. We again generated a database of 1000 cases,
and used the uniform scoring function with a uniform
prior over structure hypotheses. We ran six versions
of our algorithm, corresponding to the six possible sets
of local-structure operators as in the previous experiments. We also ran a greedy structure-search algorithm that assumes complete tables in the nodes. We
initialized this search with a global network structure
with no edges, and the operators were single-edge modifications to the graph: deletion, addition and reversal.
In Table 4 we show the results. The column labeled
COMP corresponds to the greedy search over structures with complete tables.
Once again, we note that when we allow nodes to
contain decision graphs, we get a significant improvement in solution quality. Note that the search
over complete-table structures out-performed our algorithm when we restricted the algorithm to search
for decision trees containing either (1) only complete
splits or (2) complete splits and binary splits.
In our final experiment, we repeated the previous experiment, except that we only allowed our algorithm
to add parents that are not descendants in the generative model. That is, we restricted the global search
over G to those dags that did not violate the partial ordering in the ALARM network. We also ran the same
greedy structure-search algorithm that searches over
structures with complete tables, except we initialized
the search with the ALARM network. The results of
this experiment are shown in Table 5. From the table,
we see that the constrained searches exhibit the same
relative behavior as the unconstrained searches.
For each experiment in the ALARM domain (Tables
3, 4, and 5) the values presented measure the performance of search relative to the worst performance in
that experiment. In Table 6, we compare search performance across all experiments in the ALARM domain. That is, a value of zero in the table corresponds
to the experiment and set of operators that led to the

Table 5: Performance of a restricted version of our
greedy algorithm, using different sets of operators, in
the ALARM domain. Also included is the result of
a greedy algorithm, initialized with the global structure of the ALARM network, that searches for global
structure assuming complete tables.
COMP
0

C
179

B
334

CB
307

CM
553

BM
728

CBM
790

Table 6: Comparison of Bayesian scores for all experiments in the ALARM domain

S
U
C

COMP
278
255
336

C
412
0
515

B
464
256
670

CB
443
241
643

CM
534
869
889

BM
548
976
1064

CBM
548
1136
1126

learned hypothesis with lowest posterior probability,
out of all experiments and operator restrictions we
considered in the ALARM domain. All other values
given in the table are relative to this (lowest) posterior
probability. The row labels correspond to the experiment: S denotes the first experiment that performed
local searches in a static global structure, U denotes
the second experiment that performed unconstrained
structural searches, and C denotes the final experiment
that performed constrained structural search.
Rather surprising, each hypothesis learned using
global-structure search with decision graphs had a
higher posterior than every hypothesis learned using
the generative static structures.

6

DISCUSSION

In this paper we showed how to derive the Bayesian
score of a network structure that contains parameter
maps implemented as decision graphs. We defined a
search space for learning individual decision graphs
within a static global structure, and defined a greedy
algorithm that searches for both global and local structure simultaneously. We demonstrated experimentally
that greedy search over structures containing decision
graphs significantly outperforms greedy search over
both (1) structures containing complete tables and (2)
structures containing decision trees.
We now consider an extension to the decision graph
that we mentioned in Section 2.3. Recall that in a decision graph, the parameter sets are stored in a table
within the leaves. When decision graphs are implemented this way, any parameter θabc must belong to
exactly one (distinct) parameter set. An important

consequence of this property is that if the priors for
the parameter sets are Dirichlet (Assumption 2), then
the posterior distributions are Dirichlet as well. That
is, the Dirichlet distribution is conjugate with respect
to the likelihood of the observed data. As a result, it is
easy to derive the Bayesian scoring function in closed
form.
If we allow nodes within a decision graph Di to split
on node xi , we can represent an arbitrary set of parameter constraints of the form Θ(i, j, k) = Θ(i, j 0 , k 0 )
for j 6= j 0 and k 6= k 0 . For example, consider a Baysian
network for the two-variable domain {x, y}, where x is
a parent of y. We can use a decision graph for y that
splits on y to represent the constraint
p(y = 1|x = 0, Θ, Dy , G) = p(y = 0|x = 1, Θ, Dy , G)
Unfortunately, when we allow these types of constraints, the Dirichlet distribution is no longer conjugate with respect to the likelihood of the data, and the
parameter independence assumption is violated. Consequently, the derivation described in Section 3 will
not apply. Conjugate priors for a decision graph Di
that splits on node xi do exist, however, and in the
full version of this paper we use a weaker version of
parameter independence to derive the Bayesian score
for these graphs in closed form.
We conclude by noting that it is easy to extend the definition of a network structure to represent constraints
between the parameters of different nodes in the network, e.g. Θij = Θi0 j 0 for i 6= i0 . Both Buntine
(1994) and Thiesson (1995) consider these types of
constraints. The Bayesian score for such structures
can be derived by simple modifications to the approach
described in this paper.




explored by Breese, Beckerman and Kadie

(1998),

have not relied on the order in which users express
We treat collaborative filtering as a univari­
ate time series problem: given a user's previ­
ous votes, predict the next vote. We describe
two families of methods for transforming data
to encode time order in ways amenable to
off-the-shelf classification and density estima­
tion tools. Using a decision-tree learning tool
and two real-world data sets, we compare the
results of these approaches to the results of
collaborative filtering without ordering infor­
mation.

The improvements in both predic­

tive accuracy and in recommendation quality
that we realize advocate the use of predictive
algorithms exploiting the temporal order of
data.

their preferences.

Vector-space methods draw heav­

ily on work in the information retrieval literature (see,
e.g., Baeza-Yates and Ribeiro-Neto,

1999),

where in­

dividual documents are treated as a "bag of words".
Likewise, probabilistic techniques (e.g. Hofmann and
Puzicha,

1999

and Beckerman,

Rounthwaite and Kadie,

2000)

Chickering,

Meek,

have computed proba­

bility distributions over recommendations conditioned
on the entire vote history without regard to time or­
der. In the CF literature, a "bag of votes" (i.e. atem­
poral) assumption prevails, and the collaborative fil­
tering problem is cast as classification (with classes
"relevant" and "irrelevant") or density estimation (of
the probability that a document is relevant, given a
user's votes).
We instead consider collaborative filtering as a univari­

Keywords: Dependency networks, probabilistic deci­
sion trees, language models, collaborative filtering, rec­
ommendation systems.

1

Introduction

The collaborative filtering problem arose in response
to the availability of large volumes of information to
a variety of users. Such information delivery mecha­
nisms as

Usenet and

online catalogs have created large

stores of data, and it has become the users' task to dis­
cover the most relevant items in those stores. Rather
than requiring that users manually sift through the
full space of available items, trusting that authors
respect the available system of topics, CF tools rec­
ommend items of immediate or future interest based
on all users' expressed preferences ("votes"), suggest­
ing those items of interest to other users with similar
tastes.

These votes may be either explicit, as in re­

sponse to a direct inquiry, or implicit, as by the choice
to follow one hyperlink instead of others.
In general, algorithms for the CF task, such as those

ate time series prediction problem, and represent the
time order of a user's votes explicitly when learning a
recommendation model. Further, we encode time or­
der by transforming the data in such a way that stan­
dard atemporallearning algorithms can be applied di­
rectly to the problem. Other authors (cf. Mozer,

1993)

have applied atemporal learning techniques to tempo­
ral data; we describe here two successful generic tech­
niques. As a result, researchers can simply transform
their data as we describe and apply existing tools, in­
stead of having to re-implement various collaborative
filtering algorithms for awareness of vote order. Our
approach allows CF models to encode changes in a
user's preferences over time. It also allows models to
represent (indirectly) structure built into the feature
space that would be lost in a bag of votes representa­
tion. For example, Web page viewing histories ordered
by page request can express the link structure of a Web
site because a user is most likely to follow links from
his current page. Similarly, television viewing histories
encode the weekly schedule of shows: a viewer cannot
hop from

Buffy

the

Vampire Slayer to Dawson's ·Creek

if the two are not contemporaneous.

ZIMDARS ET AL.

UAI2001

For simplicity, we assume for the remainder of this
paper that user preferences are expressed as implicit
votes (see, e.g., Breese et al., 1998). That is, a users'
vote history is a list of items that the user preferred,
as opposed to an explicit ranking of the items. In a
movie domain, for example, this means that a user's
vote history is simply a list of movies that he watched,
and we assume that he preferred those movies to the
ones he did not watch. We note, however, that the
transformations we describe are easily generalized to
explicit voting.
In Section 2, we present two methods for transform­
ing user vote histories that encode time-order infor­
mation in ways that traditional atemporal modeling
algorithms can use. In Section 3, we discuss three
candidate models that can be learned from standard
algorithms applied to the transformed data. In Sec­
tion 4, we describe the data sets and criteria by which
we will compare our approaches, and in Section 5 we
present our experimental results from using decision­
tree learning algorithms.

581

age, and G is a binary variable that denotes the per­
son's gender. Under the iid assumption a learning al­
gorithm can use observed values of S, A, and G for
other people in the population to estimate the distri­
bution p(SIA, G), then make a prediction about the
particular person of interest with that distribution.
In the following sections, we describe how data that
contains vote histories can be transformed, using var­
ious assumptions, into the case representation so that
standard machine-learning algorithms can be used to
predict the next vote in a sequence. First, we need
some notation.
We use item to denote an entity for which users ex­
press preferences by voting, and we use 1 to denote
the total number of such items.

For example, in a

scenario, 1 is the total num­
ber of movies considered by the collaborative-filtering
system. For simplicity we refer to each item by a one­
based integer index. That is, the items in the system
are mapped to the indices:
movie-recommendation

{1, ...,1'}
2

Data Transformations

In this section, we describe two methods that trans­
form time-ordered vote histories into a representation
that traditional atemporal modeling algorithms can
use; we call this representation the case representation.
In the case representation, the data D consists of a set
of cases (or records) {C1, ... ,Cm}, where each case
Ci = { x1, .., Xn} consists of a value for zero or more
of the variables in the domain X = {X1, . . , Xn}.
.

.

The important (sometimes implicit) assumption of
modeling algorithms that use the case representation
is that the observed cases are independent and identi­
cally distributed (iid) from some joint probability dis­
tribution p(X1, ... , Xn)1; an equivalent Bayesian as­
sumption is that the cases are infinitely exchangeable,
meaning that any permutation of a set of cases has
the same probability. The learning algorithms use the
observed case values in D to identify various models
of the generative distribution.
As an example, consider the problem of predicting
whether or not a particular person will watch some
television show based on that person's age and gen­
der. Using the case representation, we might assume
that all people are drawn from some joint probability
distribution p( S,A, G), where S is a binary variable
that indicates whether or not a person watches the
show, A is a continuous variable denoting a person's
1In fact, if we are interested in learning a conditional

model for Y C X, we often need only assume that the

values for the variables in Y are independent samples from
some p(YIX \ Y)

We use Vi to denote the i1h vote history (i.e. user's
votes). In particular, Vi is an ordered list of votes:
{V/,

.

..

, vt'}

where V/ denotes the item index of the j1h vote in the
list, and Ni is the total number of votes made by user

�.

As an example, suppose there are four movies The Ma­
trix, Star Wars, A Fish Called Wanda and Pulp Fic­
tion having indices 1,2,3 and 4, respectively. Suppose
there are two movie watchers in the domain: User 1
watched The Matrix and then watched Pulp Fiction,
and user 2 watched Star Wars, then watched Pulp Fic­
tion, and then watched The Matrix. Then we would
have V1 = {1,4} and V2 {2,4, 1}.
=

For each of the transformations below, we show how
convert from a set of vote histories into (1) a set of
domain variables X= {Xl.···,Xn}, and (2) a set of
cases {C1,... , Cm}, where each case Ci contains a set
of values { x1, . , Xn} for the variables in X. We also
describe what assumptions are made in the original
domain in order for the resulting cases to be iid.
to

. .

2.1

The "Bag-of-votes" Transformation

The first transformation we consider disregards the or­
der of previous votes, corresponding to the assumption
that vote order does not help predict the next vote. As
noted above, this "bag-of-votes" approach is the ap­
proach taken by many collaborative-filtering learning
algorithms.

ZIMDARS ET AL.

582

For each item k, where 1 ::; k ::; /, there is a binary
variable Xj E X, whose states x] and x� correspond
to preferred and not preferred, respectively. There are
no other variables in X. For each vote history V;, we
create a single case Ci with the following values: if
item j occurs at least once anywhere in the sequence
vi, then the value Xj in ci is equal to x] . Otherwise,
the value of Xj in Ci is equal to x5 .
The assumption that the cases are iid corresponds to
assuming that the (unordered) votes of all vote histo­
ries (i.e. users) are all drawn from the same distribu­
tion. Under this assumption, we can use an atemporal
learning algorithm with the cases from previous vote
histories learn a model for p(XJIX\XJ) for all XJ EX,
and then use these models to predict the next vote2
for any vote history.
2.2

The Binning Transformation

The second transformation we consider can be help­
ful when user preferences change over time. Although
the transformation does not explicitly use the order
of the votes, it can exploit temporal structure. The
idea is to (1) separate vote histories into bins by their
size, (2) transform the histories from each bin into
the case representation using the "bag-of-votes" trans­
formation described above, and (3) learn a separate
model from the data in each such bin. W hen it comes
time to predict the next vote in a sequence of size k,
we use the model that was learned on the cases derived
from the vote histories in the bin corresponding to k.
Suppose, for example, that we would like to train one
or more models in order to recommend movies to peo­
ple. It might be reasonable to assume that the op­
timal model for predicting the third movie for some­
one may not be a very good model for predicting the
100th movie. W ith binning, we divide up the range of
the number of movies that have previously been seen
into separate bins, and learn a recommendation model
for each. Thus, we might end up with three mod­
els: (1) a simple model that predicts popular movies
for people who do not go to the movies much, (2) a
model that perhaps identifies general viewing prefer­
ences (e.g. comedies) for the typical viewer, and (3) a
model that identifies subtle preference trends for heavy
movie watchers.
In order to perform binning, there are a number of
parameters that need to be set. First, we need to
decide how many bins to use. Second, we need to
decide, for each bin, what history lengths should be
included in that bin.
2

There are some subtleties, addressed below, about how

this prediction is made.

UAI2001

For the experiments that we present in Section 4, we
tried both two and four bins. For each bin, we set a
minimum and maximum value for the length of the
contained histories. We chose this minimum and max­
imum such that the total number of votes in each bin
are roughly the same.
As described above, the binning approach assigns each
vote history to exactly one bin. An alternative ap­
proach, which we call the prefix approach, is to allow
a single vote history to contribute to multiple bins by
adding an appropriate prefix to all of the "previous"
bins. As an example, suppose there are three bins that
accommodate histories of length up to 5, 10, and 100.
In the prefix approach, a vote history of length 90 will
have (1) the first five votes added to the first bin, (2)
the first ten votes added to the second bin, and (3) the
whole history added to the third bin.
The choice of whether or not to use the prefix approach
to binning will depend on user behavior and domain
structure. We identify the following two hypotheses
that can help determine which method is most appro­
priate.
•

The "expert/novice" hypothesis: Users with long
vote histories ("experts" in the domain) have fun­
damentally different preferences than users with
short vote histories ("novices"). As a result, we
expect that omitting prefixes of longer vote histo­
ries from bins for shorter vote histories will result
in better predictive accuracy than the prefix ap­
proach. The expert/novice hypothesis might hold
when predicting preferences for television viewing,
where couch potatoes might have different view­
ing habits than occasional viewers. On a Web site,
heavy users tend to navigate very differently than
"shallow browsers" (cf. Huberman et al., 1998).

•

The "everyone learns" hypothesis: Users with
long vote histories once expressed similar prefer­
ences to users with short vote histories. Under
this hypothesis, we expect that prefixes of long
vote histories will be distributed similarly to short
vote histories, and therefore their inclusion in the
corresponding bins will provide useful data for the
model-building algorithm; as a result, we hope
that the resulting models will be more accurate.
One can also interpret this hypothesis from the
perspective of domain structure constraining user
behavior. For users of a Web portal, initial votes
may be restricted to the home page and top-level
categories linked from that page. For subsequent
page hits, available links may constrain possible
user votes. In this domain, we would expect users
to have similarly-distributed vote prefixes because
site structure does not allow much room for inno-

UA12001

ZIMDARS ET AL.

vation.
For the domains we consider in Section 4, the latter hy­
pothesis seems more appropriate; although we ideally
should have compared the two, in the interest of time
we only used the prefix approach in our experiments.
We chose the bin boundaries so that the total number
of votes of the original (i.e. non-prefix) histories in
each bin were roughly the same.
W hether or not we use the prefix approach, the addi­
tional computational overhead of binning over no bin­
ning is proportional to a constant factor (the number
of bins), because each bin will contain no more votes ,
and no more vote histories, than would a single model
computed using the entire vote set.

Structural aspects of some prediction domains can
make difficult the choice of vote sub-histories to aug­
ment data for binning. Web sites tend to have a hierar­
chical structure with a home page at the root, but the
same cannot be said for television programming sched­
ules, which reflect periodic structure. When predicting
television viewing habits given a "snapshot" of user
viewing histories, prefixes may not reflect the periodic
nature of the program schedule. In such domains, dif­
ferent choices of contiguous vote sub-histories may be
appropriate, but the resulting profusion of data might
render binning impractical.
We should point out that binning can be applied to
collaborative filtering problems in which the temporal
order of the votes is unknown. Although the prefix ap­
proach may not be appropriate, binning based on the
number of votes can potentially lead to significantly
better accuracy in atemporal domains. Consider, for
example, the problem of recommending items in a gro­
cery store based on the products bought (the recom­
mendation may appear as a targeted coupon on a re­
ceipt). It might turn out that, regardless of the order
in which people put groceries in their shopping cart,
the number of items in their cart may indicate very
different shopping behavior; consequently the binning
approach might yield significantly better models than
a system that ignores the number of votes.
2.3

Data Expansion

The final data transformation we consider, which we
call data expansion, finds inspiration in the language
modeling literature (see, e.g., Chen and Goodman,
1996). This method of data expansion distinguishes
the most recent n votes from the entire vote history, as
well as identifying the order of the most recent votes.
All of the variables that we create in the transforma­
tion are binary, and have states x1 and xO correspond
to preferred and not preferred, respectively.

583

In the case representation, we create one binary vari­
able for each of the I items in the domain: xT =
{X[, ... ,X�}. The "T" superscript in X'[ is meant
to indicate that this is a "target variable" that repre­
sents whether or not the next vote is for item k.
The data expansion transformation is parameterized
by a history length l; this parameter, which corre­
sponds to the "n" parameter in an n-gram language
model, determines how far back in the vote history
to look when predicting the next vote. For each in­
teger history 1 � j � l, we again create one bi­
nary _variable for each of the 1 items in the domain:
{X;1, ... ,X_::;-i}. The "-j" superscript in Xf:j is
meant to indicate that this variable represents whether
or not Ph previous vote (from the one we're predicting)
is for item k. We use XL to denote the set of all lagged
variables (e.g. {X!1, . . ,X_::;-1},{X12, .. . ,X;2}).
.

There is a final set of 1 variables consisting of, for
each item, an indicator of whether or not that item
was voted for at least once previously in the given vote
history. We use xc = {Xf, ... , X0} to denote these
variables. In language-modeling p;rlance, these vari­
ables are known as cache variables.
In contrast to the "bag-of-words" approach, where
each vote history was transformed into a single case, in
the data expansion transformation, each vote in every
history _gets a corresponding case. In particular, for
vote V/, which is the lh vote in the ith vote history,
we define the values for all of the variables as follows.
For simplicity, let v = V/. We set the value of target
variable XJ to xl, and we set the value of all other
target variables to xO. For each history variable x-j
k ,
where 1 � j � l, we set the corresponding value to ei­
ther x1 if the lh previous vote in history i has value k,
or xO otherwise. Finally, we set the value of each cache
variable Xf to either xl if item k occurs as a. vote (at
least once) previous to V/ in Vi, or xO otherwise.
We should point out that in order to feasibly learn
a model using the cases that result from the data­
expansion transformation, the learning algorithm(s)
need to use a sparse representation for the cases. See
(e.g.) Chickering and Heckerman (1999) for a discus­
sion.

Consider our movie example again. For simplicity, we
use M, S, F, and P to label all variables we create
corresponding to movie items The Matrix, Star Wars,
A Fish Called Wanda and Pulp Fiction. Furthermore,
we use 1 and 0 to denote the values preferred and not
preferred, respectively.
Suppose we want to transform a vote history con­
taining The Matrix, Pulp Fiction, and Star Wars, in
that order, into the case representation with a history

584

ZIMDARS ET AL.

length of one. First we define the variables

x

=

for this model is expressed as follows:

r r r r
{ M ,s ,F ,P ,
M�1, 8-1, p-1, p-1,
MC ' SC ' pC , pC}

n

P(C

1

shows the case values that

result.
The learning algorithm we use should build a model
for each of the target variables, using all non-target
variables as predictor variables.

That is, we would

like the model to estimate, for each target variable

XJ E XT' the distribution p(XJIXL' xc).

The iid assumption in the case representation-after
performing the data-expansion transformation with
history-length l-implies that each vote is
a distribution that depends on
previous

l

votes and

(2)

(1)

drawn from

the values of the

the presence or absence of at

least one vote for previous items.

v 1 , ... ,vn )

=

P(C

Models

that can be used for collaborative filtering applica­
tions; when learned from data that is transformed as
described in the previous section, these models can
exploit the vote order to improve recommendation ac­
curacy.

Cheeseman and Stutz

1977).

(1995)

(1)

provide details of

In this setting, prediction for collaborative filtering
follows from the density estimation problem, as the
model predict the item(s) most

likely to receive an af­

firmative vote given the user's vote history.

Other latent class models (Hofmann and Puzicha,

1999)

for collaborative filtering

have been proposed

which place user and item on an equal footing. These
permit construction of a two-sided clustering model
with preference values, but they depend on multino­
mial sampling of (user, item) pairs, and as such do not
generalize naturally to new users.

Decision-tree models

v ious work (cf.

Beckerman et al.,

2000)

constructs

a forest of probabilistic decision trees, one for each
item in the database, using a Bayesian scoring crite­
rion (Chickering, Beckerman, and Meek,

1997).

This

provides a compact encoding of conditional probabil­
ities of recommendations, given previous votes.3
use this approach in Section

We

4 to evaluate our data

Memory-based algorithms

Memory-based collaborative filtering algorithms pre­
dict the votes of the active user based on some partial
information about the active user and a set of weights
calculated from the user database. Memory-based al­
gorithms do not provide the probability that the active
user will vote for a particular item. Instead, the active
user's predicted vote an item is a weighted sum of the
votes of the other users. See Breese et. al

(1998) for

a

more detailed discussion.

A

)

=c

i=l

a specific implementation of the learning algorithm.

transformations.

3.2

) IT P(vi I C

The approach that has proven most effective in pre­

In this section, we describe some well-known models

3.1

=c

the EM algorithm (see Dempster, Laird and Rubin,

3.3
3

= c,

The parameters of this model can be learned using

Next, we consider each vote in the history, and create
a case for each one. Table

UAI2001

Cluster models

standard probabilistic model is the naive Bayes

model with a hidden root node-one where the prob­
abilities of votes are conditionally independent given
membership in an unobserved class variable C, where
C ranges over a fairly small set of discrete values. This

corresponds to the intuition that users may be clus­

tered into certain groups expressing common prefer­
ences and tastes.

The joint probability distribution

3.4

Alternative models

The data expansion technique discussed in Section

2.3

suggests the application of language-modeling algo­
rithms to collaborative filtering. We have conducted

limited experiments with variants of n-gram language
models, and the results are promising (although we do
not present them here).
Hidden Markov models
themselves in this setting,

(HMMs)

also

recommend

but in our experience they

are ill-suited to a na"ive representation of the data,

where each possible vote corresponds to exactly one
feature. This reflects in part the number of parame­
ters that must be estimated when running EM for an
HMM: if the model admits
are

me

+

c2

+

c

c

hidden states, then there

parameters to estimate for the poste­

rior probabilities of states, the state transitions, and
3It also permits the construction of a family of graphical
as dependency networks, which have expres­

models known

sive strength similar to Markov networks.

ZIMDARS ET AL.

UAI2001

Table

585

Case values created for the movie example with the data expansion method.

1:

M1
1

Vote

The Matrix
Pulp Fiction
Star Wars

s·l

FJ

p'l

M-1

0
0
0

0
0
1

0

0

0
0

1

1

0

0

s

·1

F-1

p-

0
0
0

0
0
1

0
0
0

1

Me

se

Fe

p--u

0
1
1

0
0

0
0
0

0
0
1

0

Moreover, models are slow to con­

We used probabilistic decision-tree models for our ex­

verge because collaborative filtering data tend to be

periments, and compared both binning and data ex­

very sparse, in that few users vote on ariy one item. As

pansion to the default "bag-of-votes" approach of ig­

the state priors.

a result, evidence for estimating a particular variable

noring the data order.

is rarely presented in training. This sparsity is

we learned a single decision tree per page

integral

For all of the experiments,
to predict

to the collaborative filtering problem, but lethal to ac­

whether the user requests that page, based on the

curate estimation. Finally, HMMs discard much of a

transformed data available at that time.

user's history in making predictions, and our experi­

greedy tree-growing algorithm in conjunction with the

ments indicate that a long history can be informative.

Bayesian score described by Chickering et. al (1997).

We used a

In particular, the score evaluated the posterior model

4

probability using a flat parameter prior, and a model

Experiments

prior of the form r;,f, where

In this section, we describe the experiments we per­
formed to demonstrate that using vote order can im­
prove the accuracy of models.
We conducted our experiments using two real-world
data sets, both of which are Web user traces. In each,

the notion

of

"user" corresponds

to a

server

session,

and a page request was interpreted as an affirmative
vote.
The first data set consists of session traces from
data encompassed

110587

The training

page requests from

27595

users over three days in late August 1999, and the test
data included

54843 requests

from

13563

users on

14

September of the same year. The requests span a to­
tal

of 8420 URLs, roughly 400 of which correspond to
404 errors for invalid URLs. The average length of a
session trace was 4.007 votes, with a median length of
2, and the longest trace was 93 votes.
second

data

set

uses

session

traces from http://www .msnbc. com/, corresponding
to an

80%/20%

split of users on

The training data include roughly
from

22 December 1998.
1.28 million requests
data include 178158

475769 users, while the test
requests from 87714 users . The requests in these two
data sets span 1001 URLs; it is unclear whether any of
these represent invalid URLS. The average length of a
session trace was

2

experiments.

= 0.01 for all of the

In all of the data transformations described in the pre­
vious section, we created a separate binary variable for
each item that denoted whether or not the next vote
will be for that item. Defining the variables this way
can be problematic for any learning algorithm using
finite data that does not enforce the constraint that
the next vote will be for exactly one item. In particu­

http: //research. microsoft. com/.

T he

f is the number of free

parameters in the tree. We used r;,

2.696

and a longest trace of

vote, with a median length of

407 votes.

Unfortunately, we did not identify other publicly­
available data that records user preferences in time or­
der. The authors' experience with other data suggests
that the techniques outlined here may prove fruitful
with other types of sequential data.

lar, the algorithm we used to learn a forest of decision
trees did not enforce this constraint.

We solved this

problem by using the decision trees to calculate the
posterior probability that each item would be the next

vote, then renormalizing.

We applied two evaluation criteria in our experiments.
For all prediction algorithms, we adopted the "CF ac­
curacy" score outlined by Heckerman et a!.

(2000),

and specialized it to compute the CF score with re­
spect to the next item in the user's history only. The
CF accuracy score attempts to measure the probabil­
ity that a user will view a recommendation presented
in a ranked list with other recommendations. To ap­
proximate this probability, let p(k)

=

2-k/a

denote

the probability that the user views the kth item on
his list (where k counts from

0) .

For the experiments

presented here, we chose a half-life of

a

=

10.

We

computed for each user i, and for each vote vii in his
vote history, a ranked list of recommendations given

Vil,

·

·

·,

Vi(j-1)

·

One may compute the CF accuracy of a general list L

of test items spanning n users. Suppose the model rec­

ommends R; items to each user, and the users actually

prefer sets of M; items. Let O;k denote the indicator

586

ZIMDARS ET AL.

UAI2001

that user i prefers the kth recommendation. Then
accuracy CF (L)

=

n

1
- """"
n �
i

'"'R-lJ.

L.. k-o

•kP (k)

'"'M;-1 p (k)

=l L.. k=O

(2)

Let kii be the ranking assigned by our model to vote

Vij. Scoring one vote at a time, CF accuracy simplifies

to

(3)
Baseline

2 Bins

4

Bins

DE·1

DE·3

DE·S

One may compute CF accuracy for any CF algorithm
that generates a ranked list of recommendations, but

Figure

it provides a criterion specific to the collaborative fil­

constructed for the MSNBC domain.

1:

Collaborative filtering scores of the models

tering task. For the probability models we evaluated,
we also computed the mean log-probability assigned
to each of the user's actual votes, given the preced­
ing vote history. (This log-probability was normalized
over all items in dependency-network models to com­
pensate for potential inconsistencies).

Baseline

2 Bins

4 Bins

DE·1

DE·3

DE·S

-4.89
-4.895

Note that CF accuracy is a function of the relative

magnitude of density estimates, while the log score
depends on the absolute magnitude of the estimates.

5

·4.885

-4.9
-4.905
-4.91

Results

-4.915

The results presented below correspond to three fam­
ilies of models.

The "Baseline" results derive from

a forest of decision trees trained on bag-of-votes data,
shown to be a one of the best models for CF (Breese et
al., 1998).

"2

Bins" and "4 Bins" experiments applied

the binning method described in section

2.2.

Two or

four decision trees are constructed for each Web page,

Figure

2:

Log-probability scores of the models con­

structed for the MSNBC domain.

increase as a function of history length.

This might

suggest that Web page requests depend more strongly

but only one is chosen (according to the partial his­

on immediate links than on the short-term history, and

tory at hand) to make a prediction. The "DE-" exper­

that data expansion mainly embodies this structural

iments expand data as in section 2.3, with histories of

element of the Web surfing domain. (One should not

length 1, 3, and 5.

interpret this

Figure 1 and Figure

2

scores,

for all of the models in the

respectively,

show the CF scores and log

MSNBC domain.

as

a Markov assumption; in our expe­

rience, the cache variables strongly influence predic­
tion.)

The higher CF accuracy results suggest that

the relative magnitude of density estimates is more of­

ten accurate for data-expanded models than binned

There are some interesting observations to make about

models, and these relative estimates determine which

these results. F irst, we see that for the collaborative­

pages show up in a recommendation list.

filtering score, the score got worse as we increased the

number of bins. This may be an artifact of the sparsity

of long traces in Web surfing data, a phenomenon that
has been observed elsewhere (e.g., Huberman et al.,
1998). This may not impair work in other domains;

our experience with data suggests that other frequency
functions for user history length can have thicker tails.

Our results show that unlike for the CF score, the bin­
ning approach dominated both the baseline and the
data-expansion models for log-probability predictive
accuracy.

For this score, the data-expansion models

improved as the history length increased, but only the
model with the longest history (five) was competitive
with the baseline model.

We suspect that the data

Second, we see that all of the data-expansion models

were too sparse to permit accurate parameter esti­

performed significantly better than the baseline with

mates for the models learned under data expansion.

respect to CF accuracy, but that performance did not

In particular, there were roughly 50 percent more pa-

ZIMDARS ET AL.

UA12001

587

rameters to train in each of the data-expansion models

the log score. However, binning models do not indicate

than in the other models, which leads us to suspect

a steep fall-off in CF accuracy relative to the baseline,

that the learning algorithm over-fit for these models

as for the MSNBC data set. We hypothesize that typi­

to some degree. In retrospect, we regret the choice of

cal MSR visitors leave longer page traces than MSNBC

a single value of the model-prior parameter "' for all

users.

data transformations. We expect that if we had tuned
this parameter by splitting up the training data and
maximizing a hold-out prediction accuracy, we would
have identified a smaller "'for the data-expansion mod­
els that yielded better results for both criteria on the
tests set. Improvements in log score as history length
increase demonstrate the value of the additional in­
formation encoded by the expanded data, which com­
pensates in part for having too few data points per
parameter.

6

Conclusion

We have presented two techniques for transforming
data that allow the collaborative filtering problem to
be treated as a time-series prediction task.

Both of

these techniques allow state-of-the-art collaborative
filtering methods to model a richer representation of
data when vote sequence information is available. We
have evaluated these techniques, using probabilistic

Figure 3 and Figure

4

show the CF scores and log

scores, respectively, for all of the models in the MSR
domain.

decision-tree models, with two data sets for which the

order of user votes were known. Results indicate mixed
gains for each approach. Binning user data by history
length improved log-probability scores with respect to

0.6

a bag-of-votes model in our test cases, while data ex­
pansion to introduce history variables improved the

0.5

collaborative filtering accuracy score over baseline.

0.4



We begin by discussing causal independence
models and generalize these models to causal
interaction models. Causal interaction models are models that have independent mechanisms where mechanisms can have several
causes. In addition to introducing several
particular types of causal interaction models, we show how we can apply the Bayesian
approach to learning causal interaction models obtaining approximate posterior distributions for the models and obtain MAP and
ML estimates for the parameters. We illustrate the approach with a simulation study
of learning model posteriors.

1 Introduction
Models of causal independence1 such as the Noisy-or
(Good, 1961 Kim and Pearl, 1983) and Noisy-Max
(Henrion, 1987) have proved to be useful for probabilistic assessment (Pearl, 1988 Henrion, 1987 Heckerman and Breese, 1996). In addition to easier assessment, there are techniques for performing inference eciently in models with causal independence
(e.g., Heckerman and Breese, 1996 Zhang and Poole,
1996) and techniques to eciently calculate upper and
lower bounds for likelihoods where exact inference is
intractable (Jaakkola and Jordan, 1996). The essential idea of causal independence models is that the
causes lead to the eect through independent mechanisms. If this type of model is assumed then one
only needs to separately assess the probability distributions that describes a mechanism and give a rule
for combining the results of the mechanisms. On the
other hand, when using full probability tables to represent the conditional distribution of the eect given its
1 Causal independence is sometimes referred to as intercausal independence.

causes, we are essentially allowing for complete causal
interactions between the causes.
The rst part of this paper introduces causal interaction models. Like the causal independence model, a
causal interaction model is a set of mechanisms, a set
of causes, and an eect. Unlike the causal independence model, a cause need not be associated with a
single mechanism and multiple causes can be associated with a single mechanism. Allowing several causes
to be associated with a single mechanism allows for
partial causal interaction between a set of causes, thus,
causal interaction models generalize both the causal
independence model and the complete causal interaction model. In Section 2, we show how to represent
causal interaction models as directed acyclic graphical (DAG) models (a.k.a., Bayesian networks, belief
networks, causal networks) with hidden variables. In
addition we introduce a special type of causal interaction model, the exponential causal interaction model.
Examples of exponential causal interaction models are
given in Section 3.
In the second part of the paper we turn our attention
from representation to learning the structure and parameters of exponential causal interaction models. In
much of the initial work on learning (discrete) DAG
models, the focus was on learning the structure of the
network assuming there were full conditional probability tables for each variable in the network. The conditional probability table for a variable represented
the conditional probability of the variable given every possible combination of the values of its parents
in the DAG model structure. In this representation,
the number of parameters associated with a variable is
exponential in the number of parents of the variable.
This exponential explosion can restrict the set of network structures that can be learned by some methods
(e.g., MDL methods, Boukaert 1995). In part, because
of these limitations, there has been interest in learning
DAG models with more parsimonious representations
for the conditional probability of variables given their

parents. For instance in Friedman and Goldszmidt
(1996) and Chickering et al. (1997), the authors consider using decision trees and a generalization of decision trees to represent the conditional probability of
the variable given its parents. These representations
of local structure allows for dramatic reductions in the
dimension of the parameter space. Causal interaction
models provide an alternative representation for the
local structure in a DAG model. We illustrate the fact
that there are Noisy-Max-Interaction models that can
not be parsimoniously represented by decision trees
and that decision trees and other types of local structures can be embedded in causal interaction models.
Thus, causal interaction models are rich set of models
for parsimoniously representing local structure.
Since causal interaction models are DAG models with
hidden variables and hidden variables are just the extreme case of missing data we discuss learning DAG
models with missing data in Section 4. We also discuss
how one can use the EM algorithm to obtain ML and
MAP estimates for hidden variable models. Finally,
in Section 5, we illustrate the fact that one can learn
the structure of causal interaction models in a small
simulation study. In addition, Section 5 illustrates the
importance of correctly calculating the dimension of
hidden variable models when learning structure.

2 Causal Independence and Causal
Interaction models
When constructing a parameterized DAG models, one
must specify the conditional probability of each variable given each possible conguration of the parents.
Figure 1a shows a variable E with several parents
(causes). It is often not feasible to specify a complete
probability table to represent the required probabilities, because the number of probabilities grows exponentially in the number of parents. In addition, several
authors have argued that this model is inaccurate because it fails to represent the independence of causal
interactions.
To overcome both of these inadequacies, researchers
have used DAG models such as the one shown in Figure 1b to represent causal independence (e.g., Good,
1961 Kim and Pearl, 1983 Henrion, 1987 Srinivas,
1992). We shall call the Ci's the causes, E the effect, and the Xi 's the noisy mechanism variables. The
(noisy) mechanism variable Xi represents the contribution of the ith mechanism to the eect E where the
value of E is a deterministic function (indicated by
the double circle in the graph) of the values of the
mechanism variables. The independence of the causal
mechanisms is captured by (1) the conditional independence of the mechanism variables given the causes,

C1

C2

C1

C2

Cn

X1

X2

Xn

C1

C2

Cn

Cn
X1

Xm

E
E

(a) belief network for
multiple causes

(b) causal independence
model

E

(c) a causal interaction
model

Figure 1: Dierent types of local structure.
and (2) the independence between the set of mechanism variables for E and other variables in the network
(not depicted in Figure 1b) given the causes and the
eect.
A causal interaction model relaxes the restrictions that
each cause has a unique mechanism variable and that
each mechanism variable has a unique cause. Figure 1c
shows an example of a causal interaction model. With
a causal interaction model, it is possible to model relationships in which some of the causes interact to cause
the eect and some of the causes act independently.
Example of interactions are often found in medicine.
For instance, in some studies smoking and estrogen
level have been found to have a synergistic eect on
the rate of stroke in females. There is no reason to
stop the modeling of the causal process at this level.
The ith mechanism described by the conditional distribution of Xi given the parent of Xi could be modeled
as a decision tree, or a model with additional hidden
variables.
Roughly, a mechanism describes one \path" through
which a set of causes lead to an eect. A mechanism
for causes C1 : : : Cn and eect E are a set of nodes
M which are not observed (hidden) such that (1) there
is a distinguished variable called the noisy mechanism
variable (or, simply, the mechanism variable), (2) only
members of the mechanism M and causes can point
to members of M, (3) the nodes in M form a directed
acyclic graph, (4) the only variable in M that points to
a non-member of M is the mechanism variable which
only points to E. The Figure 2b illustrates and example of a mechanism. Note that a cause can point to
multiple nodes in a mechanism.
A causal interaction model is roughly a DAG model
of mechanisms which describes the conditional distribution of the eect given its causes. More precisely,
a causal interaction model is a (1) a set of causes
C1 : : : Cn, (2) an eect variable E, (3) a set of mechanisms for C1 : : : Cn and eect E, which we denote by
M1 : : : Mm , (4) where the value of the eect variable
is a deterministic function of the mechanism variables
X1 : : : Xm , which we call the combination function.

C1

C2

Ck

Cn

Cl

E

(a) general causal
interaction model

X i,2

Xi

mechanism m

mechanism 1

mechanism 1

mechanism m

X i,1

E

(b) example mechanism

Figure 2: Causal interaction models.
Let M be the set of all of the variables in mechanisms
for causes C1 : : : Cn and eect E. As in the case of
causal independence models, the independence of the
causal mechanisms is captured by (1) the conditional
independence of the set of variables in each mechanism
given the causes (i.e., for i 6= j, Mi is independent of
Mj given C1 : : : Cn), and (2) the independence between the set of all mechanism variables (M) and other
variables in the DAG model given the causes Ci and
eect E.
It is common to add a leak term to the noisy-or and
noisy-max models. A leak term is added to model
mechanisms not associated with other variables in the
model. A leak term corresponds to a mechanism variable (and thus a mechanism) which does not have any
causes that are in the DAG model.
Finally, an exponential causal interaction model is a
causal interaction model in which the conditional likelihood for each variable in each mechanism is in the
exponential family. In Section 3, we discuss a variety of specic exponential causal interaction models.
We focus on exponential causal interaction models because with these models we can often nd tractable
algorithms for inference and with tractable models for
inference we can apply the EM algorithm.

3 Examples of Exponential Causal
Interaction models
In this section we give a few examples of exponential
causal interaction models.

3.1 Noisy-Max-Interaction models
A noisy-max-interaction (NMI) model is a causal interaction model in which, (1) each mechanism consists of
a single mechanism variable which has a domain that
is a subset of the domain of the eect variable, (2) the

domain of the eect variable can be ordered by a binary relation , (3) the likelihood of each mechanism
variable given the values of its parents is in the exponential family, and (4) the combination function is
max (x1 : : : xm ). Note that the eect and the mechanism variables need not be discrete. It follows from
the combination function that
m
Y
p(E  ejC~ = ~c ) = p(Xi  ejC~ = ~c ):
i=1

An NMI model in which there is only one cause per
mechanism variable is a generalization of the Noisyor and Noisy-max models. These NMI models are
noisy-max models without a distinguished state (e.g.,
\absent" or \normal"). Of course, one can create a
Noisy-Max-Interaction model with distinguished states

by simply distinguishing one parent conguration for
each mechanism variable and forcing the associated
parameters to 0 and 1. Clearly, when one xes parameters one is reducing the number of free parameters
in the model. One benet of models without distinguished states is that they can be easier to learn. In
the case where one does not know the distinguished
states for each of the mechanism variables, we have an
additional learning problem namely we need to identify which parent congurations are the distinguished
states. Of course, if we do know which parent conguration is the distinguished state then we can force the
parameter restrictions and use the EM algorithm to
calculate the ML or MAP estimate of the parameters
and approximate the posteriors on the models.
As a special case we consider a discrete NMI model, an
NMI in which (1) E is a discrete random variable (not
necessarily nite), and (2) each mechanism contains
only a mechanism variable. Let ijk = p(Xi = kjC~ =
~c ) = p(Xi = kjPaXi = j ). Where PaXi is the
set
of Xi . Thus p(Xi  xi jC~ = ~c ) =
P of parents
k xi ijk . Let ji be the instantiation of causes for the
ith mechanism variable. As discussed in Section 4.1, to
use the EM algorithm we will need to calculate p(Xi =
k PaXi = j jC~ = ~c E = e ) = I(j = ji )p(Xi =
kjC~ = ~c E = e ), where I(j = ji ) is an indicator
function that is one if and only if j = ji .
p(Xi
8 0 = kjC~ = ~c E = e ) =
>< Q
Q

iji k ( i6=j p(Xi ejC~ =~c ); i6=j p(Xi <ejC~ =~c ))
n p(X ejC~ =~c ); n p(X <ejC~ =~c )
i
i
i=1
i=1

Q
>: QP
1 ; l<k p(Xi = ljC~ = ~c E = e )

k>e
k<e
k=e

Note that for each mechanism variable we only need
to calculate p(Xi jC~ = ~c ). If the conditional distribution is in the exponential family then it is easy to

apply the EM algorithm, e.g., if the conditional distribution p(Xi jC~ = ~c ) is distributed according to a
Poisson or multinomial distribution. In addition, we
do not need to have a unique conditional distribution
for each instantiation of the parents of the mechanism
variable. Rather, one can use a decision tree or a decision graph to reduce the number of conditional distributions and thus reduce the number of parameters
needed for specifying the conditional distribution of
the mechanism variable. This can even be done when
the conditional distribution function is the Poisson distribution. Since the conditional distribution of a mechanism variable can be represented with a decision tree,
the NMI model is at least as representationally rich as
decision trees.
A noisy-or model (a special case of an NMI model)
with n binary causes and a binary eect has n parameters. However, for almost all values of the parameters (all but a set of Lebesgue measure zero) a full
probability table, i.e., a complete decision tree, must
be used to represent the distribution exactly. Thus,
causal interaction models provide a rich representation for modeling conditional distributions. Causal interaction models can be viewed as an alternative to
decision trees or decision graphs for parsimonious local representations, however, since decision trees and
graphs can be embedded in causal interaction models,
they are strictly richer representation. The caveat, as
we shall see in Section 4, is that one must use iterative methods in approximating several quantities of
interest when using causal interaction models. Under
suitable assumptions, this is not the case for decision
trees and decision graphs.
Finally, in Noisy-or and Noisy-Max models it is common to add a leak term to model mechanisms not associated with the other variables in the model. As
discussed in Section 2, we can add leaks to NMI models, however, the extra degrees of freedom in a NMI
model as compared to a Noisy-Max can act somewhat
like a leak term in a Noisy-max model.

3.2 Noisy-Additive-Interaction models
A Noisy-Additive-interaction (NAI) model is a causal
interaction model in which, (1) each mechanism consists of a single mechanism variable which has a domain that is a subset of the domain of the eect variable, (2) the domain of the eect variable is closed
under addition, (3) the likelihood of each mechanism
variable given the values of its parents is in the exponential P
family, and (4) the combination function is
addition, mi=1 Xi .
As a special case of an NAI model in which the effect is not continuous, we consider a Poisson NAI

model. A Poisson NAI model is an NAI model in
which (1) p(Xi jPaXi = j ) = Poisson(
(ij ) ) that
x(i j)
is p(Xi = xjPaXi = j ) = exp(;(ij ) ) x! , and (2)
each mechanism contains only a mechanism variable.
 is called the rate parameter for the Poisson. In this
case (ij ) is a conditional rate parameter. Let (ij )
be the parameter for the ith mechanism variable when
the parents of the ith mechanism variable are in the j th
state. Let ji be the instantiation of parents of the ith
mechanism variable. Using the theorem that the sum
of n independent random variables having Poisson distributions with parameters 1 : : : n is distributed as
a Poisson random variable with parameter 1 +: : :+n
we can characterize the Poisson NAI
P model with the
equation p(E jC~ = ~c ) = Poisson( mi=1 (iji ) ).
Poisson random variables are useful in analyzing rates,
e.g., number of web page hits per week or number of
headaches per week. Thus, the Poisson NAI model
has potential for modeling conditional rates even in
cases where the causes of the rate can interact. As
with Noisy-Max-Interaction models, for a given mechanism variable, one need not have a unique parameter
for each instantiation of the parents of the mechanism
variable. Rather, one can use decision trees and decision graphs to reduce the number of parameters needed
for specifying the conditional distribution of the mechanism variable.
One interesting feature of the Poisson NAI model is
that it is possible to run inference using a cliquetree type inference algorithm despite the fact that the
clique potentials are innite. The trick is to form the
clique potentials only after the value of E is known.
With the value of E known we can bound the values
of the Xi 's and thus bound the size of the clique potential.
Let ji be the instantiation of causes for the ith mechanism variable. As discussed in Section 4.1, to use
the EM algorithm we will need to calculate p(Xi =
k PaXi = j jC~ = ~c E = e ) = I(j = ji )p(Xi =
kjC~ = ~c E = e ), where I(j = ji ) is an indicator
function that is one if and only if j = ji . Below is
the equation for p(X1 = kjC~ = ~c E = e ) where
there are m mechanism variables. Let ijk = p(Xi =
kjPaXi = j ). The inferences for other mechanism
variables are analogous.
p(X1 = kjC~ = ~c E = e ) =
8> 0
k>e
P
<

Pe;k  Pe;k; i li Qn oj l )
>: Pe l2 =0Pe;lkm;=0Pi li Qn o=2 i i k  e
l =0  lm =0
o=1 oji li
iji k (

1

A case where inference is even easier are Gaussian NAI

C1

C2

X1

C3

C4

X2

C5

C6

X3

E1

(a) Model A

C1

C2

X4

X5

C3

X6

C4

X7

X8

E2

E3

X1

X2

C5

C6

X9

X3

E1

(b) Expanded version
of Model A

Figure 3: Interaction model with nested structure and
conditional clique tree.
models. A Gaussian NAI model is a causal interaction
model in which the conditional distribution of each of
the mechanism variables is Gaussian. By including a
discrete nite state hidden variable inside a mechanism it is possible to have conditional distribution for
the mechanism variables which are mixtures of Gaussians. In other types of NAI models, e.g., where some
of the conditional distribution are Gaussian and some
Poisson, inference is more dicult.

3.3 Other models
Both the NMI and NAI models have fairly simple
structure. Figure 3b illustrates a causal interaction
model with a more complicated nested structure. As
with any causal interaction model, there is a layer of
mechanism nodes followed by a deterministic combination function. The expanded version of Model A
in Figure 3b illustrates that the conditional distribution of the mechanism nodes given its parent causes
can have nested structure. In this case, the mechanisms associated mechanism variables X1 and X2 have
nested causal interaction models and the mechanism
associated with mechanism variable X3 has a nested
hidden variable X9 . It is important to note that when
the values of E and the Ci's are observed all of hidden variables in the interaction model are d-separated
from other variables in the model, i.e., variables not in
the interaction model, and thus inference for EM can
be localized to the interaction model.
One might think that inference and thus using EM
would be computationally hopeless in the expanded
version of Model A in Figure 3b or more complicated
causal interaction models. This is not always the case.
For the expanded version of Model A the interaction
structure conditional on the Ci's forms a polytree.
Thus the polynomial-time algorithm of Kim and Pearl

(1983) can be used for inference. More generally, the
independence of the mechanisms in a causal interaction model lead to computational eciencies in inference because, in the clique tree conditional on the Ci 's,
the nodes from dierent mechanism are only connected
by paths through mechanism variables. This point is
illustrated by the conditional clique tree in Figure 3b.
In addition to allowing for nested structure, causal interaction models also allow for other types of combination functions. For instance, an \N-of" combination
function is the combination function for a binary eect
variable which is equal to 1 if and only if N or more binary mechanism variables are equal to 1. Clearly this
can be generalized to handle continuous variables. By
using such an additive threshold combination function
one can capture threshholding eects in a causal interaction model. Another combination function is the
X-or or parity combination function in which the binary eect variable is equal to 1 if and only if an even
number of the binary mechanism variables are equal
to 1. Causal interaction models with this combination
function where the causes are jointly independent lead
to a parameterized version of the pseudo-independence
model of Xiang et al. (1996).

4 Learning the Structure and
Parameters
In this section, we investigate how to learn the parameters and the structure for exponential causal interaction models. In Section 4.1, we show how to use
the EM algorithm (Dempster et al., 1977) to compute
the ML and MAP estimate of the parameters. In Section 4.2, we investigate asymptotic approximations of
the marginal likelihood, in particular, the CheesemanStutz approximation (1995).

4.1 Learning Parameters
We can write the causal interaction model as a DAG
model. In particular, this means that we assume that
the true (or physical) joint probability distribution for
the set of variables X = fX1 : : : Xn g in the DAG
model can be encoded in some DAG model S. In this
section, X1 : : : Xn are all of the variables in the model
an not just the mechanism variables of a causal interaction model. We write
p(xj s S) =

Yn

i=1

p(xi jpai i S)

(1)

where i is the vector of parameters for the distribution p(xi jpai i S), s is the vector of parameters
( 1 : : : n ). In addition, we assume that we have a
random sample D = fx1 : : : xN g from the true joint

probability distribution of X. We refer to an element
xl of D as a case. Finally, we have a prior probability density function p( s jS) over the parameters of the
DAG model. The problem of learning probabilities in
a Bayesian network can now be stated simply: Given a
random sample D, compute the posterior distribution
p( sjD S).
We refer to the conditional distribution p(xi jpai i S)
as a local (conditional) distribution function. In this
section, we illustrate the use of the EM algorithm in
the case where each local distribution function is collection of multinomial distributions, one distribution
for each conguration of Pai . Namely, we assume
p(xki jpaji i S) = ijk > 0
(2)
Q
where pa1i : : : paqi i (qi = Xi 2Pai ri ) denote
the
congurations of Pai, and i = ((ijk )rki=2 )qji=1 are
the P
parameters. (The parameter ij 1 is given by
1 ; rki=2 ijk .) For convenience, we dene the vector of parameters
ij = (ij 2 : : : ijri )
for all i and j. We assume that each vector ij has the
prior distribution Dir( ij jij 1 : : : ijri ). Nijk is the
number of cases in D in which Xi = xki and Pai = paji .
Dene ~s to be the conguration of s that maximizes
g( s ).
g( s )  log(p(Dj s S)  p( sjS))
(3)
This conguration also maximizes p( s jD S), and is
known as the maximum a posteriori (MAP) conguration of s . Also dene ^s to be the conguration of
s that maximizes p(Dj s S). This conguration is
known as the maximum likelihood (ML) conguration
of s .
In the case of causal interaction models, we need to
compute the posterior given incomplete data. Unlike
the complete-data case, we need to use approximation
techniques. For more details see, for instance, Heckerman (1995). These techniques include Monte Carlo
approaches such as Gibbs sampling and importance
sampling (Neal, 1993 Madigan and Raftery, 1994),
asymptotic approximations (Kass et al., 1988), and
sequential updating methods (Spiegelhalter and Lauritzen, 1990 Cowell et al., 1995).
The asymptotic approximations are based on the observation that, as the number of cases increases, the
posterior on the parameters will be distributed according to a multivariate-Gaussian distribution. As we
continue to get more cases the Gaussian peak will become sharper, tending to a delta function at the MAP
conguration ~s . In this limit, we can use the MAP
conguration to approximate the distribution.

A further approximation is based on the observation
that, as the sample size increases, the eect of the prior
p( sjS) diminishes. Thus, we can approximate ~s by
the maximum maximum likelihood (ML) conguration
of s .
One class of techniques for nding a ML or MAP is
gradient-based optimization. For example, we can
use gradient ascent, where we follow the derivatives
of g( s ) or the likelihood p(Dj s S) to a local maximum. Russell et al. (1995) and Thiesson (1995) show
how to compute the derivatives of the likelihood for a
Bayesian network with unrestricted multinomial distributions. Buntine (1994) discusses the more general case where the likelihood function comes from the
exponential family. Of course, these gradient-based
methods nd only local maxima.
Another technique for nding a local ML or MAP
is the expectation{maximization (EM) algorithm
(Dempster et al., 1977). To nd a local MAP or ML,
we begin by assigning a conguration to s somehow
(e.g., at random). Next, we compute the expected sufcient statistics for a complete data set, where expectation is taken with respect to the joint distribution
for X conditioned on the assigned conguration of s
and the known data D. In our discrete example, we
compute
Ep(xjD s S ) (Nijk ) =

N
X
l=1

p(xki paji jyl s S)

(4)

where yl is the possibly incomplete lth case in D.
When Xi and all the variables in Pai are observed in
case xl , the term for this case requires a trivial computation: it is either zero or one. Otherwise, we can use
any Bayesian network inference algorithm to evaluate
the term. This computation is called the expectation
step of the EM algorithm.
Next, we use the expected sucient statistics as if they
were actual sucient statistics from a complete random sample Dc . If we are doing an ML calculation,
then we determine the conguration of s that maximize p(Dc j s S). In our discrete example, we have
E
(N )
ijk = Pri p(ExjD s S ) ijk
k=1 p(xjD s S )(Nijk )
If we are doing a MAP calculation, then we determine
the conguration of s that maximizes p( s jDc S). In
our discrete example, we have2
 + Ep(xjD s S ) (Nijk )
ijk = Pri ijk
k=1 (ijk + Ep(xjD s S )(Nijk ))
2 The MAP conguration ~ depends on the coordinate
s
system in which the parameter variables are expressed.

This assignment is called the maximization step of the
EM algorithm. Dempster et al. (1977) showed that,
under certain regularity conditions, iteration of the expectation and maximization steps will converge to a
local maximum. The EM algorithm is typically applied when sucient statistics exist (i.e., when local
distribution functions are in the exponential family),
although generalizations of the EM have been used for
more complicated local distributions (see, e.g., Saul
et al., 1996).

4.2 Learning Structure
A key step in the Bayesian approach to learning graphical models is the computation of the marginal likelihood of a data set given a model p(DjS). Given a complete data set|that is a data set in which each sample
contains observations for every variable in the model,
the marginal likelihood can be computed exactly and
eciently under certain assumptions (Cooper and Herskovits, 1992). In contrast, when observations are
missing, including situations where some variables are
hidden or never observed, the exact determination of
the marginal likelihood is typically intractable. Consequently, we will use approximation techniques for computing the marginal likelihood of exponential causal
interaction models.
In this section, we focus attentions on an asymptotic
approximation called the Cheeseman-Stutz approximation, which use in the simulation study described in
Section 5. It was chosen for the simulation study because of its computational and performance features.
See Chickering and Heckerman (1996) for a discussion
of other approximations and experimental results.
When computing most asymptotic approximations, we
must determine the dimension of each of the model.
The dimension of a model can be interpreted in two
equivalent ways. First, it is the number of free parameters needed to represent the parameter space near
the maximum likelihood value. Second, it is the rank
of the Jacobian matrix of the transformation between
the parameters of the network and the parameters of
the observable (non-hidden) variables. In either case,
the dimension depends on the value of ^s space. In our
simulation study we use a mathematicalsoftware package to calculate the rank of the Jacobian matrix of the
transformation between the parameters of the network
and the parameters of the observable variables. For
more details and motivation see Geiger et al. (1996).
Now we turn our attention the the Cheeseman-Stutz
approximation (1995). Recall that in the EM algorithm we treat expected sucient statistics as if they
are actual sucient statistics. This use suggests an

approximation for the marginal likelihood:
log p(DjS)  logp(D0 jS)
(5)
where D0 is an imaginary data set that is consistent
with the expected sucient statistics computed using
an E step at a local ML value for s .
Equation 5 has two desirable properties. One, because
it computes a marginal likelihood, it punishes model
complexity. Two, because D0 is a complete (albeit
imaginary) data set, the computation of the criterion
is ecient.
One problem with this scoring criterion is that it may
not be asymptotically correct. Consider the asymptotically correct, Bayesian Information Criterion (BIC)
(Schwarz, 1978 Haughton, 1988)
0
log p(D0 jS) = logp(D0 j ^s S) ; d2 logN + O(1)
where d0 is the dimension of the model S given data
D0 in the region around ^s|that is, the number of
parameters of S. As N increases, the dierence between p(Dj ^s S) and p(D0 j ^s S) may increase. Also,
as we have discussed, it may be that d0 > d. In either
case, Equation 5 will not be asymptotically correct.
A simple modication to Equation 5 addresses these
problems:
log p(DjS)  log p(D0 jS)
(6)
0
d
; log p(D0 j ^s S) + 2 log N
+ log p(Dj ^s S) ; 2d log N
Equation 5 (without the correction to dimension) was
rst proposed by Cheeseman and Stutz (1995) as
a scoring criterion for AutoClass, an algorithm for
data clustering. We shall refer to Equation 6 as the
Cheeseman-Stutz scoring criterion. We note that the
scoring crireria given in Equation 5 and Equation 6
can be applied if one can compute the marginal likelihood of complete data given the model and obtain
a MAP estimate. Buntine (1994) shows how to compute the marginal likelihood for complete data given a
DAG model in which the local likelihoods are from the
exponential family and we will use the EM algorithm
to obtain a MAP estimate.

5 Simulation Study
In this section we describe a small simulation study
which highlights some of the important features of the
approach that we described in Section 4. The structure of the ve models that we used in the simulation
study are given in Figure 4. All of the variables are

C1

C2

C3

X1

X2

X3

C1

C2

X1

C3

X2

C1

C2

X1

C3

X2

C1

C2

C3

X1

X2

X3

C1

C2

C3

X1

E

E

E

E

E

F1

F2

F3

F4

F5

Model
F1 F2 F3 F4 F5
Dimension
7 7 9 10 11
Unadjusted dim. 9 9 11 15 11
Table 1: The dimension of the NMI models.

Figure 4: Noisy-Max-Interaction models used in simulation study.

model F4 over model F5 when F4 is the generating
model. Using the correct penalty for the dimension is
also important for other approaches such as MDL.

binary and the conditional distributions of the Xi 's
given the Ci's are complete probability tables. Model
F5 could also be represented without a deterministic
combination function as a complete probability table
of E given the Ci 's.
For each model we chose parameter values for the parameters and then used the parameterized model as
the generating model to generate a dataset of 6400
cases. Parameter values were chosen by hand, however, similar results would be expected for parameters
chosen at random. We approximated the model posteriors using the adjusted Cheeseman-Stutz score for
dierent sized initial segments of the 6400 cases. The
dimension of the models is calculated using Mathematica and the techniques described by Geiger et al.
(1996). Although not done for our study, it is easy to
automatically generate the equations for Mathematica to calculate the dimension and thus automate the
calculation of dimension. The results of the simulation study are summarized in Figure 5. Model posteriors are presented only for initial segments of size 100,
200, 400, 800, and 1600 cases. Not surprisingly, mass
continues to accumulate on the generating model as
the sample size increases. The one exception is when
model F1 is the generating model. The reason for the
behavior of the posterior when F1 is the generating
model is that the set of distributions that can be parameterized by F1 is a strict subset of the distributions
that can be parameterized by F2 and, surprisingly, the
dimension of the two models is identical. This unusual
relationship between F1 and F2 only occurs only when
the Ci 's and E are binary.
Finally, we would like to draw attention to the importance of using the correct dimension when calculating
the Bayesian approximation to the posterior. The unadjusted dimension of a DAG model is the number of
parameters in the model, including the parameters for
the hidden variables. Table 1 describes the dimension
of each of the models used in the simulation study.
Consider models F4 and F5. Clearly every distribution over the Ci 's and E that can be represented in
F4 can be represented in F5. If our asymptotic approximation used the unadjusted dimension then, at
least asymptotically, it would be impossible to choose

6 Related and Future Work
There has be little work done on parameter learning
for causal interaction models. The notable exception
is the work of Neal (1992). Neal showed that one could
learn the parameters of a noisy-or network using a local
learning rule. However, his particular gradient-ascent
procedure must be constrained to avoid entering an
invalid region of the parameter space. Since we are
using EM we are guaranteed to stay within the valid
region of the parameter space and guaranteed to nd
a local maximum.
We plan on investigating the representational power of
causal interaction models as compared to other local
structures, e.g., decision graphs and compare the ease
of assessment for various models In addition, we will
consider automating the learning of causal interaction
models (i.e., dening a search space, and search operators), and compare the result of such an algorithm
to other approaches for learning local structure. Also
of interest, is how to best combine a search for local
structure with a search for global structure.

Acknowledgments
We thank Bo Thiesson, Jack Breese, and Max Chickering for helpful discussion on this material.




W hen performing regression or classification,
we are interested in the conditional probabil­
ity distribution for an outcome or class vari­
able Y given a set of explanatory or input
variables X. We consider Bayesian models
for this task. In particular, we examine a spe­
cial class of models, which we call Bayesian
regression/classification (BRC) models, that
can be factored into independent conditional
(ylx) and input ( x ) models. These mod­
els are convenient, because the conditional
model (the portion of the full model that we
care about) can be analyzed by itself. We
examine the practice of transforming arbi­
trary Bayesian models to BRC models, and
argue that this practice is often inappropri­
ate because it ignores prior knowledge that
may be important for learning. In addition,
we examine Bayesian methods for learning
models from data. We discuss two criteria
for Bayesian model selection that are appro­
priate for repression/classification: one de­
scribed by Spiegelhalter et al. ( 1993), and an­
other by Buntine (1993). We contrast these
two criteria using the prequential framework
of Dawid (1984), and give sufficient condi­
tions under which the criteria agree.

Keywords: Bayesian networks, regression, classifica­
tion, model averaging, model selection, prequential cri­
teria
1

Introduction

Most work on learning Bayesian networks from data
has concentrated on the determination of relationships
among a set of variables. This task, which we call Joint

1
analysis ,

has applications in causal discovery and the
prediction of a set of observations. Another important
task is regression/classification: the determination of
a conditional probability distribution for an outcome
or class variable Y given a set of explanatory or input
variables X. When Y has a finite number of states we
refer to the task as classification. Otherwise we refer
to the task as regression.
In this paper, we examine parametric models for the
regression/classification task. In Section 2, we exam­
ine a special class of models, which we call Bayesian re­
gression/classification (BRC) models, that can be fac­
tored into independent conditional (ylx) and input (x)
models. These models are convenient, because the con­
ditional model (the portion of the full model that we
care about) can be analyzed alone. In Section 3, we ex­
amine the practice of transforming arbitrary Bayesian
models to BRC models, and argue that this practice is
often inappropriate because it ignores prior knowledge
that may be important for learning.
Also in this paper, we discuss Bayesian methods for
learning models from data. In Section 4, we compare
Bayesian model averaging and model selection. In Sec­
tion 5, we discuss two criteria for Bayesian model selec­
tion that are appropriate for regression/ classification:
one described by Spiegelhalter et al. (1993), and an­
other by Buntine (1993). We contrast these two crite­
ria using the prequential framework of Dawid (1984),
and give sufficient conditions under which the criteria
agree.
The terminology and notation we need is as follows.
We denote a variable by an upper-case letter (e.g.,
X, Y, X;, 8), and the state or value of a correspond­
by that same letter in lower case (e.g.,
We denote a set of variables by a bold­
face upper-case letter (e.g. , X, Y, X;). We use a cor­
responding bold-face lower-case letter (e.g., x, y, x;)
to denote an assignment of state or value to each vari-

ing variable

x, y, x;, (}).

1

This task is sometimes called density estimation.

224

Beckerman and Meek

able in a given set. VvTe say that variable set X is
in configuration x. We use p(X = xJY = y) (or
p(xly) as a shorthand) to denote the probability or
probability density that X = x g ive n Y
y. We
also use p(xjy) to denote the probability distribution
(both mass functions and density functions) for X
given Y = y. Whether p(xjy) refers to a probabil­
ity, a probability density, or a probability distribution
will be clear from context.

•

=

We use m and Bm to denote the structure and pa­
rameters of a model, respectively. When (m, B m) is a
Bayesian network for variables Z, we write the usual
factorization as

N
.
(
=
.
.
p zt'
' ZNIOrn' m) rrp(z; Jpa;, Bm' m)
i=l

(1)

Figure 1: ( a) A naive Bayes model for classification.
(b) A linear soft max regression model that has the
same conditional distribution for Y.

I(xl} is the indicator variable that is equal to 1
if and only if x; = xl. Consequently, we have

where

where Pa; are the variables corresponding to the par­
ents of Z; in m. We refer to p(z; Jpa;, B m,m) as the
local distribution function for Z;.
2

Regression/ Classification

In this section, we examine various parametric mod­
els for the task of regression/ classification. Mod­
els for this task are of two main types: conditional
models and joint models. A conditional model is
of the form p(yjx, em, m). A joint model is of the
form p(y, xJe m, m). We use a joint model for re­
gression/ classification by performing probabilistic in­
ference to obatin p(yjx, em' m).
Examples of joint models include Bayesian networks.
F igure la shows the structure of a naive Bayes model
in which the variables X are mutually independent
given Y. Suppose Y has r states y1 , ... , yr, each X;
is binary with states x} and xl, and each local distri­
bution function is a collection of multinomial distribu­
tions (one distribution for each parent configuration).
For this example, it is not difficult to derive the corre­
sponding conditional model (see, for example, Bishop,

Akx

_
=

Chapter 6). Namely, we have
p(yklx, Bm, m)

log ( l
)
P Y jx, em, m

1
_

(4)

eAix
+ '\"'�
0J=2

softmax(.Atx, ... , A x)

k

where each Akx is a linear function of I( xi), . . . , I(x�).

Models for

1995,

(b)

(a)

=

Ox

B(yk) � log ( ;jyk)
loge-+
(Y 1 ) L..,
O(x,-J Y 1)
i=l

(2)

for k = 2, . . . , r. After some algebra, Equation 2 be­
comes

(3)

This conditional model p(yix, em, m) is a type of
generalized linear model known as a linear softmax
2
regression. We can display the structure of this con­
ditional model as a Bayesian n etwork, as shown in Fig­
ure lb. In the figure, the input nodes X are shaded to
indicate that we observe them and hence do not care
about their joint distribution.
Now let us specialize our discussion to Bayesian mod­
els for regression/classification. In the Bayesian ap­
proach, we encode our uncertainty about Om and m
using probability distributions p(emlm) and p(m), re­
spectively. Thus, the Bayesian variant of a joint model
takes the form

p(y,x,em,m) =p(m) p(emjm) p(y,xJ8m,m)

(5)

We refer to this model as a Bayesian joint (BJ) model.
We define a Bayesian analogue to a conditional model
as follows. Suppose that em can be decomposed into
parameters (Bx, eYI x) such that

p(y,xJem,m) =p(xJBx,m) p(yjx,eyJx,m)

(6)

(7)
p(em Jm) = p(exJm) p(8yJxlm)
In this case, given data D = ((Yt,xt), ... ,(yN,XN)),
assumed to be a random sample from the true distri­
bution of Y and X, we have

p(em IY, x, m)
2 Although

{p(xJ8x,m)p(8xJm)}
· { p(yjx,eyJx1 m )p(eyJxlm)}

Y has a finite number of states, this model

is commonly referred to

as a

regression.

Models & Selection Criteria for Regression

Consequently, we can analyze the marginal
conditional

(ylx)

(x)

and

225

(B', ,m')

terms independently. In particular, if

we care only about the conditional distribution, we can
analyze it on its own. We call this model defined by
Equations

6 and

(BRC) model.

7 a Bayesian regression/classification

Simple examples of BRC models in­

clude ordinary linear regression (e.g., Gelman et al.,

1995,

ChapterS), and generalized linear models (e.g.,

Bishop,

1995,

Chapter

10).

Note that our Bayesian analogue to the conditional
model is a special case of a BJ model.

One could

imagine using a Bayesian model that encodes only the
conditional likelihood
tribution for

Bm

and

p(ylx, Bm,m)
m. However,

and a joint dis­
this approach is

flawed, because it may miss important relationships
among the domain variables or their parameters that
are important for learning. In the following section,
we consider an example of this point.

3

Figure
model.
to
to

A BERC model obtained from a naive-Bayes

Y is an ancestor
Xn ,+ l , ... , Xn.

as follows:

·p(ylpay,Om,m) ·(.IT p(xdpai,fJm,m))
o:;:n,+l

where

lowing observation.

X1, .. ., Xn

p(y,xiOm,m) = (}]p(x;lpa;,Bm,m))

Y does

Pa; in the
p(ylx , Om, m) ,

not appear in any parent set

first product.
A special class of BRC models is suggested by the fol­

of each of the nodes corresponding
Given this ordering, we can factor

the joint distribution for Y,

Embedded Regression/Classification
Models

2:

Normalizing to obtain

taking a ratio, and canceling like terms, we obtain

For many BJ models, the con­

ditional likelihood p(y lx, Om, m) is a simple function

x, whereas the expression for the input likelihood
p(xiOm, m) is more complicated. For example, given a

(8)

of

naive-Bayes model in which the variables

X are

mutu­

where
=

pa7

1, .. . , r.

ally independent given Y, the conditional likelihood is

k

lihood is a mixture distribution. Thus, assuming we

Equation

a simple generalized linear model, but the input like­

is a configuration of
(Depending on

m,

Pa.;

in which

y = yk,

some of the terms in

the sum may cancel as well.) We can trivially rewrite

8 as

are interested in the task of regression/classification,
we can imagine extracting the conditional likelihood
from a B.J model, and embedding it in a BRC model.
In particular, given a BJ model
ate a BRC model

p(ylx,Om,m).

(B:n, m' )

(Bm,m),

we can cre­

in which p(ylx,

We say that

(8:n,m')

e:n, m') =

is a Bayesian

embedded regression/ classification (BERC) model ob­
tained from

( e:n' m' ) .

Several researchers have suggested using BERC mod­
els, at least implicitly (see Bishop, 1995, Chapter

10,

and references therein). An example of a BERC model

Equation 9 shows that an BERC model is a poly­
nomial softmax regression on the indicator variables
I ( xI ) , ... , I ( Xn ) . Note that there are polynomial soft­

max regressions that cannot be obtained from any
Bayesian network.

Although BERC models are convenient, we find non­

obtained from a naive Bayes model is shown in Fig­

trivial BERC models to be problematic.

ure

lar, consider a BERC model (8:n,m') obtained from

2.

If a BERC model (8:n, m') is obtained from

a model which is itself a BERC model, we refer to

(O:n,m')

as a trivial BERC model. The BERC model

is Figure 2 is non-triv ial.
For any Bayesian network with finite-state variables,
it is not difficult to obtain its corresponding BERC
model. Let X1, ... , Xn,, Y, Xn,+l, . . . , Xn be a total

m,
Y appears as late as possible in the ordering.

a non-BERC model

(Om, m).

W hereas in the BERC

model, observations of X are necessarily uninformative
about Bylx• such observations may be informative in
the original model

(Om, m). Thus, in

constructing the

BERC model, we may be ignoring parts of our prior
knowledge that are important for learning.

ordering on the variables that is consistent with

To illustrate this point,

such that

model for binary variables Y,

The latter condition says that the node corresponding

In particu­

ping from

Bm

to

Bx

consider the naive-Bayes

XI' x2, x3.

is shown in F igure 3.

The map­
It is not

226

Heckerman and Meek

B(y1) B(xUy1) B(x�IY1) B(x1IY1) + ll(y2) B(x)ly2) B(x�ly2) B(x1lv2)
B(y1) B(xijy1) B(x�IY1)(1- B(x1lv1)) + B(y2) B(xijy2) B(x�ly2)(1- B(x1IY2))
(l(xJx�x1)

B(y1) B(x)ly1) B(x�ly1) B(x1IY1) + B(y2) B(x)IY2) B(x�IY2) B(x1lv2)

B(xix�x�)

B(y1) B(x)ly1)(1- B(x1IY1))(1- B(x1IY1)) + B(y2) B(xJiy2)(1- (l(x�ly2))(1- B(x1IY2))

B(xix1x1)

B(y1)(1- B(x)ly1)) (l(x1IY1) B(x1IY1) + B(y2)(l- B(xJiy2)) B(x�ly2) B(x1IY2)

B(xrx�x�)

B(y1 )(1- B(xJ IY1)) B(x�ly1)(1- B(x1IY1)) + B(y2)(l- B(xj IY2)) B(x�ly2)(1- B(x1IY2))

O(xrx�x1)

B(y1 )(1- B(xj IY1)) B(x�IY1) B(x1IY1) + ll(y2)(1- B(xJiy2)) B(x�ly2) B(x1IY2)
Bm to Bx for the naive-Bayes model where Y renders X 1, X , and X3 mutually in­
2 and p ; , Bm, m),
(l(y), O(xl, xz, xs), and B(x ;ly) to denote p(yiBm, m), p(x , xz, x31Bm, m),
(x ly
1

Figure 3: The mapping from

dependent. We use
respectively.

difficult to show that the rank of the Jacobian ma­

observations of X often will influence the estimate of

Bm) for almost all values of
8m (see, e.g., Geiger et al., 1996). It follows that, for
almost every point e:n in Bm, there is an inverse map­
ping from Bx to Bm in a neighborhood around e:n .3
Consequently, the possible values that Bm (and hence
Byjx) can assume will depend on the value of Bx, and
observations of X will inform By]x through Bx.

ferred to as regression/classification models-should

trix

8f}x/8Bm is full

(i.e., equal to the number of non­

redundant parameters in

More generally, conditional models-often re­

By]x ·

not be used without consideration of variational de­
pendencies that may arise from the joint model.

Learning Regression/Classification

4

Models: Averaging Versus Selection

In general, given two variables (random or otherwise)

Now that we have examined several classes of models

A and B, if the possible values that can be assumed
by A depend on the value of B, then A is said to be
variationally dependent on B. In our example, 0y]x

for the regression/classification task, let us concentrate

is variationally dependent on

8x.

Such variational

dependence is not limited to this example. For any
model

(Bm, m),

if the rank of the Jacobian matrix

Bm to Bx is full, then E>m (and
0yjx) is variationally dependent on Bx. Geiger
al. (1996) conjecture that, for naive-Bayes mod­

for the mapping from

on Bayesian methods for learning such models.

First, consider model averaging. Given a random sam­

ple

D from the

most everywhere full.

and Geiger et al.

Bm

(1996)

(1974)

could identify only one naive­

Hayes model in which the Jacobian matrix was not of
full rank almost everywhere.

Bm

p(Bmlm) p(DIBm, m)
p(8 m ID ,m) =
p(Dim)

to fix is al­

In addition, Goodman

and

p(m) p(Dim)
p(miD) =
l:m' p(m') p(Dim')

els in which all variables are binary, the rank of the
Jacobian matrix for the mapping from

m

using Bayes' rule:

hence
et

true distribution of Y and X, we com­

pute the posterior distributions for each

where

Thus, the use of non­

trivial BERC models-at least those obtained from

most naive Bayes models-is suspect.

With these quantities in hand, we can determine the

Note that our remarks extend to non-Bayesian anal­
yses.

For example, in a classical analysis, a poly­

nomial softmax regression should not be substituted

for
and

a Bayesian network. In the former model, 8y]x

E>x

are variationally independent.

model, 8y]x and

E>x

In the latter

are variationally dependent, and

3The pa.rameters Om are said to be locally identifiable
given observations of X (e.g., Goodman, 1974).

conditional distribution for Y given X in the next case

to be seen by averaging over all possible model struc­
tures and their parameters:

p(ylx,D)=
p(ylx, D,m) =

L P(miD) p(ylx, D, m)
m

(10)

j p(ylx,8m, m) p(BmiD, m)dBm

(11)

Models & Selection Criteria for Regression

Note that joint analysis is handled i n

essentially

the

The term

P(Yt, XtiYl, x1, ... , Yt-1, Xt-1, m) is the pre­
(Yt, xi ) made by model structure m after

same way. For example, to determine the joint dis­

diction for

tribution of Y and X in the next case to be seen, we

averaging over its parameters (Equation

use

p(y, xJD)

p(y,xJD,m)

=

=

LP(mJD)p(y, xjD, m)
m

227

13).

The log

of this term can be thought of as the utility for this

(12)

Jp(y,xJ11m,m) p(BmJD,m)dem

(13)

Model averaging, however, is not always appropriate
for an analysis . For example, only one or a few models
may be desired for domain understanding or for fast
prediction. In these situations, we select one or a few

prediction.5 Thus, a model structure with the highest
log marginal likelihood is also a

model

structure that

is the best sequential predictor of the data
logarithmic utility function.

Let

D given the

us now consider local criteria that are more ap­

propriate for the task of regression/classification. To

keep the discussion

brief, we discuss only the

logarith­

mic utility function, although other utility functions
may

be

more reasonable for a given problem. At least

"good" model structures from among all possible mod­

two prequential criteria are reasonable. In one situa­

cedure is known as model selection when one model is

As a result, we obtain a criterion that

els, and use them as if they were exhaustive. This pro­

chosen, and selective model averaging when more than

tion, we

imagine that we see

(1993)

al.

call a

one model is chosen. Of course, model selection and
selective model averaging are also useful when it is im­

practical to average over all possible model structures.
W hen our goal is model selection, a "good" model
for joint analysis may not be a good model for re­
gression/classification, and vice versa. Scores that de­

c

N

CNM(D,m)

Llogp(y,Jxi,Y1,x1,···,Yt-1,xt-1,m)
1=1

=

(14)

In another situation, we imagine that we first see all of

the input data x1,

... , XN,

sequentially.

criteria. A criterion commonly used for joint analy­

class sequential criterwn:

logp(DJm).

=

logp(m) +

This criterion is global in the sense that it

CSC(D,m)

N

L logp(ydy1, . . . , Yl-11 x1, . . . , XN,m)
1=1

=

is equally sensitive to possible dependencies among all

variables. Criteria for regression/classification, should
be local in the sense that they concentrate on how well
X classifies Y.

In the following section, we examme

two such criteria.

The criteria that we discuss

or

terms

of Dawid's

prequential method.

To simplify

the

(1984)

can

be

understood

predictive sequential

A simple example of this

Let us consider this example

discussion, let us assume that that

i:s uniform, so that the joint-anaiysis criterion

reduces to the log-marginal-likelihood

logp(DJ m) .4

From the chain rule of probability, the log marginal
likelihood is given by

logp(DJm)

L logp(yt, XtiY1, XJ, ... ,Yl-1,Xt-1, m)
1=1

4The generalization
straightforward.

et

al.

(1993)

describe

parameter

a

set

of

independence

tion of the class sequential criterion is exponential in

the

sample size N.

to non-uniform model priors is

Monte-Carlo or asymptotic tech­

niques can be used to perform the computation for

large N (see, e.g., Heckerman,

1995).

We have applied both criteria to small Bayesian net­
works and small data sets chosen

arbitrarily.

In all

cases, we have found that the two criteria differ.
Nonetheless,

there are conditions

under which the two

criteria are the same. In particular, we can rewrite the

two

criteria as follows:

CNM(D,m)

N

=

Spiegelhalter

Under these same assumptions, the exact computa­

first.

p(m)

decision-tree structures.

monitor can be computed efficiently in closed form.

method, applied to joint analysis, yields the posterior­
probability criterion.

(15)

used this criterion for selection among

and Dirichlet priors-under with the conditional node

Regression/Classification

in

(1993)

assumptions-essentially,

Prequential Criteria for

5

Buntine

and then see the class data

Consequently, we obtain the following

sis is the logarithm of the relative posterior probabil­

logp(m, D)

( YI, Xt) sequentially.
Spiegelhalter et

on ditional node monitor:

fine "good" model structures are commonly known as

ity of the model structure

pairs

N

=

L log P (Yt,XtlYl, x1, .. . , Yt-1, Xt- 1 , m)
1=1

p(xtiY1, x1,

·

·

·

, Yl-1, Xt-1, m)
(16)

5The utility log x is also known as a scoring rule.
Bernardo (1979) shows that this scoring rule has several
desirable properties.

228

Heckerman and Meek

CSc(D,m ) =log

P(YJ, .. ,yN,XJ, ... ,xNim)
p( x1, ...,XN Im)

(17)

.

Friedman, N. and Goldszmidt, M. (1996). Building
classifiers using Bayesian networks. In Proceed­

Therefore, the two criteria will agree when

p(x1JY1, XJ, . .. , Yl-l, Xi-1, m)

=

ings AAAI-96 Thirteenth National Conference

p(xt!XJ, ... , Xi-J, m)

(18)
for l = 0, . , N 1. It is not difficult to show that
Equation 18 holds whenever (Bm, m) is a BRC model.
Thus, the two criteria agree for BRC models.
. .

theory. The prequential approach (with Discus­
sion). Journal of the Royal Statistical Society A,
147:178-292.

-

on Artificial Intelligence,

Portland, OR, pages

1277-1284. AAAI Press, Menlo Park, CA.

Geiger, D., Beckerman, D., and Meek, C. (1996).
Asymptotic model selection for directed net­
works with hidden variables. In Proceedings of
Twelth Conference on Uncertainty in Artificial

6

Intelligence,

Discussion

Several researchers have demonstrated that Bayesian
networks for both the joint analysis and regres­
sion/ classification tasks provide better predictions
when local distribution functions are encoded with
a small number of parameters, as is the case with
the use of decision trees, decision graphs, and causal­
independence models (e.g., Friedman and Goldszmidt,
1996; Chickering et a!., 1997; Meek and Heckerman,
1997). Despite our theoretical objections to the use
of BERC models, they offer another parsimonious pa­
rameterization of local distribution functions, and may
lead to better predictions in practice. For example,
polynomial softmax regressions may be useful when
a node and its parents are discrete. Experiments are
needed to investigate these possibilities.

Portland, OR. Morgan Kaufmann.

Gelman, A., Carlin, J., Stern, H., and Rubin, D.
(1995). Bayesian Data Analysis. Chapman and
HalL
Goodman, L. (1974). Exploratory latent structure
analysis using both identifiable and unidentifi­
able models. Biometrika, 61:215-231.
Beckerman, D. (1995). A tutorial on learning Bayesian
networks. Technical Report MSR-TR-95-06, Mi­
crosoft Research, Redmond, WA. Revised Jan­
uary, 1996.
Meek, C. and Beckerman, D. (1997). Structure and
parameter learning for causal independence and
causal interaction models. In Proceedings of
Thirteenth Conference on Uncertainty in Artifi­
cial Intelligence,

Acknowledgments

We thank Max Chickering for useful discussions.


We show that if a strictly positive joint prob­
ability distribution for a set of binary random
variables factors according to a tree, then ver­
tex separation represents all and only the in­
dependence relations encoded in the distribu­
tion. The same result is shown to hold also
for multivariate strictly positive normal dis­
tributions. Our proof uses a new property of
conditional independence that holds for these
two classes of probability distributions.
1

Introduction

A useful approach to multivariate statistical model­
ing is to first define the conditional independence con­
straints that are likely to hold in a domain, and then
to restrict the analysis to probability distributions
that satisfy these constraints. An increasingly pop­
ular way of specifying independence constraints are
directed and undirected graphical models where inde­
pendence constraints are encoded through the topolog­
ical properties of the corresponding graphs (Lauritzen
1982; Lauritzen and Spiegelhalter, 1988; Pearl, 1988;
Whittaker, 1990).
The key idea behind these specification schemes is to
utilize the correspondence between vertex separation
in graphs and conditional independence in probability;
each vertex represents a variable and if a set of vertices
Z blocks all the paths between two vertices, then the
corresponding two variables are asserted to be condi­
tionally independent given the variables corresponding
to Z. The success of graphical models stems in part
from the fact that vertex separation and conditional in­
dependence share key properties which render graphs
an effective language for specifying independence con­
straints.
In this paper we show that when graphical models
are trees and distributions are from specific classes,
then the relationship between vertex separation and
conditional independence is much more pronounced.
More specifically, we show that if a strictly positive
*Part of this work was done w hile
sabbatical at Microsoft research.

the

author was on

Christopher Meek
Microsoft Research
Redmond, WA, 98052, USA
meek@microsoft.com

joint probability distribution for a set of binary ran­
dom variables factors according to a tree, then vertex
separation represents all and only the independence
relations encoded in the distribution. The same result
is shown to hold also for multivariate strictly positive
normal distributions.
The class of Markov trees has been studied in several
contexts. Practical algorithms for learning Markov
trees from data have been used for pattern recogni­
tion (Chow and Liu, 1968). Geometrical properties of
families of tree-like distributions have been studied in
(Settimi and Smith, 1999). F inally, the property of
perfectness, when a graphical model represents all and
only the conditional independence facts encoded in a
distribution, is a key assumption in learning causal
relationships from observational data (Glymour and
Cooper, 1999).
2

Preliminaries

Throughout this article we use lowercase letters for sin­
gle random variables (e.g., x, y, z ) and boldfaced low­
ercase letters (e.g., x, y, z ) for specific values for these
random variables. Set of random variables are denoted
by capital letters (e.g., X, Y, Z), and their values are
denoted by boldfaced capital letters (e.g., X, Y, Z).
For example, if Z = { x, y} then Z stands for { x, y}
where x is a value of x and y is a value of y. We use
P(X) as a short hand notation for P(X =X). We say
that P(X) is strictly positive if VX P(X) > 0. We use
X y as a short hand notation for X U {y}.
Let X, Y and Z be three disjoint sets of ran­
dom variables having a joint probability distribution
P(X, Y, Z). Then, X and Y are conditionally inde­
pendent given Z, denoted by X l_p Y I Z, if and only
if
VXVYVZ P(X, Y, Z)P(Z) = P(X, Z)P(Y, Z).
When P is strictly positive an equivalent definition is
that X l_p Y I Z holds if and only if
VXVYVZ P(X IZ) = P(XIY, Z).
When P(X, Y, Z) is a strictly positive joint normal
distribution, then X and Y are conditionally indepen­
dent given Z if and only if Pxy.Z = 0 for every x EX

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

20

and y E Y where Pxy.Z is the partial correlation coef­
ficient of x an d y given Z (Cramer, 1946).
The ternary relation X ..lp Y I Z was introduced in
(Dawid, 1979) and further studied in (e.g., Spohn
1980; Pearl and Paz 1987; Pearl 1988; Geiger and
Pearl 1993; Studeny 1992) . The ternary relation
X..lp Y I Z satisfies the following five properties which
are called the graphoid axioms (Pearl and Paz, 1987).
•

•

•

•

•

•

Decomposable transitivity:
aB ..lp De I c 1\ a ..lp e I BD =>
a ..lp c I B V c ..lp e I D

(8)

Symmetry:
X..lp y I z =} y ..lp X I z

(1)

3

Decomposition:
X..lp YW I z =}X..lp y I z

(2)

Weak Union:
X..lp YW I z =}X..lp y I zw

(3)

We now prove that decomposable transitivity holds for
strictly positive joint probability distributions of bi­
nary random variables and for strictly positive normal
distributions. We then show that decomposable tran­
sitivity holds also for vertex separation in undirected
graphs.

Contraction:
X ..lp y I z 1\ X ..lp

I ZY =}
X..lp YWIZ

(4)

If P is strictly positive, then
Intersection:
X ..lp y I zw 1\ X..lp w I ZY =}
X ..lp YW I z

(5)

w

The following property holds for joint normal distri­
butions P(X,Y, Z, c) (Pearl, 1988). It also holds for
discrete random variables if Z = 0 and c is a binary
random variable.
•

The main result in this paper is a converse to Eq. 7
under suitable conditions. W hen the converse holds
we say that G is a perfect representation of P. To
facilitate our argument we must first introduce a new
property for conditional independence.

Weak Transitivity:
X ..lp Y I Z 1\ X ..lp Y I Zc =>
X ..lp c I z v c ..lp y I z

(6)

A Markov network of a probability distribution
=
P(x1, . . . ,xn) is an undirected graph G (V, E) where
V = { x1,. . . , xn} is a set of vertices, one for each ran­
dom variable x;, and E is a set of edges each repre­
sented as (x;,xj) such that (x;,xj) E E if and only
if
•X; ..lp Xj I {x1, ... , Xn } \ {x;, Xj}·
A Markov tree is a Markov network where G is a tree.
A key property of Markov networks is the following.
Let A ..lc B I C stand for the assertion that every path
in G between a vertex in A and a vertex in B passes
through a vertex in C, where A, B, and C are mu­
tually disjoint sets of vertices. Note that whenever
A ..lc B I C holds in G, A and B are (vertex) sepa­
rated by C. The ternary relation A ..lc B I C satisfies
all the properties we listed for A ..lp B I C and some
additional properties that do not hold for A ..lp B I C
(Pearl, 1988).

Theorem 1 (Pearl and Paz, 1987; Pearl, 88)
Let G be a Markov network of P(x1,... , Xn ) , and sup­
pose Intersection holds for P. Then
A ..la B I C implies A ..lp B I C
(7)
for every disjoint set of vertices A, B, and C of G and
their corresponding random variables in {x1,. . . , Xn } ·

New property of conditional
independence

Theorem 2 Let a, c, e be binary random variables, B
and D be {possibly empty) sets of binary random vari­
ables, and P(a, c, e,B,D) be a strictly-positive joint
probability distribution for these random variables.
Then
aB ..lp De I c 1\ a ..lp e I BD =>
a ..lp c I B V c ..lp e I D
holds for P.
Proof: We use a to denote a value for a, B to denote a
value for a set of variables B, and a0 and a1 to denote
the two values of a binary random variable a.
Due to aB ..lp De I c it follows that
P(a,B,c,D,e)· P(c) = P(a,B,c)· P(c, D,e) (9)
for every value a,c, e,B,D of the corresponding ran­
dom variables. Due to a ..lp e I BD it follows that
P(a0,B,D,e0) ·P(a1,B,D,e1) =
P (al, B, D, e0) ·P(a0, B, D, e1). (10)
for every value B,D of B, D. Since c is a binary vari­
able
P(a,B,D,e)= P(a,B,c0,D,e)+ P(a,B,cl,D,e)
(11)
Now, substituting Eq. 9 into Eq. 11, then substitut­
ing the result into Eq. 10, yields using some divisions,
which are allowed because P is strictly positive, that
1+ a(B),B(D) = a(B)+ ,B(D)
where

and

P(c1,D,e0) ·P(c0,D,e1)
P(c0,D,e0) ·P(c1,D,el)
Consequently, either a(B) = 1 or ,B(D) = 1. Further­
more, since B and D are arbitrary values of B and
D, respectively, we have \fB\fD [a(B) = 1 V ,B(D) = 1]
which is equivalent to [\fB a(B) = 1) V ['v'D ,B(D) = 1]
which is equivalent to a ..lp c I B V c ..lp e I D.
0
,B(D)

=

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

Theorem 3 Let a,c, and e be continuous random
variables, B and D be {possibly empty) sets of con­
tinuous random variables, and let P(a, c, e,B, D) be
a strictly positive joint normal probability distribution
for these random variables. Then,
aB l_p De I c 1\ a l_p e I BD
a l_p c I B V

=>
c

l_p e I D (12)

21

2000

on the path 11 between a and c, or that a vertex b E B
resides on the path 12 between c and e. In the first
case vertices a and d are connected and the path that
connects them does not include c, and in the second
case vertex b and e are connected and the path that
connects them does not include c. Thus, in both cases,
aB l_a De I c does not hold in G, contradicting our
�umpt�n.
0

holds for P.

Proof: We use a formal logical deduction style to em­
phasize that the only properties of normal distribu­
tions being used are the ones encoded in Symmetry,
Decomposition, Intersection, Weak union, and Weak
transitivity. Recall that weak transitivity holds for
every normal distribution and that intersection holds
for strictly positive normal distributions. The other
properties hold for every probability distribution.
We now derive the conclusion of Eq. 12 from its an­
tecedents.

1. aB l_p De I c
( Given)
2. a l_p e I BD
( Given)
3. a l_p D I cB
( W. union, Decomposition, and Symmetry on (1))
4. B l_p e I cD
( W. union, Decomposition, and Symmetry on (1))
5. a l_p e I BDc
( Weak union and Symmetry on (1))
6. a l_p c I BD V c l_p e I BD
( Weak transitivity on (2) and (5))
7. a l_p cD I B V Be l_p e I D
( Intersection and Symmetry on (3), (4) and (6))
8. a l_p c I B V c l_p e I D
( Symmetry and Decomposition on (7))
0

Theorem 4 Let a,c, and e be distinct vertices of an
undirected graph G, and let B and D be two (possibly
empty) disjoint sets of vertices of G that do not include
a, c or d. Then,
aB l_a De I c 1\ a l_a e I BD
a l_a c I B V

=>
c

l_a e I D (13)

holds for G.

Proof: Assume the conclusion of Eq. 13 does not hold
in G but its antecedents hold. Then, there exists a
path 11 in G between a and c such that no vertices
from B reside on 11, and there exists a path /2 in G
between c and e such that no vertices from D reside
on 11. If B and D are empty, then the concatenated
path /1/2 contradicts a l_a e I BD which is assumed
to hold in G. Thus, we can assume either B or D are
not empty. The concatenated path /1/2 contains a
vertex from B or D ( or both) because a l_a e I BD is
assumed to hold in G. Assume a vertex d E D resides

4

Perfect Markovian trees

We are ready to prove the main result.

Theorem 5 Let G be a Markov tree for a probabil­
ity distribution P(x1,...,xn). If x1, ...,xn are bi­
nary random variables and P is a strictly-positive joint
probability distribution, or if x1,..., Xn are continuous
random variables and P is a strictly positive joint nor­
mal distribution then, in both cases,
A

l_a B I C if and only if

A

l_p B I C

(14)

for every disjoint set of vertices A, B, and C of G and
their corresponding random variables in { x1,...,xn}.

Proof: Theorem 1 proves one direction of Eq. 14, and
so it remains to prove that
A

l_p B I C implies

A

l_a B I C

(15)

To prove Eq. 15 it is sufficient to show that
a l_p b I C implies a l_a b I C
for every pair of vertices a E A and b E B because
A l_p B I C implies VaVb a l_p b I C and VaVb a l_a
b I C is equivalent by definition to A l_a B I C.
We proceed by contradiction. Let x and y be a pair
of vertices for which there exists a set of vertices Z
satisfying
(16)
x l_p y I Z 1\ •x l_a y I Z
and such that x and y are connected with the shortest
path among all pairs x',y' for which there exists a set
Z' satisfying x' l_p y' I Z' 1\ •x' l_a y' I Z'.
Suppose first that the path between x and y is merely
an edge connecting the two vertices. We will now reach
a contradiction by showing that G cannot be a Markov
network of P. In particular, we show that P satisfies
x l_p y I Uxy where Uxy are all vertices except x and
y. Let Ux be all the vertices on the x side of the edge
(x, y) and Uy be the rest of the vertices. ( Namely, Ux
are the vertices in the component of x after removing
the edge (x,y)). Let B = Ux n Z and D = Uy n Z.
We proceed by a formal deduction using properties of
conditional independence.
1. B l_a Dy I x
( By definition of B and D in G)
2. B l_p Dy I X
( From (1) and since G is a Markov network of P)
3. B l_p y I xD
( Weak union on (2))
4. X l_p y I BD
(Z = BD and x l_p y I Z is assumed)

22

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

5. xB l_p y I D
(Intersection and Symmetry on (3) and (4))
6. X l_p y I D
(Decomposition on (5))
7. X l_G D I y
(By definition of D in G)
8. X l_p D I y
(From (7) and since G is a Markov network of P)
9. X l_p yD 1 0
(Intersection on (6) and (8))
10. X l_p Y 1 0
(Decomposition on (9))
11. X l_G Uy I y
(Definition of Uy)
12. X l_p Uy I y
(From (11) and since G is a Markov network of P)
13. X l_p yUy 10
(Contraction on (10) and (12))
14. Ux l_c yUy I x
(Definition of Ux and Uy)
15. Ux l_p yUy I X
(From (14) and since G is a Markov network of P)
16. xUx l_p yUy 10
(Contraction and Symmetry on (13) and (15))
17. x l_p yiUxUy
(Weak union and Symmetry on (16))
Now suppose the path between x andy has more than
one edge and that c is a vertex on this path. We reach
a contradiction by showing that the pair x, y is not
the closest pair of vertices that satisfy Eq. 16 for some
set Z', contrary to our selection of these vertices. Let
B, D be a partition of Z such that B are the vertices
in Z on the x side of c and D = Z \ B. The rest of
the derivation is a formal deduction using properties
of conditional independence.
1. xB l_c Dy I c
(By definition of B and D in G)
2. xB l_p Dy I c
(From (1) and since G is a Markov network of P)
�3. x l_p yiBD
(Z = BD and x l_p y I Z is assumed)
4. X l_p c I B v c l_p y I D
(Decomposable transitivity on (2) and (3))
5. -,x l_c c I B 1\ -,c l_p y I D
(By definition of B and D in G)
v
6. [x l_p c I B 1\ -,x l_c c I B]
((4) and (5))
[ c l_p y I D 1\ -,c l_c y I D]
Each disjunct in Step (6) exhibits a pair of vertices that
are closer to each other than x and y and yet satisfy
Eq. 16 for some set Z'. Note that Step (4) uses De­
composable transitivity which holds if Xt, . . . , Xn are
binary random variables and P is a strictly-positive
joint probability distribution, or if Xt, . . . , Xn are con­
tinuous random variables and P is a strictly positive
joint normal distribution, as assumed.
D

5

Discussion

Our proof uses a new property of conditional indepen­
dence that holds for the two classes of probability dis­
tributions we have focused on. The approach of using
logical properties of conditional independence as a way
of reasoning follows the approach taken by (Pearl and
Paz, 1987) who analyzed the logical properties shared
by vertex separation and conditional independence.
The algorithmic consequence of Theorem 5 is that in
order to check whether a Markov tree of P represents
all the conditional independence statements that hold
in P, assuming P satisfies Intersection and Decom­
posable transitivity, requires one to check whether for
each edge (x, y) in G, x l_p y 1 0 holds in P. Note that
this test is more reliable and simpler than checking
for each edge (x, y) in G, whether x l_p y I Uxy holds,
as the definition of a Markov tree requires. An open
question remains as to what is the minimal computa­
tion needed to ensure that a general Markov network
represents all the conditional independence statements
that hold in P and what properties P needs to satisfy
to accommodate these computations.
A straightforward attempt to extend our results with­
out changing the tests or the assumptions on P is quite
limited because we have counter examples to Theo­
rem 5 when G is a polytree (a directed graph with
no underlying undirected cycles) and when P does
not satisfy Intersection or Decomposable transitivity.
These counter examples, together with the proof of
Theorem 5, show that if G is a Markov tree of a prob­
ability distribution P, then G is a perfect represen­
tation of P if and only if P satisfies Intersection and
Decomposable transitivity.




We describe a graphical representation of
probabilistic relationships-an alternative to
the Bayesian network-called a dependency
network. Like a Bayesian network, a depen­
dency network has a graph and a probabil­
ity component. The graph component is a
(cyclic) directed graph such that a node's
parents render that node independent of all
other nodes in the network. The probabil­
ity component consists of the probability of
a node given its parents for each node (as in
a Bayesian network) . We identify several ba­
sic properties of this representation, and de­
scribe its use in collaborative filtering (the
task of predicting preferences) and the visu­
alization of predictive relationships.
Keywords: Dependency networks, graphical models,
inference, data visualization, exploratory data analy­
sis, collaborative filtering, Gibbs sampling
1

Introduction

The Bayesian network has proven to be a valuable tool
for encoding, learning, and reasoning about probabilis­
tic relationships. In this paper, we introduce another
graphical representation of such relationships called
a dependency network. The representation can be
thought of as a collection of regression/classification
models among variables in a domain that can be com­
bined using Gibbs sampling to define a joint distribu­
tion for that domain. The dependency network has
several advantages and disadvantages with respect to
the Bayesian network. For example, a dependency net­
work is not useful for encoding causal relationships and
is difficult to construct using a knowledge-based ap­
proach. Nonetheless, in our three years of experience
with this representation, we have found it to be easy to

learn from data and quite useful for encoding and dis­
playing predictive (i.e., dependence and independence)
relationships. In addition, we have empirically verified
that dependency networks are well suited to the task of
predicting preferences-a task often referred to as col­
laborative filtering. Finally, the representation shows
promise for density estimation and probabilistic infer­
ence.
The representation was conceived independently by
Hofmann and Tresp (1997), who used it for density es­
timation; and Hofmann (2000) investigated several of
its theoretical properties. In this paper, we summarize
their work, further investigate theoretical properties of
the representation, and examine its use for collabora­
tive filtering and data visualization.
In Section 2, we define the representation and describe
several of its basic properties. In Section 3, we de­
scribe algorithms for learning a dependency network
from data, concentrating on the case where the local
distributions of a dependency network (similar to the
local distributions of a Bayesian network) are encoded
using decision trees.
In Section 4, we describe the
task of collaborative filtering and present an empirical
study showing that dependency networks are almost
as accurate as and computationally more attractive
than Bayesian networks on this task. Finally, in Sec­
tion 5, we show how dependency networks are ideally
suited to the task of visualizing predictive relationships
learned from data.
2

Dependency Networks

To describe dependency networks and how we learn
them, we need some notation. We denote a variable
by a capitalized token (e.g., X, X;, 0, Age), and the
state or value of a corresponding variable by that same
token in lower case (e.g., x, x;, 8, age). We denote a
set of variables by a bold-face capitalized token (e.g.,
X, X;, Pa;). We use a corresponding bold-face lower­
case token (e.g., x, x;, pa;) to denote an assignment of

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

state or value to each variable in a given set. We use
p(X xiY = y) (or p(xiy) as a shorthand) to denote
the probability that X = x given Y = y. We also
use p (x iy) to denote the probability distribution for X
given Y (both mass functions and density functions).
Whether p(xiy) refers to a probability, a probability
density, or a probability distribution will be clear from
context.
=

Consider a domain of interest having variables X =
(X1 , ... , Xn ) . A dependency network for X is a pair
(9, P) where q is a (cyclic) directed graph and P is a
set of probability distributions. Each node in q corre­
sponds to a variable in X. We use X; to refer to both
the variable and its corresponding node. The parents
of node X;, denoted Pa;, correspond to those variables
Pa; that satisfy
(1)
The distributions in P are the local probability distributions p (x; j p a; ), i = 1, . . . , n. We do not require
the distributions p ( x; x1, ...,Xi-1,Xi+1, ..., xn) , i =
1, .. . , n to be obtainable (via inference) from a sin­
gle joint distribution p(x). If they are, we say that the
dependency network is consistent with p (x). We shall
say more about the issue of consistency later in this
section.

l

A Bayesian network for X defines a joint distribution
for X via the product of its local distributions. A
dependency network for X also defines a joint distri­
bution for X, but in a more complicated way via a
Gibbs sampler (e.g., Gilks, Richardson, and Spiegel­
halter, 1996). In this Gibbs sampler, we initial­
ize each variable to some arbitrary value. We then
repeatedly cycle through each variable X1 , ... , Xn,
in this order, and resample each X; according to
p(x;ix1, ...,Xi-1,Xi+1, ..., Xn) = p (x ; lp a; ). We call
this procedure an ordered Gibbs sampler. As described
by the following theorem (also proved in Hofmann,
2000), this ordered Gibbs sampler defines a joint dis­
tribution for X.
Theorem 1: An ordered Gibbs sampler applied to a
dependency network for X, where each X; is discrete
and each local distribution p ( x; IPa;) is positive, has a
unique stationary joint distribution for X.

xt

tth

Proof: Let be the sample of x after the
iteration
of the ordered Gibbs sampler. The sequence x1, x2, . . .
can be viewed as samples drawn from a homogenous
Markov chain with transition matrix M having ele­
ments Mjli = p ( xt+1 =
= i). (We use the termi­
nology of Feller, 1957.) It is not difficult to see that
M is the product M1
Mn, where Mk is the "lo­
cal" transition matrix describing the resampling of Xk

jlxt

·

.

.

.

·

265

according to the local distribution p(xk IPak)· The pos­
itivity of local distributions guarantees the positivity
of M, which in turn guarantees (1) the irreducibility
of the Markov chain and (2) that all of the states are
ergodic. Consequently, there exists a unique joint dis­
tribution that is stationary with respect to M. 0
Because the Markov chain described in the proof is
irreducible and ergodic, after a sufficient number of
iterations, the samples in the chain will be drawn from
the stationary distribution for X. Consequently, these
samples can be used to estimate this distribution.
Note that the Theorem holds for both consistent and
inconsistent dependency networks. Furthermore, the
restriction to discrete variables can be relaxed, but
will not be discussed here. In the remainder of this
paper, we assume all variables are discrete and each
local distribution is positive.
In addition to determining a joint distribution, a de­
pendency network for a given domain can be used
to compute any conditional distribution of interest­
that is, perform probabilistic inference. We discuss
an algorithm for doing so, which uses Gibbs sampling,
in Heckerman, Chickering, Meek, Rounthwaite, and
Kadie (2000). That Gibbs sampling is used for in­
ference may appear to be a disadvantage of depen­
dency networks with respect to Bayesian networks.
When we learn a Bayesian network from data, how­
ever, the resulting structures are typically complex and
not amenable to exact inference. In such situations,
Gibbs sampling (or even more complicated Monte­
Carlo techniques) are used for inference in Bayesian
networks, thus weakening this potential advantage.
In fact, when we have data and can learn a model
for X, dependency networks have an advantage over
Bayesian networks. Namely, we can learn each local
distribution in a dependency network independently,
without regard to acyclicity constraints.
Bayesian networks have one clear advantage over de­
pendency networks. In particular, dependency net­
works are not suitable for the representation of causal
relationships. For example, if X causes Y (so that
X and Y are dependent), the corresponding depen­
dency network is X +-+ Y -that is, X is a parent of Y
and vice versa. It follows that dependency networks
are difficult to elicit directly from experts. Without
an underlying causal interpretation, knowledge-based
elicitation is cumbersome at best.
Another important observation about dependency net­
works is that, when we learn one from data as
we have described-learning each local distribution
independently-the model is likely to be inconsistent.
(In an extreme case, where (1) the true joint distribu-

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

266

tion lies in one of the possible models, (2) the model
search procedure finds the true model, and (3) we
have essentially an infinite amount of data, the learned
model will be consistent.) A simple approach to avoid
this difficulty is to learn a Bayesian network and apply
inference to that network to construct the dependency
network. This approach, however, will eliminate the
advantage associated with learning dependency net­
works just described, is likely to be computationally
inefficient, and may produce extremely complex local
distributions. When ordered Gibbs sampling is applied
to an inconsistent dependency network, it is important
to note that the joint distribution so defined will de­
pend on the order in which the Gibbs sampler visits
the variables. For example, consider the inconsistent
dependency network X +- Y. If we draw sample-pairs
(x, y)-that is, x and then y-then the resulting sta­
tionary distribution will have X and Y independent.
In contrast, if we draw sample-pairs (y, x), then the
resulting stationary distribution may have X and Y
dependent.
The fact that we obtain a joint distribution from any
dependency network, consistent or not, is comforting.
A more important question, however, is what distri­
bution do we get? The following theorem, proved in
Heckerman et al. (2000), provides a partial answer.
Theorem 2: If a dependency network for X is con­
sistent with a positive distribution
then the sta­
tionary distribution defined in Theorem 1 is equal to

p(x),

p(x).

When a dependency network is inconsistent, the situa­
tion is even more interesting. If we start with learned
local distributions that are only slight perturbations
(in some sense) of the true local distributions, will
Gibbs sampling produce a joint distribution that is a
slight perturbation of the true joint distribution? Hof­
mann (2000) argues that, for discrete dependency net­
works with positive local distributions, the answer to
this question is yes when perturbations are measured
with an L2 norm. In addition, Heckerman et al. (2000)
show empirically using several real datasets that the
joint distributions defined by a Bayesian network and
dependency network, both learned from data, are sim­
ilar.
We close this section with several facts about consis­
tent dependency networks, proved in Heckerman et al.
(2000). We say that a dependency network for X is
bi-directional if X; is a parent of Xj if and only if Xj is
a parent of X; , for all X; and Xj in X. We say that a
distribution
is consistent with a dependency net­
work structure if there exists a consistent dependency
network with that structure that defines

p(x)

p(x).

Theorem 3: The set of positive distributions consis­
tent with a dependency network structure is equal to
the set of positive distributions defined by a Markov­
network structure with the same adjacencies.
Note that, although dependency networks and Markov
networks define the same set of distributions, their rep­
resentations are quite different. In particular, the de­
pendency network includes a collection of conditional
distributions, whereas the Markov network includes a
collection of joint potentials.
Let pa{ be the lh parent of node X; . A consistent de­
pendency network is minimal if and only if, for every
node X; and for every parent pa{ , X; is not indepen­
dent of pa{ given the remaining parents of X; .
Theorem 4: A minimal consistent dependency net­
work for a positive distribution
must be bi­
directional.

p(x)

3

Learning Dependency Networks

In this section, we mention a few important points
about learning dependency networks from data.
When learning a dependency network for X, each local
distribution for X; is simply a regression/classification
model (with feature selection) for x; with X\ {xi} as
inputs. If we assume that each local distribution has
a parametric model p( x; !Pa;, B;), and ignore the de­
pendencies among the parameter sets Bt, ..., ()n, then
we can learn each local distribution independently us­
ing any regression/classification technique for mod­
els such as a generalized linear model, a neural net­
work, a support-vector machine, or an embedded re­
gression/classification model (Heckerman and Meek,
1997). From this perspective, the dependency network
can be thought of as a mechanism for combining re­
gression/classification models via Gibbs sampling to
determine a joint distribution.
In the work described in this paper, we use decision
trees for the local distributions. A good discussion of
methods for learning decision trees is given in Breiman,
Friedman, Olshen, and Stone (1984). We learn a deci­
sion tree using a simple hill-climbing approach in con­
junction with a Bayesian score as described in Fried­
man and Goldszmdit (1996) and Chickering, Hecker­
man, and Meek (1997). To learn a decision tree for
X; , we initialize the search algorithm with a single­
ton root node having no children. Then, we replace
each leaf node in the tree with a binary split on some
variable Xj in X\ X; , until no such replacement in­
creases the score of the tree. Our binary split on Xj is

267

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

a decision-tree node with two children: one of the chil­
dren corresponds to a particular value of Xj, and the
other child corresponds to all other values of Xj. Our
Bayesian scoring function uses a uniform prior distri­
bution for all decision-tree parameters, and a structure
prior proportional to KJ, where K > 0 is a tunable pa­
rameter and f is the number of free parameters in the
decision tree. In studies that predated those described
in this paper, we have found that the setting K = 0.01
yields accurate models over a wide variety of datasets.
We use this same setting in our experiments.
For comparison in these experiments, we also learn
Bayesian networks with decision trees for local distri­
butions using the algorithm described in Chickering,
Heckerman, and Meek (1997). When learning these
networks, we use the same parameter and structure
priors used for dependency networks.
We conclude this section by noting an interesting fact
about the decision-tree representation of local distri­
butions. Namely, there will be a split on variable X
in the decision tree for Y if and only if there is an
arc from X to Y in the dependency network that in­
cludes these variables. As we shall see in Section 5,
this correspondence helps the visualization of data.
4

Collaborative Filtering

In the remainder of this paper, we consider useful ap­
plications of dependency networks, whether they be
consistent or not.
The first application is collaborative filtering ( CF), the
task of predicting preferences. Examples of this task
include predicting what movies a person will like based
on his or her ratings of movies seen, predicting what
new stories a person is interested in based on other
stories he or she has read, and predicting what web
pages a person will go to next based on his or her
history on the site. Another important application in
the burgeoning area of e-commerce is predicting what
products a person will buy based on products he or
she has already purchased and/or dropped into his or
her shopping basket.
Collaborative filtering was introduced by Resnick, la­
covou, Suchak, Bergstrom, and Riedl (1994) as both
the task of predicting preferences and a class of al­
gorithms for this task. The class of algorithms they
described was based on the informal mechanisms peo­
ple use to understand their own preferences. For ex­
ample, when we want to find a good movie, we talk
to other people that have similar tastes and ask them
what they like that we haven't seen. The type of algo­
rithm introduced by Resnik et al. (1994), sometimes
called a memory-based algorithm, does something simi-

lar. Given a user's preferences on a series of items, the
algorithm finds similar users in a database of stored
preferences. It then returns some weighted average of
preferences among these users on items not yet rated
by the original user.
As done in Breese, Heckerman, and Kadie (1998),
let us concentrate on the application of collaborative
filtering-that is, preference prediction. In their pa­
per, Breese et al. (1998) describe several CF sce­
narios, including binary versus non-binary preferences
and implicit versus explicit voting. An example of
explicit voting would be movie ratings provided by a
user. An example of implicit voting would be know­
ing only whether a person has or has not purchased
a product. Here, we concentrate on one scenario im­
portant for e-commerce: implicit voting with binary
preferences-for example, the task of predicting what
products a person will buy, knowing only what other
products they have purchased.
A simple approach to this task, described in Breese et
al. (1998), is as follows. For each item (e.g., prod­
uct), define a variable with two states corresponding
to whether or not that item was preferred (e.g., pur­
chased). We shall use "0" and "1" to denote not
preferred and preferred, respectively. Next, use the
dataset of ratings to learn a Bayesian network for the
joint distribution of these variables X= (X1, ..., Xn)·
The preferences of each user constitutes a case in the
learning procedure. Once the Bayesian network is
constructed, make predictions as follows. Given a
new user's preferences x, use the Bayesian network
to determine p(Xi = 1lx \ Xi = 0) for each prod­
uct Xi not purchased. That is, infer the probability
that the user would have purchased the item had we
not known he did not. Then, return a list of recom­
mended products-among those that the user did not
purchase-ranked by this probability.
Breese et al. (1998) show that this approach out­
performs memory-based and cluster-based methods
on several implicit rating datasets. Specifically, the
Bayesian-network approach was more accurate and
yielded faster predictions than did the other methods.
What is most interesting about this algorithm in the
context of this paper is that only the probabilities
p(X; = 1lx \X; = 0) are needed to produce the recom­
mendations. In particular, these probabilities may be
obtained by a direct lookup in a dependency network:
p(Xi

=

1lx \X;= 0)

=

p(Xi

=

1lpa; )

(2)

where pai is the instance of Pai consistent with X.
Thus, dependency networks are a natural model on
which to base CF predictions. In the remainder of
this section, we compare this approach with that based

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

268

Table 1: Number of users, items, and items per user
for the datasets used in evaluating the algorithms.
Users in tra.mmg set
Users in test set

Total items
Mean items per user
in training set

Dataset
MS.COM
Nielsen
1,637
32,711
1,637
5,000
294
3.02

203
8.64

MSNHC
10,000
10,000
1,001
2.67

on Bayesian networks for datasets containing binary
implicit ratings.
Datasets

4.1

We evaluated Bayesian networks and dependency net­
works on three datasets: (1) Nielsen, which records
whether or not users watched five or more minutes of
network TV shows aired during a two-week period in
1995 (made available courtesy of Nielsen Media Re­
search), (2) MS. COM, which records whether or not
users of microsoft.com on one day in 1996 visited ar­
eas "vroots" ) of the site (available on the Irvine Data
Mining Repository), and (3) MSNBC, which records
whether or not visitors to MSNBC on one day in 1998
read stories among the most popular 1001 stories on
the site. The MSNBC dataset contains 20,000 users
sampled at random from the approximate 600,000
users that visited the site that day. In a separate anal­
ysis on this dataset, we found that the inclusion of ad­
ditional users did not produce a substantial increase
in accuracy. Table 4.1 provides additional information
about each dataset. All datasets were partitioned into
training and test sets at random.

(

4.2

Evaluation Criteria and Experimental
Procedure

We have found the following three criteria for collab­
orative filtering to be important: (1) the accuracy of
the recommendations, (2) prediction time-the time
it takes to create a recommendation list given what
is known about a user, and (3) the computational re­
sources needed to build the prediction models. We
measure each of these criteria in our empirical com­
parison. In the remainder of this section, we describe
our evaluation criterion for accuracy.
Our criterion attempts to measure a user's expected
utility for a list of recommendations. Of course, dif­
ferent users will have different utility functions. The
measure we introduce provides what we believe to be
a good approximation across many users.
The scenario we imagine is one where a user is shown

a ranked list of items and then scans that list for pre­
ferred items starting from the top. At some point, the
denote
user will stop looking at more items. Let
the probability that a user will examine the kth item
on a recommendation list before stopping his or her
scan, where the first position is given by k= 0. Then,
a reasonable criterion is

p(k)

cfaccuracyt (list)=

LP(k) c5k
k

(3)

where c5k is 1 if the item at position k is preferred and 0
otherwise. To make this measure concrete, we assume
that p(k) is an exponentially decaying function:

p(k)= Tk/a

(4)

where a is the "half-life" position-the position at
which an item will be seen with probability 0.5. In
our experiments, we use a= 5.
In one possible implementation of this approach, we
could show recommendations to a series of users
and ask them to rate them as "preferred" or "not
preferred" .
We could then use the average of
cfaccuarcy1 (list) over all users as our criterion. Be­
cause this method is extremely costly, we instead use
an approach that uses only the data we have. In par­
ticular, as already described, we randomly partition a
dataset into a training set and a test set. Each case
in the test set is then processed as follows. First, we
randomly partition the user's preferred items into in­
put and measurement sets. The input set is fed to the
CF model, which in turn outputs a list of recommen­
dations. Finally, we compute our criterion as
N

100 "'"' Lk cS;k
.
cfaccuracy(hst) =
K 1
N L...
i=l L:k,;,;

p(k)
p(k)

(5)

where N is the number of users in the test set, K; is
the number of preferred items in the measurement set
for user i, and c5;k is 1 if the kth item in the recommen­
dation list for user i is preferred in the measurement
set and 0 otherwise. The denominator in Equation 5
is a per-user normalization factor. It is the utility of a
list where all preferred items are at the top. This nor­
malization allows us to more sensibly combine scores
across measurement sets of different size.
We performed several experiments reflecting differing
numbers of ratings available to the CF engines. In the
first protocol, we included all but one of the preferred
items in the input set. We term this protocol all but 1.
In additional experiments, we placed 2, 5, and 10 pre­
ferred items in the input sets. We call these protocols
given 2, given 5, and given 10.
The all but 1 experiments measure the algorithms' per­
formance when given as much data as possible from

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

each test user. The various given experiments look at
users with less data available, and examine the perfor­
mance of the algorithms when there is relatively little
known about an active user. When running the given
m protocols, if an input set for a given user had less
than m preferred items, the case was eliminated from
the evaluation. Thus the number of trials evaluated
under each protocol varied.

Table 2: CF accuracy for the MS.COM, Nielsen, and
MSNBC datasets. Higher scores indicate better per­
formance. Statistically significant winners are shown
in boldface.
MS.COM

Algorithm
BN
DN

All experiments were performed on a 300 MHz Pen­
tium II with 128 MB of memory, running the NT 4.0
operating system.
4.3

269

Given2
53.18

52.68

Tables 3 and 4 compare the two methods with
the remaining criteria. Here, dependency networks
are a clear winner. They are significantly faster
at prediction-sometimes by almost an order of
magnitude-and require substantially less time and
memory to learn.

AllBut1
66.54
66.60

0. 30

0. 73

1.62

0.34

Baseline

43.37

39.34

39.32

49.77

Given2

Given10
33.84
33.80

AllBut1

24.20

GivenS
30.03
29.71

RD

0. 32

0. 40

0.65

0.72

Baseline

12.65

12.72

12.92

13.59

Nielsen

Algorithm
BN
DN

Table 2 shows the accuracy of recommendations for
Bayesian networks and dependency networks across
the various protocols and three datasets. For a com­
parison, we also measured the accuracy of recommen­
dation lists produced by sorting items on their overall
popularity, p(X; = 1). The accuracy of this approach
is shown in the row labeled "Baseline." A score in
boldface corresponds to a significantly significant win­
ner. We use ANOVA (e.g., McClave and Dietrich,
1988) with a = 0.1 to test for statistical significance.
When the difference between two scores in the same
column exceed the value of RD (required difference),
the difference is significant.

The magnitudes of accuracy differences, however, are
not that large. In particular, the ratio of ( cfac­
curacy(BN) - cfaccuracy(DN)) to (cfaccuracy(BN) cfaccuracy(Baseline)) averages 4 ± S percent across the
datasets and protocols.

Given10
51.64
51.48

RD

Results

From the table, we see that Bayesian networks are
more accurate than dependency networks. This re­
sult is interesting, because there are reasons to ex­
pect that dependency networks will be more accurate
than Bayesian networks and vice versa. On the one
hand, the search process that learns Bayesian net­
works is constrained by acyclicity, suggesting that de­
pendency networks may be more accurate. On the
other hand, the conditional probabilities used to sort
the recommendations are inferred from the Bayesian
network, but learned directly in the dependency net­
work. Therefore, dependency networks may be less
accurate, because they waste data in the process of
learning what could otherwise be inferred. For this
or perhaps other reasons, the Bayesian networks are
more accurate.

GivenS
52.48
52.54

24.99

45.55

44.30

MSNBC

Algorithm
BN
DN

Given2

GivenS

40.34

34.20

38.84

32.S3

Given10
30.39
30.03

RD

0. 35

0. 77

1.54

0. 39

Baseline

28.73

20.58

14.93

32.94

AllBut1
49.58

48.0S

Overall, Bayesian networks are slightly more accurate
but much less attractive from a computational per­
spective.
5

Data Visualization

Bayesian networks are well known to be useful for
visualizing causal relationships. In many circum­
stances, however, analysts are only interested in
predictive-that is, dependency and independency­
relationships. In our experience, the directed-arc se­
mantics of Bayesian networks interfere with the visu­
alization of such relationships.
As a simple example, consider the Bayesian network
X --+ Y. Those familiar with the semantics of
Bayesian networks immediately recognize that observ­
ing Y helps to predict X. Unfortunately, the untrained
individual will not. In our experience, this person will
interpret this network to mean that only X helps to
predict Y, and not vice versa. Even people who are
expert in d-separation semantics will sometimes have
difficulties visualizing predictive relationships using a
Bayesian network. The cognitive act of identifying a
node's Markov blanket seems to interfere with the vi­
sualization experience.
Dependency networks are a natural remedy to this

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

270

problem. If there is no arc from X to Y in a depen­
dency network, we know immediately that X does not
help to predict Y.

Table 3: Number of predictions per second for the
MS.COM, Nielsen, and MSNBC datasets.
MS.COM

Algorithm
BN
DN

Given2
3.94
23.29

Given5
3.84
19.91

Algorithm
BN
DN

Given2
22.84
36.17

Given5
21.86
36.72

Algorithm
BN
DN

Given2
7.21
11.88

Given5
6.96
11.03

Given10
3.29
10.20

AllBut1
3.93
23.48

Nielsen

Given10
20.83
34.21

AllBut1
23.53
37.41

MSNBC

Given10
6.09
8.52

AllBut1
7.07
11.80

Table 4: Computational resources for model learning.
MS.COM

Algorithm
BN
DN

Memory (Meg)
42.4
5.3

Algorithm
BN
DN

Memory (Meg)
3.3
2.1

Algorithm
BN
DN

Memory (Meg)
43.0
3.7

Learn Time (sec)
144.65
98.31

Nielsen

Learn Time (sec)
7.66
6.47

MSNBC

Learn Time (sec)
105.76
96.89

Figure 1 shows a dependency network learned from
a dataset obtained from Media Metrix. The dataset
contains demographic and internet-use data for about
5,000 individuals during the month of January 1997.
On first inspection of this network, an interest­
ing observation becomes apparent: there are many
(predictive) dependencies among demographics, and
many dependencies among frequency-of-use, but there
are few dependencies between demographics and
frequency-of-use.
Over the last three years, we have found numerous
interesting dependency relationships across a wide va­
riety of datasets using dependency networks for visu­
alization. In fact, we have given dependency networks
this name because they have been so useful in this
regard.
The network in Figure 1 is displayed in DNViewer,
a dependency-network visualization tool developed at
Microsoft Research. The tool allows a user to display
both the dependency-network structure and the de­
cision tree associated with each variable. Navigation
between the views is straightforward. To view a de­
cision tree for a variable, a user simply double clicks
on the corresponding node in the dependency network.
Figure 2 shows the tree for Shopping.Freq.
An inconsistent dependency net learned from data of­
fers an additional advantage for visualization. If there
is an arc from X to Y in such a network, we know that
X is a significant predictor of Y -significant in what­
ever sense was used to learn the network. Under this
interpretation, a uni-directional link between X and Y
is not confusing, but rather informative. For example,
in Figure 1, we see that Sex is a significant predictor
of Socioeconomic status, but not vice versa-an in­
teresting observation. Of course, when making such
interpretations, one must always be careful to recog­
nize that statements of the form "X helps to predict
Y" are made in the context of the other variables in
the network.
In DNViewer, we enhance the ability of dependency
networks to reflect strength of dependency by includ­
ing a slider (on the left). As a user moves the slider
from bottom to top, arcs are added to the graph in the
order in which arcs are added to the dependency net­
work during the learning process. When the slider is
in its upper-most position, all arcs (i.e., all significant
dependencies) are shown.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

271

Figure 1: A dependency network for Media Metrix data. The dataset contains demographic and internet-use
data for about 5,000 individuals during the month of January 1997. The node labeled Overall.Freq represents
the overall frequency-of-use of the internet during this period. The nodes Search.Freq, Edu.Freq, and so on
represent frequency-of-use for various subsets of the internet.

�
/

r.'
,
:
,
�
J
101·200(701]

�

(20C>l)

3<51·0<50(�'1!!!i':J
�
�
�
�
. 201·3150
��
�NoM(�
�

01'-(20�
�

(2802)

ahw(1373]

051·100

�
other(865)

ott..r(567)

.

Low(181)

.

Figure 2: The decision tree for Shopping.Freq obtained by double-clicking that node in the dependency network.
The histograms at the leaves correspond to probabilities of Shopping.Freq use being zero, one, and greater than
one visit per month, respectively.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

272

r

I
I

I

I s�

irko

Figure 3: The dependency network in Figure 1 with the slider set at half position.
Figure 3 shows the dependency network for the Media
Metrix data with the slider at half position. At this
setting, we find the interesting observation that the
dependencies between Sex and XXX.Freq (frequency
of hits to pornographic pages) are the strongest among
all dependencies between demographics and internet
use.
6

Summary and Future Work

We have described a new representation for probabilis­
tic dependencies called a dependency network. We
have shown that a dependency network (consistent
or not) defines a joint distribution for its variables,
and that models in this class are easy to learn from
data. In particular, we have shown how a dependency
network can be thought of as a collection of regres­
sion/classification models among variables in a domain
that can be combined using Gibbs sampling to define
a joint distribution for the domain. In addition, we
have shown that this representation is useful for col­
laborative filtering and the visualization of predictive
relationships.
Of course, this research is far from complete. There
are many questions left to be answered. For example,

what are useful models (e.g., generalized linear models,
neural networks, support-vector machines, or embed­
ded regression/classification models) for a dependency
network's local distributions? Another example of par­
ticular theoretical interest is Hofmann's (2000) result
that small 12-norm perturbations in the local distribu­
tions lead to small 12-norm perturbations in the joint
distributions defined by the dependency network. Can
this result be extended to norms more appropriate for
probabilities such as cross entropy?
Finally, the dependency network and Bayesian net­
work can be viewed as two extremes of a spectrum.
The dependency network is ideal for situations where
the conditionals p(x;lx \ x;) are needed. In con­
trast, when we require the joint probabilities p(x),
the Bayesian network is ideal because these probabil­
ities may be obtained simply by multiplying condi­
tional probabilities found in the local distributions of
the variables. In situations where we need probabili­
ties of the form p(ylx\ y), where Y is a proper subset
of the domain X, we can build a network structure
that enforces an acyclicity constraint among only the
variables Y. In so doing, the conditional probabilities
p(ylx\ y) can be obtained by multiplication.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Acknowledgments

We thank Reimar Hofmann for useful discussions.
Datasets for this paper were generously provided by
Media Metrix, Nielsen Media Research (Nielsen), Mi­
crosoft Corporation (MS.COM), and Steven White
and Microsoft Corporation (MSNBC).

