

Comprehensible explanations of probabilistic
reasoning are a prerequisite for wider acceptance
of Bayesian methods in expert systems and
decision support systems. A study of human
reasoning under uncertainty suggests two different
strategies for explaining probabilistic reasoning
specially attuned to human thinking: The first,
qualitative belief propagation, traces the
qualitative effect of evidence through a belief
network from one variable to the next. This
propagation algorithm is an alternative to the
graph reduction algorithms of Wellman (1988) for
inference in qualitative probabilistic networks. It is
based on a qualitative analysis of intercausal
reasoning, which is a generalization of Pearl's
"explaining away", and an alternative to Wellman's
definition of qualitative synergy. The other,
Scenario-based reasoning, involves the generation
of alternative causal "stories" accounting for the
evidence. Comparing a few of the most probable
scenarios provides an approximate way to explain
the results of probabilistic reasoning. Both
schemes employ causal as well as probabilistic
knowledge. Probabilities may be presented as
phrases and/or numbers. Users can control the
style, abstraction and completeness of
explanations.
1 Introduction

The developers of expert systems and decision
support systems have long been aware of the
importance of facilities to explain the computer­
based reasoning to users as a prerequisite to their
more widespread acceptance (e.g. Teach &
Shortliffe, 1981). Unless users can come to
This work was supported by the National Science
Foundation under grant IRI-8807061 to Carnegie Mellon
and by the Rockwell International Science Center.
*

understand the assumptions and reasoning of
such systems, it is impossible to develop the kind
of human-machine collaboration that is the basis
for successful use of such systems. For
explanations to be effective, their form and content
must be carefully matched to the users'
competence, knowledge, and styles of reasoning.
The approach that underlies the classic "expert
systems" paradigm is to employ computer
representations and inference mechanisms
intended to emulate human reasoning. To the
extent that this emulation is successful, the
computer-based reasoning ought to seem familiar
to people and so relatively easy to explain. While
there is ample evidence that normatively appealing
probabilistic and decision theoretic schemes are
poor models of human reasoning under
uncertainty (e.g. Kahneman et a/. 1982), there is
surprisingly little experimental evidence that the
rule-based alternatives, such as certainty factors
or fuzzy logic, are any better as descriptive
models. And even if successful descriptively, the
emulative approach would merely reproduce the
documented deficiencies of our intuitive reasoning
rather than complement and enhance it. Are we
forced to choose between the unreliable and the
inexplicable? Our approach to this dilemma is to
explore whether in fact it may be possible to
explicate probabilistic reasoning in ways better
attuned to human thinking.
Only recently has much attention begun to be paid
to the automatic generation of comprehensible
explanations for probabilistic and decision analytic
schemes. Horvitz et a/. (1986) present a system
which can explain its recommendations about
what test to perform to gather diagnostic evidence.
Langlotz et a/. (1986) present a scheme for
quantitative analysis of decision trees, which
explains qualitatively how one decision may
outweigh another in terms of expected utility.
Klein (1990) presents a scheme for qualitatively

11

explaining the implications of hierarchical additive
value functions. Elsaesser (1988) provides some
empirical evidence on the efficacy of explanations
of simple Bayesian inference, with one variable
and one observation. Strat's (1988) system
explains the dynamics of Dempster-Shafer
reasoning based on sensitivity analysis, with
interesting implications for probabilistic schemes.
Sember and Zukerman's (1989) scheme
generates micro explanations, that is local
propagation of evidence between neighbouring
variables in a belief net.
Our focus here is on approaches for generating
macro explanations, intended to explain
probabilistic reasoning over larger networks. We
wish to avoid dogmatism about what kinds of
explanation scheme will be most effective, but
rather explore a variety of approaches, including
graphical, numerical, and linguistic
representations. We are interested in both
quantitative and qualitative forms of explanation in
various combinations. This paper gives an
account of several of the key ideas that have
emerged from our initial work.
Since our goal is to produce interpretations of
probabilistic reasoning that are more compatible
with human reasoning styles, we started out with
an empirical study of human strategies for
uncertain reasoning. This provided us with the
inspiration for the design of two new and
contrasting modes of explaining probabilistic
reasoning, namely qualitative belief propagation
and scenario-based reasoning.
It is useful to distinguish explanation as the
communication of static knowledge or beliefs from
explanation of the dynamics, of how beliefs are
changed in the light of new evidence. Explanation
of the statics, though relatively straightforward, is a
prerequisite for explanation of the dynamics.
Among the issues in static explanation we discuss
are the use of belief nets, the use of linguistic
phrases to express probabilities, and the
importance of causal knowledge. Next we outline
the use of qualitative belief propagation as a
means of dynamic explanation. This includes an
analysis of qualitative intercausal reasoning,
generalizing Pearl's notion of "explaining away",
with a theorem giving a precise characterization of
when it applies. Finally, we describe a scenario­
based approach to explanation. This is illustrated
by explanations generated from our prototype
implementation in Allegro Common Lisp, QIQ
(Qualitative Interface to the Quantitative).

2 Human reasoning under uncertainty

The essence of effective explanation is to design
its content and form to mesh with the knowledge
and modes of thought of the person to whom you
are explaining. Thus, producing good
explanations of formal reasoning under uncertainty
requires an understanding of the way people
reason intuitively under uncertainty. There is a
vast literature on human judgment under
uncertainty for very simple inference problems,
typically with a single hypothesis variable and a
single observation (e.g. see Kahneman et al.,
1982 and Morgan & Henrion, 1990 for reviews),
but relatively little is known about cognitive
processes in more complex situations. To improve
our insights into this and to seek inspiration for
alternative approaches to explanation, we
conducted a series of cognitive process-tracing
studies. We recorded and analyzed verbal
protocols from subjects asked to think aloud as
they performed uncertain reasoning tasks
(Druzdzel, 1989). Here is a sample task:
Harry is in the house of a new acquaintance and
suddenly finds himself sneezing. This could be
due to an incipient cold, or to an allergy attack
brought on by a cat. Before he started sneezing
he would have judged the cold and allergy both
about equally unlikely.

I
I
I
I
I
I
I
I
I

(a) Given he is sneezing, roughly what is the
probability Harry is getting a cold?

I

(b) Suppose Harry now notices small paw-prints
on the furniture. How should this affect his
degree of belief that he is getting a cold?

I

(c) Suppose he then hears a barking of a small
dog in the room next-door. How does this
further affect his degree of belief that he is
getting a cold?

I

Most subjects were able to provide qualitative
answers to these questions rather easily. In (a)
they judged a cold was about as likely as not. In
(b) that the paw-prints should decrease his belief
in the cold, since the sneezing might be explained
by a cat, suggested by the paw-prints. And in (c),
that hearing the barking dog should increase belief
in the cold again, since the dog provides an
alternative explanation of the paw prints.
One unsurprising finding was that subjects
generally used qualitative terms for probabilities,
using quantitative terms almost not at all. This
finding is consistent with previous studies of
intuitive reasoning (e.g. Kuipers, Moskovitz &
Kassirer, 1988}. Another finding confirming
previous work (e.g. Kahneman and Tversky, 1980)

I
I
I
I
I
I
I

12

I
I
I
I
I
I
I
I
I
I

was the importance of causal reasoning in
uncertain inference.
Less expected and of considerable interest in the
current context, was evidence of two quite
different strategies for plausible reasoning. One,
which we call qualitative belief propagation,
involves propagating the qualitative impact of
evidence from event to event, following local
causal and diagnostic relationships. For example,
barking indicates the presence of the dog. The
dog explains the pawmarks, which are th�n
.
weaker evidence for the cat. Reduced belief 1n the
cat in turn reduces belief in the allergy. This is
noJ.. less of an explanation for the sneezing and so
requires an increased belief in the cold.
The other strategy, scenario-based reasoning, is
quite different, and was more common in the
protocols. The reasoner identifies one or more
scenarios, that is consistent instantiations of the
variables, forming a coherent, often causal,- story,
compatible with the known evidence. For
example, Harry has a cold, which explains the
sneezing; there is no cat, and so no allergy; the
dog explains the paw-prints and barking. Subjects
often appeared to develop one or more such
quasi-deterministic scenarios. Figure 1 shows two
such scenarios, as a subset of the event tree.
l----< �;...;;..:o:.

__

They are suggestiv� of tw<;> quite di!ferent .
explanation strateg1es, wh1ch we w111 descnbe
below.

3. Explanation of static probabilistic
knowledge
3.1 Belief nets

By now the most familiar display of qualitative
probabilistic information is the Bayesian belief net
(and influence diagram), which pr�vi �es a .
perspicuous display of purely quaht�t1ve beliefs
about conditional dependence and Independence.
Figure 2 provides a belief network for probabilistic
knowledge for the "sneeze" example.
The nodes depict the key variables. (NB, we use
the abbreviated term "Cat" to mean "the presence
of a cat in the vicinity", and so on.) As usual, the
directed arcs depict dependences between them,
(or more strictly the absence of arcs depicts
independence). The same. information could, of
course, be represented in text as a list of the
dependencies, such as,
The probability of sneezing is
affected by cold.
The probability of sneezing depends
on allergy.

and so on.

I
I
I
I
I
I
I
I
I

Figure 1: Two scenarios for the
sneeze problem

The probability of some target event (e.g. the cold)
can then be judged by the relative probability of
the scenario(s) that contain(s) it. If one considered
all possible scenarios, then this strategy is an
exact algorithm for Bayesian reasoning. This is
generally too much mental effort, but it can be a
good approximation if one considers only the few
most probable scenarios. On the other hand, if a
likely scenario is ignored or its relativ� probab �lity
misestimated, it can lead to severe b1ases. Th1s
scenario-based reasoning appears related to
explanation-based reasoning identified by a
number of psychologists as strategies for complex
reasoning tasks (e.g. Pennington & Hastie, 1988).
Both qualitative belief propagation and s�en�rio­
based reasoning can be seen as approx1mat1ons
of exact algorithms for probabilistic inference.

Figure 2: A belief network for the
"sneeze example"

The improved perspicuity of the graphical
representation in showing the locality of
relationships is immediately clear. Although for
some purposes the textual form is valuable,
particularly for those not familiar with the belief
network notation. To complete the static
explanation we need to add the probabilities in
some form.

13

3.2 Linguistic probabilities
One appealing approach to render numerical
probabilities more digestible is to translate them
Into verbal phrases, such as "very likely" or
"somewhat improbable". A considerable empirical
literature reports people's interpretations of verbal
probability phrases in terms of numerical
probabilities or ranges. In general this research
has found a degree of consistency in usage, at
least in the ordering people assign to sets of such
phrases (Budescu & Wallsten, 1985; Wallsten et
a/, 1986; Kong et a/. 1986). But it has also found
significant variability in interpretation between
people, and considerable context dependence
(Brun & Teigen, 1988). Nuclear safety engineers
mean something quite different by "uncommon"
than physicians. People interpret other people's
use of phrases somewhat differently (and with
wider range of uncertainty) from what they
themselves claim to mean by the phrases
(Wallsten et al; 1986); that is, they are sensitive to
the variability among people. This suggests
mappings from phrases to numbers, needed for
encoding, should be somewhat different, with
broader ranges than mappings from phrases to
numbers, as used here for explanations.

Probability

Adjectives

o-..-.-- impossible

Adverbs

even by event within a network. For example,
"unlikely" may mean something quite different
when applied to the chance of allergy to an
antibiotic than the chance of dying in an operation.
If desired, a different mapping may be used for
each event and influence. However we expect a
small number of mappings will be sufficient to
cover the contexts for a given network.
Relevant phrases can be divided into belief
phrases, such as "very probable" or "unlikely", and
frequency phrases, such as "common" or "rare".
Most come in both adjectival form, as above, and
adverbial form, such as "probably" or "commonly".
We have found it most natural to express marginal
prior and posterior probabilities of events in terms
of adjectival probabilities, for example,
Cold is very unlikely (p=0.08)
Cat is unlikely (p=O.l)
Dog is unlikely (p=O.l)

and to express conditional probabilities or causal
strengths (see below) in terms of adverbial
frequencies:

.

Cat commonly (p=0 8) causes allergy.
Dog as often as not (p=O.S) causes
barking.

The above examples are generated by 010 as
part of the static explanation of the sneeze belief
network.

never

Due to variations between people and contexts,
some
vagueness in interpretation inevitably
very rarely
remains. To some this is part of the attraction of
rarely
..--- unlikely
verbal phrases over numerical probabilities.
1.--- fairly unlikely
fairly rarely
Others may wish to see the numerical probability
less likely than not less often than notin addition to the verbal phrase, as in the
examples above. This allows users to pay
as often as not
-1(11..--- as likely as not
"'� �ention t� whatever they find most helpful and,
more likely than not more often than n
'\Vith expenence, perhaps to learn the mapp1ngs.
,__
fairly likely
fairly often
While most prev �l;JS empirical work has examined
____ likely
commonly
absolute probabJhtJes, one recent study (Eisaesser
.
very likely
very commonly
& Henrion, 1990) has examined mappings from
relative probabilities or changes in probabilities to
certain
fl-IMK.-always
1.
phrases such as "more likely than". They found
that a fixed mapping to phrases from differences in
Figure 3: Sample mappings from numerical
probabilities provided a better model than ratios of
probabilities to adjective and adverb phrases.
probabilities or odds. These phrases are useful for
To cope with differences in personal preference
comparing the probabilities of events or updates in
and the context-dependence of interpretations, our
degrees of belief, such as:
explanation system, 010, provides a variety of
Cold is slightly less likely than
mappings, including two mappings from the
cat (0. 08/0. 10) .
literature (Wallsten et al. 1986; Kong et a/. 1986),
No cat is a great deal more likely
and our own synthesis from the literature,
than cat (0. 1/0. 9) .
illustrated in figure 3. We have tried to use terms
which minimize ambiguity and variability among
3.3 Causal relationships
people and contexts. Users can select from these
mappings or provide their own. The context and
A key finding of the behavioral decision theory
interpretation may vary not only by domain, but
literature Is the psychological importance of causal
very unlikely

•

.

__

!

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

14

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

structure in uncertain reasoning (Tversky &
Kahneman, 1980). People find it easier to reason
from cause to effect than vice versa. As we
mentioned, this was also apparent in our protocol
studies. Some, notably Pearl (1988), have
explicitly identified the directed arcs of belief nets
with cause-effect relations. Others have argued
that there is no inherent relationship with causality:
After all, the arcs can be reversed simply by
application of Bayes' rule, but causality cannot. But
in any case, it is usually most natural to assess
influences in causal direction. We view knowledge
of causal relations as an important semantic
enrichment to the pure belief net. It is not
essential for Bayesian inference, but can be of
great help in communicating with people, both for
encoding expert opinion and for explanation.
QIQ can encode a cause-effect relationship as
supplementary knowledge about each influence
arc. This information is used in generating text
descriptions of influences, for example:
Cat commonly (p=0. 8) causes allergy.
Cat is the only cause of allergy.

The quantities described here are causal
strengths, that is the probability that the specified
precursor event, if present, is sufficient to cause
the successor. If no other cause of the successor
is present then the causal strength is the same as
the conditional probability of the effect given the
cause. This is the case with the link from cat to
allergy, where no other cause is known (in this
example). However, if other causes are possible,
then the causal strengths may be different from
the conditional probabilities. Causal strengths are
an equivalent representation to the conditional
probability representation, and each can be
derived from the other.
The best known application of causal strengths is
in the noisy-OR gate, which often arises in
situations with multiple alternative causes of a
common effect. Each link from cause to effect is
characterized by its causal strength, the probability
of the effect given only that cause is present. The
condition it embodies is sometimes called causal
independence, namely that the probability that
each present cause is sufficient to produce the
effect is independent of the presence or sufficiency
of other causes. For example, we have,
Cold very commonly (p=0. 9) causes
sneezing.
Allergy very commonly (p=0. 9) causes
sneezing.

Causal independence can be expressed as:
Cold does not affect the tendency of
allergy to cause sneezing, and vice
versa.

In this case we also assume no leaks (Henrion,
1990), i.e. no "spontaneous" occurence of the

effect in the absence of explicitly modelled causes:
There is no other cause of sneezing
than cold and allergy

In other cases we cannot rule out leaks, for
example:
Cat as often as not (p=O.S)

causes

paw marks.
Dog as often as not (p=O. S)

causes

paw marks.
There are also other very unlikely
(p=O.l) causes of paw marks.

The latter assertion gives the leak probability, that
is the probability of paw marks given no cat or dog.
Note that the former two assertions do not give the
simple conditional probability of paw marks given
the cat (dog), but given also none of the "other
causes" mentioned in the last assertion.
The entire static description of the belief net used
in these examples is completed by the following
assertions:
Dog as often as not (p=O. S)
barking.

causes

Dog is the only cause of barking.

4 Qualitative belief propagation
The goal of qualitative belief propagation is to find
the direction of the impact of an observed variable
on the degree of belief in another variable,
whether increased, decreased, or unchanged (+0). Wellman (1988) presents a scheme for
qualitative probabilistic networks (QPNs) which
provides an appealing formal basis for this task for
arbitrary belief nets. Wellman's scheme uses an
inference algorithm for QPNs using arc reversal
and graph reduction, modelled on Shachter's
algorithms for inference in quantitative belief
networks. However, human qualitative belief
propagation appears to trace the impact of
evidence locally from node to node, which seems
more reminiscent of the quantitative belief
propagation or message-passing algorithms
developed by Pearl and others than the reduction­
type algorithms.
Wellman (1988) provides a persuasive argument
for first-order stochastic dominance (FSD) as the
best formal interpretation of the informal notion of
the sign of an influence, whether knowledge of A
being true (high) increases or decreases belief in
B being true (high). Thus a positive influence of
binary variable A on variable B is defined thus:
I+(A, B)<=> P(b Ia y) 2: P( b I

a

y), 'Vy

[1]

15

(NB: We use the convention that uppercase letters
denote variables, and lowercase their values: a
means A is false.) Signs may be assigned to arcs
in the network either by direct assessment or, for
qualitative explanation of quantitative reasoning,
as an abstraction from quantitative assessments.
In the sneeze example, it is intuitively clear (and
consistent with the numbers used in the example)
that all influences are positive. For simplicity, we
will assume the network is singly connected and
limit ourselves to binary variables.

4.1 Three types of Inference
As a prerequisite to describing the algorithm for
qualitative belief propagation, we must first
distinguish the three types of inference: Predictive
(or causa� inference is in the same direction as
the original qualitative influence. Diagnostic
inference is in the reverse direction. lntercausal
inference gives the qualitative impact of evidence
for one variable A on another variable B, when
both have influences on a third variable C, about
which we have independent evidence. These
three situations are illustrated below in Figure 4.

•,.�-....

--....

Predictive
inference

Diagnostic
inference

But propagating across convergent arrows is less
straightforward: If there is no diagnostic evidence
for the common effect (or direct observation), e.g.
for C, then the two influencing variables A and B
are of course independent, and so knowledge
about A has no effect on B. On the other hand, if
we observe C (or have diagnostic evidence for it)
A and B become dependent. Thus, intercausal
inference is not a simple concatenation of
predictive and diagnostic inference. While there
has been much informal discussion of "explaining
away" (a form of intercausal reasoning) (Henrion,
1986; Pearl, 1988), a precise characterization
seems to be lacking of the general conditions
under which explaining away or other qualitative
intercausal reasoning applies. So we now turn to
this issue.

4.2 Qualitative lntercausal Influence
"Explaining away" applies when A and B are two
alternative causes of C, for example if the
influence of A and B on C is a noisy OR. Given
evidence for C, then evidence for A generally
produces a reduced belief in B. But what if the
influence is not a noisy-OR? What precisely are
the conditions on the influence of A and B on C
under which this qualitative pattern applies? Figure
5 presents the question schematically.

..

lntercausal
inference

16(E, A) & 16(A, B) => ll>*l>(E, B)
Diagnostic inference is similar to predictive
inference, although a little more complicated since
variable A inherits any relevant predecessors of B
in inverting the direction of the arrow. If we want to
propagate the effect of evidence across two
divergent arrows, we can simply chain the
diagnostic and predictive inference. Observation of
pawmarks increase belief in the cat, which in turn
increases belief in the allergy.

I
I
I
I
I
I
I
I
I
I

Figure 4: Three types of Inference.

Qualitative predictive inference is quite simple. If
we have positive evidence E that increases our
belief in A, and the influence of A on B is positive,
then E should also increase our belief in B
(actually, not decrease it, given Wellman's weak
definition of the direction of influence). More
generally, concatenation (chaining) produces an
influence whose sign is the product of the signs of
its component influences.

I

I
I

Observed

Figure 5: Schematic of the operation of qualitative
lntercausat reasoning

I

•Theorem (qualitative lntercausal Influence):
Suppose we have three logical variables, A, B. C.
all with non-zero priors, where A and B can
influence C, and A and B are marginally
independent (there are no paths between A and B
other than through C). Suppose C is observed to
be true. Then a positive qualitative influence exists
between A and B, i.e.

I

P(b I a y) � P( b I

a

y),

'Vy

[2]

I
I
I
I

16

I
I
I
I

���� ) ��e the predecessors of B other than A, if
ny

P(c I a b x) P(c I a 6 X) 2: P(c I a 6 X) P(c I a b X),
'Vx [3]
where x are the predecessors of C other than A
and B. The qualitative influence from A to B is
zero or negative, i.e. we replace the 2: by= or::;;; in
[1], if we replace the 2: in condition [3] by or ::;;;
respectively. •
=

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

This result is easily derived from [2] by the
application of Bayes' rule. Condition [3] is
reminiscent of Wellman's (1988) definition of
qualitative synergy, but instead of the multiplicative
form he employs the additive form (for binary
variables),
P(c Ia b x)+P(c Ia 6 x) 2: P(c Ia 6 x) + P(c Ia b x),
'Vx [4]
It can be shown that if either or both of the
influences from A to C and from B to C are
positive, then multiplicative synergy [3] implies
ad�itive synergy [4]. It is also easy to show that
no1sy-OR gates are subsynergistic for product
synergy, just as Wellman showed they are
subsynergistic for additive synergy. Hence, if
P(CjA,B) is a noisy O R gate and C is observed
present, there is a negative influence between A
and B. So any further evidence for B will decrease
belief in A, and vice versa, since they provide
alter�ative explan�t.ions for C. In other words [3] is
prec1sely the cond1t1on under which explaining
away applies.
Note that if the influence has positive multiplicative
synergy, A has a positive influence on B, the
�nverse of. explaining away. For example, if the
Influence 1s a "leaky noisy AND gate", in which C
has an increased chance of occurring if A and B
both occur, then given C, knowledge of A may
increase belief in B. For example, suppose that
the presence of flammable material and an ignition
source together can cause combustion, which in
turn may cause smoke (which also has other
possible causes). Observation of smoke can
create a positive influence from flammable
material to the ignition source.

in the cold, given we already have observed
sneezing and pawmarks? Since both the cases of
convergent influences (sneezing and paw marks)
are noisy ORs with observed effects, explaining
away applies, that is they can be reduced to
negative influences between the causes. We can
generate a trace of the explanation thus:
Observe sneezing and paw-marks.
Impact of barking on cold?
1.

Observation of barking is

evidence for dog.
2. Increased probability of dog
helps explain paw marks, and so
weakens evidence for cat.
3. Reduced probability of cat
reduces probability of allergy.
4. Reduced probability of allergy
reduces ability to explain sneezing,
and so increases probability of
cold.
In summary, observation of barking
increases probability of cold.

This illustrates all three kinds of propagation. Step
1 involves simple diagnostic inference over a
positive influence. Step 2 involves intercausal
inference, producing a negative influence from dog
to cat. Step 3 involves simple predictive inference,
propagating negative evidence over a positive
influence. And step 4 involves intercausal
inference, producing another negative influence
from allergy to cold. Since there are two positive
steps and two negative steps (the intercausal
inferences), the chaining produces a cumulative
positive influence between barking and cold as
shown in Figure 6. We should point out that the
scheme as described is limited to singly
connected networks of binary variables.

4.3 An example of quaiHatlve propagation

We illustrate how these ideas may be applied to
provide qualitative explanation using the sneeze
example, somewhat like that provided by some of
our subjects. Consider question (c) from above,
how does observation of barking affect our belief

Figure 6: Propagation of qualitative
probabilistic Inference.

17

5 Scenario-based explanations

Whereas inference schemes using propagation
operate on a belief network representation of
knowledge, scenario-based explanations are
based on scenario trees (also known as probability
trees, or decision trees without the decision
variables). Each path from root to an end node
represents a scenario, or sequence of events.
The psychological literature suggests that it may
be easier to understand scenarios if they are
presented as coherent causal stories. So in an
attempt to make scenarios easier to grasp, we can
order the events in a scenario so that effects follow
their causes, and employs causal conjunctions to
link them when appropriate, for example:
No cold; cat causes allergy, which
causes sneezing.
Dog causes barking and paw marks; no
cat, hence no allergy; cold causes
sneezing.

In some scenarios, an event may deviate from
what is expected, having a low probability given
its predecessors. Even though a cat is present
there may be no allergic reaction. In such cases,
we can aid interpretation by indicating such
surprises by an exception conjunction such as
"but" for an event with low conditional probability:
No cold; cat,
no sneezing.

but no allergy,

hence

The probability of each scenario is the product of
the conditional probabilities of all the events in it.
Exact Bayesian inference to find the posterior
probability of an event can be performed by
looking at the ratio of the sum of the probabilities
of all scenarios compatible with the event to the
sum of all those not compatible, after eliminating
all scenarios not consistent with the observations.
The number of possible scenarios is generally
large of course, and cognitively unmanageable.
But fortunately, it is often possible to understand
the essentials of what is going on by examining
only a few of the most probable scenarios.
The following is a simple scenario-based
explanation from the sneeze example. We ask it
to explain the probability assigned to cold given
sneezing has been observed:
why cold
Given:
Sneezing must have been caused by
cold or allergy.

The following scenario(s) are
compatible with cold:
A. Cold and no cat hence no
allergy
Other less probable
scenario(s)
The following scenario(s)

I
0.47
0. 06

No Cold and cat causing
allergy

I
I

are

incompatible with cold:
B.

I

0. 48

Scenario A is about as likely as
scenario B (0. 47/0. 48)
because cold in A is a great deal
less likely than no cold in B
(0. 08/0. 92) '
although no cat in A is a great deal
more likely than cat in B (0. 9/0. 1) .
Therefore cold is slightly more
likely than not (p=0. 52) .

QIQ first displays what is known and what can be
definitely inferred from it. It then gives a list of one
or more scenarios which are compatible with the
target variable (cold) and a second list of
scenarios which are incompatible with it. Since
currently our only observation is sneezing, the
variables paw marks, dog, and barking are
irrelevant, and so the scenarios mention only cold,
cat, and allergy.
In general there may be a vast number of possible
scenarios (exponential in the number of uncertain
variables), so it only gives the most probable
one(s) in each list. The rest are grouped as "other
less probable scenario(s)", those which collectively
contribute less than 15% of the overall probability
for that list. This parameter can be varied to
control the length and precision of the explanation.
The next part of the explanation compares the
probabilities of the most important pairs of
scenarios in terms of significant differences in the
probabilities of their component events. Any
contrasts that are significant (probabilities differing
by a factor of more than 1 .2 in the default option)
are mentioned in explaining the relative
probabilities. The explanation lists, after
"because", the contrasts favoring the more
probable s�enario, and then, after "although", the
contrasts, if any, supporting the other scenario.
This scheme is based on the principle that it is
easier to judge the relative probability of two
scenarios by comparing their differences than by
judging their absolute probabilities. The
co'!lparisons use the relative probability phrases
calibrated against numerical probability differences
by Elsaesser & Henrion (1990). In this case the

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

18

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

relatively low probability of cold in scenario A is
just about balanced by the low probability of cat in
scenario B. Another possible scenario which has
both cold and cat (hence allergy) is not even
mentioned, because having two very unlikely
events its relative probability is negligible relative
to the two scenarios each with a single unlikely
event. So, it is contained in the "other less
probable scenario(s)" group.
A second example explanation assumes that paw
marks and barking as well as sneezing have been
observed:
why

cold

Given:
Sneezing must have been caused by
cold or allergy.
Paw Marks could have been caused by
cat or dog or another unknown cause.
Barking must have been caused by
dog.
The following scenario(s) are
compatible with cold:
A. Cold and no cat hence no
0. 38
allergy and dog
B. Cold and cat causing allergy
and dog
0. 05
Other less probable scenario(s) 0.01
Total probability of cold

0. 44

The following scenario(s) are
incompatible with cold:
C. No Cold and cat causing
allergy and dog

0. 56

Scenario A is much more likely than
scenario B (0. 38/0. 05)
because no cat in A is a great deal
more likely than cat in B
(0.90/0. 10) .
Scenario A is somewhat less likely
than scenario C (0. 38/0. 56)
because cold in A is a great deal
less likely than no cold in C
(0. 08/0.92) 1
although no cat in A is a great deal
more likely than cat in C
(0. 90/0. 10) .
Therefore cold is fairly unlikely
(p=0. 44) .

Note that several techniques are provided to
abstract and simplify the explanation. First, only

relevant events are considered, that is events
whose consideration affects the target probability
given available observations. Second, only those
scenarios that contribute more than 10% of the
probability to the target event (or its complement)
are listed explicitly. This can·drastically simplify the
explanation, since many real cases seem to be
like the example above, where a few (two to four)
scenarios turn out to have the bulk of the
probability, and the vast mass can be ignored
without significant error. Thirdly, in comparisons of
the relative probability of pairs of scenarios, only
those events with substantially different
probabilities are mentioned.
Additional abstraction techniques could provide
further simplification. Some linked variables might
be combined so that they can be treated as one.
For example, allergy might be combined into cat,
considering cat to cause sneezing directly. This
reduction of variables can reduce the number of
distinct scenarios and also the complexity of each
scenario. Another improvement in a decision
context would be to consider the importance of a
scenario in terms of expected utility rather than
simply probability. In a medical context, low
probability scenarios leading to death may have a
stronger claim to be listed explicitly than higher
probability scenarios with less interesting
consequences.
There is considerable psychological evidence for
the prevalence of scenario-based reasoning in
human thinking, and of the importance of
coherent, causal stories (e.g. Pennington & Hastie,
1988). The psychological literature has focussed
generally on the ways in which this leads to
systematic distortions and biases in probabilistic
judgment. Our approach here is to generate
explanations matched to human preferences for
explanatory stories, but to select and present the
scenarios in such a way that they provide a
reasonable guide to the relative probabilities of
interest.

6 Conclusions
We have outlined a variety of approaches to
explaining probabilistic knowledge, including
combinations of graphic, textual, and numeric
information, and two new schemes for explaining
probabilistic reasoning. Any specific explanation
will be too verbose for some, too brief for others,
and simply confusing for yet others. The art in
designing a good explanation is to match the style
and focus to the skills and interests of the person
to whom you are explaining. Users are generally
the best judge of their preferences and interests,
and so it is important to provide them with levers to

19

control the style and completeness, such as the
probability cut-off parameters mentioned in
scenario generation. Providing both phrases and
numbers should please most people, and may
even give numerophobes the opportunity to
become familiar with the relationships.
Qualitative belief propagation and scenario-based
explanations will be appealing in different
_ �tion seems to wort<weJI in
situations. ProQ
cases with smgly c()nne]ted:neiworks and. strQ.ng
·q"Lia!irative influe11ces� .:T:he analysis of qualitative
intercal.Jsal·reasoning we have presented provides
a principled basis for understanding some
important patterns of intuitive reasoning, as well as
extending formal methods for qualitative
probabilistic reasoning. While belief propagation
provides intuition into the direction of the impacts
of evidence, in its purely qualitative form it
provides little guidance about the magnitude of
effects or probabilities. Sc�nario-based
explanations can work with muitlpfy connected
networf<�:Scenario�based reasoning also appears
to offer a natural way of reconciling probabilistic
reasoning to possible-worlds logic-based
approaches to reasoning. Both schemes lend
themselves to convenient abstraction and
simplification, which are essential in generating
comprehensible explanations.
__

This paper summarizes work from an initial
exploratory phase of our research program into
explaining probabilistic reasoning. There is
considerable scope for developing and refining of
these techniques. Initial experimental evaluation
suggests the value of such explanations in
improved user insight (Druzdzel, 1990). But a
more definitive understanding of the merits of such
schemes must await more extended empirical
comparisons of their effectiveness under a variety
of conditions.
Acknowledgements

We are most grateful to Michael Wellman for
insightful comments.



that we developed, the dynamic network tool that we
designed and implemented to aid knowledge engineering

We present several techniques for knowledge
engineering of large belief networks (BNs) based

for

CPCS, and the Bayesian network algorithms that we

employed for this large, complex network.

on the our experiences with a network derived
from a large medical knowledge base. The noisy­
MAX, a generalization of the noisy-OR gate, is

used to model causal independence in a BN with

knowledge engineering or evaluation, challenging. The

multivalued variables. We describe the use of leak

development of CPCS necessitated the implementation of
algorithms that could make inference in BNs of this size

probabilities

to

enforce

the

closed-world

assumption in our model. We present Netview, a

more efficient. An example of this is a generalization of the

visualization tool based on causal independence
and the use of leak probabilities. The Netview

noisy-OR gate [Cooper 1986; Peng and Reggia 1987; Pearl

software

model causal independence. The

allows

knowledge

engineers

to

dynamically view subnetworks for knowledge
engineering, and it provides version control for
editing a BN. Netview generates sub-networks in
which leak probabilities are dynamically updated

1

The CPCS is one of the largest BNs in use at the current
time, and its sheer size makes most tasks, such as

1988] that is commonly used in binary valued networks to

CPCS BN contains nodes

that are multivalued, for example, disease nodes may have
four values (absent, mild, moderate, severe), consequently
we use a generalization of the noisy-OR gate called the

noisy-MAX. The specification of a complete conditional
m with sm values and n
predecessors requires the assessment of (sm -l)IJ�=I s;

to reflect the missing portions of the network.

probability matrix for a node

INTRODUCTION

probabilities, where

s;is

the

number

of

values

predecessor i (for a binary network this reduces to

of

2n ) In
.

Given the relative maturity of algorithm development in the

contrast, the causal independence assumption in the form of

Bayesian reasoning community, attention is now starting to
focus on applying these algorithms to real-world

a noisy-gate reduces this assessment task to

probabilities. thereby simplifying knowledge

applications.

and greatly reducing storage requirements.

The Quick Medical Reference-Decision

Ln(sm -l)s;

dcquisition

(QMR-DT) project seeks to develop practical
decision analytic methods for large knowledge-based

To aid in the editing and refinement of the

systems. The first stage of the project converted the

have developed a network visualization tool we named

Theoretic

Internist-1 knowledge base [Miller, Pople et al. 1982]

(QMR 's predecessor) into a binary, two-layered belief

CPCS BN, we

Netview. The Netview tool provides dynamic views of the

BN, and can generate subnetworks by taking advantage of

1991; Shwe,

the noisy-MAX and leak assumptions. For inference, the

Middleton et al. 199 1]. In the second stage of the QMR-DT

network, or subnetworks generated by Netview, are sent to
the IDEAL package [Srinivas and Breese 1989] for

network (BN) [Middleton, Shwe et al.

project we are creating a multilayer belief network with
mu1tiva1ued variables, and developing efficient inference
algorithms for the network. This paper will concentrate on
the knowledge engineering issues we faced when building a
large multilayered BN.
To create a large multilevel, multivalued BN we took
advantage of a rich knowledge base, the Computer-based

inference. Netview is a flexible tool which can be used in
any BN that uses noisy-gates, and is described in section

5.1.
Like the Internist- 1 -derived B N, the CPCS BN uses leak
probabilities [Henrion 1988] to represent the chance of an
event occurring when all of its modeled causes are absent.

We discuss our use of leak probabilities,

and the

Patient Case Simulation system (CPCS-PM), developed
over two years by R. Parker and R Miller [Parker and

modifications to the leak probabilities required by the

Miller 1987] in the mid-1980s as an experimental extension

dynamic network tool, in section 5.2.

of the Internist-! knowledge base.
This paper makes contributions both in knowledge
engineering and in algorithm development and
implementation for large BNs. We describe the CPCS BN

2

KNOWLEDGE BASE TO BELIEF NETWORK

The CPCS-PM system is a knowledge base and simulation
program designed to create patient scenarios in the medical

Knowledge Engineering for Large Belief Networks

sub-domain of hepatobiliary disease, and then evaluate
medical students as they managed the simulated patient's
problem. Unlike that of its predecessor lnternist-1, the
CPCS-PM knowledge base models the pathophysiology of
diseases th e intermediate states causally linked between
diseases and manifestations. The original CPCS-PM system
was developed in FranzLisp. Diseases and intermediate
pathophysiological states (IPSs) were represented as Lisp
frames [Minsky 1975].

485

5

and were mapped to probability values, as described in
next section. Frequency weights from the CPCS-PM
were mapped to probability values based on previous work
in probabilistic interpretations of Internist-1 frequencies.

the

-

To construct the BN we converted the CPCS-PM knowledge
base to CommonLisp and then parsed it to create nodes. We
r e pr esented diseases and IPSs as four levels of severity in
the CPCS BN-absent, mild, moderate, and severe.
Predisposing factors of a disease or IPS node were
represented as that node's predecessors, and findings and
symptoms of a disease or IPS node as the successors for that
node. In addition to the findings, CPCS contained causal
links between disease and IPS frames, we converted these
links into arcs in the BN. Frequency weigh t s [Shwe,
Middleton et al. 1991] from the CPCS-PM ranged from 0 to

We generated the CPCS BN automatically, we did manual
consistency checking using domain knowledge to edit the
network. Because the CPCS-PM knowledge base was not
designed with probabilistic interpretations in mind, we had
to make numerous minor corrections to remove artifactual
nodes, to make node values consistent and to confinn that
only mutually exclusive values were contained within a
node.
The resultant network has 448 nodes and 908 arcs (Figure
1). A t o tal of seventy four of the nodes in the network are
predisposing factors and required prior p robab ilities the
remaining nodes required leak probabilities assessed for
each of their values. We thus had to assess over 560
probabilities to specify the network fully.

Figure I. A small portion of the CPCS BN displayed in the Netview visualization program. The node ascending­
cholangitis in the third row shown in inverse has been selected by the user.

,

486

Pradhan, Provan, Middleton, and Henrion

While the CPCS-PM knowledge base is derived from the
Internist-! knowledge base it has been significantly

3

augmented by inclusion of the IPS states, and multivalued

2

representations of both diseases and manifestations of
disesase. The original

QMR-BN transformation of the

Internist-! knowledge base used only binary valued disease
and manifestation nodes. While conceptually simple, this

0

approach does not adequately reflect the potential variation
in presentation of disease manifestations, or the severity of

3

1

0

diseases.

Figure 2.

A node x with

predecessors D i• n,

NOISY-OR

ordered states

The noisy-OR is a simplified

BN representation that

respectively, or disease and manifestation variables
respectively [Peng and Reggia

1987].

variable

variables or predecessors

X

the

variables, and is typically described for a

which are interpreted as either cause and effect variables

that has

n

cause

•••

( 1)

P(X= 2! VI).

If there are multiple "manifestation" variables

j =1 ,.. .,l,

Consider an effect

D1, ,Dn· The noisy-OR can be used when

\tf having

probabilities required to calculate

probability matrix. The noisy-OR is defined over a set of
two level network partitioned into two sets of variables,

E

{0,1,2,3 }. The

shaded area represents the

requires far fewer parameters than the full conditional­
binary-valued

3

q

GENERALIZATION OF THE NOISY-OR

3.1

2

Aj,

P(Xi =xi I VI)=1- II(l-pii)

each D has

i:D;eV1

an activation probability p; being sufficient to produce the
effect in the absence of all other causes, and

(2)

the

probability of each cause being sufficient is independent of

1988].

the presence of other causes [Henrion

In this paper, we define variables using upper-case letters,

X is
ilx. If variable X is present it is denoted using the letter x;

letters.

The domain of possible values for variable

if it is absent, it is denoted using x .
The activation of a variable

X

by a predecessor variable D;

is independent of the value of D;.
assumption,

X is

conditional probability given by
other

Under the noisy-or

activated when D;

P;

is active,_with a

= P(x I d; {'·dk).
'

In

words, this activation probability deh otes the

probability when
inactive.

where Pij is the link probability on the arc from D; to

lJi is active and all other predecessors are

For a two-level noisy-OR network, we define a set V of
cause or disease variables, and a set W of effect or

manifestation variables. If there is a set VI of V of
predecessors of

X

e

W that are present, then since the D;

X will be false when all D; are false:

P(X=X I VI)= II P(D; =d;)
i:D;eV1

P(X =X I Vj)

=

BN

application, since we need to accommodate n-ary variables.

For example, CPCS BN disease and IPSs can take on values
such as absent, mild, moderate, and severe.
3.2

NOISY- MAX

Consider a generalization of the noisy-OR situation in
which each variable is allowed to have a finite discrete state
space (rather

than just a binary state s pace). This

generalization was first proposed by [Henrion

1988], but he

did not describe the algorithmic details. In developing this
generalization, we assume that we have a set

V of

predecessor variables D1, ... ,Dn. Consider first the case
where we have a variable

in VI are independent,

Xi

The simple noisy-OR is insufficient for the CPCS

and values that variables can take on using lower-case

for

then we obtain

X with a subset

present, with the predecessors indexed by

V1 of V that are

i,j, ,q.
...

The variable domains in CPCS BN are all partially ordered,

for example, {absent, mild, moderate, severe}, and it turns
out that such a partial ordering is necessary for all variable
domains. For the remainder of our work we assume that all
variables have ordered domains.
We now want to compute

II

(1- P;).
i:D;eV1

The value xis given by
In other words,

From this, it is simple to derive

domain values

P(X=xiVI)=l- II(l-p;).
i:D1eV1

x =

max(i,j, .
.

.

,q) [Henrion 1988].

X takes on as its value the maximum of the
of

its predecessors,

predecessors are all independent.

given

that

the

Knowledge Engineering for Large Belief Networks

487

V1)

This computation of P(X ==xI
can be viewed as setting
up a hypercube of dimensions i x j x · · x q and summing
·

the cells, each of which contains a probability value

fljk··q P;jk··q. As an example of the derivation of the general
formula, we consider the case of two predecessors D; and
{}_j. If these variables take on values i,j respectively, then
the

probability P(X =xI V1) ,

where

x = max(i, j) .

For

example, Figure 2 graphically depicts the conditional

probability matrix for D; and Dj, b oth of which have

ordered states {0, 1,2,3}. If x=2, then
consists of the shaded squares of the grid.

P(X=21D;.Dj)

Figure 3. Explicit representation

of the leak probability as a cause
of X.

In this multivalued noisy-MA X situation, the probabilities

that are being calculated in these hypercubes are cumulative

probabilities, that is,

P(X s; xI D s; d) .

For notational

convenience, we define the cumulative probability for a
variable

X that has a s ingle predecessor D with maximum
d as:

domain value

\f(x I d)= P(X s; xI D s; d) .
Under the generalized noisy-OR assumption, for a given
variable

X

with a set of

which each D;

q

D1, ... , Dq
d;, we know that

predecessors

has maximum value

for

4

LEAKS

As in any other knowledge representation scheme, the BN
representation suffers from incompleteness, in that it

typically cannot model every possible case. A leak variable

can be used to enforce the closed-world assumption [Reiter
1978]. The leak variable represents the set of causes that
are not modeled explicitly.

A leak probability is assigned as

the probability that the effect will occur in the absence of

of the cause s

D1 , ...,D n that are explicitly modeled. If
the leak variable is explicitly modeled, then it can be

any

treated like any other cause and can be depicted as such

=

IJ 'P(xld;),
i:D;eV1

In this representation, the leak node is always

assumed to be "on", that is
If

Note that using this transformation, we can define the

probability assigned to

3).

(Figure

i:D;E�

P(L=true) = 1.0.

the leak L with value l is factored into the calculation of

the unconditional probability for variable X, then we obtain

X taking on a particular value Xk :

P(X s; x) = P(L s; l)

TI [p;P(D; s; d;)+(1- P;)],

i:D,�V

Explicitly representing leak nodes in the

The unconditional probability assigned to a variable can be

derived in an analogous fashion. First, we nee d to derive

D;. assuming no
1994], this is given

the unconditional probability of variable
predecessors. As described in [Provan
by

P(X $. x) = P(L s; 1)

IJ[p;P(D; s; d;) +(1- P;)]P(x I V1).

i:D,eV1

P(XS:x) =

conditioning parents. The Netview knowledge engineering

tool, described in section 5, facilitates the maintenance and

editing of leak probabilities. Netview stores leak values

separately, as if they were explicit nodes, and then inserts

the leak values into the appropriate probability
exporting a network for inference in IDEAL.

5

The unconditional probability is given by

IJ[p;P(D; :S:d;)+(l-p;) ].

i:D;EV

The unconditional probability assigned to X taking on a
particular value x is:

CPCS BN would

almost double the size of the network, so leaks are
implicitly represented in the probability tables of a node's

5.1

tables when

TOOLS FOR KNOWLEDGE ENGINEERING
NETVIEW:

A TOOL FOR NETWORK

VISUALIZATION AND EDITING

Verification and refinement of the structure of the CPCS BN
is an important part of the QMR -Df project for two reasons.
First, because the

CPCS BN was generated from a pre­

existing knowledge base. Second, the effect of model
structure on network performance and accuracy is an
important aspect of the QMR -Df project.

During the knowledge engineering process, it became

V1)

Using this approach, the value P(xI
can be computed in
time proportional to the number of predecessors in V1. This
generalized noisy-MAX has been implemented in IDEAL.

obvious

that

available tools were not suitable for

visualizing and editing a network the size of
(Figure

1).

CPCS BN

In particular, most tools only permit a static

488

Pradhan, Provan, Middleton, and Henrion

view of the network, a limitation that made editing the

disease," and so on. A node may have any number of
semantic labels. Semantic labeling is a useful technique for

CPCS network very hard.

filtering nodes to focus attention during knowledge
by allowing knowledge engineers to focus on portions of

engineering. It is possible, say, to focus only on "gastric"
findings and diseases when dealing with the appropriate

the network. The program is implemented in Macintosh

domain expert. In the future we will also use the semantic

Netview was created to help knowledge engineering efforts

CommonLISP 2.01. The main features of Netview are
o

dynamic network layout

o

semantic labeling of nodes

o

version control

•

subnetwork generation and dynamic

o

labels in the dynamic layout algorithm to improve the
appearance of subnetwork views.
It is useful to keep track of modifications while editing the

BN. To facilitate this, Netview includes basic version

leak modification

control to store deleted and added arcs and nodes and

leak value editing

changes to probability tables. Arc and node additions and
removals between versions are displayed through the use of

Because of the causal independence assumptions implied
by the use of the noisy-MAX and noisy-0

different colors.

R gates,

knowledge engineers are can select smaller parts of the

5.2

CPCS BN for viewing. Netview allows the user to view all
ancestors, all predecessors, or all ancestors and
predecessors of selected nodes. For example, in Figure l
while the node

ascending-cholangitis is selected (inverse

color), we can use Netview's ability to show all successors
and predecessors of the selected node or nodes, resulting in
the subnetwork view shown in Figure 4 . Other options
include viewing nodes' Markov blanket, and immediate
successors or predecessors.
Netview uses a dynamic layout algorithm to display the
selected nodes. The knowledge engineer is able to move
rapidly between views by selecting nodes and choosing
viewing options, or by retrieving previously saved views.
Quickly viewing a node's predecessors allows rapid
assessment of leak probabilities.
In addition to subnetwork selection, Netview allows
semantic labeling of nodes, and filtering based on semantic
labels. For example, nodes in CPCS BN are labeled "lab
finding," "symptom," "sign," "disease," "IPS," "liver

5.2.1
The

SUBNETWORK GENERATION AND DYNAMIC
LEAK MODIFICATION

Subnetwork generation
Netview

program

is

used

only

for

network

visualization and editing; Netview saves files in IDEAL
format for inference. Because of the size of the CPCS BN it

is not always desirable to send the entire network to

IDEAL

for inference. If we are only interested in verifying a small
set of diseases we can generate a subnetwork including
only those diseases of interest and their associated findings,
IPSs and predisposing factors. When we run test cases

against a subnetwork we don't require the system to
compute the posterior probabilities of diseases that we are
not interested in.

5.2.2 Dynamic leak modification
Subnetworks we select from the full

CPCS

BN using

Netview can be exported to IDEAL for inference. It is

possible to select subsets of the larger CPCS

BN for

i n f e r e n ce

leaks .

due

to

the

presence

of

Figure 4. A subnetwork of the CPCS BN displayed in Netview. This view comprises all predecessors and
successors of the node ascending-cholangitis.

Knowledge Engineering for Large Belief Networks

Figure 6. Subnetwork creation. Node D3is removed from the network, the value of the leak node
updated to p based on the probability of D3•

489

/, p is
,

'

When a subnetwork is saved Netview updates the leak
probabilities to take into account the missing diseases. In
the CPCS BN the node hepatomegaly has parents shown in
figure 5, The leak probability for hepatomegaly is therefore
calculated based on this set of predecessors. In figure 4 a
subnetwork was selected based on the ancestors and
predecessors of the disease ascending-cholangitis. Conse­
quently, the only parent of hepatomegaly in the subnetwork
is ascending-cholangitis, its other parents are not included.
The transformation of leak probabilities required during
subnetwork creation is shown in Figure 6. The leak
probability must be updated from p to p'. This updating is
done in order to preserve the total probability mass. If the
value I of L is updated to a value l' for new leak L', we can
compute the updated leak node probability as

BN, and we are exploring how much the network
performance changes when the assumption is relaxed.
5.2.3 Information metrics

When subnetworks are created some information is lost as
parts of the network are excluded. A future area of research
is to use Netview to calculate the information Joss of a
subnetwork based on information metrics [Provan 1993],
and to compare differences in posterior probability between
the complete network and the subnetwork which has been
selected.

6

RELATED WORK

The generalization of the noisy-OR was first proposed in
[Henrion 1988], and the derivation and implementation
p' = P(L '5.lA D3 '5.d3)
described here follow that original proposal. Two related
= P(L '5.l)P(D3 '5.d3)
generalizations are described in [Srinivas 1993] and [Diez
1993]. The generalization of the noisy-OR by Srinivas is
=[p3P(D3 '5.d3)+(l- p3)]
different to this proposal, and is intended for a different
application. This generalization is for circuits (or other such
If we want to combine a set of Q nodes into a leak node,
devices) which can be either functional or non-functional.
where each node d1 in Q has link probability, then the new
In the case of medicine, findings can take on values such as
leak node probability is given by:
{absent, mild, moderate, severe}, in which case the binary
generalization of Srinivas is insufficient to deal with
p' =P(L'5.lAD1 '5.d1A···ADq '5.dq)
arbitrary n-ary variables. The noisy-MAX generalization in
D i ez 1993] is virtually identical to the one described here,
[
P(L '5./) fl P(Dt '5. d;)
and
we have derived our noisy-MAX independent of that in
i:D1eQ
[Diez 1993). Also, the proposal in [Diez 1993] is described
=P(L-5./) n[p;P(D;'5.d;)+(l-p;)].
within the context of learning models for OR-gates, and its
i:D1eQ
application to inference in Bayesian networks is not directly
apparent.
We prove in [Provan 1994] that if the network is
To our knowledge, there is no other tool which allows
hierarchical and there are no arcs between nodes at the
dynamic selection of subsets of Bayesian networks. There
same level of the hierarchy, then the leak updating is sound,
are several graphical tools for creating Bayesian networks,
that is, the probability assigned to X given the new set of
including IDEAL Edit, Ergo, Hugin [Andersen, Olesen et al.
predecessors is the same as the probability assigned to X
1989], and Demos [Morgan, Henrion et al. 1987]. But these
with the original predecessors. This proof holds if the
tools do not provide dynamic network layout and do not
subnetwork consists of a Markov blanket of a node, all
have features aimed at knowledge engineering large BNs.
predecessors and successors of a node, or all successors of
a node. The assumption for the proofs holds for the CPCS
7 CONCLUSION
=

��Figure 5. Parents of the node hepatomegaly.

In this paper we have presented several methods for
representing, and a software tool for managing, large BNs
based on our experience with the CPCS BN. The noisy-MAX
is a generalization of the noisy-OR gate for multivalued

490

Pradhan, Provan, Middleton, and Henrion

variables which reduces the complexity of the knowledge
acquisition task and storage requirements for a network.
Leak probabilities are used in the CPCS BN to model causes
other than those explicitly modeled in the network.

Middleton, B., Shwe, M. A., et al. (1991). Probabilistic
diagnosis using a reformulation of the Internist-1/QMR
knowledge base-11. Evaluation of diagnostic performance.
Methods of Information in Medicine, 30:256-67.

Based on the causal independence assumptions of the
noisy-MAX, and the use of leak probabilities we have
developed Netview, a tool for visualizing BNs based on the
dynamic layout of subnetworks, and which also provides
basic version control for editing networks. The creation of
subnetworks allows for more efficient knowledge
engineering, and for easier verification of the B N. We
describe a technique for updating leak probabilities based
on the excluded parents of a node in subnetworks.

Miller, R. A., Pople, H. E. J., et al. (1982). Internist-1: An
experimental computer-based diagnostic consultant for
general internal medicine. N Engl J Med, 307:468-476.

Recent advances in creating BNs from pre-existing data or
knowledge bases will result in networks that are larger and
more complex than those created manually. We believe that
the techniques described in this paper are important to
facilitate the management and verification of such
networks.
Acknowledgments

This work was supported by NSF Grant Project IRI9120330, and by computing resources provided by the
Stanford University CAMIS project, which is funded under
grant number LM05305 from the National Library of
Medicine of the National Institutes of Health.
The authors would like to thank K. C. Chang and R. Fung
for their graph layout algorithm on which NetView's layout
method is based, and R.Miller for access to the CPCS
knowledge base.



Intercausal reasoning is a common inference

pattern involving probabilistic dependence of

causes of an observed common effect. The
sign of this dependence is captured by a qual­
itative property called product synergy. The
current definition of product synergy is insuf­
ficient for intercausal reasoning where there
are additional uninstantiated causes of the
common effect. We propose a new definition
of product synergy and prove its adequacy for
intercausal reasoning with direct and indirect
evidence for the common effect. The new def­
inition is based on a new property matrix half
positive semi-definiteness, a weakened form
of matrix positive semi-definiteness.

1

INTRODUCTION

Intercausal reasoning is a common inference pattern
involving probabilistic dependence of causes of an ob­
served common effect. The most common form of
intercausal reasoning is "explaining away" (Henrion,
1986; Pearl, 1988), which is when given an observed
effect, and increase in probability of one cause, all other
causes of that effect become less likely. For example,
even though the use of fertilizer early in the spring
and the weather throughout the growing season can be
assumed to be probabilistically independent, once we
know that the crop was extremely good, this indepen­
dence vanishes. Upon having heard that the weather
was extraordinarily good, we find that the likelihood
that an efficient fertilizer had b ee n used early in the
spring diminishes - good weather "explains away"
the fertilizer. Although explaining away appears to
be the most common pattern of intercausal reasoning,
the reverse is also possible, i.e., observing one cause
can make other causes more likely. We call both types
of reasoning "intercausal," although strictly speak­
ing it is not necessary that the variables involved
are in causal relationships with one another and our
subsequent analysis captures probabilistic rather than
causal conditions. In fact, two variables a and b will

Max Henrion
Rockwell International Science Center
Palo Alto Laboratory
444 High Street, Suite 400
Palo Alto, CA 94301
henrion@camis.stanford. edu

be in an "intercausal relationship" given a third vari­
able c if they are independent conditional on a set of
variables \]!, but dependent conditional on every set cl>
such that c E cl>. This is captured by the graphical
structure of a Bayesian belief network in which a and
b are direct predecessors of c and there is no arc be­
twe en a and b. The applications of intercausal reason­
ing include algorithms for belief updating in qualita­
tive proba bilistic networks ( Druzdzel & Henrion, 1993;
Henrion & Druzdzel, 1991), appr oximat e search-based
algorithms for BBNs ( Henrion, 1991), and automatic
generation of expl an ations of probabilistic reasoning in
decision support systems (Druzdzel, 1993).
Intercausal reasoning has been captured formally by
a qualitative property called product synergy (Henrion
& Druzdzel, 1991; Wellman & Henrion, 1991). The
sign of the product synergy determines the sign of
the intercausal influence. Previous work on intexcausal
reasoning, and product synergy in particular, concen­
trated on situations where all irrelevant ancestors of
the common effect were assumed to be instantiated.
In this paper we propose a new definition of product
synergy that enables performing intercausal reason ing
in arbitr ary belief networks. We prove that the new
definition is sufficient for intercausal reasoning with
the common effect observed and is also sufficient for
intercausal reasoning with indirect support when the
common effect variable is bin ary.
The influence of indirect evidential support on inter­
causal reasoning in the binary common effect case has
been studied before by Wellman and Henrion (1991)
and by Agosta (1991). Our exposition deals with the
general case including uninstantiated predecessors of
the common effect variable and, therefore, advances
insight into intercausal reasoning beyond what has
been presented in those papers. Another difference
between this and Wellman and Henrion's exposition is
that here we provide more insight into the functional
dependences between nodes in intercausal reasoning.
We improve their theorem listing the conditions for
intercausal reasoning with indirect evidential support.
We generalize Agosta's analysis of intercausal reason­
ing from the binary case. Our analysis of Conditional
Inter-Causally Independent (CICI) node distribu tio ns

318

Druzdzel and Henrion

shows that there is a large class of relations for which
non-trivial evidential support can leave their direct an­
cestors independent.
All random variables that we deal with in this paper
are multiply valued, discrete variables, such as those
represented by nodes of a Bayesian belief network. We
make this assumption for the reasons of convenience in
mathematical derivations and proofs.
Following Wellman ( 1990), we will assume that all con­
ditional probability terms are well defined and those
that appear in the denominators are non-zero. This
assumption is easily relaxed at the cost of explicatory
complexity.
Lower case letters (e.g., x) will stand for random vari­
ables, indexed lower-case letters (e.g., x;) will usually
denote their outcomes. In case of binary random vari­
ables, the two outcomes will be denoted by upper case
(e.g., the two outcomes of a variable c will be denoted
by C and C). Outcomes of random variables are or-.
dered from the highest to the lowest value. And so,
for a random variable a, Va<j (a; ;:::: aj]· For binary
variables C > C, or true>false. Indexed lower case
letter n, such as na denotes the number of outcomes
of a variable a.

Definition 1 (qualitative influence) We say that
a positively influences c, written s+(a, c), iff for all
values a1 > a2, co, and x,
Pr (c;:::: colatx);:::: Pr (c ;:::: co!a2x).

This definition expresses the fact that increasing the
value of a, makes higher values of c more probable.
Negative qualitative influence, s-, and zero qualitative
influence, S0 , are defined analogously by substituting
;:::: by � and = respectively.
Definition 2 (additive synergy) Variables a and b
exhibit positive additive synergy with respect to vari­
able c, written Y+( {a, b }, c), if for all at > a2, bt > b2,
x, and co,
Pr(c � colatbtx) + Pr(c � cola2b2x)
;:::: Pr(c;:::: colatb2x) + Pr(c;:::: co!a2b1x).

The additive synergy is used with respect to two causes
and a common effect. It captures the property that the
joint influence of the two causes is greater than sum of
their individual effects. Negative additive synergy, y-,
and zero additive synergy, Y0 , are defined analogously
by substituting � by � and = respectively.

We will use bold upper-case letters (e.g., M) and bold
lower-case letters (e.g., x) for matrices and vectors re­
spectively. Elements of matrices will be doubly in­
dexed upper-case letters (e.g., M;j ).
The remainder of this paper is structured as follows.
Section 2 reviews the elementary qualitative proper­
ties of probabilistic interactions, as captured in quali­
tative probabilistic networks. Section 3 demonstrates
the problem of sensitivity of the previous definition of
product synergy to the probability distribution over
the values of uninstantiated direct ancestors of the
common effect node. Section 4 proposes a new def­
inition of product synergy that is provably sufficient
and necessary for intercausal reasoning and studies the
properties of intercausal reasoning when the evidential
support for the common effect is direct and indirect.
We discuss intercausal reasoning in Noisy-OR gates in
Section 5. Detailed proofs of all theorems can be found
in the appendix.,
2

QUA LITATIVE PROBA BILISTIC
NETWORKS

Qualitative probabilistic networks (QPNs) (Wellman,
1990) are an abstraction of Bayesian belief networks
replacing numerical relations by specification of quali­
tative properties. So far, three qualitative properties of
probability distributions have been formalized: qual­
itative influence, additive synergy, and product syn­
ergy. Since we will refer to them later in the paper, we
reproduce the definitions of these properties here after
(Wellman & Henrion, 1991).

Figure 1: Intercausal reasoning between a and b with
an additional predecessor variable x.
Definition 3 (product synergy I) Let a, b, and x
be the predecessors of c in a QPN (see Figure 1).
Variables a and b exhibit negative product synergy
with respect to a particular value c0 of c, written
x-({a, b}, c o), if for all al > a2, bt > b2, and x,
JOr(cola1b1x)JOr(cola2b2x)
�
Pr(colathr)Pr(cola2btx).

Positive product synergy, x+, and zero product syn­
ergy, X0, are defined analogously by substituting s
by ;:::: and = respectively. Note that product synergy
is defined with respect to each outcome of the common
effect c. There are, therefore, as many product syner­
gies as there are outcomes in c. For a binary variable
c, there are two product synergies, one for C and one
for C. The practical implication of product synergy
is that, under the specified circumstances, it forms a
sufficient condition for explaining away.
3

UNINSTANTIATED A N CESTOR
NODES

If a and x are both predecessors of c, with conditional
probability distribution Pr(cjax), then the relation be-

Intercausal Reasoning with Uninstantiated Ancestor Nodes

tween a and c depends on x. In other words, the prob­
abilistic influence of one variable on another may de­
pend on additional variables. Hence, the qualitative
properties defined above contain the strong condition
that they must hold for all possible instantiations of x.
If the "irrelevant" node x is uninstantiated, x will not
affect the signs of qualitative influence or additive syn­
ergy. But, surprisingly, it turns out that unobserved
predecessors may affect the product synergy.
We will present an example showing that the presence
of uninstantiated predecessor variables can affect the
intercausal relation between other parents and explain
informally the reasons for that effect. The example of a
simple BBN with binary variables, the associated con­
ditional probability distribution of the common effect
node, and the resulting qualitative properties of the in­
teraction between the variables, are given in Figure 2.
The qualitative properties of the interaction among a,
b, c, and x are all well defined. In particular, product
synergy I for C observed is for a, b, and x pairwise
negative. Still, for some distributions of x, for exam­
ple for Pr(X) = 0.5, the intercausal influence of a on
b is positive (see Figure 3).
Quantitative conditional distribution:
X

Pr(Ciabx)
A
A

B

B

B

B

0.8

0.6

0.2

0.0

I 0.99 I 0.8 I 0.99 I 0.2 I

Qualitative properties:
s+(a,t)
s+(b,c)
s+(x, c)

x-({a,b},C)
x-({a,x},C)
x-({b,x},C)

Figure 2: Example of the effect of an uninstantiated
predecessor node x on intercausal reasoning (see Fig­
ure 1). All pairwise product synergies for C observed
between a, b, and x are negative and all influences of
a, b, and x on c are positive.
Pr(AIBC)-Pr(A[C)

319

nomenon. The sign of intercausal interaction between
a and b is a function of the probability distribution
of x. This function is not linear (it will become ap­
parent in Section 4 that it is quadratic) and the fact
that the function has the same sign at the extremes
does not guarantee the same sign in all the points in
between. In the example above, we are dealing with
negative signs at the extremes (i.e., for Pr(X) = 0
and Pr(X) = 1) and a positive sign for some interval
in between (see Figure 3).
4

PRODUCT SYNERGY II

A key objective for any qualitative property between
two variables in a network is that this is invariant
to the probability distribution of other neighboring
nodes. This invariance allows for drawing conclusions
that are valid regardless of the numerical values of
probability distributions of the neighboring variables.
As we have shown in the previous section, this does not
apply to product synergy as previously defined. In this
section, we propose a new definition of product syn­
ergy that will have this property. The new definition
of product synergy is expressed in terms of a condition
that we term matrix half positive semi-definiteness.
4.1

MATRIX HALF POSITIVE
SEMI-DEFINITENESS

Half positive semi-definiteness is a weakened form of
positive semi-definiteness (see for example (Strang,
1976)). A square n x n matrix M is positive semi­
definite if and only if for any vector x, xTMx 2: 0.
M is half positive semi-definite if the above inequality
holds for any non-negative vector x.
Definition 4 (non-negative matrix) A matrix is
called non-negative (non-positive) if all its elements
are non-negative (non-positive).
Definition 5 (half positive semi-definiteness) A
square n x n matrix M is called half positive semi­
definite (half negative semi-definite) if for any non­
negative vector x consisting of n elements xTMx 2: 0
(xTMx � 0}.

The following theorem addresses the problem of test­
ing whether a given matrix is half positive semi­
definite.

-0.04

Figure 3: The intercausal interaction between a and b
as a function of probability of x.
We propose the following explanation of this phe-

Theorem 1 (half positive semi-definiteness) A
sufficient condition for half positive semi-definiteness
of a matrix is that it is a sum of a positive semi-definite
and a non-negative matrix.

It can be easily shown that the condition is also nec­
essary for 2 x 2 matrices.
Theorem 2 (2

x

2 half positive semi-definiteness)

320

Druzdzel and Henrion

A
necessary
condition
for
half
pos­
itive semi-definiteness of a 2 x 2 matrix is that it is
a sum of a positive semi-definite and a non-negative
matrix.

We can prove this condition also for 3 x 3 matrices. We
conjecture that this condition is true for n x n matrices,
although so far we have not been able to find a general
proof.
Conjecture 1 (half positive semi-definiteness)
sufficient and necessary condition for half positive
semi-definiteness of a square matrix is that it is a sum
of a positive semi-definite and a non-negative matrix.

A

Given Theorem 1, we are still left with the problem
of decomposing a n x n matrix into a sum of two ma­
trices of which one is positive semi-definite and the
other is non-negative. It can be easily shown that this
decomposition is not unique. It seems that a practi­
cal procedure for determining whether a matrix is half
positive semi-definite needs to be based on heuristic
methods. It is easy to prove that half positive semi­
definiteness necessitates 'V; (Dii :::; 0] (consider a vector
x in which only Xi is non-zero). The first test for any
matrix is, therefore, whether the diagonal elements are
non-negative. The heuristic methods might first check
whether the matrix is positive semi-definite by study­
ing its eigenvalues, pivots, or the determinants of its
upper left submatrices. Another easy check is whether
the matrix is non-negative. For any quadratic form,
there exists an equivalent symmetric form, so the ma­
trix will be non-negative if and only if all its diago­
nal elements are non-negative and 'V;j [Dij + Dji � 0],
i.e., the sum of each pair of symmetric off-diagonal
symmetric elements is non-negative. If both tests fail,
one might try to decompose the matrix by subtracting
from its elements positive numbers in such a way that
it becomes positive semi-definite. The subtracted el­
ements compose the non-negative matrix. As already
indicated, this decomposition is not unique.
4.2

PRODUCT SYNERGY II

Definition 6 (product synergy II) Let a, b, and x
be direct predecessors ofc in a QPN (see Figure 1). Let
n, denote the number of possible values of x. Variables
a and b exhibit negative product synergy with respect
to a particular value co of c, regardless of the distri­
bution of x, written x- ({a,b},co), if for all a1 > a2
and for all b1 > b 2, a square n, x n, matrix D with
elements

Dij

=

Pr(coia1blx;)Pr(coia2b2xj)
- Pr(coia2b1x;)Pr(coia1b2xj).

is half negative semi-definite. If D is half positive
semi-definite, a and b exhibit positive product syn­
ergy written as x+({a,b},c0). If D is a zero ma­
trix, a and b exhibit zero product synergy written as
X0 ({a,b },co).

Note that although the definition of product synergy II
covers the situation in which there is only one unin­
stantiated direct predecessor of c, it is easily extensi­
ble to the general case. If there are more than one
uninstantiated direct predecessors, we can conceptu­
ally replace them by a single uninstantiated variable
with the number of outcomes being the product of the
number of outcomes of each variable separately. This
is equivalent to rearranging the conditional distribu­
tion matrix of c.
Unless specified otherwise, in the remainder of this
paper we will use the term product synergy meaning
product synergy II. As product synergy I is a special
case of product synergy II, we propose to adopt this
convention in future references to this work.
4.3

INTERCAUSAL REASONING

The following theorem binds product synergy with in­
tercausal reasoning in case when the common effect
has been observed.
Theorem 3 (intercausal reasoning) Let a, b, and
x be direct predecessors of c such that a and b are
conditionally independent. A sufficient and necessary
condition for s- (a,b) on observation of Co is negative
product synergy, x-({a,b},co).
4.4

INTERCAUSAL REASONING WITH
INDIRECT EVIDENCE

The following theorem binds product synergy with in­
tercausal reasoning in case of indirect support for a
binary common effect.
Theorem 4 (intercausal reasoning) Let a, b, and
x be direct predecessors of c, and c be a direct prede­
cessor of d in a network. Let c be binary. Let there
be no direct links from a or b to d (see Figure 4).
Let X51({a,b},C), X52({a,b},C), Y53({a,b},c), and
S54(c,d).
If 84 = + and 81 = 83, then 851 (a,b) holds in the net­
work with D observed. If 84 = - and 82 =/= 83, then
S52 (a,b) holds in the network with D observed.

X

X01({a,b},C) Y03({a,b},c)
X52({a,b}, C) S54(c,d)
Figure 4: Intercausal reasoning with indirect evidence.
dis observed, x is uninstantiated.

Intercausal Reasoning with Uninstantiated Ancestor Nodes

Theorem 4 is an improvement on the theorem pro­
posed by Wellman and Henrion (1991, Theorem 6),
capturing additional conditions under which the sign
of intercausal inference with indirect support can be
resolved.
Pr(AI BD) -Pr(AID)

It turns out that Noisy-OR gates are robust against
the effect of uninstantiated predecessor variables dis­
cussed in Section 3. The conditional probability dis­
tribution of Noisy-OR gates always results in half neg­
ative semi-definite matrices used in the definition of
product synergy and, effectively, the probability distri­
bution of predecessor nodes never impacts intercausal
reasoning.
5.1

Figure 5: The intercausal interaction between a and b
as a function of the evidential support for c for various
values of qualitative properties of interaction between
a and b.
Figure 5 shows the magnitude of intercausal interac­
tion between a and b as a function of indirect evidential
support for C, for different values of qualitative prop­
erties of interaction between a and b. The strength of
evidential support is expressed by..\, the likelihood ra­
tio of the observed evidence (..\= Pr(DIC)/ Pr(DIC)).
The intercausal influence between a and b is, as ex­
pected, always zero for ..\ = 1.0 (no evidential sup­
port). ..\ = 0 corresponds to perfect evidence against
C (in other words, C is implied by D). ..\ = oo corre­
sponds to perfect evidence for c (in other words, C is
implied by D). In the proof of Theorem 4, we demon­
strate that the interaction is quadratic in ..\ and each
of the curves has at most two zero points (one of these
is a trivial zero point, for ..\ = 1.0). This result is
in agreement with Agosta's (1991) finding that inter­
causal conditional independence in binary variables is
possible at most at one state of evidence.
The product synergy and the additive synergy deter­
mine exactly the interval where the second zero point
falls. The product synergies between a and b for C
and C determine whether the curve is above or below
zero for ..\ = 0 and ..\ = oo· respectively. The additive
synergy helps to locate the second zero point of the
curve. If the evidence is positive (..\ > 1.0), and the
additive synergy is equal to the positive product syn­
ergy, then the second zero point is for ..\ < 1.0. If the
evidence is negative (0 :::; ..\ < 1.0), and the additive
synergy is not equal to the negative product synergy,
then the second zero point is for..\ > 1.0.
5

NOISY-OR DIST RIBUTION S

Noisy-OR gates (Pearl, 1988) are a common form of
probabilistic interaction used in probabilistic models.

321

UNINSTANTIATED PREDECESSOR
VARIABLES

We will demonstrate the behavior of a leaky Noisy-OR
gate c with direct binary predecessors a, b, and x (see
Figure 1). Let p, q, and r, be the inhibitor probabilities
(Pearl, 1988) for nodes a, b, and x with respect to the
node c and l be the leak probability. This determines
the elements Dij of the matrix D (see Definition 6) to
be
Du
D12
D21
D22

-(1- l)pq(1- r)
-(1- l)q(p + (1- p)r)
-(1- l)q(p-r)
-(1 - l)pq

It is easy to verify that D11 :::; 0 and D22 :::; 0. Also,
D12 + D21 = - (1-l)pq(2- r) � 0 ,
which shows that irrespective of the actual values of p,
q, r, and l, a symmetric form of the matrix D is non­
positive and, by Theorem 1, half positive semi-definite.
Binary Noisy-OR gates will, therefore, always exhibit
negative product synergy for the effect observed, re­
gardless of presence or absence of uninstantiated pre­
decessor variable x .
Pr(AIBD)-Pr(AID)

Figure 6: The intercausal interaction between a and
b as a function of the evidential support for c in a
Noisy-OR gate.

5.2

INDIRECT EVIDENCE

As the product synergy given effect observed to be ab­
sent is equal to zero ( i.e., for any Noisy-OR gate we
have X0 ( {a, b}, C)), and the product synergy given ef­
fect observed is negative (i.e., for any Noisy-OR gate

322

Druzdzel and Iienrion

x- ({a,b}, C)), Equation 13 (see the proof of Theo­
rem 4) reduces to
(,\ - 1),\ IX - ({a, b}, C) I $ 0 .
The two zero points of this expression with respect to
,\ are for ,\ = 0 and ,\ = 1. We know that there are no
other zero points (as shown in the proof of Theorem 4) ,
and it follows that intercausal influences in Noisy-OR
gates will always be negative for ,\ > 1 ( positive evi­
dence) and positive for 0 < ,\ < 1 (negative evidence)
(see F igure 6).
6

CONCLUSION

The previous definition of product synergy does not
cover situations where there are additional uninstanti­
ated causes of the common effect. We have introduced
a new definition of product synergy and we proved its
adequacy for intercausal reasoning with common effect
directly observed and also intercausal reasoning with
indirect evidential support when the common effect is
binary. We introduced the term matrix half positive
semi-definiteness, a weakened form of matrix positive
semi-definiteness.

(2 x 2 half positive semi-definiteness)
sufficient and necessary condition for half positive
semi-definiteness of a 2 x 2 matrix is that it is a sum
of a positive semi-definite and a non-negative matrix.

Theorem 2
A

Proof:
It is easy to prove that for any quadratic
form, there exists an equivalent symmetric form, so
let M be a symmetric 2 x 2 matrix of elements a, b, c

If M is half positive semi-definite, we have for any
non-negative vector [x y]

This is equivalent to

(1)
ax2 + by2 + 2cxy � 0.
It is easy to prove that both a and b have to be non­
negative (consider vectors [x 0] and [0 y] respectively ).
Now, we distinguish two cases: (1) if c � 0, then M is
non-negative; (2) if c < 0, then either a > 0 or b > 0
(note that vector x [1 1] yields a+ b + 2c � 0, which
given c < 0 implies a+ b > 0). If a > 0, then we apply
vector x [b - c] and if b > 0, then we apply vector
x [-c a]. In both cases, we obtain ab-c2 � 0, which
is satisfied if and only if M is positive semi-definite.
We have shown that if a 2 x 2 matrix is half positive
semi-definite, then it is either non-negative ( case 1)
or it is positive semi-definite ( case 2). This condition

Intercausal reasoning is useful in qualitative schemes
for reasoning under uncertainty and, because of its
prevalence in commonsense reasoning, valuable for au­
tomatic generation of explanations of probabilistic rea­
soning. The new definition of product synergy allows
for intercausal reasoning in arbitrary belief networks
and directly supports both tasks. As probabilities are
non-negative and, in many cases, the condition of ma­
trix positive definiteness may be too strong, we suspect
that the property of matrix half positive definiteness
that we introduced in this paper will prove theoreti­
cally useful in qualitative analysis of probabilistic rea­
soning.

is actually stronger than the matrix being a sum of
a non-negative and a positive semi-definite matrices.
Such strong form of the condition does not hold for
0
3 x 3 matrices.

APPENDIX: PROOFS

Theorem 3 (intercausal reasoning)

Theorem 1 (half positive semi-definiteness) A

sufficient condition for half positive semi-definiteness
of a matrix is that it is a sum of a positive semi-definite
and a non-negative matrix.
Proof:
Let M M1 + M2, where M1 is positive
semi-definite and M2 is non-negative. Since positive
semi-definiteness holds for any vector x, and in par­
=

ticular for a non-negative one, a positive semi-definite
matrix M1 is also a half positive semi-definite. We
have therefore, xTM1x � 0. Also, a non-negative ma­
trix is half positive semi-definite, since any quadratic
form with all non-negative elements cannot be nega­
tive. We have therefore that xTM2x � 0. Sum of
two non-ne�ative numbers is non-negative, therefore
xTM1x + x M2x � 0. By elementary matrix algebra

$ xTM1x + xTM2x xT (M1 + M2)x xTMx.
This proves that M is half positive semi-definite. 0
0

=

=

=

=

=

Let a, b, and
x be direct predecessors of c such that a and b are con­
ditionally independent ( see Figure 3). A sufficient and
necessary condition for s-(a,b) on observation of co
is negative product synergy, x-({a,b},ca).

Proof:
have

s-(ab)

By the definition of qualitative influence we
{::}

'Vt'tlbl>b2

Pr(a > ailb1co) $ Pr(a > ailb2co). (2)

This is equivalent to
'VNbt>b2

i-1
L[ Pr(ajlb1co)- Pr(ajlb2co)] $0.

j=O

Expansion of both components by Bayes theorem

lntercausal Reasoning with Uninstantiated Ancestor Nodes

n.,

and subsequent simplification yields

- L Pr(cola2b1xp)Pr(xp)
p:=O

'r/;'rlb,>b2

i-1 na

( LL Pr(aj)Pr(ak)(

�

Pr(colakbl)Pr(ak)

q:=O

)

Pr(colakb2)Pr(ak) $ 0.

We multiply both sides by the denominator and, for
the sake of brevity, introduce term A defined as follows
Amn

=

Pr(colambi) Pr(coJanb2)
- Pr(coJanbi)Pr(colamb2).

It is straightforward to verify that 'rim [Amm = 0) and
'rlm¢n [Amn = -Anm ]. Taking this into consideration,
we refine the summation indices, obtaining
i-1 na
L Pr(aj)Pr(a�:)Ajk $ 0. (3)
L
'r/;'r/b,>b2
j=O l:=i
The sufficient and necessary condition for the above to
hold for any distribution of a is
Vi< k Aik $ 0.
Note here that j < k and we can rewrite this inequality
as
'rla1>a2 A12 $ 0.
As 'r/; [Pr(a;) �OJ, sufficiency follows directly from
(3). We prove the necessity by contradiction. Sup­
pose that for some 61 > b2 there exist such a1 > a2
that A12 > 0. Consider a distribution of a in which
Pr(a!) > 0, Pr(a2) > 0, and Pr(a1)+Pr(a2) = 1. By
axioms of probability theory Vm¢t,m¢2 [Pr(am) = 0],
which reduces (3) to
Pr(a1)Pr(a2)A12 :S 0.
This implies that A12 is not positive, which contradicts
the assumption.
We have proven that the sufficient and necessary con­
dition for (2) is
Va1>a2 Vb1>b2

Pr(cola1b1)Pr(coia2b2)
- Pr(coia2b!)Pr(coia1b2) $ 0.

n.,

L Pr(colalblxm)Pr(xm)

m=O

n,

L Pr(cola2b2xn)Pr(xn)

n=O

After rearranging the summation operators, we get
n:a: n.t
L Pr(xm)Pr(xn)
L
'rla,>a,'1b1>b2
m=On=O
( Pr(cola1b1xm)Pr(cola2b2xn)
- Pr(coia2blxm)Pr(coiatb2xn)) $ 0,
which is equal to
n:c nz

LE

Pr(xm)Pr(xn)Dmn $ 0. (5)
m=On=O
This can be written in matrix notation as
Va,>a, vb,>b2

Va,>a, vb,>b2

p

T Dp $ 0.

(6)

where p is a vector of probabilities of various outcomes
of x (p; = Pr(x;)), and D is a square matrix with
elements Dmn. Inequality (6) will hold for any vector
of probabilities p if and only if Dis half negative semi­
definite, which is exactly the condition for the negative
0
product synergy II.
Theorem 4 (intercausal reasoning) Let a, b, and
x be direct predecessors of c , and c be a direct prede­
cessor of d in a network. Let c be binary. Let there
be no direct links from a or b to d (see Figure 4).
Let X6t({a,b},C), X62({a,b},C), Y63({a,b},c), and
S6•(c,d).
I! 84 = + and 81 = 63, then S61 (a,b) holds in the net­
work with D o b s erve d. If 84 = - and 62 =f:. 63, then
362(a, b) holds in the network with D observed.
Proof:
Let na, nc, and nx denote the number of
possible values of a, c , and x respectively. By the
definition of qualitative influence

s-(ab)

<=>

Vi Pr(a > ai\btdo) S Pr(a

>

adb2do).

This is equivalent to
i-1

(4)

Note that this condition is equivalent to product syn­
ergy I if c has no other predecessors than a and b. In
order to express this result in terms of the conditional
distribution of c given all its immediate predecessors,
we introduce x into ( 4).
'rla1>a2 'r/b,>b2

n.,

L Pr(colalb2xq)Pr(xq) $ 0.

Pr(colaibr)Pr(colakb2)

i=Dk=O
-Pr(colakbl)Pr(colaib2)))/

(�

323

'r/;

L [Pr(aiib1do)- Pr(aiib2do)] $ 0.
j:O

Expansion of both components by Bayes theorem
Pr(aiJb.do)

_- Pr(d0!a;b.(r(a;)
Pr(do b.)

and simplification yields
i-1
Vi l: Pr(aj)
j:=O
Pr(doiaibt)Pr(do\b2)- Pr(do)ajb2)Pr(do\bt) <
-0.
Pr( dolbt)Pr(do/b2)

324

Druzdzel and Henrion

Multiplying both sides of the inequality by the denom­
inator, which does not depend on the summation index
and is positive, yields

i-1

It is straightforward to verify that Vm [Cmm 0] and
Vmtn [Cmn -Cnm]. Analogous conditions are valid
for Cmn· Taking this into consideration, we refine the
=

=

summation indices, obtaining

Vi L Pr(aj)( Pr(dolaibl)Pr(dolb2)
j=O
-Pr(dolaib2)Pr(dolb!))::::; 0 .
W e expand the formulas for Pr(d 0) using
nc
Pr(dolaib.) L Pr(doick)Pr(ckiaib.)
k=O
=

and

nc

no.

Pr(dolb.) L Pr(dolcm)L Pr(cmia11b.)Pr(an),
m=O
n=O
=

which, after rearranging the summation terms, yields

i-l no.

nc n c

Vi L L Pr(ai)Pr(an)L L Pr(dolck)Pr(dolcm)
j=O n=O
k=O m=O
( Pr(cklaib1)Pr(cmlanb2)
- Pr(ckiaib2)Pr(cmlanb!))::::; 0. (7)
For a binary c (i.e.,
the following form

nc =

2,

co C, c1 C), (7) takes
=

=

Vi I:�:� l::��o Pr(aj)Pr(ak)
Pr(doiC)Pr(doiC)( Pr(Ciaibl)Pr(Ciakb2)
- Pr(Ciaib2)Pr(C!akb1))
+ Pr(doiC)Pr(doiC)( Pr(Ciaibl)Pr(Ciakb2)
- Pr(Ciaib2)Pr(Ciakbl))
+ Pr(doiC)Pr(doiC)( Pr(Ciaib1)Pr(Ciakb2)
- Pr(Ciaib2)Pr(Ciakb1))
+ Pr(doiC)Pr(doiC)( Pr(Ciaib1)Pr(Ciakb2)
- Pr(Ciaib2)Pr(Ciakbl))::::; 0.
We divide both sides twice by Pr(diC) and substi­
tute A for the likelihood ratio Pr(d!C)/Pr(d!C). Re­
arrangement and simplification yields

Vi l::�:O�L:��o Pr(ai)Pr(ak)(A-1)
( ( A ( Pr(Claib!)Pr(Clakb2)
- Pr(Ciaib2)Pr(Ciakb1))
- ( Pr(Claib1)Pr(Clakb2)
- Pr(Ciaib2)Pr(Ciakb1)))::::; 0 .
For the sake of brevity we introduce terms Cmn and
c mn defined as follows
Cmn Pr(Ciamb!)Pr(Cianb2)
- Pr(Clamb2)Pr(Clanh)
Cmn
Pr(Ciamb!)Pr(Cianb2)
- Pr(Ciamb2)Pr(Cianb1).
=

=

i-1 n,.
Vi LL Pr(ai)Pr(ak)(A - 1)(ACik-Cik)::::; 0.
j=O k=i

(8)
Note here that j < k. The sufficient and necessary
condition for the above to hold for any distribution of
a 1s
(9)
Vi<k (A - 1)(ACjk - Cik) ::::; 0 .
As Vi [ Pr(ai) 2: 0 ), sufficiency follows directly from
(8). We prove the necessity by contradiction. Suppose
there exist j and k such that (A-1)(ACjk-Cjk) >
0. Consider a distribution of a in which Pr(ai) > 0,
Pr(ak) > 0, and Pr(aj)+ Pr(ak) = 1 . By axioms
of probability theory Vm-:pj,m# [ Pr(am) = 0), which
reduces (8) to

Pr(ai)Pr(ak)(A - 1) (ACjk-Cik) ::::; 0.
implies that (A -1)(ACjk-Cjk) is negative,

This
which contradicts the assumption.

We have proven that the sufficient and necessary con­
dition for (8) is

Vi<k
(A-1) ( ( A ( Pr(Ciaib1)Pr(Ciakb2)
(10)
- Pr(Claib2)Pr(Clakbl) )
-( Pr(Ciaib1)Pr(Ciakb2)
- Pr(Ciaib2)Pr(Ciakbl)))::::; 0.
Substituting Pr(C) 1- Pr(C) in (11), and simpli­
=

fying yields an equivalent formula

Vi<k (A-1)
( (A - 1)( Pr(Claibl)Pr(Clakb2)
(11)
- Pr(Ciaib2)Pr(Ciakb!))
+ Pr(Ciaib!)- Pr(Ciakb!)
+ Pr(Ciaib2)- Pr(Ciakb2))::::; 0.
In order to express both results in terms of the condi­
tional distribution of c given all its immediate prede­
cessors, we introduce x into ( 1 1)

Vi<k

(.:\- 1)

nx n�

( A LL Pr(xm)Pr(xn)
m=O n=O
( Pr(Ciaib1xm)Pr(Ciakb2xn)
- Pr(Ciajb2xm)Pr(Ciakb1xn))
n.x
L Pr(xm)Pr(xn)
- mL
=Dn=D
( Pr(Ciaib1xm)Pr(Ciakb2xn)
- Pr(Ciaib2xm)Pr(Ciakb1xn)))::::; 0 .
n.x

·

Intercausal Reasoning with Uninstantiated Ancestor Nodes

The above can be written using matrix notation as
Yi<k

(>.- 1) (>.pT Dp- pTDp) S 0

( 12)

where p is a vector of probabilities of various outcomes
of x (Pi= Pr(x;)), D and D are square matrices with
elements Dmn for c = C and c == C respectively.
Replacement of the matrix expressions by the formulas
used for computing the value of product synergies from
the numerical distribution (we will denote the fact that
they are formulas and not the synergies by enclosing
them in straight brackets, e.g., fX 6� ( {a, b }, C) f) yields
Yi<k (.>.- 1)

(>. fx5•({a, b}, C) f- fX52({a, b}, C) f)::; 0. (13)
A similar procedure with respect to (12) yields
Yi<k (>.- 1)
n:c nx
( ,\ L L,: Pr(xm) Pr(xn)
m=On=O
( Pr(Ciajblxm)Pr(Ciak b2xn)
- Pr(Cjajb2xm)Pr(Cia�cblxn))

325

Richard Duffin for suggesting the term half positive
semi-definiteness. Professors Victor Mizel and Juan
Schaffer were the first to note the conditions for half
positive semi-definiteness. Professor Schaffer's sugges­
tion improved our proof of Theorem 2. Anonymous
reviewers provided useful remarks.


There has long been debate about the relative merits of decision theoretic methods and heuristic rule­
based approaches for reasoning under uncertainty. We report an experimental comparison of the
performance of the two approaches to troubleshooting, specifically to test selection for fault diagnosis.
We use as experimental testbed the problem of diagnosing motorcycle engines. The first approach
employs heuristic test selection rules obtained from expert mechanics. We compare it with the optimal
decision analytic algorithm for test selection which employs estimated component failure probabilities
and test costs. The decision analytic algorithm was found to reduce the expected cost (i.e. time) to arrive
at a diagnosis by an average of 14% relative to the expert rules. Sensitivity analysis shows the results are
quite robust to inaccuracy in the probability and cost estimates. This difference suggests some interesting
implications for knowledge acquisition.

1. Introduction

Although early work on automated diagnostic systems was much inspired by probabilistic inference and
decision theory (Ledley & Lusted, 1959;. Gorry & Barnett, 1968), many researchers later became
disenchanted with this approach. Reasons cited include the difficulty of obtaining the required numerical
probabilities and utilities, computational intractability, restrictive assumptions, and the apparent mismatch
between the quantitative formalism of decision theory with human reasoning (Szolovits & Pauker, 1978).
Thus, in the 1970s, this work was partly eclipsed by the development of AI approaches, which appearea
more tractable and compatible with human thinking. More recently, however, there have been signs of
renewed interest in the application of decision theoretic ideas in AI (Lemmer & Kanal, 1986; Horvitz,
Breese & Henrion, in press). This has partly been due to increased misgivings about the assumptions and
reliability of widely used heuristic methods for reasoning under certainty, and to the emergence of more
tractable approaches based on probabilistic representations and decision analysis
(e.g. Pearl, 1986;
Henrion & Cooley, 1987).
Our focus here is on the application of decision analysis to troubleshooting, and particularly to sequential
diagnosis, that is decisions about which diagnostic test to perfonn next and when to stop testing. We
defme the test sequencing task as the problem of finding the testing strategy that minimizes the total
expected cost. This task is central in any kind of diagnosis, from medicine to mechanical and electronic
devices.
If one has a causal model of the device that is entirely deterministic, logical analysis can identify possible
explanations of the observed faults (e.g. Reggia et al, 1983; Genesereth, 1984; Milne, 1987). Intuitively it
seems clear that test selection should involve some kind of balancing of the cost of the test against the
chance it will provide useful information. This suggests that it is desirable to quantify the degree of belief
in the logically possible explanations. Decision theory provides a way of quantifying the value of tests in
the form of the expected net value of information (ENVI). This is a measure of the informativeness of the
test, in terms of the expected value due to improved decision making, less the expected cost of performing
the test.
Gerry & Barnett (1968) demonstrated the application of this approach to test sequencing in medical
diagnosis. As they illustrate, the use of the ENVI can give one a voracious appetite for numbers. In

205

I
addition to the expected costs of the tests and quantifications of their diagnosticity, one requires the prior
probabilities of the alternative hypotheses (diseases), and the expected total cost to the patient for each
combination of treatment and disease (both correct and incorrect diagnoses). Further. the ENVI for
multiple tests are not additive, so the expected value of two tests is not the sum of their expected values.
Typically, the ENVI for each testing strategy must be computed separately. With many possible tests, the
number of strategies is vast For this reason, Gorry & Barnett used a "myopic" approach that considers
only one test at a time, rather than an entire strategy. This is suboptimal in general, but they found it to be
a good heuristic.

In many ways troublesho.oting of mechanical and electronic devices is much easier than medical

diagnosis. The usual end point is the positive identification of the faulty component to be repaired or
replaced, so the possibility of ultimate misdiagnosis can often be neglected. If all testing strategies result
in the same end point, the costs of treatment (or mistreatment) are irrelevant. Only the costs of performing
the tests are of concern when selecting tests. Typically these costs may be quantified relatively easily in
terms of the time required, that is the costs of the diagnostician's time and/or the machine downtime.
Further simplifying matters is the fact that there often exists a causal model of the device which is largely
or entirely deterministic. Such a luxury is not usually available to the medical diagnostician. Moreover,
for many machine components and subsystems, tests can determine with virtual certaintly whether they
are faulty. These aspects of troubleshooting make test selection far more amenable to a decision analytic
approach than in medicine. In some cases, the optimal test selection algorithm can be very simple, as we
shall see.
·

Our purpose in this work is twofold: First, to demonstrate the practicality of decision analysis for test
sequencing for troubleshooting; and second, to compare experimentall y the performance of this decision
theoretic approach with a more conventional expert system approach employing test selection rules
obtained from expert diagnosticians. After presenting the decision analytic formulation we will describe
its application to the example of diagnosing of motorcycle engines, to examine its performance in terms
of the expected cost (or time) to complete the diagnosis. Finally, we will examine the robustness of the
results, and discuss their practical implications.

2.

Decision analytic approach

We start by presenting a decision analytic approach to sequential diagnosis for a device representable by a
deterministic fault model. We will not develop the formulation beyond the generality required for the
experimental example.

2.1. Single level test sequence task

Suppose the system to be diagnosed consists of a set of elementary components, i from 1 to n. We
assume that each component is in one of two states, working or faulty, and that the system fails in some
observable way if there is a fault in any element Given the system failure, we assume that there is a fault
in exactly one component This is a reasonable assumption for most systems, where the chance of two
. elementary components failing simultaneously is negligible. We assume that, given the system has failed,
the probability of a component i failing is Pi· Since the failures are mutually exclusive, the probabilities
sum

to unity.

We further assume that for each component there is a corresponding test which will tell us for certain
whether it is working. The test might involve simple inspection, replacement of the component by an
identical one known to work, probing with electronic test gear, or a variety of other operations. We
assume that for each element i, the cost of testing is ci, independent of the test sequence. We assume for
simplicity that even if the fault is in the last element, which we can deduce if none others are faulty, we
will test it anyway for positive identification.
The test sequence task is to fmd the strategy that minimizes the expected cost to identify the faulty
component. It turns out that the optimal strategy is extremely simple: Select as the next element to test
the one that has the smallest value of the ratio C/Pi• and continue testing until the faulty element is
identified. We will call this the

CIP algorithm.

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

206

I

I
I
I
I
I

It is not hard to prove this result. Let Sjk be the strategy of testing each element i in sequence for i from 1
ton, until we find the faulty component We certainly will have to incur the cost C1 of testing the first
component e1. If that is working, we will have to test the second element, and so on. The probability that
the jth element having to be tested is the probability that none of its predecessors failed, which is also the
probability that either the jth element or one of its successors has failed, i.e. L.7=JPi· Let j and k be two
successive elements in this sequence, so k=j+ 1. Then the expected cost of strategy Sjk is:
�

I
I
I
I
I
I
I
I
I
I
I
I
I

11

=J

(1)

=lc

Let Skj be the same strategy, but with elements j and k exchanged. The expected cost of this strategy is:
11

I

11

11

E C(Sik)=C1 +C2!Pi+ ...+C/LPi+Ck"'):.Pi+ ... +C,;n
11

11

EC(Skj)=C1 +C2LPi+ ... +CJLPi+ Pj]+C}LPi- Pk]+ ... C,;n
i=2

i=k

i=j

The difference in expected cost between these two strategies is then:

EC(Sjk)-EC(Skj) =Cjpk- C kPj
Assuming the probabilities are positive, we get:

EC(Sjk)>EC(S�g)<=>C/Pj>Ck!Pk
In other words, strategy S1g is cheaper than Sjk if and only if the ClP value for element j is greater than the

C!P value fork. Thus any strategy with an element that has a higher C/P value than its successor can be
improved upon by exchanging the successive elements. So for an optimal strategy, all elements must be
in non-decreasing sequence of C/P value. This gives us our C/P algorithm.
This task is an example of what Simon & Kadane (1975) have called satisficing search. They show that a
similar algorithm is optimal for the related search task, in which the events that each element contains a
fault (or prize in their version) are independent rather than exclusive. They cite a number of similar
results, and provide an algorithm for the generalized task with an arbitrary partial ordering constraint on
the search sequence. See also Kadane & Simon (1977).

3. Experiment:
For the purposes of experimental comparison we chose the problem of diagnosing motorcycle engines.
This choice was guided by two reasons. Firstly we had available an existing knowledge base for
motorcycles. This allieviated the task of knowledge engineering. Secondly the motorcycle domain
provided us with a problem that was small and simple enough to be manageable but still promising
enough to make an interesting comparative study. The first goal was to explore the feasibility of
implementing it in a real-world task domain, and obtaining the numerical probabilities and test costs
required. The second goal was to evaluate its performance in terms of average cost to arrive at a diagnosis
compared to the test sequence rules derived from human experts.
3.1. The rule· based approach:

Given the task domain, we identified 5 commonly occuring symptoms. Symptoms are understood as any
deviations from expected behaviour. Symptoms can be directly observable or can be detected as a result
of some measurement(s). Associated with each symptom is a set of elementary faults which might cause
the given symptom. These elementary faults correspond to components that need to be repaired or
replaced by the mechanic. The symptoms along with their corresponding symptoms are listed in table
3-1. Associated with each symptom is an expert rule that specifies the sequence of elements to be tested
in that situation. These rules were obtained from extensive consultations with experienced mechanics and
a standard reference manual (Harley Davidson). While eliciting these rules we asked the experts to keep
in mind that the objective was to diagnose the fault as quickly as possible.
We interviewed three different mechanics. One of the mechanics refused to prescribe rules on the claim
that all symptoms were quite straightfoward and with appropriate audio visual tests the faulty component

207

I
Symptoms and Causes

I

Symptoms

Corresponding Causes

poor-idling -due-to-carburenor

idle-speed-adjustment, clogged-speed-jet,
air-leak-into-symptom, excess-fuel-fromaccelerating-pump

starts-but-runs-irregularly

def-ignition-coil, def-ignition-module, improper-timing,
air-cleaner/carburenor-adjustments. dirty-carburenor,
engine-problems

charging-system-fails

stator-grounded, stator-defective. rotor-defective,
def-regulator/rectifier

I

engine-turns-over-no-stan-no-spark

air-gap-on-trigger-lobes, ignition-coil,
circuit-between-battery-and-ignition-coil.
ignition-module-defective

I

engine-tums-over-no-stan-with-sparlc

sparlc-plugs. carburetion, advance-mechanism.
improper-timing

I

Table 3-1:

Symptoms and Causes

could be exactly located. Unfortunately he was not able to characterize these tests beyond asserting that
they are based on long years of experience. The other two experts did provide us with rules. Sometimes
for a given symptom these rules differed marlcedly between experts. This difference did not affect our
experimental study as we were only comparing the rules and C/p sequences for each given expert.

3.2. The decision analytic approach:

For purposes of comparison we applied the decision analytic approach to the same set of symptoms we
developed for the rule-based approach. We preserved all the assumptions made in section 2 for both
approaches. Firstly all variables (namely symptoms and components) are binary, either working or not
working. Both approaches also assume that there is only a single elementary component at fault.

The failure probabilities and test costs were obtailll!d by interviews from motorcycle mechanics of many
years' experience. The failure probabilities were assessed in groups. conditional on their common
symptom. Given a symptom, the approximate absolute probabilities for different components (which
cause the symptom) were assessed. The numbers were subsequently noimalized. The costs were
estimated as the average time in minutes the expert would take to deteimine whether each component was
working.
The. cost estimates between experts differed by upto a factor of three. The ordering of costs for a given
symptom was consistent between experts. The difference in the absolute cost estimates can be attributed
to personal differences. On the other hand, the probability estimates across experts was starkly different.
For a given symptom, even the relative ordering of failure rates of components did not match across
experts. One likely cause for this is the fact that different experts saw different samples of motorcycles.
The mechanics who worked at dealerships were more likely to service new bikes which they had recently
sold. Mechanics working in garages were more likely to see older bikes. As mentioned earlier these
differences are not critical for this study.
For each symptom, we applied the C/P algorithm to obtain a test sequence, which we will refer to as the
CIP sequence to distinguish it from the expert rule. We also showed the C/P sequences to the expert to
check real world feasibility.

I
I

I
I
I
I
I
I
I
I
I
I

208

I

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

3.3. Results

Using the estimated cost and failure probabilities, we computed the expected cost in minutes for
identifying the failed element for each symptom for both the expert and C/p sequences. Table 3-2
demonstrates the method used for comparison for a selected symptom for a specific expert.
Selected Symptom: Poor - Idling due to Carburettor

Components

Expert Rules

Cost(min)

Prob

C/p

C/p sequence

idle-speed-adjustments

1

15

.263

57

2

clogged speed jet

2

30

.105

206

3

air leak into system

3

15

.526

29

1

excess fuel from accelerating pump

4

30

.105

206

4

Expected Cost

50
Table 3-2:

32

Method for a selected symptom

For each given symptom, the expected cost of diagnosis for both the expert rule and C/p sequence is
tabulated in Table 3-3.
·

Expected Cost for different experts

Expert 2

Expert 1
Expert Rule

C/p sequence

Expert Rule

C/p sequence

poor-idling due to carburettor

8

8

50

32

startS-but-runs- irregular! y

24

17

43

36

charging-system-fails

13.5

13.5

7

7

engine-turns-no-start-no-spark

17.5

16.6

55

53

engine-turns-no-start-spark

18.5

9

26.5

25.2

Average Reduction

16.5
Table 3-3:

12

Expected Cost for different experts

The C/p algorithm provides a reduction in expected cost of diagnosis for 3 to 4 out of 5 cases, reducing
the diagnostic time by an average (across experts) by 14%.

3.4. Sensitivity Analysis

The failure probabilities and test costs are of course quite approximate, being subjective judgments by the
expert. An important question for any approach based on expert judgment, be it decision analytic or
heuristic, is how much the results depend on the precise numbers used. Is it possible that the expert rules
could in fact be optimal according to the C/P algorithm, if only we had obtained the expert's "actual"
probabilities and costs? Let us examine how much bias or error would there have to be in the assessment
process for this to happen.
For a given symptom, we associate with each component a probability distribution over the failure rate
and the cost of testing. Since the failure rate itself is a probability it's range is [0,1]. Therefore we
describe the failure rate as a logoddnormal distribution with the mean as the estimated value p and an
error factors. The range [mls,ms
. ] encloses 70% of the probability distribution. Similarly we associate a
lognormal distribution for the cost with mean as the estimated value C and an error factor s. Now the
difference between the expected cost for C/p sequence and the expert rule is itself a probability
distribution with s as a parameter. We assume that s is the same for all the distributions. Figure 3-1

209

I
shows the cumulative density function of the difference (of the expected cost between the C/p sequence
and expert rule) for a selected symptom with s=2. Figure 3-1 also graphs the boundaries for difference
2.5, the
which encloses 70% of the distribution around the mean. From this graph we see that for s
lower boundary intersects the zero-line. In other words if the error factor is 2.5 then the expert rule and
the C/p sequence have the same expected cost.
-

I

50

I
�
,.

0

-· · ·

· ··

I

· · ·

I

·50
0

50

100

=.e
Figure 3-1:

3

2

200

150

I

100

dmer-ln expert and C/p Mquence (minutes}

Cumulative probability

I

4

5

6

u�ty factor of the dlatributlon
1-ft.--- 50%

85%

-

- - -_-o-l_l_ne_

Sensitivity Analysis for "poor-idling'

I
I

Such an analysis for all symptoms, over all experts shows that the C/p sequence dominates the expert rule
for error factors up to 2.5. This suggests that the results obtained are fairly robust to errors in cost and
failure rate estimates.

I

3.5. Further consulations with experts

I

A major motivation for constructing fonnal models is the possibility that such models may improve on
the intuitive inference of the expert. Hence when the fonnal models lead to results different from those
suggested by experts, it becomes critical to explain these differences to the experts. If explanations (from
within the fonnal framework) are acceptable and the results insightful then the fonnal models have served
their purpose.
Keeping the above criterion in mind, we went back to the experts with the C/p sequences. Firstly, both
experts found the C/p sequences to be feasible. Hence the C/p sequences did not violate any implicit
realworld constraints. When it came to convincing the experts about the superiority of the C/p sequence
the results were mixed. Expert 2 was a little surprised at the difference between his sequencing and the
C/p sequencing. A careful explanation of the C/p algorithm convinced him of the dominance of the C/p
sequence. He readily agreed that the C/p sequence was better at minimizing the expected cost. On the
other hand expert 1 was not impressed by the C/p sequence. He felt that his sequences followed a causal
pathway while checking for faults which he thought was more desirable. This bias for causal paths made
him reject the C/p sequences.

4. Discussion

The results presented in section 3.3 suggest that the expected cost of diagnosis is significantly lower (at
least for some cases) for the decision analytic approach. This result can be understood in light of the
difference in test sequences obtained from human ex�rts relative to those derived from the C/p
algorithm. In order to explain and draw conclusions from this result we need to discuss three possible
shortcomings of the experimental setup.

I
I
I
I
I
I
I

210

I

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

The cost and failure rate estimates are approximate. Therefore it is possible that the inaccuracies in these
estimates might affect the test sequences derived from the C/p algorithm. However the sensitivity
analysis in section 3.4 shows that the results are quite robust to inaccuracies in the cost and failure rate
estimates.
The decision analytic approach sequences tests to optimize the expected cost of diagnosis. This approach
is in fact too simple to model other constraints on test sequences which might arise from the shape and
structure of the machine under consideration. As a result the C/p algorithm might in fact be ignoring
implicit but vital constraints on the test sequences. On the other hand the expert rule might have
implicitly accounted for such constraints and the difference in the two approaches might be attributed to
this factor. In order to check for this possibility, we presented the experts in question with the C/p
sequence for their comments. Specifically we were interested in the real world feasibility of the C/p
sequences. We encouraged the experts to suggest reasons as to why the expert sequences might be
preferable, so as to identify any implicit real world constraints that might be violated by the C/p sequence.
The C/p sequences used are ones that were found acceptable to the experts. We interpret this to mean that
the C/p sequences are indeed feasible ones.
The last question we need to worry about is whether the human experts (in the task domain) were actually
attempting to minimize the expected time for diagnosis. There is a real possibility that the experts in fact
had some other objective function. It is indeed more than plausible that experts have over time evolved
these sequences to maximize economic return for each given symptom. The rules might maximize the
time for diagnosis, subject to constraint that customer will not think it unreasonable.

5. Conclusions

The results of this study clearly indicate that the test sequences provide by the experts (in the task
domain) are suboptimal. Unfortunately there is uncertainty regarding the objectives which motivate the
expert test sequences. This restrains us from drawing firm conclusions about the efficacy of human
intuition for this task domain. But it is important to remember that one of the experts accepted the
validity of the C/p sequence and felt that the results are likely to be of practical interest. This suggests
that normative theories of decision making are capable of obtaining results which go beyond current
expert opinion.
This study also provides some valuable insights regarding knowledge acquistion for diagnostic expert
systems. Diagnostic problem solving can be understood as follows:
·

•

Given a symptom we have a set of components which can potentially explain the symptom.
The physics and the structure of the problem provides a partial order on the set of test
sequences allowable on the set of components. Since the partial order does not completely
constrain the test sequence, we can optimize the expected cost of diagnosis over this set of
feasible test sequences.

The knowledge required for this optimization could be acquired in two ways:

1. As an expert rule which picks one test sequence from this set of feasible test sequences.
This corresponds to the rule-based approach of our study.

2. We can push level of knowledge acquistion a level deeper and explicitly represent the cost
and failure rates.

These are once again assessed from experts.

This corresponds to the

decision analytic approach of our study.
Our study suggests that it possible to explicitly represent the cost and failure rates and it provides better
control on the objective of optimizing the expected cost of diagnosis.

I

Acknowledgements

I

This work was supported by the National Science Foundation, under grant IST 8603493. We are much
indebted to David Keeler, Woody Hepner and Larr)r Dennis for lending us their expertise as motorcycle
mechanics. We are grateful to Jeff Pepper and Gary Kahn of the Carnegie Group, Inc, for making
available the expert system, and to Peter Spines and J. Kadane for valuable suggestions.
·

I
I

211

I
I


 The expected value of information

(EVI) is

the most powerful measure of sensitivity to uncer­
tainty in a decision model:

it measures the potential

of information to improve the decision, and hence
measures the expected value of the outcome. Stan­

350 Cambridge Ave., #380, Palo Alto, CA 94306

model affect its outputs most significantly. In this way, it
helps a decision maker focus attention on what assump­
tions really matter. It also helps a decision modeler to
assign priorities to his efforts to improve, refine, or extend

dard methods for computing EVI use discrete vari­

his model by identifying those variables for which it will

that contain more than a few variables. Monte Carlo

more knowledgeable experts, or to build more elaborate

ables and are computationally intractable for models

simulation provides the basis for more tractable

evaluation of large predictive models with continu­
ous and discrete variables, but so far computation of

be most valuable to find more complete data, to interview
submodels.

EVI in a Monte Carlo setting also has appeared

The expected value of information (EVI) on a variable xi

based on preposterior analysis for estimating EVI in

information about xi and make a decision with higher­

impractical. We introduce an approximate approach

Monte Carlo models. Our method uses a linear

approximation to the value function and multiple
linear regression to estimate the linear model from
the samples. The approach is efficient and practical
for extremely large models.

It allows easy estima­

tion of EVI for perfect or panial information on

individual variables or on combinations of variables.
We illustrate its implementation within Demos (a
decision modeling system) , and its application to a
large model for crisis transponation planning.

measures the expected increase in value y if we learn new

expected value in light of that information. It is the most
powerful method of sensitivity analysis because it ana­
lyzes a variable's importance in terms of the overall pre­
scription for action, and it expresses that importance in the
utility or value units of the problem. Other methods, such
as rank-order correlation, express importance in terms of
the correlation between an uncertain variable and the out­
put of the decision model. There are many cases where a
variable can show high sensitivity in this way, yet still

1.0

EVI: What's so, and What's New

A ny model is inevitably a simplification of reality, and
most of its input quantities are

invariably uncertain. Sensi­

tivity analysis identifies which sources of uncertainty in a

have no effect on the selection of an optimal decision.
Deterministic perturbation measures importance in utility
or value units, but it ignores nonlinearities and interactions
among variables, and also fails to measure a variable's

importance in te rms of that variable's ability to change the
recommended decision.

120

Chavez and Henrion

One calculates EVPI (Expected Value of Perfect Infor·
mation) in discrete models by rolling back the decision

tree. The computation itself is straightforward in the sense

that, to compute EVI, one simply places at the front of the
tree the chance variables to be observed. The EVPI is
computed as the difference between the expected value
computed for this scenario and the expected value for the
regular tree, without observations.
Computing EVI with continuous variables is less intuitive,
because we have no tidy way of reversing the uncertainty,

2.0

Framework

A decision model consists of a set of n state variables

x1, ,xn, which we will denote by X. The decision maker
••

has control of a decision variable D, which can assume

one of m possible values d1 ,.. ,dm. The value or utility func­
tion v(X,di) expresses the payoff to the decision maker

when X obtains and decision di is chosen.

In a typical decision model, the state variables are uncer­

tain. We express prior knowledge about X in the form of a

unlike the discrete case. Yet continuous models are

probability distribution, denoted {XI �},where �denotes

increasingly the norm for risk and decision analysis, first

a prior state of information. The optimal Bayes' decision
1
maximizing the expected value is given by

because discretizing inherently continuous variables intro­

l = Arg

duces unnecessary approximation, and second because
Monte Carlo methods and their variants (e.g., Latin hyper­
cube) generate tractable, highly efficient solutions to pre­
dictive models that contain thousands of variables. An
especially useful feature of the Monte Carlo method is
that, for a specified error,the computational complexity

variable x , denoted d

d• x

[Morgan and Henrion, 1990]. Exact methods require com­
putation time that is exponential in the number of vari­

for computing EVI on continuous variables in a Monte
pute EVI on single variables or on any combination of

__

*

_.,

is

Arg max
(v(X,d)Jx,�)
d

EVPI (x) = (v (X, l x) I �)- {v (X, d*) I �).

In a similar fashion, we define the optimal expected­
value decision given the revelation of evidence e,

d*e• as

l =Arg max (v(X,d )Je.�)
e
d

variables, and (2) the ability to compute both perfect and
partial values of information. Perfect information
removes uncertainty entirely. Partial information reduces
uncertainty.

'

We define EVPI on x as

There is thus a need to develop flexible, efficient methods
Carlo setting. A ftexible method has (I) the ability to com­

d

l' )
(v ( X d) ..,
I .

The optimal decision given perfect information on state

increases linearly in the number of uncertain variables

ables.

max

Then the EVI for evidence e is

EVI(e) = (v(X,d.e)l�)-(v(X,l)l�)

We present a general framework for calculating EVI based
on preposterior analysis. Using that framework,we
develop a technique for computing EVI that depends on a
linear approximation to the value function and on multiple
linear regression to estimate the constants for the linear
function. We also discuss a heuristic method for measuring
the value of partial information in terms of what we call

2.1

Binary decisions and Function z

Let us consider a simplified decision problem with two
decision alternatives: one of them is the optimal Bayes'

d*; the other we denote tr".

the relative information multiple (RIM). We have

decision

implemented these methods in detachable computational

In view of the uncertainty in the state variables, there must

modules using Demos, a decision modeling system from

exist uncertainty in the outputs as well. Thus, for each

Lumina, Inc., Palo Alto, CA. We demonstrate their use on
a large model to aid in military transportation crisis plan­
ning.

1. We use Howard's inferential notation (see, for example,
Howard, 1970). {XIS) denotes the probability densit y of X condi­
tional on S; (XIS) denotes the expectation of X conditio nal on
S.

121

Efficient Estimation of the Value of Information in Monte Carlo Models

decision

d;, there exists a unique probability distribution

on value { v(X,d;)l �} (see Figure 1). For notational conve­

ing cY instead of

d'; its probability is just its correspond­

ing value on the density curve. Therefore, we have that

nience we let

v(d)

=

(v(X, d) I�).

EVPI

0

=

f lzl {zl �} dz.

(EQ 1)

We now define
z

=

v(X,l) -v(X,tf).

2.2

Preposterior analysis helps us to calculate the effect on X

Function z is the pivotal element in our framework for
computing EVI because it describes the difference in value
between the best and second-best decisions. In Figure
we have graphed the probability distribution of z. The

2,

shaded area represents the total probability of making a
bad decision, i.e., doing

d' when cY would yield higher

value. Exploiting information encoded in the shaded, neg­

ative portion of the z distribution's curve will provide the
necessary clues to compute EVPI and EVI.

FIGURE 1.

Pro

density

Preposterior Analysis

of our seeing evidencee, given a prior state of information

�. At the heart of preposterior analysis is the specification
of a preposterior distribution, which is a a prior proba­
bility distribution on a posterior mean. Probability theory
provides a principled basis for calculating a preposterior
distribution, given a prior and an adequate means of speci­
fying the effects of learning new information.
How do we represent perfect information on a continuous

Probability �istributions on value for the two
decisions d and cr-.

random variable

X? If X were known with certainty, then

its variance would be equal to 0. Thus, we can think of evi­

dence

e

as an information-gathering activity that somehow

reduces the variance of X to

{v(X,tf)[�} {v(X,l)[s}

0. Evidencee that provides

partial information reduces the variance on the prior of X,

without shrinking that prior to

0.

The following lemma, taken from basic probability theory,
is known as the conditional expectation formula.
Lemma 1:

(XIs>

=

((X] e)l �) .

A further useful result is the following lemma, which

FIGURE 2.

Function z: the difference in value between
the best and second-best decisions.

gives the formula for conditional variance.
Lemma 2: Var(
Let J.l'

*
d best

They

(X] e, �))

=

Var(Xls) -(V a r (XI e) I 1;).

and u' 2 z denote the prior mean and variance of z.

�e computed from our prior uncertainties X and our

value function

v.

If we observe e, then we might ask how

influences z; in particular, we would like to know howe

e

affects J.l' . We will denote the posterior mean of z given
evidence

� by J.l"

z

.The distribution { J.l"

sity on the posterior mean J.l"

density. Substituting J.l"
z

In fact, we can use the intuition behind Figure 2 to write an
expression for the general EVPI, which is EVPI on all
state variables. The absolute value of z in the negative

shaded portion is the utility that we could gain by chaos-

z

z

I�}

is a prior den-

; that is, it is a preposterior

h

for t e inner

(XI e) on the right­

hand side of the equation in Lemma 1 reveals that

(EQ 2)

122

Chavez and Henrion

Eq. 2shows that the mean of the preposterior distribution
{ ll" I � } is the same as the prior mean ll'
z

If cr"

z

.

2 denotes the posterior variance of z after

z

The type of integral given in Eq. 4 is known as a

linear
loss integral. In general, such an integral is impossible to
evaluate analytically, so we must rely on statistical tables

e,

then

or numerical approximatation methods to evaluate it.

application of Lemma 2 shows that

Var {!l" z1

�}

==

cr'2z - cr" 2.

(EO 3)

z

If the prior and posterior on z are normal, then, as proved
in [Raiffa and Schlaifer,

1961], the preposterior on z is

normal also. That is, the normal distribution is conjugate

to the normal sampling process. We thus require z to be
distributed normally.

3.0

Complexity and Non-Additivity of

EVI
Inference in probabilistic models with discrete variables is

exponential in the number of variables so we would
,

expect the exact calculation ofEVI to be exponential in
the number of variables also. For simplicity, we assume a
single decision variable with

In Figure

3, we show a prior on z. a possible posterior on z

m

alternatives.

Let k; be the

number of states for the ith state variable x;. To evaluate a

given evidence e, and a preposterior density { ll" I� } .

decision tree with n state variables, we require a number of

Note that the preposterior has the same mean as the prior,

value computations at the leaves equal to

z

and that its variance is the difference between the prior

m

variance and the posterior variance.

FIGURE 3.

IJk;.

i= 1

Prior, posterior, and preposterior densities.

Computing EVI on some subset of variables requires at
least the same number of value computations at the leaves,
and thus we see that

exact calculation ofEVI is exponen

­

tial inn. Also note that EVI calculations in discrete models
are possible for perfect information only.
If c; represents the value of information on state variable

X;, and C represents the value of information on all the

state variables simultaneously, then

C'#L,c;·
The above relation makes it difficult to devise separable,
or incremental, procedures for computingEVI, because
EVI will often demonstrate nonlinearities for varying
combinations of variables and for varying cases of perfect
and partial information.
The preposterior density encodes a state of knowledge

about z in light of what evidence e might reveal. Its inter­
pretation is the same as in Figure 2. Because it is a proba­
bility density on value, we can integrate over its negative
area to calculate the EVI of evidence

EVI (e)

e.

Thus, we have

Approximation of EVI

We are ready to apply the preceding analysis to develop an
efficient algorithm

for estimating EVI. We introduce a lin­

ear approximation to the value function, which in tum

0

J I ll"

4.0

z

l {j!" zl S} dj!" z'

(EO 4)

allows us to derive an expression for z, the net difference
in value between two decision alternatives. Preposterior

analysis on z provides a flexible mechanism for estimating

EVI.

123

Efficient Estimation ot the Value of Information in Monte Carlo Models

4.1

rior variance on xk. Since

e is perfect information on xk,
cr" �=0. Eq. 10 gives an approximation to the prior vari­

The Linear Value Model

We require a key approximating assumption: The value

function v(X,d;) can be approximated by a first-order
(linear) equation for each decision d;. that is, we can

write v; as a linear function of the x;.

v (X, d) L�ijxi+ a1

ance on z,

cr';. Given e, we know that the kth term in the

expression in Eq. 10 must be equal to zero. We can thus

write the posterior variance for z given

e,

cr"

2:

z

i=l...n,j=l,2.

=

j

(EQ 11)

We assume, for now, that the x; are independent. (The
assumption is not necessary; we use it to simplify our pre­

In view of Eq.

3, the preposterior variance on z is

sentation.) We denote the prior mean of x; by �·; and

denote its prior variance by cr'2;. Our approximating

Var [J.l"

assumption allows us to perform simple but useful proba­
bilistic analysis. First, by linearity of expectation, we can

write the mean ii (d;) as
v

4.2

(d;) = L,l\p'; + ar

(EQ 5)

i

�)�cr·;.

=

;

and one for

=

{XI�}. A
Xs is ann-tuple of state- variable assignments
to X. v(Xs,di) is equal to the value or utility generated by
scenario

(EQ 6)

v(X,d\

v(X,l)

Monte Carlo methods: Estimation of the
Coefficients

narios by sampling from the prior distriubtions

By our approximating assumption, we can write a linear
approximation for

the sth scenario for the ith decision alternative.

L,13:X;+a*;
i

(EO 7)

the average of the values

The optimal Bayes' decision is the maximum of those

averages. (Naturally, higher sample sizes give answers of
greater precision.) We represent this process for our binary
decision problem inTable 1:

TABLE 1.
(EQ 8)

Combining Eqs. 5-8 with the definition of z, we can write

expressions for the prior mean and variance of z:

J.l' z

v(l)-v(cf)

"
�

(A�� +) f.i.'. +(a* -a+).
1-'f
I

l

sth
Scenario

e

Value with

d1

XI

(EQ 9)

Xwo

expresses perfect information on xk and no

i denote the poste-

Value

with d2

v(X1,d2)

v(X1rod2)

v(X100,d1)

100v(X d)
L 100
s'

Average
information about the other x;. Let cr"

Determining the optimal decision in Monte Carlo decision
sample size= 100 and two decision

ar�alysis with
alternatives

(EQ 10)

Suppose that

d1 as
v(Xs,di) over the scenario in dex

We can estimate the expected value of each decision
s.

v(X,d+),

=

(EQ 12)

In Monte Carlo simulation, we generate a sample ofn sce­

Second, the variance of { v(X,d;)} can be written as

Var{v(X,d;)i �}

zl �]

s

=I

I

too

2:

s=1

v (Xs,

d1)

100

124

Chavez and Henrion

l ,

arg max

)=1,2

source could tell me roughly twice as much as I know

( � v(Xs, d) )
s"' I

now, then the equivalent RIM is 2.

100

(pk-�k)

The only outstanding task is to estimate the constants for
the linear-approximation model. To this end, we apply
multiple linear regression analysis to estimate the con­
sion alternatives, and letj be an index into the

n

2 '2
o-" 2 = -1 (rtf'k• _A+)
�-'k o k'

variables. From [Shavelson, 1988], we can use multiple

as follows:

n

z

state

linear regression to write constants for v;- value for the

z

by

stants in Eqs. 7 and 8. Let i be an index into set of m deci­

ith decision alternative in terms of the

o-' 2

is
A varia ble xk's contribution to the prior variance
z
+ 2 '2
. nm
. Eq . 10 as rt*
o k.For aRIM=rofevig1ve
dence e on variable xk, the posterior variance o" 2 is given

r

(EQ 14)

The preposterior variance is estimated as

state variables­

Var [)l" zl S)

r=

I

-r-

2
(� -�+) o-' 2k·
•

(EQ 15)

The preposterior mean for partial information stays the

(EQ 13)

same, as in Eq.

4.4

9.

Z Is Normal

where Rij = correlation(vi,x). Ru= correlation(xi,x) ,S; =

We will assume that the x;, are normally distributed. In

estimate these quantities directly from our Monte Carlo

our linear-approximating assumption requires z to be

standard deviation(v;), and o ;=standard deviation(xj). We

light of the following proposition from probability theory,

samples.

normally distributed also.

Recall from Section 2. 1 that the v; generate probability

Proposition: Let X; be a collection of n normal random

to think of them as random variables with corresponding

the random vari able Y as

distributions in a Monte Carlo model. Thus, it makes sense

variables with means given by J..li and variances

sample correlations and standard deviations. The a. are

i

n

aj

4.3

=

Then Y is normally distributed also, with mean given by

(v)- L �;}i'r
;,. I

a+

Relative Information Multiple

Suppose now that e expresses partial, rather than perfec t

information on xk. It is not immediately obvious how to

,

specify partial information on an uncertain variable. We

suggest the following method, based on our concept of a

RIM. ARIM of evidence e on variable xk is defined to be
the ratio between the prior variance o' and the posterior
variance o " on xk after e has been seen. In intuitive

i

�

terms, theRIM measures how much we could know rela­
tive to what we know

Y = a+'�.x
L,. I I..

1

estimated as follows:

now.

It is a

m ultiple

o'2• Define

on missing but

knowable information. For example, if an information

ri �iJ..li'

and variance

0

Observe that our approximating assumption allows us to
write the mean and variance of z using standard probabil­
ity formulae. There is nothing about our framework, how­

ever, that forces the actual distribution of z to belong to the
same family as do z's component distributions. For exam-

Efficient Estimation of the Value of Information in Monte Carlo Models

ple, if the x; are Poisson, normal, and exponential, then z is
a hard-to-assess, mongrel distribution. Assuming that the
xi are normal forces z to be normal also. If the x; are non­
normal, then we must make an extra approximating
assumption that z is normal also, although we must
emphasize that this assumption would not be analytically
true.
A limiting aspect of the technique presented here is that it
measures EVI relative to only two decisions. In [Chavez,
1994], we show how to extend it to accommodate multiple
(�3) decision alternatives.

4.5

Var {�"

2. Define variable z as the difference between v(X,d*) and
v(X,d+).

3. Calculate regression constants ��,
and
�+,a·,
I
I

a+.

4. Using Eqs. 2 and 9, calculate the mean (!l" zl s> of the
preposterior distribution of z:
·

(!l" z Js> = 11' z

5. For

=

+ (a·-a+).
"c�·-�+)!l'
£..,
J
J
j

j

perfect informati on on Xi, define

Var {!l" z1 S}

=

•
2
(� -�+) a'2;·

For partial information on Xi with RIM=k, define

Var{!l"zJS}

k;- 1

k�l( �·-� +)2a,2i·

=

For perfect information on variables with indices in S,
define

i

k;- I

=

l: -k.l

l

•

2

(� -�+) a'\

;

Normal ( (!l" zl

2

a'

2

;+I,

je S

•

(� -�+)

2

a'

2

s>

=

ll'z· Var {!l" zl S})

11. Express EVI as
0

=

f ill"zl

{!l"zJS}d!"l z

12. Perform the integration in (11) numerically.

5.0

Application

We now describe an application of our method to a large
decision model developed at Rockwell's Palo Alto Science
Laboratory to support Course of Action (COA) analysis
for Noncombatant Evacuation Operations (NEO).
Implemented in Demos, NEO-COA allows a user to
instantiate a generic NEO plan with specific parameter
values for locations, forces, and destinations of troops and
civilians. The model provides insights into the relative
strengths of alternative plans by scoring them using differ­
ent evaluation metrics, such as time to complete the opera­
tion. Because many of the elements of a real-world
military planning scenario are not known with certainty,
several of the model's inputs are specified as continuous
probability distributions.
In the current version of NEO-COA, there are three deci­
sion variables, or factors over which a military planner
exercises control:
•

F or partial information on variables with indices i and
a corresponding ordered set of RIM's k;, define

Var {!l" zl S}

•

=I,-(�
-�+)
k

10.Define the preposterior density on z, { !l" z S}, as
l

•

8.

z1;}

Algorithm for EV I

1. Select the two decision alternatives generatin¥ the
highest and second-highest expected value, d and cr.

7.

For perfect information on variables with indices in S
mixed with partial information on variables with indi­
ces i and a corresponding ordered set of RIM's k;.

EVI

We now summarize, in algorithmic form, our general tech­
nique for estimating EVI in a Monte Carlo decision
model:

6.

9.

125

Security forces: Security forces vary in their starting
locations, dates of availability, and capabilities in pro­
viding security.
Safe havens: The places where civilians gather to take

safe havens differ in terms of distances from
the assembly areas and port capacities.

shelter,
•

Transportation assets: A configuration of transporta­
tion assets is a sequencing of transportation capability
over a fixed period of time. "Three C-141 's available

126

Chavez and Henrion

on day 2 and 5 C-14l's on days 3 through 10" is an
example of a particular transportation configuration.
Because each of these decision variables currently possess
three alternatives, there are a total of 27 available courses
of action. In addition, the NEO-COA model possesses
over 100 different input variables; of those, currently nine
are specified as probabilistic quantities.
Once the decision variables and inputs have been speci­
fied, the model performs a dynamic simulation of the flow
of U.S. citizens (the non-combatants) from their starting
locations within a country to a set of selected assembly
areas, and then on to the safe havens. It also includes risk
factors associated with both U.S. citizens and U.S. mili­
tary personnel as functions of time. For example, risk to
U.S. citizens at the assembly areas can rise and fall over
the course of an entire operation in response to uncertain
events, such as the arrival of security forces. The func­
tional representations of the risk factors are then used to
compute expected casualties- civilian and military for varying alternatives.
A top level view of the NEO-COA model as implemented
in Demos is shown in Figure 4 . There are three uncertain­
ties for the NEO-COA model: Initial USCITS, proba bil ­
ity distributions on the number of U.S. citizens in each of
the three regions of the country (capital, north, and south)
at the start of a crisis planning operation; Country
Regions Attrition Risk, which is the risk posed to non­
combatants over the course of an operation; and Transfer
Rate, which is the speed at which civilians move from
their starting locations to the assembly areas. Thus a total
of nine continuous probability distributions must be
assigned; typically, these are subjective assessments pro­
vided by military planners using the model.
In Figure 5, we show the results of applying the EVI
approximation technique to the NEO model for perfect
information. We see, for example, that the uncertainty
about the number of American citizens in the capital has
EVI equal to about six lives, and the uncertainty about the
transfer rates in the capital has perfect information value
equal to more than seven lives. In all cases, the value of
information is highest for uncertainties relating to the cap­
ital region, reflecting that the highest number of citizens
are concentrated there. The integral in Eq. 4 is evaluated
numerically. Perfect information calculations on nine

uncertainties took Demos 1 minute, 53 seconds running on
a Macintosh Ilfx computer.

6.0 Conclusions and Future Directions
We have described a general analytic framework for esti­
mating EVI in a decision model using preposterior analy­
sis. It employs a linear-approximating assumption that
allows us to write the value function as a first-order equa­
tion in the inputs. We define variable z to be the difference
in value for the two decision alternatives. Multiple linear
regression on the inputs provides the necessary constants
for the linear value equation; we estimate the regression
constants from Monte Carlo sample information. Applying
preposterior analysis to z allows us to write an approxima­
tion to the value of perfect and partial information for any
combination of state variables.
There are several areas in which we plan to extend the
work presented here. First, we would like to develop a sis­
ter technique for approximating EVI on continuous deci­
sion variables. Second, we would like to examine how
well our technique performs relative to an exact, more
costly approach. To this end we will apply our method to
several large models, run it several times, and compare its
results to the corresponding exact answers. Third, we wilk
apply statistical proof techniques to analyze formally the
algorithm's convergence and error characteristics.

7.0



Since exact probabilistic inference is intractable
in general for large multiply connected belief
nets, approximate methods are required. A
promising approach is to use heuristic search
among hypotheses (instantiations of the network)
to find the most probable ones, as in the TopN
algorithm. Search is based on the relative
probabilities of hypotheses which are efficient to
compute. Given upper and lower bounds on the
relative probability of partial hypotheses, it is
possible to obtain bounds on the absolute
probabilities of hypotheses. Best-first search
aimed at reducing the maximum error
progressively narrows the bounds as more
hypotheses are examined. Here, qualitative
probabilistic analysis is employed to obtain
bounds on the relative probability of partial
hypotheses for the BN20 class of networks
networks and a generalization replacing the noisy
OR assumption by negative synergy.
The
approach is illustrated by application to a very
large belief network, QMR-BN, which is a
reformulation of the Internist-! system for
diagnosis in internal medicine.

1 INTRODUCTION
Bayesian belief networks provide a tractable basis for
expressing uncertain knowledge at both qualitative and
quantitative levels, in a way that is formally sound and
intuitively appealing. They are already being used in a
wide variety of applications, including knowledge bases of
up to about one thousand nodes. A major obstacle to their
application for still larger applications is the limitations
of available algorithms for diagnostic inference. Exact
diagnostic inference in general belief networks has been
shown to be NP-hard (Cooper, 1991). Hence, there is
considerable interest in the development of methods that
provide greater efficiency at the cost of imprecision in the
results (Henrion, 1990b ).
There have been two main directions in which researchers
have sought efficient approximate algorithms. One

approach involves random sampling of network
instantiations, also known as stochastic simulation
(Henrion, 1988). The other involves search among the
space of instantiations (hypotheses) to find those that are
most probable. Cooper (I 984) employed this approach in
Nestor, to obtain the most probable hypotheses. Peng and
Reggia (1987a, I987b) and Henrion (1990a) developed
more powerful admissability heuristics to prune the search
tree, allowing more efficient search of BN20 networks,
that is bipartite networks consisting of independent
diseases, conditionally independent findings, and noisy
ORs, as descnoed in Section 3. These methods are
guaranteed to find the most probable composite
hypotheses, and their relative probabilities (ratio of
posterior probabilities of hypotheses). Peng and Reggia
(1989) and Henrion (1990a) also describe methods to
bound the absolute probabilities of the composite
hypotheses.
Peng and Reggia's approach to abductive reasoning is
based on the notion of minimal covering sets of diseases
which explain observed findings. They use logical
techniques initially to identify covering sets for the given
findings, and then use probabilistic methods to find the
most p robable hypotheses. This scheme assumes zero
leaks, that is that no findings can occur "spontaneously"
in the absence of any explicitly modelled cause. For the
QMR-BN application to be described here, and indeed
most medical problems, most findings have non-zero leak
rates due to false positives, and so an adequate diagnosis
does not necessarily all have to explain all observed
This makes the covering set approach
findings.
inapplicable.
Shimony and Charniak (1990) describe a search-based
method that finds the MAP (Maximum A-posteriori
Probability) assignments to general belief networks. They
show how any belief network can be converted to an
equivalent weighted boolean function DAG, and that
solving the best selection problem (minimum cost
assignment) for this network is equivalent to finding the
MAP assignment for the belief network. While the best
selection problem is also NP-hard, standard best-first
search can be relatively efficient in practice.

Search-Based Methods to Bound Diagnostic Probabilities in Very Large Belief Nets

If the results of diagnostic inference or abductive reasoning
are to be used as the basis for making decisions, for
example how to treat a patient, or what additional tests to
order, knowing the relative probabilities of the most
likely complete hypothesis is not enough. We want to
know the absolute probabilities, or at least have bounds
on them, and we want often want to know the marginal
posterior probabilities of individual diseases, or of one or
two diseases, rather than of complete assignments which
include instantiations of all the other nodes.
To obtain bounds on the absolute probabilities, we need
bounds on the relative probabilities of all the hypothesis
that we have not explicitly examined in the search. That is
we want to find bounds on the sum of the relative
probabilities of the possible extensions of a given
hypothesis. Given bounds on the relative probabilities of
all hypotheses, we can compute bounds on the absolute
probabilities. However, to find such bounds requires
additional knowledge of properties of the network.
Qualitative knowledge about influences (Wellman, 1990;
Wellman & Henrion, 1991) is a useful source of
information to obtain bounds, as we shall see.
This paper presents improvements and generalizations to
the TopN algorithm. First, I will describe the QMR-BN
belief network which is the application providing a
context and motivation for this work on algorithm design.
I then describe a generalization of the noisy-OR
assumption of the BN20 networks, to negative product
synergy. This forms a basis for generalized bounding
theorems, including a new lower bound, that provides a
significant improvement on TopN as presented in Henrion
(1990b). Qualitative probabilistic analysis, using signs
of influence and synergies, provides a clearer and more
general basis for obtaining these. I then describe a method
to obtain bounds on the posterior probability of
hypotheses and for individual diseases. Finally, I present
results from application to the QMR-BN network,
showing progressive improvement as search is extended.
2 QMR AND INTERNIST -1

QMR (Quick Medical Reference) is a knowledge-based
system for supporting diagnosis by physicians in internal
medicine (Miller et a/, 1986). It is a successor to the
Internist-! system (Miller et a/, 1982). The version of the
knowledge-base used here contains information for 576
diseases (of the estimated 750 diseases comprising internal
medicine) and over 4000 manifestations, such as patient
characteristics, medical history, symptoms, signs, and
laboratory results. In this paper, these are referred to
generically as findings. QMR contains over 40,000
disease-finding associations. It represents about 25 person­
years of effort in knowledge engineering and is one of the
most comprehensive structured medical knowledge-bases
currently existing.
The knowledge-base consists of a profile for each disease,
that is, a list of the findings associated with it. Each such
association between disease d and finding/ is quantified by
two numbers: The evoking s trength is a number between

0 and 5 which answers the question "Given a patient with
finding f, how strongly should I consider disease d to be
its explanation?". The frequency is a number between 1
and 5 answering the question "How often does a patient
with disease d have finding f?". Associated with each
finding f is an import, being a number between 1 and 5
answering "To what degree is one compelled to explain
the presence of finding/ in any patient?".
3 QMR-BN: A PROBABILISTIC
INTERPRETATION OF QMR

The aim of this project1 is to develop a coherent
probabilistic interpretation of QMR, which we call QMR­
BN (for Belief Network), and eventually a version with
treatment decisions and cost or value models, which we
call QMR-DT (for Decision Theory). The first goal is to
improve the consistency of the knowledge base and to
explicate the independence assumptions it incorporates. A
second goal is to provide a challenging example to
develop and test new algorithms for probabilistic
reasoning. The current version is a reformulation of the
Internist-! knowledge-base. See Henrion (1990a), Shwe et
a/, (1991) and Middleton et a/, (1991) for more details.
Diseases

Figure 2: BN20 Belief net
A probabilistic representation can be divided into two
aspects: The framework of qualitative assumptions about
dependence and indcpendences, and the quantification of the
probabilities within that framework. QMR-BN currently
follows INTERNIST-I and QMR in assuming that all
diseases and findings are binary variables, being either
present or absent, without intermediate values. The initial
qualitative formulation incorporates the following
assumptions, expressed by the belief net in Figure 2:
Assumption 1 (MID): Diseases
independent.

are

marginally

Assumption 2 (CIF): All findings are conditionally
independent of each other given any hypothesis.

1 This project is a collaboration with Gregory Cooper, David
Heckcrman, Eric Horvitz, Blackford Middleton,
Shwe.

and Michael

143

144

Henrion

Assumption 3 (LNOG): The effects of multiple
diseases on a common finding are combined as a Leaky
Noisy OR Gate. Suppose S df is the link event that
disease d is sufficient to cause finding f. 2 The noisy OR
assumption is that finding f will occur if any link event
occurs linking a present disease to f, and that these link
events are independent. (This is sometimes known as
causal independence.) With a leaky noisy OR an additional
leak event Lt is possible, which can cause f to occur even

prevalence rates for each disease, a quantity with no
correspondence in the INTERNIST-1/QMR knowledge
base. These were estimated from data compiled by the
National Center for Health Statistics on the basis of
hospital discharges, conditional on the specified
demographic (age and sex) categories. In summary, the
qualitative independence assumptions of BN20, together
with the link probabilities, leak probabilities, and disease
probabilities conditional on age and sex, specify a
reformulation of QMR in coherent probabilistic form.

with no explicit disease present.

Definition 1 (BN20): The class of bipartite belief
nets conforming to Assumptions 1, 2 and 3, are termed
BN20.
Some of the findings in INTERNIST-!, such as the
demographics or family history of a patient, are not
actually caused by diseases, but rather circumstances or
risk factors that may affect disease probabilities. These
variables should rearranged for ease of assessment so that
they influence the diseases rather than vice versa.
Currently, we have done this with age and sex as
represented in figure 3.

Demographic factors

4 INFERENCE ALGORITHMS
Given this BN20 representation, is there a tractable
method for diagnostic inference? To compute the exact
posterior probability of any hypothesis, we need to
compute the sum of relative probabilities of all
hypotheses. Since the set of complete hypotheses (disease
combinations) is the powerset of the set of diseases and
has cardinality of z576, this may seem a rather daunting
prospect. We have explored at least three different
approaches for diagnostic inference for this class of
networks. These include an exact method (Quickscore),
and two approximate methods, one using a forward
sampling or simulation scheme, (l ikelih ood weighting),
and one using search of the hypothesis tree with
probability bounding (TopN).
The QuickScore algorithm (Heckerman, 1989) uses an
ingenious rearrangement of the summation.
Its
complexity is polynomial in the number of diseases but is
exponential in the number of findings observed. In
practice it can score cases with 12 findings in about 10
minutes (Lightspeed Pascal on a Mac IIci), but it becomes
too slow if there are many more findings. Since BN20
has large numbers of intersecting loops, exact methods
seem unlikely to be tractable for larger problems.

Findings
Figure 3: Belief net with causative
factors and disease dependencies.
The second stage is to assign probabilities to this
framework, either derived from the QMR numbers, or
elsewhere. Heckerman & Miller (1986) have demonstrated
a fairly reliable monotonic correspondence between the
frequency numbers and P(ffd), the link probabilities of a
finding f given only disease d. Since there are over
40,000 frequencies in QMR, the ability to use a direct
mapping does a great deal to ease the reformulation
process by avoiding the need to reassess all the disease­
finding relationships. We have also developed a mapping
from imports to leak probabilities. Finally, our
probabilistic representation requires prior probabilities or

2 Reggia and Peng (1987a) term this the causation event and
notate it as f:d.

Likelihood weighting (Shachter & Peot, 1989; Fung &
Chang, 1989) is a development of logic sampling
(Henrion, 1988) in which each randomly generated
hypothesis is weighted by the likelihood of the observed
findings conditional on the hypothesis. Further efficiency
is achieved by using importance sampling, in which the
sampling probabilities of diseases are iteratively adjusted
to reflect the evolving estimate of their actual
probabilities. The S algorithm (Shwe & Cooper, 1990)
initializes the probabilities with a version of tabular Bayes
(assuming mutual exclusivity of diseases) as a starting
point for sampling. This version converges to reasonable
estimates of the posterior probabilities in about 40,000
samples taking an average of 94 minutes for the SAM
cases (on a Macintosh Ilci).
The TopN algorithm takes a quite different approach,
searching among hypotheses, that is complete
instantiations of the diseases. It relies for its efficiency on
576
the assumption that of the vast (z
for QMR-BN) set of
possible hypotheses, only a tiny fraction of them account
for most of the probability mass. Hypotheses with more
than a few diseases (five or six at most) have negligible
probabilities, since the improbability of that many

Search-Based Methods to Bound Diagnostic Probabilities in Very Large Belief Nets

diseases rapidly outweighs any possible improvement in
explaining the observed findings. The second key idea is
that, even though computing the absolute posterior
probability of a hypothesis is intractable in general
(requiring summing over all hypotheses), it is easy to
compute the relative probabilities of two hypotheses (see
also Cooper 1984; de Kleer & Williams, 1986; and Peng
& Reggia 1987a). The third key element i s an
admissability heuristic to prune paths that cannot led to
the most probable hypothesis (or most probable N
hypotheses, hence the name TopN), so that only a small
part of the space need be searched. A fourth element are
some theorems that allow bounding of the sum of relative
probabilities of all extensions of each hypothesis, and
hence allow obtaining bounds on the absolute
probabilities of hypotheses without examining them all.
In the following I will give more detail on these, with
some extensions and generalizations of previous results.
5 NOTATION:

I will use the common convention that lower case letters,
such as d, refer to variables, with uppercase, D and D,
referring to the events d=true and d=false, respectively.
Analogously, if h is a set of diseases, then H denotes the
event that all diseases in h are true (present), and JH[
denotes the event that all diseases in h are false (absent): 2
H= U D, G= U D
VdEh

Given a set of diseases, 6= { d!, dz, ... dn), a complete
hypothesis is an event that assigns a value, true or false,
to every disease in 6. A partial hypothesis assigns a
6,

leaving the

rest unspecified. If hc6, then H is a partial hypothesis,
since diseases not in h remain unspecified.
Adjacency of events denotes conjunction. So the event
HG specifies that all diseases in h are present, all those in
g

are absent, and the rest unspecified. (We assume hng =

0.)
Underlining makes a complete hypothesis from a partial
one, assigning absent to all diseases not specified. Thus H
denotes the event that all diseases in h are present and all
others in 6 absent:
H = U Du U D
VdEh

We define h0=0 as the empty set of diseases, and H o is

the corresponding event that no disease from 6 is present.
The relative probability of a hypothesis H is the ratio of
the posterior probability of H given findings F to the
posterior probability of hypothesis Ho:
Rill) =

R(H)
P(H F)
P(H I F)
=
=
Rili
P(Ho F)
P(Hol F)
o)

Vd<�.h

[1]

TopN starts its search from h0, extending it by adding one
disease at a time. To generate the next candidate
hypothesis, it adds to the current hypothesis the disease
which leads to the largest relative probability. To identify
the n most probable hypotheses (hence "TopN"), it
applies an admissibility heuristic, which abandons a
search path when it provably cannot lead to any
hypothesis more probable than the nth best so far.
TopN's admissibility criterion is based on the concept of
Marginal Explanatory Power (MEP).
Definition (MEP): The Marginal Explanatory Power
(MEP) of a disease d with respect to a hypothesis set of
diseases, h, is the ratio of the posterior probability of the
extended hypothesis hud to the posterior of h alone:
MEP(D, H) =

V dE g

value to a proper subset of the diseases in

6 RELATIVE PROBABILITY AND
MARGINAL EXPLANATORY POWER

P(HDI F) R(HD)
=
R(H)
P(Hl F)

[2]

The MEP is a measure of the increase or decrease
(according to whether it is greater or less than 1) in the
degree to which the hypothesis explains the findings F due
to the addition of d. The use of the MEP as the basis of
an admissable search heuristic depends on the following
result (Henrion 1990a):
Theorem la (declining MEP): Given a BN20
network, for any disease d, and disease sets h and g, the
marginal explanatory power (MEP) of d with respect to h
cannot be less than the MEP of d for any extension hug,
i.e.
MEP(D, H)� MEP(D, HG)

[3]

When searching for the most probable hypothesis from
current hypothesis h, if MEP(D, H) � 1 then d can be
eliminated as a path for exploring as an extension to H,
since it cannot lead to a more probable hypothesis. It can
also be eliminated as a candidate for extending other
extensions of H. Thus the only diseases which need to be
considered as extensions of H are those for which MEP(D,
H)> 1.
7 NEGATIVE PRODUCT SYNERGY
AND THE MEP THEOREM

2

Note that lHI is not equivalent to H. the event that at least

one of the diseases in h is absent.

It turns out that Theorem I a does not require the leaky
noisy OR assumption 3 of BN20, assumed in Henrion

145

146

Henrion

(1990a); a weaker assumption, negative product synergy
will suffice. First, we define this property, and then show
the more general version of the theorem.
Definition 2a (two cause NPS): Suppose there are
two propositions, d and e, and other variable(s) x, that
influence finding F according to the conditional
probability distribution P(FI d ex), there is negative
product synergy in the influence of d and e on J, iff
P(FIDE x)
-

P(FIDE

<_

-

P(FIDE x) ....,
vx.
P(FIDE x)
--

x)

[4]

We can now obtain a generalization of Theorem 1a, which
applies to BN2NPS:
Theorem lb (declining MEP): Given a B N 2 NPS
network, then, for any disease subsets x, y, z of .1., the
complete set of diseases, the marginal explanatory power
(MEP) of x with respect to z cannot be less than the MEP
of x for any extension yuz, i.e.
MEP(X, Z) � MEP(X, YZ)

Proof: Taking the ratio of the two sides, and
substituting the definition of MEP [2],

This is the condition required for disease d to "explain
away" the evidence F, that is, given F, there is a negative
influence between d and e (Henrion & Druzdzel, 1990):

MEP(X, Z)
MEP(X, YZ)

-

P(EID F x) � P(E I D F x) "iix.

[5]

It is simple to show that the noisy OR (with or without
leaks) exhibits negative product synergy, and so gives
rise to this explaining away phenomenon.
Wellman and Henrion ( 1991) generalize the definition of
product synergy for n-ary variables, and discuss its
relation to additive synergy. Here we generalize the
definition in a different way to apply where there are more
than two variables which together influence another
variable:
Definition 2b (n cause NPS): Consider a set .1. of
propositions which influence finding F, as specified by
conditional probability distribution P(Fit.). The influence
exhibits negative product synergy, iff for any sets of
propositions x,y,z�Ll. there is negative product synergy
between x andy given z, i.e.
P(FIXYZ)
P(FI.YZ)

�

P(FIXZ)
P(FIZ)

.

[7]

[6]

Assumption 4 (POS): The influence of every disease
d on every finding f is positive, that is, for any set of
diseases h not containing d,
-

P(FIDH) � P(FIDH), "iihcll, where dE h
Since the inequality is weak, this also allows diseases and
findings to be unlinked (independent). Positive influence
from disease to finding is an automatic consequence of
Assumption 3, the leaky noisy ORs, but not of negative
product synergy.
We can now define a class of bipartite belief nets that
generalizes the leaky noisy OR of BN20 to positive links
with negative product synergy:
Definition 3 (BN2NPS): A bipartite network is said
to be BN2NPS if it satisfies Assumption 1 (marginally
independent diseases), Assumption 2 (conditionally
independent findings), Assumption 4 (positive links), and
negative product synergy (NPS) in the influence of the
diseases on each finding .

=

=

P(F XZ) P(F .YZ)
P(F Z) P(F XYZ)

P(FIXZ) P(FI.YZ) P(XZ) P(.YZ)
P(FIZ) P(FIXYZ) x P(Z) P(XYZ)

[S]

From the definition of n cause negative product synergy
[6] above, we know the first term of the produce above is
�1. From the marginal independence of diseases, we
know that
P(XZ) = P(Z) fiO(D), where O(D) =
dez

��m)

Expanding P(.YZ) and P(XYZ) similarly in the second
term, the top and bottom cancel out. Hence we are left
with the entire ratio as �I. QED.
8 BOUNDS ON THE PROBABILITY OF
EXTENSIONS

We want not just to identify the most probable
hypotheses using their relative probabilities, but to obtain
bounds on their absolute probabilities. To do this we
need to obtain bounds on the relative probabilities of all
the extensions of hypotheses in the search tree, so that we
can put bound on the contributions of all the hypotheses
we do not examine explicitly.
So far we have considered only complete hypotheses, such
as H. The relative probability of a partial hypothesis H is
the sum of the relative probabilities of all complete
extensions of H, that is all complete hypotheses in which
all diseases in h are present, that is,
R(H) =

.LN.Qh R(,S)

[9]

We also need the relative probabilities of partial
hypotheses that contain excluded diseases, such as:
R(HG) =

L"i! s where gc::2.Qh R(,S),

[IOJ

where gC is the complement of g, i.e. the set of diseases
in L1 but not in g.

Search-Based Methods to Bound Diagnostic Probabilities in Very Large Belief Nets

The following result gives an upper bound for the relative
probability of a partial hypothesis h excluding diseases in
g. It gives it in terms of the relative probability of the
corresponding complete hypothesis and the MEP for
candidate extension diseases d with respect to h, which are
relatively easy to compute:
Theorem 2 (UBI):
R(HG) � R(ID fl[l+MEP(D, H)].
'v'dehug

[11]

Unfortunately the upper bound UBI is not always a good
guide when there are many diseases each of which can
explain a lot relative to Ho, i.e. MEP(D, H0)» 1. In the
beginning of the search in a case with twenty or more
positive findings, UB1 can be very large, for example
overflowing an 8 byte floating point number (> 1 0300 ),
unless computed as logs. An upper bound avoids this
tendency is given by:
Theorem 5 (UB2):
< R(ID +
R(HG) -

This follows from the observation that that at most there
is no overlap between the findings explained by each
disease, and so the MEP(D, H) for each disease d is the
same, no matter how many other diseases are in the
hypothesis h it is extending. It is a generalization of
Theorem 2 given in Henrion ( 1990c) for the BN20
assumptions. The complete proof relies on the Declining
MEP Theorem lb, and so it also follows from the more
relaxed BN2NPS assumptions.

P(HG) - P(H)
P(FI Ho)P(Ho)

where P(HG) - Pili)
= flP(D) fl[l-P(D)]- flP(D) fl[l-P(D)]
d£h
dEif
dEh
d£g
= flP(D)
lit= h

[

fl[l-P(D)] lit= g

R(H) provides a simple lower bound (LBl) for R(HG).
This bound would be attained if all proper extensions S::Jh
had probability R(S)=O (Henrion, 1990c).
An higher lower bound is given by the following:
Theorem 3 (LB2):

R(HG)

>

_

IT

1
l-P(D)"
'v'dehug

R(ID

[12]

This follows from Assumption 4 of positive influences,
that extending a hypothesis h by disease d cannot reduce
the likelihood of evidence F, that is P(FIHD) :2: P(FI.!:D.
There are often diseases d which explain nothing more
than hypothesis h, that is for which P(FIHD)=P(FIH).
Since these diseases are independent of the rest conditional
on H, it is possible to factor out their contributions to a
partial hypothesis HG thus:
Theorem

4

(Factoring independents):

R(HG) = R(HGW)

IT i

1- (D)'
'v'dEw

where w= ( d : P(FIHD)=P(FI.!:D}.
This allows us to remove all such independent (non­
explanatory) diseases, w. from the candidate list as
extensions of h. while accounting for their contribution.
Note that some diseases have relatively high priors (e.g.
peptic ulcer with prior 1.6%) and so are not infrequently
among the top ten hypotheses even if there is no specific
evidence for them. Application of this result prevents
them from cluttering up the search process.

[13]

;(Ho)

]·

[ 1-P(D)]
dEh

This is based on the observation that at most any
extension D to H will completely explain all findings,
that is P(FIDH) � 1. This bound is complementary to
UBI, with use early in the search in cases with many
positive findings.
9 SEARCH METHOD
The search uses a best-first approach, where "best" means
the candidate partial hypothesis with the greatest possible
contribution to uncertainty about the relative posterior
probability. This uncertainty is measured as the maximum
error, the difference between the lower bound 2 and the
least of the upper bounds:
MaxErr(h) = Min(UBl (h), UB2(h)) - LB2(h)

[14]

We order the candidate hypotheses by MaxErr and select
the top one as the next one to expand. This is the one for
which expansion has the largest scope for reducing its
contribution to the overall uncertainty about the relative
probability of all unsearched hypotheses. Each time a
hypothesis is expanded, this reduces the bounds on its
parents. Search terminates, either when the MaxErr is
less than a criterion, Pmin, expressed as a fraction of the
upper bound on the total relative probability, or when the
search runs out of space for the hypothesis tree. As in
most best-first or A* searches, the algorithm is liable to
be memory bound, running out of space before running
out of time.
10 OBTAINING ABSOLUTE
PROBABILITIES

So far we have obtained bounds on the relative probability
of a variety of partial hypotheses, including LBR(H),
UBR(H) for each hypothesis H in the search tree, each

147

148

Henrion

disease D, LBR(D), UBR(D), and Ho. Note that the partial
hypothesis H0 is all extensions of the no disease
hypothesis, i.e. all possible hypotheses, so P(H0) = 1.
R(Ho)

=

L'v's;;;!ho

="
� 'v'S;;;!h

R (S)

P (S, F )
o P(Ho F)

P(F)
P(Ho F)

[15]
[16]

Hence, P(F) = R(Ho) Pilio F)

The posterior probability of any partial hypothesis H is
P(HIF)

=

P(H F)
P(F)

Substituting in from the definition of relative probability
P(H F)= R(H) P<Bo F) and [16] we get
P(HIF) =

..1ill!L .

The upper bound for this is when R(H) is at its upper
bound UBR(H) and R(Ho) is lower bound LBR(Ho ), but
note that since the partial hypothesis H0 includes H, we
need replace LBR(H) as a component of LBR(Ho) by the
the upper bound of H in the denominator too. Thus, we
get the upper bound on the posterior probability of H is:
UBR(H)
LBR(Ho)-LBR(H)+UBR(H)'

[181

and similarly the lower bound is
LBP(HIF)

_

-

LBR(H)
UBR(Ho)-UBR(H)+LBR(H)'

[19]

The maximum total error due to probability of hypotheses
not examined in the search is given by
UBR@o)-LBR(Ho)
UBR@o)

[20]

TopN also produces a "best" probability estimate for each
hypothesis, h , defined as the ratio of the sum of the
relative probabilities of all complete hypotheses actually
examined that contain h, to the relative probability of all
hypotheses examined, e:
Best(H) =

For analysis of timing and accuracy we examined 12 of
the 16 SAM cases in which Quickscore can be run for
comparison, that is cases with less than 14 positive
findings. These cases have an average 9 positive and 11
negative findings. Table 4 gives results for on the
performance of TopN for series of runs using a search
precision (Pmin) of I0-5. The number of hypotheses
examined varies from 277 to 30000. (In two cases search
was cut off after 30000 hypotheses due to exhausting
memory space. ) Since the distributions of hypotheses,
time, and precision are highly skewed, Table 1 includes
minimum, maximum and median, as well as mean values.
Table 1: Performance on 12 SAM cases using TopN
algorithm with a search precision Pmin of w-5

[17]

R(Ho)

UBP(HIF) =

Scientific American Medicine (SAM) Continuing Medical
Education Service. More details of the coding process are
given in Shwe et a/ (1991).

LV' gEe where g;;2h R(Q_)
L 'v'gEe R(Q.)
-

[21]

This probability estimate is guaranteed to be between the
lower and upper bounds on the absolute probability.
1 1 PERFORMANCE OF TOPN:

The QMR-BN research team has assembled cases for
testing the performance of alternative inference
algorithms. These include 16 cases abstracted from the

Min
Num of findings
positive
negative
Num of hyps
Run time (sees)
Max prob bound
St. err. of "best"

9
6
0
277
1.2
0.008
<0.00001

Max
28
14
20
30000
65.3
1.000
0.064

Mean
20
9
11
11215
17.8
0.31
0.009

Median
22
8
11
3794
7.7
0.21
0.005

TopN took an average of 18 seconds (maximum of just
over a minute) for the 12 SAM cases. The S sampling
algorithm was run for 40,000 samples to achieve adequate
convergence for the SAM cases, taking an average of 94
minutes on a Macintosh Ilci (about three times faster than
the machine used for the Quickscore and TopN runs).
In some cases the maximum probability bound is at or
near 1, and quite useless. But it turns out that the actual
accuracy of the "best estimate" probabilities is very good
when compared with the exact results from QuickScore,
with a mean standard error between of 1.2%. Thus it
appears that the bounds are highly conservative (much
larger than necessary) in most cases. This finding suggests
the sampled hypotheses are quite representative in terms of
disease probabilities of the unsampled ones. Of course this
may not always be true, but it suggests some interesting
conjectures about properties of the hypothesis population.
To examine the effect of computational effort on the error,
the precision for terminating search Pmin was varied by
factors of 10 from w-3 to I0-7. Decreasing Pm in
increases the number of hypotheses explored, and decreases
the maximum bound on the probability error. The
computation time is approximately linear in the number
of hypotheses examined, with 30,000 hypotheses taking
about 65 seconds on a plain Macintosh II. Figure 3 shows
the effect of increasing the number of hypotheses searched
on the error bound for the 16 SAM cases. Most converge
satisfactorily according to the error bound by 30,000
hypotheses, but four do not.

Search-Based Methods to Bound Diagnostic Probabilities in Very Large Belief Nets

_e-.,
"'

�
··�

1.0

"'
"'

0.8

5 �

0.2

�.s
.g 0
... � 0.6
P-..c:
='"0
0 "' 0.4
tl .s
,.Q

><

!;j =
"'
P-o
P-o ;:I

:::>Cl

0.0
100

1000

10000

100000

Number of hypotheses searched

Figure 3: Error bound as maximum probability unaccounted for as a
function of the extent of the search for 16 SAM cases.

Figure 3 illustrates how the uncertainty about the
computed probabilities decreases as the search of the
hypothesis tree is extended, that is as the cumulative
probability of all the hypotheses examined approaches
one. Thus TopN is an "any time" algorithm: If it is
stopped at any point after initialization, it will give
bounds on the posterior probabilities; and the longer it
runs, the narrower these bounds will be. Given an
estimate of the convergence rate, a meta-reasoner could
select the run-time to be allocated according to the urgency
of the diagnosis, the importance of precision, and the cost
of computing.
CONCLUSIONS
The QMR-BN belief network confronts us with the
general intractability of exact algorithms for diagnostic
inference. Search-based algorithms such as TopN appear a
promising approximate approach for such networks. They
may be seen as smarter than forward sampling techniques
in that they search specifically to find the most probable
instantiations. They rely on exact methods for bounding
the error in the resulting probabilities instead of the
statistical error estimation methods available for some
sampling techniques.
We have presented a variety of results that bound the
relative probabilities of partial hypotheses in BN20 and
BN2NPS networks. These results illustrate the value of
applying methods of qualitative probabilistic analysis,
based on knowledge of the signs of influences and
synergies. For the QMR-BN project, and no doubt others,
there remains a need to develop more general results, for
example for networks with prior dependences among
diseases. The generality of search-based methods for
bounding probabilities remains an open question. It
seems unlikely that the kind of bounding results used here
will be obtainable for completely general networks, but
some further generality may be obtainable from
knowledge of qualitative probabilistic properties of other
classes of network.

Acknowledgements
This work was supported in part by the National Science
Foundation under grant IRI-8807061 to Carnegie Mellon,
and in part by the Rockwell International Science Center.


Qualitative and infinitesimal probability
schemes are consistent with the axioms of
probability theory, but avoid the need for
precise numerical probabilities. U sing
qualitative probabilities could substantially
reduce the effort for knowledge engineering and
improve the robustness of results. We examine
experimentally how well infinitesimal
probabilities (the kappa-calculus of Goldszmidt
and Pearl) perform a diagnostic task troubleshooting a car that will not start - by
comparison with a conventional numerical belief
network. We found the infinitesimal scheme to
be as good as the numerical scheme in
identifying the true fault. The performance of
the infinitesimal scheme worsens significantly
for prior fault probabilities greater than 0.03.
These results suggest that infinitesimal
probability methods may be of substantial
practical value for machine diagnosis with small
prior fault probabilities.
Keywords:
Bayesian
probabilities, kappa
probabilities, diagnosis.

networks, qualitative
calculus, i nfinitesimal

1 BACKGROUND AND GOALS

Bayesian and decision theoretic methods have long

been criticized for an excessive need for quantification.
They require many numerical probabilities and

Brendan Del Faverol

Gillian Sanders2

1oepartment of Engineering-Economic Systems,
Stanford University, CA 94305
2Section on Medical Informatics
Stanford University, CA 94305

utilities that are difficult to assess and are liable to
judgmental biases. Some people claim that since
human thinking is inherently qualitative, it is
incompatible with quantitative schemes. These
criticisms have fueled interest in alternative
formalisms for reasoning and decision making under
uncertainty that are intended to be easier to use and
more compatible with human cognition. Among these
alternative schemes are: various generalizations of
decision theory [Edwards, 1992]; Dempster-Shafer
belief functions [Shafer, 1976];generalizations of logic,
including default and non-monotonic logics [Ginsberg,
1987]; fuzzy logic [Zadeh, 1983]; possibility theory
[Dubois and Prade, 1988]; and fuzzy probabilities.
If,

however, our goal is simply to provide a qualitative
basis for reasoning and decision making under
uncertainty, there is no need to abandon Bayesian
decision theory. The axioms of decision theory,
indeed, assume only the ability to make qualitative
judgments - that is, to order events by probability or
outcomes by desirability. The quantification of
probabilities and utilities can be based on purely
qualitative judgments. Furthermore, several schemes
have been developed that are purely qualitative, but
are consistent with the axioms of decision theory.
One such scheme is qualitative probabilities, originated
by Wellman [1990; Henrion & Druzdzel 1991;
Wellman & Henrion, 1993]. A second approach to
qualitative probabilities is the kappa-calculus
[Goldszmidt and Pearl, 1992], which represents all
probabilities in a Bayesian belief network by e'K, where
is an integral power of E. The K -calculus is

K

Henrion, Prova n Del Favero, and Sanders

320

,

consistent with the axioms of probability where E---+0.
Events are ranked according to K. Events with larger K
are assumed to be negligible relative to events with
smaller K. The calculus provides a plausible set of
events: those with the smallest (most probable)
consistent with the observed findings. The calculus is
sometimes called

qualitative probability.

To avoid

confusion with other qualitative probability schemes,
we call this representation
Pearl

infinitesimal probabilities.

[1993] has extended this scheme to handle

similar difficulties.
Much current research on
qualitative simulation is directed towards integrating
quantitative information to resolve ambiguities (and
the resultant combinatorial explosions of the search
space).
In this paper, we report the results of an initial
experimental

study

comparing

the

diagnostic

performance on a specific belief network using (1) the
K -calculus or infinitesimal probabilities, and

(2)

qualitative utilities to support decision making.

numerical probabilities. Our goal is to examine how

The K-calculus or infinitesimal probabilities can be

approximation to the numerical representation.

well

the

infinitesimal

scheme performs as

an
We

looked at in two ways: (a) as providing a scheme for

start with a fully assessed numerical representation,

non-monotonic reasoning whose semantics are firmly

convert this into a kappa-representation using finite e

grounded in probability and decision theory; or (b) as

values, and perform inference on a set of test cases.

providing a simplification of belief networks with

We first explain the mappings we used to obtain

numerical probabilities. In this paper, we are focus on

infinitesimal

the second view, and examine the performance of

probabilities, and how we mapped back from the

infinitesimal probabilities as an approximation to
numerical probabilities.

or

K-values

from

the

numerical

pos terior K-values into probabilities for comparison of

From this perspective,

performance. Then, we describe the experimental

proponents of infinitesimal probabilities may claim

design, including the sample network, the set of test

four possible advantages over traditional numerical

cases,

belief networks:

probabilities, the epsilon values used in mapping, and

1. It may be easier to express beliefs by partitioning

and

our

variations

of

the

prior

fault

the number of findings observations per case.

The

number of sets of relative

infinitesimal scheme provides a set of the most

plausibility, that is values, than by assigning

plausible diagnoses for each case. In the results, we

events into a small

each event a precise numerical probabilities.

2. Results from reasoning with infinitesimal

compare these plausible sets with the posterior
probabilities for the diagnoses produced by the

probabilities are more robust and therefore more

numerical

trustworthy since they are based on less specific

implications of these results for the application of the

inputs.

K-calculus as a practical representation.

scheme.

Finally,

we

discuss

the

3. Reasoning with infinitesimal probabilities is
easier to understand and explain.
4. Inference methods with infinitesimal probabilities

can be computationally more efficient.

Initial analysis of the computational complexity of

[1992]

suggests that, in general, it is of the same order as
reasoning with numerical probabilities, that is NP­
hard

[Cooper,

1990].

There

may

be modest

computational savings from doing arithmetic with
small integers instead of floating point numbers.
Most

research

on

qualitative probabilities has

concentrated on developing the formalisms and
efficient algorithms.

AND INFINITESIMAL
PROBABILITIES

Hitherto, these claims have been largely untested.
reasoning infinitesimal probabilities Darwiche

2 MAPPINGS BETWEEN NUMERICAL

There has been little concerted

effort to demonstrate their application to real tasks and
to evaluate their practicality. Initial studies of QPNs
[Henrion and Druzdzel,

1990; Druzdzel and Henrion,
1993; Druzdzel, 1993] suggest that they are often

inconclusive for nontrivial cases. For example, QPNs
give vacuous results in any case with conflicting
evidence. Studies of qualitative simulation have found

In order to be a b le to apply

the

K -calculus to

probabilistic reasoning on a belief network with finite
probabilities, we need to provide a mapping from
probabilities into kappa values. In order to compare
the results we need to map the kappa results back
again into probabilities. Strictly, the K-calculus is only
valid as E---tO.

We use an approximation for finite

values of E. For a finite E, the K-calculus partitions the

real interval

[0,1] into regions identified by integers,

based on the smallest power of in the polynomial. This
mapping is illustrated in Figure 1.
More specifically, consider the real [0,1] interval I,
which is the interval used by probability theory, and a
discretized representation of I, which we call 5. 5 is a
set of non-negative integers which the -calculus uses to
represent probability measures in the interval I.

We

wish to explore the mappings f: I---tS (i.e., from
numerical to infinitesimal probability) and g: S ---t I

Nume rical and Qualitative Probabilistic Reasoning

(i.e., from infinitesimal to

numerical probability).

Note that there is information loss in the mapping f,
since it is not injective. Moreover, the mapping g is
not surjective.

Definition 1

321

[ K"-map] [Spohn 1988] The mapping f

from probability measures to

K"-values takes a

probability 1r and a threshold probability e and

outputs a K"-value K" e S such that

3 APPLICATION DOMAIN: WHY YOUR

CAR DOES NOT START
The task is to troubleshoot why a car is not starting,
given evidence on the status of the lights, battery, fuel,

fan belt, and so on. Figure 2 shows the Bayesian belief

network displaying the causal and conditional
independence relations.

We are grateful to David

Heckerman for providing the original belief network
and to Paul Dagum for lending us his expertise as a
Figure 1 shows an example of a mapping for £

=

0.1.

car mechanic in adjusting some of the probabilities.

All variables are binary (present or absent), except for
battery charge which has three values (high, low,
none). The initial network contains fully quantified,
numerical conditional probability distributions for

Kappa
1C(X)

each influence and prior probabilities for each fault
(source variable).

3

common

effect

Effects of multiple causes of a
are combined

with noisy-ORs,

generalized where necessary.
There are nine explicitly identified faults in this model:
spark plugs bad
distributor bad
fuel line bad
fuel pump bad
gas tank empty

0
0

0.001

0.01

Prd:lability p(X)

0.1

starter bad
battery bad

Figure 1: An example mapping giving kappa as a

fan belt loose

function of probability, for £=0.1.

alternator bad

Figure

2: Bayesian network representing the car diagnosis domain. Leak events represent all the

potential causes of a fault other than those shown explicitly. The number in each origin fault of a leak
node represents its prior probability in the original network. The numbers attached to each influence
arrow represent causal strengths -that is the probability that the successor is broken given that the
predecessor is broken, and all other predecessors are normal.

322

He nrion, Prova n Del Favero, and Sanders
,

We also identified three leaks. Each leak event
represents all possible causes of an event that are not
explicitly identified above. The probability of a leak is
the probability that its associated effect will be
observed even though none of its identified causes are
present.

from 10 to 1000. Table 1 shows the mean and range of

the resulting prior odds we used.

Table 1: The minimum, mean, and maximum prior
fault probabilities. The top line shows the original

engine start other

The

network with larger probabilities. To do this, we
multiplied the prior odds by an odds factor ranging

probabilities. Those below are derived by multiplying

engine tum over other

the odds of each prior by the odds factor and

charging system other

converting back to probabilities.

leaky noisy

or model assigns a probability to each

leak, to handle the fact that the network is inevitably
incomplete. In our adjusted network, the probability

Odds

of each leak was substantially smaller than the sum of

factor

the probabilities of the identified causes for each event.

Minimum

Mean

Maximum

1

0.00001

0.00036

0.00100

10

0.00010

0.00361

0.00991

50

0.00051

0.01750

0.04766

100

0.00103

0.03376

0.09099

300

0.00307

0.08900

0.23095

1000

0 010 17

0.21364

0.50025

There are 10 observable findings in the model_ listed
here in non-decreasing order of expense to test:

1. engine-start
2. gas-gauge
3. engine-tum-over

4. lights
5. radio
6. fan-belt
7. battery-age
8. distributor

.

9. spark-plugs
10. alternator

Note that there are four findings that are also
enumerated faults, namely fan belt, alternator, spark

4.2 Test Cases and quantity of evidence

plugs, and distributor.

We expected that the performance of both numerical

4 EXPERIMENTAL DESIGN

function of the quantity of evidence. We also wished

We wish to investigate the effects of three factors on

relative

the diagnos tic
probabilities:

performance

and infinitesimal schemes would improve as a

of

inf initesimal

to examine the effect of the quantity of evidence on the
performance

of

the

two

schemes.

Accordingly, we needed a representative set of test
cases with varying numbers of findings.

(a) The choice of the value of E on the mapping
between numerical and infinitesimal probabilities.

(b) The range of prior fault probabilities
(c) The quantity of evidence in the test cases.
We have already discussed factor (a). Here, we will
discuss our choice of each of these factors, and the
conduct of the experiment.

We generated a set of 116 test cases, in the following
manner: For each of twelve faults (nine identified
faults plus three leaks), we identified the most likely
(modal) value for each of the ten observable findings.
For each fault, we created a base

case

consisting of all

findings at their modal value. In four cases, the fault is
itself a finding, which we omitted from the base test

case, since including the true fault as observed in the
test case would be trivial. We then generated a second
case for each fault by omitting the most expensive
observation from the base case.

Further cases were

4.1 Range of prior fault probabilities

generated by omitting the next most expensive

The numbers in Figure

finding that the engine does not start. In this way, we
created a series of ten cases for eight faults, and nine

2 are the original prior fault

probabilities. To examine the effect of the magnitude
of the priors on the relative performance of the
infinitesimal calculus, we created versions of the

observation in tum.

In all cases, we retained the

Numerical and Qualitative Probabilistic Reasoning

cases for the four faults that are observable, resulting
in a total of 116 test cases in all.

4.3

323

faults are clearly identifiable, having probabilities at

least an order of magnitude greater than those of all
other faults. We found that this approach, as expected,

gave very similar results to the exact IC-calculus

Computation

inference using CNETS .

To obtain results for the numerical probabilistic

scheme, we employed IDEAL [Srinivas and Breese,

1990], using the clustering algorithm from the I DEAL
library. We applied each of the 116 test cases to the
network using each of the six sets of priors,
performing a total of 696 run. For each run we
computed the posterior probability for each of the
twelve faults resulting in 8352 probabilities.

5 RESULTS
Our first goal was to examine the effect of E values on
the performance of the infinitesimal probability
scheme. We then selected the value of E that gave the
best results and examined the effect of varying the

quantity of evidence on the performance of both

numerical and infinitesimal schemes.

We also converted the original numerical probabilities
into K-values, using the three values e (0.1, 0.01, 0.001),
resulting in a total of 2088 additional runs. We ran

5.1

calculus developed at

we might expect it to perform better for small£, where

each case using CNETS, a full implementation of the K­
the

Rockwell

Palo Alto

Laboratory [Darwiche, 1994], producing posterior K­
values for each fault. For each run, we computed the

plausible set, that is the subset of faults with the
minimal K value.
Definition

[Plausible Set]

2

Consider a set

V:;;{v1,v2, ,vm}representing m possible hypotheses,
•••

Let

vmin

](­

=minvj by the minimum ](-value.
J

probability interval

(0,

1], as shown in Figure 1, and

larger e. To investigate this we analyzed an initial set
of 72 test cases usin g E values of 0.0001, 0.001, 0.01, 0.1,

0.2. Figure 3 shows a graph of average probability
against e. It is interested to note that the average score

identical fore= 0.1 and e

To compare the infinitesimal scheme with the
numerical one, we converted K-values of diagnoses
back to probabilities as follows:
De fi n it ion 3:

original probabilities, with less information lost.

Accordingly, we might expect it to do better with

is identical for E = 0.01 and E

Cll(V)={j:vj =vminl·

[Pro b a bility

setV={v1,v2, ... ,vm}r e presenting
assigned a

the approximation will be mere exact. On the other

hand, a larger E provides rnore partitions to the

score assigned to the true diagnosis for these cases,

The plausible set is given by

hypotheses,

Since the kappa calculus is only strictly correct as E---+0,

consequently, it provides a finer discretization of the

in which each hypothesis has been assigned a
value.

Effect of E values

score]

m

For

=

= 0.001, and also

0.2. Overall, there is an

improvement in performance with increasing E up to

0.2. Accordingly, we selected E

=

0.1 for use in our

remaining experiments.

a

p o s s i b 1 e

in which each hypothes is has been

](-value, the corresponding probability

distribution is given by

ifvj=vmax

(3)

0.3
.,
"'
"' 0.25
..
-<
0.2
I>
., .....
s:: ..
0.15
.. ....
a: "'
.. "
0.1
r;,. ...
.,
"
..
!..
0

��

otherwise

That is, the probability ni= 1/n is assigned to the true
faults if it is in the plausible set of size n. Otherwise,
we assigned p = 0.

�

"

�"'

0.05

0.0001

0.001

0.01

0.1

As an additional test, we also ran IDEAL using the

exact algorithm, but using fault probabilities mapped
to O.OlK for the values obtained from the mapping
using the full set of K values.

subset of 72 test cases.

We applied this to a

In the results, the plausible

Figure 3: Effect of E o n the score (probability

assigned to the true fault) by the infinitesimal scheme

Henrion, Provan, Del Favero, and Sanders

324

5.2 Effect of Number of Findin gs on the

Plausible set

...., 0.5
';
1:1
...

As the quantity of evidence increases, we should
expect the performance of both numerical and
infinitesimal schemes to improve.

Accordingly, we

classified the cases by the number of findings. Figure 4
graphs the average size of the plausible set (number of

Gl
:I
I.
...,
..
Cl
.=

e

Cl,.

0.25

plausible faults) identified by the infinitesimal scheme
as a function of the number of findings. These results
summarize all116 cases fore

=

01
. . As expected, the

average size of the plausible set of faults decreases
with the number of findings, from 7 faults with 1
finding to1. 21 faults for 10 findings. With10 findings,

o �----2
0
6
8
4
10
Number of findings

this scheme provides almost complete specificity that
is, the plausible set usually consists of just a single
diagnosis.
..
.,
Ill

Figure 5: The probability assigned to the true
fault for each scheme as a function of number of
findings

10

.,
...
.Ill
. ..
Ill
:I
"
...
liloo

What is, perhaps, surpnsmg is how closely the
performance of the infinitesimal scheme tracks the
performance of the numerical scheme.

..
0
.,
N

. ..
"'

Indeed the

infinitesimal scheme appears to perform better than
the numerical scheme for intermediate numbers of

5

findings, but this difference is not significant.

Since

the infinitesimal representation is derived from the
numerical one, we could not expect it to do better, on
average.
Note that, even with all ten findings, both schemes

o+-----�----�---r---,--�
0

2

6

Number of fi nd i ngs

8

10

average about 0.5 probability for the true diagnosis.
This relatively poor performance arises because of the
limited scope of the network, which does not provide
the means to differentiate among several classes of

Figure 4: The average size of the plausible set

as a function of the number of findings in each
case.
5.3

Comparing the performance of
infinitesimal and numerical schemes

Next, we compare how the number of findings affects
the diagnostic performance for the infinitesimal and
numerical schemes. Figure 5 graphs the performance
in terms of the average probability each assigns to the
true fault, as a function of the number of findings. For
both schemes, as expected, the average probability
assigned to the true fault increases with increasing
evidence, from about 0.15 with 1 finding, to about 0. 47
with 10 findings.

fault.

5.3 The magnitude of priors and the
performance of infinitesimal

probabilities
The infinitesimal probability scheme appears to
perform very well relative to numerical probabilities
for the original car network, in which the prior fault
probabilities are very small, on average 0.00036

To

examine if it performs equally well for larger priors,
we multiplied the prior odds by five odds factors, as
shown in Table

1.

Figure 6 shows the average

probability assigned to the true diagnosis as a function
of the average priors.

Interestingly, the two schemes

are almost indistinguishable up to an average fault
prior 0. 033. Above that, the performance of the
infinitesimal probability drops off sharply - that is,
for average priors of 0.089 and 0.214.

These results

Numerical and Qualitative Probabilistic Reasoning

confirm our ex pect ation that infinitesimal works well
for small priors, but not so well for large pr i ors.
..
-

=
Cl

...
..
....
c

0.4

ordering of diagnosis. A third, would be to evaluate

even more, the quality of decisions will be less rather
than more sensitive to these differences in
representation.

0.3

While these findings are encouraging for the practical
usefulness of infinitesimal pr oba b il ities, we should

.CI
c
...

a.

them. Another way would be to compare the rank
the quality of decisions based on the diagnosi s. In
general, scoring rules based on ranks of diagnosis or,

....

�
=

325

remember that these initial results are on a single
domain. This car model dom ain is simple, with few

0.2

loops and short chains.

This kind of experiment

should be conducted on a wide range of types of
network to see how far these initial results will hold

0.1-

up.
In the introduction, we distinguished view
infinitesimal

o+-----�----�--+-��o.oo1
0 .01
1
o.oo01
0.1
Aver age prior fault probability
Figure 6: Comparison of the average performance of
infinitesimal and numerical probability schemes as a
function of prior fault probabilities.

probabilities,

as

an

(a)

approach

of
to

nonmonotonic reasoning, from view (b), as an
approximation to numerical probabilities.

We

reiterate that this paper, we focus on (b), and we are
not attempting to evaluate its use as an approach to
nonmonotonic logic.

Conclusions about the former

have limited relevance to the latter.
Infinitesimal pro babi lit ies are quite appealing as an
alternative to numerical probab il ities. They should be

6 CONCLUSIONS

significantly easier to eli ci t from experts. Inference

We find these initial results very encouraging in terms

of the diagnostic performance of the infinitesimal
probability scheme. For this example domain, we

found the best performance occurs using E 0.1 to 0.2.
Performance for E
0.01 was slightly worse.
=

may be more effjcient. And resulting inferences should
be somewhat more robust to changes in probabilities.
Some questions that need further investigation
include:

=

Performance of the infinitesimal scheme relative to the
numerical

scheme

does

not

appear

to

Does the best choice of E vary with the domain?

vary

significantly with the quantity of evi dence. The
performance using infinitesimal probability is not

Does these results hold for larger networks, with
more complex structures?

noticeably worse than the numerical probabilities for
prior fault probabilities up to about 0.03. For larger
average fault probabilities, the relative perform ance of

Can this infinitesimal approximation be extended
to utilities and decision making?

infinitesimal probabilities starts to drop off sharply.

This findings suggests that infinitesimal probabilities

Can we obtain a clearer analytic characterization

are more likely to be reliable for diagnosis tasks with

of when performance

very small prior fault probabilities, such as most
machine and electronic devices. They may also work
for some med ical domains, as long as the
priors are less than

disease

1%.

we have used is very simple.

In

addition,

engineering

The mapping from K-values back to probabilities that
More sophistic ated

mappings are pos sible, making use of higher values.

We should also point out that the scoring methods that
we have used to evaluate performan ce have been
based on posterior probability of the true diagnosis,
which is perhaps the most exacting way to compare

will be or won't be

reliable?
we

methods

need practical knowledge
for

eliciting

infinitesimal

probabilities. We an ticipate that, in the long run, the

best p r actical tools will
quantitative methods.

combine qualitative and

326

Henrion, Provan, Del Favero, and Sanders

Intelligence Conference,
Acknowledgments

M. Goldszmidt and J. Pearl.
causal relations.

This work was supported by the National Science
Institute for Decision Systems Research. We would
like to thank David Beckerman for use of the car

pages 99-110, Vermont, 1992.
entropy approach to nonmonotonic reasoning.

refining some of the probabilities.

M.

Shac�ter,

The Logic of Conditionals.

G.F. Cooper. The Computational Complexity of
Probabilistic Inference Using Belief Networks.

Artificial Intelligence, 42:393-405, 1990.
Darwiche. A symbolic generalization of probability
theory. Ph.D. dissertation, Computer Science Dept.,
Stanford University, Palo Alto, CA, 1992.
M.

&

Goldzmidt.

CNETS:

A

computational environment for generalized causal
networks. 1994, {this volume).

M. Druzdzel and M. Henrion. Efficient reasoning in

Proceedings of
the American Association for Artificial Intelligence
Conference, pages 548-553, Washington D.C., 1993.
J. Druzdzel. Probabilistic Reasoning in Decision
Support Systems: From Computation to Common
Sense. PhD thesis, Department of Engineering and
qualitative probabilistic networks. In

M.

Public

Policy,

Carnegie

Mellon

University,

Pittsburgh, Pa, 1993.

H. Prade. Possibility Theory: an Approach
to Computerized Processing of Uncertainty. Plenum

D. Dubois and

Press, NY, 1988.

Utility Theories: Measurements and
Applications. Kluwer Academic, 1992.
H. A. Geffner. Default Reasoning: Causal and Conditional

W. Edwards.

Theories.
M.

MIT Press,

Henrion.

and

Cambridge, MA, 1992.
M.

Druzdzel

"Qualitative

propagation and scenario-based explanation of
probabilistic reasoning". In M. Henrion and R.
S h achter,

editors,

Intelligence.

6,

Uncertainty in Artificial

Elsevier

Science

B.V.

(North­

Holland), 1991.
M.

Hendon.

"Search-based methods to bound

diagnostic probabilities in very large belief nets".

M.

In Proceedings of Conf on Uncertainty and Artificial
Intelligence, 1991.
Ginsberg. Readings in Nonmonotonic Reasoning.
Morgan Kaufmann, San Mateo, CA, 1987.

M. Goldszmidt and J. Pearl. System Z+ :A formalism
for reasoning with variable strength defaults. In

Proceedings of American Association for Artificial

editors,

Uncertainty in Artificial

Intelhgence, pages 129-138. Elsevier Science B.V.

D. Reidel,

Dordrecht, Netherlands, 1975.

D arwiche

IEEE Transactions on Pattern Analysis and Machine
Intelligence, 15:3:220-232, 1993.
He nrion. An Introduction to Algorithms for
Inference in Belief Networks. In M. Henrion and R.


