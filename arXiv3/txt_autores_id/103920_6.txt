
This paper presents a new approach for com­
puting posterior probabilities in Bayesian
nets, which sidesteps the triangulation prob­
lem. The current state of art is the clique tree
propagation approach. When the underlying
graph of a Bayesian net is triangulated, this
approach arranges its cliques into a tree and
computes posterior probabilities by appropri­
ately passing around messages in that tree.
The computation in each clique is simply di­
rect marginalization. When the underlying
graph is not triangulated, one has to first tri­
angulated it by adding edges. Referred to
as the triangulation problem, the problem of
finding an optimal or even a "good" trian­
gulation proves to be difficult. In this pa­
per, we propose to first decompose a Bayesian
net into smaller components by making use
of Tarjan's algorithm for decomposing an
undirected graph at all its minimal complete
separators. Then, the components are ar­
ranged into a tree and posterior probabili­
ties are computed by appropriately passing
around messages in that tree. The compu­
tation in each component is carried out by
repeating the whole procedure from the be­
ginning. Thus the triangulation problem is
sidestepped.

1

INTRODUCTION

There has been in recent years extensive research in
computing marginal and posterior marginal probabili­
ties in Bayesian nets. Largely due to the work of Pearl
(1988), Lauritzen and Spiegehalter (1988), Shafer and
Shenoy (1988), and Jensen et al. (1990), an effi­
cient algorithm called clique tree propagation has come
into being. When the underlying graphs are triangu­
lated, the algorithm arranges the cliques of the un­
derlying graphs into join trees and obtains the wanted
marginals and posteriors by passing messages around
in those trees. The computation in each clique is

simply direct marginalization. When the underlying
graphs are not triangulated, one needs to triangulate
them first (Lauritzen and Spiegehalter 1988), or ap­
peal to the technique of conditioning on loop cutsets
(Pearl 1988). A similar preprocessing step is also re­
quired in the goal directed approach of shacter et a/.
(1990). In this paper, we shall refer to the problem
of finding an optimal or a "good" triangulation for an
untriangulated graph as the triangulation problem.
It is NP-hard to find either optimal loop cutsets
(Stillman 1990) or optimal triangulations (Yannakakis
1981). Although heuristics are available, Stillman
(1990) has "demonstrated that no heuristic algorithm
.... can be guaranteed to produce loop cutsets within a
constant difference from optimal". We conjecture that
the same thing is true for triangulation. Thus, it is in­
teresting to investigate the possibility of approaches
that sidestep the triangulation problem.
Poole and Neufeld (1991) describes an interesting im­
plementation of Bayesian nets in Prolog. The imple­
mentation encodes conditional probabilities as Prolog
goal reduction rules and it organizes reasoning by us­
ing Bayes' Theorem and causality relationships among
the variables. The need for triangulation is avoided.
In this paper, we describe a new approach for comput­
ing posterior probabilities in Bayesian nets, which also
sidesteps the triangulation problem. What differenti­
ates our approach from Poole and Neufeld's approach
is that the former makes full use of decompositions,
while the latter does not at all. The cornerstone of
our approach is Tarja.n's algorithm for decomposing
undirected graphs.
An undirected graph may contain many complete sep­
arators even if it not triangulated. Tarjan (1985)
presents a O(nm) time algorithm for decomposing an
undirected graph at all its minimal complete separa­
tors, where n and m are the numbers of vertices and
edges of in the graph respectively. It is easy to con­
ceive a procedure that decomposes a Bayesian net into
as many components as possible by using Tarjan's al­
gorithm on the moral graph.
Our approach can be viewed as an advancement of the

Sidestepping the Triangulation Problem in Bayesian Net Computations

clique tree propagation approach. Like the clique tree
approach, this approach first arranges the components
of a Bayesian net into a tree, and then computes poste­
rior probabilities by appropriately passing information
around in that tree. Unlike the clique tree approach,
the computation in each component is not carried out
by direct marginalization. Rather it is carried by re­
peating the whole procedure from the beginning.
The organization of the paper is as follows. Section
2 starts the paper by reviewing some graph theory
terminologies and some basic concepts pertaining to
Bayesian nets. Section 3 introduces the concept of de­
composition in terms of semi-Bayesian nets. Section 4
shows how decomposition leads to the parallel reduc­
tion technique. Section 5 introduces serial reduction the basic means for passing messages from one com­
ponent to another, and section 6 constructs the com­
ponent tree - the basic means for organizing message
passing. The algorithm is given in section 7, together
with an example illustrating how it works. The paper
concludes at section 8.
2

PREREQUISITE

no parents are roots. The set of all the parents of a
vertex v will be notated by 1r(v). A directed cycle is a
sequence of vertices in which every vertex is a parent
of the vertex after it and the last vertex is a parent
of the first vertex. An acyclic directed graph is one in
which there are no directed cycles.
To marry all the parents of a vertex in a directed graph
is to add an (undirected) edge between each pair of its
parents. The moral graph m(G) of a directed graph G
is an undirected graph obtained from G by marrying
all the parents of each vertex respectively and ignoring
all the directions on the arcs. A directed graph is
connected if its moral graph is.
Let us now review a few concepts pertaining to
Bayesian nets. A Bayesian net}/ is a triplet (V, A, P),
where

1. V is a set of variables.
2. A is a set of arcs, which together with V consti­
tutes a directed acyclic graph G = (V,A).
3. P = {P(vJ7r(v)) : v E V}, i.e P is the set the
conditional probabilities of the all variables given
their respective parents 1.

Let us begin by reviewing some graph theory termi­
nologies. An (undirected) graph G = (V, E) consists
of a set V of vertices and a set E of edges, which are
(unordered) pairs of vertices. A path is a sequence
of vertices in which every pair of consecutive vertices
is an edge. A loop is a path where no vertex appear
twice except the first one and the last one, which are
the same. A chordless loop is a loop in which no pair
of non-consecutive vertices is an edge. A genuine loop
is a chordless loop of length greater than three. A
triangulated graph is one without genuine loops.

Variables in a Bayesian net will also be referred to
as vertices and nodes when the emphasis is on the
underlying graph.

A graph is connected if there is a path between any
two vertices. If a graph G is disconnected, a connected
component of G consists of a subset of vertices in which
vertices are connected to each other but not to vertices
outside the subset, and of all the edges that are com­
posed of vertices in the subset.

Y � V be the set of variables observed and Yo be
the corresponding set of values. Let X � V be the
set of variables of interest. The posterior probability
Px(XIY; Yo) of X in a Bayesian net N' given obser­
vations Y = Yo is obtained by first conditioning Px on
Y = Yo and marginalizing the resulting joint probabil­
ity onto X 2• The problem of concern to this paper is
how to compute P(XIY =Yo)?

A separator of a graph is a subset of its vertices whose
deletion from the graph will leave it disconnected. Two
vertices is separated by a separator if every path con­
necting them contains at least one vertex in the separa­
tor. A separator is minimal if none of it proper subsets
are separators. A subset of vertices is complete if ev­
ery pair of its elements is an edge. Separators can be
complete.
Cliques are maximal complete subsets of vertices.
When the set of all the vertices is complete, we say
that the graph is complete.

A directed graph G = (V, A) consists of a set V of
vertices and a set A of arcs, which are ordered pairs
of vertices. If there is an arc from vertex v1 to vertex
v2, then v1 is a parent of v2 and v2 is a child of v1.
Vertices with no children are leaves and vertices with

The prior joint probability Px of a Bayesian net }/ is
defined by
Px(V) =

IT P(vl1r(v)).

(1)

vEV

Observations may be made about variables.

Let

Developed by Lauritzen and Spiegehalter (1988),
Shafer and Shenoy (1988), Jensen et al (1990), the
clique tree propagation approach consists of three
steps: (1) Triangulate the moral graph of G; (2) ar­
range the cliques into a join tree, and (3) properly pass
messages around in the tree.
Consider the Bayesian net nett in Figure 1 (1). The
following prior probability and conditional probabil­
ities are given as part of the specification: P(c),
P(aic), P(eia), P(fle), P(gJJ), P(bia,g), P(hlb), and
1Note that when vis a root 1r(v) is empty. In such a
case, the expression P(vJlr(v)) simply stands for the prior
probability of v.
2That is to sum out all the variables not in X.

361

362

Zhang and

Poole

are not given. Thus, Bayesian nets are semi-Bayesian
nets whose set of unspecified roots is empty.
In a semi-Bayesian net N = (V,A, R, 1'), the R i s the
set of roots of the directed graph ( V, A) whose prior
probabilities are not given in P. So, we call R the set
(I)

of unspecified roots of (V, A, 1').

1111U

The prior joint potential P.N a semi-Bayesian net .N =
( V, G, R, P) is defined by

Px(V) =

IT

P(vj1r(v)).

(2)

11EY-R

-

And for any subset X of V , the {marginal) potential
Px(X) of X in .N is obtained by marginalizing P#(V)
onto X, i.e by summing out all the variables in V- X.

�.

-�

---Q

(3) l'rop;>plioa

If N is a. Bayesian net, then its prior joint potential and
marginal potentials are exactly the same as its prior
joint probability and marginal probabilities defined in
the previous section.

,.,

Figure 1: Bayesian net, triangulation and message
passing.

P(dJc, h). The moral graph of its underlying graph
is shown 1 (2). The dashed edges are added for tri­
angulation. The corresponding join tree is shown in
Figure 1 (3). One can obtain Pnttl (dJe =eo) by pass­
ing messages in the way as indicated by the arrows.
See the cited papers for details about the contents of
the messages and how are they actually passed in the
join tree.
Let X be the set of variables of interest. Let Y be
the set of observed variables, and Yo be the corre­
sponding set of observed values. We are concerned
with queries of the form P#(XJY = Yo). Noticing
that P#(XIY =Yo) can be obtained normalizing the
marginal probability P.N(X, Y =Yo), we shall concern
ourselves with queries of the form P.N(X, Y = Yo) in
this paper.

3

DECOMPOSITION

In this section, we shall introduce the concept of de­
composition in terms semi-Bayesian nets.
A semi-Bayesian net .N is a quadruplet .N

(V, A, R, 1'), where

=

1. V is a set of variables.
2. A is the set of arcs, which together with V con­
stitutes a directed acyclic graph G =(V, A).
3. R is a set of roots of G.
4. 1' = {P(vj1r(v)): v E (V- R)} .
In words, semi-Bayesian net are Bayesian nets with un­
specified roots, i.e with roots whose prior probabilities

Let us now turn to decomposition of undirected
graphs. An undirected graph G = (V, E) decomposes
into Gt = (Vi,Et) and G2 = ( V2,E2), or simply into
Vt and V2 if:

1. Vt and V2 are proper subsets of V and Vt U V2 = V,
2. Ei (i = 1, 2) consists of all the edges in E that lie
completely within v; and Et U E2 = E, and
3. Vt n V2 is a complete separator of G.
In such a case, we say that G is de c o m p osable.
There is a one-to-one correspondence between com­
plete separators and decompositions. Given any com­
plete separator S of an undirected graph G, G can
be decomposed at s into to Vi and v2 such that
VI n v2 = s. And if G decomposes into vl and v2,
then Vt n V2 is a complete sep arator. Tarjan (1985)
presents an O(nm) time algorithm for decomposing an
undirected graph at all its minimal complete separa­
tors, where n and m stands for the numbers of ver­
tices and edges of G respectively. Tarjan's algorithm
is very important to our approach for computing pos­
terior probabilities in Bayesian nets.
A semi-Bayesian net .N =(V, A, R, 1') is decomposable
if the moral graph m ( G) of the underlying directed
graph G = (V, A) is decomposable. A decomposi­
tion {Vt, V2} of m( G) induces the decomposition of the
semi-Bayesian net N into .Nt

J/2 = (V2, A2, R2, 1'2), where
•

•

= (Vt, At, R1, PI) and

P1 is obtained from 1' by removing all the items
that do not involve any variables in V1 - V2 and
1'2 =1' -1'1.

And fori

E

{1, 2},

- A, is obtained A by removing any arcs whose

two vertices do not simultaneously appear in
any item of 'Pi, and

Sidestepping the Triangulation Problem in Bayesian Net Computations

Px(X,z, Y ) =

E Px, (X,S)Px,(S,Y).

(3)

S-Z

Figure

- R;

is

2:

The theorem follows from the definition decomposi­
tions of semi-Bayesian nets and the distributivity of
multiplication w .r .t summation. Instead of giving the
detailed proof, we shall provide an illustrating exam­
ple.

A decomposition of net 1.

the

("', A;' 'P;).

set

of

unspecified

roots

of

Consider the Bayesian net net1 in Figure 1 (a). The
The
set {a, b} is a minimal complete separator.
moral graph of the directed graph decomposes into
{a,b,c,d,h} and {a,b,e,f,g}. This induces the de­
composition ofnet1 into semi-Bayesian nets net2 and
net3 as shown in Figure 2. In net2, P(c), P(alc),
P(hlb), and P(dlb,c) are inherited from net1. There is
no arc from a to b because they do not simultaneously
appear in any of those (conditional) probabilities of
net2. The root b of net2 is unspecified because there
is no P( b) in net2. The conditional probabilities in
net3 are P(ela), P(fle), P(glf), and P(bla,g). The
root a is unspecified because there is no P(a).
A semi-Bayesian net is simple if it contains onl y one
leaf node and all other nodes are parents of this leaf
node. In any semi-Bayesian net that is not simple,
there is always a leaf node, say l. The set 1r(l) of the
parents of { is a complete separator, which separates I
from all the nodes not in 1r(l) U {1}. So, there always
a minimal complete separator S that separates l from
all the nodes not in S U {1}. Thus we have

Proposition 1 If a semi-Bayesian net is not simple,
then it is decomposable.
Remark: We do not want to decompose a Bayesian
net at a separator that is not complete for two rea­
sons. First, it increases the complexity of the problem
itself, and second the number of separators that are
not complete may be too large.

4

PARALLEL REDUCTION

In this section, we shall show how the concept of de­
composition can be used in computing marginal po­
tentials in semi-Bayesian nets (marginal probabilities
in Bayesian nets). First of all, we have the following
theorem.

Theorem 2 Suppose a semi-Bayesian net }/ decom­
poses into }/1 and }/2 at a minimal complete separator
S {of the moral graph of its underlying directed graph).
Let X be a subset of variables of J./1 which do not ap­
pear in S, let Y bet a subset of variables of J/2 which
do not appear in S, and let Z be a subset of S. Then

Suppose we want to compute Pnan(d,a,e) in the
Bayesian net net1 shown Figure 1 (1). By definition,
we have

Prun(d ,a,e)
=

L P(c)P(alc)P(eia)P(fle)P(glf)

c,b,j,g,h

P(bla, g)P(h]b)P(dic, b)

= L {L P(c)P(alc)P(hlb)P(dlc, b)}
b

c,h

{L P(ela)P(f le)P(ulf)P(bla, g)} (4)
J,g

=L;Pnatz(d,a,b)Pnat3(a,b,e).
b

(5)

Equation (4) is true because of the distributivity of
multiplication w.r.t summation, and equation (5) im­
mediately follows from Lhe definition of marginal po­
tentials in semi-Bayesian nets.
Suppose we want to compute the marginal potential
in a semi-Bayesian net N. And sup­
pose )1/ decomposes into J./1 and J./2 at a minimal com­
plete separator S of the moral graph of its underlying
directed graph. Let X1 be the set of variables in X
and in J./1 but not in S, X:� be the set of variables in
X and in J./2 but not in S, and Xs =X n S. The sets
Yt, Y2 and Ys are defined from Y in the same way.
Let Y-1 = YsUYz and Y-2 = YsUYt. Let (Y-t)a and
(Y-2)o be the corresponding sets of values of Y_1 and

Px(X,Y =Yo)

Y-2·

The query induced in }/1 by Px(X,Y = Yo) is
Px,(Xt , S- Ys,Y-2 = (Y-2)o), and the query induced
in J/2 is Px,(Xz,S- Ys,Y-1 = (Y-do). According to
Theorem 2, we have

PN(X,Y =Yo)
L

Px,(Xt,S - Ys,Y-2 = (Y-z)o)

5-Xs-Ys

PN,(Xz,S- Ys, Y-1 = (Y-t)o)

(6)

Px(X,Y = Yo), we can first com­
Ys,Y-2 = (Y-2)o) and Px,(X2,S­
Ys, Y-t = (Y-do) (possibly in parallel), and then

Thus to compute
pute Px1 (X t,S-

363

364

Zhang and Poole
combine the results by equation (6). This leads to
the technique of parallel reduction.

gin by introducing the concept of parametric semi­
Bayesian nets.

Given a semi-Bayesian net /11, the following procedure
computes PJI(X, Y =Yo):

A parametric semi-Bayesian net /II = (V, A, R, 'P) is a
semi-Bayesian net, except that some of its conditional
probabilities contain parameters, i.e variables that are
not members of V. Thus, semi-Bayesian nets are para­
metric semi-Bayesian nets that do not contain any pa­
rameters.

Parallel-Reduction
H /II is simple, then compute PJ,/(X, Y =Yo)

directly by marginalization,
else find a minimal complete separator S of
the moral graph of the underlying directed
graph of /If, decompose /If at S into /111 and
/112 .
1. Repeat the procedure to compute the
induced query Px. (XI. S- Ys, Y -2 =
(Y-2)o),
2. Repeat the procedure to compute the
induce query Px2(X2,S- Ys,Y-1 =
(Y-do),
3. Combine the answers to the two induced
queries by using equation (6).
The procedure is termed parallel reduction because line
1 and line 2 can be executed strictly in parallel.
Because of Proposition 1, the procedure Parallel Re­
duction is able to compute marginal potentials in semi­
Bayesian nets without triangulating the underlying
graphs. But the algorithm can be very inefficient.
The main purpose of this paper is to describe another
algorithm called component tree propagation, which
we hope is as efficient as the clique tree propagation
approach based on an optimal triangulation. The algo­
rithm computes posterior probabilities in a Bayesian
net as follows: first the Bayesian net is decomposed
into components, the components are arranged into a
tree, and then posterior probabilities are obtained by
appropriately passing messages around in that tree. In
the next section, we shall introduce the basic means
for passing messages from one component to another
- the serial reduction technique. In the section af­
ter, we shall present the basic means for controlling
message passing - component trees.

5

SERIAL REDUCTION

Suppose a semi-Bayesian net /II decomposes into two
components. A query in /II reduces into two sub­
queries, one in each of the components. The paral­
lel reduction procedure first computes both of the two
subqueries (possibly in parallel), and then use an ad­
ditional formula to combine the answers to get the
answer to the original query. In contrast, the serial
reduction procedure will first compute only one of two
subqueries. The answer is then send to the other sub­
query, which is thereby updated. The answer to the
updated subquery is the same as the answer to the
original query.
This section is devoted serial reduction. Let us be-

Suppose /II is a semi-Bayesian net with parameters
W . And suppose we want to compute the potential
of (X, Y =Yo). The answer will be a function of the
parameters W as well as of X. So, we shall write
the query as PJ,/(X,Y = Yo : W), where the column
mark separates parameters from variables in the semi­
Bayesian net.
Given a query PJ,/(X, Y = Yo : W) in a parametric
semi-Bayesian net /11 , a node is laden if it is a leaf of
/II and it is in the set Y (i.e it is observed ).
Suppose /II decompose into /111 and N2 at a minimal
complete separator S. Let the sets X1, Xs, X2, Y1,
Ys, Y2, Y_1, and Y_2 are defined as before. Let W1
and w2 be the parameters of /Ill and /112 respectively.
As in the case of parallel reduction, the query Q:
PJ,/(X, Y =Yo : W) induces a subquery Ql in /111 and
a subquery Q2 in /112. The induced subquery Q1 is
P.N1 (XI. S- Ys , Y -2 = (Y-2)o : WI), and the induced
subquery Q2 is PJ1l(X2, S- Ys , Y- 1 = (Y-do : W2).
The answer to the subquery Q2 is a function fo(X2, S­
Ys , W2). We extend it to be a function f(X2, S, W2)
by setting
2!(X2' S ' W:)
_

To
to

{ fo(Xz,S- Ys,W2)
0

append the answer

ifYs = (Ys)o
otherwise

f(X- 2, S, W2) of Q2

to

/111 is

1. Introduce an auxiliary variable v into /111, which
has only two possible values 0 and 1,
2. Draw an arc to v from each variable inS,
3. Set P(v =OI S) = !(X2,S, W2),
Let /II{ denote the resulting semi-Bayesian net. To use
the query Q is to replace
it by Q': PJI:(X�,Xs,Y-2 = (Y-2)o,v = 0: X2, W).
Note that the auxiliary variable v is a laden variable
in the updated subquery. Also note that the the semi­
Bayesian net /II{ contains the parameters W U X2 .
the answer of Q2 to update

Theorem 3 Suppose /II is semi-Bayesiant net with

parameters W, which decomposes at a minimal com­
p l ete separator S into /111 and /112. Let all the symbols
be as defined above. Then

PJI(X,Y =Yo: W) =
PJit(Xt,Xs, Y-2 = (L2)o,v

=

0: X2, W).(7)

Sidesteppin g the Triangulation Problem in Bayesian Net Computations

(1 �.>
(]) l
��
-

"""

Q

0-0>

-

�: ···:·
y

�

,

G)�

...

-

(

�

aotiO

Figure 3: A serial reduction of net1.

Px(X, Y ==
Yo : W), we can first compute the induced subquery
Px,(X2,S- Ys, Y_t = (Y-do : W), use the answer

�..e---Q
-

lidII

The theorem says, to compute the query

to construct Jl{, and then compute the updated query
P.w;(X1,Xs,Y_2 = (Y-2)0 : X2, W). This procedure

Figure 4: A (the) component tree for nett.

Again, instead of give the detailed proof of the theo­

all the non-trivial minimal complete (NMC) separators
of J1 and decompose Jl at them into smaller compo­
nents. The component tree of J1 is a tree whose nodes
consists of all those smaller components. The tree is

is called serial reduction.

rem, we shall provide an illustrating example. Con­
sider computing Pnetl (d, e ) in the Bayesian net net 1
shown in Figure 1 (1). The net decomposes into net4
and netS (in Figure 3) at the minimal separator {a, g }.
"\Ve first compute Pnets(a,g,e ) , and append the result
to net4. This gives us net6. Drawn in dotted cycle,
v is an auxiliary laden variable introduced. The con­
ditional probability of v = 0 give a and g is set by
P(v = Oja,g) == Pnets(a,g, e ) . Thus net6 contains the
parameter e. The updated query is Pnet6(d,v == 0: e).
To see that Pnd6(d,v = 0: e) = Pnen(d,e), we notice
that net6 decomposes into net4 and net7, and that

Pnea(a,g,v = 0)
Pnet6(d,v
==

=

=

P(v

=

0: e )

L Pnet4 (d,a, g)Pnet1 (a,g,v

==

0)

2: Pnet4(d,a, g)Pnecs (a,g,e)
a, g

==

constructed as follows:
Start with an arbitrary component. While
there are still components left out of the tree,
choose one such component, say C, which
contains an NMC separator that is also con­
tained in one or more components in the tre€.
Add C to the tree by connect it to one of
those components that also contain the NMC
separator. 3.

Ola,g) = Pnecs(a,g,e). So

a,g
==

algorithm, one can easily conceive a procedure to find

Pnen(d,e).

For the Bayesian net net1 in Figure 1, there are three
NMC separators are: {c ,h}, {a,b}, and {a,g}. As
shown in Figure 4 the resulting components are netB
with prior probability P(c) and conditional proba­
bilities P(djc, h); net9 with conditional probabilities

P(alc) and P(hlb); netlO with conditional probabil­
P(bla,g); and net11 with conditional probabilities
P(e/a), P{f/e), and P(g/f). In this example, there is

ity

only one component tree, which is denoted by treel

and is shown in Figure 4.

6

COMPONENT TREES

Given a query Px(X,Y = Yo : W) in a parametric
semi-Bayesian net J./, a minimal complete separator
(of the moral graph of the underlying directed graph
of J./) is trivial if it is a subset of r(l) for some laden
node l and its deletion from the moral graph only re­
sult in no more than two connected components.
Tarja.n's algorithm decomposes an undirected graph
at all its minimal complete separators. Based on this

The union of two semi-Bayesian nets (Vt,A,,R,,Pt)
and (V2, A2,R2, 1'2) is the semi-Bayesian net (Vt U
V2,A1 U A2,R,1't U 1'2), where R is the set of un­
specified roots of (Vt U V2, A, U A2, 1', U 1'2).
SupposeT is a component tree of a semi-Bayesian net
N, and )/1 is a leaf of T. Then )./ decomposes into
)/1 and )/2 - the union of all other nodes ofT. Let
Q be a query in J1 and let Q1 be the induced query in
3It can be proved that this construction does result in
a tree

365

366

Zhang and Poole

N1• Then we can use the answer to Q1 to update the

0�---.

query Q.
7

�:.�-=

COMPONENT TREE
PROPAGATION

1111t10•

We are now ready to give the component tree propaga­
tion algorithm. Let N be a parametric semi-Bayesian
net with parameters W. Here is our algorithm for com­
puting the answer to the query PJI(X, Y = Yo : W).

.....

Main{N, (X, Y =Yo)):
1. If N has NMC separators, call the pro­
cedure Serial�Reduction(N, (X, Y =
Yo)).
2. If N does not have any NMC sep­
arators, call the procedure Parallel·
Reductionl(N,

Serial-Reduction(N,

(X, Y =Yo)).

(X, Y =Yo)):

1. Decompose N at all its NMC separators,
and construct a component tree T.
2. While there is at least two nodes in T
do
•

•

•

Pick a leaf N1 of T by calling the pro­
cedure Pick-leaf(T, (X, Y =Yo)),
Call Main to compute the induced
subquery Q1 in N1,
Append the answer of Q1 to the com­
ponent that is the neighbor of N1 in
T.

•

Remove N1 from T and use the an­
swer to Q1 to update the current
query.

3. When there is only one node left in T,
call Main to compute the current query.

In each pass of the while loop, the current query is
updated. According to Theorem 3, the answer to the
current query is the same as the answer to the updated
query. Thus, the answer to the current query when
there is only one node left in T is the same as the
answer to the original query P.N(X, Y =Yo : W).
The input of to the procedure Parallel-Reductionl
is parametric semi-Bayesiant net N and a query
P.N(X, Y = Y0 : W) such that N does not have any
NMC separators. The output is the answer to the
query.
Parallel-Reductionl(N,

(X, Y = Yo)):

If N is simple, then compute the answer to
the query P.N(X, Y = Yo : W) directly by
marginalization,
else

Figure 5: Semi-Bayesian nets created in computing
Pnen(d, e ) .

1. Pick a laden node

1 by calling the pro­
cedure Pick-laden-node(N, (X, Y =

Yo)),
2. Decompose N at the set 1r(l) of parents
of l into N1 and N2,
3. Call Main to compute the induced sub­
squeries in N1 and in N2,

4. Combine the answers to the subqueries
using equation (6).

Our primary investigations indicate that the proce­
dures Pick-leaf and Pick-laden-node are important
to the the performance of the algorithm. How can one
define those procedures so that the al gorithm achieves
its optimal performance and how does this optimal
performance compare to the performance of the clique
tree propagation approach are topics for future re­
search.
To end this section, let us look at an example. Con­
sider computing Pnetl ( d, e ) in the Bayesian net net 1
shown in Figure 1. Since net1 has NMC separators,
the procedure Serial-Reduction will be called. The
procedure will first decompose net 1 into components
(semi-Bayesian nets) netS, net9, net10, and net 1 1
(Figure 4). It will then arrange those components into
a component tree tree1, which has two leaves netS
and net11. If the procedure Pick-leaf first returns
net11, then the induced query Pnem(e,a,g) will first
be computed. The answer will be appended to the
neighbor net10 of netll in tree1, resulting in the
parametric semi-Bayesian net net10 1 (Figure 5) with
auxiliary variable v1 and parameter e. The answer will
also be used to update the query Pnetl ( d, e ) to the new
query Pnetu(d, v1 = 0 : e ) , where net 12 stands for the
union of netS, net9, and net 101•
After net11 is removed from tree1, there are
again two leaves netS and net tO'. The current
query Pnetl2( d, VJ = 0 : e) induces the query
Pnetl o•(a,b,vl =0: e) in net101• I!net10' is chosen
by Pidc-leafthis time, the induced query will be com­
puted. Its answer will be appended to net9, resulting

Sidestepping the Triangulation Problem in Bayesian Net Computations

in net9 1 (Figure 5). The answer will also be used to
update the current query PMn2(d, v1 = 0: e) to the
new query Pnen3 (d,v2 = 0: e), where net13 stands
for the union of netS and net9 1 •
If Pick. leaf now picks net9 1 , then the query
Pnet9' ( c, h, v2 = 0 : e) induced in net9' by the cur­
rent query Pnen3(d, v2 = 0,: e) will be computed. Its
answer will be appended to netS, resulting in netS'.
And the answer will also to used to update the cur­
rent query Pne113(d, v2 = 0 : e) to the new query
Pnets•(d, V3 = 0: e).
On the next level, PnetlO' (a, b, v1 = 0 : e) and
Pnets•(d, V3 = 0 : e) will be computed by the pra.
cedure Parallel·Reductionl since the parametric
semi-Bayesian nets net 10' and netS' do not have any
NMC separators. On the other hand, net11 andnet9'
do have NMC separators. So, the procedure Serial·
Reduction will again be called in computing both
Pnetu(a,g,e) and Pnet9•(c,h,v2 = 0: e).
We notice that there was no triangulation in the above
process of computing Pnett(d,e), while triangulation
is a must in the clique tree propagation approach (see
section 2).
8

CONCLUSIONS

In this paper, we have described a approach for com­
puting posterior probabilities in a Bayesian net, which
sidesteps the triangulation problem. Our approach
begin by decomposing the Bayesian net into smaller
components by making use of Tarjan's algorithm for
decomposing undirected graphs at all their minimal
complete separators. Like the clique tree approach,
our approach arranges those components into a tree,
and then computes posterior probabilities by appropri·
ately passing information around in that tree. Unlike
the clique tree approach, the computation in each com­
ponent is not carried out by direct marginalization.
Rather it is carried by repeating the whole procedure
from the beginning. Thus, the need for triangulation
is avoided.
How does the performance of our approach compare to
that of the clique tree propagation approach based on
an optimal triangulation? This question is yet to be
answered. Our hope is that proper choices of the pro.
cedure Pick-leaf' and Pick-laden-node c ou l d ensure
the performance of our approach lay close to optimal.

Acknowledgement
The first author gained his understanding of Bayesian
nets when he was with Glenn Shafer and Prakash
Shenoy at Business School, University of Kansas from
October 1987 to October 1988 and from September
1989 to August 1990. The paper has benefited from
comments by D'Ambrosio and the reviewers for UAI
92. Research is supported by NSERC Grant OG-

P0044121.



We present a technique for speeding up
the convergence of value iteration for par­
tially observable Markov decisions processes
(POMDPs). The underlying idea is similar
to that behind modified policy iteration for
fully observable Markov decision processes
(MDPs). The technique can be easily incor­
porated into any existing POMDP value it­
eration algorithms. Experiments have been
conducted on several test problems with one
POMDP value iteration algorithm called in­
cremental pruning. We find that the tech­
nique can make incremental pruning run sev­
eral orders of magnitude faster.

1

INTRODUCTION

POMDPs are a model for sequential decision making
problems where effects of actions are nondeterministic
and the state of the world is not known with certainty.
They have attracted many researchers in Operations
Research and AI because of their potential applica­
tions in a wide range of areas (Monahan 1982, Cas­
sandra 1998b). However, there is still a significant gap
between this potential and actual applications, primar­
ily due to the lack of effective solution methods. For
this reason, much recent effort has been devoted to
finding efficient algorithms for POMDPs.
This paper is concerned with only exact algorithms.
Most exact algorithms are value iteration algorithms.
They begin with an initial value function and improve
it iteratively until the Bellman residual falls below a
predetermined threshold. See Cassandra (1998a) for
excellent descriptions, analyses, and empirical compar­
isons of those algorithms.
There are also policy iteration algorithms for
POMDPs. The first one is proposed by Sondik (1978).

A simpler one is recently developed by Hansen (1998).
It is known that, in terms of number of iterations, pol­
icy iteration for MDP converges quadratically while
value iteration converges linearly (e.g. Puterman 1990,
page 369). Hansen has empirically shown that his pol­
icy iteration algorithm for POMDPs also converges
much faster than one of the most efficient known
value iteration algorithms, namely incremental prun­
ing (Zhang and Liu 1997, Cassandra et a/1997).

Policy iteration for MDPs solves a system of linear
equations at each iteration. The numbers of unknowns
and equations in the system are the same as the size of
the state space. Consequentially, it is computationally
prohibitive to solve the system when the state space
is large. Modified policy iteration (MPI) (Puterman
1990, page 371) alleviates the problem using a method
that computes an approximate solution without actu­
ally solving the system. Numerical results reported in
Puterman and Shin (1978) suggest that modified pol­
icy iteration is more efficient than either value iteration
or policy iteration in practice.
Hansen (1998) points out that the idea of MPI can
also be incorporated into his POMDP policy iteration
algorithm and finds that such an exercise is not very
helpful (Hansen 1999).
The paper describes another way to apply the MPI
idea to POMDPs. Our method is based on the view
that MPI is also a variant of value iteration (van Nunen
1976) 1. Under this view, the basic idea is to "improve"
the current value function for several steps using the
current policy before feeding it to the next step of value
iteration. Those improvement steps are less expensive
than standard value iteration steps. Nonetheless, they
do get the current value function closer to the optimal
value function.
MPI for MDPs improves a value function at all states.
1 As a matter of fact, it was first proposed
of value iteration by van Nunen.

as

a variant

Speeding Up POMDP Value Iteration

This cannot be done for POMDPs since there are infi­
nite many belief states. Our method improves a value
function at a finite number of selected belief states. A
nice property of POMDPs is that when a value func­
tion is improved at one belief state, it is also improved
in the neighborhood of that belief state.
We call our method point-based improvement for the
lack of a better name. It is conceptually much simpler
than Hensen's policy iteration algorithm. Nonetheless,
it is as effective as, in some cases more effective than,
Hansen's algorithm in reducing the number of itera­
tions it takes to find a policy of desired quality and
hence drastically speeds up incremental pruning.
2

2.1

POMDPs

A partially observable Markov decision process
(POMDP) is a sequential decision model for an agent
who acts in a stochastic environment with only partial
knowledge about the state of the environment. The set
of possible states of the environment is referred to as
the state space and is denoted by S. At each point in
time, the environment is in one of the possible states.
The agent does not directly observe the state. Rather,
it receives an observation about it. We denote the set
of all possible observations by Z. After receiving the
observation, the agent chooses an action from a set
A of possible actions and execute that action. There­
after, the agent receives an immediate reward and the
environment evolves stochastically into a next state.
Mathematically, a POMDP is specified by the three
sets S, Z, and A; a reward function r(s,a); a transi­
tion probability P( s 'j s, a); and an observation probabil­
ity P(zjs', a). The reward function characterizes the
dependency of the immediate reward on the current
state s and the current action a. The transition prob­
ability characterizes the dependency of the next state
'
s on the current state s and the current action a. The
observation probability characterizes the dependency
of the observation z at the next time point on the next
state s' and the current action a.
2.2

contained in the current observation, previous obser­
vations, and previous actions can be summarized by
a probability distribution over the state space. The
probability distribution is sometimes called a belief
state and denoted by b. For any possible states, b(s)
is the probability that the current state is s. The set of
all possible belief states is called the belief space. We
denote it by B.
A policy prescribes an action for each possible belief
state. In other words, it is a mapping from B to A.
Associated with policy 1r is its value function V". For
each belief state b, V" (b) is the expected total dis­
counted reward that the agent receives by following
the policy starting from b, i.e.

POMDP AND VALUE
ITERATION

We begin with a brief review of POMDPs and value
iteration.

Policies and value functions

Since the current observation does not fully reveal the
identity of the current state, the agent needs to con­
sider all previous observations and actions when choos­
ing an action. Information about the current state

697

00

V"(b)

=

E,,b["L A1rt]
t=O

where r1 is the reward received at timet and A (0 <
A < 1) is the discount factor. It is known that there
'
exists a policy 7r* such that V" (b) � V"(b) for any
other policy 1r and any belief state b. Such a policy
is called an optimal policy. The value function of an
optimal policy is called the optimal value function. We
denote it by v•. For any positive number E, a policy
1r is E-optimal if
V"(b) + f?: V*(b)
2.3

Vb

E B.

Value iteration

To explain value iteration, we need to consider how
belief state evolves over time. Let b be the current
belief state. The belief state at the next point in time
depends not only on the current belief state, but also
on the current action a and the next observation z.
We denote it by b�. For any states', b�(s') is given by

baz (8')

=

E. P(s', zjs, a)b(s)
P(zjb, a)
'

(1)

and
where
P(z, s'js, a)=P(zjs', a)P(s'js,a)
P(zjb, a)= Es,s' P(z, s'js, a)b(s) is the renormalization
constant. As the notation suggests, the constant can
also be interpreted as the probability of observing z
after taking action a in belief state b.
Define an operator T that takes a value function V
and returns another value function TV as follows:
TV(b) = maxa[r(b,a) +A

L P(zjb, a)V(b�)]Vb E B

(2)

z

where r(b, a) = E. r(s, a)b(s) is the expected imme­
diate reward for taking action a in belief state b. For

698

Zhang, Lee, and Zhang

a given value function V, a policy
improving if
7r(b)

=

arg maxa[r(b, a)+ A

71"

is said to be V ­

L P(zlb, a)V(b�)]

VI:

1. Vo +- {0}, n +- 0.
2. do {
3.
n +- n + 1.
Vn +- DP-UPDATE(Vn-1).
4.
5. } while maxbiVn(b)- Vn-1(b)l > <(1- >.)/2>..
6. return Vn.

(3)

z

for all belief states b.
Value iteration is an algorithm for finding <-optimal
policies. It starts with an initial value function Vo and
iterates using the following formula:
Vn

=

TVn-1·

It is known (e.g. Puterman 1990, Theorem 6.9)
that Vn converges to V* as n goes to infinity.
Value iteration terminates when the Bellman residual
ma.xb IVn(b)- Vn-1(b)l falls below <(1- >.)/2>.. When
it does, a Vn-improving policy is <-optimal.
Since there are infinite many possible belief states,
value iteration cannot be carried explicitly. Fortu­
nately, it can be carried out implicitly. Before explain­
ing how, we first introduce several technical concepts
and notations.
2.4

Technical and notational considerations

For convenience, we call functions over the state space
vectors. We use lower case Greek letters a and (3 to
refer to vectors and script letters V and U to refer to
sets of vectors. In contrast, the upper letters V and U
always refer to value functions, i.e. functions over the
belief space B. Note that a belief state is a function
over the state space and hence can be viewed as a
vector.
A set V of vectors induces a value function as follows:
f(b)

=

maxaEva.b

Vb

E

B,

where a.b is the inner product of a and b, i.e.
a.b= Ls a(s)b(s). For convenience, we also use V(.)
to denote the value function defined above: For any
belief state b, V(b) stands for the quantity given at the
right hand side of the above formula.
A vector in a set is extraneous if its removal does not
change the function that the set induces. It is useful
otherwise. A set of vector is parsimonious if it contains
no extraneous vectors.
2.5

Implicit value iteration

A value function V is represented by a set of vectors if
it equals the value function induced by the set. When
a value function is representable by a finite set of vec­
tors, there is a unique parsimonious set of vectors that
represents the function.

Figure 1: Value iteration for POMDPs.
Sondik (1971) has shown that if a value function Vis
representable by a finite set of vectors, then so is the
value function TV. The process of obtaining a par­
simonious representation for TV from a parsimonious
representation of Vis usually referred to as dynamic­
programming update. Let V be the parsimonious set
that represents V. For convenience, we sometimes use
TV to denote the parsimonious set of vectors that rep­
resents TV.
In practice, value iteration for POMDPs is implicitly

carried in the way as shown in F igure 1. One be­
gins with a value function Vo that is representable by
a finite set of vectors. In this paper, we assume the
initial value function is 0. At each iteration, one per­
forma dynamic-programming update on the parsimo­
nious set Vn-1 of vectors that represents the previous
value function Vn-1 and obtains a parsimonious set
of vectors Vn that represents the current value nmc­
tion Vn. One continues until the Bellman residual
maxbJVn(b)- Vn-1 (b)l, which is determined by solving
a sequence of linear programs, falls below a threshold.
3

PROPERTIES OF VALUE
ITERATION

This paper presents a technique for speeding up the
convergence of value iteration. The technique is de­
signed for POMDPs with nonnegative rewards, i.e.
POMDPs such that r(s, a)?:O for all s and a. In this
section, we study the properties of value iteration in
such POMDPs and show how a POMDP with negative
rewards can be transformed into one with nonnegative
rewards that is in some sense equivalent. Most proofs
are omitted due to space limit.
We begin with a few definitions. In a POMDP, a value
function U dominates another V if U(b)?:V(b) for all
belief states b. It strictly dominates V if it dominates
V and U(b)>V(b) for at least one belief state b. A
value function is (strictly) suboptimal if it is (strictly)
dominated by the optimal value function.
A set of vectors is (strictly) dominated by a value func­
tion if its induced value function is. A set of vectors

Speeding Up POMDP Value Iteration

is (strictly) suboptimal if it is (strictly) dominated by
the optimal value function.

VI1:

1. Vo +- {0}, n +- 0.
2. do {
3.
n +- n + 1;
Un +- DP-UPDATE(Vn-di
4.
.Sf- maxb\Un(b)- Vn-I(b)\;
5.
if .s > t(1 - >..) /2>..
6.
Vn f- improve(Un)i
7.
8. } while .S > e(1 - >..)/2>...
9. Return Vn·

A set of vectors is (strictly) dominated by another set
of vectors if it is (strictly) dominated by the value func­
tion induced by the that set.
3.I

General properties of value iteration

In any POMDP, if a set of vectors V is
suboptimal, then so is TV. Moreover, if V dominates
another set of vectors V', then TV dominates TV'.

Lemma I

In any POMDP, if a set of vectors V is
strictly suboptimal, then there exist at least one belief
state b such that TV(b)>V(b).

Figure 2: A new variant of value iteration.

Lemma 2

3.2

Unless explicitly stated otherwise, all POMDPs con­
sidered from now on are with nonnegative rewards.

Properties of value iteration in POMOPs
with nonnegative rewards

4

Using Lemma 2, one can show

Consider running VI on a POMDP with
nonnegative rewards. Let Vn-! and Vn be respectively
the sets of vectors produced at the n-lth and nth iter­
ation. Then, Vn-! is strictly dominated by Vn, which
in tum is dominated by the optimal value function.

Theorem I

Note that the theorem falls short of saying that, when
the reward function is nonnegative, TV strictly domi­
nates V for any suboptimal set of vectors V. As a mat­
ter of fact, this is not always the case. As a counter ex­
ample, assume r(s0,a)=O for a certain state so regard­
less of the action. Let b0 be the belief state that is 1
at so and 0 everywhere else. Further assume V* (bo>O
and let n0 be a vector such that no(so)=V*(bo) and
n0 ( s) =0 for any other states s. It is easy to see that
if V consists of only no, then TV(bo)<V(bo).
Despite of the fact TV does not always strictly dom­
inate V, TV(b) is strictly larger than V(b) for beliefs
b in most parts of the belief space when the reward
function is nonnegative.
3.3

699

POMDPs with negative rewards

A POMDP with negative rewards can always be trans­
formed into one with nonnegative rewards by adding
a large enough constant to the reward function. It is
easy to see that an t-optimal policy for the transformed
POMDP is also t-optimal for the original POMDP
and vice versa. Moreover, the value function in the
original POMDP of a policy equals that in the trans­
formed POMDP minus C/(1->..), where Cis the con­
stant added. In other words, the original POMDP is
solved if the transformed POMDP is solved. There­
fore, we can restrict to POMDPs with nonnegative
rewards without losing generality.

S PEEDING UP VALUE
ITERATION

The section develops our technique for speeding up
value iteration in POMDPs with nonnegative rewards.
We begin with the basic idea.
4.I

Point-based improvement

Consider a suboptimal set of vectors V. By improving
V, we mean to find another suboptimal set of vectors
that strictly dominates V. By improving V at a belief
state b, we mean to find another suboptimal set of
vectors U such that U(b)>V(b).
Value iteration starts with the singleton set {0}, which
is of course suboptimal, and improves the set itera­
tively using dynamic-programming update (Theorem
1). Dynamic-programming is quite expensive, espe­
cially when performed on large sets of vectors. To
speed up value iteration, we devise a very inexpensive
technique called point-based improvement for improv­
ing a set of vectors and use it multiple times in be­
tween dynamic-programming updates. This technique
can be incorporated into value iteration as shown in
Figure 2. Applications of the technique are encapsu­
lated in the subroutine improve. The Bellman residual
li is used in improve to determine how many times the
technique is to be used.

If, for any suboptimal set of vectors U,
the output of improve(U, li) - another set of vectors
- is suboptimal and dominates U, then VIl termi­
nates in a finite number of steps.

Theorem 2

Let Vn and V� be respectively the sets of vec­
tors produced at the nth iteration of VIl and VI. From
Lemma 1 and the condition imposed on improve, we
conclude that Vn is suboptimal and dominates V�.

Proof:

700

Zhang, Lee, and Zhang

Since V� monotonically increases with n (Theorem 1)
and converges to V* as n goes to infinity, Vn must also
converge to V*. The theorem follows. D
4.2

R(a,U) = {bEBia.b>a'.b \la'EU\{a}},
R(a,U) = {bEBia.b�a'.b Va'EU\{a}}.

Improving a set of vectors at one belief
state

For the rest of this section, we let V be a fixed sub­
optimal set of vectors and let U=TV. We develop a
method for improving U.
To begin with, consider how U might be improved at a
particular belief state b. According to (2), there exists
an action a such that

U(b) = r(b,a) + A

L P(zlb,a)V(b�).

(4)

z

For each observation z, let f3z be a vector in U that has
maximum inner product with b�. Define a new vector
by

f3(s) = r(s,a) + A

L P(z,s 'ls,a)f3z(s')
z,s

'

Vs E S. (5)

We sometimes denote this vector by backup(b,a, U).
Theorem 3

For the vector f3 given by (5), we have

b.f3 = r(b,a) + A

L P(zlb,a)U(b�).

(6)

z

As pointed out at the end of Section 3.2, U(b�) is of­
ten larger than V(b�) A quick comparison of (4) and
(6) leads us to conclude that b.f3 is often larger than
U(b). When this is the case, we have found a set that
improves U at b, namely the singleton set {(3}. The
set is obviously suboptimal.
.

There is an obvious variation to the idea presented
above. Instead of using the vector backup(b, a, U) for
the action that satisfies (4), we can consider the vectors
backup(b, a',U) for all possible actions a' and choose
the one whose inner product with b is the maximum.
This should, hopefully, improve U at b even further.
We tried this variation and found that the costs are
almost always greater than the benefits.
4.3

of the belief space B respectively given by

Improving a set of vectors at multiple
belief states

It is straightforward to generalize the idea of the previ­
ous subsection from one belief state to multiple belief
states. The question is what belief states to use. There
are many possible answers. Our answer is motivated
by the properties of dynamic-programming update.
For any vector ainU, define its witness region R(a,U)
and closed witness region R(a,U) w.r.t U to be regions

During dynamic-programming update, each vector a
in U is associated with a belief state that is in the
closed witness region of a. We say that the belief
state is the anchoring point provided for a by dynamic­
programming update and denote it by point(a). The
vector is also associated with an action, which we de­
note by action(a). It is the action prescribed for the
belief state point(a) by a V(.)-improving policy. Be­
cause of those, equation (4) is true if b is point(a) and
a is action(a).
We choose to improve U on the anchoring points using

U1

=

{backup(point(a),action(a),U)Ia E U}.

(7)

According to the discussions of the previous subsec­
tion, the value function U1 (.) is often larger than U(.)
at the anchoring points. When a value function is
larger than another one at one belief state, it is also
larger in the neighborhood of the belief state. There­
fore, the value function U1 (.) is often larger than U(.)
in regions around the anchoring points. Our experi­
ence reveal that it is often larger in most parts of the
belief space. The explanation is that the anchoring
points scatter "evenly" over the belief space w.r.t U in
the sense that there is one in the closed witness region
of each vector of U.
There is one optimization issue. Even that the inner
product of the vector backup(point(a), action(a),U)
with the belief state point(a) is often larger than that
of a with the belief state, it might be smaller some­
times. When this is the case, we use a instead of
backup(point(a), action(a),U) so that the value at
the belief state is as large as possible.
4.4

Relation to modified policy iteration for
MDPs

Point-based improvement is closely related to MPI for
MDPs (Puterman 1990, page 371). It can be shown
that, for each anchoring point b,
ul (b)

=

r(b,7r(b)) + A

L P(zlb,7r(b))U(b�(b)),

(8)

z

where 1r is a V(.)-improving policy. This formula is
very similar to formula (6.37) of Puterman (1990).
MDP modified policy iteration uses the latter formula
to "improve" the value of each possible state of the
state space. We cannot apply the above formula to all
possible belief states since there are infinite many of

Speedi ng Up POMOP Value Iteration

improve(U):
1. Uo +-- U, k t-- 0.
2. do {
3. k +-- k+ 1, uk +-- 0, w +-- 0.
4. for each vector a in Uk-1
a' +--backup(point(a),action(a),Uk-1 U W).
5.
6.
if a. point(a)> a' .point(a)
7.
a' +--a .
else W +-- WU {a'}.
8.
point(a') t-- point(a),
9.
10. action(a') +--action(a);
11. uk +--uk u {a'}.
12.} while stop(Uk,Uk-d =false.
13. return uk u u.
Figure 3: The improve subroutine.
them. So, we choose to use the formula to improve the
values of a finite number of belief states, namely the
anchoring points.
4.5

Repeated improvements

We now know how we might improve U at the anchor­
ing points. In hope to get as much improvement as
possible, we want, of course, to apply the technique
on U1 and try to improve it further. This can easily
be done. Observe that there is a one-to-one correspon­
dence between vectors in U and U1: for each vector a
in U, we have backup(point(a),action(a),U) in U1.
We associate the latter with the same belief state and
action as the former. Then we can improve U1 at the
anchoring points the same way as we improve U. The
process can of course be repeated for the resulting set
of vectors and so on.
The above discussions lead to the routine shown in Fig­
ure 3. The routine improves the input vector set at the
anchoring points iteratively. Improvement takes place
at line 5. Lines 6 and 7 guarantee that the values of the
anchoring point never decrease. The improved vector
a' is added to W at line 8 so that better improve. ­
ments can be achieved for vectors yet to be processed.
At lines 9 and 10, the belief state and action associated
with a vector of the previous iteration are assigned to
the corresponding vector at the current iteration.
The stopping criterion we use is

where E1 is a positive number smaller than 1. In our
experiments, E1 is set at 0 .. Compared with the stop­
ping criterion of value iteration, the stopping criterion
is stricter. The reason for this is that the improvement
step is computationally cheap.

701

Finally, the union UkUU is returned instead of Uk for
the following reason. While Uk(b) is no smaller than
U(b) at the anchoring points, it might be smaller at
some other belief states. In other words, Uk might
not dominate v. If improve simply returns uk, the
conditions of Theorem 2 are not met. Consequently,
the union ukuu is returned.
4.6

Pruning extraneous vectors

The union UkUU usually contains many extraneous
vectors. They should be pruned to avoid unnecessary
computations in the future. One way to doing so is to
simply apply Lark's algorithm (White 1991 ) .
Lark's algorithm solves a linear program for each input
vector. It is expensive when there is a large number of
vectors. We use a more efficient method. The moti­
vation lies in two observations: First, most vectors in
Uk are not extraneous. Second, many vectors in U are
componentwise dominated by vectors in uk and hence
are extraneous. The method is to replace line 13 with
the following lines:
13. Prune from U vectors that are componentwise
dominated by vectors in uk.
14. Prunes from U vectors a such that
R(a,UkUU) is empty.
15. return ukuu.
At line 14, a linear program is solved for each vector in
U. Since no linear programs are solved for vectors in Uk
and the set U usually becomes very small in cardinality
after line 13, the method is much more efficient than
simply applying Lark's algorithm to the union UkUU.
4. 7

Recursive calls to

improve

Consider the set U after line 14 of the algorithm seg­
ment given in the previous subsection. If it is not
empty, then every vector a in the set is useful. This is
determined by solving a linear program. In addition
to determining the usefulness of a, solving the linear
program also produces a belief state b that is in the
closed witness region R(a,UkUU).
The facts that a is from the input set U and that b
is in R(a,UkUU) imply that the value at b has not
been improved. To achieve as much improvement as
possible, we improve the value by a recursive call to
improve. To be more specific, we reset point(a) to b
at line 14 and replace line 15 with the following:
15. if U 'f 0, return improve(UkUU,8).
16. else return uk.

702

5

Zhang, Lee, and Zhang

EMPIRICAL RESULTS

ng.,

Experiments have been conducted to empirically de­
termine the effectiveness of point-based improvement
in speeding up value iteration and to compare it im­
provement with Hansen's policy iteration algorithm.
Four problems were used in the experiments for both
purposes. The problems are commonly referred to as
Tiger, Network, Shuttle, and Aircraft ID in the litera­
ture and were downloaded from Cassandra's POMDP
page 2• Information about their sizes is summarized
in the following table.

IISIIIZIIIAII
Tiger
Network
Shuttle
Aircraft ID
5.1

2
7
8
12

2
2
2
5

VI -+-�·
Vl1 -+-

0.001

�.,.........._�....._...
....
..._ ....
._ c...
..
....1

0.0001
0.01

The effectiveness of point-based improvement is deter­
mined by comparing VI and VI1. We borrowed an
implementation of VI by Cassandra and VIl was im­
plemented on top of his program. Cassandra's pro­
gram provides a number of algorithms for dynamic­
programming update. For our experiments, we used
a variation of incremental pruning called restricted re­
gion (Cassandra et a/1997). The discount factor was
set at 0.95 and experiments were conducted on an UJ­
traSparc II.
For the purpose of comparison, we collected informa­
tion about the quality of the policies that VI and VIl
produce as a function of the times they take. The
quality of a policy is measured by the distance be­
tween its value function to the optimal value function,
i.e. the minimum € such that the policy is €-optimal.
The smaller the distance, the better the policy. Since
we do not know the optimal value function, the dis­
tance cannot be exactly computed. We use an upper
bound derived from the Bellman residual.
One experiment was conducted for each algorithm­
problem combination. The experiment was terminated
when either an 0.01-optimal policy was produced or
the run time exceeded 24 hours, i.e. 86400 seconds,
CPU time.

1

10

100 1000

100000

CPU time In seconds
Nelworl<

i
..

f

0

3
4
3
6

Effectiveness of point-based improvement

0.1

100

-

10

:-\\

V11 -+0.1

__

0.01
0.001
O.o1

1

0.1

CPU time in seconds

10

100 1000

100000

0.1

CPU time In seconds

10

100

1000

100000

1000

§"

8.
..
.�

�

0

10

0.1
0.01

1

Aircraft IO

10000
1000

i
!
..

100
10
VI -+-·
VII -+-

1
0.1
0.01
0.001
0.0001
1e-05
O.o1 0.1

1

10

100

10000

CPU time In seconds

10+06

Figure 4: Empirical results. See text for explanations.
We see that VIl was able to produce a 0.01-optimal
policy for all four problems in a few iterations. On the
other hand, VI took 215 iterations to produce a 0.01optimal policy for Network. Within the time limit, VI
completed only 35, 16, and 10 iterations respectively
for Tiger, Shuttle, and Aircraft ID. Those suggest that
the technique proposed in this paper is very effective
in reducing the number of iterations that is required
to produce good policies.

It is also clear that VIl is much faster than VI. For
Network, VI took about 17,000 seconds to produce
a 0.01-optimal policy, while VIl took only about 350
seconds. The speedup is about 50 times. VI was not
able to produce "good" policies for Tiger, Shuttle, and
Aircraft ID within the time limit, while VIl produced
2
0.01-optimal or better policies for them in 0.53, 130,
http://vvw.cs.brovn.edu/research/ai/pomdp/index.html and 38,424 seconds respectively.

The data are summarized in the four charts in Figure
4. Note that both axes are in logarithmic scale. There
is one chart for each problem. In each chart, there are
two curves: one for VI and one for VI 1. On each curve,
there is data point for each iteration taken.

Speeding Up POMDP Value Iteration

5.2

Comparisons with Hansen's policy
iteration algorithm

In his implementation, Hansen used standard in­
cremental pruning, instead of restricted region, for
dynamic-programming update. Moreover, while the
round-off threshold is set at w-9 in Cassandra's pro­
gram, Hansen set it at w-6 probably because the rou­
tines for solving linear equations cannot handle pre­
cision beyond w-6• For fairness of comparison, we
implemented VI1 on top Hansen's code.
The following table shows the numbers of iterations
and amounts of time VI1 and Hansen's algorithm took
to find 0.01-optimal policies. We see that VI1 took
fewer iterations than Hansen's algorithms on all prob­
lems. It took less on the first two problems and took
roughly the same time on the last two problems.

Tiger
Network
Shuttle
Aircraft ID

6

VI1

I Hansen

4
10
6
8

14
18
9
9

Time
VI1 I Hansen
3.3
0.51
1122
395
65
73
66,964
72,377

CONCLUS IONS

We have developed a technique, namely point-based
improvement, for speeding up the convergence of value
iteration for POMDPs. The underlying idea is simi­
lar to that behind modified policy iteration for MDPs.
The technique can easily be incorporated into any ex­
isting POMDP value iteration algorithms.
Experiments have been conducted on several test prob­
lems. We found that the technique is very effective in
reducing the number of iterations that is required to
obtain policies with desired quality. Because of this, it
greatly speeds up value iteration. In our experiments,
orders of magnitude speedups were observed.
Acknowledgement

Research is supported by Hong Kong Research Grants
Council Grant HKUST6125 /98E. The authors thank
Cassandra and Hansen's for sharing with us their pro­
grams and the anonymous reviewers for useful com­
ments.



policy

1!'.

The value function of an optimal policy is

usually referred to as the

denoted by v·.

P lanning problems where effects of actions

optimal value function

and

are non-deterministic can be modeled a8

MDPs have been studied extensively in the dynamic

Markov decision processes.

programming literature (e.g.

Planning prob­

lems are usually goal-directed.

man

This paper

1990,

BertsekaB

1987,

1960, Puter­
1993}. Dean and
Wellman (1991) ini­

Howard

White

proposes several techniques for exploiting the
goal-directedness to accelerate value itera­

Kanazawa (1989) and Dean and
tiated the use of MDPs in planning problems where

tion, a standard algorithm for solving Markov

effects of actions are not deterministic. Planning prob­

decision processes.

Empirical studies have

lems typically have a large number of states. Solving

shown that the techniques can bring about

MDPs with large state space has hence become a hot

significant speedups.

topic in AI (e.g. Dean

et al1993,

Boutillier

e t al1995).

A planning problem can be modeled as an MDP in
such way that (1) there is a state designated to be
the

Keywords: decision-theoretic planning, Markov de­
cision processes, value iteration, efficiency.

1

goal

( ) {

r sa '

INTRODUCTION

ing an actiona ha8 two consequences: The agent re­

ceives an immediate reward r (s,a), which depends on
the current state 8 of the world a8 well a8 the action
executed, and the world probabilistically moves into

transition probability

P(s'ls,a).
The action is chosen based on the current state of the
world. A

poli cy

1r

prescribes an action for each possi­

ble state. In other words, it is a mapping from the set

S of all possible states to A. The set of possible states
is assumed to be finite in this paper. The quality of

a policy 1r is mea8ured by its

(

for any states,
reward V,..

8)

V'"(s)

-

and (3) the

In a Markov de cision process (MDP), an agent must,
at each time point, choose an action from a finite set
A of possible actions and execute the action. Execut­

another state s' according to a

and an action called

value function V,..(s);

1

if

0

otherwise;

1r,

re­

optimalif v1r• (s)� V,..(s) for any states and any other

ceives starting from an initial state s. A policy rr• is

the

a=delcare-goal and s=goal,

action declare_goal cannot

(1)

be executed

more than once. We call MDPs with such properties

goal-directed MDPs.
Value iteration is a standard algorithm for solving
MDPs. This paper proposes several techniques for ac­
celerating value iteration in goal-directed MDPs. Let
us begin with a brief review of value iteration and of
previous works

2

on

speeding up value iteration.

VALUE ITERATION

A value function is a mapping from the set S of pos­
sible states to the real line. Given a value function V,
define another value function TV by

TV(s)= max0[r(s,a) + 'Y

L P(s'ls,a)V(s')]

(2)

s'

is the expected total discounted

the agent, under the guidance of

declare-goal; (2)

reward function r ( 8, a) is given by

for each state

s,

where 0::;"(<1 is a discount factor.

T is an mapping from the space of value functions to
itself. For any function V, its norm !lVII is defined

490

Zhang and Zhang

IIVII = max.!V(s)l. Tis the contraction mapping
Puterman 1990) in the sense that for any two
value functions U and V,

by

(e.g.

IITU -TVII � 'YIIU- VII·

2, subtracts an appropriate value function from Vn
and MacQueen (1969) proposes to subtract V..(s 0) ­
the value of

by Schweitzer
tanon

For any positive number £, we say that a value function

V

is

Vn

(1989)

et al (1985}

and Bertsekas and Cas­

interleave standard VI steps with aggre­

gation/disaggregation steps, which improve the cur­
rent value function by solving the optimality equation

f.-contracted if

for an simpler MDP obtained from the original MDP

through state aggregation. Dean and Lin
The optimal value function satisfies the

tion

optimal equa­

(1995)

and

decompose an MDP with a large

state space into a number of MDPs with smaller state

used to construct a solution to the original MDP.
Three pieces of previous workd are of direct relevance

induces a policy through

1r(s) = arg maxa[r(s, a)+'Y

L

P(s'ls , a)V(s'))]. (3)

s'

IT the value function is f.-contracted for a small number
€1

et al (1997)

are solved using standard VI and their solutions are

and hence is 0-contracted.

V

Dean

spaces through state aggregation. The smaller MDPs

V* = TV*,

A value function

so.

itself at a predetermined state

The aggregation/disaggregation techniques introduced

the induced policy is "good enough" in the sense

that

to this paper. The first one is the Gauss-Seidel variant
of standard VI proposed by Hastings

(1969).

Let p be

an ordering among the possible states. Instead of the
operator

T

defined in equation

variant uses another operator

(2),
T' to

the Gauss-Seidel
improve the cur­

rent value function. For any value function V,

T'V(s)

is defined for each state s by starting from the state

IIV11" - V*ll � 2£")'
1--y

(4)

•

Proof of this inequality can be found in, for instance,
Puterman (1990). It is evident the policy induced by

that comes first in the ordering p and moving back­
wards. The values

T'V(s)

for earlier states are used

in defining the values for later states. Specifically,

T'

is given by

the optimal value function is optimal.
Value iteration (VI) (Bellman

1957)

starts with an ar­

T'V(s) = maxa[r(s,a)+1' LP(s'ls,a)V (s'),]

bitrary value function and improves it iteratively until

(5)

•'

the value function becomes t:-contracted. Here is the
pseudo-code.
VI

1.

Choose an initial value function
set n=O.

2. Vn+l =TVn.
3. If IIVn+l- Vnii>E,
go to step 2.
4. Else return

V0

where V(s')=T'V(s1) when s' comes before
ordering p and V(s')=V(s') otherwise.

and

The anytime algorithm presented in Dean

in the

et al (1993)

is also closely related to the methods to be proposed in
increment n by

1

this paper. The algorithm restricts standard VI inside

and

an envelope, a subset of possible states, that contains
at least one path from the initial state to the goal state.

Vn+l·

The envelope is gradually enlarged to get better and

T is a contraction mapping, IITVnH - Vn+l II =
IITVn+l - TVn ll � 1'IIVnH- Vnl l � €. Hence, Vn+l is

Since

£-contracted.

3

s

better solutions.
Boyan and Moore

(1996)

study value iteration in

acyclic goal-directed MDPs. A goal-directed MDP is
acyclic if once leaving a. state, the world can never
come back to that state again.

PREVIOUS WORK

Boyan and Moore

point out that value iteration for goal-directed acyclic

VI converges geometrically at rate 'Y·

MDPs can be carried out in one sweep by starting from
Convergence

the goal state and working backwards. Thereby the

Various modifica­

tions to standard VI have been proposed and all have

amount of computations needed to compute the opti­

been theoretically or empirically shown to lead to

iteration of standard VI. The method is an extension

is slow when 1' is close to 1.

faster convergence.

Morton and Wecker

(1977)
T in

gest that one, before applying the operator

sug­
step

mal value function is reduced to that needed in one
of the DAG-SHORTEST-PATH algorithm (Carmen

al1990) for finding

shortest path in acyclic graphs.

et

Value Iteration for Goal-Directed MOPs

4

491

PARSIMONIOUS VALUE

There is no guarantee that the value function returned

ITERATION

by PVI is €-contracted. However, the value function
should be close to be €-contracted. We suggest to use

We introduce several new variants to standard VI for
goal-directed MDPs. Called

tion (PVI),

parsimonious value itera­

the first variant relies on the following in­

PVI as a preprocessing step to VI, i.e.

to use the

value function it returns as the initial value function
of VI. This way an €-contracted value function can

tuition. Suppose value iteration begins with the zero

be obtained. Since the value function return by PVI is

value function.

numer of steps. In our experiments, it t erminat ed in

Then at early iterations, the value

function remains zero for states far away from the goal.
At later iterations, the value function does not change
much for states close to the goal. The number of states
whose values change significantly from one iteration to
the next can be much smaller than the total number
of states.

At each iteration, PVI updates the value

for a state only when the value is expected to change
significantly.

iteration n+1 (n�1),

PVI performs a test to

detect states whose values change substantially from
iteration n to iteration

n+1.

The value of a state is

updated only if it passes the test.
Let

Vn-1

and

Vn be the value functions PVI computed

at the previous two iterations.

At the current itera­

s if IVn(81) - Vn-1 (81) I '$ 5 for a small positive con­
stant 8 and each state 81 such that maxaP(81 j s, a)>O.
Since the number of states reachable from s by execut­

ing one action is usually small, this test is cheap. It
is usually much cheaper than calculating

TVn(s),

es­

pecially when one maintains a list of nodes reachable
from each state by executing one action.

Theoretical underpinings of the test are as follows. If

the value functions

Vn

and

Vn-1

were the value func­

tions computed by VI, one could easily show that if

IVn+l (s) - Vn(8) I'$ -yo. In

s passes the test then

words, the value for
n

8

other

does not change much from it­

to iteration n+ 1.

Here is the pseudo-code for PVI.

PVI
1. Vo(s)=O for

any 8, n=O.

2. For each states,
(a)

The idea behind PVI is rather similar to the idea
underlying Boyan and Moore's one--sweep algorithm;
start from the goal and work backwards. PVI does not

assume acyclicity and hence is more general. W hen the
MDP is acyclic, it is almost identical to the one--sweep

Ifn�1 and 1Vn(8')-Vn-1(8')1 � Hor
all 81 such that maxaP(s'ls, a)>O,
Vn+l(s) = V..(s).

et al ( 1993)

in the sense that values are updated only

for some states at each iteration.

The difference lies

in the fact that in PVI the states whose values are

the anytime algorithm whether the value for a state

is updated depends on whether it is in the envelop
and does not change with iteration.

Also the entire

value iteration process needs to be carried out for each
envelop.

5

GREEDY AND DOUBLE VALUE
ITERATION

Even though the test in PVI is cheap, the fact that
it has to be carried out for each state

is

somewhat

unsatisfying. Greedy value iteration (GVI) avoids the
test by working

in a way similar to DAG-SHORTEST­

PATH.
Before describing GVI, we need to introduce t he con­

s' is ideally
reachable in one step fr om another state s if after exe-­
cuting a certain action in state s the probability of the
world ending up in state 81 is the highest. A state s�e
is ideally reachable in k steps from another state s0 if
there are states 811
, s��:-1 such that si+l is ideally

cept of ideal reachability. We say a state

reachable from Bi in one step for all O�i:::;k-1. Any
• • •

state is ideally reachable from itself in 0 step.
For any state s, let d{s) be the minimum number of
steps in which the goal is ideally reachable from

(b) Else

shall refer to d( s) as the

Vn+l(s) = TVn(s).
3. If IIVn+l - Vn II>£, inc re ment n by 1 and
go to step 2.

4. Else return

PVI is also related to the anytime algorithm by Dean

updated change from iteration to iteration, while in

tion n+ 1, PVI does not update the value for a state

eration

just one iteration.

algorithm.

Specifically, PVI begins with the zero value function.
At each

close to be co-contracted, VI should terminate in a small

Vn.H·

distancefrom s

8.

We

to the goal.

At each iteration n, GVI only updates the values for
the states from which the goal is ideally reachable in
n steps. Let N be the maximum number of steps that
the goal can be ideally reached from any state. Then
GVI terminates in exactly N iterations. For later con-

492

Zhang and Zhang

venience, we assume that GVI takes a value function

where

a.s input and uses it a.s the initial value function. Here

,
V(s,s) =
A

is the psuedo-code for GVI.

1. For n=O

ping.
toN,

if d(s' ) <
otherwise.

2. Return

Hence the value function returned by DVI is

It is evident see that DVI is almost identical to the
Gauss-Seidel variant of standard VI, except that it pro­

If d(s)==n, set
Vn+I(s) :::::: TVn(s).

poses one particular way to order the possible states;
the states are ordered according to their distances to

(b) Else

the goal. By introducing DVI through GVI, we hope
to provide another way of looking at the Gauss-Seidel
variant of standard VI in the context of goal-directed
MOPs.

VN.

When the MDP is acyclic, GVI is identical to Boyan
and Moore's one-sweep algorithm and hence returns
the optimal value function. When the MDP is cyclic,
however, the value function it returns could be of very
poor quality. Using it a.s a preprocessing step to VI
might not help much.

6

IMPROVING PVI

The alternative understanding of DVI can be used to
improve PVI. We call the improved algorithm PVIl.
The pseudo-code is a.s follows.

PV!l

On the positive side, the amount of computations GVI

1. Vo(s)=O for any s, n=O.

does is identical to that carried out by one iteration of

2. For

standard VI. Also because GVI is an approximation of
the entire value iteration process, the extent to which
it improves the input value function should be greater
than that brought about by one iteration of standard

m=O

toN,

(a) For each state

(b)

VI. Thus we can expect VI to converge faster if the

s such that d(s)=m
lfn�1 and IVn(s')- Vn-l(s')l � o for
ails' such that maxa.P(s'ls,a)>O,
Vn+l(s) = Vn(s).

second line is replaced by "Vn+l = GVI(Vn)". This
leads to new algorithm called double value iteration

(c) Else

(DVI) .

Vn+l(s)
Choose an initial value function

n=O.
2. Vn+l = GVI(Vn)·
3. If IIVn+t(8)- Vn(8)11>e,
1 and go to step 2.
4. Else return Vn.

V0

3. IfiiV.-.+1- V.-.li>e,

and

set

increment

n by

new operator T', instead of the operator T given in
Equation (2), to update the value function Vn· For

any value function V, T'V(s) is defined for each state
8 by starting with the goal state and gradually moving
away. The value T'V(s) for a state s is defined after
the values T'V(s') for all the states s1 closer to the

8

having been defined. It is given by

T'V(s) :::::: max8[r(s, a)+')' 2:: P(s'ls,a)V(s', s), ]
s'

=

T'Vn(s).

increment

n by 1 and

go to step 2.

As it turns out, DVI can be described directly with­
out the reference to GVI. At each iteration, it uses a

goal than

d(s),

€-contracted.

For each state s,

(a)

DVI
1.

V(s')

It can be proved that r is also a contract map­

GVI(VQ)

•

{ TV(s')

4. Else return Vn+l·
As PVI, PVIl should be used a.s a preprocessing step
to VI.

7

EXPERIMENTS

Preliminary experiments have been carried to compare
the algorithms proposed in this paper with standard
value iteration. Four office environment navigation

problems borrowed from Ca.ssandra et al (1996) were
used . The problems differ in corridor layout and the
total number of states. There are two sets of transition
probabilities, referred to a.s standard and noisy tran­
sition probabilities respectively. Effects of actions are
less certain under noisy transition probabilities than
under standard transition probabilities.

493

Value Iteration for Goal·Directed MDPs

-- -

-- -

1.$

..,.. ......
---

1.4

'VI"-+"DVV" .....
wr ......
W11" ......

'Dill"�

Wl1.. ..,._

1.2

l

l

l

l

0.8

...

o.e
OA
0.2
0
2<)0

210

220

1.4

1
I

...,

... ...
--

vo

no

DO

0
100

:100

...

�- -

Ul

...

""'

&DO
--

..,.

700

60(>

-� - -

'VI'+"D\11" .....

---

""F�·u·...-

1.2

I

"'Il"+"OVI" -+'Wr+"PV''f" .......

1

I
---·-

�00�-��-�---�--�&00�--�-�-�700-�60<>
-Figure 1: Convergence times of the algorithms in four
navigation problems.

Figure 2: Differences in perfor manc e among the
rithms as problem size increases.

algo­

The threshold for the Bellman residual was set at 0.001
and the discount factor at 0.99. Figure 1 shows that
convergence times of the algorithms in the four prob­
lems. The X·axis repr es ents the sizes of the probl em ,
while theY-axis represents convergence time in CPU
seconds. Data were collected using a SPARC20. The
curves VI and DVI display the convergence times of
VI and DVI respectively, while PVI and PVIl display
the convergence times for the combinations of PVI and
PVIl with VI.

The convergence times are shown in Figure 2. We see
that the differences in performance among the algo­
rithms become larger as the problem size increases.
In the smallest problem PVIl converges about three
times faster than VI, while in the largest problem it

Under both standard and noisy transition probabili­
ties, DVI and PVI converges much faster than VI and
PVIl converges even faster. DVI converges slightly
faster than PVI in the smallest problem but slower in
all other problems. Performances of all algorithms are
slightly worse under noisy transition probabilities than
under standard transition probabilities. Their differ­
ences are also slightly larger.

We propose several techniques for exploiting the goal­
directedness of planning problems to speed up value it­
eration for their MDP models. Empirical studies have
shown that the techniques can bring a bo ut significant
speedups.

gain an idea about how the comparisons change
with problem sizes, we made copies of one environment
and glu e them together to form larger environmen ts .

be modeled as partially obs ervable

To

converges six times faster.

8

CONCLUSIONS AND FUTURE
DIRECTIONS

MDPs
world.

know

assume

perfect observation

of

the

state

of

the

In many real-world problems, one does not

Such problems can
MDPs (POMDPs).
POMDPs are much harder to solve than MDPs. We
are currently investigating the possibility of applying
the true state

of the

world.

494

Zhang and Zhang

the ideas introduced in this paper to POMDPs.
Aclcnowledgements

We thank Peter Dayan, Thomas L. Dean, and Michael
Littman for pointers to references and thank Wenju
Liu and D. Y. Yeung for useful discussions. Re­
search was supported by Hong Kong Research Coun­
cil under grants HKUST 658/95E and Hong Kong
University of Science and Technology under grant
DAG96/97.EG01(RI).



Most exact algorithms for general par­
tially observable Markov decision processes
(POMDPs) use a form of dynamic program­
ming in which a piecewise-linear and con­
vex representation of one value function is
transformed into another. We examine vari­
ations of the "incremental pruning" method
for solving this problem and compare them to
earlier algorithms from theoretical and em­
pirical perspectives. We find that incremen­
tal pruning is presently the most efficient ex­
act method for solving POMDPs.

1

Littman

Computer Science Dept.
Brown University
Providence, RI 02912

INTRODUCTION

Partially observable Markov decision processes
(POMDPs) model decision theoretic planning problems
in which an agent must make a sequence of decisions
to maximize its utility given uncertainty in the effects
of its actions and its current state (Cassandra, Kael­
bling, & Littman 1994; White 1991). At any moment
in time, the agent is in one of a finite set of possible
states S and must choose one of a finite set of possible
actions A. After taking action a E A from state s E S,
the agent receives immediate reward ra(s) E �and the
agent's state becomes some states' with the probabil­
ity given by the transition function Pr(s'Js,a) E [0, 1].
The agent is not aware of its current state, and in­
stead only knows its information state x, which is a
probability distribution over possible states (x(s) is
the probability that the agent is in state s). After
each transition, the agent makes an observation z of
its current state from a finite set of possible obser­
vations Z. The function Pr(zis',a) E [0,1] gives the
probability that observation z will be made after the
agent takes action a and makes a transition to state
s'. This results in a new information state x� defined

by
a
xz (s ')

=

Pr( z j s' , a) "EsES Pr( s ' ls , a)x(s)
'
Pr(zjx, a)

(1)

where
Pr(zix,a)

=

L Pr(zls', a) L Pr(s'Js, a)x(s).

s'ES

sES

Solving a POMDP means finding a policy 1r that maps
each information state into an action so that the
expected sum of discounted rewards is maximized
(0 � 1 � 1 is the discount rate, which controls how
much future rewards count compared to near-term re­
wards). There are many ways to approach this prob­
lem based on checking which information states can
be reached (Washington 1996; Hansen 1994), search­
ing for good controllers (Platzman 1981), and using
dynamic programming (Smallwood & Sondik 1973;
Cheng 1988; Monahan 1982; Littman, Cassandra, &
Kaelbling 1996).
Most exact algorithms for general POMDPs use a
form of dynamic programming in which a piecewise­
linear and convex representation of one value func­
tion is transformed into another. This includes algo­
rithms that solve POMDPs via value iteration (Sawaki
& Ichikawa 1978; Cassandra, Kaelbling, & Littman
1994), policy iteration (Sondik 1978), accelerated
value iteration (White & Scherer 1989), structured
representations (Boutilier & Poole 1996), and ap­
proximation (Zhang & Liu 1996). Because dynamic­
programming updates are critical to such a wide ar­
ray of POMDP algorithms, identifying fast algorithms
is crucial.
Several algorithms for dynamic-programming updates

have been proposed, such as one pass (Sondik 1971),
exhaustive (Monahan 1982), linear support (Cheng
1988), and witness (Littman, Cassandra, & Kaelbling
1996). Cheng (1988) gave experimental evidence that
the linear support algorithm is more efficient than the

Incremental Pruning for POMDPS

states to value and is defined in terms of relatively
simple transformations of other value functions.

one-pass algorithm. Littman, Cassandra and Kael­
bling (1996) compared the exhaustive algorithm, the
linear support algorithm, and the witness algorithm
and found that, except for tiny problems with approx­
imately 2 observations or 2 states, which all three al­
gorithms could solve quickly, witness was the fastest
and had a number of superior theoretical properties.

The transformations preserve piecewise linearity and
convexity (Smallwood & Sondik 1973; Littman, Cas­
sandra, & Kaelbling 1996). This means that if the
function V can be expressed as V(x) = maxaESX ·a
for some finite set of [$[-vectors S (which it can in
most applications)' then we can express vza (a;) =
maxaES� X . a, va(x) = maxat=s� X . a, and V'(x) =
maxo:ES' X. a for some finite sets of /S/-vectors s�, sa'
and S' (for all a E A, and z E Z). These sets have a
unique representation of minimum size (Littman, Cas­
sandra, & Kaelbling 1996), and we assume that the
symbols S�, sa, and S' refer to the minimum-size sets.

Recently, Zhang and Liu (1996) proposed a new
method for dynamic-programming updates in POMDPS
called incremental pruning. In this paper, we analyze
the basic algorithm and a novel variation and com­
pare them to the witness algorithm. We find that
the incremental-pruning-based algorithms allow us to
solve problems that could not be solved within reason­
able time limits using the witness algorithm.

2

Here is a brief description of the set and vector nota­
tion we will be using. Vector comparisons are com­
ponentwise: a1 � a2 if and only if for all s E S,
a1(s) ::_>: a2(s). Vector sums are also componentwise.
Vector dot products are defined by a·/1 =I:. o:( s )/1( s) .
In vector comparisons and dot products, 0 is a vector
of all zeros and 1 a vector of all ones. For all s E S, the
vector e8 is all zeros except e8 (s) = 1. The cross sum
of two sets of vectors is AEB B = {a+/1/a E A, /1 E B};
this extends to collections of vector sets as well. Set
subtraction is defined by A\B = {o: E Ala� B}.

DP UPDATES

The fundamental idea of the dynamic-programming
(DP) update is to define a new value function V' in
terms of a given value function V. Value functions
are mappings from information states to expected dis­
counted total reward. In value-iteration algorithms, V'
incorporates one additional step of reward compared
to V and in infinite-horizon algorithms, V' represents
an improved approximation that is closer to the opti­
mal value function.

Using this notation, we
described earlier as

The function V' maps information states to values and
is defined by

V'(x)

=

�E�

(z=
sES

ra(s)x(s) + 'Y L Pr(zjx,a)V(x�)
zEZ

)

(2)
In words, Equation 2 says that the value for an infor­
mation state x is the value of the best action that can
be taken from x of the expected immediate reward for
that action plus the expected discounted value of the
resulting information state (x�, as defined in Equa­
tion 1).
We can break up the value function V' defined in
Equation 2 into simpler combinations of other value
functions:

V'(x)
va(x)

maxVa(x)

(3)

z= vza(x)

(4)

aEA

=

z

These definitions are somewhat novel and form an im­
portant step in the derivation of the incremental prun­
ing method, described in Section 4. Each va and
vza function is a value function mapping information

55

S'

.

=

purge

can

characterize the "S" sets

(Usa)
(ffi S�)

(6)

aEA

sa
saz

=

purge

zEZ
purge({T(a,a,z)lo:

(7)
E

S}),

(8)

where T ( a, a, z) is the lSI-vector given by

r(a,a,z)(s)

(1//ZI)ra(s) + 'Y L o:(s') P r(z/s' a) Pr(s' j s , a),

,

•'

and purge(·) takes a set of vectors and reduces it to its
unique minimum form. Equations 6 and 7 are easily
justified by Equations 3 and 4 and basic properties of
piecewise-linear convex functions. Equation 8 comes
from substituting Equation 1 into Equation 5, sim­
plifying, and using basic properties of piecewise-linear
convex functions.
The focus of this paper is on efficient implementations
for computing sa (Equation 7). Equations 6 and 8
can be implemented efficiently using an efficient im­
plementation of the purge function, described in the
next section.

56

Cassandra, Littman, and Zhang

PURGING SETS OF VECTORS

3

Given a set of lSI-vectors A and a vector a, define

R(a, A) = {xlx 2': O , x 1
·

1 ,x·a > x·a',a'

=

E A\{a}};

it is the set of information states for which vector

(9)
a is

the clear "winner" (has the largest dot product) com­
pared to all the other vectors in A. The set

R(a, A) is

called the witness region of vector a, because for any
information state

maxa'EAu{a} x

·

x

in this set maxa'EA\{a}

a ' ; in a sense,

x

x ·a' i­

can testify that a is

needed to represent the piecewise-linear convex func­
tion given by

AU{a}.

Using the definition of

R,

F�F\{w}

5

6 while F i- 0
7 do¢ E F
X� DOMINATE(c/J, W)
8
ifx=.l
9
10
then F � F\ {¢}
11
else w � argmax4>EF x ·¢
W � WU{w}
12
F � F \ {w}
13
14 return

we can define

purge(A) = { al aE

FILTER(F)
1 W�0
2 for each s in S
3 do w � argmax4>EF e, · <P
4
w�wu{w}

A, R(a,A) i-

W

0};

it is the set of vectors in A that have non-empty wit­

Figure 1: Lark's algorithm for purging a set of vectors.

ness regions and is precisely the minimum-size set for
representing the piecewise-linear convex function given
by A (Littman, Cassandra, & Kaelbling
Figure

1

1996)1.

gives an implementation of purge(F)-given

a set of vectors

F, FILTER(F)

returns the vectors in

F

that have non-empty witness regions, thereby "purg­
ing" or "filtering" or "pruning" out the unnecessary
vectors. The algorithm is due to Lark (White
Littman, Cassandra, & Kaelbling

(1996)

1991);

analyze the

algorithm and describe the way that the argmax op­
erators need to be implemented for the analysis to
hold (ties must be broken lexicographically).

DoMINATE( a, A)

procedure called in line

information state

x for

8

The

returns an

which a gives a larger dot prod­

uct than any vector in A (or .l if no such

x

DOMINATE(a, A)
1 L � LP(variables:x(s), 8; objective: max&)
2 for each a' in A\ {a}
3 do ADDCONSTRAINT(L, x ·a 2': 8 + X a')
4 AooCoNSTRAINT(L, x 1 = 1)
5 AooCoNSTRAINT(L, x 2': 0)
6 iflNFEASIBLE(L)
7
then return (.l)
else (x,8) � SOLVELP(L)
8
9
ifo>O
then return (x)
10
else return .l
11
·

·

exists)­

2:

that is, it returns an information state in the region

Figure

R(a,A).

information state in a vector's witness region.

It is implemented by solving a simple linear

program, illustrated in Figure
The

FILTER

2.

algorithm plays a crucial role in the in­

cremental pruning method, so it deserves some addi­
tional explanation. The set

w

with vectors

R(w, F);

W, initially

empty, is filled

e8

information states.

The "while" loop starting on line

</J E

3.1

USING PURGE IN DP

that have non-empty witness regions

they are the "winners." Lines 3-5 find those

winning vectors at the

6

goes through the

DOMINATE is
x E R(¢, W). If there is not,
we know R(¢, F) is empty, since x E R(¢, F) implies
x E R(</J, W) since W � F. If DOMINATE finds an x E
R(¢, W), we add the winning vector (not necessarily
¢1) at x to W and continue. Each iteration removes
vectors

Linear-programming approach to finding an

F one by one.

For each,

used to see if there is an

a vector from F, and when it is empty, every vector

FILTER procedure, it is trivial to compute
s: sets from S and to compute S' from the sa. sets
(Equations 8 and 6).
Given the

the

A straightforward computation of the sa sets from the
S� sets (Equation 7) is also easy, and amounts to an
exhaustive enumeration of all possible combinations
of vectors followed by filtering the resulting sets. This
algorithm is not efficient because the number of com­
binations of vectors grows exponentially in

IZI.

This

can be a large number of vectors even when the sa

from F will have been classified as either a winner or

sets are relatively small. This approach to computing

not a winner.

the

1

This assumes that A is a true set in that it contains
no duplicate vectors.

so.

s� sets was essentially proposed
(1982) (under the name of "Sondik's one­

sets from the

by Monahan

pass algorithm").

Incremental Pruning

3.2

INCPRUNE(Sg1, , Sgk)
1
W +- FILTER(S:1 EB SgJ

COMPLEXITY ANALYSIS

•

We seek to express the running time of algorithms
in terms of the number of linear programs they solve
and the size of these linear programs. We choose this
metric because all of the algorithms in this paper use
linear programming as a fundamental subroutine (in
the form of calls to DOMINATE( a, A)) and the solu­
tion of these linear programs is by far the most time­
consuming part of the algorithms. In addition, tra­
ditional "operation count" analyses are cumbersome
and unenlightening because of the difficulty of pre­
cisely characterizing the number of primitive opera­
tions required to solve each linear program.
We will express the running time of W +- FILTER(F)
in terms of the size of the sets F and W, the number
of states lSI, and m, the number of vectors in W that
are found by checking the e8 information states.
As is evident in Figure I, each iteration of the "while"
loop on line 6 removes one vector from F, and m vec­
tors are removed before the loop. This means the while
loop is executed precisely IFI-m times. Each iteration
of the "while" loop makes a single call to DoMINATE,
so there are IF I- m linear programs solved in all cases.
Each of these linear programs has one variable for each
state in S and one for J. The total number of con­
straints in any one of these linear programs will be
between m + 1 and /WI+ 1. In the best case, the total
number of constraints will be IFI(m + 1) - miWI +
/W/(/W/- 1)/2- m(m + 1)/2 and the worst case will
have an additional (IF/ -/W/)(/W/- m) constraints.
W hen checking the e. information states, at least one
vector in W will be found. Further, when /WI > 1 we
are guaranteed to find at least two of the vectors in
W. For the remainder of this paper, we assume that
/W/ > 1, since the case of /WI 1 is trivial.
=

The witness algorithm has been analyzed previ­
ously (Littman, Cassandra, & Kaelbling 1996), and
we list the basic results here for easy comparison.
The total number of linear programs solved by wit­
ness is O::::z jSgl - /Zi)/Sa/ + /Sa/
1; asymptoti­
cally, this is e ( ISa l Lz IS:I). Note that this is not
a worst-case analysis; this many linear programs will
always be required. The number of constraints in each
linear program is bounded by /Sal + 1. The total
number of constraints over all the linear programs is
8(1Sa/2 Lz /Sg/) asymptotically2•

-

2In the best case there are

1/2(Lz /S�/-/Z/ +1)(/S"/+

l)(JS"/ + 2)- Lz /S�/ + /Z/
worst case there are JSa/(/Sa/
constraints.

-

3 constraints and in the

+ 1)(2:. JS�/- /Z/-

57

for POMDPS

1/2)

• •

2 for i +- 3 to k
3 do W +- FILTER(W EB Sg;)

4

4

return

W

Figure

3:

The incremental pruning method.

INCREMENTAL PRUNING

This section describes the incremental pruning
method (Zhang & Liu 1996), which computes sa effi­
ciently from the s� sets.
Recall the definition for sa in Equation 7:

sa

=

purge

zEZ

here, k

=

s�

(EB )

= purge(s�l

EB

s�2

ffi

...EB s�k);

/Zj. Note that

purge(A EBB EB C)
so Equation

7

=

purge(purge(A EBB)

EB

C),

can be rewritten as

Sa =purge(... purge(purge(S �1 EB S�2) EBS�3)

• • •

ffi

S�k).
(10)

The expression for sa in Equation 10 leads to a very
natural solution method, called incremental pruning,
illustrated in Figure 3. In addition to being conceptu­
ally simpler than the witness algorithm, we will show
that it can be implemented to exhibit superior perfor­
mance and asymptotic complexity.
The critical fact required to analyze incremental prun­
ing is that if A=purge(A) and B=purge( B) (neither
contain extra vectors) and W = purge(A ffi B), then

/WI ;:: max(IAI, IB/).

(11)

Equation 11 follows from the observation that for ev­
ery w E W, every R(w, W) region is contained within
R(a,A) and R({3, B) for some a E A and j3 E B.
This means that the size of the W set in INCPRUNE
is monotonically non-decreasing; it never grows explo­
sively compared to its final size.
Figure 3 illustrates a family of algorithms that we col­
lectively call the incremental pruning method; specific
incremental pruning algorithms differ in their imple­
mentations of the FILTER procedure. The most basic
incremental pruning algorithm is given by implement­
ing FILTER by Lark's algorithm (Figure 1); we call
the resulting algorithm IP. In Section 5, we describe
several other variations.

58

Cassandra, Littman, and Zhang

R( ¢, D \ { ¢}) is empty, we need to
R(¢, A EBB) is empty. We can show this by
contradiction. Assume there is an x* E R(¢,A ffi B).
Since (D \ {¢})�A ffi B, x• E R(¢,D \ {¢}). But we
know that R( ¢,D \ { ¢>}) is empty, so this cannot be.

9(1Sal I:z jS;I) linear pro­
0(1Sal2 I:z IS: I ) constraints3. In the worst

Proof:

The complexity of IP is
grams and

case, these bounds are identical to those of the witness

3.2).

algorithm (Section

However, there are POMDPs

for which the expression for the total number of con­
straints is arbitrarily loose; the best-case total number

To prove the second part, let

of constraints for IP is asymptotically better than for

FILTER in lNCPRUNE (Figure 3)
FILTER(A EBB). This section modifies
implementation of FILTER to take advantage of
All the calls to

are

of the form

the
the

deal of regularity. The modification yields a family of

FILTER algorithms,

some of which render incremental

appearing in Figure

1

is used.

The change is to replace line

x
D

+--

8

Different choices of
in Figure

1

with

Equation

(a+ !3) =¢for a E A and j3 E B. For every
a1 E A and f3t E B, if (at+ j3t) E W, then either
(at +!3d E D, or (at + /3) E D,or (a+ f3t) E D.
Let

D

(i.e., IP,

16

17

and

algorithm. Using either

16 or 17 exclusively in the incremental prun­

constraints. Although

O(ISal L:z IS;I2 + 1Sai2IZI)

the asymptotic total number of

linear programs does not change,

that satisfy the

RR actually requires

slightly more linear programs than IP in the worst
case. However, empirically it appears that the savings

The following lemma shows that any such choice of
allows us to use the domination check in Equation

in the total constraints usually saves more time than
the extra linear programs require.
An even better variation of incremental pruning selects
whichever

16 and 17.

vector in purge( A EBB) that has not yet been
to W (note that ¢fl. W).

the

IP

algorithm

are

plicated, upper bounds are possible.

15,

The only extra work that is required is

some bookkeeping to track how vectors were created
and the sizes of the various sets that we will choose
from.

added

IS" I L:z IS� I linear program s and ISai(ISal + 1) I;, IS�I3IZI total constraints. Note that tighter, though more com­

set is smallest from among Equations

This will usually yield a faster algorithm in

practice, though it makes this variation much harder

D
12

Lemma 1 If R(c/>,D\{4>}) = 0, thenR(c/>,AEBB) = 0.
If x E R(¢,D \ {¢}), then x E R(w, W) for some
wE (A$B) \ W.

D

to analyze.

to either remove ¢ from consideration, or to find a

on

(RR)

plexity of the algorithm to

(13)
A ffi B,
({a} ffi B) U (A EB {/3}),
(14)
w,
(15)
({a} ffi B ) U {at+ /31(at + !3) E W}, (16)
(A EB {/3}) U {f3t + aj(a + /31) E W}, (17)

bounds

1991) in lNCPRUNE

ing algorithm will improve the total constraint com­

above properties. For example,

upper

filtering algorithm in

We refer to variations of the incremental pruning
as the restricted region

There are a number of choices for

(1982)

is equivalent to using Lark's

method using a combination of Equations

1. D � (AffiB).

3Simple

15

filtering algorithm (W hite

is the set of winning vectors found so far):

=

equiv­

as described earlier).

W

and

D set,

13 is

lNCPRUNE, Equation

of vectors satisfying the properties below

A ffi B

result in different incremental

alent to using Monahan's

can be used and still give a correct algorithm (recall
that we are filtering the set of vectors

D

pruning algorithms. In general, the smaller the
the more efficient the algorithm. Equation

(12)

DOMINATE(¢,D \ {¢}).

D

The lemma follows.

pruning more efficient than when the standard version

D
D
D
D
D

argmax<I>'EAffiB x ·¢'.

x·w > x·w'
w'EW. Let (at + !3t) = w' for any w' E W,
a1 E A and [31 E B and let (a + /3) = ¢> for a E A and
/3 E B. By the conditions on D, we know that either
(at + /31} E D, or (at + /3) E D, or (a+ (31) E D.
Assume (a1 +!3) ED (the other two cases are similar).
Sincex E R(¢,D\{¢}), x·¢>::: x·(a+f3) > x·(a1 +/3).
This implies that x · a > x·a1• Adding /31 to both sides
gives us that x · (a+ !3!) > x ·(a1 + /31) = x·w1• By the
definition of w, x·w � x ·(a+ 81). Hence x·w > x·w'.

fact that the set of vectors being processed has a great

2.

=

for all

GENERALIZING IP

Any set

w

The lemma is proved if we can show that

witness.

5

First, if

show that

In principle, it is also possible to choose aD set that is
the smallest set satisfying conditions

1

and 2. This ap­

pears to be closely related to the NP-hard vertex-cover
·

problem; we are investigating efficient alternatives.

Incremental Pruning for POMDPS

6

EMPIRICAL RESULTS

Although asymptotic analyses provide useful informa­
tion concerning the complexity of the various algo­
rithms, they provide no intuition about how well algo­
rithms perform in practice on typical problems. An­
other shortcoming of these analyses is that they can
hide important constant factors and operations re­

Table 2: Total execution time

Test Problem
1D maze

W itness

TTOTAL

59

(sec. )

IP

RR

9.3

2.3

2.3

2.2

4x3

727.1

346.0

157.0

>28800

4x4

3226.0

1557.0
2 15.7

909.2

216.7

351.8

203.3

>28800

5608. 4

4249.2

5226.4

1116.9

6422.9

1066.6

722.5

>28800

Exh.

quired outside of the linear programs. To address these
shortcomings, we have implemented IP and variations
and have run them on a suite of test problems to gauge
their effectiveness. All times given are in CPU seconds
on a SPARC-10.

Cheese
Part painting
Network
Aircraft ID
Shuttle
4x3 CO

found that more than
95% of the total execution time was spent solving lin­
ear programs4, verifying that the linear programs are
the single most important contributor to the complex­
ity of the algorithms.

Table 3: Total execution time (sec.) for extended tests.

417.0

234. 1

166.0

>28800

1676.7

200.8

145.9

>28800

24.6

22.8

22.7

>28800

We profiled the execution and

To ensure fairness in comparison, we embedded all of
the algorithms in the same value-iteration code and
used as many common subroutines as possible. We
also used a commercial linear programming package
to maximize the stability and efficiency of the imple­
mentation.
We ran IP, RR, exhaustive and linear support al­
gorithms on 9 different test problems listed in Ta­
ble 1 (complete problem definitions are available at
http://www.cs.brown.edu/people/arc/research/

) The "Stages" column reports
the number of iterations of value iteration we ran and
the "IVtl" column indicates the number of vectors in
the final value function5.

pomdp-examples. html .

Table 2 lists the total running time for each algorithm
on each of the 9 test problems. The results indicate
that RR works extremely well on a variety of test prob­
lems. We do not list run times for the linear support
algorithm because, in all cases, it was unable to run
to completion. This is because of memory limitations;
space requirements

for the linear support algorithm

increase dramatically as a function of the number of
states. We terminated algorithms that failed to com­
plete in 8 hours (28800 seconds); as a result, the ex­
haustive algorithm ("Exh.") was only able to complete
three of the test problems (all of which had only two
observations). On the three small test problems the
4

This profiling data was computed running witnes s, IP,
and RR on the 4x3 problem for 8 stages.
5
The number of stages was determined by finding the
maximum number of stages that the witness algorithm was
able to complete within 7200 seconds. In some of the test
problems, the witness algorithm found the optimal infinite­
horizon value function in under 7200 seconds, so we picked
the number of iterations required to conv('!rge to within
machine precision of the optimal value function.

Test Problem
Network
Shuttle

TTQTAL

Stages

W itness

IP

RR

20

>28800

4976.8

2621.3

9

>28800

5121.3

2767.7

exhaustive algorithm was able to complete, it actually
out performed all the other algorithms.
For all but two of the test problems, the witness al­
gorithm was within a factor of 5 of the performance
of RR. To highlight the advantage of the incremental­
pruning-based algorithms, we chose the two test prob­
lems for which RR was more than 5 times faster than
witness (Network, 8.9, and Shuttle, 11.5) , and ran for
a larger number of stages. As shown in Table 3, the
witness algorithm is unable to solve a problem in 8
hours that RR can solve in 43 minutes (2621 seconds).
Although linear programming consumes most of the
running time in the algorithms we examined, there
are actually three phases of the value-iteration algo­
rithm that contribute linear programs: finding the
minimum-size

sg

sets, constructing the

ga sets from

the s: sets, and constructing S' by combining the sa
sets. Of these, only constructing the sa sets is differ­
ent between witness, IP, and RR, so we have chosen to
present the execution times in two ways. The first, as
illustrated in Table 2 as TTOTAL, represents the com­
plete running time for all stages and all phases. The
second, shown in Table 4 as Tsa -BUILD, is the exe­
cution time over all stages that was devoted to con­
structing the sa sets from the s: sets.
As the data in Tables 2 and 4 show, IP performs better
than the witness algorithm on all the test problems.
These tables also show how difficult it is to analyze
the exact amount of savings IP yields; the amount of
savings achieved varies considerably across problems.

60

Cassandra, Littman, and Zhang

Table 1: Test problem parameter sizes.
Test Problem
1D maze
4x3
4x3 CO
4x4
Cheese
Part painting
Network
Shuttle
Aircraft ID

States
4
11
11
16
11
4
7
8
12

Acts.
2
4
4
4
4
4
4
3
6

Obs.
2
6
11
2
7
2
2
5
5

Stages
70
8
367
374
373
371
14
7
4

!Vtl

4
436
4
20
14
9
438
481
258

Reference
Parr & Russell (1995)
Russell & Norvig (1994)
Cassandra, Kaelbling, & Littman (1994)
McCallum (1993)
Kushmerick, Hanks, & Weld (1995)
Chrisman (1992)

Table 4: Total time (sec.) spent constructing sa sets.
Test Problem
1D maze
4x3
4x4
Cheese
Part painting
Network
Aircraft ID
Shuttle
4x3 CO

Tsa-BUILD

Witness
7.1
599.1
2252.6
221.9
5226.5
5954.7
359.1
1566.2
2.6

IP
<0.1
220.5
644. 4
84.4
3834.4
615.4
176.4
92.5
0.9

RR
<0.1
31.0
0.9
72.2
4819.5
255.4
108.3
38.1
0.9

For RR, the set D was defined by Equation 16 if
\BI < \A\ and Equation 17 otherwise; in most cases,
this is equivalent to using the equation that leads to
the smaller size for D. Looking at the data for RR, we
see that in all but one case it is faster than IP. Again,
the precise amount of savings varies and is difficult to
quantify in general.

DP-UPDATE(S)
1 for each a inA
2 do for each z in Z
do 5� t- FILTER({r(a:,a,z)ia:
3
sa t- INCPRUNE(S:,, . , s:,)
4
5 S' f-- FILTER (Ua sa)
6 returnS'
.

DISCUSSION & CONCLUSIONS

In this paper, we examined the incremental pruning
method for performing dynamic-programming updates
in partially observable Markov decision processes. In­
cremental pruning compares favorably in terms of ease
of implementation to the simplest of the previous al­
gorithms (exhal'stive), has asymptotic performance as
good as or better than the most efficient of the previ­
ous algorithms (witness), and is empirically the fastest
algorithm of its kind for solving a variety of standard
POMDP problems.

A complete incremental pruning algorithm (RR) is
shown in Figure 4.
There are several important outstanding issues that
should be explored. The first is numerical precision-

St-1})

INCPRUNE(S�,, ... , S�k)

1 W

t-

RR(S:,, S�2)

for i t- 3 to k
3 do W t- RR(W, S�;)
4 return W
2

RR(A,B)
1

2
3
4
5
6
7

7

E

.

8
9
10
u

12
13
14
15
16
17
18

Ft-AEIJB
Wt-- 0
for each s inS
do w f-- argmaxq,EF es ¢
·

Wt-W U {w}
Ft-F\{w}
while F 1:- 0
do (o: + ,B) E F
D1 t- ({o:} EBB) U {a:1 + ,Bj(o:1 +,B) E W}
D2 t- (A ffi {,8}) U {,81 + a:i(o: + ,81) E W}
if IBI < IAI
x

then D +- D1
else D t- D2
f-- DOMINATE(a:+ ,8, D)

if X = .l
then F t- F \{a:+ ,8}
else w +-- argmaxq,EF x ¢
·

W +- WU {w}
F t- F \ {w}

19

20 returnW
Figure 4: Complete RR algorithm.

Incremental Pruning for POMDPS
each of the algorithms we studied, witness, IP, and
RR, have a precision parameter c:, but the effect of
varying c on the accuracy of the answer differs from
algorithm to algorithm. Future work will seek to de­
velop an algorithm with a tunable precision parameter
so that sensible approximations can be generated.
From a theoretical standpoint, there is still smne work
to be done developing better best-case and worst-case
analyses for RR. This type of analysis might shed some
light on whether there is yet some other variation that
would be a consistent improvement over IP.
In any event, even the slowest variation of the incre­
mental pruning method that we studied is a consistent
improvement over earlier algorithms. We feel that this
algorithm will make it possible to greatly expand the
set of POMDP problems that can be solved efficiently.

C., and Poole, D. 1996. Computing optimal
policies for partially observable decision processes us­
ing compact representations. In Proceedings of the
Boutilier,

Thirteenth National Conference on Artificial Intelli­

1168-1175. AAAI Press/The MIT Press.

Cassandra, A. R.; Kaelbling, L. P.; and Littman,
M. L. 1994. Acting optimally in partially observ­
able stochastic domains. In Proceedings of the Twelfth
National Conference on Artificial Intelligence, 10231028.
Cheng, H.-T. 1988.

on

Machine

190-196. Amherst, Massachusetts: Mor­
gan Kaufmann.
Learning,

Monahan, G. E. 1982. A survey of partially observ­
able Markov decision processes: Theory, models, and
algorithms. Management Science 28(1):1-16.
Parr, R., and Russell, S. 1995. Approximating op­
timal policies for partially observable stochastic do­
mains. In Proceedings of the International Joint Con­
ference on Artificial Intelligence.

Platzman, L. K. 1981. A feasible computational ap­
proach to infinite-horizon partially-observed Markov
decision problems. Technical report, Georgia Insti­
tute of Technology, Atlanta, GA.
Russell, S. J., and Norvig, P.
ligence: A Modern Approach.

1 994. Artificial Intel­
Englewood Cliffs, NJ:

Prentice-Hall.




To determine the value of perfect informa­
tion in an influence diagram, one needs first
to modify the diagram to reflect the change
in information availability, and then to com­
pute the optimal expected values of both the
original diagram and the modified diagram.
The value of perfect information is the dif­
ference between the two optimal expected
values. This paper is about how to speed
up the computation of the optimal expected
value of the modified diagram by making use
of the intermediate computation results ob­
tained when computing the optimal expected
value of the original diagram.

1

INTRODUCTION

The concept of the value of perfect information is
very useful in gaining insights about decision problems.
Matheson (1990) has demonstrated that influence di­
agrams provide a more suitable paradigm to address
the issue than decision trees. This paper is concerned
with the problem of computing the value of perfect
information in influence diagrams.
An influence diagram is a graphical representation of
a particular decision problem (Howard and Matheson
1984). It is an acyclic directed graph with three types
of nodes: decision nodes, random nodes and value
nodes. The influence diagram in Fig. 1 represents
an extension to the well-known oil wildcatter problem
( Raiffa 1968, Shachter 1986). The decision nodes are
depicted as rectangles, random nodes as ellipses, and
value nodes as diamonds.
The total value of the diagram is oil-sales minus the
sum of test-cost, drill-cost, and sale-cost. The
expectation of the total value depends on the decisions
made. The optimal expected value of the diagram is de­
fined to be the maximum of the expected total values.
There are arcs from test and test-result to

drill.
This means that the drill decision
is to be made knowing the type of test per­
formed and the test-result. There is no arc
from market-information to drill. This means
that market-information (at the time when the
oil-sale-policy is to be made) is not available at
the time the drill decision is to be made.

To reduce risks, the oil wildcatter may choose to,
before making the drill decision, hire an expert
to predicate market-information at the time the
oil-sale-policy is to be made. This operation may
be expensive. So, before making up his mind, the oil
wildcatter may wish to determine the value of the ex­
pert's predications. The value of perfect information
on market-information serves as an upper bound for
the value of the predications. Specifically, it is defined
as follows. Modify the diagram by adding an arc from
market-information to drill. The value of perfect
information on market-information is the difference
between the optimal expected value of the modified
diagram and that of the original diagram.
A straightforward method of computing the value of
perfect information is to exactly follow the definition.
That is to respectively compute the optimal expected
values of the original influence diagram and of the
modified diagram, and then figure out the difference.
This paper shows that one can do better than that.
Since the original and the modified diagrams differ
very little, there must be computation overlaps in the
processes of evaluating them. By avoiding those over­
laps, one can speed up computing the optimal ex-

Figure 1: The influence diagram for an extension of
the oil wildcatter problem.

Computing the Value of Information

pected value of the modified diagram. This is espe­
cially interesting if one wants to assess the value of
perfect information for a number of cases.
The exposition will be carried out in the terms of
stepwise-decomposable influence diagrams, which is
reviewed in section 2. Section 3 introduces the concept
of influence diagram condensation. Section 4 shows
how to use this concept to uncover the computation
overlaps mentioned in the previous paragraph. The
paper concludes at section 5.

2

STEPWISE-DECOMPOSABLE
INFLUENCE DIAGRAMS

Stepwise-decomposable influence diagrams (SDID's)
were first introduced by Zhang and Poole (1992) as
a generalization to the traditional notion of influence
diagrams. Better references in this regard are Zhang,
Qi and Poole (1993b) and Zhang (1993). This section
reviews the concept of SDID's.

401

Note: Traditionally, arcs into decision nodes are inter­
preted as indications of information availability. Now
that the no-forgetting constraint has been lifted, those
arcs need to be re-interpreted as indication of both
information availability and dependency. More explic­
itly, the lack of an arc from a node c to a decision node
d no longer implies that information c is not observed
when making decision d. It may as well mean that d
is independent of c given the parents of d.
Acyclicity and the leaf-node constraint together de­
fine a very general concept of influence diagrams. One
theme of Zhang (1993) is to identify subclasses of in­
fluence diagrams with various computational proper­
ties. One important property for an influence diagram
to possess is the so-called stepwise-solvability, which
says that the diagram can be evaluated by considering
one decision node at a time. If an influence diagram
is not stepwise-solvable, then one needs to simulta­
neously consider several, even all the decision nodes,
which usually tends to be computationally expensive.

Acyclicity, which requires that there be no di­
rected loops in influence diagrams;

When an influence diagram is stepwise-solvable? The
answer: when it is stepwise-decomposable. It can
be shown that a stepwise-decomposable influence di­
agram can be evaluated not only by considering one
decision node at a time, but also by considering one
section of the diagram at a time (Zhang, Qi and Poole
1993). It can also be shown that an influence di­
agram is stepwise-solvable only when it is stepwise­
decomposable (Zhang 1993).

2. Regularity, which requires that there be a total
ordering among all the decision nodes;

In the rest of this section, we define stepwise­
decomposable influence diagrams.

3. The no-forgetting constraint, which requires that
any decision node and its parents be parents to
all subsequent decision nodes;

2.2

2.1

THE PATH TO SDID'S

Traditionally, there are five constraints imposed on in­
fluence diagrams:
1.

4. The single-value-node constraint, which requires
that there be only one value node; and

5. The leaf-value-node constraint, which requires
that the value node have no children.
Influence diagrams that satisfy all the five constraints
will be referred to as no-forgetting influence diagrams.
We propose to lift constraints 2-4 and to develop a
general theory of influence diagrams starting with con­
straints 1 and 5 only.
There are several advantages to lift constraints 2-4.
For instance, by lifting the no-forgetting constraint we
are able to, anmong other things, represent the facts
that some decision nodes are conditionally indepen­
dent of certain pieces of information. In the extended
oil wildcatter problem (Fig. 1), it is reasonable to as­
sume that the decision oil-sale-policy is indepen­
dent of information on test-result given the quality
and quantity of oil-produced. This piece of knowl­
edge can not be represented if the no-forgetting con­
straint is enforced. The reader is referred to Zhang
(1993), Zhang, Qi and Poole (1993) for other rationale
for lifting constraints 2-4.

INFLUENCE DIAGRAMS

An influence diagram is an acyclic directed graph con­
sisting of a set of random nodes C, a set of decision
nodes D, and a set of value nodes U. The value nodes
have no children. A random node c represents an un­
certain quantity whose value is determined according
to a given conditional probability distribution P(cJ7rc),
where 7rc stands for the set of the parents of c. A value
node v represent one portion of the decision maker's
utilities, which is characterized by a value function fv.
Let I be an influence diagram. For any node x in I,
let 'lrx denote the set of the parents of X . Let nx
denote the frame of x, i.e the set of possible values of
x. For any set J of nodes, let nJ = nxEJ nx.
Let d1, ... , dk be all the decision nodes in I. For a
decision node d;, a mapping 8; : n.,.d· ---+ nd, is called
a decision function for d;. The set �f all the decision
functions for d;, denoted by di, is called the decision
function space for d;. The Cartesian product of the
decision function spaces for all the decision nodes is
called the policy space of I. We denote it by .6..
Given a policy 8 = (81, ... , 8k) E .6. for I, a probabil­
ity P0 can be defined over the random nodes and the

402

Zhang, Qi, and Poole

decision nodes as follows:
Po(C, D)=

k

II P(cl7rc) II Po;(dil11"d.),

(1)

i=l

cEC

where P(cl7rc) is given in the specification of the influ­
ence diagram, while P6;(dil11"d;) is given by Oi as fol­
lows:
if 6;(7rd;) = d,,
(2)
otherwise
For any value node v, 11"11 must consist of only deci­
sion and value nodes, since value nodes do not have
children. Hence, we can talk about P0(7r11). The ex­
pectation of the value node v under Po, denoted by
E5[v], is defined as follows:

....

A regular influence diagram is stepwise-decomposable
if for any decision node d, none of the decision nodes
that precede d are in the downstream of 71"d.
The influence diagram in Fig. 1 is a SDID. No­
forgetting influence diagrams are SDID's.
One desirable property of SDID's is that they are
stepwise-solvable. As an example, consider the SDID
in Fig. 1. One can first compute an optimal policy
for oil-sale-policy in the part of the diagram that
lies to the right of oil-produced with oil-produced
included, and then find an optimal policy for drill,
and then for test. The optimal expected value of
the diagram obtained as a by-product of computing
an optimal policy for test. See Zhang, Qi and Poole
(1993b) for details.
3

CONDENSING SDID'S

The summation of the expectations of all the value
nodes is called the value of I under the policy 6, We
denote this denoted by E5[I]. The maximum of E5[I)
over all the possible policies 6 is the optimal expected
value of I. An optimal policy is a policy that achieves
the optimal expected value. To evaluate an influence
diagram is to determine its optimal expected value and
to find an optimal policy.

This section presents a two-stage approach for evaluat­
ing SDID's. In the first stage, a SDID is "condensed"
into a Markov decision process (Denardo 1982). This
involves two types of operations: the operation of com­
puting conditional probabilities and the operation of
summing up several functions. In the second stage,
the condensed SDID is evaluated by the various algo­
rithms (Qi 1993, Qi and Poole 1993).

An influence is regular if there exists a total ordering
among all the decision nodes. Even though all our
results in this paper readily generalizes to influence
diagrams which are not necessarily regular, we shall
limit the exposition only to regular influence diagrams
for the sake of simplicity.

This two-stage approach is interesting because it al­
lows easy implementation of influence diagrams on top
of a system for Bayesian network computations (Zhang
1993). The approach is also of fundamental signifi­
cance to the current paper, as the reader will see in
Section 4.

2.3

STEPWISE-DECOMPO SABLE
INFLUENCE DIAGRAMS

To introduce stepwise-decomposable influence dia­
grams (SDID), we need the concepts of moral graph
and of m-separation. Let G be a directed graph. The
moral graph G is an undirected graph m(G) with the
same vertex set as G such that there is an edge be­
tween two vertices in m( G) if and only if either there
is an arc between them in G or they share a common
child in G. The term moral graph was chosen because
two nodes with a common child are "married" into an
edge (Lauritzen and Spiegehalter 1988).

This approach has been developed from a similar ap­
proach in terms of decision graphs (Qi 1993, Zhang, Qi
and Poole 1993a). In the rset of this paper, we shall
concentrate on the first stage, i.e. condensation. Let
us begin with smoothness in SDID's.
3.1

SMOO THNESS IN SDID'S

An influence diagram is smooth at a decision node d
if there is no arcs from the downstream of 7rd to 7rd.
If an influence diagram is smooth at all the decision
nodes, we say that the diagram is smooth.

In an undirected graph, two nodes x and y are sepa­
rated by a set of nodes A if every path connecting them
contains at least one node in A. In a directed graph G,
x and y are m-separated by A if they are separated by
A in the moral graph m(G). One implication of this
definition is that A m-separates every node in A from
any node outside A.
For any decision node d of I, the downstream of 7rd is
the set of nodes that are not m-separated from d by
7rd. The upstream of 7rd is that set of nodes outside 7rd
that are m-separated from d by 7rd.

Figure 2: The influence diagram in Fig.
smoothing.

1 after

403

Computing the Value of Information

upstream o:f

'Kc�t !

! downstream o! n;dj,
;------------·------------------·----·

------------------------1

� ...

d i-1

�

""

di "'"'

du1

...

�

Figure 4: An abstract view of a smooth regular SDID.
The terminal section I(dk, - ) consists of the nodes in
the 1rd�e and the nodes in the downstream of 1rdk
•

I

(d, a)

Figure 3: The sections of the SDID in Fig. 2.
SDID's may be not smooth. For example, the SDID
in Fig. 1 is not smooth at the decision node drill.
The arc from seismic-structure to test-result is
from the downstream of 1rdrill to 7rdrill·
Two influence diagrams are strongly equivalent if they
have the same set of nodes, the same optimal policies,
and the same optimal expected value. A non-smooth
SDID can always be transformed, by a series of arc
reversals (Shachter 1986 ) , into a strongly equivalent
smooth SDID (Zhang, Qi, and Poole 1993b ). For ex­
ample, the SDID in Fig. 1 can be transformed into a
strongly equivalent SDID whose underlying graphical
structure is shown in Fig. 2. This SDID is smooth.
From now on, we shall only be talking about smooth
SDID's.
3.2

SECTIONS IN SDID'S

The concept of sections in SDID is a prerequisite for
the concept of condensation.
Let I be a smooth regular SDID. Let d1, d2, ..., dk
be the decision nodes. Since I is regular, there is a
total ordering among the decision nodes. Let the total
ordering be as indicated by the subscriptions of the
decision nodes. As a consequence, we have that d;
precedes di+1, and there is no other decision node d
such that d; precedes d and d precedes di+l·
For any i E {1, 2, . . . , k- 1}, the section of I from 1rd;
to 1rd;+l' denoted by I(d;,di+l), is the subnetwork of
I that consists of the following nodes:
1. the nodes in 1rd; U 1rd;+1 ,
2. the nodes that are in both the downstream of 1rd;
and in the upstream of 1rd;+1 ,
The graphical connections among the nodes remain
the same as, in I except that all the arcs among the
nodes in 1rd; U {di} are removed.
The initial section I(-,dt) consists of the nodes in 1rd,
and the nodes in the upstream of 1rd,. It consists of
only random and value nodes.

The nodes in a section that lie outside 1rd; U {d;} are
either random nodes or value nodes. Their conditional
probabilities and value functions are the same as those
in I. The nodes in 1rd; U {d;} are either decision nodes
or random nodes. There are no conditional probabili­
ties are associated with these nodes.
Let us temporarily denote the SDID in Fig. 2
by I. Let us denote the variables test by t,
drill by d, oil-sale-policy by s, drill-cost by
de, test-result by tr, oil-produced by op, and
market-in:formation by mi.
There are four sections in this SDID: I(-, t), I(t,d),
I (d, s) , and I(s,-). The initial section I(- , t) is
empty. All the other sections are shown in Fig. 3.
The concept of sections provides us with a perspec­
tive of viewing smooth regular SDID's. A smooth
regular SDID I can be thought of as consisting of a
chain sections I(-, d1), I(d1, d2), ..., I(dk-1, dk), and
I(dk,-). Two neighboring sections I(di-l,di) and
I(d;, di+l) share the nodes in 7rd,, which m-separate
the other nodes in I(d;_1,d;) from all the other nodes
I(d;,d;+l)· Fig. 4 shows this abstract view of a
smooth regular SDID.
In the extended oil wildcatter example as shown
in Fig. 3, the sections I(t, d) and I (d, s) share
the nodes test and test-result, and the sections
I(d, s ) and I(s-) share the nodes oil-produced and
market-in:formation.
3.3

CONDITIONAL PRO BABILITIES
AND LOCAL VALUES IN THE
SECTIONS

In the section I(d;,d;+1), there is no decision node out­
side 1rd; U { di}. The value nodes are at leaves by defini­
tion. So, one is able to compute the conditional prob­
ability PI(d;,d;+1)(1rd;+1/7rd,, d;) of the nodes in 7rd;+1
given the nodes in 1rd; and d;. We shall refer to this
probability as the conditional probability of 1rd;+1 given
1rd; and d; in I.
In the initial section I(-,d1), one can compute the
probability PI(-,d1l7rd1). We shall refer to this prob­
ability as the prior probab ility 1r1 in I.
For a value node Vj in I(d;, d;+l), one can compute
conditional probability PI(d;,d;+1)(1rvi l1rd., d;). Define

404

Zhang, Qi1 and Poole

a function!�.J : n,.d·•
f�/7r d., d;)

><

nd,- n by

=

L.:

11'v; - ( 11'd; u{ d;})

PI(d;,d;+1)( 1f'v; !1rd;, d;) fv; ( 'lrv; )(3)

where fv; is the value function of

Vj

Figure 5: The condensation of the SDID in Fig. 2.

in I.

Let v1, . . . , Vm be all the value nodes in the section
I(d;, di+1)· The local value function f;: n,.d· X nd, n of the section I(d;, d;+t) is defined by
I

m

fa( 1f'd;' d;) == L.: !�;( 71'd;' d;).
1=1

(4)

•

•

3.4

CONDENSATIO N

Intuitively, condensing a smooth regular SDID I
means to do the following in each section I(d;, d;+l)
of I: (1) getting rid of all the random nodes that are
neither in the 7rd; nor in 7rd'+" (2) combining all the
value nodes into one single value node vf, and (3) col­
lecting the nodes in 1ra, into one compound variable
x;. This results in a Markov decision process.
Now the formal definition. The condensation of I, de­
noted by Ic, is defined as follows:

1. It consists of the following nodes:
• Random nodes x; (0:::; i:::; k), where x; is the
compound variable consists of all the nodes
in 1ra, when 71'd, =/= 0. When 7rd; = 0 or when
i = 0, x; is a degenerated variable that has
only one possible value, say, o1.
• The same decision nodes d; ( 1:::; i :::; k) as in
I; and
• Value nodes vf (0 :::; i:::; k ),
2. The graphical connections among the nodes are
as follows:
• For any i E {2, 3, . . . ,k}, there are two arcs
converging at x;, one from Xi-1 and the other
from di-1·
• For any i E {1, 2, . . . ,k}, there is an arc from
x; to d;,
• For any i E {1, 2, . . . ,k}, there are two arcs
converging at vf, one from Xi and the other
from d;.
1
The presence of the node xo makes the picture ugly.
But we need it for two reasons. First, there may be value
nodes in the initial sec tion. Second, we want to be able to
talk about the condensation of an influence diagram that
contains no decision nodes.

Xo,

one to

3. The conditional probabilities and value functions
are as follows:

When there are no value node in the section, then /;
is defined to be the constant 0.
We can also define the local value "function" for the
initial section, which is not really a function, but just
a constant. We shall denote this constant by fo.

There are two arcs emitting from
vg and the other to x1.

•

•

The conditional probability pe( Xi+llx;,d;)
(i E {1, . . . ,k- 1}) is defined to be
PI(d;+l,d;)(7rd;+ll7rd., d;);
The conditional probability pe(x1!xo = <>) is
defined to be PI( -,dl)( 1rd,), and the probabil­
ity pc(x0) is trivially defined since x0 takes
only one value <>;
The value function fv� for vf (i E
'
{0, 1, . . . , k}) is defined to be /;.

Fig. 5 depicts the condensation of the SDID shown
Fig.2. Since test has no parent, x1 is a degener­
ated variable. The variable x2 stands for the com­
pound variable consisting of test and test-cost,
and X3 stands for the compound variable consist­
ing of oil-produced and oil-market. The condi­
tional probability pe(x3lx2, d) , for instance, is the con­
ditional probability PJ(d,s) ( op, mi It, tr, d) of op
and mi given t, tr, and din the section I(d,s).
The value function fvc1 for the value node vf is
a representation of test-cost, fv� is a representa­
tion of drill-cost, and fvc3 is a representation of
oil-produced and sale-cost.
There is no value node in the initial section. So fvo is
the constant 0. The node is kept only for uniformity.
Two decision networks are equivalent if they have the
same optimal value and share the same optimal poli­
cies. The following theorem is proved in Zhang (1993).
1 A smooth regular SDID is equivalent to
its condensation.

Theorem

To end this section, we would like to echo what we said
at the beginning of the section. The process of con­
densing a SDID only involves two types of operations:
the operation of computing conditional probabilities
and the operation summing up functions (see subsec­
tion 3.3). The latter is straightforward. The formmer
can be carried out by by any well established Bayesian
network evaluation algorithm. One advantage of the
concept of condensation is that it leads to a simple way
of implementing influence diagrams on top of a system
for Bayesian network computation.

Computing the Value of Information

4

COMPUTING THE VALUE OF
PERFECT INFORMATION

Let I be a regular SDID. Let d3 and c respectively be
a decision node and a random node in I, such that
there is no arc from c to d8 in I. Let I' be the diagram
obtained from I by adding arcs from c to d3 and to
all the subsequent decision nodes. If there is no direct
cycles in I'2, then I' is again a regular SDID. In such
a case, the value of perfect information on c at d3 in
I is defined to be the difference between the optimal
expected value of I' and that of I.
In the following, we shall use 71'� to denote the set of
parents of d in I'.
To determine the value of perfect information on c at
d8, one needs to compute the optimal expected val­
ues of both I and I'. To this end, we adopt the two
stage approach described in the previous section, i.e
we first compute the condensations of I and I', and
then evaluate the condensations respectively. An ad­
vantage of this approach is that it can make use of in­
formation stored in the condensation of I in comput­
ing the condensation of I'. More explicitly, for each
section I(d;, di+1) of I, the conditional probability
Pr(a,,a,+,>{1l'a,+,l1l'a,, d;) and the local value function
/; are computed and stored in the condensation of I.
This paper seeks to make use of this conditional prob­
ability and this local value function in computing the
conditional probability Pl'(d;,d;+1)(11'�•+117l'd;• d;) and
the local value function ff of the corresponding sec­
tion I'(d;, d;+1) of I'.
To see an example, 'let I be the SDID shown in
Fig. 2. Consider the value of perfect informa­
tion on market-information at drill. In this
case, I' is the same as I expect for the arc from
market-information to drill. The section I'(s, -)
is the same as I(s, - ) . Thus, when computing the
condensation of I', the conditional probability and the
local value function for this section can simply be re­
trieved from the condensation of I.
The section I'(t,d) is the same as I(t,d) except that

u

�

tains one extra random node market-information.
The node market-information is isolated in I'(t,d).
In the condensation of I', one needs P1,(t,d)(mi, trlt).
This can be computed by
P1,(t,d)(mi, trjt) = P1(t,d)(trjt)P(mi),

where P(mi) is given in the specification of the diagram
and PJ(t,d)(trlt) can be retrieved from the condensa­
tion of I.
The section I'(d, s) is also the same as I(d, s) . How­
ever, the decision nodes drill has one more parent,
2It is always the case if the influence diagram is in the
so-called Howard normal form. See Matheson (1990).

405

namely market-information in I' than in I. Thus,
to
obtain
the
condensation of I', one needs the conditional proba­
bility P1, (d,s )(op,mijt,tr,d,mi). In the condensation
of I, one has PJ( d,s)(op, milt,tr,d). The nice thing is
that one can easily compute P1,(d,s)(op,milt,tr, d,mi)
from Pl'(d,s)(op,milt,tr,d), which is the same as
PJ(d,s)(op,mijt,tr,d), which in turn can be retrieved
from the condensation of I.
To summarize, it takes very little computation to ob­
tain the condensation of I' from the condensation of I.
The rest of this section is to show that the same can
be true for many other cases. We shall do this case by
case. But first, some preparations.
4.1

REMOVABLE ARCS

A random node can be in more than one section. In
the oil wildcatter example, market-information is in
both the section I(d, s) and the section I( s, - ). Let dt
be the last decision node such that c is in the section
I(dt_1, dt) (remember that there is a total ordering
among the decision nodes ) .
The reader is advised to pay close attention to the
definition of dt and the definition of d, (given at the
beginning of this section) , ince we shall use them fre­
quently in the rest of the paper.
It follows from a result of Zhang and Poole (1992) that
in I' the arcs from c to the decision nodes subsequent
to dt, i.e to the decision nodes dt+b ... , dk are remov­
able, in the sense that the removal of those arcs results
in an equivalent influence diagram. As a corollary, if
t < s, all those arcs in I' that are not in I are remov­
able. Hence, I and I' are equivalent. In other words,
if c is in the upstream of 71',, the value of perfect infor­
mation on c at d6 is 0. In the extended oil wildcatter
example, it is of no value to acquire perfect knowledge
about seismic-structure at the time one is to make
the oil-sale-policy.
From now on, we shall let I' stand for the diagram
after the removal of those removable arcs.
4.2

TWO ASSUMPTIONS

We assume that c is a root random node, i.e it has no
parents. A consequence of this assumption is that if
I is smooth, so is I'. W hen c is not a root, one can
transform the diagram by a series of arc reversals so
that c becomes a root in the resulting diagram. This is
very similar to the operation of smoothing mentioned
in subsection 3.1. See Zhang (1993) for details.
We also assume that dt_1 is a parent for every value
node in the section I(dt-1, dt)· The assumption is
to assure that if a value node appears in a section
I(d; ,di+1) of I, then it appears in the corresponding
section I'(d;, di+l) of I'. This a1lows us more chances
in making use of the local value functions of the sec-

406

Zhang, Qi, and Poole

tions of I in computing the local value functions. of I',
as the reader will see in the following. The assumption
is not restrictive because one can always pretend that
the value function f, of a value node v in I(dt-1. dt)
depends on dt even thought it actually does not.
Under those two assumptions, we can show that
I'(dj,dJ+t) is the same as I(dj,dJ+1) except that it
may contain the extra random node c.
4.3

SECTIONS BEFORE da-1 AND
SECTIONS AFTER dt

the section I'(da-1,d8):
J;_1(?Ta,_,,da-1) = !s-1(1rd,_"ds-l),

where fa-1 (1Td,_1,ds-1) can be retrieved from the con­
densation of I.
In the extended oil wildcatter example, the section
from test to drill falls into this case.
SECTIONS IN BETWEEN d. ANDdt-1

4.5

We need to consider four cases. Let us first discuss
the easiest case: the sections before d•-1 and sections
after dt.
This case occurs when i � s-2 or i"?:t. In such a case,
I'(d;,,d£+1) is exactly the same as I(d;,, d;,+l)· So, the
conditional probability and the local value function in
I'(d;, ,d£+1) are the same as those in I(d;, d;+l), which
can simply be retrieved from the condensation of I.

This subsection considers the case when s < i < t - 1.
In this case, the section I'(d;,d;,+1) is t he same as
I(d;,d;,+l), except that it contains one extra node
c. This node is isolated in I'(d;,,d£+1). So, c is in­
dependent of all other nodes in I'(d1,d;,+1). Since
1rd; == 1rd; u {c} and ?ra;+< = ?Td;+1 u {c}, the condi­
tional probability pl'(d;,di+l)(1rai+l11ra;' d;,) satisfies
pl'(d;,d;+, )<?rai+l I?Ta;,d;,)
= Pl'(d;,d;+,)(?Td;+t,c\1rd0 c,d;,)
= pl'(d;,di+l)(1Tdi+l \?Tdi> d;,)
= PI(d;,d;+t)(1Td;+l!1rd;,d;),

In the extended oil wildcatter example, the terminal
section falls into this category.
4.4

THE SECTION FROM

d a-1

TO

d.

The section I'(da-1> d,) is the same as I(d•-1. d,), ex­
cept that it contains one extra node c. This node is
isolated in I' (d,_1,d.). Thus c is independent of all
the other nodes in I'(d.-1,d,).
Since 7ra,_, = 1rd,_, and 7ra, = 1r d, U {c}, the condi-.
tional probability PI'(d,_, ,d,)(?ra,I?Ta,_,,da-1) can be
computed by
Pl'(d ,_,,d ,)(?ra,l?ra,_,, d.-1)
= Pl'(d,_,,d,)(7rd,,ci?Td,_" d,_l)
= Pl'(d,_1,d,)(1rd,\1rd,_1,da-1)P(c)

= PI(d,_"d,)( 1Td,i1Td,_1,d•-1) P(c),

(5)
(6)

where equation (5) is due to the fact that c is inde­
pendent of all the other nodes in I'(ds-1> d.). In equa­
tion (6), P(c) is given in the specification of I and
PI(d,_1,d,)(1Td,\1Td,_.,ds-1) can be retrieved from the
condensation of I.
We now turn to the local value function. For any value
node v in the section I(d._ 1,d.), we have that c rt?Tv,
because c is in another section. By making use of the
fact that c is independent of all the other nodes in
I'(d s-1> d8) again, we get
Pfl(d,_1,d,)(1r� l?rd,_,, d.-1)
= Pl'(d,_1,d,)(1rv 17rd,_1, da-1)
= PI(d,_1,d,)(1rv!1Td,_1,da-1)·

This equation and equations (3, 4) give us the following
formula for computing the local value function /!_1 in

(7)

(8)

where PI(d;,d;+1)(1Td;+1\?Td;,d;,) can be retrieved from
the condensation of I since c E 1rd,
•

We now turn to the local value function. For any value
node v in the section I(d;,d;+l), we have that c � 1r,.
By making use of the fact that c is independent of all
the other nodes in I'(d;, di+l) again, we get
p11(d;,d;+t)( 11"� l?rd;> d;)
= Pl'(d;,di+t)(1rv\1rd;,d;,) = PI(d;,d;+1)(1rv l1rd., d;,).

This equation and equations (3, 4) give us the following
formula for computing the local value function ff in
the section I'(d;, d£+1):

(9)
where /;(7rdi> d;) can be retrieved from the condensa­
tion of I.
THE SECTION FROM

4.6

dt_1

TO

dt

This subsection considers the section from dt-l to dt.
The section I'(dt-1> dt) is the same as I(dt-1> d t), and
?Ta,_, ='lTd,_, U {c} and ?Ta, = 1rd,· Thus we have
pl'(d,_,,d,)(1id, \7id,_,' dt-1)
== PI(d,_,,d,) (1rd, \?rd,_,' c, dt-1 ).

If c

E

1rd,, we have

PI(d,_,,d,)(1rd, I?Td,_,,c,dt-1)
PI(d,_t ,d,)('lTd, l?ra,_,, dt-1)
(lO)
L...d1_1 -{ c} PI(dt-t,dt)(11"d, \1rd1-u dt-1).

Computing the Value of Information

407

Thus, one can use the right nand side of equation (10)
to compute PI'(d,_1,d,)(1rd,l1Td,_,,dt-d whencE 7rd,·
In the extended oil wildcatter problem, the section
from drill to oil-sale-policy is an example of this
case.

I. Of fundamental importance to the method is the
concept of condensation, which also leads easy imple­
mentations of SDID's on top of a system for Bayesian
network computations.

On the other hand, if c rJ. 1Td,, there is no obvious
way to make use of P1(d,_1,d,)(1rd, l7rd,_1, dt-d in com­
puting Pl'(d,_1,d,)(1rd, i1Td1_1 ,dt-d· In such a case,
PI'(d,_1,d,)(1rd,i1Td,_1 ,dt-1) needs to be computed from
scratch.

Acknowledgement

Now, the local value function. If for every value node
in the section I( dt-1,dt), one has that 1r1J � 1Td1_1 U
{dt-d, then it is easy to see that

v

ff-1 ( 1Tdt-1 ' dt-d = ft-1 ( 1Tdt-1' dt-d·

Again in the extended oil wildcatter problem, the sec­
tion from drill to oil-sale-policy is an example of
this case.
In any other case, we see no way to make use of ft-1
in computing JI_1. One needs to compute fi_1 from
scratch.
4. 7

How much computation savings?

To end this section, we would like to give the reader
some idea about how much computation our approach
can save. There are two SDID's, the original I and the
modified I'. The savings are in the process of evalu­
ating I'. There are two stages: in stage 1 one con­
denses I', and in stage 2 one evaluates the condensed
SDID. As we have shown in this section that it takes
very little computation to obtain the condensation of
I' from that of I. This means a lot of savings if there
are many random nodes in I that are not parents of
any decision nodes, since those are the nodes that the
condensation precess needs to get rid of. As we have
pointed out earlier, our approach is especially useful if
one wishes to evaluate the value of perfect information
for a number cases. One can compute the condensa­
tion of I once and use it for all the cases. We can also
save some computation in stage 2. Since I'(d;,di+1)
and I(di,di+1) are the same for all i 2: t, we need
not to re-evaluate these sections at all. Furthermore,
we can save more if we adopt a top down approach for
evaluating the condensed diagrams. See Qi (1993) and
Zhang, Qi, and Poole ( 1993a' ) for details.
5

CONCLUSIONS

The value of perfect information in an influence dia­
gram is defined as the difference between the optimal
expected value of a properly modified influence dia­
gram I' and' that of the I itself. In this paper, we
have described a method for computing the value of
perfect information. The method is incremental in the
sense that it computes the value of I' by using the in­
termediate computation results obtained in evaluating

This paper is partly supported by NSERC Grant OG­
P0044121.



a must in real-world applications.
approximation methods (e.g.

This

paper

is

concerned

in stochastic domains by

with

planning

means of par­

tially observable Markov decision processes
(POMDPs). POMDPs are difficult to solve.
This paper identifies a subclass of POMDPs
called region observable POMDPs, which are
easier to solve and can be used to approxi­
mate general POMDPs to arbitrary accuracy.

Most previous

Cheng 1988, Lovejoy

1991b, and Parr and Russell1995) are value function
approximation methods in the sense that they approx­

imate optimal value functions of POMDPs directly.
We advocate model approximation methods.

Such a

method approximates a POMDP itself by another that
is easier to solve and uses the solution of the latter
to construct an approximate solution to the original
POMDP.
Model approximation can be in the form of a more
informative observation model, or a more deterministic

Keywords:

planning under uncertainty,

action model, or an aggregation of the state space,

partially observable Markov decision pro­

or a combination of two or all of them.

cesses, problem characteristics.

investigates the first alternative.

This paper

The idea of approximating a POMDP by assuming a

1

more informative observation model is not new. Cas­

INTRODUCTION

sandra et al (1996) have proposed to approximate

To plan is to find a policy that will lead an agent to
achieve a goal with minimum cost.

W hen the envi­

ronment of the agent, henceforth referred to as the
world, is completely observable and the effects of ac­
tions are deterministic, planning is reduced to finding
the shortest sequence of actions that leads the agent
to the goal.

POMDPs by using MDPs. This paper generalizes the
idea. We transform a POMDP by assuming that, in
addition to the observations obtained by itself, the
agent also receives a report from an oracle who knows
the true state of the world. The oracle does not report
the true state itself. Rather, he selects, from a list of
candidate regions, a region that contains the true state
and reports that region. The transformed POMDP is

In real-world applications, however, the world is rarely

said to be region observable because the agent knows

completely observable and effects of actions are almost

for sure that the true state is in region reported by the

always nondeterministic. For this reason, a growing

oracle.

number of researchers concern themselves with plan­
ning in stochastic domains (e.g.

Dean and Wellman

1991, Cassandra et al 1994 , Boutillier et al1995, Parr

and Russell 1995). Partially observable Markov deci­

sion processes (POMDPs) can be used as a model for
planning in such domains.

In this model, nondeter­

minism in effects of actions is encoded by transition
probabilities, partial observability of the world by ob­
servation probabilities, and goals and criteria for good
plans by reward functions.
POMDPs are difficult to solve and approximation is

W hen all candidate regions are singletons, the oracle
actually reports the true state of the world. In such
a case, the region observable POMDP reduces to an
MDP. MDPs are much easier to solve than POMDPs.
One would expect the region observable POMDP to
be solvable when all candidate regions are small.

In terms of quality of approximation, the larger the
candidate regions, the less extra information the ora­
cle provides and hence the more accurate the approx­
imation. In the extreme case when there is only one

473

An Approximation Scheme for Decision-Theoretic Planning

candidate region and it consists of all possible states of
the world, the oracle provides no extra information at

all. Hence the region observable POMDP is identical

to the original POMDP.
A way to determine the quality of approximation will
be described. This allows one to make the tradeoff be­
tween approximation quality and computational com­

plexity

as

follows: start with small candidate regions

and increase their sizes gradually until the approxima­

As a background example, consider path planning for
a robot who acts in an office environment. Here S
is the set of all location-orientation pairs, 0 is the
set of possible sensor readings, and A consists of
actions move-forward, tum-left, turn-right,
declare-goal.

and

The current observation o depends on the current state

of the world s. Due to sensor noise, this dependency is

uncertain in nature. The observation o sometimes also

tion becomes accurate enough or the region observable

depends on the action that the robot has just taken a_.

POMDP becomes untractable.

The minus sign in the subscript indicates the previous

In many applications, the agent often has a good idea
about the true state of the world.
planning

as an

example.

Take robot path

Observing a landmark,

a

room number for instance, would imply that the robot

is at the proximity of that landmark.

Observing a

feature about the world, a corridor T-junction for in­
stance, might imply the robot is in one of several re­
gions. Taking history into account, the robot might
be able to determine a unique region for its current
location. Also, an action usually moves the true state
of the world to only a few "nearby" states. Thus if
the robot has a good idea about the current state of
world, it should continue to have a good idea about it

in the next few steps.

time point.
of

o

upon

s

In the POMDP model, the dependency

and

a..

is numerically characterized by

P(ois, a.. ) , which
observation probability.

a conditional probability
referred to as the

is usually
It is the

observation model.

In a region observable POMDP, the current observa­
tion also depends on the previous state of the world
The observation probability for this case can be

8-.

P(o!s,a.., 8-).

written

s+ the world will be in after taking an ac­
a depends on the action and on the current state

The state
tion

8.

The plus sign in the subscript indicates the next

time point.

This dependency

is

again uncertain in

nature due to uncertainty in the actuator.

In the
a is

W hen the agent has a good idea about the true state

POMDP model, the dependency of 8+ upon 8 and

at all time, accurate approximation can be achieved

numerically characterized by a conditional probability

with small candidate regions.
We shall begin with a brief review of planning under
uncertainty and POMDPs. We shall then formally in­

P(s+l8, a), which is usually referred to
tion probability. It is the action model.

as the

transi­

We will often need to consider the joint conditional

P(s+, o+ is, a) of the next state of the world

troduce region observable POMDPs as an approxima­

probability

tion to general POMDPs.

and the next observation given the current state and

Thereafter, we shall de­

scribe a way to determine the quality of approxima­

the current action. It is given by

tion. Finally, we shall report empirical results, which
suggest that when there is not much uncertainty, a

P(8+,o+l8,a) = P(s+ls,a)P(o+l8+,a,s).

POMDP can be approximated accurately by a region
observable POMDP that has small candidate regions
and can hence be solved exactly.

The POMDP model encodes the starting state by a
probability mass function

Po over S. The planning
function such as the fol­

goal is encoded by a reward

2

lowing:

PLANNING UNDER
UNCERTAINTY AND POMDPs

r(s' a)=

To specify a planning problem, one needs to give a set

{1
0

if

a=d�lcare-goal and 8=goal,

otherwise.

(1)

DECISION MAKING IN POMDPs

S of possible states of the world, a set 0 of possible
observations, and a set A of possible actions. In this

3

paper, all those three sets are assumed to be finite.

The agent chooses and executes an action at each time

One needs also to give an observation model, which
describes the relationship between an observation and

edge about the true state of the world, which is sum­

the state of the world; and an action model, which

marized by a probability distribution over the set of

point. The choice is made based on the agent's knowl­

describes the effects of each action. Furthermore, one

possible states and called a

needs to specify the initial state of the world and

lief state

goal state.

a

and a

belief state.

The initial be­

b is the current belief state,
is the current action. H the observation o+ is
is P0• Suppose

474

Zhang and Liu

obtained at the next time point, then the next belief
state b+ is given by

b+(s+) = k LP(s+,o+ls,a)b(s),

(2)

8

where k=1/ La,s+ P(s+, o+ls, a)b(s) is the normaliza­
tion constant (Cassandra et al1994). To signify the
dependence of b+ upon b, a, and o+, we shall some-.
times write it as b+(.lb,a, o+)·
A policy 1r prescribes an action for each possible belief
state. Formally it is a mapping from the set B of all
possible belief states to A. For each belief state b,
1r(b) is the action prescribed by 1r for b. The value
function of 11" is defined for all belief states b by v11" (b) =
Eb(L:o ·lrt], where 0<-y<1 is the discount factor and
rt is the reward received at the tth step in the future.
Intuitively, it is the expected discounted reward the
agent can expect to receive starting from belief state
b if it behaves according to policy 1r. An policy 1r* is
optimal if v,..• (b);::: V,..(b) for all b and all other policies
1r.
The value function of an optimal policy is called
the optimal value function and is usually denoted by
v•.
Policies for POMDPs can be found through value it­
eration (Bellman 1957). Value iteration begins with
an arbitrary initial function "Y(t{b) and improves it by
using the following equation

Vt(b) = maxa[r(b,a) +-y LP(o + lb,a) Vt 1( b+)], (3)
__

0+

where P(o+ lb ,a) = Ls,s+ P(s+, o+ls,a)b(s), and b+ is
a shorthand for b+(-lb,a, o+)· If V0*=0, 'V;* is called
the t-step optimal value function.
It is well known that when the Bellman residual
maxbEBIV't*(b) - yt�1 ( b)l becomes small, l't* is close
to V* and the greedy policy based on vt*

1r(b) = arg maxa[r(b,a) + ')' LP(o+lb,a)l/t* (b+)] (4)
0+

is a good approximation of the optimal policy (e.g.
Puterman 1990).
Since there are uncountably infinite many belief states,
value iteration cannot to carried out explicitly. For­
tunately, it can be carried out implicitly due to the
piecewise linearity of the t-step optimal value function
(Sondik 1971). More specifically, there exists a list Vt
of function of s, usually referred to simply as vectors,
such that for any belief state

vt*(b) = maxvEV1 L V(s)b(s).

(5)

8

Exact methods for solving POMDPs (Monahan 1992,
Eagle 1984, and Larke 1991 (see W hite 1991), Sondik

1971, Cheng 1988,Cassandra et al 1994) attempt to
find a minimum list of vectors that satisfies the
above equation. Unfortunately, even the most effi­
cient algorithm can only solve POMDPs with no more
than twenty states and fifteen observations exactly
{Littman et al1995, Cassandra et al1997). Approxi­
mation is a must for real-world problems.
Most previous approximate methods (e.g. Cheng 1988,
Lovejoy 1991b, and Parr and Russell 1995) attempt to
find a list of vectors that satisfies equation (5) approxi­
mately. This paper proposes to approximate POMDPs
themselves by others that have more informative ob­
servations and hence are easier to solve.
4

PROBLEM CH ARACTERJSTICS
AND APPROXIMATIONS

We make the following assumption about problem
characteristics. Even though in a POMDP M the
agent does not know the true state of the world, he
often has a good idea about it. See the introduction
for justifications of this assumption.
Consider another POMDP M' which is the same as
M except that in addition to the observation made by
itself, the agent also receives a report from an oracle
who knows the true state of the world. The oracle does
not report the true state itself. Rather he selects, from
a list of candidate regions, a region that contains the
true state and report that region.
More information is available to the agent in M' than
in M; extra information is provided by the oracle.
When the agent already has a good idea about the true
state of the world, the oracle does not provides much
extra information even when the candidate regions are
small. In such a case, M' is a good approximation of
M.
In M', the agent knows for sure that the true state of

the world is in the region reported by the oracle. For
this reason, we say that it is region obseroable. The
region observable POMDP M' can be much easier to
solve than M when the candidate regions are small.
For example, if the oracle is allowed to report only sin­
gleton regions, then he actually reports the true state
of the world and hence M' is an MDP. MDPs are much
easier the solve than POMDPs.
We now set out to make the idea more concrete. Let
us begin with the concept of region systems.
4.1

A

Region Systems

region is simply
region system is a

a subset of states of the world. A
collection of regions such that no
region is a subset of other regions in the collection and

475

An Approximation Scheme for Decision-Theoretic Planning

the union of all regions equals the set of all possible

states of the world. We shall use R to denote a region
and n to denote a reg ion system. Region systems are

contain the true state of the world, one that supports
the function P(s, ols_,a.) of s to the maximum degree.
Where there is more than one such regions, choose

to be used to restrict the regions that the oracle can

the one that comes first in a predetermined orde ri ng

choo se to report .

among the regions.

There are many p o ssi ble ways to construct a region

Here are the intuitions.

system. A natural way is to create a region for each

state by in clud ing its "nearby" states.
this more precise.

Each action has an

Let us make

intended effect.

If the previous world st at e
a. were known to the agent, then his current belief
state b(8) would be proportional to P(s,ols.,aJ. In
this case, the rule minimizes extra information in the

The intended effect of move-forward, for instance, is

sense th at it supports the current belief state to the

to move one step forward. We say a sta te

maximum degree.

reachable in one step from another state

inform ative enough, being a landmark for instance, to

an

action whose intended effe<:t

8 is ideally
81 if there is

is, when the world

is

currently in state s', to take the world into state s.

A state

8k

is ideally reachable ink steps from another

state so if there are state

s1,

• • •

, BA:-1

such that si+l is

Also if the current observation is

ensure that the world state is in
regio n

a.

certain region, then

chosen using the rule fully supports the current

belief st ate. In such a case, no extra information is
provided.

ideally reachable from Si in on e step for all 0:5i:5k-1.
Any state is ideally reachable from itself in 0 step.

We do not claim that the rule described above is op­

For any non-neg ative integer k, the radius-k region

is still an open problem.

centered at a state

ally reachable from

s

8

consists of states that are ide­
in k or less steps.

A radius-k

region system is the one obtained by creating a radius­
k re gion for each state and then removing, one after
another, regions that are subsets of others.
W hen k is

0,

the radius-k region system consists of

singlet on regi on s.

On the

other hand, if

there is

timal. Finding a rule that minimize extra information
The

probability P(Ris, o, s_, a_)

P(Ris,o,s.,a.)

=

a k

such t hat any st ate is ideally reachable from any other

state in k or less steps, then there is only one region
in the radius-k region system , which is the set of all
possible states.
4.2

Region Observable POMDPs

the system. This subsection discusses how t he oracle
should choose regions from the system. The main issu e

minimizes the amount of extra information.
little

extra information as possible, the

oracle should consider what the agent alr eady knows.

However, he c annot take the entire history of past ac­

tions

and observations into ac count because if h e did,

M' would not be a POMDP. We suggest the foll ow ing
rule.

For

any

non- negative

any region R,

function f(s) of s and

we call t he quantity supp(f, R)=

EseR f(s)f'EseS f(s) the degree of support of f by
R. If R supp orts f to degree 1, we say that R fully
supports f.
Let s. b e the previous true state of the world,

11

being

sER
R'

if R is the first region s.t.
and for any other region

Es'eRP(s',ols.,a.):2:
Es'ER' P(s', ols., a.)
0 otherwise.

The region observable POMDP M' differs from the
original POMDP M o nly in terms of observation; in

addition to the observation o made by himself, the
an o bser vation in M' by z and
Observat ion model ofM' is given by

den ote

and t he oracle is al low ed to choose region only from

To provi de a.s

R

agent also receives a report R from the oracle. We shall

To complete the definition o£ the region observable
POMDP M', assume a region system has been given

is to

of a region

chosen under the above scheme is given by

P(zis,a_, s.)
4.3

=

P(o, Rls, a., 8. ) = P(ols, a.)P(Ris, o, s_, a_).

S olving Region Observable POMDPs

For any region R, let

8R be the set of belief states that

are fully supported by R. For any region system 'R,
let

BR-

=

UReR-Bn.

Let n be the region system underlying the region

servable POMDP M'.

be the

previous action, and o be the current observation. The
oracle should choose, among all the regions in n that

ob­

It is easy to see that no m atter

what the current belief state b is, the next belief s t at e
b+ must be in f3n. We assume that in M' the initial

belief state is in Bn. Then all possible belief states the
agent

Bn. This implies that poli­
8n and value
restricted to the subset 8 n of 8.

might have are

in

cies for M' need only be defined over

iteration for .M'

can

Restricting value iteration for M' to
a.

write z=(o,R).

8n

implies that

the t- s tep optimal value function Ui of M' is de­

fined only over

Bn and the
u;_l (b)J.

maxbEBR IU; (b)-

Bellman residual is now

Zhang and Liu

476

Like value iteration, restricted value iteration can be
Due to region observability, re­
stricted implicit value iteration in M' can be done
more efficiently than implicit value iteration in M. See
Zhang and Liu (1996) for details.

carried out implicitly.

Implicit restrict value iteration gives us a vectors,
which will be henceforth denoted by Ut. It repre­
sents the t-step optimal value function Ui(b) of M' in
the sense that Ut(b)=maxveu, L8 b(s)V(s) for any
bEBn. The greedy policy for M' based on Ut is as
follows: for any beBn

1r'( b)

=

arg max0[r(b,a) +1 LP(z+lb,a)Ui(b+)] , (6)
Z+

where z+ stands for observation of the next time
point a.od b+ is a shorthand for the next belief state
b+(-lb,a,z+)·
5

POLICY FOR THE ORIGINAL

POMDP

Suppose we have solved the region observable POMDP
M'. The next step is to construct a policy 1r for the
original POMDP M based on the solution forM'.
Even though it is our assumption that in the original
POMDP M the agent has a good idea about the state
of the world at all time, there is no guarantee that its
belief state will always be in B"R.· There is n o oracle in
M. A policy should prescribes actions for belief states
in Bn as well as for belief states outside BR.. An is­
sue here is that the policy 1r' for M' is defined only
for belief states in BR.. Fortunately, 1r1 can be natu­
rally extended to the entire belief space by ignoring
the constraint bEB"R. in equation (6). We hence define
an policy 1f forM as follows: for any bEB,
1r(b)

=

arg m axa[r(b, a)+ 1 L P(z+lb, a)Ui(b+)]. (7)
Z+

Let k be the radius of the region system underlying
M'. The policy 1r for M given above will be referred
to as the mdius-k approximate policy for M. The en­
tire process of obtaining the policy, including the con­
struction and solving of the region observable POMDP
M', will be referred to as region-based approximation.
It is wor t hwhile

to compare this equation with equa­
tion (4). In equation (4), there are two terms on the
right hand side. The first term is the immediate re­
ward for t aking action a and the second term is the
discounted future reward the agent can expect to re­
ceive if it behaves optimally. Their sum is the total
expected reward for taking action a. The action with
the highest total reward is chosen.

The second term is difficult to obtain. In essence,
equation (7) approximates the second term using the
optimal expected future reward the agent can receive
with the help of the oracle, which is easier to compute.
It should be emphasized that the presence of the oracle
is assumed only in the process of computing the radius­
k approximate policy. The oracle is not present when
executing the policy.
QUALITY OF APPROXIMATION

6

AND SIMULATION
In general, the quality of an approximate policy 1f is
measured by the distance between the optimal value
function V"'(b) and the value function V..-(b) of 1f. This
measurement does not consider what the agent might
know about the initial state of the world. As such, it is
not appropriate for a policy obtained through region­
based approximation. One cannot expect such a policy
be of good quality if the agent is very uncertain about
the initial state of the world because it is obtained
under the assumpti on that the agent has a good idea
about the state of the world at all time.

This section describes a scheme for determining the
quality of an approximate policy in cases where the
agent knows the initial state of the world with cer­
tainty. The scheme can be generalized to cases where
there is a small amount of uncertainty about the ini­
tial state; for example, cases where the initial state is
known to be in some small region.

The agent might need to reach the goal fro m dif­
ferent initial states at different times. Let P(s) be
the frequency it will start from state sl. The qual­
ity of an approximate policy 1r can be measured by
Ls IV*(s)- V��"(s)IP(s), where V*(s) and V��" denote
the rewards the agent can expect to receive starting
from state s if it behaves optimally or according to 1f
respectively.
By definition v• (s);?: V"��" (s) for all s. Let u• be
the optimal value function of the region observable
POMDP M'. Since more information is available
to the agent in M', U*(s);?:V•(s) for all s. There­
fore, 'E.[U • (s) - V��"(s)]P(s) is an upper bound on
L8[V*(s)- V��"(s)]P(s).
Let

1r1 be the policy for M' given by (6). When the
Bellman residual is small, 7r1 is close to optimal for M 1
and the value function v.,..' of 1f1 is close to u·. Con­
sequently, L:8[V��"' (s)- V��"(s)]P(s) is an upper bound
on L:,[V*(s)- v1r(s)]P(s) when the Bellman residual
is small enough.

1This is not to be confused with the initial belief state
Po.

477

An Approximation Scheme for Decision-Theoretic Planning

One

way to estimate the quantity

V1r(s)]P(s)

'Z:,[V1r' ( s)

-

is to conduct a large number of simula­
tion trials. In each trial, an initial state is randomly
generated according to P(s). The agent is informed of
the initial state. Simulation takes place in bothM and
M'. In M, the agent chooses, at each step, an action
using 1r based on the its current belief state. The ac­
tion is passed to a simulator which randomly generates
the next state of the world and the next observation
according to the transition and observation probabili­
ties. The observation (but not the state) is passed to

Environment A

the agent, who updates its belief state and chooses the
next action. And so on and

so

forth. The trial termi­

nates when the agent chooses the action declare-goal
or a maximum number of steps is reached. Simulation
inM 1 takes place in a similar manner except that the

C•cut.h)

bvira�Hnt B

observations and the observation probabilities are dif­
ferent and actions are chosen using 1r'.

Figure 1: Synthetic Office Environments.

H the goal is correctly declared at the end of a trial,
the agent receives a reward of the amount "Yn, where
n is the number of steps. Otherwise, the agent receive

no reward. The quantity

'E.,[V1r' (s)- v1r(s)]P(s)

can

be estimated using the difference between the average
reward received in the trials for M' and the average
reward received in the trials fo rM .

7

of region system and (2) where there is not much un­
certainty, a POMDP can be accurately approximated
by a region-observable POMDP that can be solved ex­
actly. This section reports on the experiments.

8.1

Synthetic Office Environments

TRADEOFF BETWEEN

Our experiments were carried using two synthetic

QUALITY OF APPROXIMATION

office environments borrowed from Cassandra et al

(1996)

AND COMPLEXITY
Intuitively, the larger the radius of the region system,
the less the amount of extra information the oracle pro­

with some minor modifications. Layouts of the

environments are shown in Figure 1, where squares
represent locations. Each location is represented as
four states in the POMDP model, one for each ori­

vides. Hence the closerM' is toM and the narrower
the gap between 'E. v1r' (s)P(s) and Es V7r(s)P(s).

entation. The dark locations are rooms connected to

pirical results (see the next section) do suggest that
Ls V,.-(s)P(s) increases with the radius of the region

goal location with the correct orientation.

Although we have not theoretically proved this, em­

system while

Ls v1r' (s)P(s)

the extreme case when there

decreases with it.

is

At

one region in the re­

gion system that contains all the possible states of
the world, M and M' are identical and hence so are

corridors by doorways.

In each environment, a robot needs to reach the

tions: move-forward, tum-left, tum-right, and
declare-goal. The two sets of action models given

Action

Those discussions lead to the following scheme for
making the tradeoff between complexity and quality.

move-forward

Start with the radius-0 region system and increases
the radius gradually until the quantity 'E .. [V,..' (s) -

tum-left

V,..(s)]P(s)

becomes sufficiently small or the region ob­

SIMULATION EXPERIMENTS

Simulation experiments have been carried out to show
that (1) quality of approximation increased with radius

Standard

Noisy outcomes

outcomes

tum-right
declare -:_g_oal

8

in

the following table were used.

E. v1r' (s)P(s) and E. V1r(s)P(s).

servable POMDP M' becomes untractable.

At each

step, the robot can execute one of the following ac­

N(O.ll), F(0.88),
F-F(0.01)
N(0.05), L(0.9),
L-L(0.05)
N(0.05), R(0.9),
R-R(0.05)
N_(l.O)

N(0.2), F(0.7),
F-F(0.1)
N(0.15), L (0 . 7) ,
L-L(O.l5)
N(0.15), R(O.7),
R-R(0.15)
N(LO}

For the action move-forward, the term

F-F (0.01)

means that with probability 0.01 the robot actually
moves two steps forward. The other terms are to be
interpreted similarly. H an outcome cannot occur in a

478

Zhang and Liu

certain

state of the

world, then the robot is left i n the

In each state, the robot is able to perceive

in each
of t hree nominal directions (front, left, and right)
whether there is a doorway, wall, open, or it is
undetermined. The following two sets of observation
mo dels were used:
Actual

Standard observations

case
wall

wall

(0.90),

undetermined
open

doorwa

8.2

(0.02)

(0.02),
open (0.90),
doorway (0.06),
undetermined (0.02)
wall (0.15),
open (0.15),
doorway (0.69),
undetermined (0.01)
wall

I

l
�

15
e

�

Noisy observations

·
····

1100
800
.

.g

I

"3
e
z

•
..

•

..

.rO-ot'acle"

···-··

"rff

-

'r1-<>n�cW•n" --

800
500

///

�··.r

20

1000
J/1.

..

/.

..

(0.70),
open (0.19),
doorway (0.09),
undetermined (0.02
wall (0.19),
open (0.70),
doorway (0.09),
undetermined (0.02
wall (0.15),
open (0.15),
doorway (0.69),
undetermined (0.01

..

/·

../

700

<100
15

wall

(0.04),
doorway (0.04),

open

Env�nmon\A

1000

last state before the impossible outcome.

25

Sleps
Envin>n"""'t B

35

30

llOO

800
700

•rO-oracle"' •••••.
•n-orac'-• .r1. -·-

800

'r(f"-

"

500
<100
10

15

20

Sleps

25

30

F igure 2: Experiments with standard action and noisy
models. The POMDPs are accurat el y approximated
by region observable POMDPs with radius zero or one.

Complexity of Solving the POMDPs

8.3

One of the POMDPs have 280 possible states while
the other has 200. They both have 64 possible ob­
servations and 4 possible actions. Since the l argest
POMDPs that researchers have been able to solve exactly so far have less than 20 states and 15 observations, it is safe to say no existing en.ct algorithms can
solve those two POMDPs.

Quality of Approximation for Standard
Models

To determine the quality of the radius-0 and radius-1
approximate policies for the POMDPs with s tand ard
action and observation models, 1000 simulation trials
were conducted using the scheme described in Section
6. It was assumed that the agent is equally likely to
We were be able to solve the radius-O and radius-1
start from any state. Instead of the average reward
approximations (region observable POMDPs) of the
over the trials, the performance of the agent is sumtwo POMDPs on a SUN SPARC2o computer. The
marized by the distribution of the numbers of steps it
threshold for the Bellman residual was set at 0.001
took to successfully complete the trials, i.e. by a funcand the discount factor at 0_99_ The amounts of time
tion g(n) of st eps n, where for each n, g(n) is the numit took in CPU seconds are collected in the following
her
of trials where the goal was reached and declared
table.
r-:::--..,---.-..,---=---:-...,...,=---r---:-::-:---�,-----. in n or less steps. The average reward over the tri­
t-;::----;o:--�-:;::-.....-;---,.--+-;;:;:--..--;<-..-.,---=-:-:-1 als can be computed by E�o In (g (n)-g(n-1)) /1000.
F.==�=:::;::::=l=
::::;:
::::;::=:===:l==
=:::==:===¥==::::;:====l We choose the function g(n) instead of the average re­
ward because it is more informative than the latter.

I

.:. __..
...:
;;.... L..._...:.:.::.
.;. _.J.._....;_;...;___J
..: -'-___;....;..:.._
---J...---.:.....:.L-.

We see that the radius-1 approximations took much
longer time to solve than the radius-0 approximations.
Also notice that the region observable POMDPs with
noisy action and observation models took more time
to solve that those with the standard models.
We were unable to solve the radius-2 approximations.
Other approximation techniques need to be incorpo­
rated in order to solve th e approximations based on
region sy stems with radius larger than or equal to 2.

Simulation results are shown in Figure 2. The curves
instance, represent the g-functions for
simulations in the radius-0 region observable POMDPs
(i.e. with the help of the oracle) using their opti­
mal policies. In contrast, the curves rO represent the
g-functions for simulations in the original POMDPs
(without the help of the oracle) using radius-0 approx­
imate policies. For readability, only top portions of the
g-functions are shown.
rO-oracle, for

We see that the gap between rO-oracle and rO is quite

An Approximation Scheme for Decision-Theoretic Planning

small in both cases. This indicates that the radius-0
region observable POMDPs (MDPs) are quite accu­
rate approximations of the original POMDPs. The

Environ..-! A

1000

...-··

1100

I

radius-0 approximate policies are close to optimal for

I

the original POMDPs.
The gaps between the curve'3 rl-oracle and rl are

'l5
E
"
z

even narrower. For environment A, there is essentially

no gap. Also n otice that the curves rl lie above rO

..

700
aoo

500
<100

creases with radius of region system.

,./
/
/
;

•ro-o,.c-.- -·� -·
·
"r1-onoc•·­
•n• -·­

20

80

'
..

300

!

"rfl'-

I

�
0

·

/
!

900

/

100

and the curves rl-oracle lie below rO-oracle. Those
support our claim that quality of approximation in­

479

..
0

100

1000
1100

There is a couple other facts worth mentioning. The

..

BOO

)

BOO

..e

gaps are larger in environment B than in environment
A. This is because environment B is more symmet­
ric and consequently observations a.re less effective in

700

�

8
0

disambiguating uncertainty in the agent's belief about

----.
"10-orac'-"
•r1-oi'IIC.. •n• -·­

�

"rfl'-

z

the state of the world.
There were a few failures in environment A even with

the presence of the oracle (curve rl-oracle). The
occurred due to uncertainties in the actions

failures

models: The agent was one step away from the goal

and had an very good idea about the state of the world.
An action towards the goal was taken and afterwards
the agent believed strongly that the world is in the
goal state. However, the action failed to effect any

Figure
models.

3:

Experiments with noisy action and noisy

The POMDPs are not accurately approxi­

mated by region observable POMDPs with radius zero
or one.

movement and the orcale's report did point this out2•
So a failure.

8.4

POMDPs exactly.

Quality of Approximation for Noisy
Models

Tracing through the trials, we learned some interesting
facts. In environment B, the agent, under the guidance
of the radius-1 approximate policy, was able to quickly

One thousand trials were also conducted for the

get to the neighborhood of the goal even when starting

POMDPs with noisy action and observation models.

from far way. The fact that the environment around

Results are shown in F igure

the g oal is highly symmetric was the cause of the poor
performance. Often the agent was not able to deter­

3.

We see that the gaps between rl-oracle and rl is sig­
nificantly narrower than the gaps between rO-oracle
and rO, especially for environment A. The curves rl
lie above the curves rO and the curves rl-oracle lie
below rO-oracle. Again, those support our claim that
quality of approximation increases with radius of re­

gion system.
As far as absolute quality of approximation is con­
cerned, the radius-0 POMDPs are obviously very poor
approximations of the original POMDPs since the gaps
gaps between the curves rO-oracle and rO are very
wide. For Environment A, the radius-1 approxima­
tion is fairly accurate. However, the radius-1 ap­
proximation remains poor for environment B. The ra­
dius of region system needs to be increased. Unfortu­
nately, increasing the radius beyond 1 renders it com­
putationally impossible to solve the region observable
2The oracle reported a region that contains both the

goal and the actual state.

mine whether it was at the goal location (room), or
in the opposite room, or in the left most room, or in

the room to the right of the goal location. The perfor­
mance would be close to optimal if the goal location
had some distinct features.

In environment A, the agent, again under the guidance
of the radius-! approximate policy, was able to reach
and declare the goal successfully once it got to the
neighborhood. However, it often took many unneces­
sarily steps before reaching the neighborhood due t o
the undesirable effects o f the turning actions.

Take

the lower left corner as an example. When the agent
reached the corner from above, it was facing down­
ward. The agent executed the action turn_left. Fif­
teen percent of the time, it ended up facing upward
instead of to the right - the desired direction. The
agent then decided to move-forward, thinking that it
was approaching the goaL But it was actually moving
upward and did not realize this until a few steps later.

Zhang and

480

Liu

The agent would perform much better there were in­
formative landmarks around the corners.
9

[6] II. T. Cheng (1988), Algorithms for partially ob­
servable Markov decision processes, PhD thesis,
University of British Columbia, Vancouver, BC,
Canada.

CONCLUSIONS

We propose to approximate a POMDP by using a
region observable POMDP. The region observable
POMDP has more informative observations and hence
is easier to solve. A method for determining the qual­
ity of approximation is also described, which allows
one to make the tradeoff between quality of approxi­
mation and computational complexity by starting with
a coarse approximation and refining it gradually. Sim­
ulation experiments have shown that when there is not
much uncertainty in the effects of actions and obser­
vations are informative, a P OMD P can be accurately
to approximated by a region observable POMDP that

be solved exactly. However, this becomes infeasi­
the degree of uncertainty increases. Other ap­
proximate methods need to be incorporated in order to
solve region observable POMDPs whose radiuses are
not small.

can

ble

as

Acknowledgement

Research was supported by Hong Kong Research
Council under grants HKUST 658/95E and Hong
Kong University of Science and Technology under
grant DAG96/97.EG01 (Rl).



and those that exploit structures in the probability ta­
bles (e.g.

[3], [1]).

We are interested in exploiting structures in the prob­

This paper explores the role of independence

ability tables induced by independence of causal influ­

of causal influence (ICI) in Bayesian network

ence

inference. ICI allows one to factorize a con­

(ICI).

Heckerman

ditional probability table into smaller pieces.

The concept of ICI was first introduced by

[3]

under the name causal independence.

It refers to the situation where multiple causes inde­

We describe a method for exploiting the fac­

pendently influence a common effect. We use the term

torization in clique tree propagation (CTP)

"independence of causal influence" instead of "causal

- the state-of-the-art exact inference algo­

independence" because many researchers have come

rithm for Bayesian networks. We also present

to agree that it captures the essence of the situation

empirical results showing that the resulting

better than the latter.

algorithm is significantly more efficient than
the combination of CTP and previous tech­

Knowledge engineers had been using specific models of

niques for exploiting ICI.

ICI in simplifying knowledge acquisition even before
the inception of the concept

Keywords:

Bayesian networks,

causal influence

(causal

independence of

independence),

inference,

clique tree propagation.

[13}

and Heckerman

be used to simplify the structures of Bayesian networks

[8])

[16],

made the observation that ICI

into smaller pieces and showed how the
Howard and Matheson

are a knowledge representation framework widely

used by AI researchers for reasoning under uncertainty.
They are directed acyclic graphs where each node rep­
resents a random variable and is associated with a
conditional probability table of the node given its par­
ents. This paper is about inference in Bayesian net­
works.

([23])

enables one to factorize a conditional probability table

INTRODUCTION

Bayesian networks (Pearl

Olesen et al

([5], [1 3]).

have also shown how ICI can

so that inference can be more efficient.

Zhang and Poole

1

[3]

There exists a rich collection of algorithms.

The state-of-the-art is an exact algorithm called clique

VE algorithm

- another exact inference algorithm - can be
tended to take advantage of the factorization.

ex­

This

paper extends CTP to exploit conditional probability
table factorization. We also present empirical results
showing that the extended CTP is more efficient than
the combination of CTP and the network simplifica­
tion techniques. In comparison with Zhang and Poole

[23], this paper presents a deeper understanding ofiCI.
The theory is substantially simplified.

tree propagation1 (CTP) (Lauritzen and Spiegelhalter

[12],

Jensen et al

[10],

and Shafer and Shenoy

[20]).

Unfortunately, there are applications that CTP can­
not deal with or where it is too slow (e.g. [18]). Much
recent effort has been spent on speeding up inference.
The efforts can be classified into those that approxi­
mate (e.g.
1

[15), [2), [9], [6], [7], [17], [22], [11],

Also known

as

junction tree propagation.

and

[19])

2

BAYESIAN NETWORKS

A Bayesian network (BN) is an annotated directed
acyclic graph, where each node represents a random
variable and is attached with

a

conditional probabil­

ity of the node given its parents. In addition to the
explicitly represented conditional probabilities, a BN
also implicitly represents conditional independence as-

482

Zhang and Yan

sertions. Let x1, x2,
, Xn be an enumeration of all
the nodes in a BN such that each node appears before
its children, and let rrz, be the set of parents of a node
x,. The following assertions are implicitly represented:

is conditionally independent of all other
all other ej's given c;, and

• • •

For i=l, 2, ... n, x; is conditionally indepen­
dent of variables in {xt,X2,····x•-d\rrz,
given variables in 1l",w
The conditional independence assertions and the con­
ditional probabilities t ogether entail a joint proba bility
over all the variables. As a matter of fact, by the chain
rule, we have
n

P(z1, x2, . . . , zn.)

=

IJ P(z;lxll x2, . . . , Xi-1 )

i=1
n

IT P(z;l1l"z1),

(1)

i=1

where the second equation is true because of the con­
ditional independence assertions and the conditional
probabilities P(x;l'lr:r:,) are given in the specification of
the BN. Consequently, one can, in theory, do arbitrary
probabilistic reasoning in a BN.
3

Bayesian networks place no restriction on how a node
depends on its parents. Unfortunately this means that
in the most general case we need to specify an expo­
nential (in the number of parents) number of condi­
tional probabilities for each node. There are many
cases where there is structure in the probability ta­
bles. One such case that we investigate in this paper
is known as independence of causal influence (ICI).
The concept of ICI was first introduced by Heckerma.n
[4]. The following definition first appeared in Zhang
and Poole [24].
In one interpretation, arcs in a BN represent causal
relationships; the parents c1, c2, . .. , Cm of a node e
are viewed as causes that jointly bear on the effect e.
,
ICI refers to the situation where the causes c1o c2
and Cm contribute independently to the effect e. In
other words, the ways by which the c;'s influence e are
independent.
•

• •

More precisely, Ct, c2
, and Cm are said to influence
e independently if there exist random variables �1, �2
... , and �m that have the same frame- set of possible
values - as e such that
• • •

C;

and

There exists a commutative and associative bi­
nary op erato r * over the frame of e such that
e == el *6* ... *�m·

We shall refer to ei as the co ntrib ution of e; to e. In less
technical terms, causes influence their common effect
independently if individual contributions from differ­
ent causes are independent and the total influence is a
combination of the individual contributions.
We call the variable e a convergent variable for it
is where independent contributions from different
sources are collected and combined (and for the lack of
a better name). Non-convergent variables will simply
be called regular variables. We also call * the base com­
bination operator of e. Different convergent variables
can have difference base combination operators.
The reader is referred to [24] for more detailed expla­
nations and examples of ICI.
The conditional probability table P(elct, . .. , em ) of a
convergent variable e can be factorized into smaller
pieces. To be more specific, let /; (e, e;) be the function
defined by

fi(e= a, e;) = P(ei=aie;),

INDEPENDENCE OF CAUSAL
INFLUENCE

1. For each i, e. probabUistically depends on

2.

Cj 's

and

for each possible value o of e. It will be referred to
as the contributing factor of e; to e. Zhang and Poole
[24] have shown that

P(elc11 .. . , Cm)

=

®�di(e, c;),

where ® is an operator for combining factors to be
defined in the following.
Assume there is a fixed list of variables, some of which
are designated to be convergent and others are desig­
nated to be regular. We shall only consider functions
of variables on the list.
Let f(et, ... ,e�c , A, B) and g(e1, . . . ,e�c,A,C) be two
functions that share convergent variables et, .. . , e,.
and a list A of regular variables. B is the list of vari­
ables that appear only in I, and C is the list of vari­
ables that appear only in g. Both B and C can contain
convergent variables as well as regular variables. Sup­
pose *i is the base combination operator of ei· Then,
the combination f®g off and g is a function of vari­
ables e1,
, e1c and of the variables in A, B, and C.
It is defined by
. • .

j®g(et=a11

• • •

,e�c=a�c, A,B, C)

f(et =au, . . .,e�c=akl, A, B)
g(et =Otz, . . . , e�c=a�cz, A, C), (2)

Independence of Causal Influence and Clique Tree Propagation

constitute

483

heterog e­

a

neous factorization of P(a, b,c, e1,e2, e3 ) because the

joint probability can be obtained by combining those

factors in a proper order using either multiplication or
the operator

®. The word heterogeneous is

to signify

the fact that different factor pairs might be combined

in different ways. We shall refer to the factorization
as the heterogeneous factorization represented by the

BN in Figure 1.
The heterogeneous factorization is of finer grain than
the homogeneous factorization. The purpose of this

Figure 1: A Bayesian network.

paper is to exploit such finer-grain factorizations to
speed up inference.

write

e,. We shall sometimes
/ ®gas /(e1,••. ,e�:,A,B)®g(eb ... ,e�:, A,C) to
make explicit the arguments of f and g.

5

The operator ® is associative and commutative. W hen

In a heterogeneous factorization, the order by which
factors can be combined is rather restrictive. The con�

for each possible value ai of

f and g do not share convergent variables,

j®g is sim­

ply the multiplication fg.

4

tributing factors of a convergent variable must be com�
bined with themselves before they can be multiplied

FACTORIZATION OF JOINT
PROBABILITIES

A BN represents a factorization of a joint probability.
For example, the Bayesian network in Figure 1 factor­
izes the joint probability

DEPUTATION

P(a,b,c,et,e2,ea)

into the

following list of factors:

P(a), P (b),P (c), P(e1la,b, c), P(e2la ,b, c), P(ealel,e2).

with other factors. This is the main issue that we need
to deal with in order to take advantage of conditional
probability table factorizations induced by

To alleviate the problem, we introduce the concept of
deputation. It was originally defined in term of BNs
[24]. In this paper, we define it in terms of heteroge­

neous factorizations themselves.

In the heterogeneous factorization represented by
e is to make

BN, to depute a convergent variable
copy

e1

of

e and

ing factors of
The joint probability can be obtained by multiply­
ing the factors.

We say that this factorization is

multiplication-homogeneous because all the factors are
combined in the same way by multiplication.
Now suppose the e,'s are convergent variables. Then
their conditional probabilities can be further factorized

as

follows:

replace

e with e 1

e. The variable

a
a

in all the contribut­

e' is called

the

deputy of

e and it is designated to be convergent. After depu­

tation, the original convergent variable e is no longer
convergent and is called a new regular variable. In con�
trast, variables that are regular before deputation are

called old regular variables.

After deputing all convergent variables, the heteroge­
neous factorization represented by the BN in Figure 1
becomes the following list of factors:

P(e1la, b, c)

=

P(e2!a,b,c)

=

P (ealell e2)

=

where the

ICI.

/u (e1, a)®!I2{e11 b)®ha(el! c) ,
h1(e2, a)®!22 (e2 , b)®ha(e2, c),

/u(ei,a), /1 2(e;,b), !Ia( e;,c), !21(e;,a),
/22(e�,b), /23(e;,c), /31(e�,e1), /32{e�,e2),
P(a), P(b), P(c).

/a1(ea, et)®/a2(ea,e2),

factor !11 {e1,a),
a to e1•

for instance, is the con­

tributing factor of

We say that the following list of factors

The rest of this section is to show that deputation
renders it possible to combine the factors in arbitrary
order.

fu (e1, a),i12(e11 b),fta(el, c),

5.1

/2I(e2,a),/22(e2,b),f2a(e2,c),
fai(ea,e1), /a2(ea, e2),
P(a),P(b), and P(c)

replace it with the corresponding new regular variable

ELIMINATING DEPUTY VARlABLES
IN FACTORS

Eliminating a deputy variable e1 in a factor f means to

484

e.

Zhang and Yan

The resulting factor will be denoted by

be more specific, for any factor
a list A of other variables,

f(e,e',A)

fle•=e·

Procedure

To

of e, e' and

1. If x1 is a new regular variable, remove
from F all the factors that involve the

fle•=e(e=a, A) = f(e=a, e'=a, A),
for each possible value
of

e'

and a list

A of

a

of

e.

deputy x� of :lh, combine them by 0 re­
sulting in, say, f . Add the new factor

For any factor f(e', A)

flz�=z1

other variables not containing e,

e.

volve x1, combine them by using ® re­
sulting in, say, g. Add the new factor g

For any factor f not

to

3.

Suppose f involve two deputy variables e� and e� and
we want to eliminate both of them. It is evident that
the order by which the deputy variables are eliminated
does not affect the resulting factor. We shall denote
the resulting factor by

5.2

/le� =e1 ,e;=e2

Return

:F.

Theorem 2 The
list
of
factors
returned
by sumoutc(:F, x1) is a ®-homoogeneous factorization
of P(xz, ... , Xn ) = E:�:1 P(xt. x2, ... ,xn)·
7

MODIFYING CLIQUE TREE
PROPAGATION

For later convenience, we introduce the concept of ®­
homogeneous factorization in term of joint potentials.

, Xn be a list of variables. A joint poten­
tialP(xl, x2, . . . , Xn ) is simply a non-negative function
• • •

o f the variables. Joint probabilities are special joint
potentials that sum to one.
Consider a joint potential P(et, ... , e�::, x1::+1, . . . , Xn )
of new regular variables e, and old regular variables
Xi.

:F.

•

®-HOMOGENEOUS
FACTORIZATIONS

Let :z:1, :z:2,

to F. Endif

2. Remove from F all the factors that in­

fle•=e(e=a, A) = f(e'=a, A ),
for each possible value o of
invol ving e', fle•=e =f.

sumoutc(F, x1)

A list of factors /t, ..., fm of the e,'s, their

Theorem 2 allows one to exploit ICI in many inference
algorithms, including VE and CTP. This paper shows
how CTP can be modified to take advantage of the
theorem. The modified algorithm will be referred to
as CTPI. As CTP, CTPI consists of five steps; namely
clique tree construction, clique tree initialization, ev­
idence absorption, propagation, and posterior proba­
bility calculation. We shall discuss the steps one by

one. Familiarity with CTP is assumed.

'

deputies eL and the xi s is a ®-homogeneous factor­

ization (reads circle cross homogeneous factorization)
, Xn ) if

of P(el! ... , e�:, Xk+l•

P(el,.

. .

'e�::, Xk+l•

7.1

CLIQUE TREE CONSTRUCTION

• . •

• • •

' Xn )

=

(®�di)le�=et,... ,e�=e....

Theorem 1 Let :F be the heterogeneous factorization
represented by a BN and let :F' be the list of factors
obtained from :F by deputing all convergent variables.
Then :F' is a ®-homogeneous factorization of the joint
probability entailed by the BN.
All proofs are omitted due to space limit. Since the
operator ® is commetative and associative, the theo­
rem states that factors can be combined in arbitrary
order after deputation.

A clique is simply a subset of nodes.

A clique tree

is a tree of cliques such that if a node appear in two
different cliques then it appears in all cliques on the
path between those two cliques.
A clique tree for a BN is constructed in two steps: first
obtain an undirected graph and then build a clique tree
for the undirected graph. CTPI and CTP differ only
in the first step. CTP obtains an undirected graph
by marrying the parents of each node (i.e. by adding
edges between the parents so that they are pairwise
connected) and then drop directions on all arcs. The
resulting undirected graph is called a moral graph of
the BN.

6

SUMMING OUT VARJABLES

Summing out a variable from a factorization is a funda­
mental operation in many inference algorithms. This
section shows how to sum out a variable from a ®­
homogeneous factorization of a joint potential.
Let

:F be a ®-homogeneous factorization of a joint po­

In CTPI, only the parents of regular nodes (represent­
ing old regular variables) are married. The parents of
convergent nodes (representing new regular variables)
are not married. The clique tree constructed in CTPI
has the following properties: (1) for any regular node
there is a clique that contain the node as well as all its
parents and (2) for any convergent node e and each of

tential P(x1, x2, . . , xn). Consider the following pro­

its parents x there is a clique that contains both e and

cedure.

x.

.

485

Independence of Causal Influence and Clique Tree Propagation

7.2

7.3

CLIQUE TREE INITIALIZATION

EVIDENCE ABSORPTION

Suppose a variable x is observed to take value a. Let
Xz=:a:(:z:) be the function that takes value 1 when :t==a

CTPI initializes a clique tree as follows:
1. For each regular node, find one clique that con­

tains the node as well as all it s parents and attach
the conditional probability of the node to that
clique.

and
that

0 otherwise. CTPI absorbs the piece of evidence
x=a as follows: find all factors that involve x and

multiply X:�:=a(x) to those factors.
Let

Xm+l,

O'm+b •

2.

,

Xn

be all the observed variables and

be their observed values.

Let Xt,

. • •

,

After evidence ab­

sorption, the factors associated with the cliques con­

(a) H there is a clique that contains the node and
all its parents, regard e as a regular node and

stitute a ®-homogenous factorization of joint poten­
tialPx
( t, ... 1Xm,Xm+l=am+11
,Xn=an) ofx11 . . . ,
• • •

Xm•

1.

( b) Otherwise for each parent

• • •

, an

Xm be all unobserved variables.

For each convergent node e

proceed as in step

. .

x

be the contributing factor of

of e, let

x to e.

f (x, e)

Find one

clique that contains both e and x, attached
to that clique the factor f(x,e'), where e' is
the deputy of e.

7.4

CLIQUE TREE PROPAGATION

Just as in CTP, propagation in CTPI is done in two
sweeps. In the first sweep messages are passed from
the leaf cliques toward a pivot clique and in the sec­
ond sweep messages are passed from the pivot clique

After initialization, a clique is associ ated with a list

toward the leaf cliques.

Unlike in CTP where mes­

sages passed between neighboring cliques are factors,

(possibly empty) of factors.

in CTPI messages passed between neighboring cliques

A couple of notes are in order. Factorizing the condi­

are lists of factors.

tional probability table of a convergent variable e into

Let C and C' be two neighboring cliques. Messages
can be passed from C to C' when C has received mes­

smaller pieces can bring about gains in inference ef­

fidency because the smaller pieces can be combined
with other factors before being combined with them­
selves, resulting in smaller intermediate factors.

H

there is a clique that contains e and all its parents,

then all the smaller pieces are combined at the same
time when processing the clique. In such a case, we are
better off to regard

e

as a regular node (representing

Procedure

this intuition.
On the other hand, if all variables that appear in one
factor f in the list also appear in another factor g in
the list, it does not increase complexity to combine f
and g.

Thus we can reduce the list by carrying out

such combinations. Thereafter, we keep the reduced
list of factors and combine a factor with others only
when we have to.

Since ® is commutative and associative, the factors
associated the cliques constitute a ®-homogenousfac­
torization of the joint probability entailed by the BN.

End­

2. Red uce the list F offactors and send

the

reduced list to C'.

visable to do the same in CTPI because the factors in­

and leads to inefficiency. Experiments have confirmed

sumoutc(F, Xi ) ,

=

for

combined at initialization and the resulting factor still
involves only those variables in the clique. It is not ad­

of new regular variables in the clique. Combining them

,x1

send.Message(C, C')

1. For i=l to l, :F

Second, in CTP all factors associated with a clique are

all right away can create an unnecessarily large factor

• • •

all the variables in C\C'. Let :F b e the list of the
factors associated with C and the factors sent to C
from all o ther neighbors of C. Messages are passed
from C t o C' by using the following subroutine.

an old regular variable).

volve not only variables in the clique but also deputies

Xt,

sages from all the other neighbors. Suppose
are

7.5

POSTERIOR PROBABILITIES

a clique and let x1,
, x1 be
all unobserved variables in C. Then the factors asso­

Theorem 3 Let C be

• • .

ciated with C and the factors sent to C from all its
neighbors constitute a ®-homogeneous factor ization of

P(Xt, ... 1 Xl1 Xm+l ='Om+11

• • •

, Xn=an).

Because of Theorem 3, the posterior probability of any

unobserved variable x can be obtained as follows:
Procedure

getProb(x)

1. Find a clique C that contains x. (Letx2,
... , x1 be all other variables in C. Let F
be the list of the {actors associated with

486

Zhang and Yan

a from

summing out variable

the list h of factors. It

is the following list of one factor:

{P.H4(e�,e;)},
where /.'l-+ 4(eL e;) = Ea J>(a)fu(ei, a)ht (e�,a).
Messages from cliques 2 and 3 to 4 are similar.
To figure out the message from clique
3

and sent to clique

Figure

2:

cliques

4

2 and 3 is:

from the list of factors. Summing out

=

for

f

ea

Hence the message is the following list of factors:

be the

{J.t2-+4(e�,e;)®J.ts-+4 (e�,e;),tJI(et,e2)}.

resulting factor.

4. If x is a new regular
flz•=f E:r: / l:r:•=:r:·
5. Else return f / E:r: f.

results in

tJI(et,e2)= L J>(ealet,e2)Xe1=a-(et)·

sumoutc(:F, x,), End­

Combine all factors in :F. Let

e3

a new factor

neighbors.)

2. For i= 2 to l, :F

variable return

where the first two factors are combined due to factor
list reduction. Messages from clique

3 are

4 to

cliques

2 and

similar.

Consider computing the posterior probability of

AN EXAMPLE

ea .

T he only clique where we can do this computation is
clique

A clique tree for the BN in Figure 1 is shown in Figure

2.

4 from

The message is obtained by summing out the variable

C and the factors sent to C from all its

8

1, we

{J>(ealet, e2)X<!1=a-(et), /.'2-+4(e�,e;),f.l3-+4 (e�,e;)}.
A clique tree for the BN in Figure 1.

e3

3.

4 to clique

notice that the list of factors associated with clique

4.

The list of factors associated with clique 4

and factors sent to clique

4 from all its

neighbors is

After initialization, the lists of factors associated

{J>(ealet,e2)Xe1=a-(et),
/.'2--+4(eJ., e2), 1'3-+4(e1,e2)}

with the cliques are as follows:

lt =
l2 =
la =
l4 =

{J>(a)ftt(e�,a),/2t{e;,a)},
{J>(b)!t2(e�,b),/22(e;,b)},
{J>(c)fta(e�,c),/2a(e;,c)},
{J>(ea!eh e2)}.

There are two variables to sum out, namely
Assume

tion because they do not share convergent variables.
and all its parents appear in clique

4,

its conditional probability is not factorized. It is hence
regarded as an old regular variable.

ing the piece of ev idence changes the list of factors

4 to

Then

e1

And then

e;

=

e1

is eliminated, yielding a new factor

e2

=

chosen to be the pivot. Then mes-

sages are first propagated from cliques
clique

4

and then from clique

The message from clique

1

4

1, 2,

to cliques 1,

to clique

4 is

and

3

to

2, and 3.

obtained by

Finally,

cf>2(e2, e;,ea)le;=w

is summed out, yielding a new factor

¢4(ea)

4 is

yielding a new

l: P(ealel,e2)Xe1=a-(et)¢t(et,e�).

¢a(e2,ea)

the following:
And then

Suppose clique

e1 and e2.

The first step

itself is summed out, yielding a new factor

Since

J>(eslell e2) is the only factor that involves et, absorb­
associated with clique

is to eliminate

e 2.
e i,

¢1(e1,e;) =
{mul-+4(e� ,e�)®mu2-+4(e�,e;)®mua-+4(ei,e2)]le�=e1•

¢2(e2,e;,ea)

Suppose e1 is observed to t ake value a.

e'

factor

tion and combination of factors reduces to multiplica­

e3

is summed out before

in summing out

Several factors are combined due to factor list reduc­

Also because

e1

==

L ¢a(e2,ea).
e2

487

Independence of Causal Influence and Clique Tree Propagation

9

EMPIRICAL COMPARISONS

100

WITH OTHER METHODS

This section empirically compares CTPI with CTP.
We also compare CTPI with PD&CTP, the combina­
tion of the parent-divorcing transformation [14] and
CTP, and with TT &CTP, the combination temporal
transformation [4] and CTP.

'CTP'

�
:;::

NN
145
145
245
245

NN:

number of nodes;
average number of parents;
average number of possible values of
a node.

AN-PN:
AN-PVN:

AN-PN
1.14
1.14
1.45
1.45

AN-PVN
2.0
2.27
2.0
2.25

Since clique tree construction and initialization need
to be carried out only once for each network, we shall
not compare in detail the complexities of algorithms
in those two steps, except saying that they do not dif­
fer significantly. Computing posterior probabilities af­
ter propagation requires very little resources compared
to propagation. We shall concentrate on propagation
time.
In standard CTP, incoming messages of a clique are

combined in the propagation module after message
passing. In CTPI , on the other hand, incoming mes­
sages are not combined in the propagation module.
For fairness of comparison, the version of CTP we
implemented postpones the combination of incoming
messages to the module for computing posterior prob­
abilities.
Let us define a case to consist of a list of observed vari­
ables and their observed values. Propagation time and
memory consumption varies from case to case. In the
first three networks, the algorithms were tested using
150 randomly generated cases consisting of 5, 10, or
15 observed variables. In the fourth network, only 15
cases were used due to time constraints. Propagation
times and maximum memory consumptions across the
cases were averaged. The statistics are in Figure 3,
where the Y-axises are in logscale. All data were col­
lected using a SPARC20.

10

i

l

The CPCS networks [19] are used in the comparisons.
They are a good testbed for algorithms that exploits
ICI since all non-root nodes are convergent. The net­
works vary in the number of nodes, and the average
number of parents of a node, and the average number
of possible values of a node (variable). Their specifi­
cations are given in the following table.
Networks
Network 1
Network 2
Network 3
Network 4

·K-­

'POCTP' ·9·-­
'TTCTP' -+-­
'CTPI' -

0.1

0.01

1

1e+08

:::0

f

3

4

3

4

Networlls
"CTP' ·1+­
'PDCTP" -ra-·­
'TTCTP' -+-·­

'CTPI'-+-

1e+07

t

2

1e+06

100000

10000

1

2

Networl<s

Figure 3: Average space and time complexities of CTP,
PD&CTP, TT&CTP, and CTPI on the CPCS net­
works.
We see that CTPI is faster than all other algorithms
and it uses much less memory. In network 4, for in­
stance, CTPI is about 5 faster than CTP, 3 times faster
than TT&CTP, a.nd 3.5 times faster than PD&CTP.
On average it requires 7MB memory, while CTP re­
quires 15MB, TT&CTP requires 22MB, and PD&CTP
require 17MB.
The networks used in our experiments are quite sim­
ple in the sense that the nodes have a average number
of less than 1.5 parents. As a consequence, gains due
to exploitation of ICI and the differences among the
different ways of exploiting ICI are not very signifi­
cant. Zhang and Poole [24] have reported experiments
on more complex versions of the CPCS networks with
combinations of the VE algorithm and methods for ex­
ploiting ICI. Gains due to exploitation of ICI and the
differences among the different ways of exploiting ICI
are much larger. Unfortunately, none of the combina­
tions of CTP and methods for exploiting ICI was able
to deal with those more complex network; they all ran
out memory when initializing clique trees.
The method of exploiting ICI described in this paper
is more efficient than previous method because it di-

488

Zhang and Yan

rectly takes advantage of the fact that ICI implies con­

[9]

F., Jensen and S. K. Andersen (1990), Approxima­
tions in Bayesian belief universes for knowledge-based
systems. in Proceedings of the Sixth Conference on Un­
certainty in Artificial Intelligence, Cambridge, MA,
pp. 162-169.

[10]

F. V. Jensen, K. G. Olesen, and K. Anderson (1990),
An algebra of Bayesian belief universes for knowledge­
based systems, Networks, 20, pp. 637 - 659.

[11]

U. Kjrerulff (1994), Reduction of computational com­
plexity in Bayesian networks through removal of weak
dependences. in Proceedings of the Tenth Conference
on Uncertainty in Artificial Intelligence, pp. 374-382.

ditional probability factorization, while previous meth­
ods make use of implications of the fact.

10

CONCLUSIONS

We have proposed to method for

exploiting

ICI

in

CTP. The method has been empirically shown to

be more efficient than the combination of CTP and
the network simplification methods for exploiting ICI.
Theoretical underpinnings for the method have their
roots in Zhang and Poole

[24]

and are significantly

simplified due a deeper u nderstanding of ICI.

[12)

ACKNOWLEDGEMENT
This

paper

has

David Poole.

Kong Research
and

Sino

benefited

from

discussions

with

[13]

K. G. Olesen, U. Kjrerulff, F . Jensen, B. Falck, S.
Andreassen, and S. K. Andersen (1989), A MUNIN
network for the median nerve - a case study on loops,
Applied Artificial Intelligence 3, pp. 384-403.

[14]

K. G. Olesen and S. Andreassen (1993), Specifica­
tion of models in large expert systems based on
causal probabilistic networks, Artificial Intelligence in
Medic ine 5, pp. 269-281.

[15]

J. Pearl {1987), Evidential reasoning using stochastic
simulation of causal models, Artificial Intelligence, 32,
pp. 245-257.

[16]

J. Pearl (1988), Probabilistic Reasoning in Intelligence
Systems: Networks of Plausible Inference, Morgan
Kaufmann Publishers, Los Altos, CA .

[17]

D. Poole

Research was supported by Hong
Council under grant

HKUST658/95E
grant

Software Research Center under

SSRC95 /96.EG01.



It is well known that conditional indepen­
dence can be used to factorize a joint prob­
ability into a multiplication of conditional
probabilities. This paper proposes a con­
structive definition of intercausal indepen­
dence, which can be used to further factorize
a conditional probability. An inference algo­
rithm is developed, which makes use of both
conditional independence and intercausal in­
dependence to reduce inference complexity in
Bayesian networks.
Key words: Bayesian networks, intercausal indepen­
dence (definition, representation, inference)
1

INTRODUCTION

In one interpretation of Bayesian networks, arcs are
viewed as indication of causality; the parents of a ran­
dom variable are considered causes that jointly influ­
ence the variable (Pearl 1988). The concept intercausal
independence refers to situations where the mechanism
by which a cause influences a variable is independent of
the mechanisms by which other causes influence that
variable. The noisy OR-gate and noisy adder models
(Good 1961, Pearl 1988) are examples of intercausal
independence.
Special cases of intercausal independence such as the
OR-gate model have been utilized to reduce the
complexity of knowledge acquisition (Pearl 1988, Hen­
rion 1987) as well as the complexity of inference (Kim
and Pearl 1983). Beckerman (1993) is the first re­
searcher to try to formally define intercausal indepen­
dence. His definition is temporal in nature. Based
on this definition, a graph-theoretic representation of
intercausal independence has been proposed.

noisy

This paper attempts a constructive definition. Our
definition is based on the following intuition about in­
tercausal independence: a number of causes contribute
independently to an effect and the total contribution
is a combination of the individual contributions. The

definition allows us to represent intercausal indepen­
dence by factorization of conditional probability, in a
way similar to that conditional independence can be
represented by factorization of joint probability.
The advantages of our factorization-of-conditional­
probability representation of intercausal independence
over Beckerman's graph-theoretic representation are
twofold. Firstly, the symmetric nature of intercausal
independence is retained in our representation. Sec­
ondly and more importantly, our representation allows
one to make full use of intercausal independence to re­
duce inference complexity.
While Heckerman uses intercausal independencies to
alter the topologies of Bayesian networks, we follow
Pearl (1988) (section 4.3.2) to exploit intercausal in­
dependencies in inference. While Pearl only deals with
the case of singly connected networks, we deal with the
general case.

The rest of this paper is organized as follows. A
constructive definition of intercausal independence is
given in Section 2. Section 3 discusses factorization of
a joint probability into a multiplication of conditional
probabilities, and points out intercausal independence
allows one to further factorize conditional probabili­
ties into "even-smaller" factors. The fact that those
"even-smaller" factors might be combined by opera­
tors other than multiplication leads to the concept of
heterogeneous factorization (HF). After some technical
preparations (Sections 4 and 5), the formal definition
of HF is given in section 6. Section 7 discusses how
to sum out variables from an HF. An algorithm for
computing marginals from an HF is given in Section
8, which is illustrated through an example in Section
9. Related work is discussed in Section 10.
2

CONSTRUCTIVE INTERCAUSAL
INDEPENDENCE

This sections gives a constructive definition of inter­
causal independence. This definition is based on the
following intuition: a number of causes c1, c2, . . . , Cm
contribute independently to an effect e and the total

607

Intercausal Independence and Heterogeneous Factorization

contribution is a combination of the individual contri­
butions.
Let us begin with an example - the noisy OR-gate
model (Good 1961, Pearl 1988, Beckerman 1993). In
this model, there is a random binary variable �i in
correspondence to each c;, which is also binary. �;
depends on c; and is conditionally independent of any
other �i given the c; 's. e is 0 if and only if all the �; 's
are 0, and is 1 otherwise. In formula, e=6 V . . . V �m·
Consider the case when m=2 and consider the condi­
tional probability P(eict, c2). For any value /3; of c;
(i::::1:: , 2), we have

P(e=Oic1 =f3t, c2=fJ2)
= P(�t V�2=0ict:::::/31, c2=/12)
= P(6 :::::Oict =f31)P(6=0ic2=f32),
and

Define ft(e=a:bct=f3t)=deJP(6=a:tict=f3t) and de­
fine !2(e:::::a:2, c2=.B2)=d•J P(6=a:2Jc2=.62). We can
rewrite the above two equations in a more compact
form as follows:
P(e=a:lcl=f3t, c2=.62) =
ft(e=o:t, Ct=f3dh(e=a:2, c2=.B2),

(1)

a1va-,=a

where o:, a:11 and a:2 can be either 0 or
motivates the following definitions.

1.

This example

Let e be a discrete variable and let *• be an
commutative and associative binary operator over
the frame n. - the set of possible values of e. In the previous example, *• is the logic
OR operator V. Let f(e, x1, . .. ,xr,Yt, ... ,y,) and
g( e, Xt, . . . , Xr, Zt, ... , Zt) be two functions, where the
Yi 's are different from the Zj 's. Then, the combination
!®.g off and g is defined as follows: for any value a:
of e,
/(i)eg(e:::::a:, XI, ... , Xr, Yt, . .. , y,, Zt, ..., Zt)
= dej L [f(e=o:t,XJ1 . .. ,Xr,YI, .. . ,y,)

where ®e is the *e-induced combination operator. The
right hand of the equation makes sense because ®e
is commutative and associative. When c1, . . . , em
contribute independently to e, we call e a bastard
variable1• A non-bastard variable is said to be nor­
mal. We also say that f;(e, c;) is (a description of) the
contribution by c; to e.
Intuitively, the base combination operator (e.g. V)
determines how contributions from different sources
are combined, while the induced combination operator
is the reflection of the base operator at the level of
conditional probability.

P(e=llcFf3t, c2=P2)
P(€1 V6=1!c1 =f3t, c2=f32)
P(6=1 let=f3t)P(6=0Jcz=f32)
+P(�t =OJct =f3t)P(6=lic2=f32)
+P(6=1ict=f3t)P(6=llc:�=f3:�).

2:::

Here is our constructive definition of intercausal in­
dependence. We say that c1, ... , Cm contribute in­
dependently to e or e receives contributions indepen­
dently from Ct, ••• , Cm if there exists a commutative
and associative binary operator *• over the frame of
e and real-valued non-negative functions It (e, c1 ), ...,
f m (e, Cm) such that

X

Because of equation (1), the noisy-OR gate model is
an example of constructive intercausal independence,
with the logic OR Vas the base combination operator.
As another example, consider the noisy adder model
(Beckerman 1993). In this model, there is a random
Variable �i in correspondence to each Cj j e; depends
On Cj and is Conditionally independent of any other ej
given the c,'s. The �;'s are combined by the addition
operator "+" to result in e, i.e. e =6 + .. . +em.
To see that e is a bastard variable in this model, let
the base combination operator *e be simply "+" and
let the description of individual contribution /;(e, c;)
be as follows: for any value o: of e and any value f3 of
C; '

/; (e==o:, c;=f3)=def P(e;=aJc;=,B).
Then it is easy to verify

that

equation (3) is

satisfied.

It is interesting to notice the similarity between equa­
tion (3) and the following property of conditional in­
dependence: if a variable x is independent of another
variable z given a third variable y, then there exist
non-negative functions f(x, y) and g(y, z) such that
the joint probability P(x, y, z ) is given by
P(x,y, z)=f(x, y)g(y, z).

(4)

1Those who are familiar with clique tree propagation

We shall refer to *• as the base combination operator
and ®e as the *e-induced combination operator. We
would like to alert the reader that *e combines values
of e, while ®e combines functions of e. It is easy to
see that the induced operator ®e is also commutative
and associative.

may remember that the :first thing to do in constructing

a clique tree from a Bayesian network is to "marry" the

(

parents of each node variable)

1988).

( Lauritzen and Spiegehalter

As implies by the word "bastard", the parents of

a bastard node will not be married. This is because the
conditional probability of a bastard node is factorized into
a bunch of factors, each involving only one parent.

608

Zhang and Poole

In (4) conditional independence allows us to factorize
a joint probability into factors that involve less vari­
ables, while in (3) intercausal independence allows us
to factorize a conditional probability into a bunch of
factors that involve less variables. The only difference
lies in the way the factors are combined.
Conditional independence has been used to reduce in­
ference complexity in Bayesian networks. The rest of
this paper investigates how to use intercausal indepen­
dence for the same purpose.
3

FACTORIZATION OF JOINT
PROBABILITIES

This section discusses factorization of joint probabili­
ties and introduces the concept of heterogeneous fac­
torization (HF).
A fundamental assumption under the theory of proba­
bilistic reasoning is that a joint probability is adequate
for capturing experts' knowledge and beliefs relevant
to a reasoning-under-uncertainty task. Factorization
and Bayesian networks come into play because joint
probability is difficult, if not impossible, to directly
assess, store, and reason with.

Let P(xb x2, ..., xn) be a joint probability over variables x1, x2, ... , Xn. By the chain rule of probabilities,
we have
P(x1, X2,
, Xn)
=P(xi)P(x2lxt) ... P(xnlxl, ..., Xn-1)·
.

•

.

(5)

For any i, there might be a subset 71'; � {x1, ..., :r;_I}
such that X& is conditionally independent of all the
other variables in {x1, ... , X&-d given the variables in
11';, i.e P(x;lxb ..., x;_I)=P(x;l7r;). Equation (5) can
hence be rewritten as
n

P(x1,x2, . . . , Xn)= IT P(x&l7r;).

(6)

&=1

Equation
(6)
factorizes
the
joint proba­
bility P(z 1 , x2, ..., z,. ) into a multiplication of factors
P(xd1Ti)· While the joint probability involves all then
variables, the factors usually involves less than n vari­
ables. This fact implies savings in assessing, storing,
and reasoning with probabilities.
A Bayesian network is constructed from the factoriza­
tion as follows: construct a directed graph with nodes
x1, x2, . . . , :r,. such that there is an arc from :rj to
x; if and only if Xj E 71';, and associate the conditional
probability P(x;l7r;) with the node x;. P(x1,...,Xn) is
said to be the joint probability of the Bayesian network
so constructed. Also nodes in 11'& are called parents of
:r;.

The above factorization is homogeneous in the sense
that all the factors are combined in the same way, i.e
by multiplication.

Figure 1: A Bayesian network, where e1 and e2 re­
ceive contribution independently from their respective
parents.
Let x;1, ... , x;m, be the parents of x;. If x; is a bastard
variable with base combination operator *i, then the
conditional probability P(xd7r;) can be further factor­
ized by

where®; is the *;-induced combination operator. The
fact that ®; might be other than multiplication leads
to the concept of heterogeneous factorization (HF).
The word heterogeneous reflects the fact that differ­
ent factors might be combined in different manners.
As an example, consider the Bayesian network in Fig­
ure 1. The network indicates that P(a, b, c, e1,e2,ea, y)
can be factorized into a multiplication of P(a),
P(b), P(c), P(e1la,b), P(e2la, b, c), P ( eale1 , e2) , and
P(ylea).
Now if the e;'s are bastard variables, then there exist
base combination operators *i (i=l, 2, 3) such that
the conditional probabilities of the e; 's can be further
factorized as follows:
P(e1la, b)
P(e2la,b,c)
P(ealet, e2)

fu(el, a)®dt2(el> b)
/21(e2, a)®2!n(e2, b)®2/2a(e2, c)
!at(ea,el)®a!a2(e3,el)

where fu(et, a), for instance, denotes the contribution
by a to e1, and where the ®i's are the combination
operators respectively induced by the *i's.
The factorization of P(a, b, c, et, e2 , e3, y) into the
factors:
P(a), P(b), P(c), P(ylea), fu(el, a),
(
,
b),
e2, a), /22(e2, b), ha(e2 , c), fat(ea, et),
h1(
!t2 e1
and !a2 (e3 , e2) is called the HF in correspondence to
the Bayesian network in Figure 1. We shall call the
fii 's heterogeneous factors since they might be com­
bined by operators other than multiplication. On the
other hand, we shall say that the factors P(a), P(b),
P(c), and P(yle3) are normal.

Intercausal Independence and Heterogeneous Factorization

609

To prevent I(e1, eD from being mistaken to be the con­
tribution by ei to e1, we shall always make it explicit
that I(e1, e�) is a normal factor, not a heterogeneous
factor.

COMBININ G FACTORS THAT

5

IN V OLVE M OR E THAN ONE
BAS TARD VARIABLE
Even though deputation guarantees that every hetero­

Figure 2: The Bayesian network in Figure
deputation of bastard nodes.

4

1

after the

geneous factor involves only one bastard variable at
the beginning, inference may give rise to factors that
involve more than one bastard variable. In Figure 2,
for instance, summing out the variable a results in a
factor that involves both e1 and e2. This section in­
troduces an operator for combining such factors.
Suppose e1, ... , e�o are bastard variables with base
opera­
combination
tor *t, . . , *k· Let f(et, ...,e,.,xt, ...,xr, Yl, ..., y.)
and g ( et,... ,e�:,xt,····x.
, ,zt, .. . ,zt ) be two func­
tions, where the xi's are normal variables and the yj's

DEPUTATION OF BAS TARD

.

NODES

'

Consider the heterogeneous factor h1( es, e1) from the
previous example. It contains two bastard variables e1
to e3. As we shall see later, it is desirable for every
heterogeneous factor to contain at most one bastard
variable. The concept of deputation is introduced to
guarantee this.

are different from the zr s (they can be bastard as well
as normal variables). Then, the combination f®g of
f and g is defined as follows: for any particular value
a; of e; ,

f®g(el =a:1, .. . ,e�o=O:i:, Xt,.. ., Xr,
Yl,- . . , y,, Zt, . . . , Zt )

Let e be a bastard node in a Bayesian network. The
deputation of e is the following operation: make a copy
e' of e, make the children of e to be children of e',
'
make e a child of e, and set the conditional probability
P(e'le) as follows:

P(e'Ie ) =

{0
1

if e = e'
otherwise

(8)

We shall call e' the deputy of e. We shall also call
P(ele') the deputing function, and rewrite it as I(e, e')
since P(ele') ensures that e and e' be the same.
The Bayesian network in Figure 1 becomes the one in
Figure 2 after the deputation of aU the bastard nodes.
We shall call the latter a a deputation Bayesian net­

work.

Proposition 1 Let N' be a Bayesian network, and let
N' is the Bayesian network obtained from N' by the

deputation of all bastard nodes. Then the joint proba­
bility of N can be obtained from that of N' by summing
out all the deputy variables. 0

In Figure 1, we have the heterogeneous factors
h1(es,el) and f32(es,e2), which involves two bastard
variables. This may cause confusions and is undersir­
able for other reasons, as we shall see soon. After dep­
utation, each heterogeneous factor involves only one
bastard variable. As a matter of fact, fst(es, et) and
fs2(es,e2) have become fst(es,eD and fs2(es,e�).

[f(et =au, . . . , e�o=akl, x1,..., Xr, Yl, , Ys) X
g(e1 =0:121 , ek:::0:1:21 Xt, .. ., Xr1 Zt 1 • • • 1 Zt)]. (9)
.

•

•

.

•

•

A few notes are in order. First, fixing a list of bas­
tard variables and their base combination operators,
one can use the operator® to combined two arbitrary
functions. In the following, we shall always work im­
plicitly with a fixed list of bastard variables, and we
shall refer to ® as the general combination operator.
Second, when

(2).

k

=

1 th is definition reduces to equation

Third, since the base combination operators are com­
mutative and associative, the operator ® is also com­
mutative and associative.
Fourth, when
off and g.

5.1

k

==

0, f®g

is simply the multiplication

Combining all the Heterogeneous Factors
in a Bayesian networks

Equipped with the general combination operator ®,
we now consider combining all the heterogeneous fac­
tors of the Bayesian network in F igure 2. Because of
the third note above, we can combine them in any
order. Let us first combine fu(et, a ) with !t2(e2, b),
!21(e2, a) with h2(e2, b) and hs(e2, c), and fst(es, eD

610

Zhang and Poole

:F

In the following, we shall also say that the
of the function F(X).

We now combine the resulting conditional probabili­
ties. Because of the fourth note, the combination of
P(etia, b), P(e2 la , b, c) , and P(e3lei,e2) is their multi­
plication. So, the combination of all the heterogeneous
factors of the Bayesian network in Figure 2 is simply
the multiplication ofthe conditional probabilities of all
the bastard variables. This is true in general.

Suppose N is a deputation Bayesian network. Sup­
pose :F is the HF that corresponds to N. :F has two
interesting properties.

In a deputation Bayesian network,
the multiplication of the conditional probabilities of all
the bastard variables is the same as the result of com­
bining of all the heterogeneous factors. D

Proposition 2

6.1

is an

HF

with /32(e3, e2). Because of the second note, we have
/H®/t2(et,a,b ) = P(eda,b),
P(e2la,b, c) ,
ht®h2®!23(e2,a,b, c)
P(e31e2,eD.
ht®/32(e3,e2,eD

HF's in Correspondence to Deputation
Bayesian Networks

First, according to Proposition 2 the combination of all
the heterogeneous factors is the multiplication of the
conditional probabilities of all the bastard variables.
Thus, the joint of :F is simply the joint probability of

N.

The joint of the HF that corresponds
to a deputation Bayesian network N is the same as
the joint probability of N.

Proposition 3

Note that in Figure 1, since ht(e3,e1) and h2(e3, e2)
involve two bastard variables, the combination
fn(et,a) ® ... ® f23(e2,c) ® ht(e3,et) ® /a2(e3 ,e2)
would not the same as the multiplication of the condi­
tional probabilities of the bastard variables.

To reveal the second interesting property, let us first
define the concept of tidness. An HF is tidy if for each
bastard variable e, there exists at most one normal
factor that involves e. Moreover, this factor, if exists,
involves only one other variable in addition to e itself.

This is why we need deputation; deputation allows us
to combine the heterogeneous factors by a single com­
bination operator ®, which opens up the possibility of
combining the heterogeneous factors in any order we
choose. This flexibility turns out to be the key to the
method of utilizing intercausal independence we are
proposing in this paper.

An HF that corresponds to a deputation Bayesian net­
work is tidy. For each bastard variable e, I(e, e') is the
only one normal factor that involves e, and this factor
involves only one other variable, namely e'.

6

HETEROGENEOUS
FACTORIZATION

We now formally define the concept of heterogeneous
factorization. Let X be a set of discrete variables. A
heterogeneous factorization (HF) F over X consists of
1. A list e1, .. . , em of variables in X that are said
to be bastard variables. Associated with each bas­
tard variable ei is a base combination operator *i,
which is commutative and associative,
2. A set :Fo of heterogeneous factors, and
3. A set :F1 of normal factors.
We shall write an HF as a quadruplet :F =(X,
{(e1, *t ) , . . . , (em, *m ) } , :Fo, Ft). Variables that are
not bastard are called normal.
In an HF, the combination F0 of all the heterogeneous
factors is given by
(10)
The joint F(X) of an HF is the multiplication of Fa
and all the normal factors. In formula
F=deJ(®Je:F0f)

IT
g€1"1

g.

(11)

Tidy HF's do not have to be in correspondence to a
deputation Bayesian network. As a matter of fact, we
shall start with a tidy HF that corresponds to a dep­
utation Bayesian network, and then sum out variables
from the HF. We shall sum out variables in such a
way such that the tidness is retained. Even though
the HF we start out with corresponds to a deputation
Bayesian network, after summing out some variables,
the resulting tidy HF might no longer correspond to
any deputation Bayesian network.
However, we shall continue to use the terms deputy
variable and deputing function.
7

SUMM ING OUT VARIABLES
FROM TIDY HF'S

Let F(X) be a function. Suppose A is a subset of X.
The projection F(A) of F(X) onto A is obtained from
F(X) by summing out all the variables in X -A. In
formula
(12)
F(A)=d•J I: F(X).
X-A

When F(X)
probability.

is

a joint probability, F(A) is a marginal

Summing variables out directly from F(X) usually re­
quire too many additions. Suppose X contains n vari­
ables and suppose all variables are binary. One needs
to perform 2n - 1 additions to sum out one variable.

Intercausal Independence and Heterogeneous Factorization

A better idea is to sum out variables from an factoriza­
tion of F(X) if there is one. This section investigates
how to sum out variables from tidy HF's. The follow­
ing two lemmas are of fundamental importance, and
they readily follow the definition of the general com­
bination operator @.
Both m'llltiplication and @ are distributive
w. r. t summation.
More specifically, s'llppose f and g
are two functions and variable x appears in f and not
in g . Then

Lemma 1

1.

'Er(fg) = ('Er f)g, and

summing out z does not affect the deputing functions.
Therefore, :F' remains tidy.
When z is a bastard variable, summing out z will not
affect the deputing functions of any other bastard vari­
ables. Therefore, :F' also remains tidy. 0
general, a variable can appear in more than one nor­
mal and heterogeneous factors. The next proposition
reduces the general case to the case where the variable
appear in at most two factors, one normal and one
heterogeneous.

In

F(X), and let z
be a variable in X. Let It , ... , fm be all the heteroge­
neous factors that involve z and let 91, . . , Un be all
the normal factors that involve z. Define

Proposition 5 Let :F be an HF of

2. E., (I®g)= CEr f)® g.

.

0

f=aej 0?;1 /; ,

The following lemma spells out two conditions under
which multiplication and ® are associative with each
other.
Lemma 2
1.

h{f®g}={hf} ®g.

n

g=aeJ IT Ui·
j=l

Let f and g be two functions.

If h is a function that involves no bastard vari­
ables, then

(13)

Let :F' be the HF obtained from :F by removing the fi 's
and the Ui 's, and by adding a new heterogeneous factor
f and a new normal factor g. Then
1. :F' is also an HF of F(X), and f and

g are the
only two factors that involve z. In particular,
when either m=O or n=O, there is only one factor
in :F' that involves z.

2. If h is a function such that all the bastard variables
in

h

f and not in g,
h{f®g } ={hf}®g.

appear only in

then

(14)

0

We now proceed to consider the problem of summing

out variables from a tidy HF in such a way that the tid­
ness is retained. First of all the following proposition
deals with the case when the variable to be summed
out appears in only one factor.
4 Let :F be an HF of F(X) and is tidy.
Suppose z is a variable that appears only in one factor
!(A), normal or heterogeneous. Define h

Proposition

h(A - {z} )=d�J Lf(A).
z

Let :F' be the HF obtained from :F by replacing

f with

h 2• Then, :F' is a HF of F(X- { z } )
the projection
of F(X) onto X-{z} . Moreover if z is not a dep'llty
-

variable, then :F' remains tidy.

Proof:

611

The first part of proposition follows from

Lemma 1.

For the second part, since z is not a deputy variable, it
can be either a non-deputy normal variable or a bas­
tard variable. When z is a non-deputy normal variable,
2The factor h is heterogeneous or normal if and only if
f is.

2.

If z

is not a dep'llty variable, then when :F is tidy,
so is :F'.

Proof: The first part of the proposition follows from

the commutativity and associativity of multiplication
and of the general combination operator @.
For the second part, since z is not a deputy variable, it
can either be a non-deputy normal variable or a bas­
tard variable. When z is a non-deputy normal vari­
ables, the operations performed by the proposition do
not affect the deputing functions. Thus, :F' remains
tidy.
When z is a bastard variable, the deputing functions
are not affect either. Because for each bastard variable
e, its deputing functions is the only normal factor that
involves e. So, :F' also remains tidy. D.
The following proposition merges a normal factor into
a heterogeneous factor.
Proposition 6 Let :F be an HF of F(X) and is tidy.

Suppose z is a variable that appears in only one normal
factor g and only one heterogeneous factor f. Define
h by

h=aeJfg.
Let :F' be the HF obtained from :F by removing g and
f, and by adding a heterogeneous judor h. If z is not

612

Zhang and Poole

a deputy variable, then the joint of :F' is also F(X)
and :F' is tidy. Moreover, h is only one factor in :F'

that involves

z.

Proof: We first consider the case when z is a non­
deputy normal variable. Because the tidness of :F, g
involves no bastard variables. According to Lemmas 2

(1), the joint ofF' is also F.

Since g is not a deputing function, the operation of
combining f and g into one factor does not affect the
deputing functions. Hence, :F' remains tidy.
Let us now consider the case when z is a bastard vari­
able. Since :F is tidy, g must be the deputing function
of z. Since f is the only heterogeneous factor that in­
volves z, all other heterogeneous factors do not involve
z. According Lemma 2 (2), the joint of :F' is also F.
After combining f and g into a heterogeneous factor,
there is no normal factor that involve z. Also, the
deputing functions of the other bastard variables are
not affected. Hence, :F' remains tidy. D.
The

in Bayesian networks. To this end, we need only con­

sider deputing functions I(e, e') such that I(e, e') = 1
if e = e' and I(e, e') = 0 otherwise. Let us say such
deputing functions are identifying. Since for any func­
tion f(e, e', x1, ... , xn ) ,

LI(e, e')f(e, e',

Procedure PROJECTION ( :F, A,
•

8

AN ALGORITHM

This section presents an algorithm for computing pro­
jections of a function F(X) by summing variables from
a tidy HF of F(X). Because of Proposition 3, the al­
gorithm can be used to compute marginal probabili­
ties, and hence posterior probabilities, in Bayesian net­
works.
To sum out the variables in X- A, an ordering needs
to be specified (Lauritzen and Spiegehalter 1988). In
the literature, such an ordering is called an elimi­
nation ordering, which can be found by heuristics
such as the maximum cardinality search (Tarjan and
Yannakakis 1984) or the maximal intersection search
(Zhang 1993).
At the end of the last section, we said that a deputy
variable should be summed out only after the corre­
sponding bastard variable has been summed out. If e
is a bastard variable in A, what should we do with its
deputy variable e'?
The paper is concerned with intercausal independence

p)

Input:
1. :F - A tidy HF of a certain func­
tion F(X) such that all the deputing
functions are identifying,

a tidy HF, bastard variables and non-deputy normal
variables. You may ask: how about deputy variables?
As it turns out, after summing out a bastard variable
e, its deputy e1 becomes a non-deputy normal variable.
So, we can also sum out deputy variables; we just have
to make sure to sum out a deputy variable after the
corresponding bastard variable has been summed out.
variable e' needs to be summed out after the corre­
sponding bastard variable e. As a matter of fact, sum­
ming out e ' before e is the inverse of the deputation of
e. But we have shown at the end the Section 5 that
deputation is necessary.

) = f(e, e, x 1 , .. . , Xn),

we can handle the deputies of bastard variables in A as
follows: wait till after all the other variables outside
A have been summed out and all the heterogeneous
factors have been combined, then simply remove all
the deputing functions, replace each occurrence of a
deputy variable with the corresponding bastard vari­
able. This operation can be viewed as the inverse of
deputation.

above three propositions allow us to sum out, from

It is possible to intuitively understand why a deputy

Xt, ... , Xn

e'

2. A- A subset of X,

3. p - An elimination ordering consist­
ing all the variables other than the
variables A and their deputies. In
p, a deputy variable e� comes right
after the corresponding bastard vari­
able e;.

•

Output:
onto A.

F(A) - The projection of F

1. If p is empty, combine all the het­
erogeneous factors by using the gen­
eral combination operator ®, resulting
in f; remove all the deputing functions
and replace each occurrence of a deputy
variable with the corresponding bastard
variable; multiply f together with all the
normal factors; output the resulting fac­
tion; and exit.
2. Remove the first variable

z

from the or­

dering p.

3. Remove from :F all the heterogeneous
factors ft, .. . , fl� that involve z, and
set

f=dej

®f=l k

Let B be the set of all the variables that
appear in f.
4. Remove from :F, all the normal factors
91 , . .., Om that involve z, and set

m

D= deJ

IT 9j·

j= l

Let C be the set of all the variables that
appear in g.

Intercausal Independence and Heterogeneous Factorization

5. If k=O, define a function

The bastard variable e3 appears in heterogeneous fac­
tors /31 (e3ei) and /a2(e 3, e�), and in the normal factor
I3(e3,e �). After summing out eg the factors become:

h by

h(C-{z})=def Lg(C),
Add h into F

as

613

{T,bt(eL e�,e�), fu(el,a),
Fo
!21(e2,a), /22(e2, b), ha(e2,c)};
F1={P(a), P(b), P(c), P( yie�),
h(e2, e�)},

•

a normal factor,

6. Else if m=O, define a function h by

•

h(B-{z})=deJ L f(B),

/t 2(e1, b),
l1(et,ei),

where
Add h into F as a heterogeneous factor,

7. Else define a function h by

h(BUC-{z} )=de/ L f(B)g(C),
Add h into F as a heterogeneous factor.
Endif
8. Recursively call PROJECTION(F,

A, p)

The correctness of PROJECTION is guaranteed by
Propositions 4, 5, and 6.

summing out a variable re­
quires combinin g only the factors that involve the vari­
able. This is why PROJECTION allows one to ex­

Note that in the algorithm

ploit intercausal independencies for efficiency gains. If
one ignores intercausal independencies, to sum out one
variable one needs combine all the conditional proba­
bilities that involve the variable. There is a gain in effi­
ciency by using PROJECTION because intercausal in­
dependence allows one to further factorize conditional
probabilities into factors that involve less variables. In
Figure 1, for instance, summing out a requires com­
bining P(e tla, b) and P(e2la, b, c) when intercausal in­
dependencies are ignored; there are five variables in­
volved here. By using PROJECTION, one needs to
combine f11 (e 1 , a) and !21(e2,a); there are only three
variables involved in the case.
Finally, we would like to remark that the algorithm
is an extension to a simple algorithm for computing
marginal probabilities from a homogeneous factoriza­
tion (Zhang and Poole 1994).

tPt(e�, e�,e3)=def L(/31 (ea,eD®faz(e3, e� ))I3(e3, e3).
"�

Now e� is the next to sum out. e� appears in the
heterogeneous factor t/J1 and the normal factor P(yle�),
After summing out e�, the factors become:
•

•

where

?f>2(e�, e�, y)=def L if>1 (e� ,e�,e3)P(yle3).
e;

Next, summing out
•

•

Suppose the elimination ordering pis:

e�, c.
•

•

e3, e�, a, b, e1,

Initially, the factors are as follows:

:Fo = {/u(et,a), /t2(e1,b), ht(e2, a ) , /22(e2,b)
h3(e2,c), h1(e3,eD, h2(e3,e�)};
:F1 = {P(a), P(b), P(c), P(yle�), h(el,eD,
l2(e2,e�), l3(ea, e�)}.

tfi2(e�,e�,y),
Fa={ tP3(e1 ,e2),
h2(e2, b), /23(e2,c)};

!t2(e1,b),

F1={P(b), P(c), I1(e11 ei), I2(e2, e�)},

a

Then, summing out

•

work N shown in Figure 2. Since P(e2ly=O) can
be readily obtained from the marginal probability
P(e2,y), we shall show how PROJECTION computes
the latter.

gives us:

a

•

To illustrate PROJECTION, consider computing the
conditional probability P(e2ly=O ) in the Bayesian net­

a

where

An example

9

:Fo=N2(eL e�,y), /u(et,a), !t2(e1,b), !21(e2,a),
!22(e2,b), h3(e2, c)};
:F1= {P(a), P(b), P(c), It(et, ei), I2(e2,e�)},

b gives

us:

Fo={ 'tj.>4(e1, e2),
!23(e2,c)};
Ft= {P(c), lt(et,eD, I2(e2,e�)},

where

if>4(et,e2)

=def
=

L P(b)[/t2(et,b)®f22(e2,b)]
L P(b)!t2(e1,b)h2(e2, b).
b

The next variable on p is e1, which appears in hetero­
geneous factors 'tj.>3 (e1,e2) and 'tj.>4 (e 1, e2) and normal
factor h (e1, ei). After summing out e1 the factors be­
come:

614

Zhang and Poole

together with conditional independencies, to further
reduce inference complexity.

where

t/Js(e2, e�)=deJ

2: It (et, eD[t/Ja(e1, e2)®t/J4(e1, e2)].
"1

Due to space limit, we have to discontinue the example
here. Hopefully, the following two points shoul be be
clear now. F irst, in summig out one variable, PRO­
JECTION combines only the factors that involve the
variable.
Second, since

not have

e1

is a bastard variable, we usually do

Acknowledgement
The authors are grateful for the three anonymous re­
viewers for their valuable comments and suggestions.
Research is supported by NSERC Grant OGP0044121
and by a travel grant from Hong Kong University of
Science and Technology.




While influence diagrams have many ad­
vantages as a representation framework for
Bayesian decision problems, they have a se­
rious drawback in handling asymmetric de­
cision problems. To be represented in an
influence diagram, an asymmetric decision
problem must be symmetrized. A consid­
erable amount of unnecessary computation
may be involved when a symmetrized influ­
ence diagram is evaluated by conventional al­
gorithms. In this paper we present an ap­
proach for avoiding such unnecessary compu­
tation in influence diagram evaluation.

1

INTRODUCTION

Decision trees were used as a simple tool both for prob­
lem modeling and optimal policy computation in the
early days of decision analysis (Rai'ffa 1968). A deci­
sion tree explicitly depicts all scenarios of the problem
and specifies the "utility" the agent can get in each sce­
nario. An optimal policy for a decision problem can
be computed from the decision tree representation of
the problem by a simple "average-out-and-fold-back"
method.
Though conceptually simple, decision trees have
First, the depen­
a number of drawbacks.
dency /independency relationships among the variables
in a decision problem cannot be represented in a deci­
sion tree. Second, a decision tree specifies a particular
order for the assessment on the probability distribu­
tions of the random variables in the decision problem.
This order is in most cases not a natural assessment
order. Third, the size of a decision tree for a decision
problem is exponential in the number of variables of
the decision problem. Finally, a decision tree is not
easily adaptable to changes in a decision problem. If
a slight change is made in a problem, one may have to
draw a decision tree anew.
*Scholar of Canadian Institute for Advanced Research

David Poole*

Department of Computer Science
UBC
Vancouver B. C. Canada V6T 1Z4
E-mail: qi@cs.ubc.ca

Influence diagrams were proposed as an alternative to
decision trees for decision analysis (Howard and Math­
eson, 1984, Miller et. al. 1976). As a representation
framework, influence diagrams do not have the afore­
mentioned drawbacks of decision trees. The influence
diagram representation is expressive enough to explic­
itly describe the dependency /independency relation­
ships among the variables in the decision problem; it
allows a more natural assessment order on the proba­
bilities of the uncertain variables; it is compact; and it
is easy to adapt to the changes in the problem.
However, in comparison with decision trees, influence
diagrams have one disadvantage in representing asym­
metric decision problems (Covaliu and Oliver 1992,
Fung and Shachter 1990, Phillips 1990, Shachter 1986,
Smith et al. 1993). Decision problems are usually
asymmetric in the sense that the set of possible out­
comes of a random variable may vary depending on
different conditioning states, and the set of legitimate
alternatives of a decision variable may vary depending
on different information states. To be represented as
an influence diagram, an asymmetric decision problem
must be "symmetrized" by adding artificial states and
assuming degenerate probability distributions (Smith
et al. 1993). This symmetrization results in two prob­
lems. First, the number of information states of de­
cision variables are increased. Among the informa­
tion states of a decision variable, many are "impos­
sible" (having zero probability). The optimal choices
for these states need not be computed at all. How­
ever, they are computed by conventional influence di­
agram evaluation algorithms (Shachter 1986, Smith et
a/. 1993, Shachter and Peot 1992, Zhang and Poole
1992, Zhang et al. 1993). Second, for each informa­
tion state of a decision variable, because the legitimate
alternatives may constitute only a subset of the frame
of the decision variable, an optimal choice is chosen
from only a subset of the frame, instead of the en­
tire frame. However, conventional influence diagram
algorithms have to consider all alternative in order to
compute an optimal choice for a decision in any of its
information states. Thus, it is evident that conven­
tional influence diagram evaluation algorithms involve
unnecessary computation.

492

Qi, Zhang, and Poole

In this paper, we present an approach for overcom­
ing the aforementioned disadvantage of influence di­
agrams. Our approach consists of two independent
components: a simple extension to influence diagrams
and a top-down method for influence diagram evalu­
ation. Our extension allows explicitly expressing the
fact that some decision variables have different frames
in different information states. Our method, similar to
Howard and Matheson's (1984), evaluates an influence
diagram in two conceptual steps: it first maps an in­
fluence diagram into a decision tree (Qi 1994) in such
a way th at an optimal solution tree of the decision
tree corresponds to an optimal policy of the influence
diagram. Thus the problem of computing an optimal
policy is reduced to the problem of searching for an
optimal solution tree of a decision tree, which c an be
accomplished by various algorithms (Qi 1994). Like
Howard and Matheson's method, ours avoids comput­
ing optimal choices for decision variables in imp ossible
states. Furthermore, our method has two advantages
over Howard and Matheson's. First, the size of the
intermediate decision tree generated by our method
is much smaller than that generated by Howard and
Matheson's for the same influence diagram. Second,
our method provides a clean interface between in­
fluence diagram evaluation and Bayesian net evalu­
ation so that various well-established algorithms for
Bayesian net evaluation can be used in influence di­
agram evaluation. This method works for influence
diagrams with or without our extension.
The rest of th is paper is organized as follows. The
next section introduces influence diagrams. Section
3 uses an example to illustrate the disadvantage that
influence diagrams and their solution algorithms have
with asymmetric decision problems. In Section 4, we
present our approach for overcoming the disadvantage.
Section 5 gives an analysis on how much can be saved
by exploiting asymmetry in decision problems. Section
6 discusses related work and Section 7 concludes the
paper.
2

INFLUENCE DIAGRAMS

The following definition for influence diagrams is bor­
rowed from (Zhang et a/. 1993). An influence diagram
I is defined as a quadruple I= (X, A, P,U) where
•

(X, A) is a directed acyclic graph with node set
X and arc set A. The node set X is partitioned
into random node set C, decision node set D and
value node set U . All the nodes in U have no
child.
Each decision node or random node has a set,
called the frame, asso ciated with it. The frame
of a node consists of all the possible outcomes of
the (decision or random) variable denoted by the
node. For any node x E X , we use 1r( x) to de­
note the parent set of node x in the graph and use
!.lx to denote the frame of node x . For any subset

J � C U D , we use !.lJ to denote the Cartesian
product Ilxo !.lx.
•

•

Pis a set of probability d istributions P{cl1r(c)}
for all c E C. For each o E !.lc and s E !.1.-(c), the
distribution specifies the conditional probability
of event c = o, given that1 1r(c) = s.

U is a set {gv : !.1.-(v)

---> Rlv E U} of value func­
for the value nodes, w here R is the set of
the real.

tions

For a decision node d;, a m apping 8; : !.1.-d, -+ !.ld,
is called a decision function for d;. The set of all the
decision functions for d;, denoted by ��, is called the
decision function space for d;. Let D
{d1, ..., dn} be
the set of decisi on nodes in influence diagram I. The
Cartesian product � TI�=l �� is called the policy
space of I.
=

=

For a decision node d;, a value x E !.1.-cd,) is called an
information state of d;, and a mapping 8; : n,..(d;) -+
!.ld, is called a decision function for d;. The set of all
the decision functions for d;, denoted by �;, is called
the decision function space for d;. The Cartesian prod­
uct of the decision function spaces for all the decision
nodes is called the policy space of I. We denote it by
�Given a p olicy {) = (81, ... ,8k) E �for I, a probabil­
P0 can be defined over the random nodes and the
decision nodes as follows:

ity

k

P0(C, D)=

II P(cl1r(c)) II P0,(d;l1r(d;)),

cec

(1)

where P(cl1r(c)) is given in the specification of the in­
fluence diagram, while P0,(d;l7r(d;)) is given by b; as
follows:

Pa,(d;l7r(d;)) =

{6

when b;(7r(d;))
otherwise

=

d;,

(2)

For any value node v, 1r(v) must consist of only deci­
sion and random nodes, since value nodes do not have
children. Hence, we can talk about Pc(?r(v)). The
expectation of the value node v under Pa, denoted by
Eo[v], is defined as follows:

E,�[v]

=

2: P0(7r(v))fv(7r(v)).
.-(v)

The summation Eo = I:veu E0[v] is called the value of
I under the policy 8. The maximum of Ea over all the
possible policies 8 is the optimal expected value of I.
An optimal policy is a policy that achieves the optimal
expected value. To evaluat e an influence diagram is to
1 In
e

this paper, for any variable set J and any element

E OJ, we use J

= e

to denote the set of assignments

that assign an element of

in J.

e

to the corresponding variable

Solving Asymmetric Decision Problems with Influence Diagrams

determine its optimal expected value and to find an
optimal policy.
An influence is regular if there e xis ts a total ordering
among all the decision nodes. The results presented in
this paper are applicable to regular stepwise decompos­
able influence diagrams (Q i 1993, Qi and Pool e 1993,
Zhang and Poole 1992). We shall , however, limit the
exposition only to regular influence diagrams with a
single value node for simplicity.

3

WHY INFLUENCE DIAGRAMS
ARE NOT GOOD FOR
ASYMMETRIC DECISION
PROBLEMS

In this section, we illustrate by an example the dis­
advantages of conventional influence diagrams with
asymmetric decision problems. We use the used car
buyer problem (Howard 1962) because it is a typical
asy m metric decision problem and it has bee n used by
other researchers (Shenoy 1993, S mi t h et al. 1993).

3.1

THE USED CAR BUYER PROBLEM

Joe is considering to buy a used car. T h e marked
price is $1000, while a three years old c ar of this model

worths $1100, if it has no defect.

Joe is uncertain

whether the car is a "peach" or a "lemon". But Joe
knows that, of the ten major subsystems in the car,
a peach has a defect in only one subsystem whereas a
lemon has a defect in six subsystems. Joe also knows
that the probability for the used car being a peach is
0.8 and the probability for the car being a lemon is 0.2.
Finally, Joe knows that it will cost him $40 to repair
one defect and $200 to repair six defects.

3.2

493

INFLUENCE DIAGRAM
REPRESENTATION FOR THE USED
CAR BUYER PROBLEM

An influence diagram for the used car problem is
shown in Fig. 1. The random variable CC represents
the car's condition. The frame for CC has two elements:
peach and lemon. The variable h as no parent in the
graph, thus, we specify its prior probability distribu­
tion in Table 1.
The decision variable T1 represents the first test de­
CISion.
The frame for T1 has four elements: nt,
st, f&:e and tr, repr esent in g respectively the options
of performing no test, testing the steering subsystem
alone, testing the fuel and electrical subsystems, and
testing the transmission subsystem with a possibility
of testing the differential subsystem next.

The ran dom variable R1 represents the first test re­
sults. The frame for R1 has four elements: nr, zero,
one and two representing respectively the four possi­
ble outcomes of the first test: no result, no defect, one
defect and two defects. The probability distribution
of the variables, conditioned on T1 and CC, is given in
Table 2.
The de c isi on variable T2 represents the second test de­
ctswn.
Th e frame for T2 has two elem e nt s : nt and
diff, de n ot i ng the two options of performing no test
and testing the differential subsystem.
The random variable R2 represents the second test re­
sults. The frame for the random variable R2 has three
el ement s : nr, zero and one, representing respectively
the three possible outcomes of the second test: no re­
sult, no defect and one defect. The probability distri­
bution of the vari ables conditioned on T1, R1, T2 and
cc, is gi ven in Table 3.
,

The decision variable B represents the purchase deci­
sion. The frame for B h as three elements: b, b and
g, d enoting resp ectiv ely the options of not buying the
car, bu yi ng the car without the anti-lemon guarantee
and buying the car with the anti-lemon guarantee.

Observ ing Joe's concern about the pos s i bil i ty that the
car may be a lemon, the dealer offers an "anti-lemon
guarantee" option. For an additional $60, the anti­
lemon guarantee will cover the full repair cost if the
car is a lemon, and cove r half of the repair cost oth­
erwise. At the same time, a mechanic suggests that
some

mech�

examination should h€4p .J.ee..-detef­

mine the car's condition. In particular, th e mechanic
gives Joe three alternatives: test the steering subsys­
tem alone at a cost of $9 ; test the fuel and electrical
subsystems at a total cost of $13; a two-test sequence
in which, the transmission subsystem will be tested at
a cost of $10, and after knowing the test result, Joe
can decide whether to test the differential subsystem
at an additional cost of $4. All tests a re guaranteed to
detect a defect if one exists in the subsystem(s) being
tested.

Figure 1: An influence diagram for the used car buyer
problem
The used car buyer problem is asymmetric in a num­
b er of aspects. F irst, the set of the possible outcomes
of the first test result varies, depending on the choice
for the first test. If the choice for the first test is nt,

494

Qi, Zhang, and Poole

Table 1: The prior probability distribution of the car's
condition P{cc}

Table 2: The probability distribution of the first test
result P{RtiT1, cc}
T1

nt

nt
st
st

cc
-

-

-

Rl

pro b

nr

1.0

others

0

nr

0

two

0

zero

0.9

st

pe ach

st

peach

one

st

lemon

zero

0.4

st

lemon

one

0.6

f&e

0.1

0

nr

zero

0.8

f&e

peach

f&e

peach

one

f&e

peach

two

f&e

lemon

zero

0.13

f&e

lemon

one

0.53

f&e

lemon

two

0.33

0.

2

0

then there is only one possible outcome for the first
test result - nr (representing no result). If the choice
for the first test is st or tr, then there are two possible
outcomes for the first test result- zero and one (rep­
resenting no defect and one defect, respectively). If
the choice for the first test is fl:e, then there are three
possible outcomes for the first test result -zero, one
and two (representing no defect, one defect and two
defects, respectively). However, in the influence dia­
gram representation, the frame of the variable R1 is a
common set of outcomes for all the three cases. The
impossible combinations of the test choices and the
test results are characterized by assigning zero prob­
ability to them (as shown in Table 2). A similar dis­
cussion is applicable to the variable R2. Second, from
the problem statement we know that testing differen­
tial subsystem is possible only in the states where the
first test performed is on the transmission subsystem.
However, in the influence diagram representation, it
appears that the second test is possible in any situa­
tion, while the fact that the option of testing differ­
ential subsystem is not available in some situations is
characterized by assigning unit probability to outcome
nr of the variable R2 conditioned on these situations.
Third, when we examine the information states of the
decision variable T2, we will see many combinations of
test options and test results are impossible. For ex­
ample, if Joe first tests the transmission subsystem, it
is impossible to observe nr and two. If the influence
diagram is evaluated by conventional algorithms, an
optimal choice for the second test will be computed
for each of the information states, including many im­
possible states. Similar argument is applicable to the
decision variable B. Because it is not necessary to com­
pute optimal choices of a decision variables for impos­
sible states, it is desirable to avoid the computation.
4

Table 3: The probability distribution of the second
test result P{R2IT1, R1, T2, cc}
Tl

nt

nt
st
st
f&e
f&en

Rl

T2

-

-

-

tr

nr

tr

nr

tr

two

tr

two
-

tr
tr
tr

-

z ero

tr

zero
zero

tr

zero

tr

one

tr

tr

on e

tr

one

tr

one

-

cc
-

R2

nr

prob
1.0

others

0

nr

1.0

others

0

nr

1.0

others

0

nr

1.0

-

-

others

0

-

-

nr

1.0

others

0

nr

1.0

-

nt
nt

cliff
diff
diff
cliff
diff
cliff
cliff
diff

-

-

-

others

0

zero

0.89

zero

0.67

lemon

one

0.33

peach

zero

1.0

peach

one

peach
peach
lemon

one

0.11

0

lemon

ze ro

0.44

lemon

one

0.56

OUR SOLUTION

In this section, we present an approach for overcom­
ing the aforementioned disadvantage of influence di­
agrams. Our approach consists of two independent
components: a simple extension to influence diagrams
and a top-down method for influence diagram evalua­
tion.
Our extension allows explicitly expressing the fact that
some decision variables have different frames in dif­
ferent information states. We achieve this by intro­
ducing a framing function for each decision variable,
which characterizes the available alternatives for the
decision variable in different information states. With
the help of framing functions, our solution algorithm
effectively ignores the unavailable alternatives when
computing an optimal choice for a decision variable
in any information state. Our extension is inspired
by the concepts of indicator valuations and effective
frames proposed by Shenoy (1993).
Conceptually, our evaluation method, similar to
Howard and Matheson's method (Howard and Math-

Solving Asymmetric Decision Problems with Influence Diagrams

eson 1984), consists of two steps: in order to evaluate
an influence diagram, a decision tree is generated and
the evaluation is then carried out on the decision tree.
The first step will be described in this section. The
second step can be carried out either by the simple
"average-out-and-fold-back" method (Raiffa 1968),
or by a top-down search algorithm (Qi 1994). An ad­
vantage of using a search algorithm is that the two
steps of tree generation and optimal policy computa­
tion can be combined into one, and only a portion of
the tree needs to be generated, due to heuristic search.
Our method successfully avoids the unnecessary com­
putations by pruning those impossible states and ig­
noring those unavailable alternatives for the decision
variables.
In comparison with than Howard and Matheson's
method, ours has two distinct advantages. F irst, for
the same influence diagram, our method generates a
much smaller decision tree. Second, our method pro­
vides a clean interface to utilizing effi cient Bayesian net
algorithms (Lauritzen and Spiegelhalter 1988, Pearl
1988).

4.1

4.2

generated by our method for an
choice node corresponds to an in­
formation state of a decision variable, and a chance
node corresponds to an uncertain state resulting from
choosing an alternative for a decision variable in an
information state. Two states are consistent if the
variables common to both states have the same out­
comes.
The

In the used car problem, the framing functions for the
first test decision and the purchase decision are simple
-they map every information state to the correspond­
ing full frames.
The frame function for the second test decision can
follows:

specified as

/T.,(X)

=

{ {nt
diff}
{nt}

if ur, (X)

otherwise.

=

tr

be

root, a chance node representing the
is in the decision tree.

Initially, the
empty state,

•

For each information state S of the first decision
variable d1 , there is a choice node, as a child of
the root in the decision tree. The arc from the
root to the node is labeled with the probability
P {1r(d1 ) = S} . A choice node in the decision tree
is pruned if the probability on the arc to it is zero.

•

Let N be a choice node not pruned in the decision
tree, and SN be the inforn1ation state associated
with N. Assume that SN is for decision variable
d. Then, N has lfd(SN )I children, each corre­
sponding to an alternative in !d (SN) . These chil­
dren are all leaf nodes if d is the last decision vari­
able. Otherwise, they are chance nodes. The node
corresponding to alternative a E fd(SN) repre­
sents the state 1r(d) = SN, d =a.

•

Let N

{fd

Similarly, we define a decision function for a decision
node d; as a mapping 8; : Orr( a,) _, nd,. In a ddi t ion al ,
6; must satisfy the following constraint: For each s E
n,.(d;), 6;(s) E fa;(s) . In words, the choice prescribed
by a decision function for a decision variable d in an
information state must be a legitima te alternative.

decision tree is recursively specified as follows:

•

We extend

The framing functions express the fact that the legiti­
mate alternative set for a decision variable may vary in
different information states. More specifically, for a de­
cision variable d and an information state s E O,.(d) ,
/d(s) is the set of the legitimate alternatives the de­
cision maker can choose for d in information state s .
Following Shenoy (1993), we call fd(s) the effective
frame of decision variable d in informa tion state s.

CONSTRUCTING DECISION TREES
FROM INFLUENCE DIAGRAMS

In the decision tree
influence diagram, a

EXTENDING INFLUENCE
DIAGRAMS

influence diagrams by introducing fram­
ing functions to the definition given in Section 2.
With this extension, an influence diagram I is a tuple
I= (X,A,P,U,:F) where X,A,P,U have the same
meaning as before, and F is a set
: Orr(d} __. 2°"}
of framing functions for the decision nodes.

495

be a chance node representing a state
1r(d;-1) = SN,di-1 = a, and let A be the
subset of the information states of decision vari­
able di which are consistent with 7r(d;_1) =
SN,di-1 = a. Node N has IAI children, each
being a choice node representing an information
state in A . Let S be the information state rep­
resented by a child of N . The arc from N to the
child is labeled with the conditional probability
P{1r(d;) = S!1r(d;_t) = SN,di-l = a } .

In the above specification, we effectively prune all of
the impossible information states for all decision vari­
ables and ignore the unavailable alternatives to deci­
sion variables.
We have not specified how to compute the probabili­
ties on the arcs from chance nodes nor how to compute
the values associated with the leaf nodes. As illus­
trated in (Qi and Poole 1993), various well established
Bayesian Net algorithms can be employed for comput­
ing the probabilities , and computing the values as­
sociated with the leaf nodes, which normally involve
only small portions of the influence diagram. In par­
ticular , in order to further exploit asymmetry, Smith's
method (Smith et al. 1993) can also be used for com­
puting those probabilities.

496

5

Qi, Zhang, and Poole

HOW WELL OUR ALGORITHM
DOES FOR THE USED CAR
BUYER PROBLEM

When applying our algorithm to the used car buyer
problem, a decision tree shown in Fig. 2 is generated.
In the graph, the leftmost box represents the only sit­
uation in which the first test decision is to be made.
The boxes in the middle column correspond to the in­
formation states i n which the second test decision is
to be made. Similarly, the boxes in the right column
correspond to the information states in which the pur­
chase decision is to be made. From the figure we see
that among those nodes corresponding to the infor­
mation states of the second test, all but two have only
one child because the effective frames of the second
test in the corresponding information states have only
a single element. Making use of the f ramin g function
this way is equivalent to six prunings , each cutting
a subtree under a node corresponding to an informa­
tion state of the second test. Those shadowed boxes
correspond to the impossible states. Our algorithm
effectively detect s those impossible states and prune
them when they are created. Each of such pruning
amounts to cutting a subtree under the cor responding
node. Consequently, our algorithm does not compute
optimal choices for a decision node for those impossi ble
states. For the used car buyer pro blem , our algorithm
computes optimal choices for the purchase deci sion for
only 12 information states, and opti mal choices for the
second test for only 8 information states (among which
six can be computed trivially ) . These constitute the
minimal information state set one has to consider in
order to compute an optimal policy for the used car
buyer problem. This suggests that, as far as decision
making concerned, our method exploits asymmetry to
the maximum extent. In contras t , whereas those al­
gorithms that do not exploit asymmetry will compute
the optimal choices for the pu rch ase decision for 96
(4 x 4 x 2 x 3) information sta tes and wi ll compute
optimal choices for the second test for 16 information
states.
6

RELATED WORK ON
HANDLING ASYMMETRIC
DECISION PROBLEMS

Recognizing that influence diagrams are not effec­
tive asymmetric decision problems, several researchers
have recently proposed alternative r epresent ations.
Fung and Shachter (1990) propose contingent influence
diagrams for explicitly expressing asymmetry of deci­
sion problems. In that representation, each variable is
associated with a set of contingencies, and associated
with one relation for each contingence. These relations
collectively specify the condi t ional distribution of the
variable.
Covaliu and Oliver (1992) p rop ose a different

represen-

Figure 2: A decision tree generated for
buyer problem

the used

car

tation for representing decision problems. This repre­
sentati on uses a decision diagram and a formulation
table to specify a decision problem . A decision dia­

gram is a directed acyclic graph whose directed paths
identify all possible sequences of decisions and events
in a decision problem. In a sense, a decision diagram
is a degenerate decision tree in which paths having a
common sequence of events are collap sed into one path
( Covaliu and Oliver 1992). Numer ical data are stored
in the formulation table.
Shenoy (1993) proposes a "factorization" approach for
representing degenerate probability distributions. In
that appro ach , a degenerate probability distribution
over a set of variables is decomposed into several fac­
tors over subsets of the variables such that the their
"product" is equivalent to the original distribution.
Smith et ai. (1993) present some interesting progress
towards exploiting asymmetry of decision problems.
They observe that an asymmetric decision problem of­
ten has some degenerate probability distributions, and
that the influence diagram evaluation can be sped up
if these degenerate probability distributions are used
properly. Their philosophy is analogous to the one
behind various algorithms for sparse matrix computa­
tion. In t hei r propos al , a conv entional influence dia­
gram is used to represent a decision problem at the
level of relation. In addition, they propose to use a
decision tree-like representation to describe the con­
ditional probability distributions associated with the
random variables in the influence diagram. The deci-

Solving Asymmetric Decision Problems with Influence Diagrams

sion tree-like representation is effective for economi­
cally representing degenerate conditional probability
distributions. T hey propose a modified version of
Shachter's alg ori thm (Sha chter 1986) for influence di­
agram evaluation, and show how the decision tree­
like representation can be used to increase the effi­
ciency of arc reversal, a fundamental operation used in
Shachter's algorithm. However, their alg o r it hm cannot
avoid computing optimal choices for decision variables
with respect to impossible information states.
CONCLUSIONS

7

analyzed a drawback of influence di­
a gram s with asymmetric decision problems, which in­
duces some unnecessary computation in solving asym­
metric decision problems through influence diagram
evaluat i on. We presented an approach for overcoming
the drawb ack. Our ap p r o ach consists of a simple ex­
tension to influence diag rams and a top-down method
for influence diagram evaluation. The exte nsion fa­
cilitate s expressing asymmetry in influence diagrams.
The top-down method effectively avoids unnecessary
computation.

In this paper we

Acknowledgement
research reported in this paper is partially
under NSERC grant OGP0044121 and
Project B5 of I RIS. The authors wish to thank
Craig Boutili e r, Andre w Csi ng er, Mike Horsch, Keiji

The

pported

su

Kanazawa, Jim Little, Alan Mackworth, Maurice
Queyranne, Jack Snoeyink and Ying Zhang for t he i r
val u able comments.


