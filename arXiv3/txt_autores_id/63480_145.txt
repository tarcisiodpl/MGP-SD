
Efficient planning plays a crucial role in
model-based reinforcement learning. Traditionally, the main planning operation is a
full backup based on the current estimates of
the successor states. Consequently, its computation time is proportional to the number of successor states. In this paper, we
introduce a new planning backup that uses
only the current value of a single successor
state and has a computation time independent of the number of successor states. This
new backup, which we call a small backup,
opens the door to a new class of model-based
reinforcement learning methods that exhibit
much finer control over their planning process
than traditional methods. We empirically
demonstrate that this increased flexibility allows for more efficient planning by showing
that an implementation of prioritized sweeping based on small backups achieves a substantial performance improvement over classical implementations.

1. Introduction
In reinforcement learning (RL) (Kaelbling et al., 1996;
Sutton & Barto, 1998), an agent seeks an optimal control policy for a sequential decision problem in an initially unknown environment. The environment provides feedback on the agent’s behavior in the form
of a reward signal. The agent’s goal is to maximize
the expected return, which is the discounted sum of
rewards over future timesteps. An important performance measure in RL is the sample efficiency, which
refers to the number of environment interactions that
is required to obtain a good policy.
Many solution strategies improve the policy by iteratively improving a state-value or action-value function,
which provide estimates of the expected return under
a given policy for (environment) states or state-action

pairs, respectively. Different approaches for updating
these value functions exist. In terms of sample efficiency, one of the most effective approaches is to estimate the environment model using observed samples
and to compute, at each time step, the (action-)value
function that is optimal with respect to the model estimate using planning techniques. A popular planning
technique used for this is value iteration (VI) (Sutton,
1988; Watkins, 1989), which performs sweeps of backups through the state or state-action space, until the
(action-)value function has converged.
A drawback of using VI is that it is computationally very expensive, making it infeasible for many
practical applications. Fortunately, efficient approximations can be obtained by limiting the number of
backups that is performed per timestep. A very effective approximation strategy is prioritized sweeping (Moore & Atkeson, 1993; Peng & Williams, 1993),
which prioritizes backups that are expected to cause
large value changes. This paper introduces a new
backup that enables a dramatic improvement in the
efficiency of prioritized sweeping.
The main idea behind this new backup is as following.
Consider that we are interested in some estimate A
that is constructed from a sum of other estimates Xi .
The estimate A can be computed using a full backup:
X
Xi .
A←
i

If the estimates Xi are updated, A can be recomputed
by redoing the above backup. Alternatively, if we know
that only Xj received a significant value change, we
might want to update A for only Xj . Let us indicate
the old value of Xj , used to construct the current value
of A, as xj . A can then be updated by subtracting this
old value and adding the new value:
A ← A − xj + Xj .
This kind of backup, which we call a small backup, is
computationally cheaper than the full backup. The

Planning by Prioritized Sweeping with Small Backups

trade-off is that, in general, more memory is required
for storing the estimates xi associated with A. In planning, where the X estimates correspond to state-value
estimates and A corresponds to a state or state-action
estimate, this is not a serious restriction, because a
full model is stored already. The additional memory
required has the same order of complexity as the memory required for storage of the model.
The core advantage of small backups over full backups is that they enable finer control over the planning process. This allows for more effective update
strategies, resulting in improved trade-offs between
computation time and quality of approximation of
the VI solution (and hence sample efficiency). We
demonstrate this empirically by showing that a prioritized sweeping implementation based on small backups yields a substantial performance improvement over
the two classical implementations (Moore & Atkeson,
1993; Peng & Williams, 1993).
In addition, we demonstrate the relevance of small
backups in domains with severe constraints on computation time, by showing that a method that performs one small backup per time step has an equal
computation time complexity as TD(0), the classical
method that performs one sample backup per timestep.
Since sample backups introduce sampling variance,
they require a step-size parameter to be tuned for
optimal performance. Small backups, on the other
hand, do not introduce sampling variance, allowing
for a parameter-free implementation. We empirically
demonstrate that the performance of a method that
performs one small backup per time step is similar to
the optimal performance of TD(0), achieved by carefully tuning the step-size parameter.

2. Reinforcement Learning Framework
RL problems are often formalized as Markov decision
processes (MDPs), which can be described as tuples
hS, A, P, R, γi consisting of S, the set of all states; A,
s′
the set of all actions; Psa
= P r(s′ |s, a), the transition
probability from state s ∈ S to state s′ when action
a ∈ A is taken; Rsa = E{r|s, a}, the reward function
giving the expected reward r when action a is taken
in state s; and γ, the discount factor controlling the
weight of future rewards versus that of the immediate
reward.
Actions are selected at discrete timesteps t = 0, 1, 2, ...
according to a policy π : S × A → [0, 1], which defines
for each action the selection probability conditioned
on the state. In general, the goal of RL is to improve
the policy in order to increase the return G, which is

the discounted cumulative reward
Gt = rt+1 + γ rt+2 + γ 2 rt+3 + ... =

∞
X

γ k−1 rt+k ,

k=1

where rt+1 is the reward received after taking action
at in state st at timestep t.
The prediction task consists of determining the value
function V π (s), which gives the expected return when
policy π is followed, starting from state s. V π (s) can
be found by making use of the Bellman equations for
state values, which state the following:
X ′
V π (s) = Rs + γ
Pss V π (s′ ) ,
(1)
s′

P
P
′
s′
.
where Rs = a π(s, a)Rsa and Pss = a π(s, a)Psa
Model-based methods use samples to update estimates
′
of the transition probabilities, P̂ss , and reward function, R̂s . With these estimates, they can iteratively
improve an estimate V of V π , by performing full backups, derived from Equation (1):
X ′
V (s) ← R̂s + γ
P̂ss V (s′ ) .
(2)
s′

In the control task, methods often aim to find the optimal policy π ∗ , which maximizes the expected return.
This policy is the greedy policy with respect to the optimal action-value function Q∗ (s, a), which gives the
expected return when taking action a in state s, and
following π ∗ thereafter. This function is the solution
to the Bellman optimality equation for action-values:
X ′
s
Q∗ (s, a) = Rsa + γ
Psa
max
Q∗ (s′ , a′ ) . (3)
′
s′

a

The optimal value function is related to the optimal action-value function through:
V ∗ (s) =
maxa Q∗ (s, a).
Model-based methods can iteratively improve estimates Q of Q∗ by performing full backups derived from
Equation (3):
X ′
s
Q(s, a) ← R̂sa + γ
P̂sa
max
Q(s′ , a′ ) ,
(4)
′
s′

a

′

′

s
s
where R̂sa and P̂sa
are estimates of Rsa and Psa
, respectively.

Model-free methods do not maintain an model estimate, but update a value function directly from samples. A classical example of a sample backup, based on
sample (s, r, s′ ) is the TD(0) backup:
V (s) ← V (s) + α (r + γV (s′ ) − V (s)) ,
where α is the step-size parameter.

(5)

Planning by Prioritized Sweeping with Small Backups

3. Small Backup
This section introduces the small backup. We start
with small state-value backups for the prediction task.
Section 3.3 discusses small action-value backups for
the control task.
3.1. Value Backups
In this section, we introduce a small backup version
of the full backup for prediction (backup 2). In the
introduction, we showed that a small backup requires
storage of the component values that make up the current value of a variable. In the case of a small value
backup, the component values correspond to the values of successor states. We indicate these values by
the function Us : S × S → IR. So, Us (s′ ) is the value
estimate of s′ associated with s.
Using Us , V (s) can be updated with only the current
value of a single successor state, s′ , as demonstrated
by the following theorem. The three backups shown
in the theorem form together the small backup.
Theorem 3.1 If the current relation between V (s)
and Us is given by
X ′′
V (s) = R̂s + γ
(6)
P̂ss Us (s′′ ) ,
s′′

then, after performing the following backups:
tmp ←
V (s) ←
′

Us (s ) ←

V (s′ )
V (s) +

(7)
′
γPss [V

′

′

(s ) − Us (s )]

tmp ,

(8)
(9)

relation (6) still holds, but Us (s′ ) is updated to V (s′ ).
Proof Backup (8) subtract the component in relation
(6) corresponding to s′ from V (s) and adds a new component based on the current value estimate of s′ :
′

′

V (s) ← V (s) − γ P̂ss Us (s′ ) + γ P̂ss V (s′ ) .
Hence, relation (6) is maintained, while Us (s′ ) is updated. Note that V (s′ ) needs to be stored in a temporary variable, since backup (8) can alter the value of
V (s′ ) if s′ = s.
3.2. Value Correction after Model Update
Theorem 3.1 relies on relation (6) to hold. If the
model gets updated, this relation now longer holds.
In this section, we discuss how to restore relation (6)
in a computation-efficient way for the commonly used
model estimate:
′
P̂ss

←

R̂s

←

′
Nss /Ns
Rssum /Ns

(10)
,

(11)

where Ns counts the number of times state s is vis′
ited, Nss counts the number of times s′ is observed as
successor state of s, and Rssum is the sum of observed
rewards for s.
Theorem 3.2 If currently, the following relation
holds:
X ′′
P̂ss Us (s′′ ) ,
V (s) = R̂s + γ
s′′

′

and a sample (s, r, s ) is observed, then, after performing the backups:
′

′

Ns ← Ns + 1; Nss ← Nss + 1
(12)
h
i
V (s) ← V (s)(Ns − 1) + r + γUs (s′ ) /Ns . (13)
the relation still holds, but with updated values for R̂s
′′
and P̂ss .
Proof (sketch) Backup (13) updates V (s) by computing a weighted average of V (s) and r + γUs (s′ ).
The value change this causes is the same as the value
change caused by updating the model and then performing a full backup of s based on Us .
Algorithm 1 shows pseudo-code for a general class of
prediction methods based on small backups. Surpisngly, while it is a planning method, R̂s is never explicitly computed, saving time and memory. Note that the
computation per time step is fully independent of the
number of successor states. Members of this class need
to specify the number of iterations (line 8) as well as
a strategy for selecting state-successor pairs (line 9).
Algorithm 1 Prediction with Small Backups
1: initialize V (s) arbitrarily for all s
2: initialize Us (s′ ) = V (s′ ) for all s, s′
′
3: initialize Ns , Nss to 0 for all s, s′
4: loop {over timesteps}
5:
observe transition (s, r, s′ )
′
′
6:
Ns ← Nsh + 1; Nss ← Nss + 1
i
7:

8:
9:
10:
11:
12:
13:
14:

V (s) ← V (s)(Ns − 1) + r + γ Us (s′ ) /Ns
loop {for a number of iterations}
′
select a pair (s̄, s̄′ ) with Ns̄s̄ > 0
tmp ← V (s̄′ )
′
V (s̄) ← V (s̄) + γNs̄s̄ /Ns̄ · [V (s̄′ ) − Us̄ (s̄′ )]
Us̄ (s̄′ ) ← tmp
end loop
end loop

3.3. Action-value Backups
Before we can discuss small action-value backups, we
have to discuss a more efficient implementation of the

Planning by Prioritized Sweeping with Small Backups

full action-value backup. Backup (4) has a computation time complexity of O(|S||A|). A more efficient implementation can be obtained by storing statevalues, besides action-values, according to V (s) =
maxa Q(s, a). Backup (4) can then be implemented
by:
X
Q(s, a) ← R̂sa + γ
V (s′ )
(14)
s′

V (s) ← max
Q(s, a) .
′
a

(15)

The combined computation time of these backups is
O(|S| + |A|), a considerable reduction.
Backup (14) is similar in form as the prediction
backup. Hence, we can make a small backup version of
it similar to the one in the prediction case. The theorems below are the control versions of the theorems for
the prediction case. They can be proven in a similar
way as the prediction theorems.
Theorem 3.3 If the current relation between Q(s, a)
and Usa is given by
X
s′′
Q(s, a) ← R̂sa + γPsa
Usa (s′′ ) ,
(16)
s′′

then, performing the following backups:
′

s
Q(s, a) ← Q(s, a) + γPsa
[V (s′ ) − Usa (s′ )]
Usa (s′ ) ← V (s′ ) ,

maintains this relation while updating Usa (s′ ) to V (s′ ).
Theorem 3.4 If relation (16) holds and a sample
(s, a, r, s′ ) is observed, then, after performing backups
′

′

s
s
← Nsa + 1; Nsa
← Nsa
+1
h
i
Q(s, a) ← Q(s, a)(Nsa − 1) + r + γUsa (s′ ) /Nsa ,

Nsa

relation (16) still holds, but with updated values for
s′′
R̂sa and P̂sa
.
A small action-value backup is a finer-grained version
of backup (14): performing a small backup of Q(s, a)
for each successor state is equivalent (in computation
time complexity and effect) as performing backup (14)
once. While in principle, backup (15) can be performed after each small backup, it is not very efficient to do so, since small backups make many small
changes. More efficient planning can be obtained when
backup (15) is performed only once in a while.
In Section 4, we discuss an implementation of prioritized sweeping based on small action-value backups.

3.4. Small Backups versus Sample Backups
A small backup has in common with a sample backup
that both update a state value based on the current
value of only one of the successor states. In addition,
they share the same computation time complexity and
their effect is in general smaller than that of a full
backup.
A disadvantage of a sample backup, with respect to a
small backup, is that it introduces sampling variance,
caused by a stochastic environment. This requires the
use of a step-size parameter to enable averaging over
successor states (and rewards). A small backup does
not introduce sampling variance, since it is implicitly
based on an expectation over successor states. Hence,
it does not require tuning of a step-size parameter for
optimal performance.
A second disadvantage of a sample backup is that it affects the perceived distribution over action outcomes,
which places some restrictions on reusing samples. For
example, a model-free technique like experience replay
(Lin, 1992), which stores experience samples in order
to replay them at a later time, can introduce bias,
which reduces performance, if some samples are replayed more often than others. For small backups this
does not hold, since the process of learning the model
is independent from the backups based on the model.
This allows small backups to be combined with effective selection strategies like prioritized sweeping.

4. Prioritized Sweeping with Small
Backups
Prioritized sweeping (PS) makes the planning step of
model-based RL more efficient by using a heuristic (a
‘priority’) for selecting backups that favours backups
that are expected to cause a large value change. A priority queue is maintained that determines which values
are next in line for receiving backups.
There are two main implementations:
one by
Moore & Atkeson (1993) and one by Peng & Williams
(1993) 1 . All PS methods have in common that they
perform backups in what we call update cycles. By adjusting the number of update cycles that is performed
per time step, the computation time per time step can
be controlled. Below, we discuss in detail what occurs
in an update cycle for the two classical PS implementations.
1

We refer to the version of ‘queue-Dyna’ for stochastic
domains, which is different from the version for deterministic domains.

Planning by Prioritized Sweeping with Small Backups

4.1. Classical Prioritized Sweeping
Implementations

answer is to put states in the priority queue and to
perform backup (15) for the top state.

In the Moore & Atkeson implementation the elements
in the queue are states and the backups are full value
backups. In control, a full value backup is different
from backup (2). Instead, it is equivalent (in effect and
computation time) to performing backup (14) for each
action, followed by backup (15). Hence, the associated
computation time has complexity O(|S||A| + |A|).

The priority associated with a state is based on the
change in action-value that has occurred due to small
backups, since the last value backup. This priority assures that states with a large discrepancy between the
state value and action-values, receive a value backup
first.

An update cycle consists of the following steps. First,
the top state is removed from the queue, and receives
a full value backup. Let s bet the top state and ∆Vs
the value change caused by the backup. Then, for
all predecessor state-action pairs (s̄, ā) a priority p is
computed, using:
s
p ← Ps̄ā
· |∆Vs | .

(17)

If s̄ is not yet on the queue, then it is added with
priority p. If s̄ is on the queue already, but its current priority is smaller than p, then the priority of s̄ is
upgraded to p.
The Peng & Williams implementation differs from the
Moore & Atkeson implementation in that the backup
is not a full value backup. Instead, it is a backup
with the same effect as a small action-value backup,
but with a computational complexity of O(|S| + |A|).
So, it is a cheaper backup than a full backup, but its
value change is (much) smaller. The backup requires a
state-action-successor triple. Hence, these triples are
the elements on the queue. Predecessors are added to
the queue with a priorities that estimate the actionvalue change.
4.2. Small Backup implementation
A natural small backup implementation might appear to be an implementation similar to that of Peng
& Williams, but with the main backup implemented
more efficiently. The low computational cost of a small
backup, however, allows for a much more powerful implementation. The pseudo-code of this implementation is shown in Algorithm 2. Below, we discuss some
key characteristics of the algorithm.
The computation time of a small backup is so low,
that it is comparable to the priority computation in
the classical PS implementations. Therefore, instead
of computing a priority for each predecessor and performing a backup for the element with the highest priority in the next update cycle, we can perform a small
backup for all predecessors. This raises the question
of what to put in the priority queue and what type of
backup to perform for the top element. The natural

One surprising aspect of the algorithm is that it does
not use the function Usa , which forms an essential part
of small action-value backups. The reason is that due
to the specific backup strategy used by the algorithm,
Usa (s′ ) is equal to V (s′ ) for all state-action pairs (s, a)
and all successor states s′ . Hence, instead of using
Usa , V can be used, saving memory and simplifying
the code.
Table 1 shows the computation time complexity of
an update cycle for the different PS implementations.
The small backup implementation is the cheapest one
among the three.

Moore & Atkeson
Peng & Williams
small backups

top-element
backups
O(|S||A| + |A|)
O(|S| + |A|)
O(|A|)

other
O(Pre )
O(Pre )
O(Pre )

Table 1. Computation time associated with one update cycle for the different PS implementations. Pre indicates the
number of predecessors, state-action pairs that transition
to the state whose value has just been updated.

5. Experimental Results
In this section, we evaluate the performance of a minimal version of Algorithm 1, as well as the performance
of Algorithm 2.
5.1. Small backup versus Sample backup
We compare the performance of TD(0), which performs one sample backup per time step, with a version
of Algorithm 1 that performs one small backup per
time step. Specifically, its number of iterations (line
8) is 1, and the selected state-successor pair (line 9) is
the pair corresponding to the most recent transition.
Their performance is compared on two evaluation
tasks, both consisting of 10 states, laid out in a circle. State transitions only occur between neighbours.
The transition probabilities for both tasks are generated by a random process. Specifically, the transition
probability to a neighbour state is generated by a random number between 0 and 1 and normalized such that

Planning by Prioritized Sweeping with Small Backups

Algorithm 2 Prioritized Sweeping with Small Backups
1: initialize V (s) arbitrarily for all s
2: initialize Q(s, a) = Qprev (s, a) = V (s) for all s, a
s′
3: initialize Nsa , Nsa
to 0 for all s, a, s′
4: loop {over episodes}
5:
initialize s
6:
repeat {for each step in the episode}
7:
select action a, based on Q(s, ·)
8:
take action a, observe r and s′
s′
s′
9:
Nsa ← Nsa + 1; Nsa
← Nsa
+1

10:
Q(s, a) ← Q(s, a)(Nsa − 1)+ r + γV (s′ ) /Nsa

Each time a transition is observed and the corresponding backup is performed, the root-mean squared
(RMS) error over all states is determined. The average
RMS error over the first 10.000 transitions, normalized with the initial error, determines the performance.
Figure 1 shows this performance, averaged over 100
runs. The standard error is negligible: the maximum
standard error in the first task was 0.0057 (after normalization) and in the second task 0.0007. Note that
the performance for d = 0 is equal to the performance
for α = 1, as it should, by definition. The normalized
performance for α = 0 is 1, since no learning occurs in
this case.

p ← |Q(s, a) − Qprev (s, a)|
if s not on queue or p > current priority s,
then promote s̄ to p
for a number of update cycles do
remove top state s̄′ from queue
for all b: Qprev (s̄′ , b) ← Q(s̄′ , b)
tmp ← V (s̄′ )
V (s̄′ ) ← maxb Q(s̄′ , b)
∆V ← V (s̄′ ) − tmp
s̄′
for all (s̄, ā) pairs with Ns̄ā
> 0 do
s̄′
Q(s̄, ā) ← Q(s̄, ā) + γNs̄ā /Ns̄ā · ∆V
p ← |Q(s̄, ā) − Qprev (s̄, ā)|
if s̄ not on queue or p > current priority
s̄, then promote s̄ to p
end for
end for
s ← s′
until s is terminal
end loop

These experiments demonstrate three things. First,
the optimal step-size can vary a lot between different
tasks. Second, selecting a sub-optimal step-size can
cause large performance drops. Third, a small-backup,
which is parameter-free, has a performance similar to
the performance of TD(0) with optimized step-size.
Since the computational complexity is the same, the
small backup is a very interesting alternative to the
sample backup in domains with tight constraints on
the computation time, where previously only sample
backups where viable. Keep in mind that a sample
backup does require a model estimate, so if there are
also tight constraints on the memory, a sample backup
might still be the only option.

11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:

the sum of the transition probabilities to the left and
right neighbour is 1. The reward for counter-clockwise
transitions is always +1. The reward for clockwise
transitions is different for the two tasks. In the first
task, a clockwise transition results in a reward of -1;
in the second task, it results in a reward of +1. The
discount factor γ is 0.95 and the initial state values are
0.
For TD(0), we performed experiments with a constant
step-size for values between 0 and 1 with steps of 0.02.
In addition, we performed experiments with a decaying, state-dependent step-size, according to
α(s) =

1
d · (Ns − 1) + 1

,

(18)

where Ns is the number of times state s has been visited, and d specifies the decay rate. We used values of
d between 0 and 1 with steps of 0.02. Note that for
d = 0, α(s) = 1, and for d = 1, α(s) = 1/Ns .

5.2. Prioritized Sweeping
We compare the performance of prioritized sweeping with small backups (Algorithm 2) with the
two classical implementations of Moore&Atkeson and
Peng&Williams on the maze task depicted in the top
of Figure 2. The reward received at each time step
is -1 and the discount factor is 0.99. The agent can
take four actions, corresponding to the four compass
directions, which stochastically move the agent to a
different square. The bottom of Figure 2 shows the
relative action outcomes of a ‘north’ action. In free
space, an action can result in 15 possible successor
states, each with equal probability. When the agent is
close to a wall, this number decreases.
To obtain an upper bound on the performance, we
also compared against a method that performs value
iteration (until convergence) at each time step, using
the most recent model estimate.
As exploration strategy, the agent select with 5% probability a random action, instead of the greedy one. On
top of that, we use the ‘optimism in the face of uncertainty’ principle, as also used by Moore & Atkeson.
This means that as long as a state-action pair has not
been visited for at least M times, it’s value is defined as

Planning by Prioritized Sweeping with Small Backups

normalized RMS error

1
0.8

TD(0), constant step−size
TD(0), decaying step−size
small backup

0.6
0.4
0.2
0
0

0.2

0.4

α/d

0.6

0.8

1

0.8

1

normalized RMS error

1
0.8

TD(0), constant step−size
TD(0), decaying step−size
small backup

two classical implementations. The results also show
that the Peng & Williams method performs considerably worse than the one of Moore & Atkeson in the
considered domain. This can be explained by the different backups they perform. The effect of the backup
of Peng & Williams is proportional to the transition
1
. In contrast,
probability, which in most cases is 15
the Moore & Atkeson method performs a full backup
each update cycle. While the small backup implementation also uses backups that are proportional to the
transition probability, it performs a lot more backups
per update cycle. Specifically, a number that is proportional to the number of predecessors. In general,
this number will increase when the stochasticity of the
domain increases.

0.6
0.4
0.2
0
0

0.2

0.4

α/d

0.6

Figure 1. Average RMS error over the first 10.000 observations, normalized by the initial error, for different values
of the step-size parameter α, in case of constant step-size,
or different values of the decay parameter d, in case of decaying step-size. The top graph corresponds with the first
evaluation task; the bottom graph with the second.

some optimistically defined value (0 for our maze task),
instead of the value based on the model estimate. We
optimized M for the value iteration method, resulting
in M = 4, and used this value for all methods.

S
G

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

0
0
3
1
1

0
0
3
1
1

1
1
1
2
0

1
1
1
2
0

1
1
1
2
0

0
0
1
01
0
1
01

2
2
2
4
1
0

1
1
1
2
01

We performed experiments for 1, 3, 5 and 10 update
cycles per time step. Figure 3 shows the average return
over the first 200 episodes for the different methods.
The results are averaged over 100 runs. The maximum
standard deviation is 0.1 for all methods, except for the
method of Peng & Williams, which had a maximum
standard deviation of 1.0.

Figure 2. Above, the maze task, in which the agent must
travel from S tothe G. Below, transition probabilities
1
) of a ‘north’ action for different positions of the agent
(· 15
(indicated by the circle) with respect to the walls (black
squares).

The computation time per update cycle was about the
same for the three different PS implementations, with
a small advantage for the small backup implementation, which shows that the O(Pre ) computation (see
Table 1) is dominant in this task. The computation
time per observation of the value iteration method was
more than 400 times as high as a single update cycle.

6. Discussion

PS with small backups turns out to be very effective.
With only a single update cycle, the value-iteration
result can be closely approximated, in contrast to the

Prioritized sweeping can be viewed as a generalization
of the idea of replaying of experience in backward order (Lin, 1992), which by itself is related to eligibility
traces (Sutton, 1988; Watkins, 1989; Sutton & Singh,
1994). What all these techniques have in common is
that new information (which can be value changes, but
at its core all value changes originate from new data) is
propagated backwards. Whereas backward replay and
eligibility traces use the recent trajectory for backward

Planning by Prioritized Sweeping with Small Backups

makes new, more efficient, update strategies possible.
In addition, small backups can be useful in domains
with very tight time constraints, offering a parameterfree alternative to sample backups, which were up to
now often the only feasible option for such domains.

−50

avg. return over first 200 episodes

−52
−54
−56
−58
−60




incomplete on each step, still efficiently computes optimal
actions in a timely manner.

We consider the problem of efficiently learning
optimal control policies and value functions over
large state spaces in an online setting in which
estimates must be available after each interaction
with the world. This paper develops an explicitly
model-based approach extending the Dyna architecture to linear function approximation. Dynastyle planning proceeds by generating imaginary
experience from the world model and then applying model-free reinforcement learning algorithms to the imagined state transitions. Our
main results are to prove that linear Dyna-style
planning converges to a unique solution independent of the generating distribution, under natural conditions. In the policy evaluation setting,
we prove that the limit point is the least-squares
(LSTD) solution. An implication of our results
is that prioritized-sweeping can be soundly extended to the linear approximation case, backing
up to preceding features rather than to preceding
states. We introduce two versions of prioritized
sweeping with linear Dyna and briefly illustrate
their performance empirically on the Mountain
Car and Boyan Chain problems.

The Dyna architecture (Sutton 1990) provides an effective
and flexible approach to incremental planning while maintaining responsiveness. There are two ideas underlying the
Dyna architecture. One is that planning, acting, and learning are all continual, operating as fast as they can without
waiting for each other. In practice, on conventional computers, each time step is shared between planning, acting,
and learning, with proportions that can be set arbitrarily according to available resources and required response times.

Online learning and planning

Efficient decision making when interacting with an incompletely known world can be thought of as an online learning
and planning problem. Each interaction provides additional
information that can be used to learn a better model of the
world’s dynamics, and because this change could result in a
different action being best (given the model), the planning
process should be repeated to take this into account. However, planning is inherently a complex process; on large
problems it not possible to repeat it on every time step without greatly slowing down the response time of the system.
Some form of incremental planning is required that, though

The second idea underlying the Dyna architecture is that
learning and planning are similar in a radical sense. Planning in the Dyna architecture consists of using the model
to generate imaginary experience and then processing the
transitions of the imaginary experience by model-free reinforcement learning algorithms as if they had actually occurred. This can be shown, under various conditions, to
produce exactly the same results as dynamic-programming
methods in the limit of infinite imaginary experience.
The original papers on the Dyna architecture and most subsequent extensions (e.g., Singh 1992; Peng & Williams
1993; Moore & Atkeson 1993; Kuvayev & Sutton 1996)
assumed a Markov environment with a tabular representation of states. This table-lookup representation limits the
applicability of the methods to relatively small problems.
Reinforcement learning has been combined with function
approximation to make it applicable to vastly larger problems than could be addressed with a tabular approach.
The most popular form of function approximation is linear function approximation, in which states or state-action
pairs are first mapped to feature vectors, which are then
mapped in a linear way, with learned parameters, to value
or next-state estimates. Linear methods have been used
in many of the successful large-scale applications of reinforcement learning (e.g., Silver, Sutton & Müller 2007;
Schaeffer, Hlynka & Jussila 2001). Linear function approximation is also simple, easy to understand, and possesses some of the strongest convergence and performance
guarantees among function approximation methods. It is

natural then to consider extending Dyna for use with linear
function approximation, as we do in this paper.
There has been little previous work addressing planning
with linear function approximation in an online setting.
Paduraru (2007) treated this case, focusing mainly on sampling stochastic models of a cascading linear form, but
also briefly discussing deterministic linear models. Degris,
Sigaud and Wuillemin (2006) developed a version of Dyna
based on approximations in the form of dynamic Bayes networks and decision trees. Their system, SPITI, included
online learning and planning based on an incremental version of structured value iteration (Boutilier, Dearden &
Goldszmidt 2000). Singh (1992) developed a version of
Dyna for variable resolution but still tabular models. Others
have proposed linear least-squares methods for policy evaluation that are efficient in the amount of data used (Bradtke
& Barto 1996; Boyan 1999, 2002; Geramifard, Bowling &
Sutton 2006). These methods can be interpreted as forming and then planning with a linear model of the world’s
dynamics, but so far their extensions to the control case
have not been well suited to online use (Lagoudakis &
Parr 2003; Peters, Vijayakumar & Schaal 2005; Bowling,
Geramifard, & Wingate 2008), whereas our linear Dyna
methods are naturally adapted to this case. We discuss
more specifically the relationship of our work to LSTD
methods in a later section. Finally, Atkeson (1993) and others have explored linear, learned models with off-line planning methods suited to low-dimensional continuous systems.

2

Notation

We use the standard framework for reinforcement learning with linear function approximation (Sutton & Barto
1998), in which experience consists of the time indexed
stream s0 , a0 , r1 , s1 , a1 , r2 , s2 , . . ., where st ∈ S is a state,
at ∈ A is an action, and rt ∈ R is a reward. The actions are selected by a learning agent, and the states and rewards are selected by a stationary environment. The agent
does not have access to the states directly but only through
a corresponding feature vector φt ∈ Rn = φ(st ). The
n
agent selects actions
P according to a policy, π : R × A →
[0, 1] such that a∈A π(φ, a) = 1, ∀φ. An important step
towards finding a good policy is to estimate the value function for a given policy (policy evaluation). The value function is approximated as a linear function with parameter
vector θ ∈ Rn :
(∞
)
X
θ> φ(s) ≈ V π (s) = Eπ
γ t−1 rt | s0 = s ,
t=1

where γ ∈ [0, 1). In this paper we consider policies that are
greedy or -greedy with respect to the approximate statevalue function.

Algorithm 1 : Linear Dyna for policy evaluation, with random sampling and gradient-descent model learning
Obtain initial φ, θ, F, b
For each time step:
Take action a according to the policy. Receive r, φ0
θ ← θ + α[r + γθ> φ0 − θ> φ]φ
F ← F + α(φ0 − F φ)φ>
b ← b + α(r − b> φ)φ
temp ← φ0
Repeat p times (planning):
Generate a sample φ from some distribution µ
φ0 ← F φ
r ← b> φ
θ ← θ + α[r + γθ> φ0 − θ> φ]φ
φ ← temp

3

Theory for policy evaluation

The natural place to begin a study of Dyna-style planning
is with the policy evaluation problem of estimating a statevalue function from a linear model of the world. The model
consists of a forward transition matrix F ∈ Rn × Rn (incorporating both environment and policy) and an expected
reward vector b ∈ Rn , constructed such that F φ and b> φ
can be used as estimates of the feature vector and reward
that follow φ. A Dyna algorithm for policy evaluation goes
through a sequence of planning steps, on each of which a
starting feature vector φ is generated according to a probability distribution µ, and then a next feature vector φ0 = F φ
and next reward r = b> φ are generated from the model.
Given this imaginary experience, a conventional modelfree update is performed, for example, according to the linear TD(0) algorithm (Sutton 1988):
θ ← θ + α(r + γθ> φ0 − θ> φ)φ,

(1)

or according to the residual gradient algorithm (Baird
1995):
θ ← θ + α(r + γθ> φ0 − θ> φ)(φ − γφ0 ),

(2)

where α > 0 is a step-size parameter. A complete algorithm using TD(0), including learning of the model, is
given in Algorithm 1.
3.1

Convergence and fixed point

There are two salient theoretical questions about the Dyna
planning iterations (1) and (2): Under what conditions on
µ and F do they converge? and What do they converge
to? Both of these questions turn out to have interesting answers. First, note that the convergence of (1) is in question
in part because it is known that linear TD(0) may diverge
if the distribution of starting states during training does not
match the distribution created by the normal dynamics of

the system, that is, if TD(0) is used off-policy. This suggests that the sampling distribution used here, µ, might
have to be strongly constrained in order for the iteration
to be stable. On the other hand, the data here is from the
model, and the model is not a general system: it is deterministic1 and linear. This special case could be much better
behaved. In fact, convergence of linear Dyna-style policy
evaluation, with either the TD(0) or residual-gradient iterations, is not affected by µ, but only by F , as long as µ exercises all directions in the full n-dimensional vector space.
Moreover, not only is the fact of convergence unaffected by
µ, but so is the value converged to. In fact, we show below
that convergence is to a deterministic fixed point, a value
of θ such that the iterations (1) and (2) leave it unchanged
not just in expected value, but for every individual φ that
could be generated by µ. The only way this could be true is
if the TD error (the first expression in parentheses in each
iteration) were exactly zero, that is, if
0

=

r + γθ> φ0 − θ> φ
>

>

b φ + γθ F φ − θ φ

=

(b + γF > θ − θ)> φ.

And the only way that this can be true for all φ is for the
expression in parenthesis above to be zero:
0

Before verifying the conditions of this result, let us
rewrite (4) in terms of the matrix G = I − γF :
θk+1

= θk + αk (b> φk + θk> (γF − I)φk )φk

= b + (γF > − I)θ,

= θk + αk sk .

=

(I − γF > )−1 b,

(4)

where θ0 ∈ Rn is P
arbitrary. AssumeP
that (i) the step-size
∞
∞
sequence satisfies k=0 αk = ∞, k=0 αk2 < ∞, (ii)
r(F ) ≤ 1, (iii) (φk ) are uniformly
 bounded
 i.i.d. random
variables, and that (iv) C = E φk φ>
is non-singular.
k
Then the parameter vector θk converges with probability
one to (I − γF > )−1 b.

= θk + αk (b> φk − θk> Gφk )φk

Here sk is defined by the last equation.
(3)

assuming that the inverse exists. Note that this expression
for the fixed point does not depend on µ, as promised.
If I − γF > is nonsingular, then there might be no fixed
point. This could happen for example if F were an expansion, or more generally if the limit (γF )∞ were not
zero. These cases correspond to world models that say the
feature vectors diverge to infinity over time. Failure to converge in these cases should not be considered a problem for
the Dyna iterations as planning algorithms; these are cases
in which the planning problem is ill posed. If the feature
vectors diverge, then so too may the rewards, in which case
the true values given the model are infinite. No real finite
Markov decision process could behave in this way.
It remains to show the conditions on F under which the iterations converge to the fixed point if one exists. We prove
next that under the TD(0) iteration (1), convergence is guaranteed if the numerical radius of F is less than one,2 and
1

θk+1 = θk + αk (b> φk + γθk> F φk − θk> φk )φk ,

= b + γF > θ − θ

which immediately implies that
θ

Theorem 3.1 (Convergence of linear TD(0) Dyna for policy evaluation). Consider the TD(0) iteration with a nonnegative step-size sequence (αk ):

Proof. The idea of the proof is to view the algorithm as a
stochastic gradient descent method. In particular, we apply
Proposition 4.1 of (Bertsekas & Tsitsiklis 1996).

>

=

then that under the residual-gradient iteration (2), convergence is guaranteed for any F as long as the fixed point exists. That F ’s numerical radius be less than 1 is a stronger
condition than nonsingularity of I − γF > , but it is similar
in that both conditions pertain to the matrix trending toward
expansion when multiplied by itself.

The model is deterministic because it generates the expectation of the next feature vector; the system itself may be stochastic.
2
The numerical radius of a real-valued square matrix A is defined by r(A) = maxkxk2 =1 xT Ax.

The cited proposition requires the definition of a potential function J(θ) and will allow us to conclude that
limk→∞ ∇J(θk) = 0 with probability one. Let
 us choose
J(θ) = 1/2 E (b> φk + γθ> F φk − θ> φk )2 . Note that
by our i.i.d. assumptions on the features, J(θ) is welldefined. We need to check four conditions (because the
step-size conditions are automatically satisfied): (i) The
nonnegativity of the potential function; (ii) The Lipschitz
continuity of ∇J(θ); (iii) The pseudo-gradient property of
the expected update direction; and (iv) The boundedness of
the expected magnitude of the update, more precisely that
E ksk k22 |θk ≤ O(k∇J(θk )k22 ). Nonnegativity is satisfied
by definition and the boundedness condition (iv) is satisfied
thanks to the boundedness of the features.
Let us show now that the pseudo-gradient property (iii) is
satisfied. This condition requires the demonstration of a
positive constant c such that
ck∇J(θk )k22 ≤ −∇J(θk )> E [sk |θk ] .

(5)

Define sk = E [sk |θk ] = Cb − CG> θk . A simple calculation gives ∇J(θk ) = −Gsk . Hence k∇J(θk )k22 =
>
>
>
s>
k G Gsk and −(∇J(θk )) sk = sk Gsk . Therefore (5)
> >
>
is equivalent to c sk G Gsk ≤ sk Gsk . In order to make
this true with a sufficiently small c, it suffices to show that

s> Gs > 0 holds for any non-zero vector s. An elementary
reasoning shows that this is equivalent to 1/2(G + G> ) being positive definite, which in turn is equivalent to r(F ) ≤
1, showing that (iii) is satisfied.
Hence, we have verified all the assumptions of the
cited proposition and can therefore we conclude that
limk→∞ ∇J(θk ) = 0 with probability one. Plugging in the
expression of ∇J(θk ), we get limt→∞ (Cb−CG> θk ) = 0.
Because C and G are invertible (this latter follows from
r(F ) ≤ 1), it follows that the limit of θk exists and
limk→∞ θk = (G> )−1 b = (I − γF > )−1 b.

verges with probability one to (I − γF > )−1 b, assuming
that (I − γF > ) is non-singular.
Proof. As all the conditions of Proposition 4.1 of (Bertsekas & Tsitsiklis 1996) are trivially satisfied with the
choice J(θ) = E [J(θ, φk )], we can conclude that θk converges w.p.1 to the minimizer of J(θ). In the previous theorem we have seen that the minimizer of J(θ) is indeed
θ = (I − γF > )−1 b, finishing the proof.
3.2

Convergence to the LSTD solution

Several extensions of this result are possible. First, the requirement of i.i.d. sampling can be considerably relaxed.
With an essentially unchanged proof, it is possible to show
that the theorem remains true if the feature vectors are generated by a Markov process given that they satisfy appropriate ergodicity conditions. Moreover, building on a result by Delyon (1996), one can show that the result continues to hold even if the sequence of features is generated in an algorithmic manner, again provided that some
ergodicity conditions are met.
PKThe major assumption then
is that C = limK→∞ 1/K k=1 φk φ>
k exists and is nonsingular. Further, because there is no “noise” to reject, there
is noP
need to decay the step-sizes towards zero (the condi∞
tion k=0 αk2 < +∞ in the proofs is used to “filter out
noise”). In particular, we conjecture that sufficiently small
constant step-sizes would work as well (for a result of this
type see Proposition 3.4 by Bertsekas & Tsitsiklis 1996).

So far we have discussed the convergence of planning given
a model, but we have said nothing about the relationship
of the model to data, or about the quality of the resultant
solution. Suppose the model were the best linear fit to a
finite dataset of observed feature-vector-to-feature-vector
transitions with accompanying rewards. In this case we can
show that the fixed point of the Dyna updates is the least
squares temporal-difference solution. This is the solution
for which the mean TD(0) update is zero and is also the solution found by the LSTD(0) algorithm (Barto & Bradtke
1996).

On the other hand the requirement on the numerical radius of F seems to be necessary for the convergence of the
TD(0) iteration. By studying the ODE associated with (4),
we see that it is stable if and only if CG is a positive stable
matrix (i.e., iff all its eigenvalues have positive real part).
From this it seems necessary to require that G is positive
stable. However, to ensure that CG is positive stable the
strictly stronger condition that G + G> is positive definite must be satisfied. This latter condition is equivalent
to r(F ) ≤ 1.

Proof. It suffices to show that the respective solution sets
of the equations

We turn now to consider the convergence of Dyna planning
using the residual-gradient Dyna iteration (2). This update
rule can be derived by taking the gradient of J(θ, φk ) =
(b> φk + γθ> φk − θ> φk )2 w.r.t. θ. Thus, as an immediate
consequence of Proposition 4.1 of (Bertsekas & Tsitsiklis
1996) we get the following result:
Theorem 3.2 (Convergence of residual-gradient Dyna for
policy evaluation). Assume that θk is updated according to
θk+1 = θk + αk (b> φk + γθk> F φk − θk> φk )(φk − γF φk ),
where θ0 ∈ Rn is arbitrary. Assume that the non-negative
step-size sequence (αk ) satisfies the summability condition
(i) of Theorem 3.1 and that (φk ) are uniformly bounded
i.i.d. random variables. Then the parameter vector θk con-

Theorem 3.3. Given a training dataset of feature, reward,
next-state feature triples D = [φ1 , r1 , φ01 , . . . , φn , rn , φ0n ],
let F, bPbe the least-squares model built on D. Assume that
n
C = k=1 φk φ>
k has full rank. Then the solution (3) is
the same as the LSTD solution on this training set.

0

=

n
X

φk (rk + γ(φ0k )> θ − φ>
k θ),

(6)

k=1

0

=

b + (γF > − I)θ

(7)

are the same. This is because the LSTD parameter vectors
are obtained by solving the first equation and the TD(0)
Dyna solutions are derived from the second equation.
Pn
Pn
Let D = k=1 φk (φ0k )> , and r = k=1 φk rk . A standard calculation shows that
F>

=

C −1 D

and b = C −1 r.

Plugging in C, D into (6) and factoring out θ shows that
any solution of (6) also satisfies
0

=

r + (γD − C) θ.

(8)

If we multiply both sides of (8) by C −1 from the left we
get (7). Hence any solution of (6) is also a solution of (7).
Because all the steps of the above derivation are reversible,
we get that the reverse statement holds as well.

Algorithm 2 : Linear Dyna with PWMA prioritized
sweeping (policy evaluation)
Obtain initial φ, θ, F, b
For each time step:
Take action a according to the policy. Receive r, φ0
δ ← r + γθ> φ0 − θ> φ
θ ← θ + αδφ
F ← F + α(φ0 − F φ)φ>
b ← b + α(r − b> φ)φ
For all i such that φ(i) 6= 0:
For all j such that F ij 6= 0:
Put j on the PQueue with priority |F ij δφ(i)|
Repeat p times while PQueue is not empty:
i ← pop the PQueue
δ ← b(i) + γθ> F ei − θ(i)
θ(i) ← θ(i) + αδ
For all j such that F ij 6= 0:
Put j on the queue with priority |F ij δ|
φ ← φ0

4

Algorithm 3 : Linear Dyna with MG prioritized sweeping
(policy evaluation)
Obtain initial φ, θ, F, b
For each time step:
Take action a according to the policy. Receive r, φ0
δ ← r + γθ> φ0 − θ> φ
θ ← θ + αδφ
F ← F + α(φ0 − F φ)φ>
b ← b + α(r − b> φ)φ
For all i such that φ(i) 6= 0:
Put i on the PQueue with priority |δφ(i)|
Repeat p times while PQueue is not empty:
i ← pop the PQueue
For all j such that F ij 6= 0:
δ ← b(j) + γθ> F ej − θ(j)
θ(j) ← θ(j) + αδ
Put j on the PQueue with priority |δ|
φ ← φ0

Linear prioritized sweeping

We have shown that the convergence and fixed point of policy evaluation by linear Dyna are not affected by the way
the starting feature vectors are chosen. This opens the possibility of selecting them cleverly so as to speed the convergence of the planning process. One natural idea—the
idea behind prioritized sweeping—is to work backwards
from states that have changed in value to the states that
lead into them. The lead-in states are given priority for being updated because an update there is likely to change the
state’s value (because they lead to a state that has changed
in value). If a lead-in state is updated and its value is
changed, then its lead-in states are in turn given priority
for updating, and so on. In the table-lookup context in
which this idea was developed (Moore & Atkeson 1993;
Peng 1993; see also Wingate & Seppi 2005), there could
be many states preceding each changed state, but only one
could be updated at a time. The states waiting to be updated were kept in a queue, prioritized by the size of their
likely effect on the value function. As high-priority states
were popped off the queue and updated, it would sometimes give rise to highly efficient sweeps of updates across
the state space; this is what gave rise to the name “prioritized sweeping”.
With function approximation it is not possible to identify
and work backwards from individual states, but alternatively one could work backwards feature by feature. If
there has just been a large change in θ(i), the component of
the parameter vector corresponding to the ith feature, then
one can look backwards through the model to find the features j whose components θ(j) are likely to have changed
as a result. These are the features j for which the elements
F ij of F are large. One can then preferentially construct

starting feature vectors φ that have non-zero entries at these
j components. In our algorithms we choose the starting
vectors to be the unit basis vectors ej , all of whose components are zero except the jth, which is 1. (Our theoretical
results assure us that this cannot affect the result of convergence.) Using unit basis vectors is very efficient computationally, as the vector matrix multiplication F φ is reduced
to pulling out a single column of F .
There are two tabular prioritized sweeping algorithms in
the literature. The first, due simultaneously to Peng and
Williams (1993) and to Moore and Atkeson (1993), which
we call PWMA prioritized sweeping, adds the predecessors
of every state encountered in real experience to the priority queue whether or not the value of the encountered state
was significantly changed. The second form of prioritized
sweeping, due to McMahan and Gordon (2005), and which
we call MG prioritized sweeping, puts each encountered
state on the queue, but not its predecessors. For McMahan and Gordon this resulted in a more efficient planner.
A complete specification of our feature-by-feature versions
of these two forms of prioritized sweeping are given above,
with TD(0) updates and gradient-descent model learning,
as Algorithms 2 and 3. These algorithms differ slightly
from previous prioritized sweeping algorithms in that they
update the value function from the real experiences and not
just from model-generated experience. With function approximation, real experience is always more informative
than model-generated experience, which will be distorted
by the function approximator. We found this to be a significant effect in our empirical experiments (Section 6).

Algorithm 4: Linear Dyna with MG prioritized sweeping
and TD(0) updates (control)
Obtain initial φ, θ, F, b
For each time step:

>
a ← arg maxa b>
(or -greedy)
a φ + γθ Fa φ
Take action a, receive r, φ0
δ ← r + γθ> φ0 − θ> φ
θ ← θ + αδφ
Fa ← Fa + α(φ0 − Fa φ)φ>
ba ← ba + α(r − b>
a φ)φ
For all i such that φ(i) 6= 0:
Put i on the PQueue with priority |δφ(i)|
Repeat p times while PQueue is not empty:
i ← pop the PQueue
ij
For all j s.t. there
 exists an>a s.t. F
 a 6= 0:
δ ← maxa ba (j) + γθ Fa ej − θ(j)
θ(j) ← θ(j) + αδ
Put j on the PQueue with priority |δ|
φ ← φ0

5

Theory for Control

We now turn to the full case of control, in which separate
models Fa , ba are learned and are then available for each
action a. These are constructed such that Fa φ and b>
a φ can
be used as estimates of the feature vector and reward that
follow φ if action a is taken. A linear Dyna algorithm for
the control case goes through a sequence of planning steps
on each of which a starting feature vector φ and an action
a are chosen, and then a next feature vector φ0 = Fa φ and
next reward r = ba φ are generated from the model. Given
this imaginary experience, a conventional model-free update is performed. The simplest case is to again apply
(1). A complete algorithm including prioritized sweeping
is given in Algorithm 4.
The theory for the control case is less clear than for policy evaluation. The main issue is the stability of the “mixture” of the forward model matrices. The corollary below
is stated for an i.i.d. sequence of features, but by the remark after Theorem 3.1 it can be readily extended to the
case where the policy to be evaluated is used to generate
the trajectories.
Corollary 5.1 (Convergence of linear TD(0) Dyna with
action models). Consider the Dyna recursion (4) with
the modification that in each step, instead of F φk ,
we use Fπ(φk ) φk , where π is a policy mapping feature vectors to actions and {Fa } is a collection of
forward-model matrices. Similarly, b> φk is replaced by
b>
π(φk ) φk . As before, assume that φk is an unspecified
i.i.d. process. Let (F, b)
 be the least squares
 model of
π: F = harg minG E kGφk − iFπ(φk ) φk k22 and b =
2
arg minu E (u> φk − b>
If the numerical radius
π(φk ) φk )
of F is bounded by one, then the conclusions of Theo-

-3

N

1
0
.
.
0
0

-3

N-1

-3

-3

.75
.25
.
.
0
0

-3

3

0
0
.
.
.5
.5

-3

-3

2

0
0
.
.
.25
.75

-2

1

0
0
.
.
0
1

0

0

0
0
.
.
0
0

Figure 1: The general Boyan Chain problem.
rem 3.1 hold: the parameter vector θk converges with probability one to (I − γF > )−1 b.
Proof. The proof is immediate
from
equation

 the normal


for F , which states that E F φk φ>
= E Fπ(φk ) φk φ>
k
k ,
and once we observe that, in the proof of Theorem 3.1, F
appears only in expressions of the form E F φk φ>
k .
As in the case of policy evaluation, there is a corresponding
corollary for the residual gradient iteration, with an immediate proof. These corollaries say that, for any policy with a
corresponding model that is stable, the Dyna recursion can
be used to compute its value function. Thus we can perform a form of policy iteration—continually computing an
approximation to the value function for the greedy policy.

6

Empirical results

In this section we illustrate the empirical behavior of the
four Dyna algorithms and make comparisons to model-free
methods using variations of two standard test problems:
Boyan Chain and Mountain Car. Our Boyan Chain environment is an extension of that by Boyan (1999, 2002)
from 13 to 98 states, and from 4 to 25 features (Geramifard, Bowling & Sutton 2006). Figure 1 depicts this environment in the general form. Each episode starts at state
N = 98 and terminates in state 0. For all states s > 2,
there is an equal probability of transitioning to states s − 1
or s − 2 with a reward of −3. From states 2 and 1, there are
deterministic transitions to states 1 and 0 with respective
rewards of −2 and 0. Our Mountain Car environment is exactly as described by Sutton (1996; Sutton & Barto 1998),
re-implemented in Matlab. An underpowered car must be
driven to the top of a hill by rocking back and forth in a
valley. The state variables are a pair (position,velocity) initialized to (−0.5, 0.0) at the beginning of each episode. The
reward is −1 per time step. There are three discrete actions
(accelerate, reverse, and coast). We used a value function
representation based on tile-coding feature vectors exactly
as in Sutton’s (1996) experiments, with 10 tilings over the
combined (position, velocity) pair, and with the tiles hashed
down to 10,000 features. In the policy evaluation experiments with this domain, the policy was to accelerate in

4

2

Boyan chain

10

9

Mountain Car

x 10

Dyna-Random
TD

7

1

Loss

Dyna-Random
Dyna-PWMA

Loss

10

5

TD

0

10

Dyna-MG

Dyna-PWMA

3

Dyna-MG
−1

10

0

20

40

60
Episode

80

100

1
0

200

400

600
Episode

800

1000

Figure 2: Performance of policy evaluation methods on the Boyan Chain and Mountain Car environments
the direction of the current velocity, and we added noise to
the domain that switched the selected action to a random
action with 10% probability. Complete code for our test
problems as standard RL-Glue environments is available
from the RL-Library hosted at the University of Alberta.
In all experiments, the step size parameter α took the form
0 +1
αt = α0 NN0 +t
1.1 , in which t is the episode number and
the pair (N0 , α0 ) was selected based on empirically finding the best combination out of α0 ∈ {.01, .1, 1} and
N0 ∈ {100, 1000, 106 } separately for each algorithm and
domain. All methods observed the same trajectories in policy evaluation. All graphs are averages of 30 runs; error
bars indicate standard errors in the means. Other parameter
settings were  = 0.1, γ = 1, and λ = 0.
We performed policy evaluation experiments with four algorithms: Dyna-Random, Dyna-PWMA, Dyna-MG (as in
Algorithms 1–3), and model-free TD(0). In the case of
the Dyna-Random algorithm, the starting feature vectors
in planning were chosen to be unit basis vectors with the 1
in a random location. Figure 2 shows the policy evaluation
performance of the four methods in the Boyan Chain and
Mountain Car environments. For the Boyan Chain domain,
the loss was the root-mean-squared error of the learned
value function compared to the exact analytical value, averaged over all states. In the Mountain Car domain, the
states are visited very non-uniformly, and a more sophisticated measure is needed. Note that all of the methods
drive θ toward an asymptotic value in which the expected
TD(0) update is zero; we can use the distance from this
as a loss measure. Specifically, we evaluated each learned
value function by freezing it and then running a fixed set
of 200,000 episodes with it while running the TD(0) algorithm (but not allowing θ to actually change). The norm of
the sum of the (attempted) update vectors was then computed and used as the loss. In practice, this measure can be
computed very efficiently as ||A∗ θ − b∗ || (in the notation of

LSTD(0), see Bradtke & Barto 1996).
In the Boyan Chain environment, the Dyna algorithms generally learned more rapidly than model-free TD(0). DynaMG was initially slower than the other algorithms, then
caught up and surpassed them. The relatively poor early
performance of Dyna-MG was actually due to its being
a better planning method. After few episodes the model
tends to be of very high variance, and so therefore is the
best value-function estimate given it. We tested this hypothesis by running the Dyna methods starting with a fixed,
well-learned model; in this case Dyna-MG was the best of
all the methods from the beginning. All of these data are
for one step of planning for each real step of interaction
with the world (p = 1). In preliminary experiments with
larger values of p, up to p = 10, we found further improvements in learning rate of the Dyna algorithms over TD(0),
and again Dyna-MG was best.
The results for Mountain Car are less clear. Dyna-MG
quickly does significantly better than TD(0), but the other
Dyna algorithms lag initially and never surpass TD(0).
Note that, for any value of p, Dyna-MG does many more θ
updates than the other two Dyna algorithms (because these
updates are in an inner loop, cf. Algorithms 2 and 3). Even
so, because of its other efficiencies Dyna-MG tended to run
faster overall in our implementation. Obviously, there is a
lot more interesting empirical work that could be done here.
We performed one Mountain Car experiment with DynaMG as a control algorithm (Algorithm 4), comparing it
with model-free Sarsa (i.e., Algorithm 4 with p = 0). The
results are shown in Figure 3. As before, Dyna-MG showed
a distinct advantage over the model-free method in terms
of learning rate. There was no clear advantage for either
method in the second half of the experiment. We note
that, asymptotically, model-free methods are never worse
than model-based methods, and are often better because the
model does not converge exactly to the true system because

7

−120
−140

Return

−160

Dyna-MG

−180
−200

Sarsa
−220
−240
−260
0

20

40

60
Episode

80

100

Figure 3: Control performance on Mountain Car

Conclusion

In this paper we have taken important steps toward establishing the theoretical and algorithmic foundations of
Dyna-style planning with linear function approximation.
We have established that Dyna-style planning with familiar
reinforcement learning update rules converges under weak
conditions corresponding roughly, in some cases, to the existence of a finite solution to the planning problem, and
that convergence is to a unique least-squares solution independent of the distribution used to generate hypothetical experience. These results make possible our second
main contribution: the introduction of algorithms that extend prioritized sweeping to linear function approximation,
with correctness guarantees. Our empirical results illustrate
the use of these algorithms and their potential for accelerating reinforcement learning. Overall, our results support
the conclusion that Dyna-style planning may be a practical
and competitive approach to achieving rapid, online control
in stochastic sequential decision problems with large state
spaces.
Acknowledgements

of structural modeling assumptions. (The case we treat
here—linear models and value functions with one-step TD
methods—is a rare case in which asymptotic performance
of model-based and model-free methods should be identical.) The benefit of models, and of planning generally, is in
rapid adaptation to new problems and situations.

The authors gratefully acknowledge the substantial contributions of Cosmin Paduraru and Mark Ring to the early
stages of this work. This research was supported by
iCORE, NSERC and Alberta Ingenuity.

These empirical results are not extensive and in some cases
are preliminary, but they nevertheless illustrate some of the
potential of linear Dyna methods. The results on the Boyan
Chain domain show that Dyna-style planning can result in
a significant improvement in learning speed over modelfree methods. In addition, we can see trends that have been
observed in the tabular case re-occurring here with linear
function approximation. In particular, prioritized sweeping can result in more efficient learning than simply updating features at random, and the MG version of prioritized
sweeping seems to be better than the PWMA version.

Atkeson, C. (1993). Using local trajectory optimizers to
speed up global optimization in dynamic programming.
Advances in Neural Information Processing Systems, 5,
663–670.
Baird, L. C. (1995). Residual algorithms: Reinforcement
learning with function approximation. In Proceedings of the Twelfth International Conference on Machine Learning, pp. 30–37.
Bertsekas, Dimitri P., Tsitsiklis. J. (1996). Neuro-Dynamic
Programming. Athena Scientific, 1996.
Boutilier, C., Dearden, R., Goldszmidt, M. (2000).
Stochastic dynamic programming with factored representations. Artificial Intelligence 121: 49–107.
Bowling, M., Geramifard, A., Wingate, D. (2008). Sigma
point policy iteration. In Proceedings of the Seventh
International Conference on Autonomous Agents and
Multiagent Systems.
Boyan, J. A. (1999). Least-squares temporal difference
learning. In Proceedings of the Sixteenth International
Conference on Machine Learning, 49–56.
Boyan, J. A. (2002). Technical update: Least-squares temporal difference learning. Machine Learning, 49:233–
246.
Bradtke, S., Barto, A. G. (1996). Linear least-squares al-

Finally, we would like to note that we have done extensive experimental work (not reported here) attempting to
adapt least squares methods such as LSTD to online control domains, in particular to the Mountain Car problem. A
major difficulty with these methods is that they place equal
weight on all past data whereas, in a control setting, the policy changes and older data becomes less relevant and may
even be misleading. Although we have tried a variety of
forgetting strategies, it is not easy to obtain online control
performance with these methods that is superior to modelfree methods. One reason we consider the Dyna approach
to be promising is that no special changes are required for
this case; it seems to adapt much more naturally and effectively to the online control setting.



This paper presents the first actor-critic algorithm for off-policy reinforcement learning.
Our algorithm is online and incremental, and
its per-time-step complexity scales linearly
with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not
take advantage of the recent advances in offpolicy gradient temporal-difference learning.
Off-policy techniques, such as Greedy-GQ,
enable a target policy to be learned while
following and obtaining data from another
(behavior) policy. For many problems, however, actor-critic methods are more practical
than action value methods (like Greedy-GQ)
because they explicitly represent the policy;
consequently, the policy can be stochastic
and utilize a large action space. In this paper,
we illustrate how to practically combine the
generality and learning potential of off-policy
learning with the flexibility in action selection
given by actor-critic methods. We derive an
incremental, linear time and space complexity algorithm that includes eligibility traces,
prove convergence under assumptions similar to previous off-policy algorithms1 , and
empirically show better or comparable performance to existing algorithms on standard
reinforcement-learning benchmark problems.

The reinforcement learning framework is a general
temporal learning formalism that has, over the last
1

See errata in section B

Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012.
Copyright 2012 by the author(s)/owner(s).

few decades, seen a marked growth in algorithms and
applications. Until recently, however, practical online
methods with convergence guarantees have been restricted to the on-policy setting, in which the agent
learns only about the policy it is executing.
In an off-policy setting, on the other hand, an agent
learns about a policy or policies different from the one
it is executing. Off-policy methods have a wider range
of applications and learning possibilities. Unlike onpolicy methods, off-policy methods are able to, for example, learn about an optimal policy while executing
an exploratory policy (Sutton & Barto, 1998), learn
from demonstration (Smart & Kaelbling, 2002), and
learn multiple tasks in parallel from a single sensorimotor interaction with an environment (Sutton et al.,
2011). Because of this generality, off-policy methods
are of great interest in many application domains.
The most well known off-policy method is Q-learning
(Watkins & Dayan, 1992). However, while Q-Learning
is guaranteed to converge to the optimal policy for the
tabular (non-approximate) case, it may diverge when
using linear function approximation (Baird, 1995).
Least-squares methods such as LSTD (Bradtke &
Barto, 1996) and LSPI (Lagoudakis & Parr, 2003) can
be used off-policy and are sound with linear function
approximation, but are computationally expensive;
their complexity scales quadratically with the number of features and weights. Recently, these problems
have been addressed by the new family of gradientTD (Temporal Difference) methods (e.g., Sutton et
al., 2009), such as Greedy-GQ (Maei et al., 2010),
which are of linear complexity and convergent under
off-policy training with function approximation.
All action-value methods, including gradient-TD
methods such as Greedy-GQ, suffer from three important limitations. First, their target policies are deterministic, whereas many problems have stochastic optimal policies, such as in adversarial settings or in par-

Off-Policy Actor-Critic

tially observable Markov decision processes. Second,
finding the greedy action with respect to the actionvalue function becomes problematic for larger action
spaces. Finally, a small change in the action-value
function can cause large changes in the policy, which
creates difficulties for convergence proofs and for some
real-time applications.
The standard way of avoiding the limitations of actionvalue methods is to use policy-gradient algorithms
(Sutton et al., 2000) such as actor-critic methods
(e.g., Bhatnagar et al., 2009). For example, the natural actor-critic, an on-policy policy-gradient algorithm, has been successful for learning in continuous
action spaces in several robotics applications (Peters
& Schaal, 2008).
The first and main contribution of this paper is to
introduce the first actor-critic method that can be applied off-policy, which we call Off-PAC, for Off-Policy
Actor–Critic. Off-PAC has two learners: the actor and
the critic. The actor updates the policy weights. The
critic learns an off-policy estimate of the value function for the current actor policy, different from the
(fixed) behavior policy. This estimate is then used
by the actor to update the policy. For the critic, in
this paper we consider a version of Off-PAC that uses
GTD(λ) (Maei, 2011), a gradient-TD method with eligibitity traces for learning state-value functions. We
define a new objective for our policy weights and derive
a valid backward-view update using eligibility traces.
The time and space complexity of Off-PAC is linear in
the number of learned weights.
The second contribution of this paper is an off-policy
policy-gradient theorem and a convergence proof for
Off-PAC when λ = 0, under assumptions similar to
previous off-policy gradient-TD proofs.2
Our third contribution is an empirical comparison of
Q(λ), Greedy-GQ, Off-PAC, and a soft-max version of
Greedy-GQ that we call Softmax-GQ, on three benchmark problems in an off-policy setting. To the best
of our knowledge, this paper is the first to provide
an empirical evaluation of gradient-TD methods for
off-policy control (the closest known prior work is the
work of Delp (2011)). We show that Off-PAC outperforms other algorithms on these problems.

1. Notation and Problem Setting
In this paper, we consider Markov decision processes
with a discrete state space S, a discrete action space A,
a distribution P : S × S × A → [0, 1], where P (s0 |s, a)
2

See errata in section B

is the probability of transitioning into state s0 from
state s after taking action a, and an expected reward
function R : S × A × S → R that provides an expected
reward for taking action a in state s and transitioning
into s0 . We observe a stream of data, which includes
states st ∈ S, actions at ∈ A, and rewards rt ∈ R for
t = 1, 2, . . . with actions selected from a fixed behavior
policy, b(a|s) ∈ (0, 1].
Given a termination condition γ : S → [0, 1] (Sutton et
al., 2011), we define the value function for π : S ×A →
(0, 1] to be:
V π,γ (s) = E [rt+1 + . . . + rt+T |st = s] ∀s ∈ S

(1)

where policy π is followed from time step t and terminates at time t + T according to γ. We assume
termination always occurs in a finite number of steps.
The action-value function, Qπ,γ (s, a), is defined as:
Qπ,γ (s, a) =
X
P (s0 |s, a)[R(s, a, s0 ) + γ(s0 )V π,γ (s0 )]

(2)

s0 ∈S

for
for all s ∈ S. Note that V π,γ (s) =
P all a ∈ A and
π,γ
(s, a), for all s ∈ S.
a∈A π(a|s)Q
The policy πu : A × S → [0, 1] is an arbitrary, differentiable function of a weight vector, u ∈ RNu , Nu ∈ N,
with πu (a|s) > 0 for all s ∈ S, a ∈ A. Our aim is to
choose u so as to maximize the following scalar objective function:
Jγ (u)

=

X

db (s)V πu ,γ (s)

(3)

s∈S

where db (s) = limt→∞ P (st = s|s0 , b) is the limiting
distribution of states under b and P (st = s|s0 , b) is
the probability that st = s when starting in s0 and
executing b. The objective function is weighted by
db because, in the off-policy setting, data is obtained
according to this behavior distribution. For simplicity
of notation, we will write π and implicitly mean πu .

2. The Off-PAC Algorithm
In this section, we present the Off-PAC algorithm in
three steps. First, we explain the basic theoretical
ideas underlying the gradient-TD methods used in the
critic. Second, we present our off-policy version of the
policy-gradient theorem. Finally, we derive the forward view of the actor and convert it to a backward
view to produce a complete mechanistic algorithm using eligibility traces.

Off-Policy Actor-Critic

2.1. The Critic: Policy Evaluation
Evaluating a policy π consists of learning its value
function, V π,γ (s), as defined in Equation 1. Since
it is often impractical to explicitly represent every
state s, we learn a linear approximation of V π,γ (s):
V̂ (s) = vT xs where xs ∈ RNv , Nv ∈ N, is the feature
vector of the state s, and v ∈ RNv is another weight
vector.
Gradient-TD methods (Sutton et al., 2009) incrementally learn the weights, v, in an off-policy setting,
with a guarantee of stability and a linear per-time-step
complexity. These methods minimize the λ-weighted
mean-squared projected Bellman error:
MSPBE(v) = ||V̂ − ΠTπλ,γ V̂ ||2D
where V̂ = Xv; X is the matrix whose rows are all xs ;
λ is the decay of the eligibility trace; D is a matrix with
db (s) on its diagonal; Π is a projection operator that
projects a value function to the nearest representable
value function given the function approximator; and
Tπλ,γ is the λ-weighted Bellman operator for the target
policy π with termination probability γ (e.g., see Maei
& Sutton, 2010). For a linear representation, Π =
X(X T DX)−1 X T D.
In this paper, we consider the version of Off-PAC that
updates its critic weights by the GTD(λ) algorithm
introduced by Maei (2011).

The two theorems below provide justification for this
approximation3 .
Theorem 1 (Policy Improvement). Given any policy
parameter u, let
u0 = u + α g(u)
Then there exists an  > 0 such that, for all positive
α < ,
Jγ (u0 ) ≥ Jγ (u)
Further, if π has a tabular representation (i.e., separate weights for each state), then V πu0 ,γ (s) ≥ V πu ,γ (s)
for all s ∈ S.
(Proof in Appendix3 ).
In the conventional on-policy theory of policy-gradient
methods, the policy-gradient theorem (Marbach &
Tsitsiklis, 1998; Sutton et al., 2000) establishes the relationship between the gradient of the objective function and the expected action values. In our notation,
that theorem essentially says that our approximation
is exact, that g(u) = ∇u Jγ (u). Although, we can not
show this in the off-policy case, we can establish a relationship between the solutions found using the true
and approximate gradient:
Theorem 2 (Off-Policy Policy-Gradient Theorem).
Given U ⊂ RNu a non-empty, compact set, let

2.2. Off-policy Policy-gradient Theorem

Z̃ = {u ∈ U | g(u) = 0}

Like other policy gradient algorithms, Off-PAC updates the weights approximately in proportion to the
gradient of the objective:
ut+1 − ut ≈ αu,t ∇u Jγ (ut )

(4)

where αu,t ∈ R is a positive step-size parameter. Starting from Equation 3, the gradient can be written:
"
#
X
X
b
π,γ
∇u Jγ (u) = ∇u
d (s)
π(a|s)Q (s, a)
s∈S

=

X
s∈S

db (s)

Z = {u ∈ U | ∇u Jγ (u) = 0}
where Z is the true set of local maxima and Z̃ the set
of local maxima obtained from using the approximate
gradient, g(u). If the value function can be represented
by our function class, then Z ⊂ Z̃. Moreover, if we
use a tabular representation for π, then Z = Z̃.
(Proof in Appendix3 ).

a∈A

X

[∇u π(a|s)Qπ,γ (s, a)

a∈A

+ π(a|s)∇u Qπ,γ (s, a) ]
The final term in this equation, ∇u Qπ,γ (s, a), is difficult to estimate in an incremental off-policy setting.
The first approximation involved in the theory of OffPAC is to omit this term. That is, we work with
an approximation to the gradient, which we denote
g(u) ∈ RNu , defined by
X
X
∇u Jγ (u) ≈ g(u) =
db (s)
∇u π(a|s)Qπ,γ (s, a)
s∈S

The proof of Theorem 2, showing that Z = Z̃, requires
tabular π to avoid update overlap: updates to a single
parameter influence the action probabilities for only
one state. Consequently, both parts of the gradient
(one part with the gradient of the policy function and
the other with the gradient of the action-value function) locally greedily change the action probabilities
for only that one state. Extrapolating from this result, in practice, more generally a local representation
for π will likely suffice, where parameter updates influence only a small number of states. Similarly, in the
non-tabular case, the claim will likely hold if γ is small

a∈A

(5)

3

See errata in section B

Off-Policy Actor-Critic

Algorithm 1 The Off-PAC algorithm
Initialize the vectors ev , eu , and w to zero
Initialize the vectors v and u arbitrarily
Initialize the state s
For each step:
Choose an action, a, according to b(·|s)
Observe resultant reward, r, and next state, s0
δ ← r + γ(s0 )vT xs0 − vT xs
ρ ← πu (a|s)/b(a|s)
Update the critic (GTD(λ) algorithm):
ev ← ρ (xs + γ(s)λev )

0
T
v ← v + αv δe
− λ)(w
ev )xs
 v − γ(s )(1

w ← w + αw δev − (wT xs )xs
Update thehactor:
i
u (a|s)
+ γ(s)λeu
eu ← ρ ∇πuuπ(a|s)
u ← u + αu δeu
s ← s0

(the return is myopic), again because changes to the
policy mostly affect the action-value function locally.
Fortunately, from an optimization perspective, for all
u ∈ Z̃\Z, Jγ (u) < minu0 ∈Z Jγ (u0 ), in other words,
Z represents all the largest local maxima in Z̃ with
respect to the objective, Jγ . Local optimization techniques, like random restarts, should help ensure that
we converge to larger maxima and so to u ∈ Z. Even
with the true gradient, these approaches would be incorporated into learning because our objective, Jγ , is
non-convex.
2.3. The Actor: Incremental Update
Algorithm with Eligibility Traces
We now derive an incremental update algorithm using
observations sampled from the behavior policy. First,
we rewrite Equation 5 as an expectation:
"
#
X
g(u) = E
∇u π(a|s)Qπ,γ (s, a) s ∼ db
a∈A

"

π(a|s) ∇u π(a|s) π,γ
Q (s, a) s ∼ db
=E
b(a|s)
b(a|s) π(a|s)
a∈A


= E ρ(s, a)ψ(s, a)Qπ,γ (s, a) s ∼ db , a ∼ b(·|s)
X

= Eb [ρ(st , at )ψ(st , at )Qπ,γ (st , at )]
∇u π(a|s)
where ρ(s, a) = π(a|s)
b(a|s) , ψ(s, a) = π(a|s) , and we introduce the new notation Eb [·] to denote the expectation implicitly conditional on all the random variables
(indexed by time step) being drawn from their limiting
stationary distribution under the behavior policy. A
standard result (e.g., see Sutton et al., 2000) is that an
arbitrary function of state can be introduced into these
equations as a baseline without changing the expected
value. We use the approximate state-value function
provided by the critic, V̂ , in this way:
h

i
g(u) = Eb ρ(st , at )ψ(st , at ) Qπ,γ (st , at ) − V̂ (st )

The next step is to replace the action value,
Qπ,γ (st , at ), by the off-policy λ-return. Because these
are not exactly equal, this step introduces a further
approximation:
h

i
[ = Eb ρ(st , at )ψ(st , at ) Rλ − V̂ (st )
g(u) ≈ g(u)
t
where the off-policy λ-return is defined by:
Rtλ = rt+1 + (1 − λ)γ(st+1 )V̂ (st+1 )
λ
+ λγ(st+1 )ρ(st+1 , at+1 )Rt+1

Finally, based on this equation, we can write the forward view of Off-PAC:


ut+1 − ut = αu,t ρ(st , at )ψ(st , at ) Rtλ − V̂ (st )

#

The forward view is useful for understanding and analyzing algorithms, but for a mechanistic implementation it must be converted to a backward view that
does not involve the λ-return. The key step, proved in
the appendix, is the observation that
h

i
Eb ρ(st , at )ψ(st , at ) Rtλ − V̂ (st ) = Eb [δt et ] (6)
where δt = rt+1 + γ(st+1 )V̂ (st+1 ) − V̂ (st ) is the conventional temporal difference error, and et ∈ RNu is
the eligibility trace of ψ, updated by:
et = ρ(st , at ) (ψ(st , at ) + λet−1 )
Finally, combining the three previous equations, the
backward view of the actor update can be written simply as:
ut+1 − ut = αu,t δt et
The complete Off-PAC algorithm is given above as Algorithm 1. Note that although the algorithm is written
in terms of states s and s0 , it really only ever needs
access to the corresponding feature vectors, xs and
xs0 , and to the behavior policy probabilities, b(·|s), for
the current state. All of these are typically available
in large-scale applications with function approximation. Also note that Off-PAC is fully incremental and
has per-time step computation and memory complexity that is linear in the number of weights, Nu + Nv .
With discrete actions, a common policy distribution
is the Gibbs distribution, which uses a linear combiT

nation of features π(a|s) =

eu φs,a
P uT φ
s,b
be

where φs,a are

state-action features for state s, action a, and where

Off-Policy Actor-Critic

P
u π(a|s)
ψ(s, a) = ∇π(a|s)
= φs,a − b π(b|s)φs,b . The stateaction features, φs,a , are potentially unrelated to the
feature vectors xs used in the critic.

3. Convergence Analysis
Our algorithm has the same recursive stochastic form
as the off-policy value-function algorithms
ut+1 = ut + αt (h(ut , vt ) + Mt+1 )
N

N

where h : R → R is a differentiable function and
{Mt }t≥0 is a noise sequence. Following previous offpolicy gradient proofs (Maei, 2011), we study the behavior of the ordinary differential equation
u̇(t) = u(h(u(t), v))
The two updates (for the actor and for the critic) are
not independent on each time step; we analyze two
separate ODEs using a two timescale analysis (Borkar,
2008). The actor update is analyzed given fixed critic
parameters, and vice versa, iteratively (until convergence). We make the following assumptions.
(A1) The policy viewed as a function of u, π(·) (a|s) :
RNu → (0, 1], is continuously differentiable, ∀s ∈
S, a ∈ A.
(A2) The update on ut includes a projection operator,
Γ : RNu → RNu , that projects any u to a compact set U = {u | qi (u) ≤ 0, i = 1, . . . , s} ⊂ RNu ,
where qi (·) : RNu → R are continuously differentiable functions specifying the constraints of the
compact region. For u on the boundary of U,
the gradients of the active qi are linearly independent. Assume the compact region is large enough
to contain at least one (local) maximum of Jγ .
(A3) The behavior policy has a minimum positive value
bmin ∈ (0, 1]: b(a|s) ≥ bmin ∀s ∈ S, a ∈ A
(A4) The sequence (xt , xt+1 , rt+1 )t≥0 is i.i.d. and has
uniformly bounded second moments.
(A5) For every u ∈ U (the compact region to which u
is projected), V π,γ : S → R is bounded.
Remark 1: It is difficult to prove the boundedness of
the iterates without the projection operator. Since we
have a bounded function (with range (0, 1]), we could
instead assume that the gradient goes to zero exponentially as u → ∞, ensuring boundedness. Previous
work, however, has illustrated that the stochasticity in
practice makes convergence to an unstable equilibrium
unlikely (Pemantle, 1990); therefore, we avoid restrictions on the policy function and do not include the
projection in our algorithm

Finally, we have the following (standard) assumptions
on features and step-sizes.
(P1) ||xt ||∞ < ∞, ∀t, where xt ∈ RNv
T

(P2) Matrices C = E[xt xt T ], A = E[xt (xt − γxt+1 ) ]
are non-singular and uniformly bounded. A, C
and E[rt+1 xt ] are well-defined because the distribution of (xt , xt+1 , rt+1 ) does not depend on t.
(S1) α
are deterministic P
such that
u,t > 0, ∀tP
Pv,t , αw,t , αP
2
α
=
α
=
α
=
∞
and
v,t
w,t
u,t
tP
t
t αv,t <
Pt 2
αu,t
2
∞, t αw,t < ∞ and t αu,t < ∞ with αv,t → 0.
.
(S2) Define H(A)
=
(A + AT )/2 and let
−1
λmin (C H(A)) be the minimum eigenvalue
of the matrix C −1 H(A)4 . Then αw,t = ηαv,t for
some η > max(0, −λmin (C −1 H(A))).
Remark 2: The assumption αu,t /αv,t → 0 in (S1)
states that the actor step-sizes go to zero at a faster
rate than the value function step-sizes: the actor update moves on a slower timescale than the critic update (which changes more from its larger step sizes).
This timescale is desirable because we effectively want
a converged value function estimate for the current
policy weights, ut . Examples of suitable step sizes are
1
αv,t = 1t , αu,t = 1+t1log t or αv,t = t2/3
, αu,t = 1t .
(with αw,t = ηαv,t for η satisfying (S2)).
The above assumptions are actually quite unrestrictive. Most algorithms inherently assume bounded features with bounded value functions for all policies;
unbounded values trivially result in unbounded value
function weights. Common policy distributions are
smooth, making π(a|s) continuously differentiable in
u. The least practical assumption is that the tuples
(xt , xt+1 , rt+1 ) are i.i.d., in other words, Martingale
noise instead of Markov noise. For Markov noise, our
proof as well as the proofs for GTD(λ) and GQ(λ),
require Borkar’s (2008) two-timescale theory to be extended to Markov noise (which is outside the scope of
this paper). Finally, the proof for Theorem 3 assumes
λ = 0, but should extend to λ > 0 similarly to GTD(λ)
(see Maei, 2011, Section 7.4, for convergence remarks).
We give a proof sketch of the following convergence
theorem5 , with the full proof in the appendix.
Theorem 3 (Convergence of Off-PAC). Let λ = 0 and
consider the Off-PAC iterations with GTD(0)6 for the
critic. Assume that (A1)-(A5), (P1)-(P2) and (S1)(S2) hold. Then the policy weights, ut , converge to
4

Minimum exists as all eigenvalues real-valued (Lemma 4)
See errata in section B
6
GTD(0) is GTD(λ) with λ = 0, not the different algorithm called GTD(0) by Sutton, Szepesvari & Maei (2008)
5

Off-Policy Actor-Critic

d = 0} and the value function
Ẑ = {u ∈ U | g(u)
weights, vt , converge to the corresponding TD-solution
with probability one.
Proof Sketch: We follow a similar outline to the
two timescale analysis for on-policy policy gradient
actor-critic (Bhatnagar et al., 2009) and for nonlinear
GTD (Maei et al., 2009). We analyze the dynamics
for our two weights, ut and zt T = (wt T vt T ), based on
our update rules. The proof involves satisfying seven
requirements from Borkar (2008, p. 64) to ensure convergence to an asymptotically stable equilibrium.

Behavior

Greedy-GQ

Softmax-GQ

Off-PAC

4. Empirical Results
This section compares the performance of Off-PAC to
three other off-policy algorithms with linear memory
and computational complexity: 1) Q(λ) (called QLearning when λ = 0), 2) Greedy-GQ (GQ(λ) with
a greedy target policy), and 3) Softmax-GQ (GQ(λ)
with a Softmax target policy). The policy in Off-PAC
is a Gibbs distribution as defined in section 2.3.
We used three benchmarks: mountain car, a pendulum
problem and a continuous grid world. These problems all have a discrete action space and a continuous state space, for which we use function approximation. The behavior policy is a uniform distribution
over all the possible actions in the problem for each
time step. Note that Q(λ) may not be stable in this
setting (Baird, 1995), unlike all the other algorithms.
The goal of the mountain car problem (see Sutton &
Barto, 1998) is to drive an underpowered car to the
top of a hill. The state of the system is composed of
the current position of the car (in [−1.2, 0.6]) and its
velocity (in [−.07, .07]). The car was initialized with
a position of -0.5 and a velocity of 0. Actions are a
throttle of {−1, 0, 1}. The reward at each time step
is −1. An episode ends when the car reaches the top
of the hill on the right or after 5,000 time steps.
The second problem is a pendulum problem (Doya,
2000). The state of the system consists of the angle (in
radians) and the angular velocity (in [−78.54, 78.54])
of the pendulum. Actions, the torque applied to the
base, are {−2, 0, 2}. The reward is the cosine of the
angle of the pendulum with respect to its fixed base.
The pendulum is initialized with an angle and an angular velocity of 0 (i.e., stopped in a horizontal position).
An episode ends after 5,000 time steps.
For the pendulum problem, it is unlikely that the behavior policy will explore the optimal region where the
pendulum is maintained in a vertical position. Consequently, this experiment illustrates which algorithms
make best use of limited behavior samples.

Figure 1. Example of one trajectory for each algorithm
in the continuous 2D grid world environment after 5,000
learning episodes from the behavior policy. Off-PAC is the
only algorithm that learned to reach the goal reliably.

The last problem is a continuous grid-world. The
state is a 2-dimensional position in [0, 1]2 . The actions are the pairs {(0.0, 0.0), (−.05, 0.0), (.05, 0.0),
(0.0, −.05), (0.0, .05)}, representing moves in both dimensions. Uniform noise in [−.025, .025] is added
to each action component. The reward at each
time step for arriving in a position (px , py ) is defined as: −1 + −2(N (px , .3, .1) · N (py , .6, .03) +
N (px , .4, .03)·N (py , .5, .1)+N (px , .8, .03)·N (py , .9, .1))
√
(p−µ)2
where N (p, µ, σ) = e− 2σ2 /σ 2π. The start position is (0.2, 0.4) and the goal position is (1.0, 1.0). An
episode ends when the goal is reached, that is when
the distance from the current position to the goal is
less than 0.1 (using the L1-norm), or after 5,000 time
steps. Figure 1 shows a representation of the problem.
The feature vectors xs were binary vectors constructed
according to the standard tile-coding technique (Sutton & Barto, 1998). For all problems, we used ten
tilings, each of roughly 10 × 10 over the joint space
of the two state variables, then hashed to a vector of
dimension 106 . An addition feature was added that
was always 1. State-action features, ψs,a , were also
106 + 1 dimensional vectors constructed by also hashing the actions. We used a constant γ = 0.99. All
the weight vectors were initialized to 0. We performed
a parameter sweep to select the following parameters:
1) the step size αv for Q(λ), 2) the step-sizes αv and
αw for the two vectors in Greedy-GQ, 3) αv , αw and
the temperature τ of the target policy distribution for
Softmax-GQ and 4) the step sizes αv , αw and αu for
Off-PAC. For the step sizes, the sweep was done over
the following values: {10−4 , 5 · 10−4 , 10−3 , . . . , .5, 1.}

Off-Policy Actor-Critic
Mountain car

0

Pendulum

3000

Continuous grid world

0
-2000

2000
-1000

-4000

1000

-3000

0

Average Reward

Average Reward

Average Reward

-6000

Behaviour
Q-Learning
Greedy-GQ
Softmax-GQ
Off-PAC

-2000

Behaviour
Q-Learning
Greedy-GQ
Softmax-GQ
Off-PAC

-1000
-2000

Behaviour
Q-Learning
Greedy-GQ
Softmax-GQ
Off-PAC

-8000
-10000
-12000

-3000

-14000

-4000
-4000
-5000

0

20

60

40

80

100

-5000

-16000

0

50

100
Time steps

Episodes

Mountain car
αw αu , τ αv λ
Reward
Behavior:

Q(λ):

200

-18000

0

1000

3000

2000

4000

Pendulum
Continuous grid world
αw αu , τ αv λ Reward αw αu , τ αv λ
Reward

final

na

na

na na

−4822±6

na

na

na na

−4582±0

na

na

na na −13814±127

na

na

na na

−4880±2

na

na

na na −4580±.3

na

na

na na

final

na

na

.1

.6

−143±.4

na

na

.5 .99

1802±35

na

na .0001

0

−5138±.4

overall

na

na

.1

0

−442±4

na

na

.5 .99

376±15

na

na .0001

0

−5034±.2

.0001

na

.1

.4

−131.9±.4

0

na

.5

.4

1782±31

0.05

1.0

.2

−5002±.2

.0001

na

.1

.2

−434±4

.0001

na

.01

.4

785±11

0

na .0001

0

−5034±.2

.0005

.1

.1

.4

−133.4±.4

0

.1

.5

.4

1789±32

.1

.2

.05 .005

overall
Softmax-GQ:final

na

−14237±33

50

.5

.6

−3332±20
−4450±11

overall

.0001

.1 .05

−470±7

.0001

.6

620±11

.1

50

.5

.6

final

.0001

1.0 .05

0 −108.6±.04

.005

.5

.5

0

2521±17

0

.001

.1

.4

−37±.01

1.0

0

−356±.4

0

.5

.5

0

1432±10

0

.001

.005

.6

−1003±6

overall

.001

.5

5000

Episodes

overall

Greedy-GQ: final

Off-PAC:

150

Figure 2. Performance of Off-PAC compared to the performance of Q(λ), Greedy-GQ, and Softmax-GQ when learning
off-policy from a random behavior policy. Final performance selected the parameters for the best performance for the
last 10% of the run, whereas the overall performance was over all the runs. The plots on the top show the learning curve
for the best parameters for the final performance. Off-PAC had always the best performance and was the only algorithm
able to learn to reach the goal reliably in the continuous grid world. Performance is indicated with the standard error.

divided by 10+1=11, that is the number of tilings
plus 1. To compare TD methods to gradient-TD methods, we also used αw = 0. The temperature parameter, τ , was chosen from {.01, .05, .1, .5, 1, 5, 10, 50, 100}
and λ from {0, .2, .4, .6, .8, .99}. We ran thirty runs
with each setting of the parameters.
For each parameter combination, the learning algorithm updates a target policy online from the data
generated by the behavior policy. For all the problems, the target policy was evaluated at 20 points in
time during the run by running it 5 times on another
instance of the problem. The target policy was not updated during evaluation, ensuring that it was learned
only with data from the behavior policy.
Figure 2 shows results on three problems. SoftmaxGQ and Off-PAC improved their policy compared to
the behavior policy on all problems, while the improvements for Q(λ) and Greedy-GQ is limited on the continuous grid world. Off-PAC performed best on all
problems. On the continuous grid world, Off-PAC was
the only algorithm able to learn a policy that reliably
found the goal after 5,000 episodes (see Figure 1). On
all problems, Off-PAC had the lowest standard error.

5. Discussion
Off-PAC, like other two-timescale update algorithms,
can be sensitive to parameter choices, particularly the
step-sizes. Off-PAC has four parameters: λ and the
three step sizes, αv and αw for the critic and αu for
the actor. In practice, the following procedure can
be used to set these parameters. The value of λ, as
with other algorithms, will depend on the problem and
it is often better to start with low values (less than
.4). A common heuristic is to set αv to 0.1 divided
by the norm of the feature vector, xs , while keeping
the value of αw low. Once GTD(λ) is stable learning
the value function with αu = 0, αu can be increased
so that the policy of the actor can be improved. This
corroborates the requirements in the proof, where the
step-sizes should be chosen so that the slow update
(the actor) is not changing as quickly as the fast inner
update to the value function weights (the critic).
As mentioned by Borkar (2008, p. 75), another scheme
that works well in practice is to use the restrictions
on the step-sizes in the proof and to also subsample
updates for the slow update. Subsampling updates
means only updating every {tN, t ≥ 0}, for some N >

Off-Policy Actor-Critic

1: the actor is fixed in-between tN and (t + 1)N while
the critic is being updated. This further slows the
actor update and enables an improved value function
estimate for the current policy, π.
In this work, we did not explore incremental natural
actor-critic methods (Bhatnagar et al., 2009), which
use the natural gradient as opposed to the conventional
gradient. The extension to off-policy natural actorcritic should be straightforward, involving only a small
modification to the update and analysis of this new
dynamical system (which will have similar properties
to the original update).
Finally, as pointed out by Precup et al. (2006), offpolicy updates can be more noisy compared to onpolicy learning. The results in this paper suggest that
Off-PAC is more robust to such noise because it has
lower variance than the action-value based methods.
Consequently, we think Off-PAC is a promising direction for extending off-policy learning to a more general
setting such as continuous action spaces.

6. Conclusion
This paper proposed a new algorithm for learning
control off-policy, called Off-PAC (Off-Policy ActorCritic). We proved that Off-PAC converges in a standard off-policy setting. We provided one of the first
empirical evaluations of off-policy control with the new
gradient-TD methods and showed that Off-PAC has
the best final performance on three benchmark problems and consistently has the lowest standard error.
Overall, Off-PAC is a significant step toward robust
off-policy control.

7. Acknowledgments
This work was supported by MPrime, the Alberta Innovates Centre for Machine Learning, the Glenrose Rehabilitation Hospital Foundation, Alberta Innovates—
Technology Futures, NSERC and the ANR MACSi
project. Computational time was provided by Westgrid and the Mésocentre de Calcul Intensif Aquitain.
Appendix: See http://arXiv.org/abs/1205.4839

