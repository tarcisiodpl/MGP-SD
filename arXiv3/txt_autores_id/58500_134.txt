

We propose a framework for building graph­
ical causal model that is based on the con­
cept of causal mechanisms. Causal models
are intuitive for human users and, more im­
portantly, support the prediction of the ef­
fect of manipulation. We describe an imple­
mentation of the proposed framework as an
interactive model construction module, Ima­
GeNie, in SMILE (Structural Modeling, In­
ference, and Learning Engine) and in GeNie
(SMILE's Windows user interface).

1

INTRODUCTION

Graphical probabilistic models, such as Bayesian net­
works and influence diagrams, have become popular
modeling tools for supporting decision making under
uncertainty. The normative character of the graphi­
cal decision models guarantees the correctness of the
inference procedure. Consequently, the quality of the
advice suggested by the models depends directly on
the requisiteness of the models. A model is requisite if
it contains everything that is essential for solving the
problem and no new insights about the problem will
emerge by elaborating on it (Philips 1982). To build
a requisite model requires human intuition and cre­
ativity since the notion of requisiteness is subjective.
Construction of graphical models, therefore, is labo­
rious and demanding in terms of domain expertise.
While support for obtaining model parameters, such
as prior and conditional probability distribution, has
received much attention in behavioral decision theory
literature (see von Winterfeldt and Edwards (1988) for
a review) and in artificial intelligence (Druzdzel & van
der Gaag 2000), relatively little work has been done on
composing model structure. At the same time, there
are strong indications that the quality of advice is more
sensitive to the model structure than to the precision

Tze Yun Leong
Medical Computing Laboratory
Department of Computer Science
School of Computing
National University of Singapore
Singapore 119260
leongty@comp. nus. edu. sg
of its numerical parameters (Pradhan et al. 1996).
There are essentially four approaches to aid model
building. The first approach focuses on providing more
expressive building tools. The Noisy-OR model (Pearl
1988; Henrion 1989) and its generalizations (Dlez 1993;
Srinivas 1993) simplify the representation and elici­
tation of independence interactions among multiple
causes. Beckerman (1990) developed the similarity
network and partition as tools for representing subset
independence to facilitate the structure construction
and probability elicitation. The second approach, usu­
ally referred to knowledge-based model construction
(KBMC), emphasizes aiding model building by auto­
mated generation of decision models from a domain
knowledge-base guided by the problem description and
observed information (see a special issue at the journal
IEEE Transactions on Systems, Man and Cybernetics
on the topic of KBMC (Breese, Goldman, & Wellman
1994)). The third approach focuses on algorithms that
can learn the model structure and parameters from a
database of observations (Cooper & Herskovits 1991;
Pearl & Verma 1991; Spirtes, Glymour, & Scheines
1993). Although model construction from data can
reduce the knowledge engineering effort, the learning
approach faces other problems such as small data sets,
unmeasured variables, missing data, selection bias, and
the flexibility of model granularity.
While we acknowledge that in the future it may be
possible to build powerful computer systems that will
model human creativity, sense for relevance, and sim­
plicity, we believe that these tasks are and will long be
performed better by humans. Our view is that model
building, a task that relies on all these capacities, is
best implemented as an interactive process. The fourth
approach on aiding model construction that is most re­
lated to our work is to apply system engineering and
knowledge engineering techniques for aiding the pro­
cess of building Bayesian networks. Laskey and Ma­
honey (1996; 1997) address the issues of modulariza­
tion, object-orientation, knowledge-base, and evalua-

354

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

tion in a spiral model of development cycle. Koller
and Pfeffer (1997; 1999) developed Object-Oriented
Bayesian Networks (OOBN) that use objects as orga­
nizational units to reduce the complexity of modeling
and increase the speed of inference.
Our approach on aiding model construction is based
on the concept of causal mechanisms. Causal mecha­
nisms, which are local interactions among domain vari­
ables, are building blocks that determine the causal
structure of a model. As they encode our understand­
ing of local interactions and are fairly model inde­
pendent, causal mechanisms can be easily reused in
various models. When the algebraic form of the in­
teraction is known, causal mechanisms are captured
by so called structural equations. When less informa­
tion is available about the interaction, it can be spec­
ified in a probabilistic format. As shown by Druzdzel
and Simon (1993), conditional probability tables in
Bayesian networks that model causal relations among
their variables can be also viewed as descriptions of
causal mechanisms. Similarly to object-hierarchy ab­
straction, causal mechanism can be organized hierar­
chically in nearly decomposable system (Iwasaki & Si­
mon 1994). At the same time they provide a valu­
able heuristic for acquiring and managing knowledge:
causality.
In our framework, we encode causal mechanisms as
functional relations among variables and, wherever
causal mechanisms are asymmetric, the direction of
causal influence among variables. We extend Simon's
causal ordering algorithm (Simon 1953) to develop a
modeling process that uses the output graph of this
algorithm in the interaction with users. We assist the
model building process by helping user (1) to identify
a set of mechanisms related to the current model and
to bring them into model workspace (2) to integrate
the newly added mechanisms with the model under
construction (3) to specify the variables that can be
manipulated, and (4) to extract reusable causal mech­
anisms from existing models into the knowledge base.
The final model structures generated by our modeling
process are guaranteed to be causal if the underlying
structural equations reflect causal mechanisms of the
modeled problem.
In addition to being intuitive for human users and facil­
itating crucial user interface functions such as explana­
tion, causal models support prediction of the effect of
manipulation, i.e., changes in structure (Simon 1953;
Spirtes, Glymour, & Scheines 1993; Pearl 1995). The
users of such models (and that includes autonomous
robots) can ask questions like "What will happen if
I perform action A?" Manipulation is especially im­
portant in strategic planning, where it is important to
derive creative decision options and not only evaluate

existing decision options. In the process of creating a
model, a user may want to explore the possibility of
manipulating its different elements. Supporting this
manipulation is not straightforward, as some mecha­
nisms may be reversible, i.e., acting in reverse direc­
tion. For example, when driving up the hill, car engine
causes the wheels to turn; but when driving down the
hill in a low gear, the model should be able to predict
that the wheels will cause the engine to slow down.
Our approach supports causal modeling that includes
reversible causal mechanisms and offers an integrated
framework for building and using causal models.
The remainder of this paper is structured as follows.
Section 2 gives an overview of structural equation mod­
els, causal mechanisms, and how these support changes
in structure. Section 3 discusses the process of inter­
active model construction, including issues related to
the representation of causal mechanism, assistant in­
terface, and the extension of causal ordering algorithm.
Section 4 presents an example of user interaction with
our system, ImaGeN/e. F inally, we discuss the impli­
cations of our approach and outline the direction for
our future work.
2

STRUCTURAL EQUATION
MODELS

When scientists study phenomena or problems, they
normally focus on systems, pieces of the real world
that can reasonably be studied in isolation. Scien­
tists identify the relevant variables, the ranges of the
variables' values, and the relations among variables to
form abstractions of these systems, known as mod­
els. One way of representing models is by systems
of structural equations where each structural equa­
tion describes a conceptually distinct causal mecha­
nism active in the system. Such systems are known as
Structural Equation Models (SEMs) (Haavelmo 1943;
Simon 1953). A structural equation describing a causal
mechanism M is often encoded as an implicit function

where f is some algebraic function and its arguments
V; are variables that directly participate in the mech­

anism M.
A variable in a SEM is exogenous if it summarizes
an outside influence on the system, i.e., its value is
determined outside of the model. An exogenous vari­
able is truly exogenous if it represents a variable in the
real world system that we cannot manipulate without
changing the boundaries of the system. An exogenous
variable is a policy variable if it represents a variable
that we can manipulate, i.e., set its value. For exam­
ple, we normally model outside temperature as a truly

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

exogenous variable in an agricultural model, but we
can model the temperature as a policy variable in a
model of a greenhouse. For each exogenous variable,
there is a value assignment structural equation to des­
ignate the observed value ( or a probability distribution
over observed values) for the truly exogenous variable
or the chosen value for the policy variable. A variable
in a SEM is endogenous if its value is derived by substi­
tuting the values of exogenous variables into the core
structural equations that depict the relations among
modeled variables in the system and by solving these
equations in SEM .
A SEM S with m causal mechanisms and n variables
is represented as
m
S

=

U fM,(Vl, V2, V3, ... , Vn).
i=l

Since the knowledge of which variables participate in
which mechanisms is sufficient to determine the di­
rection of causation, 1 in the remainder of this paper
we will only use structure matrix (Druzdzel & Simon
1993), a qualitative representation of a SEM.
Definition 1 (structure matrix) A structure ma­
trix A of a SEM S = U:1 fM,(Vl, V2, V3, . .., Vn) = 0
is a m x n matrix with element a;j = x if Vj participates
in f M;, where x is a marker, and a;j = 0 otherwise.
Let Amxn be the structure matrix of a SEM S with m
equations and n variables. S is non-over-constrained
if following property holds.
Definition 2 (non-over-constrained system) A
system of m structural equations S is non-over­
constrained if in any subset of k ::; m equations of
S at least k different variables appear with nonzero co­
efficients.
A non-over-constrained Amxn is self-contained if m =
n. A non-over-constrained Amxn is under-constrained
if m < n. Amxn is over-constrained if it violates non­
over-constrained property.
Example:

The University Performance Budget Planning
Model (UPBPM) (Simon, Kalagnanam, & Druzdzel 2000)
is comprised of 38 core equations that describe interactions
among 88 variables in the university strategic budget plan­
ning context. The model has been adopted by the Office
for Planning and Budget at Carnegie Mellon University for
the purpose of strategic planning of university operations.

The following simple model, StudentFacultyRatio model,
extracted from UPBPM, consists of one core equations and
two value assignment equations and describes the interac­
tion among three variables: StudentFacultyRatio (SFR),
NumberOJStudents (NS), and NumberOfFaculty (NF).

1 Only when calculating the strength of the influences,
we need the exact form of equations.

355

The corresponding structure matrix for this self-contained
model is shown at the right hand side.

{"

/2:
h:

2.1

NS
NF
SFR

=
=
=

22102
3006
NSjNF

NS
h
h
h

X

0

X

NF
0
X
X

SFR
0
0
X

0

Causal Ordering

As shown by Simon (1953), a self-contained SEM ex­
hibits asymmetries that can be represented by a di­
rected acyclic graph and interpreted causally. Simon
developed a causal ordering algorithm that takes a self­
contained structure matrix A as input and outputs
a causal graph G = {N(G), A(G)}, where the nodes,
N(G), are sets of variables and the arcs, A (G), describe
causal relations among them.
Let B be a subset of equations in a non-over­
constrained SEM and Cpxq be the structure matrix
of B. We say that B is a self-contained subset if
p = q; B is a under-constrained subset if p < q. A
self-contained subset is minimal if it does not contain
any self-contained ( proper) subsets itself. A minimal
self-contained subset is a strongly coupled component
if it contains more than one equation, which usually
represents a feedback system in the real world.
The causal ordering algorithm starts with identifying
the minimal self-contained subsets in input A. These
identified minimal self-contained subsets are called
complete subsets of 0-th order and a node is created
for each subset. Next, the algorithm removes the equa­
tions of the complete subsets of 0-th order from A as
solving the values of variables. Then it removes all
variables that occur in the complete subsets of 0-th or­
der from the remaining equations in A as substituting
the values of solved variables into remaining equations.
The remaining set of equations is called the derived
system of first order, a self-contained structure. The
algorithm repeats the process of identifying, solving,
and substituting on the derived system of k-th order
until it is empty. In addition, whenever a node m is
created for a minimal self-contained subset M, the al­
gorithm refers the set of equations EM of M back to
the original set of equations OEM in A and adds arcs
from the nodes representing variables in OVM \ VM to
m, where VM is the set of variables participating in
EM and OVM is the set of variables participating in
OEM·

Example:
The UPBPM (Simon, Kalagnanam, &
Druzdzel 2000) implements Simon (1953) causal ordering
algorithm that given an assignment of values to 50 exoge­
nous variables, derives the structure of the model.
When applying the causal ordering algorithm to the struc­
ture matrix of StudentFacultyRatio model, we first identify

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

356

h and h as the complete subset of 0-th order. After
solving and substituting of NS and NF, we then identify

NS

f� as the complete subset of 1-st order. The structure ma­

X

trix and the corresponding causal graph are shown below.

NS

NF

SFR

0

0

X

0
0

X

X

X

NF

SFR

0

0

X

X

X

0

0

X

()liL-D NF

~

In the revised system, the causal ordering shows that NF
has become an endogenous variable affected by NS and
SFR. Now, changing the number of students will affect

X

the number of faculty. Manipulation has lead to a change
in structure.

D

Given the causal graph, we can read off the causal rela­
tions among the nodes by focusing on the node of interest
and its parents. For example, SFR directly depends on
D

NF and NS.

Notice that the causality that we read off causal graphs
is defined within models and causal asymmetries arise
when mechanisms are placed in context. If the context
has changed, it may result in changes in structure.
2.2

Changes in Structure

The main value of structural equation models is that
they support prediction of the effects of changes in
structure, i.e., external manipulations that intervene
in the mechanisms captured by the original system of
equations. Such changes are modeled by modifying
the equations that describe the affected mechanisms
and leaving those equations that correspond to unaf­
fected mechanisms unmodified. The causal ordering
algorithm applied to the modified SEMs derives the
new causal structure of the system.
Normally, the effect of external manipulation is lo­
cal and, when related back to the graph, amounts to
arc cutting (Pearl 1995; Spirtes, Glymour, & Scheines
1993). The assumption underlying the arc-cutting op­
eration is that imposing a value on a variable by an
external intervention makes that variable independent
of its direct causes. This assumption is valid for mech­
anisms with strong asymmetric relationship between
a variable and its causes; for example, wearing sun­
glasses protects our eyes from the sun but it does
not make the sun go away. However, when a model
contains reversible causal mechanisms (Simon 1953;
Druzdzel & van Leijen 2000), manipulation can have
a drastic effect on the graph.
Example: From the causal graph of StudentFacultyRa­
tio model in previous example, we k now that changing NS
will affect SFR but not NF. Now, consider that the budget
planning officer would lik e to set the StudentFacultyRa­
tio to advertise their faculty availability. If needed, she
is willing to adjust the NumberOfFaculty (e. g. , hire more
faculty) . According to the revised modeling context, she
needs to designate the variable SFR as exogenous, e. g. ,
j4 : SFR = 10, and release h : NF
3, 006. The result­
ing structure matrix and corresponding causal graph are:
=

Figure 1: Interactive and Iterative Model Construction
System Architecture. The arcs show the direction of
the information flow.

3

INTERACTIVE MODEL
CON STRUCTION

We have developed an interactive and iterative model
construction environment, ImaGeNie, that assists
users in building graphical decision model in causal
form. We use the causal ordering algorithm to gen­
erate the causal model structures which can later be
associated with different node types and parameters
and transformed into Bayesian networks or influence
diagrams. Figure 1 shows the architecture of ImaGe­
N/e. It includes three knowledge structures: mech­
anism knowledge bases, which hold domain knowl­
edge expressed as causal mechanisms, model build­
ing workspace, which serve as a blackboard for model
composition, and models. The domain knowledge
can be maintained either by the equation authoring
interface, where model builders can compose struc­
tural equations directly, or by the mechanism ex­
traction operation that enables model builders to ex­
tract reusable causal mechanisms from existing mod­
els. Model builders can use hierarchy navigation in­
terface to locate the mechanisms of interest and select

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

them into the model building workspace with assis­
tance of the mechanism selection operation. In ad­
dition to mechanism selection and traditional model
authoring operations, model builders can manipulate
variables and merge mechanisms as model building
process evolves. The underlying causal ordering mod­
ule will restructure the models according to the user
actions.
3.1

Knowledge Representation

In ImaGeNie, the fundamental knowledge representa­
tion units are causal mechanisms, which are encoded as
structural equations. For example, we can specify the
student faculty ratio as h (S F R, NS, N F ). Users may
optionally provide explicit functions for causal mech­
anisms such as algebraic functions, conditional prob­
ability tables, truth tables, value/utility tables, and
choice tables.
While most mechanisms will be described in one, per­
haps their only, mode of operation, some mechanisms
are reversible in the sense of being flexible as to the
direction of causality that they imply when they are
embedded in different contexts. We define the manip­
ulativeness and observability for each variable in our
domain knowledge base to express the characteristics
of the variable that may aid in the process of model
building. Along with the manipulativeness character­
istic, a variable can be truly exogenous, manipulatable,
or truly endogenous. For the sun and sunglasses exam­
ple, we may use two structural equations !5(S, G) and
f6(S) to describe causal relation between S and G and
assign S as a truly exogenous variable to express the
fact that it is impossible to manipulate the sun in the
current modeling domain. We may assign G as manip­
ulatable to designate it as a potential policy variable.
A variable is truly endogenous if its value has to be de­
rived from embedded mechanisms. The observability
is important in deciding whether adding this variable
(observable or unobservable) will be of benefit to the
model. In the diagnostic domain, it may be desired to
develop the cost model that can associate manipulation
cost/ observation cost with manipulatable/observable
variables.
Our domain knowledge base is organized as a hierar­
chical system that consists of subsystems and causal
mechanisms as its fundamental building elements. The
hierarchical approach not only helps domain experts to
express their domain knowledge in cognitively mean­
ingful units but also helps knowledge engineers to
access stored mechanisms easily. Our approach is
similar to type-hierarchy in (Koller & Pfeffer 1997;
Laskey & Mahoney 1997) but without imposing the
inheritance constraint since knowledge can be pos­
sibly organized hierarchically from different perspec-

357

tives. More details on the syntax of our knowledge
representation language can be found in (Lu 1999).
3.2

Extending Causal Ordering to
Under-constrained Model

In ImaGeNie, the model construction process is a re­
flection of our problem solving. The under-constrained
models evolved in such process reveal different problem
recognition stages. In an under-constrained model, the
mechanisms are our observations of how the problem
should be described so far. Model building process is
strongly related to causal manipulation. The exoge­
nous variables are those outside influences that have
been committed. An under-constrained model can­
not be drawn as a directed acyclic graph, as the di­
rection of causal interactions is not completely deter­
mined until the model is self-contained. However, it
is desired to have a graphical representation of under­
constrained models during the whole process of model
construction, since the graphical representation may
help model builder identify her focus and change her
commitments of the outside influences. We extend Si­
mon's causal ordering algorithm to explicate the causal
ordering that has been identified in under-constrained
models. We also propose a graphical representation
to depict the causal ordering results in an informative
graphical form that aims to help user in model build­
ing.
In order to formalize our extensions, we need to re­
state the theorem that was originally proved by Simon
(1953).

Theorem 1 Let A and B be two minimal self­
contained subsets of equations of a non-over­
constrained SEM, S. Then the structural equations
of A and B, and likewise the variables in A and B are
disjunct.
Consider any subset B of the equations of a non-over­
constrained SEM. We will denote the number of equa­
tions in Bas ne8, and the number of variables appear­
ing in B as nvB.

Theorem 2 Let S be a non-over-constrained system
and D be the derived system of structural equations
from S by applying identification, solving, and sub­
stitution. If D is not empty, then D is non-over­
constrained.
Proof:

In the process of identification, let M be the
union of all the minimal self-contained subsets, M = M 1 U
M2 U ... U Mk, and the remainder R. We k now R is not
empty since V is not empty.
Suppose that V violates the non-over-constrained property.

Then there exists a subset £' of V such that net:' >

nvt:'·

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

358

Let & be the subset of R that &' derives from. We know

M U &.
The equations of M and & are disjunct because M and R
that

ne£

=

Now, consider the subset :F

ne£1·

are disjunct and &

neM + ne£1.

� R. Therefore,

ne;:

=

=

neM + ne£

=

Since &' derives from & by substitution, the

variables appearing in & are either in

M

or in &'.

sequently, the variables in :F are either in
Moreover, the variables in

M

M

Con­

or in &'.

and &' are disjunct because

M.
Therefore, nv;: = nvM + nV£1. Since the equations of M ,
i
and likewise the variables in M , are disjunct by Theo­
i
&' derives from & by substituting out the variables in

nVM = L;nvM, and neM = L;neM;·
nVM = neM. Therefore, ne;: = neM + ne£ =
neM + ne£1 > neM + nV£1 = nvM + nV£1 = nv;:, i.e., the
rem 1, we have
Hence

Procedure ExtendedCausalOrdering
Input:
A structure
constrained SEM.

V0

Definition 3 (strictly under-constrained sub­
sets) The strictly under-constrained subsets of a non­
over-constrained SEM are those under-constrained
subsets that do not contain any self-contained subsets.
Theorem 3 A SEM, S, is under-constrained if and
only if there exists a derived strictly under-constrained
subset inS.
Proof (sketch):

We can prove => by construction and

¢:: by contradiction given Theorem
the formal proof.

2.

See (Lu

1999)

Mi

(a) Create nodes

vM•. ·
J

non-over-

Vi,

Mi
Vi

is
is

Nj for all variables in

(b) Add arcs from the nodes represent

OVMJ'. \ VMJ'. to nodes in Nj, where
OVMJ'. is the set of the variables

D

Given Theorem 2, we can keep applying identifica­
tion, solving, and substitution operations on derived
non-over-constrained system until either V is empty
or there are no more minimal self-contained subsets
that can be identified. If V is empty, we know that S
is self-contained. If V is not empty and no more self­
contained subsets can be identified, we know that S
is under-constrained and we call V the derived strictly
under-constrained subsets.

a

1. for each minimal self-contained subset
Mj E Mi, where 1:::; j:::; IMil

2 contradicting the fact that S is non-over-constrained.

V must be non-over-constrained.

of

Let i := 0 and
:= A
while there exists
c
where
the complete subset of i:th order and
the derived structure of i-th order.

variables of :F. In other words, the set :F violates Defini­
We conclude that

A

Output: A graph G = {V, A( G)}, where V are
the variables in A and A( G) is a set of directed, hi­
directed, or undirected arcs.

number of equations of :F is greater than the number of
tion

matrix

in

OEM'.,
J

equations
(c) if

INJI

the original equations of

EM'.
J

>

1,

of

Mj

in A.

add pair-wise hi­

directed arcs among elements of

Nj.

Mi from Vi to deriveR; (solv­
Mi from
Ri to derive V (substituting).
3. Let i := i + 1 and Vi : = V.
if vi is not empty
2.

Remove

ing) and remove variables of

for each remaining equation
where 1:::; k:::;

vi,
1.

ek in

IV'I
Create nodes Nek for the
of variables, v.k' in ek.

set

2.

Add arcs from nodes repre­
senting
to

3.

Add pair-wise undirected arcs
between nodes

OV.k \ Vek

N.k.

N.k.

Figure 2: Extended Causal Ordering Algorithm

for
D

Figure 2 outlines our extended causal ordering algo­
rithm that is based on Theorem 3. The input of the
algorithm is a non-over-constrained structure matrix
A. The output is a graph G = {V, A(G)}, where the
nodes V are variables and A(G) is a set of directed,
hi-directed, or undirected arcs. The algorithm essen­
tially follows the steps of identification, solving, and
substitution as Simon's causal ordering algorithm un­
til there are no more self-contained subsets that can be
identified from the derived system. The algorithm will
explicitly depict the causal relations and relevant rela­
tions encoded in the strictly under-constrained subset,
if there remains one.
The graph generated by our extended causal ordering
algorithm is specifically designed to aid the process of

model construction. Unlike the original causal order­
ing algorithm, each variable in the system is repre­
sented as a separate node so that the model builder
can access and manipulate it directly. Directed arcs
depict the causal relations among variables. In addi­
tion to these, our algorithm explicates the causal re­
lations encoded in the under-constrained system. Bi­
directed arcs denote feedback mechanisms in strongly­
coupled subsets. User can visualize the effect of break­
ing the feedback system by manipulating one of vari­
ables connected by the hi-directed arc. Undirected arcs
visually express relevant but undetermined causal rela­
tions among variables so that model builder can focus
on clarifying the mechanisms governing these variables
and complete the model.
Example: Suppose the budget planning officer wants to

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

extend the StudentFacultyRatio model to take the average
class size into account. She adds the structural equations
h : CS = (NS * CL)/(NF * TL) and Is : CL = 15 to
describe the relations among ClassSize ( CS), ClassLoad
( CL), TeachingLoad ( TL), NS and NF. The structure ma­
trix of the extended model is as follows.

NS
/1
h
h

h

Is

X

NF
0

0

X

X

X

X

X

0

0

SFR
0
0

cs

0
0

X

X

0
0
0

CL
0
0
0

TL
0
0
0

0

X

0

X

X

After applying the extended causal ordering algorithm to
the system, she obtains the following under-constrained
causal graph.

359

nipulatable variables as exogenous helps in obtaining
a self-contained system, i.e., orienting all arcs in the
model graph. If the user assigns a potential policy
variable, a manipulatable variable that is endogenous
in a self-contained system, as exogenous, the whole
model becomes over-constrained, because the num­
ber of equations is greater than the number of vari­
ables. We allow a model to be under-constrained or
self-contained at any stage of the model development
in ImaGeNie, but we disallow a model to be over­
constrained. W hen a model becomes over-constrained,
the system pops up a list of mechanisms that are
currently in the model and asks users to release one
of them in order to change the system into a self­
contained or an under-constrained system.

CL
4

SESSION

TL
From the under-constrained causal graph, she can read
off the current stage of problem formulation as follows:

StudentFacultyRatio is determined by NumberOIStudents
NumberOfFaculty; currently both ClassSize and
TeachingLoad depend on NumberOfStudents, Num­
berOfFaculty, and ClassLoad, but the relation between
ClassSize and TeachingLoad is not yet determined, which

and

is the consequence of the fact that the system is still
under-constrained.
3.3

EXAMPLE MODEL BUILDING

D

We continue on extending our simple model to demon­
strate how to interact with ImaGeNie to build a sim­
plified university budget model from U P B PM knowl­
edge base encoded in ImaGeN/e. Suppose the officer
has designated TL variable as exogenous with equa­
tion fg : TL 6. Figure 3 shows ImaGeNie interface
with the navigation tree of the knowledge base and the
model we have built so far in the workspace.
=

'f GeNie GeNie1

PI�

Modeling Process

The modeling process starts with an initial focus,
which is normally, in the spirit of value-focused think­
ing (Keeney 1994), the value variable. Users can also
start with other focus variables, for example decision,
observation, and whatever else is relevant or impor­
tant a-priori. W ith the assistant interface, users can
interactively browse the mechanisms related to their
focus variables, select those that best depict the prob­
lem at hand, merge them, or specify exogenous vari­
ables to set the boundary of the system. However, we
suggest the users to focus on one variable and add rel­
evant mechanisms one at a time as the model evolves,
since it resembles the action of focusing on a variable
of interest, explaining or observing it in terms of its
underlying mechanism. The user repeats the process
iteratively until the model is requisite. In other words,
users make decisions on the level of granularity and
when to stop with the model building process. The
system only plays the passive role of an assistant: sug­
gesting mechanisms to choose from, indicating the pos­
sible mechanisms to merge, and denoting the manipu­
latable variables.
Normally a model evolves from an under-constrained
system to a self-contained system. Designating rna-

Ur�ver:sity_Mecharum_�
Smple Unversity �
UnivetsityCore
ClassS�te

Figure 3: ImaGeNie Interface: Navigation Tree and
the Graphical Model Including Equations: JI, ]2, h ,
h, fs, and fg.
Suppose she would like to plan the expenses related to
faculty salary. She may use the navigation tree to lo­
cate mechanisms for faculty salary. Suppose she identi­
fies the mechanism f10: FS =(OJ +TA*NS)j(NF*
( 1 + 0)) that describes the interactions among vari­
ables: FacultySalary (FS), Otherlncome (OJ), Tu­
itionAmount (TA), Overhead (0), NS and N F . She
drags it into the workspace. In order to maintain the
unique variable identifiers in the model, ImaGeNie au­
tomatically renames the NS and N F into NSO and

360

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

NF O. The extended causal ordering algorithm gener­
ates the graph shown in Figure 4.
IJGeNiel

1!!11!1£1

Figure 4: Model builder selects and drags !Io into
workspace; the extended causal ordering algorithm
generates a corresponding graph.
She can then integrate the added mechanism with the
model by merging NS to NSO and NF to NFO. ( See
Figure 5).
iii! GeNiel

2000

that describes the dependence relations among those
variables of interests ( See Figure 6 top). She can now
read off the following dependency relations from the
complete model:
•

Faculty salary is determined by the number of stu­
dents, the number of faculty, tuition amount, other
income, and overhead.

•

Student-faculty ratio is determined by the number of
students and the number of faculty.

•

Class size is determined by the number of students,
the number of faculty, class load, and teaching load.

After inspecting the current self-contained model, she
would like to analyze the model under the condition
that the average class size is fixed at 15 students per
class. She makes the variable CS exogenous by spec­
ifying a value assignment equation as fi4 : CS = 15.
Consequently the original self-contained model will be­
come over-constrained. ImaGeNie will ask her to re­
lease one of the equations ( Figure 6 top). Suppose that
she chooses to release the value assignment equation
for the variable T L. The resulting graph generated by
the causal ordering is shown in Figure 6 bottom.

IIIII!JE'l

CS •INS "Cl.)/(NF"TLJ
.,e;;:___..c;;:__-j��:(�JtJA"NS)I(NF•(1 +0))
CL • 15
NF • 3006
01

ill GeNiel

•

30000000

I!II(!J £l

Figure 5: Model builder performs the merge opera­
tions for NS (top). The causal ordering generates the
corresponding graph (bottom).

Figure 6: A change in structure on a self-contained
model. The user manipulates CS by setting JI4 :
CS = 15 and releasing f9: TL = 6 (top). The causal
ordering generates the corresponding graph (bottom).

She then makes TA , 0, and OJ exogenous by assigning
equations: fn : TA = 1, 200, !I2 : 0 = 0.48, and
JI3 : OJ= 30,000,000 and obtains a complete model

Now, she can read off the local effect of her change
on the system from the causal graph: teaching load is

361

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

determined by the number of students, the number of
faculty, class load, and class size.

ImaGeNie also supports model builder in extract­
ing reusable mechanisms from the workspace into the
knowledge base. Model builder simply selects the
nodes of interest and drags them into the destination
branch in the navigation tree. Due to space limita­
tions, we are omitting this example.
5

DISCUSSION A N D FUTURE
WORK

Support for building model structure is one of the
best ways of improving the quality of advice based on
decision-theoretic models. While existing approaches
focus on automatic model construction either from
knowledge base or directly from data, our approach
favors a closely-coupled loop between the system and
its user. This is based on our belief that human judge­
ment with respect to relevance, model size, complete­
ness, and granularity is more reliable. Built on the
assumption that under-constrained models reflect our
problem recognition stages, ImaGeNie assists users in
encoding their conceptual problem framing in a causal
graph generated by the extended causal ordering al­
gorithm. Furthermore, ImaGeNie provides users with
the flexibility to choose building blocks from knowl­
edge base to extend the model, to manipulate the
variables in order to observe the effect of intervention
(structure changes), and to extract reusable mecha­
nisms from existing models to knowledge bases. The
concept of causal mechanisms, on which ImaGeNie re­
lies, provides a general mean to accommodate different
forms of knowledge description and makes knowledge
acquisition task easier.
Recent research in applying the object-oriented frame­
work to extend Bayesian networks for modeling com­
plex domains (Koller & Pfeffer 1997; Laskey & Ma­
honey 1997; Pfeffer et al. 1999) is closely related to
our work. Each of these approaches organizes do­
main knowledge into a hierarchical system. In Object­
Oriented Bayesian Network ( O O B N), the domain
knowledge is structured explicitly as class-hierarchy
for the type system and as object-hierarchy for the
real model. In our framework, we do not impose any
constraint on how users should organize their domain
knowledge in the knowledge base. In the future, we
would like to explore the semantics for combining type
system with causal mechanisms so that our knowledge
base can efficiently store the domain knowledge and be
effectively used by users. As for the constructed mod­
els, ImaGeNie provides submodels to group nodes into
a graphical organization unit for the sake of succinct
presentation, but there is no special semantic meaning

attached to submodels in terms of inference. We plan
to impose d-sepset (Xiang, Poole, & Beddoes 1993)
constraint on submodels composition such that each
submodel has well defined 1/0 sets to resemble object
hierarchy in O O B N.
Once the model structures generated from our frame­
work are associated with variable ranges and their nu­
merical parameters such as explicit equations or con­
ditional probability tables ( CP Ts), manipulation on
the model may invalidate these numerical parameters.
Druzdzel and van Leijen (2000) have shown the special
conditions under which the CP Ts in Bayesian networks
can be reversed under manipulation. As for the ex­
plicit equations, ImaGeNie tries to solve the manipu­
lated system symbolically if there exists a solution. We
would like to further explore conditions under which
we can derive the numerical parameters from the mix­
ture models after manipulation.

ImaGeNie provides a flexible interactive model build­
ing environment for users to build models in causal
form with as much system assistance as possible but
without giving up their control over the model build­
ing process. We believe our efforts in incorporating
causality as a heuristic in aiding model building and
knowledge acquisition is an important extension to the
existing approaches.
Acknowledgments
This research was supported by the Air Force Office of Sci­
entific Research, grants F49620-97-1-0225 and F4962000-1-0112,

by the National Science Foundation under

Faculty Early Career Development (CAREER) Program,
grant IRI-9624629 , and by a strategic research grant num­
ber RP960351 from the National Science and Technology
Board and the Ministry of Education in Singapore. We
thank anonymous reviewers for suggestions improving the
clarity of the paper.

SMILE and GeNie are available at

http://www2.sis.pitt.edu/�genie.




nostic system. Given that so many variables are involved,
even the best solution by MAP or MPE may have an ex-

Most Relevant Explanation (MRE) is a method
for nding multivariate explanations for given
evidence in Bayesian networks [12].

tremely low probability, say in the order of

10−6 .

It is hard

to make any decision based on such hypotheses.

This pa-

In real-world problems, it is observed that usually only a

per studies the theoretical properties of MRE

few target variables are most relevant in explaining any

and develops an algorithm for nding multiple

given evidence. For example, there are many possible dis-

top MRE solutions. Our study shows that MRE

eases in a medical domain, but a patient can have at most

relies on an implicit soft relevance measure in

a few diseases at one time, as long as he or she does not

automatically identifying the most relevant tar-

delay treatments for too long. It is desirable to nd diag-

get variables and pruning less relevant variables

nostic hypotheses containing only those relevant diseases.

from an explanation. The soft measure also en-

Other diseases should be excluded from further tests or

ables MRE to capture the intuitive phenomenon

treatments. In a recent work, Yuan and Lu [12] propose

of explaining away encoded in Bayesian net-

an approach called Most Relevant Explanation (MRE) to

works.

Furthermore, our study shows that the

generate explanations containing only the most relevant tar-

solution space of MRE has a special lattice struc-

get variables for given evidence in Bayesian networks. Its

ture which yields interesting dominance relations

main idea is to traverse a trans-dimensional space contain-

among the solutions. A K-MRE algorithm based

ing all the partial instantiations of the target variables and

on these dominance relations is developed for

nd one instantiation that maximizes a relevance measure

generating a set of top solutions that are more

called generalized Bayes factor [3].

representative. Our empirical results show that

shown in [12] to be able to nd precise and concise ex-

MRE methods are promising approaches for ex-

planations. This paper provides a study of the theoretical

planation in Bayesian networks.

properties of MRE and offers further evidence for its valid-

The approach was

ity. The study shows that MRE relies on an implicit soft
relevance measure that enables the automatic identication

1

Introduction

of the most relevant target variables and pruning of less relevant variables from an explanation. Furthermore, the solu-

Bayesian networks offer compact and intuitive graphical

tion space of MRE has a special lattice structure that allows

representations of uncertain relations among the random

two interesting dominance relations among the solutions to

variables of a domain and provide a foundation for many

be dened. These dominance relations are used to design

diagnostic expert systems.

and develop a K-MRE algorithm for nding a set of top

However, these systems typi-

cally focus on disambiguating single-fault diagnostic hy-

explanations that are more representative.

potheses because it is hard to generate just right multiple-

results show that MRE methods are promising approaches

fault hypotheses that contain only the most relevant faults.

for explanation in Bayesian networks.

Maximum a Posteriori (MAP) assignment and Most Probable Explanation (MPE) are two explanation methods for
Bayesian networks that nd a complete assignment to a
set of target variables as the best explanation for given evidence and can be applied to generate multiple-fault hypotheses. A priori, the set of target variables is often large
and can be in tens or even hundreds for a real-world diag-

Our empirical

The remainder of the paper is structured as follows. We
rst review methods for explanation in Bayesian networks,
including Most Relevant Explanation. Then we introduce
several theoretical properties of Most Relevant Explanation. We also develop a K-MRE algorithm for generating
multiple top explanations and evaluate it empirically.

632

YUAN ET AL.
3

A (0.016)
Input

Output

C (0.15)

networks. However, they often fail to nd just-right ex-

D (0.1)

planations containing the most relevant target variables.

(a)

Many existing methods make simplifying assumptions and

Input
current100%
noCurr 0%

def 10%
ok 90%

Related Work

Many methods exist for explaining evidence in Bayesian

B (0.1)

B

UAI 2009

focus on singleton explanations [5, 7]. However, singleton

A
def 2%
ok 98%

explanations may be underspecied and are unable to fully
explain given evidence. For the running example, the pos-

D

C

output of B

def 10%
ok 90%

noCurr 90%

output of D

A, B, C, and D failing independently
0.391, 0.649, 0.446, and 0.301 respectively. Therefore,
(¬B) is the best singleton explanation1 . However, B alone
does not fully explain the evidence. C or D has to be interior probabilities of

output of A

def 15%
ok 85%

current 10%

current 2%

are

noCurr 98%

output of C

current 1%

current 1%

noCurr 99%

noCurr 99%

volved. Actually, if we are not focusing on faulty states,

(D)
Total Output

(0.699) is the best singleton explanation. It is clearly

not an adequate explanation for the evidence.

current 4%
noCurr 96%

For a domain in which target variables are interdependent,

(b)

multivariate explanations are often more natural for exFigure 1: (a) A probabilistic digital circuit and (b) a corresponding diagnostic Bayesian network

plaining given evidence.

However, existing methods of-

ten produce hypotheses that are overspecied. MAP nds
a conguration of a set of target variables that maximize
the joint posterior probability given partial evidence on
the other variables.

2

For the running example, if we set

A, B, C and D as the target variables, MAP will nd
(A∧¬B∧¬C ∧D) as the best explanation. However, given
that B and C are faulty, A and D are somewhat redundant

A Running Example

for explaining the evidence. MPE nds an explanation with
Let us rst introduce a running example used throughout
this paper.

Consider the circuit in Figure 1(a) adapted

from [9, 12]. Gates

A, B, C

and

D are defective if they are

closed. The prior probabilities that the gates close independently are

0.016, 0.1, 0.15

and

0.1

respectively. Connec-

tions between the gates may not work properly with certain
small probabilities. The circuit can be modeled with a diagnostic Bayesian network as shown in Figure 1(b). Nodes

A, B, C

and

D

correspond to the gates in the circuit and

each has two states: defective and ok. Others are input

even more variables. Several other approaches use the dependence relations encoded in Bayesian networks to prune
independent variables [10, 11].

They will nd the same

explanation as MAP because all of the target variables are
dependent on the evidence. Yet several other methods measure the quality of an explanation using the likelihood of the
evidence [1]. Unfortunately they will overt and choose

(¬A ∧ ¬B ∧ ¬C ∧ ¬D)

as the explanation, because the

likelihood of the evidence given that all the target variables
fail is almost

1.0.

or output nodes and have two states: current or noCurr.

There have been efforts trying to generate more appropri-

Uncertainty is introduced to the model such that an output

ate explanations. Henrion and Druzdzel [6] assume that a

node is in state current with a certain probability less than

system has a set of pre-dened scenarios as potential ex-

1.0

if its parent gate, when exists, is defective and any

planations and nd the scenario with the highest posterior

of its other parents is in state current. Otherwise, it is

probability. Flores et al. [4] propose to grow an explanation

in noCurr state with probability

1.0.

For example, node

output of B takes state current with probability 0.99 if
parent gate B is in state defective and parent Input is in
state current.

Input

and

T otal Output

in the

Bayesian network are both in the state current. The task
is to diagnose the system and nd the best fault hypotheses.
Based on our knowledge of the domain, we know there are
three basic scenarios that most likely lead to the observa-

A is defective; (2) B
B and D are defective.
tion: (1)

able at each step while maintaining the probability of each
explanation above certain threshold. Nielsen et al. [8] use
a different measure called causal information ow to grow

Suppose we observe that current ows through the circuit,
which means that nodes

tree incrementally by branching the most informative vari-

and

C

are defective; and (3)

the explanation trees. Because the explanations in the trees
have to branch on the same variable(s), they may still contain redundant variables. Finding more concise hypotheses
also have been studied in model-based diagnosis [2]. The
approach focus on truth-based systems and cannot be easily
generalized to deal with Bayesian networks.
1

We use a variable and its negation to stand for its ok and

defective states respectively

UAI 2009
4

YUAN ET AL.

Most Relevant Explanation

633
5

There are two most essential properties for a good expla-

A Theoretical Study

5.1

Theoretical properties of MRE

nation. First, the explanation should be precise, meaning
it should explain the presence of the evidence well. Sec-

We now discuss several theoretical properties of MRE.

ond, the explanation should be concise and only contain the

Since MRE relies heavily on the

most relevant variables. The above discussions show that

ating its explanations, it is not surprising that these proper-

existing approaches for explaining evidence in Bayesian

ties are mostly originated from

networks often generate explanations that are either under-

properties can be found in the appendix.

specied (imprecise) or overspecied (inconcise).

GBF

GBF .

measure in gener-

The proofs of these

First, we note that GBF can be expressed in a different way

To address the limitations, Yuan and Lu [12] propose a

using the belief update ratio.

method called Most Relevant Explanation (MRE) to au-

Denition 3. The belief update ratio of

tomatically identify the most relevant target variables for

r(x1:k1 ; e), is dened as

x1:k1

given

e,

given evidence in Bayesian networks. First, explanation in
Bayesian networks is formally dened as follows.
Denition 1.

Given a set of target variables

Bayesian network and evidence

e

X

in a

x1:k

of

X, i.e., X1:k ⊆ X and X1:k 6= ∅.

(4)

GBF can then be expressed as the ratio between the belief
update ratios of
given

MRE is then dened as follows [12].
Denition 2. Let

P (x1:k |e)
.
P (x1:k )

on the remaining vari-

ables, an explanation for the evidence is a partial instantiation

r(x1:k ; e) ≡

X be a set of target variables,

and

x1:k1

and alternative explanations

x1:k1

e, i.e.,

e be

GBF (x1:k1 ; e) =

the evidence on the remaining variables in a Bayesian net-

r(x1:k1 ; e)
.
r(x1:k1 ; e)

(5)

work. Most Relevant Explanation is the problem of nding an explanation
Bayes Factor score

x1:k that has the maximum Generalized
GBF (x1:k ; e), i.e.,

M RE(X, e) ≡ arg maxx1:k ,X1:k ⊆X,X1:k 6=∅ GBF (x1:k ; e) ,
(1)
where

GBF

The most important property of MRE is that it is able to
weigh the relative importance of multiple variables and
only include the most relevant variables in explaining the
given evidence. The degree of relevance is evaluated using
a measure called conditional Bayes factor (CBF) implicitly

is dened as

GBF (x1:k1 ; e) ≡

encoded in the GBF measure and dened as follows.

P (e|x1:k1 )
.
P (e|x1:k1 )

(2)

Denition 4. The conditional Bayes factor of hypothesis

y1:m for given evidence e conditional on x1:k is dened as

Therefore, MRE traverses the trans-dimensional space containing all the partial assignments of
ment that maximizes the

GBF

score.

Potentially, MRE

can use any measure that provides a common ground for
comparing the partial instantiations of the target variables.

GBF

CBF (y1:m ; e|x1:k ) ≡

X and nds an assign-

is chosen because it is shown to provide a plausible

P (e|y1:m , x1:k )
.
P (e|y1:m , x1:k )

Then, we have the following theorem.
Theorem 1. Let the conditional Bayes factor of y1:m given

measure for representing the degree of evidential support

x1:k

in recent studies in Bayesian conrmation theory [3].

ratio of the alternative explanations

be less than or equal to inverse of the belief update

MRE was shown to be able to generate precise and con-

CBF (y1:m ; e|x1:k ) ≤

cise explanations for the running example [12]. The best
explanation according to MRE is:

GBF (¬B, ¬C; e) = 42.62 .

(3)

e and write GBF (¬B, ¬C).
explanation than both (¬A) (39.44)

For simplicity we often omit
(¬B, ¬C ) is a better

(6)

x1:k , i.e.,
1
,
r(x1:k ; e)

(7)

the following holds

GBF (x1:k ∪ y1:m ; e) ≤ GBF (x1:k ; e).

(8)

and (¬B, ¬D ) (35.88), because its prior and posterior probabilities are both relatively high; The posterior probabilspectively.

0.394, 0.391,

0.266

Therefore,

CBF (y1:m , e|x1:k ) provides a soft measure on

re-

the relevance of a new set of variable states with regard to

Therefore, MRE seems able to automatically

an existing explanation and can be used to decide whether

ities of the explanations are

and

GBF

identify the most relevant target variables and states as the

or not to include them in an existing explanation.

explanations for given evidence.

also encodes a decision boundary, the inverse belief update

634

YUAN ET AL.

ratio of alternative explanations

x1:k

given

e,

UAI 2009

which pro-

a

A

b

B

c

C

vides a threshold on how important the remaining variables
ab

should be in order to be included in the current explanation.

aB

ac

aC

Ab

AB

Ac

AC

bc

bC

Bc

BC

1

CBF (y1:m ; e|x1:k ) is greater than or equal to r(x1:k ;e) ,
y1:m is regarded as relevant and will be included. Otherwise, y1:m will be excluded from the explanation.

If

abc

abC

aBc

aBC

Abc

AbC

ABc

ABC

Figure 2: Solution space of Most Relevant Explanation

Theorem 1 has several intuitive and desirable corollaries.
First, the following corollary shows that, for any explanation

x1:k

with belief update ratio greater than or equal to

1.0, adding any independent variable to the explanation will
decrease its GBF score [12].

x1:k be an explanation with r(x1:k ; e) ≥
1.0, and y be a state of a variable Y such that
P (y|x1:k , e) ≤ P (y|x1:k ). Then

Corollary 3. Let

x1:k be an explanation with r(x1:k ; e) ≥
1.0, and y be a state of variable Y independent from variables in x1:k and e. Then

Corollary 1. Let

GBF (x1:k ∪ {y}; e) ≤ GBF (x1:k ; e).

GBF (x1:k ∪ {y}; e) ≤ GBF (x1:k ; e).

(11)

(9)
This is again an intuitive result; a variable state whose posterior probability decreases for given evidence should not
be part of an explanation for the evidence.

Therefore, adding an irrelevant variable dilutes the explanative power of an existing explanation. MRE is able to
automatically prune such variables. This is clearly a desir-

The above theoretical results can be veried using the running example. For example,

able property.
Note that we focus on the explanations with belief update
ratio greater than or equal to 1.0. We believe that an explanation whose probability decreases given the evidence

>

GBF (¬B, ¬C)
GBF (¬B, ¬C, A) & GBF (¬B, ¬C, D)

>

GBF (¬B, ¬C, A, D) .

is unlikely to be a good explanation for the evidence.
Corollary 1 requires the additional variable
pendent from both

X1:k

and

E.

Y

to be inde-

The assumption is rather

strong. The following corollary relaxes it to be that
conditionally independent from

E

given

X1:k

Y

is

The results suggest that GBF has the intrinsic capability
to penalize higher-dimensional explanations and prune less
relevant variables.

and shows

the same result still holds.

5.2

x1:k be an explanation with r(x1:k ; e) ≥
1.0, and y be a state of a variable Y conditionally independent from variables in e given x1:k . Then

Corollary 2. Let

GBF (x1:k ∪ {y}; e) ≤ GBF (x1:k ; e).

(10)

Explaining away

One unique property of Bayesian networks is that they can
model the so called explaining away phenomenon using the

V

structure, i.e., a single variable with two or more parents.

This structure intuitively captures the situation where an
effect has multiple causes. Observing the presence of the
effect and one of the causes reduces the likelihood of the
presence of the other causes. It is desirable to capture this

Corollary 2 is a more general result than corollary 1 and
captures the intuition that conditionally independent variables add no additional information to an explanation in
explaining given evidence, even though the variable may
be marginally dependent on the evidence. Also note that
these properties are all relative to an existing explanation.
It is possible that a variable is independent from the evidence given one explanation, but becomes dependent on
the evidence given another explanation.

In other words,

GBF score is not monotonic. Looking at variables one by
one does not guarantee to nd the optimal solution.
The above results can be further relaxed to accommodate
cases where the posterior probability of
than its prior, i.e.,

y given e is smaller

phenomenon when generating explanations.
MRE seems able to capture the explaining away effect using CBF. CBF provides a measure on how relevant a new
variable is to an existing explanation.

In an explaining-

away situation, if one of the causes is already present in
the current explanation, other causes typically do not receive high CBF scores.

Again for the running example,

(¬B, ¬C) and (¬A) are both good explanations for the evidence by themselves. The CBF of ¬A given only e (the
effect) is equal to its GBF (39.44), which is rather high.

(¬B, ¬C) (one of the causes) is also obCBF (¬A; e|¬B, ¬C) becomes rather low and is
only equal to 1.03. Clearly, CBF is able to capture the exHowever, when
served,

plaining away phenomenon in this example.

UAI 2009
5.3

YUAN ET AL.

635
GBF(¬ B,

¬ C) = 42.62
¬ B, ¬ C) = 42.15
GBF(¬ B, ¬ C, D) = 39.93
GBF(A, ¬ B, ¬ C, D) = 39.56

Dominance relations

GBF(A,

MRE has a solution space with an interesting lattice structure similar to the graph in Figure 2 for three binary target

GBF(¬ A) = 39.44
GBF(¬ A, B) = 36.98
GBF(¬ A, C) = 35.99
GBF(¬ B,

¬ D) = 35.88

variables. The graph contains all the partial assignments of
the target variables. Two explanations are linked together

Table 1: The top solutions ranked by GBF. The solutions in

if they only have a local difference, meaning they either

boldface are the top minimal solutions.

have the same set of variables with one variable in different
states, or one explanation has one fewer variable than the
other explanation with all the other variables being in the
same states.
There are two dominance relations among these potential
solutions that are implied by Figure 2. The rst concept is
strong dominance.
Denition 5. An explanation

x1:k

other explanation y1:m if and
GBF (x1:k ) ≥ GBF (y1:m ).
If

x1:k

strongly dominates an-

only if

x1:k ⊂ y1:m

and

y1:m , x1:k is clearly a better
y1:m , because it not only has a no-worse

strongly dominates

explanation than

explanative score but also is more concise. We only need
to consider

x1:k

when nding multiple top MRE explana-

tions. The second concept is weak dominance.
Denition 6. An explanation

x1:k

y1:m if and
GBF (x1:k ) > GBF (y1:m ).
other explanation

In this case,

x1:k

the solutions.
The dominance relations dened in the last section allow
us to develop a K-MRE algorithm to nd a set of top solutions that are more representative.

Let us look at the

running example again to illustrate the idea.

The expla-

nations in Table 1 have the highest GBF scores. If we simply select top three explanations solely based on GBF, we
will obtain these rather similar explanations: (¬B, ¬C),
(A, ¬B, ¬C), and (¬B, ¬C, D), which are rather similar.
Since (A, ¬B, ¬C), (¬B, ¬C, D), and (A, ¬B, ¬C, D)
are strongly dominated by (¬B, ¬C), we should only consider (¬B, ¬C) out of those four explanations. Similarly,
(¬A, B) and (¬A, C) are strongly dominated by (¬A).
These dominated explanations should be excluded from the

weakly dominates an-

only if

output all the top solutions rather than selecting any one of

x1:k ⊃ y1:m

and

top solution set. In the end, we get the set of top explanations shown in boldface in Table 1, which is clearly more
diverse and representative than the original set. MAP and

has a strictly larger

GBF

score than

MPE clearly do not have this nice property.

but the latter is more concise. It is possible that we

Therefore, our proposed K-MRE algorithm works as fol-

can include them both and let the decision makers to decide

lows. Whenever we generate a new explanation, we check

whether they prefer higher score or conciseness. However,

its score against the best solution pool. If it is lower than

y1:m ,

we believe that we only need to include

x1:k ,

because its

the worst score in the pool, reject the new explanation. If

K

higher GBF score indicates that the extra variable states are

there are fewer than

relevant to explain given evidence and should be included

new explanation is higher than the worst score in the pool,

in the explanation.

we consider adding the new explanation to the top pool. We

Based on the two kinds of dominance relations, we dene
the concept minimal.

best solutions or if the score of the

rst check whether the new solution is strongly or weakly
dominated by any of the top explanations. If so, reject the
new explanation. Otherwise, we add the new explanation

Denition 7. An explanation is minimal if it is neither

to the top pool. However, we then need to check whether

strongly nor weakly dominated by any other explanation.

there are existing top explanations that are dominated by
the newly added explanation. If yes, these existing expla-

In case we want to nd multiple top explanations, we only

nations should be excluded. Otherwise we delete the top

need to consider the minimal explanations, because they

explanation with the least score.

are the most representative ones.

6

7

K-MRE Algorithm

7.1

Empirical Results
Experimental design

In many decision problems, outputting the single top solution may not be the best practice.

Decision makers typ-

We tested the K-MRE algorithm on a set of benchmark

ically would like multiple competing options to choose

models, including Alarm, Circuit, Hepar, Munin, and

from. This is especially important when there are multi-

SmallHepar. We chose these several models because we

ple solutions that are almost equally good. For the circuit

have the diagnostic versions of these networks, whose vari-

example, all three basic explanations will lead to the same

ables have been annotated into three categories: target, ob-

observation. However, we can only recover one explana-

servation, and auxiliary. For generating the test cases, we

tion if we are satised with one top solution. It is better to

used the networks as generative models and sampled with-

636

YUAN ET AL.

UAI 2009

Precision

1.0

K=1
F 1

0.8

1.0

1.0

1.0

1.0

0.6

0.8

0.8

0.8

0.8

0.4

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

1 0.0

0.0

0.0

Singleton
F-MAP

0.2

P-MAP
MRE

0.0
0

0.5

0

Recall

K=3
F=0
K=3
F 1

0.5

1

0

0.5

1

0.0
0

0.5

1

1.0

1.0

1.0

0.8

0.8

0.8

1.0

0.8

0.6

0.6

0.6

0.8

0.6

0.4

0.4

0.6

0.4

0.2

0.4

0.5

1

0.0
0

0.5

1

0.5

1

0

0.5

1

1.0

1.0

1.0

1.0

1.0

0.8

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.2

0.0

0.0

0.0
0

0.5

1

0

0.5

1

0

0.5

0.5

1

1.0

1.0

1.0

1.0

0.8

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.2

0.0

0.0

0.0

0.0

0.5

0

1

1

0

0.5

1

0

0.5

1

0.0

0

1

1.0

0

0.5

0.0

0.0

0



0.0

0

0.2


K=1
. 
F=2
)





K=3
. 
F=2
)




1

0.2

0.2

0.0
0

0.5

0.4

0.2

0.0

0

1.0

0.5

1

0

0.5

0.0
0

1

0.5

1

1.0

Singleton

P MAP

1.0

1.0

1.0

1.0

0.8

F MAP

MRE

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.6

I-rate 0.4
K1F1 0.2

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0

0.0

0.0

0.0

0.0

Alarm

Circuit

Hepar

Munin

SmallHepar

Figure 3: Precision vs recall plots of the results by four algorithms, Marginal, P-MAP, F-MAP, and MRE, on a set of
benchmark diagnostic Bayesian networks.K shows the number of top solutions generated. F shows the least number
of faulty target variables in test cases. F Score shows the F-Scores of the results of the algorithms. Marginal algorithm
did not appear in rows K3F1 and K3F2 because it has only one solution.

out replacement from their prior probability distributions.

centage of faulty states correctly identied among all faulty

We only kept those test cases with at least one abnormal ob-

explanation variables) and recall (the percentage of faulty

servation and used the abnormal observations as evidence.

states correctly identied among all faulty variables in test

Since Circuit and SmallHepar have 4 and 3 target variables

cases) of these algorithms in Figure 3.

respectively, we collected as many test cases as possible.

sample results on F-Score, which is dened as

Munin also has 4 target variables but each with many more
states. Hepar and Alarm have 9 and 12 target variables re-

F-Score

spectively. We collected 50 test cases for the last three net-

=

We also include

2 × (precision × recall)
.
(precision + recall)

(12)

works. We also extracted from them the test cases which
contain at least two faulty target variables for separate ex-

7.2

Results and analysis

periments on multiple-fault test cases.
Our experiments compared MRE with MAP given their
similarities. We tested two versions of the MAP algorithm,
one focusing on all the target variables (F-MAP) and the
other only on the target variables selected by MRE (PMAP). In addition, we compared with the Marginal algorithm, which neglects the interdependence among the target variables and uses the marginal posterior probabilities
to determine the most likely states of the target variables.
We plot the accuracy statistics, including precision (the per-

We make the following observations from these results.
First, MRE is able to achieve higher precision and/or recall
rates in identifying the faulty target variables than the other
algorithms on all the networks except Munin. An outstanding example is the SmallHepar network. Marginal, F-MAP
and P-MAP all failed badly on this model in identifying the
faulty variables, while MRE was able to achieve reasonable
performance. It is clearly desirable given that one major
goal of diagnosis or explanation is to identify problems,
e.g.

faulty states.

We investigated the results of Munin

UAI 2009

YUAN ET AL.

network further and found that all target variables of these

637
8

Concluding Remarks

test cases are in faulty states. Marginal and F-MAP have
exactly the same statistics, which suggests that the target

In this paper, we discuss several theoretical properties of

variables may have weak correlations with each other. This

Most Relevant Explanation (MRE) and develop an algo-

puts MRE in disadvantage because MRE takes into account

rithm for nding multiple top MRE solutions. Our study

such weak correlations and generate concise explanations

shows that MRE relies on an implicit soft relevance mea-

with fewer target variables. On average, the explanations of

sure in automatically identifying the most relevant target

4.3 variables out of 12 target variables for
Alarm, 1.7/4 for Circuit, 4/9 for Hepar, 2.5/4 for Munin,
and 2.3/3 for SmallHepar. For networks with strong correMRE identies

variables and pruning less relevant variables from an explanation.

The soft measure also enables MRE to cap-

ture the intuitive phenomenon of explaining away encoded

lations among the target variables, e.g. Circuit and Hepar,

in Bayesian networks. Furthermore, we dene two dom-

MRE has much higher precision/recall rates. The sample

inance relations among the explanations that are implied

F-score results in the case of K1F1 further conrmed the

by the structure of the solution space of MRE. These rela-

observation.

tions allow us to design and develop a K-MRE algorithm

Second, by comparing rows K1F1 vs.
K1F2 vs.

K3F1 and

K3F2, we found that using multiple top

for nding top MRE solutions that are much more representative.

solutions helps MRE signicantly in improving the preci-

Our empirical results agree quite well with the theoretical

sion/recall rates than the other algorithms. With multiple

understanding of MRE. The results show that MRE is ef-

solutions, we kept the results with the maximum precision

fective in identifying the most relevant target variables, es-

rates. The results seem to support our claim that K-MRE

pecially the true faulty target variables. Furthermore, K-

was able to generate solutions that are more representative.

MRE seems able to generate more representative top ex-

It is somewhat surprising that the precision/recall rates of

planations than K-MAP methods.

F-MAP were not improved at all on the networks, but those

is especially suitable for systems in which target variables

of P-MAP were improved. Our hypothesis is that, since the

are strong correlated with each other and can generate more

explanations by F-MAP are more grained because more

precise and concise explanations for these systems.

variables are involved, its top explanations tend to agree
with each other on the faulty variables and differ mostly
in the less important non-faulty variables. Generating multiple top solutions could not really help F-MAP much in
improving its accuracy statistics.

We believe that MRE

This research has many future works. It is desirable to understand the theoretical complexity of MRE. It has a solution space even larger than MAP and is believed to be at
least as hard. Currently we rely on an exhaustive search
algorithm for solving MRE and K-MRE. More efcient

Third, although P-MAP gets the target variables identied

methods for solving MRE need be developed to make it

by MRE as input, it still failed badly on the SmallHepar

applicable to large real-world problems.

network in identifying faulty states of the target variables.
It did not show any signicant advantage over F-MAP on

Acknowledgement

other networks either. The results suggest that relying on

National Science Foundation grant IIS-0842480.

posterior probabilities may not work well in certain diag-

experimental data have been obtained using SMILE, a

nostic systems.

Bayesian inference engine developed at the Decision Sys-

Fourth, although multiple-fault cases are believed to be

tems Laboratory at University of Pittsburgh and available

more difcult because of their low likelihood, the algo-

at

This research was supported by the
All

http://genie.sis.pitt.edu.

rithms in our experiments seem able to maintain the same
level of accuracy rates in face of multiple-fault test cases
(rows K1F2 and K3F2).


