

explored by Breese, Beckerman and Kadie

(1998),

have not relied on the order in which users express
We treat collaborative filtering as a univari­
ate time series problem: given a user's previ­
ous votes, predict the next vote. We describe
two families of methods for transforming data
to encode time order in ways amenable to
off-the-shelf classification and density estima­
tion tools. Using a decision-tree learning tool
and two real-world data sets, we compare the
results of these approaches to the results of
collaborative filtering without ordering infor­
mation.

The improvements in both predic­

tive accuracy and in recommendation quality
that we realize advocate the use of predictive
algorithms exploiting the temporal order of
data.

their preferences.

Vector-space methods draw heav­

ily on work in the information retrieval literature (see,
e.g., Baeza-Yates and Ribeiro-Neto,

1999),

where in­

dividual documents are treated as a "bag of words".
Likewise, probabilistic techniques (e.g. Hofmann and
Puzicha,

1999

and Beckerman,

Rounthwaite and Kadie,

2000)

Chickering,

Meek,

have computed proba­

bility distributions over recommendations conditioned
on the entire vote history without regard to time or­
der. In the CF literature, a "bag of votes" (i.e. atem­
poral) assumption prevails, and the collaborative fil­
tering problem is cast as classification (with classes
"relevant" and "irrelevant") or density estimation (of
the probability that a document is relevant, given a
user's votes).
We instead consider collaborative filtering as a univari­

Keywords: Dependency networks, probabilistic deci­
sion trees, language models, collaborative filtering, rec­
ommendation systems.

1

Introduction

The collaborative filtering problem arose in response
to the availability of large volumes of information to
a variety of users. Such information delivery mecha­
nisms as

Usenet and

online catalogs have created large

stores of data, and it has become the users' task to dis­
cover the most relevant items in those stores. Rather
than requiring that users manually sift through the
full space of available items, trusting that authors
respect the available system of topics, CF tools rec­
ommend items of immediate or future interest based
on all users' expressed preferences ("votes"), suggest­
ing those items of interest to other users with similar
tastes.

These votes may be either explicit, as in re­

sponse to a direct inquiry, or implicit, as by the choice
to follow one hyperlink instead of others.
In general, algorithms for the CF task, such as those

ate time series prediction problem, and represent the
time order of a user's votes explicitly when learning a
recommendation model. Further, we encode time or­
der by transforming the data in such a way that stan­
dard atemporallearning algorithms can be applied di­
rectly to the problem. Other authors (cf. Mozer,

1993)

have applied atemporal learning techniques to tempo­
ral data; we describe here two successful generic tech­
niques. As a result, researchers can simply transform
their data as we describe and apply existing tools, in­
stead of having to re-implement various collaborative
filtering algorithms for awareness of vote order. Our
approach allows CF models to encode changes in a
user's preferences over time. It also allows models to
represent (indirectly) structure built into the feature
space that would be lost in a bag of votes representa­
tion. For example, Web page viewing histories ordered
by page request can express the link structure of a Web
site because a user is most likely to follow links from
his current page. Similarly, television viewing histories
encode the weekly schedule of shows: a viewer cannot
hop from

Buffy

the

Vampire Slayer to Dawson's ·Creek

if the two are not contemporaneous.

ZIMDARS ET AL.

UAI2001

For simplicity, we assume for the remainder of this
paper that user preferences are expressed as implicit
votes (see, e.g., Breese et al., 1998). That is, a users'
vote history is a list of items that the user preferred,
as opposed to an explicit ranking of the items. In a
movie domain, for example, this means that a user's
vote history is simply a list of movies that he watched,
and we assume that he preferred those movies to the
ones he did not watch. We note, however, that the
transformations we describe are easily generalized to
explicit voting.
In Section 2, we present two methods for transform­
ing user vote histories that encode time-order infor­
mation in ways that traditional atemporal modeling
algorithms can use. In Section 3, we discuss three
candidate models that can be learned from standard
algorithms applied to the transformed data. In Sec­
tion 4, we describe the data sets and criteria by which
we will compare our approaches, and in Section 5 we
present our experimental results from using decision­
tree learning algorithms.

581

age, and G is a binary variable that denotes the per­
son's gender. Under the iid assumption a learning al­
gorithm can use observed values of S, A, and G for
other people in the population to estimate the distri­
bution p(SIA, G), then make a prediction about the
particular person of interest with that distribution.
In the following sections, we describe how data that
contains vote histories can be transformed, using var­
ious assumptions, into the case representation so that
standard machine-learning algorithms can be used to
predict the next vote in a sequence. First, we need
some notation.
We use item to denote an entity for which users ex­
press preferences by voting, and we use 1 to denote
the total number of such items.

For example, in a

scenario, 1 is the total num­
ber of movies considered by the collaborative-filtering
system. For simplicity we refer to each item by a one­
based integer index. That is, the items in the system
are mapped to the indices:
movie-recommendation

{1, ...,1'}
2

Data Transformations

In this section, we describe two methods that trans­
form time-ordered vote histories into a representation
that traditional atemporal modeling algorithms can
use; we call this representation the case representation.
In the case representation, the data D consists of a set
of cases (or records) {C1, ... ,Cm}, where each case
Ci = { x1, .., Xn} consists of a value for zero or more
of the variables in the domain X = {X1, . . , Xn}.
.

.

The important (sometimes implicit) assumption of
modeling algorithms that use the case representation
is that the observed cases are independent and identi­
cally distributed (iid) from some joint probability dis­
tribution p(X1, ... , Xn)1; an equivalent Bayesian as­
sumption is that the cases are infinitely exchangeable,
meaning that any permutation of a set of cases has
the same probability. The learning algorithms use the
observed case values in D to identify various models
of the generative distribution.
As an example, consider the problem of predicting
whether or not a particular person will watch some
television show based on that person's age and gen­
der. Using the case representation, we might assume
that all people are drawn from some joint probability
distribution p( S,A, G), where S is a binary variable
that indicates whether or not a person watches the
show, A is a continuous variable denoting a person's
1In fact, if we are interested in learning a conditional

model for Y C X, we often need only assume that the

values for the variables in Y are independent samples from
some p(YIX \ Y)

We use Vi to denote the i1h vote history (i.e. user's
votes). In particular, Vi is an ordered list of votes:
{V/,

.

..

, vt'}

where V/ denotes the item index of the j1h vote in the
list, and Ni is the total number of votes made by user

�.

As an example, suppose there are four movies The Ma­
trix, Star Wars, A Fish Called Wanda and Pulp Fic­
tion having indices 1,2,3 and 4, respectively. Suppose
there are two movie watchers in the domain: User 1
watched The Matrix and then watched Pulp Fiction,
and user 2 watched Star Wars, then watched Pulp Fic­
tion, and then watched The Matrix. Then we would
have V1 = {1,4} and V2 {2,4, 1}.
=

For each of the transformations below, we show how
convert from a set of vote histories into (1) a set of
domain variables X= {Xl.···,Xn}, and (2) a set of
cases {C1,... , Cm}, where each case Ci contains a set
of values { x1, . , Xn} for the variables in X. We also
describe what assumptions are made in the original
domain in order for the resulting cases to be iid.
to

. .

2.1

The "Bag-of-votes" Transformation

The first transformation we consider disregards the or­
der of previous votes, corresponding to the assumption
that vote order does not help predict the next vote. As
noted above, this "bag-of-votes" approach is the ap­
proach taken by many collaborative-filtering learning
algorithms.

ZIMDARS ET AL.

582

For each item k, where 1 ::; k ::; /, there is a binary
variable Xj E X, whose states x] and x� correspond
to preferred and not preferred, respectively. There are
no other variables in X. For each vote history V;, we
create a single case Ci with the following values: if
item j occurs at least once anywhere in the sequence
vi, then the value Xj in ci is equal to x] . Otherwise,
the value of Xj in Ci is equal to x5 .
The assumption that the cases are iid corresponds to
assuming that the (unordered) votes of all vote histo­
ries (i.e. users) are all drawn from the same distribu­
tion. Under this assumption, we can use an atemporal
learning algorithm with the cases from previous vote
histories learn a model for p(XJIX\XJ) for all XJ EX,
and then use these models to predict the next vote2
for any vote history.
2.2

The Binning Transformation

The second transformation we consider can be help­
ful when user preferences change over time. Although
the transformation does not explicitly use the order
of the votes, it can exploit temporal structure. The
idea is to (1) separate vote histories into bins by their
size, (2) transform the histories from each bin into
the case representation using the "bag-of-votes" trans­
formation described above, and (3) learn a separate
model from the data in each such bin. W hen it comes
time to predict the next vote in a sequence of size k,
we use the model that was learned on the cases derived
from the vote histories in the bin corresponding to k.
Suppose, for example, that we would like to train one
or more models in order to recommend movies to peo­
ple. It might be reasonable to assume that the op­
timal model for predicting the third movie for some­
one may not be a very good model for predicting the
100th movie. W ith binning, we divide up the range of
the number of movies that have previously been seen
into separate bins, and learn a recommendation model
for each. Thus, we might end up with three mod­
els: (1) a simple model that predicts popular movies
for people who do not go to the movies much, (2) a
model that perhaps identifies general viewing prefer­
ences (e.g. comedies) for the typical viewer, and (3) a
model that identifies subtle preference trends for heavy
movie watchers.
In order to perform binning, there are a number of
parameters that need to be set. First, we need to
decide how many bins to use. Second, we need to
decide, for each bin, what history lengths should be
included in that bin.
2

There are some subtleties, addressed below, about how

this prediction is made.

UAI2001

For the experiments that we present in Section 4, we
tried both two and four bins. For each bin, we set a
minimum and maximum value for the length of the
contained histories. We chose this minimum and max­
imum such that the total number of votes in each bin
are roughly the same.
As described above, the binning approach assigns each
vote history to exactly one bin. An alternative ap­
proach, which we call the prefix approach, is to allow
a single vote history to contribute to multiple bins by
adding an appropriate prefix to all of the "previous"
bins. As an example, suppose there are three bins that
accommodate histories of length up to 5, 10, and 100.
In the prefix approach, a vote history of length 90 will
have (1) the first five votes added to the first bin, (2)
the first ten votes added to the second bin, and (3) the
whole history added to the third bin.
The choice of whether or not to use the prefix approach
to binning will depend on user behavior and domain
structure. We identify the following two hypotheses
that can help determine which method is most appro­
priate.
•

The "expert/novice" hypothesis: Users with long
vote histories ("experts" in the domain) have fun­
damentally different preferences than users with
short vote histories ("novices"). As a result, we
expect that omitting prefixes of longer vote histo­
ries from bins for shorter vote histories will result
in better predictive accuracy than the prefix ap­
proach. The expert/novice hypothesis might hold
when predicting preferences for television viewing,
where couch potatoes might have different view­
ing habits than occasional viewers. On a Web site,
heavy users tend to navigate very differently than
"shallow browsers" (cf. Huberman et al., 1998).

•

The "everyone learns" hypothesis: Users with
long vote histories once expressed similar prefer­
ences to users with short vote histories. Under
this hypothesis, we expect that prefixes of long
vote histories will be distributed similarly to short
vote histories, and therefore their inclusion in the
corresponding bins will provide useful data for the
model-building algorithm; as a result, we hope
that the resulting models will be more accurate.
One can also interpret this hypothesis from the
perspective of domain structure constraining user
behavior. For users of a Web portal, initial votes
may be restricted to the home page and top-level
categories linked from that page. For subsequent
page hits, available links may constrain possible
user votes. In this domain, we would expect users
to have similarly-distributed vote prefixes because
site structure does not allow much room for inno-

UA12001

ZIMDARS ET AL.

vation.
For the domains we consider in Section 4, the latter hy­
pothesis seems more appropriate; although we ideally
should have compared the two, in the interest of time
we only used the prefix approach in our experiments.
We chose the bin boundaries so that the total number
of votes of the original (i.e. non-prefix) histories in
each bin were roughly the same.
W hether or not we use the prefix approach, the addi­
tional computational overhead of binning over no bin­
ning is proportional to a constant factor (the number
of bins), because each bin will contain no more votes ,
and no more vote histories, than would a single model
computed using the entire vote set.

Structural aspects of some prediction domains can
make difficult the choice of vote sub-histories to aug­
ment data for binning. Web sites tend to have a hierar­
chical structure with a home page at the root, but the
same cannot be said for television programming sched­
ules, which reflect periodic structure. When predicting
television viewing habits given a "snapshot" of user
viewing histories, prefixes may not reflect the periodic
nature of the program schedule. In such domains, dif­
ferent choices of contiguous vote sub-histories may be
appropriate, but the resulting profusion of data might
render binning impractical.
We should point out that binning can be applied to
collaborative filtering problems in which the temporal
order of the votes is unknown. Although the prefix ap­
proach may not be appropriate, binning based on the
number of votes can potentially lead to significantly
better accuracy in atemporal domains. Consider, for
example, the problem of recommending items in a gro­
cery store based on the products bought (the recom­
mendation may appear as a targeted coupon on a re­
ceipt). It might turn out that, regardless of the order
in which people put groceries in their shopping cart,
the number of items in their cart may indicate very
different shopping behavior; consequently the binning
approach might yield significantly better models than
a system that ignores the number of votes.
2.3

Data Expansion

The final data transformation we consider, which we
call data expansion, finds inspiration in the language
modeling literature (see, e.g., Chen and Goodman,
1996). This method of data expansion distinguishes
the most recent n votes from the entire vote history, as
well as identifying the order of the most recent votes.
All of the variables that we create in the transforma­
tion are binary, and have states x1 and xO correspond
to preferred and not preferred, respectively.

583

In the case representation, we create one binary vari­
able for each of the I items in the domain: xT =
{X[, ... ,X�}. The "T" superscript in X'[ is meant
to indicate that this is a "target variable" that repre­
sents whether or not the next vote is for item k.
The data expansion transformation is parameterized
by a history length l; this parameter, which corre­
sponds to the "n" parameter in an n-gram language
model, determines how far back in the vote history
to look when predicting the next vote. For each in­
teger history 1 � j � l, we again create one bi­
nary _variable for each of the 1 items in the domain:
{X;1, ... ,X_::;-i}. The "-j" superscript in Xf:j is
meant to indicate that this variable represents whether
or not Ph previous vote (from the one we're predicting)
is for item k. We use XL to denote the set of all lagged
variables (e.g. {X!1, . . ,X_::;-1},{X12, .. . ,X;2}).
.

There is a final set of 1 variables consisting of, for
each item, an indicator of whether or not that item
was voted for at least once previously in the given vote
history. We use xc = {Xf, ... , X0} to denote these
variables. In language-modeling p;rlance, these vari­
ables are known as cache variables.
In contrast to the "bag-of-words" approach, where
each vote history was transformed into a single case, in
the data expansion transformation, each vote in every
history _gets a corresponding case. In particular, for
vote V/, which is the lh vote in the ith vote history,
we define the values for all of the variables as follows.
For simplicity, let v = V/. We set the value of target
variable XJ to xl, and we set the value of all other
target variables to xO. For each history variable x-j
k ,
where 1 � j � l, we set the corresponding value to ei­
ther x1 if the lh previous vote in history i has value k,
or xO otherwise. Finally, we set the value of each cache
variable Xf to either xl if item k occurs as a. vote (at
least once) previous to V/ in Vi, or xO otherwise.
We should point out that in order to feasibly learn
a model using the cases that result from the data­
expansion transformation, the learning algorithm(s)
need to use a sparse representation for the cases. See
(e.g.) Chickering and Heckerman (1999) for a discus­
sion.

Consider our movie example again. For simplicity, we
use M, S, F, and P to label all variables we create
corresponding to movie items The Matrix, Star Wars,
A Fish Called Wanda and Pulp Fiction. Furthermore,
we use 1 and 0 to denote the values preferred and not
preferred, respectively.
Suppose we want to transform a vote history con­
taining The Matrix, Pulp Fiction, and Star Wars, in
that order, into the case representation with a history

584

ZIMDARS ET AL.

length of one. First we define the variables

x

=

for this model is expressed as follows:

r r r r
{ M ,s ,F ,P ,
M�1, 8-1, p-1, p-1,
MC ' SC ' pC , pC}

n

P(C

1

shows the case values that

result.
The learning algorithm we use should build a model
for each of the target variables, using all non-target
variables as predictor variables.

That is, we would

like the model to estimate, for each target variable

XJ E XT' the distribution p(XJIXL' xc).

The iid assumption in the case representation-after
performing the data-expansion transformation with
history-length l-implies that each vote is
a distribution that depends on
previous

l

votes and

(2)

(1)

drawn from

the values of the

the presence or absence of at

least one vote for previous items.

v 1 , ... ,vn )

=

P(C

Models

that can be used for collaborative filtering applica­
tions; when learned from data that is transformed as
described in the previous section, these models can
exploit the vote order to improve recommendation ac­
curacy.

Cheeseman and Stutz

1977).

(1995)

(1)

provide details of

In this setting, prediction for collaborative filtering
follows from the density estimation problem, as the
model predict the item(s) most

likely to receive an af­

firmative vote given the user's vote history.

Other latent class models (Hofmann and Puzicha,

1999)

for collaborative filtering

have been proposed

which place user and item on an equal footing. These
permit construction of a two-sided clustering model
with preference values, but they depend on multino­
mial sampling of (user, item) pairs, and as such do not
generalize naturally to new users.

Decision-tree models

v ious work (cf.

Beckerman et al.,

2000)

constructs

a forest of probabilistic decision trees, one for each
item in the database, using a Bayesian scoring crite­
rion (Chickering, Beckerman, and Meek,

1997).

This

provides a compact encoding of conditional probabil­
ities of recommendations, given previous votes.3
use this approach in Section

We

4 to evaluate our data

Memory-based algorithms

Memory-based collaborative filtering algorithms pre­
dict the votes of the active user based on some partial
information about the active user and a set of weights
calculated from the user database. Memory-based al­
gorithms do not provide the probability that the active
user will vote for a particular item. Instead, the active
user's predicted vote an item is a weighted sum of the
votes of the other users. See Breese et. al

(1998) for

a

more detailed discussion.

A

)

=c

i=l

a specific implementation of the learning algorithm.

transformations.

3.2

) IT P(vi I C

The approach that has proven most effective in pre­

In this section, we describe some well-known models

3.1

=c

the EM algorithm (see Dempster, Laird and Rubin,

3.3
3

= c,

The parameters of this model can be learned using

Next, we consider each vote in the history, and create
a case for each one. Table

UAI2001

Cluster models

standard probabilistic model is the naive Bayes

model with a hidden root node-one where the prob­
abilities of votes are conditionally independent given
membership in an unobserved class variable C, where
C ranges over a fairly small set of discrete values. This

corresponds to the intuition that users may be clus­

tered into certain groups expressing common prefer­
ences and tastes.

The joint probability distribution

3.4

Alternative models

The data expansion technique discussed in Section

2.3

suggests the application of language-modeling algo­
rithms to collaborative filtering. We have conducted

limited experiments with variants of n-gram language
models, and the results are promising (although we do
not present them here).
Hidden Markov models
themselves in this setting,

(HMMs)

also

recommend

but in our experience they

are ill-suited to a na"ive representation of the data,

where each possible vote corresponds to exactly one
feature. This reflects in part the number of parame­
ters that must be estimated when running EM for an
HMM: if the model admits
are

me

+

c2

+

c

c

hidden states, then there

parameters to estimate for the poste­

rior probabilities of states, the state transitions, and
3It also permits the construction of a family of graphical
as dependency networks, which have expres­

models known

sive strength similar to Markov networks.

ZIMDARS ET AL.

UAI2001

Table

585

Case values created for the movie example with the data expansion method.

1:

M1
1

Vote

The Matrix
Pulp Fiction
Star Wars

s·l

FJ

p'l

M-1

0
0
0

0
0
1

0

0

0
0

1

1

0

0

s

·1

F-1

p-

0
0
0

0
0
1

0
0
0

1

Me

se

Fe

p--u

0
1
1

0
0

0
0
0

0
0
1

0

Moreover, models are slow to con­

We used probabilistic decision-tree models for our ex­

verge because collaborative filtering data tend to be

periments, and compared both binning and data ex­

very sparse, in that few users vote on ariy one item. As

pansion to the default "bag-of-votes" approach of ig­

the state priors.

a result, evidence for estimating a particular variable

noring the data order.

is rarely presented in training. This sparsity is

we learned a single decision tree per page

integral

For all of the experiments,
to predict

to the collaborative filtering problem, but lethal to ac­

whether the user requests that page, based on the

curate estimation. Finally, HMMs discard much of a

transformed data available at that time.

user's history in making predictions, and our experi­

greedy tree-growing algorithm in conjunction with the

ments indicate that a long history can be informative.

Bayesian score described by Chickering et. al (1997).

We used a

In particular, the score evaluated the posterior model

4

probability using a flat parameter prior, and a model

Experiments

prior of the form r;,f, where

In this section, we describe the experiments we per­
formed to demonstrate that using vote order can im­
prove the accuracy of models.
We conducted our experiments using two real-world
data sets, both of which are Web user traces. In each,

the notion

of

"user" corresponds

to a

server

session,

and a page request was interpreted as an affirmative
vote.
The first data set consists of session traces from
data encompassed

110587

The training

page requests from

27595

users over three days in late August 1999, and the test
data included

54843 requests

from

13563

users on

14

September of the same year. The requests span a to­
tal

of 8420 URLs, roughly 400 of which correspond to
404 errors for invalid URLs. The average length of a
session trace was 4.007 votes, with a median length of
2, and the longest trace was 93 votes.
second

data

set

uses

session

traces from http://www .msnbc. com/, corresponding
to an

80%/20%

split of users on

The training data include roughly
from

22 December 1998.
1.28 million requests
data include 178158

475769 users, while the test
requests from 87714 users . The requests in these two
data sets span 1001 URLs; it is unclear whether any of
these represent invalid URLS. The average length of a
session trace was

2

experiments.

= 0.01 for all of the

In all of the data transformations described in the pre­
vious section, we created a separate binary variable for
each item that denoted whether or not the next vote
will be for that item. Defining the variables this way
can be problematic for any learning algorithm using
finite data that does not enforce the constraint that
the next vote will be for exactly one item. In particu­

http: //research. microsoft. com/.

T he

f is the number of free

parameters in the tree. We used r;,

2.696

and a longest trace of

vote, with a median length of

407 votes.

Unfortunately, we did not identify other publicly­
available data that records user preferences in time or­
der. The authors' experience with other data suggests
that the techniques outlined here may prove fruitful
with other types of sequential data.

lar, the algorithm we used to learn a forest of decision
trees did not enforce this constraint.

We solved this

problem by using the decision trees to calculate the
posterior probability that each item would be the next

vote, then renormalizing.

We applied two evaluation criteria in our experiments.
For all prediction algorithms, we adopted the "CF ac­
curacy" score outlined by Heckerman et a!.

(2000),

and specialized it to compute the CF score with re­
spect to the next item in the user's history only. The
CF accuracy score attempts to measure the probabil­
ity that a user will view a recommendation presented
in a ranked list with other recommendations. To ap­
proximate this probability, let p(k)

=

2-k/a

denote

the probability that the user views the kth item on
his list (where k counts from

0) .

For the experiments

presented here, we chose a half-life of

a

=

10.

We

computed for each user i, and for each vote vii in his
vote history, a ranked list of recommendations given

Vil,

·

·

·,

Vi(j-1)

·

One may compute the CF accuracy of a general list L

of test items spanning n users. Suppose the model rec­

ommends R; items to each user, and the users actually

prefer sets of M; items. Let O;k denote the indicator

586

ZIMDARS ET AL.

UAI2001

that user i prefers the kth recommendation. Then
accuracy CF (L)

=

n

1
- """"
n �
i

'"'R-lJ.

L.. k-o

•kP (k)

'"'M;-1 p (k)

=l L.. k=O

(2)

Let kii be the ranking assigned by our model to vote

Vij. Scoring one vote at a time, CF accuracy simplifies

to

(3)
Baseline

2 Bins

4

Bins

DE·1

DE·3

DE·S

One may compute CF accuracy for any CF algorithm
that generates a ranked list of recommendations, but

Figure

it provides a criterion specific to the collaborative fil­

constructed for the MSNBC domain.

1:

Collaborative filtering scores of the models

tering task. For the probability models we evaluated,
we also computed the mean log-probability assigned
to each of the user's actual votes, given the preced­
ing vote history. (This log-probability was normalized
over all items in dependency-network models to com­
pensate for potential inconsistencies).

Baseline

2 Bins

4 Bins

DE·1

DE·3

DE·S

-4.89
-4.895

Note that CF accuracy is a function of the relative

magnitude of density estimates, while the log score
depends on the absolute magnitude of the estimates.

5

·4.885

-4.9
-4.905
-4.91

Results

-4.915

The results presented below correspond to three fam­
ilies of models.

The "Baseline" results derive from

a forest of decision trees trained on bag-of-votes data,
shown to be a one of the best models for CF (Breese et
al., 1998).

"2

Bins" and "4 Bins" experiments applied

the binning method described in section

2.2.

Two or

four decision trees are constructed for each Web page,

Figure

2:

Log-probability scores of the models con­

structed for the MSNBC domain.

increase as a function of history length.

This might

suggest that Web page requests depend more strongly

but only one is chosen (according to the partial his­

on immediate links than on the short-term history, and

tory at hand) to make a prediction. The "DE-" exper­

that data expansion mainly embodies this structural

iments expand data as in section 2.3, with histories of

element of the Web surfing domain. (One should not

length 1, 3, and 5.

interpret this

Figure 1 and Figure

2

scores,

for all of the models in the

respectively,

show the CF scores and log

MSNBC domain.

as

a Markov assumption; in our expe­

rience, the cache variables strongly influence predic­
tion.)

The higher CF accuracy results suggest that

the relative magnitude of density estimates is more of­

ten accurate for data-expanded models than binned

There are some interesting observations to make about

models, and these relative estimates determine which

these results. F irst, we see that for the collaborative­

pages show up in a recommendation list.

filtering score, the score got worse as we increased the

number of bins. This may be an artifact of the sparsity

of long traces in Web surfing data, a phenomenon that
has been observed elsewhere (e.g., Huberman et al.,
1998). This may not impair work in other domains;

our experience with data suggests that other frequency
functions for user history length can have thicker tails.

Our results show that unlike for the CF score, the bin­
ning approach dominated both the baseline and the
data-expansion models for log-probability predictive
accuracy.

For this score, the data-expansion models

improved as the history length increased, but only the
model with the longest history (five) was competitive
with the baseline model.

We suspect that the data

Second, we see that all of the data-expansion models

were too sparse to permit accurate parameter esti­

performed significantly better than the baseline with

mates for the models learned under data expansion.

respect to CF accuracy, but that performance did not

In particular, there were roughly 50 percent more pa-

ZIMDARS ET AL.

UA12001

587

rameters to train in each of the data-expansion models

the log score. However, binning models do not indicate

than in the other models, which leads us to suspect

a steep fall-off in CF accuracy relative to the baseline,

that the learning algorithm over-fit for these models

as for the MSNBC data set. We hypothesize that typi­

to some degree. In retrospect, we regret the choice of

cal MSR visitors leave longer page traces than MSNBC

a single value of the model-prior parameter "' for all

users.

data transformations. We expect that if we had tuned
this parameter by splitting up the training data and
maximizing a hold-out prediction accuracy, we would
have identified a smaller "'for the data-expansion mod­
els that yielded better results for both criteria on the
tests set. Improvements in log score as history length
increase demonstrate the value of the additional in­
formation encoded by the expanded data, which com­
pensates in part for having too few data points per
parameter.

6

Conclusion

We have presented two techniques for transforming
data that allow the collaborative filtering problem to
be treated as a time-series prediction task.

Both of

these techniques allow state-of-the-art collaborative
filtering methods to model a richer representation of
data when vote sequence information is available. We
have evaluated these techniques, using probabilistic

Figure 3 and Figure

4

show the CF scores and log

scores, respectively, for all of the models in the MSR
domain.

decision-tree models, with two data sets for which the

order of user votes were known. Results indicate mixed
gains for each approach. Binning user data by history
length improved log-probability scores with respect to

0.6

a bag-of-votes model in our test cases, while data ex­
pansion to introduce history variables improved the

0.5

collaborative filtering accuracy score over baseline.

0.4




We describe research and results centering on
the construction and use of Bayesian mod­
els that can predict the run time of problem
solvers. Our efforts are motivated by observa­
tions of high variance in the time required to
solve instances for several challenging prob­
lems. The methods have application to the
decision-theoretic control of hard search and
reasoning algorithms. We illustrate the ap­
proach with a focus on the task of predict­
ing run time for general and domain-specific
solvers on a hard class of structured con­
straint satisfaction problems. We review the
use of learned models to predict the ultimate

length of a trial, based on observing the be­
havior of the search algorithm during an early
phase of a problem session. Finally, we dis­
cuss how we can employ the models to inform
dynamic run-time decisions.
1

Introduction

The design of procedures for solving difficult problems
relies on a combination of insight, observation, and it­
erative refinements that take into consideration the be­
havior of algorithms on problem instances. Complex,
impenetrable relationships often arise in the process of
problem solving, and such complexity le ads to uncer­
tainty about the basis for observed efficiencies and in­
effi.ciences associated with specific problem instances.
We believe that recent advances in Bayesian methods
for learning predictive models from data offer valuable
tools for designing, controlling, and understanding au­
tomated reasoning methods.
We focus on using machine learning to characterize
variation in the run time of instances observed in in­
herently exponential search and reasoning problems.
Predictive models for run time in this domain could

Design, real-rime control,

World, Context

j

Contex.tual

evidence

insights

Run time

Structural
evidence

Ex.ecution
evidence

GQlliJ

Feature refinement, insights

Figure 1: Bayesian approach to problem solver design
and optimization. We seek to learn predictive mod­
els to refine and control computational procedures as
well as to gain insights about problem structure and
hardness.

provide the basis for more optimal decision making at
the microstructure of algorithmic activity as well as
inform higher-level policies that guide the allocation
of resources.
Our overall methodology is highlighted in Fig. 1. We
seek to develop models for predicting execution time
by considering dependencies between execution time
and one or more classes of observations. Such classes
include evidence about the nat ure of the generator that
has provided instances, about the structural properties
of instances noted before problem solving, and about
the run-time behaviors of solvers as they struggle to
solve the instances.
The research is fundamentally iterative in nature. We
exploit learning methods to identify and continue to

refine observational variables and models, balancing
the predictive power of multiple observations with the
cost of the real-time evaluation of such evidential dis-

HORVITZ ET AL.

236

tinctions. We seek ultimately to harness the learned
models to optimize the performance of automated rea­
soning procedures. Beyond this direct goal, the overall
exploratory process promises to be useful for providing
new insights about problem hardness.
We first provide background on the problem solving
domains we have been focusing on. Then, we describe
our efforts to instrument problem solvers and to learn
predictive models for run time. We describe the for­
mulation of variables we used in data collection and
model construction and review the accuracy of the in­
ferred models. Finally, we discuss opportunities for
exploiting the models. We focus on the sample appli­
cation of generating context-sensitive restart policies
in randomized search algorithms.
2

Hard Search Problems

UAI2001

distinct symbols in which some cells may be empty
but no row or column contains the same element twice.
The Quasigroup Completion Problem (QCP) can be
stated as follows: Given a partial quasigroup of order
n can it be completed to a quasigroup of the same
order?

n

Figure 2: Graphical representation of the quasigroup
problem. Left: A quasigroup instance with its comple­
tion. Right: A balanced instance with two holes per
row/column.

We have focused on applying learning methods to char­

acterize run times observed in backtracking search pro­
cedures for solving NP-complete problems encoded as
constraint satisfaction (CSP) and Boolean satisfiabil­
ity (SAT). For these problems, it has proven extremely
difficult to predict the particular sensitivities of run
time to changes in instances, initialization settings,
and solution policies. Numerous studies have demon­
strated that the probability distribution over run times
exhibit so-called heavy-tails [1 0 ]. Restart strategies
have been used in an attempt to find settings for an
instance that allow it to be solved rapidly, by avoiding
costly journeys into a long tail of run time. Restarts
are introduced by way of a parameter that terminates
the run and restarts the search from the root with a
new random seed after some specified amount of time
passes, measured in choices or backtracks.
Progress on the design and study of algorithms for
SAT and CSP has been aided by the recent devel­
opment of new methods for generating hard random
problem instances. Pure random instances, such as
k-Sat, have played a key role in the development of al­
gorithms for propositional deduction and satisfiability
testing. However, they lack the structure that char­
acterizes real world domains. Gomes and Selman [9]
introduced a new benchmark domain based on Quasi­
groups, the Quasigroup Completion Problem (QCP) .
QCP captures the structure that occurs in a variety of
real world problems such as timetabling, routing, and
statistical experimental design.
A quasigroup is a discrete structure whose multipli­
cation table corresponds to a Latin Square. A Latin
Square of order n is an n x n array in which n dis­
tinct symbols are arranged so that each symbol occurs
once in each row and column. A partial quaisgroup (or
Latin Square) of order n is an n x n array based on

QCP is an NP-complete problem [5] and random in­
stances have been found to exhibit a peak in prob­
lem hardness as a function of the ratio of the number
of uncolored cells to the total number of cells. The
peak occurs over a particular range of values of this
parameter, referred to as a region of phase transition
[9, 2]. A variant of the QCP problem, Quasigroup with
Holes (QWH) [ 2] , includes only satisfiable instances.
The QWH instance-generation procedure essentially
inverts the completion task: it begins with a randomly­
generated completed Latin square, and then erases col­
ors or "pokes holes." Completing QWH is NP-Hard
[2]. A structural property that affects hardness of in­
stances significantly is the pattern of the holes in row
and columns. Balancing the number holes in each row
and column of instances has been found to significantly
increase the hardness of the problems [1].
3

Experiments with Problem Solvers

We performed a number of experiments with Bayesian
learning methods to elucidate previously hidden dis­
tinctions and relationships in SAT and CSP reason­
ers. We experimented with both a randomized SAT
algorithm running on Boolean encodings of the QWH
and a randomized CSP solver for QWH. The SAT al­
gorithm was Satz-Rand [11], a randomized version of
the Satz system of Li and Anbulagan [20]. Satz is the
fastest known complete SAT algorithm for hard ran­
dom 3-SAT problems, and is well suited to many inter­
esting classes of structured satisfiability problems, in­
cluding SAT encodings of quasigroup completion prob­
lems [10] and planning problems [17]. The solver is a
version of the classic Davis-Putnam (DPLL) algorithm
[7] augmented with one-step lookahead and a sophisti-

UAI2001

cated variable

HORVITZ ET AL.

choice heuristic. The lookahead opera­

3.1

237

Formulating Evidential Variables

tion is invoked at most choice points and finds any

choices that would immediately lead
contradiction after unit propagation; for these,
the opposite variable assignment can be immediately
made. The variable ch oice heuristic is based on picking
a variable that if set would cause the greatest number
of ternary clauses to be reduced to binary clauses. The
variable choice set was enlarged by a noise parameter
of 30%, and value selection was performed determin­
istically by always branching on 'true' first.

variable/value
to a

The second backtrack search algorithm we studied is
randomized version of a specialized CSP solver for
quasigroup completion problem s, written using the
ILOG solver constraint programming library. The
backtrack search algorithm uses as a variable choice
heuristic a variant of the Brelaz heuristic. Further­
more, it uses a sophisticated propagation method to
enforce the constraints that assert that all the colors
in a row/ column must be different. We refer to such
a constraint as alldiff. The propagation of the alldiff
constraint corresponds to solving a matching problem
on a bipartite graph using a network-flow algorithm
[9, 26, 24].
a

learned predictive models for run-time, motivated
two different classes of target problems. For the
first class of problem, we assume that a solver is chal­
lenged by a n instance and must solve that specific
problem as quickly as possible. We term this the Sin­
gle Instance problem. In a second class of problem,
we draw cases from a distribution of instances and are
required to solve any instance as soon as possible, or
as many instances as possible for any amount of time
allocated. We call these challenges Multiple Instance
problems, and the subproblems as the Any Instance
and Max Instances problems , respectively.
We

by

We collected evidence and built models for CSP and
Satz solvers applied to the QWH problem for both
the Single In st an ce and Multiple Instances challenge.
We shall refer to the four problem-solving experiments
as CSP-QWH-Single, CSP-QWH-Multi, Satz -QW H ­
Single, and S atz-Q WH- Multi. Building predictive
Bayesian models for the CSP- Q WH-S ingle and Satz­
QWH-Single problems centered on gathering data on
the probabilistic relationships between observational
variables and run time for single instances with ran­
domized restarts. Experiments for the CSP-QWH­
Multi and S atz -Q WH- Multi problems centered on per­
forming single runs on multiple instances drawn from
the same instance generator.

We worked to define variables that we believed could
provide information on problem-solving progress for a
period of observation in an early phase of runs that we
refer to as th e observation horizon. The defin iti on of
variables was initially guided by intuition. However,
results from our early experiments helped us to refine
sets of variables and to propose additional candidates.
We initially explored a large number of variables, in­
cluding those that were difficult to compute. Although
we planned ultimately to avoid the use of costly ob­
servations in real-time forecasting settings, we were
interested in probing the predictive power and inter­
dependencies among features regardless of cost. Un­
der st andin g such informational dependencies promised
to be useful in understanding the potential losses in
predictive power with the removal of costly features,
or substitution of expensive evidence with less expen­
sive, approximate observations. We eventually limited
the features explored to those that could be computed
with low (constant) overhead.
We sought to collect information about base values as
well as several variants and combinations of these val­
ues. For example, we formulated features that could
capture higher-l evel patterns and dynamics of the state
of a prob l em solver that could serve as useful probes
of solution progress. Beyond exploring base observa­
tions about the program state at particular points in
a case, we defined new families of observations such as
first and second derivatives of the base variables, and
summaries of the status of variables over time.

Rather than include a separate variable in the model
for each feature at each choice point-which would
have led to an explosion in the number of variables
and severely limited generalization-features and their
dynamics were represented by variables for their sum­
mary statistics over the observation horizon. The sum­
mary statistics included initial, final, average, mini­
mum, and maximum values of the features during the
observation period. For example, at each choice point,
the SAT solver recorded the current number of binary
clauses. The training data would thus included a vari­
able for the average first derivative of t he number of
binary clauses during the observation period. Finally,
for several of the features, we also computed a sum­
mary statistic that measured the number of times the
sign of the feature changed from negative to positive
or vice-versa.
We developed distinct sets of observational var iables
for the CSP and Satz solvers. The features for the
CSP solver included some that were generic to any
constraint satisfaction problem, such as the number
of backtracks, the depth of the search tree, and the

HORVITZ ET AL.

238

average domain size of the unbound CSP variables.
Other features, such as the variance in the distribution
of unbound CSP variables between different columns
of the square, were specific to Latin squares. As we
will see below, the inclusion of such domain-specific
features was important in learning strongly predictive
models. The CSP solver recorded 18 basic features
at each choice point which were summarized by a to­
tal of 135 variables. The variables that turned out
to be most informative for prediction are described in
Sec. 4.1 below.
The features recorded by Satz-Rand were largely
generic to SAT. We included a feature for the num­
ber of Boolean variables that had been set positively;
this feature is problem specific in the sense that under
the SAT encoding we used, only a positive Boolean
variable corresponds to a bound CSP variable (i.e. a
colored squared). Some features measured the current
problem size (e.g. the number of unbound variables),
others the size of the search tree, and still others the
effectiveness of unit propagation and lookahead.
We also calculated two other features of special note.
One was the logarithm of the total number of possible
truth assignments (models) that had been ruled out
at any point in the search; this quantity can be effi­
ciently calculated by examining the stack of assumed
and proven Boolean variable managed by the DPLL
algorithm. The other is a quantity from the theory of
random graphs called .\, that measures the degree of
interaction between the binary clauses of the formula
(23]. In all Satz recorded 25 basic features that were
summarized in 127 variables.

4

Collecting Run-Time Data

For all experiments, observational variables were col­
lected over an observational horizon of 1000 solver
choice points. Choice points are states in search pnr
cedures where the algorithm assigns a value to vari­
ables heuristically, per the policies implemented in the
problem solver. Such points do not include the cases
where variable assignment is forced via propagation of
previous set values, as occurs with unit propagation,
backtracking, lookahead, and forward-checking.
For the studies described, we represented run time as a
binary variable with discrete states short versus long.
We defined short runs as cases completed before the
median of the run times for all cases in each data set.
Instances with run times shorter than the observation
horizon were not considered in the analyses.

Models and Results

We employed Bayesian structure learning to infer pre­
dictive models from data and to identify key variables
from the larger set of observations we collected. Over
the last decade, there has been steady progress on
methods for inferring Bayesian networks from data
[6, 27, 12, 13]. Given a dataset, the methods typically
perform heuristic search over a space of dependency
models and employ a Bayesian score to identify mod­
els with the greatest ability to predict the data. The
Bayesian score estimates p(modelldata) by approxi­
mating p( data lmodel)p( model). Chickering, Hecker­
man and Meek [4] show how to evaluate the Bayesian
score for models in which the conditional distributions
are decision trees. This Bayesian score requires a prior
distribution over both the parameters and the struc­
ture of the model. In our experiments, we used a uni­
form parameter prior. Chickering et al. suggest using
a structure prior of the form: p(model) r;,fP, where
0 < ,.. :::; 1 and fp is the number of free parameters in
the model. Intuitively, smaller values of r;, make large
trees unlikely a priori, and thus ,.. can be used to help
avoid overfitting. We used this prior, and tuned r;, as
described below.
=

We employed the methods of Chickering et a!. to infer
models and to build decision trees for run time from
the data collected in experiments with CSP and Satz
problem solvers applied to QWH problem instances.
We shall describe sample results from the data col­
lection and four learning experiments, focusing on the
CSP-QWH-Single case in detail.
4.1

3.2

UAI2001

CSP-QWH-Single Problem

For a sample CSP-QWH-Single problem, we built a
training set by selecting nonbalanced QWH problem
instance of order 34 with 380 unassigned variables. We
solved this instance 4000 times for the training set and
1000 times for the test data set, initiating each run
with a random seed. We collected run time data and
the states of multiple variables for each case over an
observational horizon of 1000 choice points. We also
created a marginal model, capturing the overall run­
time statistics for the training set.
We optimized the r;, parameter used in the structure
prior of the Bayesian score by splitting the training set
70/30 into training and holdout data sets, respectively.
We selected a kappa value by identifying a soft peak
in the Bayesian score. This value was used to build a
dependency model and decision tree for run time from
the full training set. We then tested the abilities of the
marginal model and the learned decision tree to pre­
dict the outcomes in the test data set. We computed
a classification accuracy for the learned and marginal

UAI2001

HORVITZ ET AL.

models to characterize the power of these models. The
classification accuracy is the likelihood that the classi­
fier will correctly identify the run time of cases in the
test set. We also computed an average log score for
the models.
Fig. 3 displays the learned Bayesian network for this
dataset. The figure highlights key dependencies and
variables discovered for the data set. Fig. 4 shows the
decision tree for run time.
The classification accuracy for the learned model is
0.963 in contrast with a classification accuracy of 0.489
for the marginal model. The average log score of the
learned model is -0.134 a.nd the average log score of
the marginal model was -0.693.
Because this was both the strongest and most com­
pact model we learned, we will discuss the features it
involves in more detail. Following Fig. 4 from left to
right, these are:
VarRowColumn measures the variance in the number
of uncolored cells in the QWH instance across rows
and across columns. A low variance indicates the open
cells are evenly balanced throughout the square. As
noted earlier, balanced instances are harder to solve
than unbalanced ones [1]. A rather complex summary
statistic of this quantity appears at the root of the de­
cision tree, namely the minimum of the first derivative
of this quantity during the observation period. In fu­
ture work we will be examining this feature carefully
in order to determine why this particular statistic was
most relevant.
AvgColumn measures the ratio of the number of uncol­
ored cells and the number of columns or rows. A low
value for this feature indicates that the quasigroup is
nearly complete. The decision tree shows that a run is
likely to be fast if the min i mum value of this quantity
over the entire observation period is small.
MinDepth is the minimum depth of all leaves of the
search tree, and the summary statistic is simply the fi­
nal value of this quantity. The third and fourth nodes
of the decision tree show that short runs are associ­
ated with high minimum depth and long runs with
low minimum depth. This may be interpreted as in­
dicating the search trees for the shorter runs have a
more regular shape.
AvgDepth is the average depth of a node in the search
tree. The model discovers that short runs are associ­
ated with a high frequency in the change of the sign
of the first derivative of the average depth. In other
words, frequent fluctuations up and down in the aver­
age depth indicate a short run. We do not yet have an
intuitive explanation for this phenomena.

239

VarRowColumn appears again as the last node in the
decision tree. Here we see that if the maximum vari­
ance of the number of uncolored cells in the QWH
instance across rows and columns is low (i.e., the prob­
lem remains balanced) then the run is long, as might
be expected.
4.2

CSP-QWH-Multi Problem

For a CSP-QWH-Multi problem, we built training and
test sets by selecting instances of nonbalanced QWH
problems of order 34 with 380 unassigned variables.
We collected data on 4000 instances for the training
set and 1000 instances for the test set.
As we were running instances of potentially different
fundamental hardnesses, we normalized the feature
measurements by the size of the instance (measured in
CSP variables) after the instances were initially sim­
plified by forward-checking. That is, although all the
instances originally had the same number of uncolored
cells, polynomial time preprocessing fills in some of the
cells, thus revealing the true size of the instance.
We collected run time data for each instance over
an observational horizon of 1000 choice points. The
learned model was found to have a classification accu­
racy of 0.715 in comparison to the marginal model ac­
curacy of 0.539. The average log score for the learned
model was found to be -0.562 and the average log score
for the marginal model was -0.690.
4.3

Satz-QWH-Single Problem

We performed analogous studies with the Satz solver.
In a study of the Satz-QWH-Single problem, we stud­
ied a single QWH instance (bqwh-34-410-16). We
found that the learned model had a classification ac­
curacy of 0.603, in comparison to a classification accu­
racy of 0.517 for the marginal model. The average log
score of the learned model was found to be -0.651 and
the log score of the marginal model was -0.693.
The predictive power of the SAT model was less than
that of the corresponding CSP model. This is reason­
able since the CSP model had access to features that
more precisely captured special features of quasigroup
problems (such as balance). The decision tree was still
relatively small, containing 12 nodes that referred to
10 different summary variables.
Observations that turned out to be most relevant for
the SAT model included:
•

The maximum number of variables set to 'true'
during the observation period. As noted earlier,
this corresponds to the number of CSP variables
that would be bound in the direct CSP encoding.

HORVITZ ET AL.

240

UAl 2001

Figure 3: The learned Bayesian network for a sample CSP-QWH-Single problem. Key dependencies and variables
are highlighted.

I:Y'· •I
/

6

/-

/

Nat< -5.03 (1174)

Nat< 233 (187]

Not< 19.S (543)

. } ,:,1
/
........

< -5.03 (22�)

Nat < 3 .33 (897)

<3.33(1

�

< 19.S (354]

Not< 18.9 (322)

�

•18.9[

1.3(�)

<21.3

�

Figure 4: The decision tree inferred for run time from data gathered in a CSP-QWH-Single experiment. The
probability of a short run is captured by the light component of the bargraphs displayed at the leaves.

UAI2001

HORVITZ ET AL.

•

The number of models ruled out.

•

The number of unit propagations performed.

•

•

4.4

The number of variables eliminated by Satz's
lookahead component: that is, the effectiveness
o f lookahead.
The quantity ..\ described in Sec. 3.1 above, a mea­
sure of the constrainedness of the binary clause
subproblem.
Satz-QWH-Multi Problem

For the experiment with the Satz-QWH-Multi prob­
lem, we executed single runs of QWH instances
with the same parameters as the instance studied in
the Satz-QWH-Single Problem (bqwh-34-410) for the
training and test sets. Run time and observational
variables were normalized in the same manner as for
the CSP-QWH-Multi problem. The classification ac­
curacy of the learned model was found to be 0. 715.
The classification accuracy of the marginal model was
found to be 0.526. The average log score for the model
was -0.557 and the average log score for the marginal
model was -0.692.
4.5

Toward Larger Studies

For broad application in guiding computational prob­
lem solving, it is important to develop an understand­
ing of how results for sample instances, such as the
problems described in Sections 4.1 through 4.4, gener­
alize to new instances within and across distinct classes
of problems. We have been working to build insights
about generalizability by exploring the statistics of the
performance of classifiers on sets of problem instances.
The work on studies with larger numbers of data sets
has been limited by the amount of time required to
generate data sets for the hard problems being stud­
ied. With our computing platforms, several days of
computational effort were typically required to pro­
duce each data set.
As an example of our work on generalization, we re­
view the statistics of model quality and classification
accuracy, and the regularity of discriminatory features
for additional data sets of instances in the CSP-QWH­
Single problem class.
We defined ten additional nonbalanced QWH problem
instances, parameterized in the same manner as the
CSP problem described in Section 4.1 (order 34 with
380 unassigned variables). We employed the same data
generation and analysis procedures as before, building
and testing ten separate models. Generating data for
these analyses using the ILOG libary executed on an

241

Intel Pentium III (running at 600 Mhz) required ap­
proximately twenty-four hours per 1000 runs. Thus,
each CSP dataset required approximately five days of
computation.
In summary, we found significant boosts in classi­
fication accuracy for all of the instances. For the
ten datasets, the mean classification accuracy for the
learned models was 0.812 with a standard deviation of
0.101. The average log score for the models was -0.388
with a standard deviation of 0.167. The predictive
power of the learned models stands in contrast to the
classification accuracy of using background statistics;
the mean classification accuracy of the marginal mod­
els was 0.497 with a standard deviation of 0.025. The
average log score for the marginal models was -0.693
with a standard deviation of 0.001. Thus, we observed
relatively consistent predictive power of the methods
across the new instances.
We observed variation in the tree structure and dis­
criminatory features across the ten learned models.
Nevertheless, several features appeared as valuable
discriminators in multiple models, including statistics
based on measures of VarRowColumn, AvgColumn,
AvgDepth, and MinDepth. Some of the evidential fea­
tures recurred for different problems, showing signifi­
cant predictive value across models with greater fre­
quency than others. For example, measures of the
maximum variation in the number of uncolored cells
in the QWH instance across rows and columns (Max­
VarRowColumn) appeared as being an important dis­
criminator in many of the models.
5

Generalizing Observation Policies

For the experiments described in Sections 3 and 4, we
employed a policy of gathering evidence over an obser­
vation horizon of the initial 1000 choice points. This
observational policy can be generalized in several ways.
For example, in addition to harvesting evidence within
the observation horizon, we can consider the amount
of time expended so far during a run as an explicit
observation. Also, evidence gathering can be general­
ized to consider the status of variables and statistics
of variables at progressively later times during a run.
Beyond experimenting with different observational
policies, we believe that there is potential for harness­
ing value-of-information analyses to optimize the gath­
ering of information. For example, there is opportu­
nity for employing affine analysis and optimization to
generate tractable real-time observation policies that
dictate which evidence to evaluate at different times
during a run, conditioned on evidence that has already
been observed during that run.

242

5.1

HORVITZ ET AL.

Time Expended

as

Evidence

In the process of exploring alternate observation
policies, we investigated the value of extending the
bounded-horizon policy described in Section 3, with
a consideration of the status of time expended so far
during a run. To probe potential boosts with inclusion
of time expended, we divided several of the data sets
explored in Section 4.5 into subsets based on whether
runs with the data set had exceeded specific run-time
boundaries. Then, we built distinct run-time-specific
models and tested the predictive power of these models
on test sets containing instances of appropriate mini­
mal length. Such time-specific models could be used
in practice as a cascade of models, depending on the
amount of time that had already been expended on a
run.
We typically found boosts in the predictive power of
models built with such temporal decompositions. As
we had expected, the boosts are greatest for models
conditioned on the largest amounts of expended time.
As an example, let us consider one of the data sets
generated for the study in Section 4.5. The model
that had been built previously with all of the data
had a classification accuracy of 0. 793. The median
time for the runs represented in the set was nearly
18,000 choice points. We created three separate sub­
sets of the complete set of runs: the set of runs that
exceeded 5,000 choice points, the set that exceeded
8,000 choice points, and the set that had exceeded
11,000 choice points. We created distinct predictive
models for each training set and tested these mod­
els with cases drawn from test sets containing runs of
appropriate minimal length. The classification accu­
racies of the models for the low, medium, and high
time expenditure were 0.779, 0.799, and 0.850 respec­
tively. We shall be continuing to study the use of time
allocated as a predictive variable.
6

Application: Dynamic Restart
Policies

A predictive model can be used in several ways to
control a solver. For example, the variable selection
heuristic used to decompose the problem instance can
be designed to minimize the expected solution time
of the subproblems. Another application centers on
building distinct models to predict the run time as­
sociated with different global strategies. As an ex­
ample, we can learn to predict the relative perfor­
mance of ordinary chronological backtrack search and
dependency-directed backtracking with clause learn­
ing [16]. Such a predictive model could be used to
decide whether the overhead of clause learning would
be worthwhile for a particular instance.

UA12001

Problem and instance-specific predictions of run time
can also be used to drive dynamic cutoff decisions on
when to suspend a current case and restart with a new
random seed or new problem instance, depending on
the class of problem. For example, consider a greedy
analysis, where we deliberate about the value of ceas­
ing a run that is in progress and performing a restart
on that instance or another instance, given predictions
about run time. The predictive models described in
this paper can provide the expected time remaining
until completion of a current run. Initiating a new
run will have an expected run time provided by the
statistics of the marginal model. From the perspec­
tive of a single-step analysis, when the expected time
remaining for the current instance is greater than the
expected time of the next instance, as defined by the
background marginal model, it is better to cease ac­
tivity and perform a restart. More generally, we can
construct richer multistep analyses that provide the
fastest solutions to a particular instance or the highest
rate of completed solutions with computational effort.
We can also use the predictive models to perform com­
parative analyses with previous policies. Luby et al.
[21] have shown that the optimal restart policy, as­
suming full knowledge of the distribution, is one with
a fixed cutoff. They also provide a universal strat­
egy ( using gradually increasing cutoffs) for minimizing
the expected cost of randomized procedures, assum­
ing no prior knowledge of the probability distribution.
They show that the universal strategy is within a log
factor of optimal. These results essential settle the
distribution-free case.
Consider now the following dynamic policy: Observe a
run for 0 steps. If a solution is not found, then predict
whether the run will complete within a total of L steps.
If the prediction is negative, then immediately restart;
otherwise continue to run for up to a total of L steps
before restarting if no solution is found.
An upper bound on the expected run of this policy can
be calculated in terms of the model accuracy A and the
probability Pi of a single run successfully ending in i
or fewer steps. For simplicity of exposition we assume
that the model's accuracy in predicting long or short
runs is identical. The expected number of runs until
a solution is found is E(N)
1/(A(PL- Po)+ Po).
An upper bound on the expected number of steps in
a single run can be calculated by assuming that runs
that end within 0 steps take exactly 0 steps, and that
runs that end in 0+ 1 to L steps take exactly L steps.
The probability that the policy continues a run past 0
steps (i.e., the prediction was positive) is APL + (1A) ( 1- PL). An upper bound on the expected length of
a single run is Eub(R) 0 + (L- O)(APL + (1- A) (1PL)). Thus, an upper bound on the expected time to
=

=

UAI2001

solve a

HORVITZ ET AL.

proble m

E(N)Eub(R).

using the policy is

It is important to note that the expected time depends
on both the accuracy of the model and the prediction
point L; in general, one would want to vary L in or­
der to optimize the solution time.

Furthermore, in

general, it would be better to design more sophisti­
cated dynamic policies that made use of all informa­
tion gathered over a run, rather than just during the
first 0 steps. But even a non-optimized policy based
directly on the models discussed in this paper can out­
perform the optimal fixed policy. For example, in the
CSP-QWH-single problem case, the optimal fixed pol­
icy has an expected solution

time of 38,000 steps, while

the dynamic policy has an expected solution time of
only

27,000

steps. Optimizing the choice of L should

provide about an order of magnitude further improve­
ment.

243

c1s1ons about the partition of resources

formulation and inference.
and Klein

[14]

between

re­

In other work, Horvitz

constructed Bayesian models consid­

ering the time expended so far in theorem proving.
They monitored the progress of search in a proposi­
tional theorem prover and used measures of progress
in updating the probability of truth or falsity of as­

sertions. A Bayesian model was harnessed to update
belief about different outcomes as a function of the
amount of time that problem solving continued with­
out halting. Stepping back to view the larger body of
work on the decision-theoretic control of computation,
measures of

expected value of computation [15,

8,

25],

employed to guide problem solving, rely on forecasts
of the refinements of partial results with future com­
putation. More generally, representations of problem­
solving progress have been central in research on flex­
ible or anytime methods-procedures that exhibit a

While it may not be surprising that a dynamic policy

relatively smooth surface of performance

can outperform the optimal fixed policy, it is interest­

location of computational resources.

with the al­

ing to note that this can occur when the observation
time 0 is

greater

than the fixed cutoff.

That is, for

proper values of L and A, it may be worthwhile to ob­
serve each run for

1000 steps

even if the optimal fixed

strategy is to cutoff after 500 st eps. These and other

issues concerning applications of prediction models to
restart policies are examined in detail in a forthcoming
paper.

8

Future Work and Directions

This work represents a vector in a space of ongoing re­
search. We are pursuing several lines of research with
the goals of enhancing the power and generalizing the
applicability of the predictive methods. We are explor­
ing the modeling of run time at a finer grain through
the use of continuous variables and prototypical named

7

distributions. We are also exploring the value of de­

Related Work

composing the learning problem into models that pre­

Learning methods have been employed in previous re­
search in a attempt to enhance the performance opti­
mize reasoning systems. In work on "speed-up learn­
ing," investigators have attempted to increase plan­
ning efficiency by learn i ng goal-specific preferences for

plan operators

[22, 19].

Khardon and Roth explored

the offline reformulation of representations based on
experiences with problem solving in an environment
to enhance run-time efficiency

[18].

Our work on using

probabilistic models to learn about algorithmic perfor­
m ance and to guide problem solving is most c losely re­

lated to research on flexible computation and decision­
theoretic control. Related work in this arena focused
on the use of predictive models to control computa­
tion, Breese and Horvitz [3] collected data about the

dict the average execution times seen with multiple
runs and models that predict how well a particular in­
stance will do relative to the overall hardness of the
problem.

In other extensions, we are exploring the

feasibility of inferring the likelihood that an instance
is solvable versus unsolvable and building models that
forecast the overall expected run time to completion
by conditioning on each situation. We are also inter­
ested in pursuing more general, dynamic observational
policies and in harnessing the value of information to
identify a set of conditional decisions about the pattern
and timing of monitoring. F inally, we are continuing
to investigate the formulation and testing of ideal poli­
cies for harnessing the predictive models to optimize
restart policies.

progress of search for graph cliquing and of cutset anal­
ysis for use in minimizing the time of probabilistic in­

ference with Bayesian networks.

9

Summary

The work was mo­

tivated by the challenge of identifying the ideal time

We presented a methodology for characterizing the run

for preprocessing graphical models for faster inference
before initiating inference, trading off reformulation

time of problem instances for randomized backtrack­
style search algorithms that have been developed to

time for inference time.

Trajectories of progress as

solve a hard class of structured constraint-satisfaction

a function of

of Bayesian network prob­

parameter s

lem instances were learned for use in dynamic de-

problems. The methods are motivated

by recent suc­

cesses with using fixed restart policies to address the

HORVITZ ET AL.

244

UAI 2001

high variance in running time typically exhibited by

[12]

backtracking search algorithms. We described two dis­
tinct formulations of problem-solving goals and b uilt

D. Beckerman , J. Breese, and K . Rommelse. Decision­
theoretic troubleshooting. CA CM, 38:3:49-57, 1995.

[13]

D . Beckerman, D . M . Chickering, C. Meek, R. Roun­
thwaite, and C. Kadie. Dependency networks for den­
sity estimation, collaborative filtering, and data visu­
alization. In Proceedings of UA I-2000, Stanford, CA,
pages 82-88. 2000.

[14]

E. Horvitz and A. Klein. Reasoning, metareasoning,
and mathematical truth: Studies of theorem proving
under limited resources. In Proceedings of UA I- 95,
pages 306-314, Montreal, Canada, August 1995. Mor­
gan Kaufmann, San Francisco.

[15]

E . J . Horvitz. Reasoning under varying and uncer­
tain resource constraints. In Proceedings of A A A I-88,
pages 1 1 1-1 16. Morgan Kaufmann, San Mateo, CA,
August 1988.

butions and feedback.

[16]




A simple advertising strategy that can be
used to help increase sales of a product is
to mail out special offers to selected poten­
tial customers. Because there is a cost as­
sociated with sending each offer, the optimal
mailing strategy depends on both the ben­
efit obtained from a purchase and how the
offer affects the buying behavior of the cus­
tomers. In this paper, we describe two meth­
ods for partitioning the potential customers
into groups, and show how to perform a sim­
ple cost-benefit analysis to decide which, if
any, of the groups should be targeted. In par­
ticular, we consider two decision-tree learning
algorithms. The first is an "off the shelf" al­
gorithm used to model the probability that
groups of customers will buy the product.
The second is a new algorithm that is sim­
ilar to the first, except that for each group, it
explicitly models the probability of purchase
under the two mailing scenarios: (1) the mail
is sent to members of that group and (2) the
mail is not sent to members of that group.
Using data from a real-world advertising ex­
periment, we compare the algorithms to each
other and to a naive mail-to-all strategy.

1

INTRODUCTION

Consider an advertiser who has a large list of potential
customers for his product. For a specific real example,
we will use Microsoft as the advertiser and a Microsoft
Network (MSN) subscription as the product of inter­
est. The potential customers are the people who have
registered Windows 95 with Microsoft. Because regis­
tering Windows 95 involves filling out a questionnaire,
Microsoft has access to lots of useful information about
all of the potential MSN subscribers. A typical adver-

tising strategy is to mail out advertisements, perhaps
including a special offer for a reduced monthly rate,
to a set of potential customers in the hopes that this
offer will entice them into signing up for MSN.
Before deciding how to target, the advertiser may be
able to perform a preliminary study to determine the
effectiveness of the campaign. In particular, the ad­
vertiser can choose a small subset of the potential cus­
tomers and randomly mail the advertisement to half
of them. Based on the data collected from the exper­
iment, the advertiser can make good decisions about
which members of the remaining population should be
targeted.
Perhaps the most obvious approach is to mail all Win­
dows 95 registrants the advertisement for MSN. As
described by Hughes (1996), such a mass marketing
or mail-to-all strategy can often be cost effective. An­
other strategy that has gained a lot of attention in re­
cent years (e.g. Ling and Li, 1998) is to apply machine­
learning techniques to identify those customers who
are most likely to subscribe to MSN, and concentrate
the campaign on this subset. Assuming that there is a
cost to mail the special offer, both strategies may yield
negative expected return, and it is unlikely that either
strategy will yield the optimal expected return.
In this paper, we describe methods for using experi­
mental data to identify groups of potential customers
for which targeting those groups will yield high ex­
pected profit for the advertiser. Our approach differs
from the machine-learning techniques we identified in
the literature by explicitly using expected profit in­
stead of expected response as our objective. In Sec­
tion 2, we describe how to make the decision whether
or not to target a particular group by using a sim­
ple cost-benefit analysis with the data collected from
the experiment. In Section 3, we describe methods for
dividing the population into groups, with the specific
goal of maximizing revenue. In Section 4, we present
the results from applying our techniques to real-world
data. Finally, in Section 5, we conclude with a discus-

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

sion of future direction for this work.
2

MAKING THE RIGHT DECISION

In this section, we show how to use the data from an
experiment to decide whether or not to send an ad­
vertisement to a particular set of potential customers.
To understand the problem with the obvious strate­
gies, it is useful to consider how an individual will
respond to both receiving and not receiving the adver­
tisement. For any individual, there are only four pos­
sible response behaviors he can have. The first behav­
ior, which we call always-buy, describes a person who
is going to subscribe to MSN, regardless of whether or
not he receives the advertisement. The second behav­
ior, which we call persuadable, describes a person who
will subscribe to MSN if he receives the offer and will
not subscribe to MSN if he does not receive the offer.
The third behavior, which we call anti-persuadable, is
the opposite of persuadable: the person will subscribe
to MSN if and only if he does not receive the offer
(perhaps this type of person is offended by the adver­
tisement). Finally, the fourth behavior, which we call
never-buy, describes a person who is not going to sub­
scribe to MSN, regardless of whether he receives the
advertisement.
Assuming that the subscription price exceeds the mail­
ing cost, the optimal strategy is to mail the offer to
the persuadable potential customers; for each poten­
tial customer that is not persuadable, we lose money
by targeting him. If we target an always-buyer, we lose
both the cost of the mailing and the difference between
the regular subscription price (which the always-buyer
was willing to pay) and the (potentially reduced) price
that we offer in the advertisement. If we target a never­
buyer, we lose the cost of the mailing. The worst is
to mail to an anti-persuadable person; in this case,
we lose both the cost of the mailing and the regular
subscription price.
A potential problem with the mail-to-all strategy is
that the advertiser is necessarily mailing to all of
the always-buy, never-buy and anti-persuadable cus­
tomers. The likely-buyer strategy can be problematic
as well if a large percent of the people who subscribe
are always-buyers.
It is very unlikely that we will ever be able to identify
individual response behaviors of potential customers.
We can, however, use experimental data to learn about
the relative composition of response behaviors within
groups of potential customers to easily decide whether
or not it is profitable to mail to people in those groups.
Let NAlw, NPers, NAnti, and NNever denote the num­
ber of people in some population with behavior always-

2000

83

buy, persuaded, anti-persuaded, and never-buy, re­
spectively, and let N denote the total number of people
in that population. Let c denote the cost of sending
out the mailing, let ru denote the revenue that results
from an unsolicited subscription, and let r8 denote the
revenue that results from a solicited subscription (ru
minus any discount from the offer). The expected gain
from mailing to a person in a population with the given
composition is

That is, we pay c to send out the mail; if the person is
an always buyer (probability NAlw/ N) or a persuaded
person (probability NPers/ N), then he will pay r8• If
the person has either of the other two behaviors, he
will not pay us anything. Similarly, the expected gain
from not mailing is

That is, the always-buyers and the anti-persuaded will
pay the unsolicited price ru if they do not receive the
advertisement; the other two types of people will not
subscribe.
Given our analysis, the decision of whether or not to
mail to a member of the population is easy: send out
the advertisement to a person if the expected gain from
mailing is larger than the expected gain from not mail­
ing.

Or equivalently:

·r s +-

(NAlw + NAnti)
N

u

·r

-c > Q

(1)
We call the left side of the above inequality the expected
lift in profit, or ELP for short, that results from the
mailing.
Both fractions in the above equation are identifiable
(that is, they can be estimated from data). In par­
ticular, (NA lw + NPers)/N is precisely the fraction of
people who will subscribe to MSN if they receive the
advertisement, and consequently we can estimate this
fraction by mailing to a set of people and keeping track
of the fraction of people who sign up for MSN. Sim­
ilarly, (NAlw + NAnti)/N is precisely the fraction of
people who subscribe to MSN if they do not receive
the advertisement, and consequently we can estimate
this fraction by NOT mailing to a set of people and
keeping track of the fraction of people who sign up for
MSN.

84

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

Let M be the binary variable that denotes whether or
not a person was sent the mailing, with values m0 (not
mailed) and m1 (mailed). Let S be the binary variable
that denotes whether or not a person subscribes to
MSN, with values s0 (did not subscribe) and s1 (did
subscribe). Using these variables, we can re-write the
(identifiable) fractions involved in the expected lift as
(MLE) probabilities:
(NAlw

+

N
(NAlw

+

NPers)
NAnti)

N

p(S= s t iM= mo)

Note that MAP estimates for these probabilities can
be obtained instead from the given fractions and prior
knowledge. Plugging into the definition of ELP (left
side of Equation 1) we have:

2000

the values for the variables in X to define the sub­
populations that may have different values for ELP.
The statistical model we build is one for the proba­
bility distribution p(SIM, X). There are many model
classes that can be used to represent this distribution,
including generalized linear models, support vector
machines, and Bayesian networks. In this paper, we
concentrate on decision trees which are described by
(e.g.) Breiman, Friedman, Olshen and Stone (1984).
The probability distribution p(S I M, X) can be used
to calculate the ELP for anyone in the population. In
particular, if we know the values { x1, ... ,Xn} for the
person, we have:

ELP =
r s p(S = StiM = mt,Xt = X],...,Xn = Xn)
-ru. p(S = StiM = mo,Xt = Xj, . . ,Xn = Xn)
·

.

-c

ELP=
rs p(S = StiM = mt)
-ru p(S= StiM = mo)

(2)

·

·

- c

In the next section, we describe methods for automat­
ically identifying sub-populations that yield large ex­
pected lifts in profit as a result of the mailing. As
an example, the expected profit from mailing to the
entire population (i.e. using the mail-to-all strategy)
may be negative, but our methods might discover that
there is lots of money to be earned by mailing to the
sub-population of females.
3

IDENTIFYING PROFITABLE
TARGETS

In this section, we describe how to use the data col­
lected from the randomized experiment to build a sta­
tistical model that can calculate the ELP for anyone
in the population. In particular, we introduce a new
decision-tree learning algorithm that can be used to di­
vide the population of potential customers into groups
for the purpose of maximizing profit in an advertising
campaign.
The experimental data consists of, for each person, a
set of values for all distinctions in the domain of inter­
est. The distinctions in the domain necessarily include
the two binary variables M (whether or not we mailed
to the person) and S (whether or not the person sub­
scribed to MSN) that were introduced in the previous
section. We use X= {X1,...,Xn} to denote the other
distinctions that are in our data. These distinctions
are precisely those that we collected in the Windows
95 registration process. The statistical model uses

A decision tree T can be used to represent the dis­
tribution of interest. The structure of a decision tree
is a tree, where each internal node I stores a map­
ping from the values of a predictor variable Xj (or M)
to the children of I in the tree. Each leaf node L in
the tree stores a probability distribution for the target
variable S. The probability of the target variable S,
given a set of values {M = m,X1 = x1, ... ,Xn= Xn}
for the predictor variables, is obtained by starting at
the root of T and using the internal-node mappings
to traverse down the tree to a leaf node. We call the
mappings in the internal nodes splits. When an inter­
nal node I maps values of variable Xj (or M) to its
children, we say that Xj is the split variable of node
I, and that I is a split on X1.
For example, the decision tree shown in Figure 1 stores
a probability distribution p(S I M, X1,X2). In the ex­
ample, X1 has two values {1,2}, and X2 has three
values {1,2, 3}. In the figure, the internal nodes are
drawn with circles, and the leaf nodes are drawn with
boxes. As we traverse down the tree, the splits at each
internal node are described by the label of the node
and by the labels of the out-going edges. In partic­
ular, if the current internal node of the traversal is
labeled with X;, we move next to a child of that node
by following the edge that is labeled with the given
value x;.
Given values {X1 = 1, X2 = 2, M= m0} for the pre­
dictors, we obtain p(S I X t = 1, X2 = 2, M = m0} by
traversing the tree in Figure 1 as follows (the traver­
sal for this prediction is emphasized in the figure by
dark edges). We start at the root node of the tree,
and see that the root node is a split on X2• Because
x2
2, we traverse down the right-most child of the
root. This next internal node is a split on X1, which
=

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

2000

85

(1996) and Chickering, Beckerman and Meek (1997)
both grow decision trees to represent the conditional
distributions in Bayesian networks.

Figure 1: Example decision tree for the distribution
p(SIM,X)
is has value 1, so we move next to the left child of
this node. Finally, because M = m0, we move to the
right child, which is a leaf node. We extract the condi­
tional distribution directly from the leaf, and conclude
that p(S = s1IX1 = 1, Xz = 2, M = mo) = 0.2 and
p(S = so!X1 = 1, X2 = 2, M = mo) 0. 8.
=

As an example of how we would use this tree for tar­
geting, suppose we have a potential customer with val­
ues xl = 1, x2 = 2, and we would like to decide if we
should mail to him. Further suppose that ru = 10,
rs = 8, and c = 0.5. We plug these constants and the
probabilities extracted from the tree into Equation 2
to get:

ELP = 8

X

0.4 - 10 X 0.2 - 0.5 = 0. 7

Because the expected lift in profit is positive, we would
send the mailing to the person. Note that under the
given cost / benefit scenario we should not send the
mailing to a person for which x2 = 1 or x2 = 3, even
though mailing to such a person increases the chances
that he will subscribe to MSN.
There are several types of splits that can exist in a
decision tree. A complete split is a split where each
value of the split variable maps to a separate child.
Examples of complete splits in the figure are splits on
the binary variables. Another type is a binary split,
where the node maps one of the values of the split
variable to one child, and all other values of the split
variable to another. The root node is a binary split in
the figure.
Decision trees are typically constructed from data us­
ing a greedy search algorithm in conjunction with a
scoring criterion that evaluates how good the tree is
given the data. See Breiman et al. (1984) for ex­
amples of such algorithms. Buntine (1993) applies
Bayesian scoring to grow decision trees; in our experi­
ments, we use a particular Bayesian scoring function to
be described in Section 4. Friedman and Goldszmidt

The objective of these traditional decision-tree learn­
ing algorithms is to identify the tree that best models
the distribution of interest, namely p(SjM, X). That
is, the scoring criterion evaluates the predictive ac­
curacy of the tree. In our application, however, the
primary objective is to maximize profit, and although
the objectives are related, the tree that best models
the conditional distribution may not be the most use­
ful when making decisions about who to mail in our
campaign. We now consider a modification that can
be made to a standard decision-tree learning algorithm
that more closely approximates our objective.
Recall that the expected lift in profit is the differ­
ence between two probabilities: one probability where
M = m1 and the other probability where M = m0.
Consequently, it might be desirable for the decision­
tree algorithm (or any statistical model learning algo­
rithm) to do its best to model the difference between
these two probabilities rather than to directly model
the conditional distribution. In the case of decision
trees, one heuristic that can facilitate this goal is to
insist that there be a split on M along any path from
the root node to a leaf node in the tree.
One approach to ensure this property, which is the
approach we took in our experiments, is to insist that
the last split on any path is on M. Whereas most tree
learning algorithms grow trees by replacing leaf nodes
with splits, algorithms using this approach need to be
modified to replace the last split (on M) in the tree
with a subtree that contains a split on some variable
X; E X, followed by a (last ) split on M for each child
1
of the node that splits X; . An example of such a
replacement is shown in Figure 2. In Figure 2a, we
show a decision tree where the last split on every path
is on M. In Figure 2b we show a replacement of one
of these splits that might be considered by a typical
learning algorithm.
Note that because leaves used to compute the ELP for
any person are necessarily siblings in these trees, it is
easy to describe an advertising decision in terms of the
other variables. In particular, any path from the root
node to an M split describes a unique partition of
the population, and the sibling leaf nodes determine
the mailing decision for all of the members of that
population. As an example, a path in the tree to a
split on M might correspond to males who have lots
11n fact, our implementation of this algorithm does not
explicitly apply the last split in the tree. Instead, our scor­
ing criterion

is modified to evaluate the tree

a last split on M for every leaf node.

as

if there was

86

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

2000

the previous section. The second algorithm, which we
call NORMAL, simply tries to maximize the scoring
criterion, without forcing any splits on M.

(a)

(b)

Figure 2: Example of learning trees with the last split
required to be on M. (a) A candidate solution consid­
ered by the learning algorithm and (b) a replacement
of the last split to create a new candidate solution.
of memory in their computer; the fact that this group
of people has a high or low ELP may be particularly
interesting to the advertiser.
The hope is that forcing the split on M will steer learn­
ing algorithms to trees that are good at predicting
ELP. Because we are forcing the last split on M, how­
ever, the final tree may consist of splits on M that do
not yield statistically significant differences between
the probabilities in sibling leaf nodes. This is poten­
tially problematic, because any such differences are
amplified when computing ELP (see Equation 2), and
this may lead to bad decisions, particularly in situa­
tions when the response benefit is particularly high.
To avoid the problem of statistical insignificance, we
post-process the decision trees. In particular, we first
remove all of the (last) splits on M in the final tree
if doing so increases the score (according to whatever
scoring criterion we used to grow the tree). Next, we
repeat the following two steps until no change to the
tree is made: (1) delete all last non-M splits, (2) if any
leaf node does not have a parent that is a split on M,
replace that leaf node with a split on M if doing so
increases the score for the model.
In the following section, we evaluate how well a greedy
tree-growing algorithm performs using the techniques
described in this section.
4

EXPERIMENTAL RESULTS

In this section, we present the results from applying
two greedy decision-tree learning algorithms to the
data collected from an experiment in advertising for
MSN subscriptions. The first algorithm, which we call
FORCE, searches for trees that have the last split on
M, and then post-processes the tree as described in

The MSN advertising experiment can be described
as follows. A random sample of Windows 95 regis­
trants was divided into two groups. People in the first
group, consisting of roughly ninety percent of the sam­
ple, were mailed an advertisement for an MSN sub­
scription, whereas the people in the other group were
not mailed anything. After a specified period of time
the experiment ended, and it was recorded whether or
not each person in the experiment signed up for MSN
within the given time period. The advertisement did
not offer a special deal on the subscription rate (that
is, rs = ru ) .
We evaluated the two algorithms using a sample of ap­
proximately 110 thousand records from the experimen­
tal data. Each record corresponds to a person in the
experiment who has registered Windows 95. For each
record, we know whether or not an advertisement was
mailed (M), and whether or not the person subscribed
to MSN within the given time period (S). Addition­
ally, each record contains the values for 15 variables;
these values were obtained from the registration form.
Examples of variables include gender and the amount
of memory in the person's computer.
We divided the data into a training set and a testing
set, consisting of 70 percent and 30 percent, respec­
tively, of our original sample. Using the training set,
we built trees for the distribution p(SIM, X) using the
two algorithms FORCE and NORMAL.
For both algorithms, we used a Bayesian scoring cri­
terion to evaluate candidate trees. In particular, we
used a uniform parameter prior for all tree parameters,
and a structure prior of 0.001K, where K is the num­
ber of free parameters that the structure can support.
Both algorithms were simple greedy searches that re­
peatedly grew the tree by applying binary splits until
reaching a local maximum in the scoring criterion.
To evaluate an algorithm given a cost-benefit scenario
(i.e. given c and rs = ru), we used the test set to esti­
mate the expected revenue per person obtained from
using the resulting tree to guide the mailing decisions.
In particular, for each record in the test set, we calcu­
lated the expected lift in profit using Equation 2, and
decided to send the mail if and only if the lift was posi­
tive. We should emphasize that the known value for M
in each record was ignored when making the decisions;
the values for M are "plugged in" directly to Equation
2. Next, we compared our recommendation (mail or
do not mail) to what actually happened in the exper­
iment. If our recommendation did not match what
happened, we ignored the record and moved on to the

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

87

next. Otherwise2, we checked whether or not the cor­
responding person subscribed to MSN; if he did, we
added r8 - c to our total revenue, and if he did not,
we added -c . Finally, we divided our total revenue by
the number of records for which our recommendation
matched the random assignment in the experiment to
get an expected revenue per person.
For comparison purposes, we also calculated the per­
person expected revenue for the simple strategy of
mailing to everyone. Then, for both of the algorithms,
we measured the improvement in the corresponding
per-person revenue over the per-person revenue from
the mail-to-all strategy. We found that comparing the
algorithms using these improvements was very useful
for analyzing multiple cost/benefit scenarios; the im­
provement from using a tree strategy over using the
mail-to-all strategy converges to a small number as the
benefit from the advertisement grows large, whereas
the per-person revenue from a tree strategy will con­
tinue to increase with the benefit.
Figure 3 shows our results using a single c = 42 cents
and varying r8 = ru from 1 to 15 dollars. For both
algorithms, the improvement in the per-person revenue
over the mail-to-all per-person revenue is plotted for
each value of rs.
The new algorithm FORCE slightly outperforms the
simple algorithm NORMAL for benefits less than ten
dollars, but for larger benefits the trees yield identi­
cal decisions. Although the improvements over the
mail-to-all strategy decrease with increasing subscrip­
tion revenue, they will never be zero, and the strategy
resulting from either tree will be preferred to the mail­
to-all strategy in this domain. The reason is that both
models have identified populations for which the mail­
ing is either independent of the subscription rate, or
for which the mailing is actually detrimental.

0.25

0.2
---*-FORCE
- -o- -

F

..!!!
0

NORMAL

0.15

e.
tn
c
Cll

E

�
e

c.

.E

0.1

0.05

DISCUSSION

5

In this paper we have discussed how to use machine­
learning techniques to help in a targeted advertising
campaign. We presented a new decision-tree learning
algorithm that attempts to identify trees that will be
particularly useful for maximizing revenue in such a
campaign. Because experimental data of the type used
in Section 4 is difficult to obtain, we were only able to
evaluate our algorithm in a single domain.
An interesting question is why the new approach only
provided marginal improvement over the simpler al­
gorithm. It turns out that for the MSN domain, all
2

In our experiments, the number of times that our rec­

ommendation matched the experiment ranged from a low
of roughly 5,000 times to a high of roughly 25,000 times

Benefit

(Dollars)

Figure 3: Expected per-person improvement over the
mail-to-all strategy for both algorithms, using c = 42
cents and varying rs = ru from 1 to 15

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

88

of the trees learned using the off-the-shelf algorithm
had splits on M for the majority of the paths from
the root to the leaves. That is, the condition that mo­
tivated our new algorithm in the first place is almost
satisfied using the simple algorithm. We expect that
in general this will not be the case, and that our al­
gorithm will prove to lead searches to better trees for
targeting.
An obvious alternative approach that meets our split­
on-M criterion is to make the first split in the tree on
M. Although our heuristic criterion is met, the ap­
proach clearly does not encourage a greedy algorithm
to identify trees that predict ELP = p(S = s1[M
ml )
p(S = S1[M = m0 ) . In fact, this approach
is equivalent to independently learning two decision
trees: one for the data where M = m0 and another
for the data where M = m1. In experiments not pre­
sented in this paper, we have found that the approach
results in significantly inferior trees.
=

-

An interesting extension to this work is to consider
campaigns where the advertisement offers a special
price. In fact, if the experiment consisted of mailing
advertisements of various discounts, we could use our
techniques to simultaneously identify the best discount
and corresponding mailing strategy.



We describe a graphical representation of
probabilistic relationships-an alternative to
the Bayesian network-called a dependency
network. Like a Bayesian network, a depen­
dency network has a graph and a probabil­
ity component. The graph component is a
(cyclic) directed graph such that a node's
parents render that node independent of all
other nodes in the network. The probabil­
ity component consists of the probability of
a node given its parents for each node (as in
a Bayesian network) . We identify several ba­
sic properties of this representation, and de­
scribe its use in collaborative filtering (the
task of predicting preferences) and the visu­
alization of predictive relationships.
Keywords: Dependency networks, graphical models,
inference, data visualization, exploratory data analy­
sis, collaborative filtering, Gibbs sampling
1

Introduction

The Bayesian network has proven to be a valuable tool
for encoding, learning, and reasoning about probabilis­
tic relationships. In this paper, we introduce another
graphical representation of such relationships called
a dependency network. The representation can be
thought of as a collection of regression/classification
models among variables in a domain that can be com­
bined using Gibbs sampling to define a joint distribu­
tion for that domain. The dependency network has
several advantages and disadvantages with respect to
the Bayesian network. For example, a dependency net­
work is not useful for encoding causal relationships and
is difficult to construct using a knowledge-based ap­
proach. Nonetheless, in our three years of experience
with this representation, we have found it to be easy to

learn from data and quite useful for encoding and dis­
playing predictive (i.e., dependence and independence)
relationships. In addition, we have empirically verified
that dependency networks are well suited to the task of
predicting preferences-a task often referred to as col­
laborative filtering. Finally, the representation shows
promise for density estimation and probabilistic infer­
ence.
The representation was conceived independently by
Hofmann and Tresp (1997), who used it for density es­
timation; and Hofmann (2000) investigated several of
its theoretical properties. In this paper, we summarize
their work, further investigate theoretical properties of
the representation, and examine its use for collabora­
tive filtering and data visualization.
In Section 2, we define the representation and describe
several of its basic properties. In Section 3, we de­
scribe algorithms for learning a dependency network
from data, concentrating on the case where the local
distributions of a dependency network (similar to the
local distributions of a Bayesian network) are encoded
using decision trees.
In Section 4, we describe the
task of collaborative filtering and present an empirical
study showing that dependency networks are almost
as accurate as and computationally more attractive
than Bayesian networks on this task. Finally, in Sec­
tion 5, we show how dependency networks are ideally
suited to the task of visualizing predictive relationships
learned from data.
2

Dependency Networks

To describe dependency networks and how we learn
them, we need some notation. We denote a variable
by a capitalized token (e.g., X, X;, 0, Age), and the
state or value of a corresponding variable by that same
token in lower case (e.g., x, x;, 8, age). We denote a
set of variables by a bold-face capitalized token (e.g.,
X, X;, Pa;). We use a corresponding bold-face lower­
case token (e.g., x, x;, pa;) to denote an assignment of

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

state or value to each variable in a given set. We use
p(X xiY = y) (or p(xiy) as a shorthand) to denote
the probability that X = x given Y = y. We also
use p (x iy) to denote the probability distribution for X
given Y (both mass functions and density functions).
Whether p(xiy) refers to a probability, a probability
density, or a probability distribution will be clear from
context.
=

Consider a domain of interest having variables X =
(X1 , ... , Xn ) . A dependency network for X is a pair
(9, P) where q is a (cyclic) directed graph and P is a
set of probability distributions. Each node in q corre­
sponds to a variable in X. We use X; to refer to both
the variable and its corresponding node. The parents
of node X;, denoted Pa;, correspond to those variables
Pa; that satisfy
(1)
The distributions in P are the local probability distributions p (x; j p a; ), i = 1, . . . , n. We do not require
the distributions p ( x; x1, ...,Xi-1,Xi+1, ..., xn) , i =
1, .. . , n to be obtainable (via inference) from a sin­
gle joint distribution p(x). If they are, we say that the
dependency network is consistent with p (x). We shall
say more about the issue of consistency later in this
section.

l

A Bayesian network for X defines a joint distribution
for X via the product of its local distributions. A
dependency network for X also defines a joint distri­
bution for X, but in a more complicated way via a
Gibbs sampler (e.g., Gilks, Richardson, and Spiegel­
halter, 1996). In this Gibbs sampler, we initial­
ize each variable to some arbitrary value. We then
repeatedly cycle through each variable X1 , ... , Xn,
in this order, and resample each X; according to
p(x;ix1, ...,Xi-1,Xi+1, ..., Xn) = p (x ; lp a; ). We call
this procedure an ordered Gibbs sampler. As described
by the following theorem (also proved in Hofmann,
2000), this ordered Gibbs sampler defines a joint dis­
tribution for X.
Theorem 1: An ordered Gibbs sampler applied to a
dependency network for X, where each X; is discrete
and each local distribution p ( x; IPa;) is positive, has a
unique stationary joint distribution for X.

xt

tth

Proof: Let be the sample of x after the
iteration
of the ordered Gibbs sampler. The sequence x1, x2, . . .
can be viewed as samples drawn from a homogenous
Markov chain with transition matrix M having ele­
ments Mjli = p ( xt+1 =
= i). (We use the termi­
nology of Feller, 1957.) It is not difficult to see that
M is the product M1
Mn, where Mk is the "lo­
cal" transition matrix describing the resampling of Xk

jlxt

·

.

.

.

·

265

according to the local distribution p(xk IPak)· The pos­
itivity of local distributions guarantees the positivity
of M, which in turn guarantees (1) the irreducibility
of the Markov chain and (2) that all of the states are
ergodic. Consequently, there exists a unique joint dis­
tribution that is stationary with respect to M. 0
Because the Markov chain described in the proof is
irreducible and ergodic, after a sufficient number of
iterations, the samples in the chain will be drawn from
the stationary distribution for X. Consequently, these
samples can be used to estimate this distribution.
Note that the Theorem holds for both consistent and
inconsistent dependency networks. Furthermore, the
restriction to discrete variables can be relaxed, but
will not be discussed here. In the remainder of this
paper, we assume all variables are discrete and each
local distribution is positive.
In addition to determining a joint distribution, a de­
pendency network for a given domain can be used
to compute any conditional distribution of interest­
that is, perform probabilistic inference. We discuss
an algorithm for doing so, which uses Gibbs sampling,
in Heckerman, Chickering, Meek, Rounthwaite, and
Kadie (2000). That Gibbs sampling is used for in­
ference may appear to be a disadvantage of depen­
dency networks with respect to Bayesian networks.
When we learn a Bayesian network from data, how­
ever, the resulting structures are typically complex and
not amenable to exact inference. In such situations,
Gibbs sampling (or even more complicated Monte­
Carlo techniques) are used for inference in Bayesian
networks, thus weakening this potential advantage.
In fact, when we have data and can learn a model
for X, dependency networks have an advantage over
Bayesian networks. Namely, we can learn each local
distribution in a dependency network independently,
without regard to acyclicity constraints.
Bayesian networks have one clear advantage over de­
pendency networks. In particular, dependency net­
works are not suitable for the representation of causal
relationships. For example, if X causes Y (so that
X and Y are dependent), the corresponding depen­
dency network is X +-+ Y -that is, X is a parent of Y
and vice versa. It follows that dependency networks
are difficult to elicit directly from experts. Without
an underlying causal interpretation, knowledge-based
elicitation is cumbersome at best.
Another important observation about dependency net­
works is that, when we learn one from data as
we have described-learning each local distribution
independently-the model is likely to be inconsistent.
(In an extreme case, where (1) the true joint distribu-

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

266

tion lies in one of the possible models, (2) the model
search procedure finds the true model, and (3) we
have essentially an infinite amount of data, the learned
model will be consistent.) A simple approach to avoid
this difficulty is to learn a Bayesian network and apply
inference to that network to construct the dependency
network. This approach, however, will eliminate the
advantage associated with learning dependency net­
works just described, is likely to be computationally
inefficient, and may produce extremely complex local
distributions. When ordered Gibbs sampling is applied
to an inconsistent dependency network, it is important
to note that the joint distribution so defined will de­
pend on the order in which the Gibbs sampler visits
the variables. For example, consider the inconsistent
dependency network X +- Y. If we draw sample-pairs
(x, y)-that is, x and then y-then the resulting sta­
tionary distribution will have X and Y independent.
In contrast, if we draw sample-pairs (y, x), then the
resulting stationary distribution may have X and Y
dependent.
The fact that we obtain a joint distribution from any
dependency network, consistent or not, is comforting.
A more important question, however, is what distri­
bution do we get? The following theorem, proved in
Heckerman et al. (2000), provides a partial answer.
Theorem 2: If a dependency network for X is con­
sistent with a positive distribution
then the sta­
tionary distribution defined in Theorem 1 is equal to

p(x),

p(x).

When a dependency network is inconsistent, the situa­
tion is even more interesting. If we start with learned
local distributions that are only slight perturbations
(in some sense) of the true local distributions, will
Gibbs sampling produce a joint distribution that is a
slight perturbation of the true joint distribution? Hof­
mann (2000) argues that, for discrete dependency net­
works with positive local distributions, the answer to
this question is yes when perturbations are measured
with an L2 norm. In addition, Heckerman et al. (2000)
show empirically using several real datasets that the
joint distributions defined by a Bayesian network and
dependency network, both learned from data, are sim­
ilar.
We close this section with several facts about consis­
tent dependency networks, proved in Heckerman et al.
(2000). We say that a dependency network for X is
bi-directional if X; is a parent of Xj if and only if Xj is
a parent of X; , for all X; and Xj in X. We say that a
distribution
is consistent with a dependency net­
work structure if there exists a consistent dependency
network with that structure that defines

p(x)

p(x).

Theorem 3: The set of positive distributions consis­
tent with a dependency network structure is equal to
the set of positive distributions defined by a Markov­
network structure with the same adjacencies.
Note that, although dependency networks and Markov
networks define the same set of distributions, their rep­
resentations are quite different. In particular, the de­
pendency network includes a collection of conditional
distributions, whereas the Markov network includes a
collection of joint potentials.
Let pa{ be the lh parent of node X; . A consistent de­
pendency network is minimal if and only if, for every
node X; and for every parent pa{ , X; is not indepen­
dent of pa{ given the remaining parents of X; .
Theorem 4: A minimal consistent dependency net­
work for a positive distribution
must be bi­
directional.

p(x)

3

Learning Dependency Networks

In this section, we mention a few important points
about learning dependency networks from data.
When learning a dependency network for X, each local
distribution for X; is simply a regression/classification
model (with feature selection) for x; with X\ {xi} as
inputs. If we assume that each local distribution has
a parametric model p( x; !Pa;, B;), and ignore the de­
pendencies among the parameter sets Bt, ..., ()n, then
we can learn each local distribution independently us­
ing any regression/classification technique for mod­
els such as a generalized linear model, a neural net­
work, a support-vector machine, or an embedded re­
gression/classification model (Heckerman and Meek,
1997). From this perspective, the dependency network
can be thought of as a mechanism for combining re­
gression/classification models via Gibbs sampling to
determine a joint distribution.
In the work described in this paper, we use decision
trees for the local distributions. A good discussion of
methods for learning decision trees is given in Breiman,
Friedman, Olshen, and Stone (1984). We learn a deci­
sion tree using a simple hill-climbing approach in con­
junction with a Bayesian score as described in Fried­
man and Goldszmdit (1996) and Chickering, Hecker­
man, and Meek (1997). To learn a decision tree for
X; , we initialize the search algorithm with a single­
ton root node having no children. Then, we replace
each leaf node in the tree with a binary split on some
variable Xj in X\ X; , until no such replacement in­
creases the score of the tree. Our binary split on Xj is

267

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

a decision-tree node with two children: one of the chil­
dren corresponds to a particular value of Xj, and the
other child corresponds to all other values of Xj. Our
Bayesian scoring function uses a uniform prior distri­
bution for all decision-tree parameters, and a structure
prior proportional to KJ, where K > 0 is a tunable pa­
rameter and f is the number of free parameters in the
decision tree. In studies that predated those described
in this paper, we have found that the setting K = 0.01
yields accurate models over a wide variety of datasets.
We use this same setting in our experiments.
For comparison in these experiments, we also learn
Bayesian networks with decision trees for local distri­
butions using the algorithm described in Chickering,
Heckerman, and Meek (1997). When learning these
networks, we use the same parameter and structure
priors used for dependency networks.
We conclude this section by noting an interesting fact
about the decision-tree representation of local distri­
butions. Namely, there will be a split on variable X
in the decision tree for Y if and only if there is an
arc from X to Y in the dependency network that in­
cludes these variables. As we shall see in Section 5,
this correspondence helps the visualization of data.
4

Collaborative Filtering

In the remainder of this paper, we consider useful ap­
plications of dependency networks, whether they be
consistent or not.
The first application is collaborative filtering ( CF), the
task of predicting preferences. Examples of this task
include predicting what movies a person will like based
on his or her ratings of movies seen, predicting what
new stories a person is interested in based on other
stories he or she has read, and predicting what web
pages a person will go to next based on his or her
history on the site. Another important application in
the burgeoning area of e-commerce is predicting what
products a person will buy based on products he or
she has already purchased and/or dropped into his or
her shopping basket.
Collaborative filtering was introduced by Resnick, la­
covou, Suchak, Bergstrom, and Riedl (1994) as both
the task of predicting preferences and a class of al­
gorithms for this task. The class of algorithms they
described was based on the informal mechanisms peo­
ple use to understand their own preferences. For ex­
ample, when we want to find a good movie, we talk
to other people that have similar tastes and ask them
what they like that we haven't seen. The type of algo­
rithm introduced by Resnik et al. (1994), sometimes
called a memory-based algorithm, does something simi-

lar. Given a user's preferences on a series of items, the
algorithm finds similar users in a database of stored
preferences. It then returns some weighted average of
preferences among these users on items not yet rated
by the original user.
As done in Breese, Heckerman, and Kadie (1998),
let us concentrate on the application of collaborative
filtering-that is, preference prediction. In their pa­
per, Breese et al. (1998) describe several CF sce­
narios, including binary versus non-binary preferences
and implicit versus explicit voting. An example of
explicit voting would be movie ratings provided by a
user. An example of implicit voting would be know­
ing only whether a person has or has not purchased
a product. Here, we concentrate on one scenario im­
portant for e-commerce: implicit voting with binary
preferences-for example, the task of predicting what
products a person will buy, knowing only what other
products they have purchased.
A simple approach to this task, described in Breese et
al. (1998), is as follows. For each item (e.g., prod­
uct), define a variable with two states corresponding
to whether or not that item was preferred (e.g., pur­
chased). We shall use "0" and "1" to denote not
preferred and preferred, respectively. Next, use the
dataset of ratings to learn a Bayesian network for the
joint distribution of these variables X= (X1, ..., Xn)·
The preferences of each user constitutes a case in the
learning procedure. Once the Bayesian network is
constructed, make predictions as follows. Given a
new user's preferences x, use the Bayesian network
to determine p(Xi = 1lx \ Xi = 0) for each prod­
uct Xi not purchased. That is, infer the probability
that the user would have purchased the item had we
not known he did not. Then, return a list of recom­
mended products-among those that the user did not
purchase-ranked by this probability.
Breese et al. (1998) show that this approach out­
performs memory-based and cluster-based methods
on several implicit rating datasets. Specifically, the
Bayesian-network approach was more accurate and
yielded faster predictions than did the other methods.
What is most interesting about this algorithm in the
context of this paper is that only the probabilities
p(X; = 1lx \X; = 0) are needed to produce the recom­
mendations. In particular, these probabilities may be
obtained by a direct lookup in a dependency network:
p(Xi

=

1lx \X;= 0)

=

p(Xi

=

1lpa; )

(2)

where pai is the instance of Pai consistent with X.
Thus, dependency networks are a natural model on
which to base CF predictions. In the remainder of
this section, we compare this approach with that based

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

268

Table 1: Number of users, items, and items per user
for the datasets used in evaluating the algorithms.
Users in tra.mmg set
Users in test set

Total items
Mean items per user
in training set

Dataset
MS.COM
Nielsen
1,637
32,711
1,637
5,000
294
3.02

203
8.64

MSNHC
10,000
10,000
1,001
2.67

on Bayesian networks for datasets containing binary
implicit ratings.
Datasets

4.1

We evaluated Bayesian networks and dependency net­
works on three datasets: (1) Nielsen, which records
whether or not users watched five or more minutes of
network TV shows aired during a two-week period in
1995 (made available courtesy of Nielsen Media Re­
search), (2) MS. COM, which records whether or not
users of microsoft.com on one day in 1996 visited ar­
eas "vroots" ) of the site (available on the Irvine Data
Mining Repository), and (3) MSNBC, which records
whether or not visitors to MSNBC on one day in 1998
read stories among the most popular 1001 stories on
the site. The MSNBC dataset contains 20,000 users
sampled at random from the approximate 600,000
users that visited the site that day. In a separate anal­
ysis on this dataset, we found that the inclusion of ad­
ditional users did not produce a substantial increase
in accuracy. Table 4.1 provides additional information
about each dataset. All datasets were partitioned into
training and test sets at random.

(

4.2

Evaluation Criteria and Experimental
Procedure

We have found the following three criteria for collab­
orative filtering to be important: (1) the accuracy of
the recommendations, (2) prediction time-the time
it takes to create a recommendation list given what
is known about a user, and (3) the computational re­
sources needed to build the prediction models. We
measure each of these criteria in our empirical com­
parison. In the remainder of this section, we describe
our evaluation criterion for accuracy.
Our criterion attempts to measure a user's expected
utility for a list of recommendations. Of course, dif­
ferent users will have different utility functions. The
measure we introduce provides what we believe to be
a good approximation across many users.
The scenario we imagine is one where a user is shown

a ranked list of items and then scans that list for pre­
ferred items starting from the top. At some point, the
denote
user will stop looking at more items. Let
the probability that a user will examine the kth item
on a recommendation list before stopping his or her
scan, where the first position is given by k= 0. Then,
a reasonable criterion is

p(k)

cfaccuracyt (list)=

LP(k) c5k
k

(3)

where c5k is 1 if the item at position k is preferred and 0
otherwise. To make this measure concrete, we assume
that p(k) is an exponentially decaying function:

p(k)= Tk/a

(4)

where a is the "half-life" position-the position at
which an item will be seen with probability 0.5. In
our experiments, we use a= 5.
In one possible implementation of this approach, we
could show recommendations to a series of users
and ask them to rate them as "preferred" or "not
preferred" .
We could then use the average of
cfaccuarcy1 (list) over all users as our criterion. Be­
cause this method is extremely costly, we instead use
an approach that uses only the data we have. In par­
ticular, as already described, we randomly partition a
dataset into a training set and a test set. Each case
in the test set is then processed as follows. First, we
randomly partition the user's preferred items into in­
put and measurement sets. The input set is fed to the
CF model, which in turn outputs a list of recommen­
dations. Finally, we compute our criterion as
N

100 "'"' Lk cS;k
.
cfaccuracy(hst) =
K 1
N L...
i=l L:k,;,;

p(k)
p(k)

(5)

where N is the number of users in the test set, K; is
the number of preferred items in the measurement set
for user i, and c5;k is 1 if the kth item in the recommen­
dation list for user i is preferred in the measurement
set and 0 otherwise. The denominator in Equation 5
is a per-user normalization factor. It is the utility of a
list where all preferred items are at the top. This nor­
malization allows us to more sensibly combine scores
across measurement sets of different size.
We performed several experiments reflecting differing
numbers of ratings available to the CF engines. In the
first protocol, we included all but one of the preferred
items in the input set. We term this protocol all but 1.
In additional experiments, we placed 2, 5, and 10 pre­
ferred items in the input sets. We call these protocols
given 2, given 5, and given 10.
The all but 1 experiments measure the algorithms' per­
formance when given as much data as possible from

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

each test user. The various given experiments look at
users with less data available, and examine the perfor­
mance of the algorithms when there is relatively little
known about an active user. When running the given
m protocols, if an input set for a given user had less
than m preferred items, the case was eliminated from
the evaluation. Thus the number of trials evaluated
under each protocol varied.

Table 2: CF accuracy for the MS.COM, Nielsen, and
MSNBC datasets. Higher scores indicate better per­
formance. Statistically significant winners are shown
in boldface.
MS.COM

Algorithm
BN
DN

All experiments were performed on a 300 MHz Pen­
tium II with 128 MB of memory, running the NT 4.0
operating system.
4.3

269

Given2
53.18

52.68

Tables 3 and 4 compare the two methods with
the remaining criteria. Here, dependency networks
are a clear winner. They are significantly faster
at prediction-sometimes by almost an order of
magnitude-and require substantially less time and
memory to learn.

AllBut1
66.54
66.60

0. 30

0. 73

1.62

0.34

Baseline

43.37

39.34

39.32

49.77

Given2

Given10
33.84
33.80

AllBut1

24.20

GivenS
30.03
29.71

RD

0. 32

0. 40

0.65

0.72

Baseline

12.65

12.72

12.92

13.59

Nielsen

Algorithm
BN
DN

Table 2 shows the accuracy of recommendations for
Bayesian networks and dependency networks across
the various protocols and three datasets. For a com­
parison, we also measured the accuracy of recommen­
dation lists produced by sorting items on their overall
popularity, p(X; = 1). The accuracy of this approach
is shown in the row labeled "Baseline." A score in
boldface corresponds to a significantly significant win­
ner. We use ANOVA (e.g., McClave and Dietrich,
1988) with a = 0.1 to test for statistical significance.
When the difference between two scores in the same
column exceed the value of RD (required difference),
the difference is significant.

The magnitudes of accuracy differences, however, are
not that large. In particular, the ratio of ( cfac­
curacy(BN) - cfaccuracy(DN)) to (cfaccuracy(BN) cfaccuracy(Baseline)) averages 4 ± S percent across the
datasets and protocols.

Given10
51.64
51.48

RD

Results

From the table, we see that Bayesian networks are
more accurate than dependency networks. This re­
sult is interesting, because there are reasons to ex­
pect that dependency networks will be more accurate
than Bayesian networks and vice versa. On the one
hand, the search process that learns Bayesian net­
works is constrained by acyclicity, suggesting that de­
pendency networks may be more accurate. On the
other hand, the conditional probabilities used to sort
the recommendations are inferred from the Bayesian
network, but learned directly in the dependency net­
work. Therefore, dependency networks may be less
accurate, because they waste data in the process of
learning what could otherwise be inferred. For this
or perhaps other reasons, the Bayesian networks are
more accurate.

GivenS
52.48
52.54

24.99

45.55

44.30

MSNBC

Algorithm
BN
DN

Given2

GivenS

40.34

34.20

38.84

32.S3

Given10
30.39
30.03

RD

0. 35

0. 77

1.54

0. 39

Baseline

28.73

20.58

14.93

32.94

AllBut1
49.58

48.0S

Overall, Bayesian networks are slightly more accurate
but much less attractive from a computational per­
spective.
5

Data Visualization

Bayesian networks are well known to be useful for
visualizing causal relationships. In many circum­
stances, however, analysts are only interested in
predictive-that is, dependency and independency­
relationships. In our experience, the directed-arc se­
mantics of Bayesian networks interfere with the visu­
alization of such relationships.
As a simple example, consider the Bayesian network
X --+ Y. Those familiar with the semantics of
Bayesian networks immediately recognize that observ­
ing Y helps to predict X. Unfortunately, the untrained
individual will not. In our experience, this person will
interpret this network to mean that only X helps to
predict Y, and not vice versa. Even people who are
expert in d-separation semantics will sometimes have
difficulties visualizing predictive relationships using a
Bayesian network. The cognitive act of identifying a
node's Markov blanket seems to interfere with the vi­
sualization experience.
Dependency networks are a natural remedy to this

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

270

problem. If there is no arc from X to Y in a depen­
dency network, we know immediately that X does not
help to predict Y.

Table 3: Number of predictions per second for the
MS.COM, Nielsen, and MSNBC datasets.
MS.COM

Algorithm
BN
DN

Given2
3.94
23.29

Given5
3.84
19.91

Algorithm
BN
DN

Given2
22.84
36.17

Given5
21.86
36.72

Algorithm
BN
DN

Given2
7.21
11.88

Given5
6.96
11.03

Given10
3.29
10.20

AllBut1
3.93
23.48

Nielsen

Given10
20.83
34.21

AllBut1
23.53
37.41

MSNBC

Given10
6.09
8.52

AllBut1
7.07
11.80

Table 4: Computational resources for model learning.
MS.COM

Algorithm
BN
DN

Memory (Meg)
42.4
5.3

Algorithm
BN
DN

Memory (Meg)
3.3
2.1

Algorithm
BN
DN

Memory (Meg)
43.0
3.7

Learn Time (sec)
144.65
98.31

Nielsen

Learn Time (sec)
7.66
6.47

MSNBC

Learn Time (sec)
105.76
96.89

Figure 1 shows a dependency network learned from
a dataset obtained from Media Metrix. The dataset
contains demographic and internet-use data for about
5,000 individuals during the month of January 1997.
On first inspection of this network, an interest­
ing observation becomes apparent: there are many
(predictive) dependencies among demographics, and
many dependencies among frequency-of-use, but there
are few dependencies between demographics and
frequency-of-use.
Over the last three years, we have found numerous
interesting dependency relationships across a wide va­
riety of datasets using dependency networks for visu­
alization. In fact, we have given dependency networks
this name because they have been so useful in this
regard.
The network in Figure 1 is displayed in DNViewer,
a dependency-network visualization tool developed at
Microsoft Research. The tool allows a user to display
both the dependency-network structure and the de­
cision tree associated with each variable. Navigation
between the views is straightforward. To view a de­
cision tree for a variable, a user simply double clicks
on the corresponding node in the dependency network.
Figure 2 shows the tree for Shopping.Freq.
An inconsistent dependency net learned from data of­
fers an additional advantage for visualization. If there
is an arc from X to Y in such a network, we know that
X is a significant predictor of Y -significant in what­
ever sense was used to learn the network. Under this
interpretation, a uni-directional link between X and Y
is not confusing, but rather informative. For example,
in Figure 1, we see that Sex is a significant predictor
of Socioeconomic status, but not vice versa-an in­
teresting observation. Of course, when making such
interpretations, one must always be careful to recog­
nize that statements of the form "X helps to predict
Y" are made in the context of the other variables in
the network.
In DNViewer, we enhance the ability of dependency
networks to reflect strength of dependency by includ­
ing a slider (on the left). As a user moves the slider
from bottom to top, arcs are added to the graph in the
order in which arcs are added to the dependency net­
work during the learning process. When the slider is
in its upper-most position, all arcs (i.e., all significant
dependencies) are shown.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

271

Figure 1: A dependency network for Media Metrix data. The dataset contains demographic and internet-use
data for about 5,000 individuals during the month of January 1997. The node labeled Overall.Freq represents
the overall frequency-of-use of the internet during this period. The nodes Search.Freq, Edu.Freq, and so on
represent frequency-of-use for various subsets of the internet.

�
/

r.'
,
:
,
�
J
101·200(701]

�

(20C>l)

3<51·0<50(�'1!!!i':J
�
�
�
�
. 201·3150
��
�NoM(�
�

01'-(20�
�

(2802)

ahw(1373]

051·100

�
other(865)

ott..r(567)

.

Low(181)

.

Figure 2: The decision tree for Shopping.Freq obtained by double-clicking that node in the dependency network.
The histograms at the leaves correspond to probabilities of Shopping.Freq use being zero, one, and greater than
one visit per month, respectively.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

272

r

I
I

I

I s�

irko

Figure 3: The dependency network in Figure 1 with the slider set at half position.
Figure 3 shows the dependency network for the Media
Metrix data with the slider at half position. At this
setting, we find the interesting observation that the
dependencies between Sex and XXX.Freq (frequency
of hits to pornographic pages) are the strongest among
all dependencies between demographics and internet
use.
6

Summary and Future Work

We have described a new representation for probabilis­
tic dependencies called a dependency network. We
have shown that a dependency network (consistent
or not) defines a joint distribution for its variables,
and that models in this class are easy to learn from
data. In particular, we have shown how a dependency
network can be thought of as a collection of regres­
sion/classification models among variables in a domain
that can be combined using Gibbs sampling to define
a joint distribution for the domain. In addition, we
have shown that this representation is useful for col­
laborative filtering and the visualization of predictive
relationships.
Of course, this research is far from complete. There
are many questions left to be answered. For example,

what are useful models (e.g., generalized linear models,
neural networks, support-vector machines, or embed­
ded regression/classification models) for a dependency
network's local distributions? Another example of par­
ticular theoretical interest is Hofmann's (2000) result
that small 12-norm perturbations in the local distribu­
tions lead to small 12-norm perturbations in the joint
distributions defined by the dependency network. Can
this result be extended to norms more appropriate for
probabilities such as cross entropy?
Finally, the dependency network and Bayesian net­
work can be viewed as two extremes of a spectrum.
The dependency network is ideal for situations where
the conditionals p(x;lx \ x;) are needed. In con­
trast, when we require the joint probabilities p(x),
the Bayesian network is ideal because these probabil­
ities may be obtained simply by multiplying condi­
tional probabilities found in the local distributions of
the variables. In situations where we need probabili­
ties of the form p(ylx\ y), where Y is a proper subset
of the domain X, we can build a network structure
that enforces an acyclicity constraint among only the
variables Y. In so doing, the conditional probabilities
p(ylx\ y) can be obtained by multiplication.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Acknowledgments

We thank Reimar Hofmann for useful discussions.
Datasets for this paper were generously provided by
Media Metrix, Nielsen Media Research (Nielsen), Mi­
crosoft Corporation (MS.COM), and Steven White
and Microsoft Corporation (MSNBC).



Recently several researchers have investigated techniques for using data to learn
Bayesian networks containing compact representations for the conditional probability
distributions (CPDs) stored at each node.
The majority of this work has concentrated
on using decision-tree representations for
the CPDs. In addition, researchers typically apply non-Bayesian (or asymptotically
Bayesian) scoring functions such as MDL to
evaluate the goodness-of-fit of networks to
the data.
In this paper we investigate a Bayesian approach to learning Bayesian networks that
contain the more general decision-graph representations of the CPDs. First, we describe
how to evaluate the posterior probability—
that is, the Bayesian score—of such a network, given a database of observed cases.
Second, we describe various search spaces
that can be used, in conjunction with a scoring function and a search procedure, to identify one or more high-scoring networks. Finally, we present an experimental evaluation
of the search spaces, using a greedy algorithm
and a Bayesian scoring function.

1

INTRODUCTION

Given a set of observations in some domain, a common problem that a data analyst faces is to build one
or more models of the process that generated the data.
In the last few years, researchers in the UAI community have contributed an enormous body of work to
this problem, using Bayesian networks as the model of
choice. Recent works include Cooper and Herskovits

(1992), Buntine (1991), Spiegelhalter et. al (1993),
and Heckerman et al. (1995).
A substantial amount of the early work on learning Bayesian networks has used observed data to infer global independence constraints that hold in the
domain of interest. Global independences are precisely those that follow from the missing edges within
a Bayesian-network structure. More recently, researchers (including Boutilier et al., 1995 and Friedman and Goldszmidt, 1996) have extended the “classical” definition of a Bayesian network to include efficient representations of local constraints that can hold
among the parameters stored in the nodes of the network. Two notable features about the this recent work
are (1) the majority of effort has concentrated on inferring decision trees, which are structures that can explicitly represent some parameter equality constraints
and (2) researchers typically apply non-Bayesian (or
asymptotically Bayesian) scoring functions such as
MDL as to evaluate the goodness-of-fit of networks
to the data.
In this paper, we apply a Bayesian approach to learning Bayesian networks that contain decision-graphs—
generalizations of decision trees that can encode arbitrary equality constraints—to represent the conditional probability distributions in the nodes.
In Section 2, we introduce notation and previous relevant work. In Section 3 we describe how to evaluate
the Bayesian score of a Bayesian network that contains
decision graphs. In Section 4, we investigate how a
search algorithm can be used, in conjunction with a
scoring function, to identify these networks from data.
In Section 5, we use data from various domains to
evaluate the learning accuracy of a greedy search algorithm applied the search spaces defined in Section 4.
Finally, in Section 6, we conclude with a discussion of
future extensions to this work.

2

BACKGROUND

In this section, we describe our notation and discuss
previous relevant work. Throughout the remainder of
this paper, we use lower-case letters to refer to variables, and upper-case letters to refer to sets of variables. We write xi = k when we observe that variable
xi is in state k. When we observe the state of every variable in a set X, we call the set of observations
a state of X. Although arguably an abuse of notation, we find it convenient to index the states of a
set of variables with a single integer. For example, if
X = {x1 , x2 } is a set containing two binary variables,
we may write X = 2 to denote {x1 = 1, x2 = 0}.
In Section 2.1, we define a Bayesian network. In Section 2.2 we describe decision trees and how they can be
used to represent the probabilities within a Bayesian
network. In Section 2.3, we describe decision graphs,
which are generalizations of decision trees.
2.1

BAYESIAN NETWORKS

Consider a domain U of n discrete variables x1 , . . . , xn ,
where each xi has a finite number of states. A Bayesian
network for U represents a joint probability distribution over U by encoding (1) assertions of conditional
independence and (2) a collection of probability distributions. Specifically, a Bayesian network B is the pair
(BS , Θ), where BS is the structure of the network, and
Θ is a set of parameters that encode local probability
distributions.
The structure BS has two components: the global
structure G and a set of local structures M . G is an
acyclic, directed graph—dag for short—that contains
a node for each variable xi ∈ U . The edges in G denote probabilistic dependences among the variables in
U . We use P ar(xi ) to denote the set of parent nodes
of xi in G. We use xi to refer to both the variable in
U and the corresponding node in G. The set of local
structures M = {M1 , . . . , Mn } is a set of n mappings,
one for each variable xi , such that Mi maps each value
of {xi , P ar(xi )} to a parameter in Θ.
The assertions of conditional independence implied by
the global structure G in a Bayesian network B impose
the following decomposition of the joint probability
distribution over U :
Y
p(xi |P ar(xi ), Θ, Mi , G)
(1)
p(U |B) =
i

The set of parameters Θ contains—for each node xi ,
for each state k of xi , and for each parent state j—
a single parameter1 Θ(i, j, k) that encodes the condiP
1
Because the sum

k

p(xi = k|P ar(xi ), Θ, Mi , G) must

x

y
z

Figure 1: Bayesian network for U = {x, y, z}
tional probabilities given in Equation 1. That is,
p(xi = k|P ar(xi ) = j, Θ, Mi , G) = Θ(i, j, k)

(2)

Note that the function Θ(i, j, k) depends on both Mi
and G. For notational simplicity we leave this dependency implicit.
Let ri denote the number of states of variable xi , and
let qi denote the number of states of the set P ar(xi ).
We use Θij to denote the set of parameters characterizing the distribution p(xi |P ar(xi ) = j, Θ, Mi , G):
i
Θij = ∪rk=1
Θ(i, j, k)

We use Θi to denote the set of parameters
characterizing all of the conditional distributions
p(xi |P ar(xi ), Θ, Mi , G):
i
Θij
Θi = ∪qj=1

In the “classical” implementation of a Bayesian network, each node xi stores (ri − 1) · qi distinct parameters in a large table. That is, Mi is simply a lookup
into a table. Note that the size of this table grows
exponentially with the number of parents qi .
2.2

DECISION TREES

There are often equality constraints that hold among
the parameters in Θi , and researchers have used mappings other than complete tables to more efficiently
represent these parameters. For example, consider the
global structure G depicted in Figure 1, and assume
that all nodes are binary. Furthermore, assume that if
x = 1, then the value of z does not depend on y. That
is,
p(z|x = 1, y = 0, Θ, Mz , G) = p(z|x = 1, y = 1, Θ, Mz , G)

Using the decision tree shown in Figure 2 to implement the mapping Mz , we can represent p(z|x =
1, y, Θ, MZ ) using a single distribution for both
p(z|x = 1, y = 0, Θ, Mz , G) and p(z|x = 1, y =
1, Θ, Mz , G).
be one, Θ will actually only contain ri − 1 distinct parameters for this distribution. For simplicity, we leave this
implicit for the remainder of the paper.

ditional distributions. Furthermore, many researchers
have developed methods for learning these local structures from data.

x
1

0

y
0
p(z|x=0, y=0)

1

p(z|x=1, y=0)
=
p(z|x=1, y=1)

p(z|x=0, y=1)

Figure 2: Decision tree for node z

Decision trees, described in detail by Breiman (1984),
can be used to represent sets of parameters in a
Bayesian network. Each tree is a dag containing exactly one root node, and every node other than the
root node has exactly one parent. Each leaf node contains a table of k − 1 distinct parameters that collectively define a conditional probability distribution
p(xi |P ar(xi ), Θ, Mi , D). Each non-leaf node in the
tree is annotated with the name of one of the parent
variables π ∈ P ar(xi ). Out-going edges from a node π
in the tree are annotated with mutually exclusive and
collectively exhaustive sets of values for the variable
π.
When a node v in a decision tree is annotated with
the name π, we say that v splits π. If the edge from v1
to child v2 is annotated with the value k, we say that
v2 is the child of v1 corresponding to k. Note that by
definition of the edge annotations, the child of a node
corresponding to any value is unique.
We traverse the decision tree to find the parameter
Θ(i, j, k) as follows. First, initialize v to be the root
node in the decision tree. Then, as long as v is not a
leaf, let π be the node in P ar(xi ) that v splits, and
reset v to be the child of v corresponding to the value
of π—determined by P ar(xi ) = j—and repeat. If v is
a leaf, we we return the parameter in the table corresponding to state k of xi .
Decision tree are more expressive mappings than complete tables, as we can represent all of the parameters
from a complete table using a complete decision tree. A
complete decision tree Ti for a node xi is a tree of depth
|P ar(xi )|, such that every node vl at level l in Ti splits
on the lth parent πl ∈ P ar(xi ) and has exactly rπl
children, one for each value of π. It follows by this definition that if Ti is a complete tree, then Θ(i, j, k) will
map to a distinct parameter for each distinct {i, j},
which is precisely the behavior of a complete table.
Researchers have found that decision trees are useful
for eliciting probability distributions, as experts often have extensive knowledge about equality of con-

2.3

DECISION GRAPHS

In this section we describe a generalization of the decision tree, known as a decision graph, that can represent a much richer set of equality constraints among
the local parameters. A decision graph is identical to a
decision tree except that, in a decision graph, the nonroot nodes can have more than one parent. Consider,
for example, the decision graph depicted in Figure 3.
This decision graph represents a conditional probability distribution p(z|x, y, Θ) for the node z in Figure
1 that has different equality constraints than the tree
shown in Figure 2. Specifically, the decision graph encodes the equality
p(z|x = 0, y = 1, Θ) = p(z|x = 1, y = 0, Θ)

x
1

0

y
0

p(z|x=0, y=0)

y
1

0

p(z|x=0, y=1)
=
p(z|x=1, y=0)

1
p(z|x=1, y=1)

Figure 3: Decision graph for node z
We use Di to denote a decision graph for node xi .
If the mapping in a node xi is implemented with Di ,
we use Di instead of Mi to denote the mapping. A
decision-graph Di can explicitly represent an arbitrary
set of equality constraints of the form
Θij = Θij 0

(3)

for j 6= j 0 . To demonstrate this, consider a complete
tree Ti for node xi . We can transform Ti into a decision
graph that represents all of the desired constraints by
simply merging together any leaf nodes that contain
sets that are equal.
It is interesting to note that any equality constraint of
the form given in Equation 3 can also be interpreted
as the following independence constraint:
xi ⊥⊥ P ar(xi ) | P ar(xi ) = j or P ar(xi ) = j 0
If we allow nodes in a decision graph Di to split on
node xi as well as the nodes in P ar(xi ), we can represent an arbitrary set of equality constraints among

the parameters Θi . We return to this issue in Section
6, and assume for now that nodes in Di do not split
on xi .

3

LEARNING DECISION GRAPHS

As we showed in Section 2, if the local structure for a
node xi is a decision graph Di , then sets of parameters Θij and Θij 0 can be identical for j 6= j 0 . For the
derivations to follow, we find it useful to enumerate the
distinct parameter sets in Θi . Equivalently, we find it
useful to enumerate the leaves in a decision graph.

Many researchers have derived the Bayesian measureof-fit—herein called the Bayesian score—for a network,
assuming that there are no equalities among the parameters. Friedman and Goldszmidt (1996) derive
the Bayesian score for a structure containing decision trees. In this section, we show how to evaluate
the Bayesian score for a structure containing decision
graphs.

For the remainder of this section, we adopt the following syntactic convention. When referring to a parameter set stored in the leaf of a decision graph, we
use a to denote the node index, and b to denote the
parent-state index. When referring to a parameter set
in the context of a specific parent state of a node, we
use i to denote the node index and j to denote the
parent-state index.

To derive the Bayesian score, we first need to make
an assumption about the process that generated the
database D. In particular, we assume that the
database D is a random (exchangeable) sample from
some unknown distribution ΘU , and that all of the
constraints in ΘU can be represented using a network
structure BS containing decision graphs.

To enumerate the set of leaves in a decision graph D a ,
we define a set of leaf-set indices La . The idea is that
La contains exactly one parent-state index for each leaf
in the graph. More precisely, let l denote the number
of leaves in Da . Then La = {b1 , . . . , bl } is defined as a
set with the following properties:

As we saw in the previous section, the structure
BS = {G, M } imposes a set of independence constraints that must hold in any distribution represented
using a Bayesian network with that structure. We define BSh to be the hypothesis that (1) the independence
constraints imposed by structure BS hold in the joint
distribution ΘU from which the database D was generated, and (2) ΘU contains no other independence
constraints. We refer the reader to Heckerman et al.
(1994) for a more detailed discussion of structure hypotheses.
The Bayesian score for a structure BS is the posterior
probability of BSh , given the observed database D:
p(BSh |D) = c · p(D|BSh )p(BSh )
1
. If we are only concerned with the relwhere c = p(D)
ative scores of various structures, as is almost always
the case, then the constant c can be ignored. Consequently, we extend our definition of the Bayesian score
to be any function proportional to p(D|BSh )p(BSh ).

For now, we assume that there is an efficient method
for assessing p(BSh ) (assuming this distribution is uniform, for example), and concentrate on how to derive
the marginal likelihood term p(D|BSh ). By integrating
over all of the unknown parameters Θ we have:
Z
h
p(Θ|BSh )p(D|Θ, BSh )
(4)
p(D|BS ) =
Θ

Researchers typically make a number of simplifying
assumptions that collectively allow Equation 4 to be
expressed in closed form. Before introducing these assumptions, we need the following notation.

1. For all {b, b0 } ⊆ La , b 6= b0 ⇒ Θa,b 6= Θa,b0
2. ∪b∈La Θa,b = Θa
The first property ensures that each index in L corresponds to a different leaf, and the second property
ensures that every leaf is included.
One assumption used to derive Equation 4 in closed
form is the parameter independence assumption. Simply stated, this assumption says that given the hypothesis BSh , knowledge about any distinct parameter set
Θab does not give us any information about any other
distinct parameter set.
Assumption 1 (Parameter Independence)
p(Θ|BSh ) =

n Y
Y

p(Θab |BSh )

a=1 b∈La

Another assumption that researchers make is the
Dirichlet assumption. This assumption restricts the
prior distributions over the distinct parameter sets to
be Dirichlet.
Assumption 2 (Dirichlet)
For all a and for all b ∈ La ,
p(Θab |BSh ) ∝

ra
Y

abc −1
Θα
abc

c=1

where αabc > 0 for 1 ≤ c ≤ ra
Recall that ra denotes the number of states for node
xa . The hyperparameters αabc characterize our prior

knowledge about the parameters in Θ. Heckerman et
al. (1995) describe how to derive these exponents from
a prior Bayesian network. We return to this issue later.
Using these assumptions, we can derive the Bayesian
score for a structure that contains decision graphs by
following a completely analogous method as Heckerman et al. (1995). Before showing the result, we must
define the inverse function of Θ(i, j, k). Let θ denote
an arbitrary parameter in Θ. The function Θ−1 (θ) denotes the set of index triples that Θ() maps into θ.
That is,
Θ−1 (θ) = {i, j, k|Θ(i, j, k) = θ}
Let Dijk denote the number of cases in D for which
xi = k and P ar(xi ) = j. We define Nabc as follows:
X
Dijk
Nabc =
ijk∈Θ−1 (θabc )

Intuitively, Nabc is the number of cases in D that provide information
about thePparameter θabc . Letting
P
Nab = c Nabc and αab = c αabc , we can write the
Bayesian score as follows:
p(D, BSh )

= p(BSh )

n Y
Y

a=1 b∈La

Γ(αab )
Γ(Nab + αab )

Y Γ(Nabc + αabc )
Γ(αabc )
c=1
|ra |

·

(5)

We can determine all of the counts Nabc for each node
xa as follows. First, initialize all the counts Nabc to
zero. Then, for each case C in the database, let kC
and jC denote the value for xi and P ar(xi ) in the
case, respectively, and increment by one the count
Nabc corresponding to the parameter θabc = p(xi =
kC |P ar(xi ) = jC , Θ, Da ). Each such parameter can
be found efficiently by traversing Da from the root.
We say a scoring function is node decomposable if it
can be factored into a product of functions that depend only a node and its parents. Node decomposability is useful for efficiently searching through the
space of global-network structures. Note that Equation 5 is node decomposable as long as p(BSh ) is node
decomposable.
We now consider some node-decomposable distributions for p(BSh ). Perhaps the simplest distribution is
to assume a uniform prior over network structures.
That is, we set p(BSh ) to a constant in Equation 5.
We use this simple prior for the experiments described
in Section 5. Another approach is to (a-priori) favor
networks with fewer parameters. For example, we can
use
n
Y
p(BSh ) ∝ κ|Θ| =
κ|Θa |
(6)
a=1

where 0 < κ <= 1. Note that κ = 1 corresponds to
the uniform prior over all structure hypotheses.
A simple prior for the parameters in Θ is to assume
αabc = 1 for all a, b, c. This choice of values corresponds to a uniform prior over the parameters, and
was explored by Cooper and Herskovits (1992) in the
context of Bayesian networks containing complete tables. We call the Bayesian scoring function the uniform scoring function if all the hyperparameters are
set to one. We have found that this prior works well
in practice and is easy to implement.
Using two additional assumptions, Heckerman et al.
(1995) show that each αabc can be derived from a prior
Bayesian network. The idea is that αabc is proportional to the prior probability, obtained from the prior
network, of all states of {xi = k, P ar(xi ) = j} that
map to the parameter θabc . Specifically, if B P is our
prior Bayesian network, we set
X
αabc = α
p(xi = k, P ar(xi ) = j|B P )
ijk∈Θ−1 (θabc )

where α is a single equivalent sample size used to asses
all of the exponents, and P ar(xi ) denotes the parents
of xi in G (as opposed to the parents in the prior network). α can be understood as a measure of confidence that we have for the parameters in B P . We call
the Bayesian scoring function the PN scoring function
(P rior N etwork scoring function) if the exponents are
assessed this way. Heckerman et al. (1995) derive
these constraints in the context of Bayesian networks
with complete tables. In the full version of this paper,
we show that these constraints follow when using decision graphs as well, with only slight modifications to
the additional assumptions.
Although we do not provide the details here, we can
use the decision-graph structure to efficiently compute
the exponents αabc from the prior network in much
the same way we computed the Nabc values from the
database.

4

SEARCH

Given a scoring function that evaluates the merit of
a Bayesian-network structure BS , learning Bayesian
networks from data reduces to a search for one or more
structures that have a high score. Chickering (1995)
shows that finding the optimal structure containing
complete tables for the mappings M is NP-hard when
using a Bayesian scoring function. Given this result,
it seems reasonable to assume that by allowing (the
more general) decision-graph mappings, the problem
remains hard, and consequently it is appropriate to
apply heuristic search techniques.

In Section 4.1, we define a search space over decisiongraph structures within a single node xi , assuming
that the parent set P ar(xi ) is fixed. Once such a space
is defined, we can apply to that space any number of
well-known search algorithms. For the experiments
described in Section 5, for example, we apply greedy
search.

0
v1

0

There are three operators we define, and each operator is a modification to the current set of leaves in a
decision graph.
Definition (Complete Split)
Let v be a leaf node in the decision graph, and let π ∈
P ar(xi ) be a parent of xi . A complete split C(v, π)
adds ri new leaf nodes as children to v, where each
child of v corresponds to a distinct value of π.
Definition (Binary Split)
Let v be a leaf node in the decision graph, and let π ∈
P ar(xi ) be a parent of xi . A binary split B(v, π, k)
adds 2 new leaf nodes as children to v, where the first
child corresponds to state k of π, and the other child
corresponds to all other states of π.
Definition (Merge)
Let v1 and v2 be two distinct leaf nodes in the decision
graph. A Merge M (v1 , v2 ) merges the v1 and v2 into
a single node. That is, the resulting node inherits all
parents from both v1 and v2 .
In Figure 4, we show the result of each type of operator
to a decision graph for a node z with parents x and y,
where x and y both have three states.
We add the pre-condition that the operator must
change the parameter constraints implied by the decision graph. We would not allow, for example, a
complete split C(v1 , y) in Figure 4a: two of v1 ’s new
children would correspond to impossible states of y
({y = 0 and y = 1} and {y = 0 and y = 2}), and
the third child would correspond to the original constraints at v1 ({y = 0 and y = 0}).
Note that starting from a decision graph containing a

y
1
0

DECISION-GRAPH SEARCH

In this section, we assume that the states of our search
space correspond to all of the possible decision graphs
for some node xi . In order for a search algorithm to
traverse this space, we must define a set of operators
that transform one state into another.

2

v2

v3

(a)

In Section 4.2 we describe a greedy algorithm that
combines local-structure search over all the decision
graphs in the nodes with a global-structure search over
the edges in G.
4.1

y
1

(b)

0

2
x
1

y
1

y

2
0

x
2

0

(c)

1,2

1,2

(d)

Figure 4: Example of the application of each type of
operator: (a) the original decision graph, (b) the result of applying C(v3 , x), (c) the result of applying
B(v3 , x, 0), and (d) the result of applying M (v2 , v3 )
single node (both the root and a leaf node), we can
generate a complete decision tree by repeatedly applying complete splits. As discussed in the previous
section, we can represent any parameter-set equalities
by merging the leaves of a complete decision tree. Consequently, starting from a graph containing one node
there exists a series of operators that result in any set
of possible parameter-set equalities. Note also that if
we repeatedly merge the leaves of a decision graph until there is a single parameter set, the resulting graph
is equivalent (in terms of parameter equalities) to the
graph containing a single node. Therefore, our operators are sufficient for moving from any set of parameter constraints to any other set of parameter constraints. Although we do not discuss them here, there
are methods that can simplify (in terms of the number
of nodes) some decision graphs such that they represent the same set of parameter constraints.
The complete-split operator is actually not needed to
ensure that all parameter equalities can be reached:
any complete split can be replaced by a series of binary
splits such that the resulting parameter-set constraints
are identical. We included the complete-split operator
in the hopes that it would help lead the search algorithm to better structures. In Section 5, we compare
greedy search performance in various search spaces defined by including only subsets of the above operators.
4.2

COMBINING GLOBAL AND LOCAL
SEARCH

In this section we describe a greedy algorithm that
combines global-structure search over the edges in G

with local-structure search over the decision graphs in
all of the nodes of G.
Suppose that in the decision-graph Di for node xi ,
there is no non-leaf node annotated with some parent
π ∈ P ar(xi ). In this case, xi is independent of π
given its other parents, and we can remove π from
P ar(xi ) without violating the decomposition given in
Equation 1. Thus given a fixed structure, we can learn
all the local decision graphs for all of the nodes, and
then delete those parents that are independent. We
can also consider adding edges as follows. For each
node xi , add to P ar(xi ) all non-descendants of xi in
G, learn a decision graph for xi , and then delete all
parents that are not contained in the decision graph.
Figure 5 shows a greedy algorithm that uses combines
these two ideas. In our experiments, we started the
algorithm with a structure for which G contains no
edges, and each graph Di consists of a single root node.
1.

Score the current network structure BS

2.

For each node xi in G

3.
4.
5.

Add every non-descendant that is not a parent
of xi to P ar(xi )
For every possible operator O to the decision
graph Di
Apply O to BS

6.

Score the resulting structure

7.

Unapply O

8.
9.
10.
11.

12.
13.
14.

Remove any parent that was added to xi in
step 3
If the best score from step 6 is better than the
current score
Let O be the operator that resulted in the best
score
If O is a split operator (either complete or binary) on a node xj that is not in P ar(xi ), then
add xj to P ar(xi )
Apply O to BS
Goto 1
Otherwise, return BS

Figure 5: Greedy algorithm that combines local and
global structure search
Note that as a result of a merge operator in a decision
graph D i , xi may be rendered independent from one
of its parents π ∈ P ar(xi ), even if D i contains a node
annotated with π. For a simple example, we could
repeatedly merge all leaves into a single leaf node,
and the resulting graph implies that xi does not depend on any of its parents. We found experimentally
that—when using the algorithm from Figure 5—this
phenomenon is rare. Because testing for these parent
deletions is expensive, we chose to not check for them

in the experiments described in Section 5.
Another greedy approach for learning structures containing decision trees has been explored by Friedman
and Goldszmidt (1996). The idea is to score edge operations in G (adding, deleting, or reversing edges) by
applying the operation and then greedily learning the
local decision trees for any nodes who’s parents have
changed as a result of the operation. In the full version
of the paper, we compare our approach to theirs.

5

EXPERIMENTAL RESULTS

In this section we investigate how varying the set of
allowed operators affects the performance of greedy
search. By disallowing the merge operator, the search
algorithms will identify decision-tree local structures in
the Bayesian network. Consequently, we can see how
learning accuracy changes, in the context of greedy
search, when we generalize the local structures from
decision trees to decision graphs.
In all of the experiments described in this section, we
measure learning accuracy by the posterior probability
of the identified structure hypotheses. Researchers often use other criteria, such as predictive accuracy on a
holdout set or structural difference from some generative model. The reason that we do not use any of these
criteria is that we are evaluating how well the search
algorithm performs in various search spaces, and the
goal of the search algorithm is to maximize the scoring
function. We are not evaluating how well the Bayesian
scoring functions approximate some other criteria.
In our first experiment, we consider the Promoter
Gene Sequences database from the UC Irvine collection, consisting of 106 cases. There are 58 variables
in this domain. 57 of these variables, {x1 , . . . , x57 }
represent the “base-pair” values in a DNA sequence,
and each has four possible values. The other variable,
promoter, is binary and indicates whether or not the
sequence has promoter activity. The goal of learning in
this domain is to build an accurate model of the distribution p(promoter|x1 , . . . , x57 ), and consequently it is
reasonable to consider a static graphical structure for
which P ar(promoter) = {x1 , . . . , x57 }, and search for
a decision graph in node promoter.
Table 1 shows the relative Bayesian scores for the best
decision graph learned, using a greedy search with various parameter priors and search spaces. All searches
started with a decision graph containing a single node,
and the current best operator was applied at each step
until no operator increased the score of the current
state. Each column corresponds to a different restriction of the search space described in Section 4.1: the
labels indicate what operators the greedy search was

Table 1: Greedy search performance for various
Bayesian scoring functions, using different sets of operators, in the P romoter domain.

uniform
U-PN 10
U-PN 20
U-PN 30
U-PN 40
U-PN 40

C
0
0
0
0
0
0

B
13.62
6.12
5.09
4.62
3.14
2.99

CB
6.07
4.21
3.34
2.97
1.27
1.12

CM
22.13
9.5
14.11
10.93
16.3
15.76

BM
26.11
10.82
12.11
12.98
13.54
15.54

CBM
26.11
12.93
14.12
16.65
16.02
17.54

allowed to use, where C denotes complete splits, B
denotes binary splits, and M denotes merges. The column labeled BM, for example, shows the results when
a greedy search used binary splits and merges, but
not complete splits. Each row corresponds to a different parameter-prior for the Bayesian scoring function.
The U-PN scoring function is a special case of the PN
scoring function for which the prior network imposes
a uniform distribution over all variables. The number following the U-PN in the row labels indicates the
equivalent-sample size α. All results use a uniform
prior over structure hypotheses. A value of zero in a
row of the table denotes the hypothesis with lowest
probability out of all those identified using the given
parameter prior. All other values denote the natural
logarithm of how many times more likely the identified
hypothesis is than the one with lowest probability.
By comparing the relative values between searches
that use merges and searches that don’t use merges,
we see that without exception, adding the merge operator results in a significantly more probable structure
hypothesis. We can therefore conclude that a greedy
search over decision graphs results in better solutions
than a greedy search over decision trees. An interesting observation is that complete-split operator actually
reduces solution quality when we restrict the search to
decision trees.
We performed an identical experiment to another classification problem, but for simplicity we only present
the results for the uniform scoring function. Recall
from Section 3 that the uniform scoring function has
all of the hyperparameters αabc set to one. This second
experiment was run with the Splice-junction Gene Sequences database, again from the UC Irvine repository.
This database also contains a DNA sequence, and the
problem is to predict whether the position in the middle of the sequence is an “intron-exon” boundary, an
“exon-intron” boundary, or neither. The results are
given in Table 2. We used the same uniform prior for
structure hypotheses.

Table 2: Greedy search performance for the uniform
scoring function, using different sets of operators, in
the Splice domain.
C
0

B
383

CB
363

CM
464

BM
655

CBM
687

Table 3: Greedy search performance for the uniform
scoring function for each node in the ALARM network.
Also included is the uniform score for the completetable model
COMP
C
B
CB CM BM CBM
0
134 186 165 257 270
270

Table 2 again supports the claim that we get a significant improvement by using decision graphs instead of
decision trees.
Our final set of experiments were done in the ALARM
domain, a well-known benchmark for Bayesiannetwork learning algorithms. The ALARM network,
described by Beinlich et al. (1989), is a handconstructed Bayesian network used for diagnosis in a
medical domain. The parameters of this network are
stored using complete tables.
In the first experiment for the ALARM domain, we
demonstrate that for a fixed global structure G, the
hypothesis identified by searching for local decision
graphs in all the nodes can be significantly better than
the hypothesis corresponding to complete tables in the
nodes. We first generated 1000 cases from the ALARM
network, and then computed the uniform Bayesian
score for the ALARM network, assuming that the parameter mappings M are complete tables. We expect
the posterior of this model to be quite good, because
we’re evaluating the generative model structure. Next,
using the uniform scoring function, we applied the six
greedy searches as in the previous experiments to identify good decision graphs for all of the nodes in the
network. We kept the global structure G fixed to be
identical to the global structure of the ALARM network. The results are shown in Table 3, and the values
have the same semantics as in the previous two tables.
The score given in the first column labeled COMP is
the score for the complete-table model.
Table 3 demonstrates that search performance using
decision graphs can identify significantly better models than when just using decision trees. The fact that
the complete-table model attains such a low score (the
best hypothesis we found is e270 times more probable
than the complete-table hypothesis!) is not surprising upon examination of the probability tables stored

Table 4: Performance of greedy algorithm that combines local and global structure search, using different
sets of operators, in the ALARM domain. Also included is the result of a greedy algorithm that searches
for global structure assuming complete tables.
COMP
255

C
0

B
256

CB
241

CM
869

BM
977

CBM
1136

in the ALARM network: most of the tables contain
parameter-set equalities.
In the next experiment, we used the ALARM domain
to test the structure-learning algorithm given in Section 4.2. We again generated a database of 1000 cases,
and used the uniform scoring function with a uniform
prior over structure hypotheses. We ran six versions
of our algorithm, corresponding to the six possible sets
of local-structure operators as in the previous experiments. We also ran a greedy structure-search algorithm that assumes complete tables in the nodes. We
initialized this search with a global network structure
with no edges, and the operators were single-edge modifications to the graph: deletion, addition and reversal.
In Table 4 we show the results. The column labeled
COMP corresponds to the greedy search over structures with complete tables.
Once again, we note that when we allow nodes to
contain decision graphs, we get a significant improvement in solution quality. Note that the search
over complete-table structures out-performed our algorithm when we restricted the algorithm to search
for decision trees containing either (1) only complete
splits or (2) complete splits and binary splits.
In our final experiment, we repeated the previous experiment, except that we only allowed our algorithm
to add parents that are not descendants in the generative model. That is, we restricted the global search
over G to those dags that did not violate the partial ordering in the ALARM network. We also ran the same
greedy structure-search algorithm that searches over
structures with complete tables, except we initialized
the search with the ALARM network. The results of
this experiment are shown in Table 5. From the table,
we see that the constrained searches exhibit the same
relative behavior as the unconstrained searches.
For each experiment in the ALARM domain (Tables
3, 4, and 5) the values presented measure the performance of search relative to the worst performance in
that experiment. In Table 6, we compare search performance across all experiments in the ALARM domain. That is, a value of zero in the table corresponds
to the experiment and set of operators that led to the

Table 5: Performance of a restricted version of our
greedy algorithm, using different sets of operators, in
the ALARM domain. Also included is the result of
a greedy algorithm, initialized with the global structure of the ALARM network, that searches for global
structure assuming complete tables.
COMP
0

C
179

B
334

CB
307

CM
553

BM
728

CBM
790

Table 6: Comparison of Bayesian scores for all experiments in the ALARM domain

S
U
C

COMP
278
255
336

C
412
0
515

B
464
256
670

CB
443
241
643

CM
534
869
889

BM
548
976
1064

CBM
548
1136
1126

learned hypothesis with lowest posterior probability,
out of all experiments and operator restrictions we
considered in the ALARM domain. All other values
given in the table are relative to this (lowest) posterior
probability. The row labels correspond to the experiment: S denotes the first experiment that performed
local searches in a static global structure, U denotes
the second experiment that performed unconstrained
structural searches, and C denotes the final experiment
that performed constrained structural search.
Rather surprising, each hypothesis learned using
global-structure search with decision graphs had a
higher posterior than every hypothesis learned using
the generative static structures.

6

DISCUSSION

In this paper we showed how to derive the Bayesian
score of a network structure that contains parameter
maps implemented as decision graphs. We defined a
search space for learning individual decision graphs
within a static global structure, and defined a greedy
algorithm that searches for both global and local structure simultaneously. We demonstrated experimentally
that greedy search over structures containing decision
graphs significantly outperforms greedy search over
both (1) structures containing complete tables and (2)
structures containing decision trees.
We now consider an extension to the decision graph
that we mentioned in Section 2.3. Recall that in a decision graph, the parameter sets are stored in a table
within the leaves. When decision graphs are implemented this way, any parameter θabc must belong to
exactly one (distinct) parameter set. An important

consequence of this property is that if the priors for
the parameter sets are Dirichlet (Assumption 2), then
the posterior distributions are Dirichlet as well. That
is, the Dirichlet distribution is conjugate with respect
to the likelihood of the observed data. As a result, it is
easy to derive the Bayesian scoring function in closed
form.
If we allow nodes within a decision graph Di to split
on node xi , we can represent an arbitrary set of parameter constraints of the form Θ(i, j, k) = Θ(i, j 0 , k 0 )
for j 6= j 0 and k 6= k 0 . For example, consider a Baysian
network for the two-variable domain {x, y}, where x is
a parent of y. We can use a decision graph for y that
splits on y to represent the constraint
p(y = 1|x = 0, Θ, Dy , G) = p(y = 0|x = 1, Θ, Dy , G)
Unfortunately, when we allow these types of constraints, the Dirichlet distribution is no longer conjugate with respect to the likelihood of the data, and the
parameter independence assumption is violated. Consequently, the derivation described in Section 3 will
not apply. Conjugate priors for a decision graph Di
that splits on node xi do exist, however, and in the
full version of this paper we use a weaker version of
parameter independence to derive the Bayesian score
for these graphs in closed form.
We conclude by noting that it is easy to extend the definition of a network structure to represent constraints
between the parameters of different nodes in the network, e.g. Θij = Θi0 j 0 for i 6= i0 . Both Buntine
(1994) and Thiesson (1995) consider these types of
constraints. The Bayesian score for such structures
can be derived by simple modifications to the approach
described in this paper.



We describe scoring metrics for learning
Bayesian networks from a combination of
user knowledge and statistical data. We
identify two important properties of metrics,
which we call event equivalence and parameter modularity. These properties have been
mostly ignored, but when combined, greatly
simplify the encoding of a user’s prior knowledge. In particular, a user can express his
knowledge—for the most part—as a single
prior Bayesian network for the domain.

1

Introduction

The fields of Artificial Intelligence and Statistics share
a common goal of modeling real-world phenomena.
Whereas AI researchers have emphasized a knowledgebased approach to achieving this goal, statisticians
have traditionally emphasized a data-based approach.
In this paper, we present a unification of the two approaches. In particular, we develop algorithms based
on Bayesian principles that take as input (1) a user’s
prior knowledge expressed—for the most part—as a
prior Bayesian network and (2) statistical data, and
returns one or more improved Bayesian networks.
Several researchers have examined methods for learning Bayesian networks from data, including Cooper
and Herskovits (1991,1992), Buntine (1991), and
Spiegelhalter et al. (1993) (herein referred to as CH,
Buntine, and SDLC, respectively). These methods all
have the same basic components: a scoring metric and
a search procedure. The metric computes a score that
is proportional to the posterior probability of a network structure, given data and a user’s prior knowledge. The search procedure generates networks for
∗
Author’s primary affiliation: Computer Science Department, Technion, Haifa 32000, Israel.

evaluation by the scoring metric. These methods use
the two components to identify a network or set of
networks with high posterior probabilities, and these
networks are then used to predict future events.
In this paper, we concentrate on scoring metrics. Although we restrict ourselves to domains containing
only discrete variables, as we show in Geiger and Heckerman (1994), our metrics can be extended to domains
containing continuous variables. A major contribution
of this paper is that we develop our metrics from a
set of consistent properties and assumptions. Two of
these, called parameter modularity and event equivalence, have been ignored for the most part, and their
combined ramifications have not been explored. The
assumption of parameter modularity, which has been
made implicitly by CH, Buntine, and SDLC, addresses
the relationship among prior distributions of parameters for different Bayesian-network structures. The
property of event equivalence says that two Bayesiannetwork structures that represent the same set of independence assertions should correspond to the same
event and therefore receive the same score. We provide justifications for these assumptions, and show
that when combined with assumptions about learning Bayesian networks made previously, we obtain a
straightforward method for combining user knowledge
and statistical data that makes use of a prior network.
Our approach is to be contrasted with those of CH
and Buntine who do not make use of a prior network,
and to those of CH and SDLC who do not satisfy the
property of event equivalence.
Our identification of the principle of event equivalence
arises from a subtle distinction between two types of
Bayesian networks. The first type, called belief networks, represents only assertions of conditional independence. The second type, called causal networks,
represents assertions of cause and effect as well as assertions of independence. In this paper, we argue that
metrics for belief networks should satisfy event equivalence, whereas metrics for causal networks need not.

Our score-equivalent metric for belief networks is similar to metrics described by York (1992), Dawid and
Lauritzen (1993) and Madigan and Raferty (1994), except that our metric scores directed networks, whereas
their metrics score undirected networks. In this paper, we concentrate on directed models rather than on
undirected models, because we believe that users find
the former easier to build and interpret.

2

Belief Networks and Notation

A belief network—the first of the two types of Bayesian
networks that we consider—represents a joint probability distribution over U by encoding assertions of
conditional independence as well as a collection of
probability distributions. From the chain rule of probability, we know
n
Y

p(xi |x1 , . . . , xi−1 , ξ)

(1)

i=1

For each variable xi , let Πi ⊆ {x1 , . . . , xi−1 } be a set
of variables that renders xi and {x1 , . . . , xi−1 } conditionally independent. That is,
p(xi |x1 , . . . , xi−1 , ξ) = p(xi |Πi , ξ)

p(x1 , . . . , xn |ξ) =

(2)

A belief network is a pair (BS , BP ), where BS is a
belief-network structure that encodes the assertions of
conditional independence in Equation 2, and BP is a
set of probability distributions corresponding to that
structure. In particular, BS is a directed acyclic graph
such that (1) each variable in U corresponds to a node
in BS , and (2) the parents of the node corresponding
to xi are the nodes corresponding to the variables in
Πi . (In the remainder of this paper, we use xi to refer
to both the variable and its corresponding node in a
graph.) Associated with node xi in BS are the probability distributions p(xi |Πi , ξ). BP is the union of
these distributions. Combining Equations 1 and 2, we
see that any belief network for U uniquely determines

n
Y

p(xi |Πi , ξ)

(3)

i=1

A minimal belief network is a belief network where
Equation 2 is violated if any arc is removed. Thus,
a minimal belief network represents both assertions of
independence and assertions of dependence.

3

Consider a domain U of n discrete variables x1 , . . . , xn .
We use lower-case letters to refer to variables and
upper-case letters to refer to sets of variables. We
write xi = k when we observe that variable xi is in
state k. We use p(x = i|y = j, ξ) to denote the probability of a person with background knowledge ξ for the
observation x = i, given the observation y = j. When
we observe the state for every variable in set X, we
call this set of observations an instance of X. We use
p(X|Y, ξ) to denote the set of probabilities for all possible observations of X, given all possible observations
of Y . The joint space of U is the set of all instances
of U . The joint probability distribution over U is the
probability distribution over the joint space of U .

p(x1 , . . . , xn |ξ) =

a joint probability distribution for U . That is,

Metrics for Belief Networks:
Previous Work

In this section, we summarize previous work,
presented—for example—in CH, Buntine, and SDLC
on the computation of a score for a belief-network
structure BS , given a set of cases D = {C1 , . . . , Cm }.
Each case Ci is the observation of one or more variables in U . We sometimes refer to D as a database.
A Bayesian measure of the goodness of a belief-network
structure is its posterior probability given a database:
p(BS |D, ξ) = c p(BS |ξ) p(D|BS , ξ)
P
where c = 1/p(D|ξ) = 1/ BS p(BS |ξ) p(D|BS , ξ) is a
normalization constant. For even small domains, however, there are too many network structures to sum
over in order to determine the constant. Therefore researchers have used p(BS |ξ) p(D|BS , ξ) = p(D, BS |ξ)
as a network-structure score. We note that this metric
treats all variables as being equally important, but can
be generalized [Spiegelhalter et al., 1993].
To compute p(D, BS |ξ) in closed form, researchers typically have made five assumptions, which we explicate
here.
Assumption 1 The database D is a multinomial
sample from some belief network (BS , BP ).
There are several assumptions implicit in Assumption 1. One is that all variables in U are discrete. We
modify this assumption in another paper in this proceedings [Geiger and Heckerman, 1994]. Another assumption is that the user may be uncertain as to which
belief-network structure is generating the data. This
uncertainty is encoded in the prior probabilities for
network structure p(BS |ξ). Also implicit is that, given
the data comes from a particular network structure,
the user may be uncertain about the probabilities for
that structure. These probabilities actually should be
thought of as being long-run fractions that we would
see in a very large database, and are called parameters
in the statistical literature. Finally, we note that Assumption 1 implies that the processes generating the
data do not change in time.

θ y|x

θx

all belief-network structures BS ,
YY
ρ(Θij |BS , ξ)
ρ(ΘBS |BS , ξ) =

θ y|x

i

x

y

x

y

j

case 1

case 2

M
Figure 1: Illustration of Assumptions 1 and 2 for the
network structure x → y, where x and y are binary.

Assumption 1 can be represented in a belief network.
Figure 1 illustrates the assumption for the network
structure x → y where x and y are binary variables.
(We shall use this two-variable domain to illustrate
many of the points in this paper.) The parameter θx
represents the long-run fraction of cases where x is
observed to be true. Given θx , the observations of x
in each case are independent. The parameters θy|x
and θy|x̄ represent the long-run fraction of cases where
y is observed to be true, in those cases where x is
observed to be true and false, respectively. If these
two parameters are known, then the observations of y
in any two cases are independent, provided x is also
observed for at least one of those cases.
In general, given a belief-network structure BS for
U = {x1 , . . . , xn }, we use ri to Q
denote the number
of states of variable xi , and qi = xl ∈Πi rl to denote
the number of instances of Πi . We use the integer j
to index these instances. That is, we write Πi = j to
denote the observation of the jth instance of the parents of xi . We use θijk to denote the long-run fraction
of cases where xi = k, in those cases where Πi = j.
We use Θij to denote the union of θijk over k, and
ΘBS to denote the union of Θij for all instances j of
all variables xi . Thus, the set ΘBS corresponds to the
parameter set BP for belief-network structure BS , as
defined in Section 2. Here, however, these parameters are long-run fractions whose values are uncertain.
Also, we use ρ(·|ξ) to denote the probability density
for a continuous variable or set of variables. For example, ρ(Θij |BS , ξ) denotes the probability density for
the set of parameters Θij , given BS and ξ.
The next assumption, which we call parameter independence, says that the parameters associated with a
given belief-network structure are independent, except
for the obvious dependence among the parameters for
a given variable (which must sum to one).

This assumption is illustrated in Figure 1 for the network structure x → y.
If all variables in a case are observed, we say that the
case is complete. If all cases in a database are complete, we say that the database is complete.
Assumption 3 All databases are complete.
We note that Spiegelhalter et al. (1993) provide an excellent survey of approximations that circumvent this
assumption.
A general metric now follows. Applying the chain rule,
we obtain
p(D|BS , ξ) =

p(Cl |C1 , . . . , Cl−1 , BS , ξ)

(4)

l=1

where Ci is the ith case in the database. Given Assumption 3, it follows that parameters remain independent when cases are observed. This conclusion is easily seen in the simple example of Figure 1.1 Therefore,
conditioning on the parameters of the belief-network
structure BS , we have
p(Cl |C1 , . . . , Cl−1 , BS , ξ) =

Z

p(Cl |ΘBS , BS , ξ)
ΘB

(5)

YY
i

S

ρ(Θij |C1 , . . . , Cl−1 , BS , ξ)

j

Also, because each case in D is complete, we have
YYY α
lijk
p(Cl |ΘBS , BS , ξ) =
θijk
(6)
i

j

k

where αlijk is 1 if and only if xi = k and Πi = j in
case Cl , and 0 otherwise. Plugging Equation 6 into
Equation 5 and the result into Equation 4 yields
p(D, BS |ξ) = p(BS |ξ)
(7)
YYYY
αlijk
·
< θijk |C1 , . . . , Cl−1 , BS , ξ >
i

j

k

l

where < θijk |ξ > denotes the expectation of θijk with
respect to ρ(Θij |ξ).
One difficulty in applying Equation 7 is that, a user
must provide prior distributions for every parameter
set Θij associated with every structure BS . To reduce
the number of prior distributions, we make the following assumption.
1

Assumption 2 (Parameter Independence) For

m
Y

In general, if a variable is observed in a belief network,
we may delete all arcs emanating from it and retain a valid
belief network.

Assumption 4 (Parameter Modularity)
If xi has the same parents in any two belief-network
structures BS1 and BS2 , then for j = 1, . . . , qi ,

a Dirichlet distribution.2 When every such parameter set of BS has this distribution, we simply say that
ρ(ΘBS |BS , ξ) is Dirichlet.

ρ(Θij |BS1 , ξ) = ρ(Θij |BS2 , ξ)

Combining our previous assumptions with this consequence of Assumption 5, we obtain
Y N 0 +Nijk −1
ρ(Θij |D, BS , ξ) ∝
θijkijk

We call this property parameter modularity, because it
says that the densities for parameters Θij depend only
on the structure of the belief network that is local to
variable xi —namely, Θij only depends on the parents
of xi . For example, in our two-variable domain, let
BS1 be the network with an arc pointing from x to y,
and BS2 be the network with no arc between x and
y. Then ρ(θx |BS1 , ξ) = ρ(θx |BS2 , ξ) because x has the
same parents (namely, none) in both belief networks.
We note that CH, Buntine, and SDLC implicitly make
the assumption of parameter modularity (Cooper and
Herskovits, 1992, Equation A6, p. 340; Buntine, 1991,
p. 55; Spiegelhalter et al., 1993, pp. 243-244). Also, in
the context of causal networks, the assumption has a
compelling justification (see Section 7). To our knowledge, however, we are the first researchers to make this
assumption explicit. As we see in the following section,
this assumption has important ramifications.

k

where Nijk is the number of cases in D where xi = k
and Πi = j. Thus, if the prior distribution for Θij
has a Dirichlet distribution, then so does the posterior distribution for Θij . We say that the Dirichlet
distribution is closed under multinomial sampling, or
that the Dirichlet distribution is a conjugate family
of distributions for multinomial sampling. Given this
family,
0
Nijk
+ Nijk
(9)
< θijk |D, ξ >=
0
Nij + Nij
Pri
P ri
0
where Nij = k=1
Nijk , and Nij0 = k=1
Nijk
. Substituting Equation 9 into each term of Equation 8, and
performing the sum over l, we obtain
p(D, BSe |ξ)

= p(BSe |ξ) ·

qi
n Y
Y

i=1 j=1

Given Assumption 3, parameter modularity holds even
after cases have been observed. Consequently, we can
drop the conditioning event BS in Equation 7, to yield

Γ(Nij0 )
Γ(Nij0 + Nij )

ri
0
Y
Γ(Nijk
+ Nijk )
·
0 )
Γ(Nijk

(10)

k=1

p(D, BS |ξ) = p(BS |ξ)
(8)
YYYY
αlijk
·
< θijk |C1 , . . . , Cl−1 , ξ >

where Γ is the Gamma function, which satisfies Γ(x +
1) = xΓ(x). We shall refer to Equation 10 as the
BD metric (Bayesian metric with Dirichlet priors), although we emphasize that this metric is not new.

In Heckerman et al. (1994), we provide greater detail about this general metric. Here, we concentrate
on a special case where each parameter set Θij has a
Dirichlet distribution.

Even with the inclusion of the assumption of parameter modularity, the application of this metric is difficult, because it requires that a user specify the Dirich0
let exponents Nijk
for every complete belief network
structure. In the following section, we introduce a
property of belief-network metrics called event equivalence. In the subsequent section, we show how this
property leads to a dramatic simplification of the assessment of these Dirichlet exponents.

i

j

k

l

An important concept to be used in much of the remaining presentation is that of a complete belief network. A complete belief-network is one with no missing edges—that is, one that represents no assertions of
conditional independence.
Assumption 5 For every complete belief-network
structure BSC , and for all Θij ⊆ ΘBSC , ρ(Θij |BSC , ξ)
has a Dirichlet distribution. Namely, there exists ex0
ponents Nijk
> 0, such that
ρ(Θij |BSC , ξ) ∝

Y

N0

θijkijk

−1

k

From this assumption and our assumption of parameter modularity, it follows that for every belief-network
structure BS , and for all Θij ⊆ ΘBS , ρ(Θij |BS , ξ) has

4

Event Equivalence and Score
Equivalence

In the previous section, we used BS as an argument of
probabilities and probability densities. However, BS
is a belief-network structure, not an event. Thus, we
should have used BSe in these situations, where BSe is
the event that corresponds to structure BS (the superscript “e” stands for event). In this section, we provide
2
CH, Buntine, and SDLC express Assumption 5 in this
form.

a definition of BSe and explicate an important property
of this definition.
A simple definition of BSe is implicit in Assumption 1. In particular, this assumption says that (1)
the database is a multinomial sample from the joint
space of U , and (2) BSe holds true iff the multinomial parameters for U satisfy the independence assertions of BS . For example, in our two-variable domain, Condition 1 corresponds to the assertion that
a given database is a multinomial sample from the
joint space {xy, xȳ, x̄y, x̄ȳ}. Given BS is the network
structure with no arc between x and y, Condition 2
says that the event BSe corresponds to the assertion
θxy + θx̄y = θxy /(θxy + θxȳ )—that is, θy = θy|x .
This definition has the following desirable property.
When two belief-network structures represent the
same assertions of conditional independence, we say
that they are isomorphic. For example, in the three
variable domain {x, y, z}, the network structures x →
y → z and x ← y → z represent the same assertion: x
and z are independent given y. Given the definition of
e
e
BSe , it follows that events BS1
and BS2
are equivalent
if and only if the structures BS1 and BS2 are isomorphic. That is, the relation of isomorphism induces an
equivalence class on the set of events BSe . We call this
property event equivalence.
There is a problem with the definition, however. In
particular, events corresponding to non-isomorphic
network structures are not mutually exclusive. For
example, the event corresponding to a complete beliefnetwork structure always holds true, and therefore implies the event corresponding to any other structure.
In this case, and in general, the scores p(D, BSe ) associated with these network structures are useless for
comparison.
A seemingly reasonable repair would be to say that
BSe holds true iff the multinomial parameters for U
satisfy the independence and dependence assertions of
BS , where BS is now interpreted as a minimal beliefnetwork structure. Under this revised definition, each
event is a set of equalities (as before), and also a set
of inequalities. For example, given BS1 is the beliefnetwork structure x → y in our two-variable domain,
e
is the event θy|x 6= θy ; and given BS2 is
then BS1
e
is
the belief-network structure with no arc, then BS2
the event θy|x = θy . These two events are mutually
exclusive. Furthermore, the events corresponding to
x → y and y → x are equal.
This repair is still not sufficient for larger domains,
however. First, the property of score equivalence may
be violated. For example, in the three-variable domain {x, y, z}, the events corresponding to complete
belief-network structures for different orderings are not

equal. We may recover this property by including in
the event corresponding to a set of isomorphic network
structures E the union of inequalities associated with
each such structure in E. Second, events corresponding to some non-isomorphic structures are not mutually exclusive. For example, the events corresponding
to the structures x → y ← z and x → y → z both
include the situation where θz|x = θz . In general, however, such overlaps will be of measure zero with respect
to the events that create the overlap. Thus, given a set
of overlapping events, we may exclude the intersection
from all but one of the events without affecting our
mathematical results or the intuitive understanding of
events by the user.
This revised definition of the event BSe guarantees that
the set of events corresponding to the set of all possible network structures for a given domain is mutually exclusive. Furthermore, the definition retains the
property of event equivalence.
Proposition 1 (Event Equivalence)
Belief-network structures BS1 and BS2 are isomorphic
e
e
if and only if BS1
= BS2
.
Because the score for network structure BS is
p(D, BSe |ξ), an immediate consequence of the property
of event equivalence is score equivalence3 :
Proposition 2 (Score Equivalence) The scores of
two isomorphic belief-network structures must be equal.
We note that, given the property of event equivalence, we technically should score each belief-networkstructure equivalence class, rather than each beliefnetwork structure. Nonetheless, users find it intuitive
to work with (i.e., construct and interpret) belief networks. Consequently, we continue our presentation
in terms of belief networks, keeping Proposition 2 in
mind.
It is easy to show that the BD metric given by Equation 10 does not exhibit the property of score equiva0
lence for most choices of the Dirichlet exponents Nijk
.
Thus, the property of event equivalence must induce
0
.
constraints on the parameters Nijk

3
In making the assumptions of parameter independence
and parameter modularity, we have—in effect—specified
the prior densities for the multinomial parameters in terms
of the structure of a belief network. Consequently, there is
the possibility that this specification violates the property
of score equivalence. In Heckerman et al. (1994), however,
we show that our assumptions and score equivalence are
consistent.

5

The Prior Belief Network

ρ(θ xy ,θ x y ,θ x y | Bxh→ y , ξ ) = ρ(θ xy ,θ x y ,θ x y | Bxh ←y , ξ )

In this section, we show how the property of event
equivalence and Assumptions 1 through 5 lead to con0
straints on the exponents Nijk
. We see that the constraints are so strong, that all exponents may be constructed from (1) a belief network reflecting the user’s
current knowledge about the next case, and (2) and
equivalent sample size for the domain as a whole.
To begin, let us see how the property of event equivalence and Assumptions 1 through 4 constrain the prior
densities ρ(ΘBS |BSe , ξ). That is, for the moment, let
us ignore the assumption that densities are Dirichlet.
First, consider only complete belief-network structures. From the property of event equivalence, we
know that the event associated with any complete
belief-network structure for a given domain U is the
same; and we use BSe C to denote this event. So, suppose that we know the density of the multinomial parameters for the joint space of U conditioned on BSe C .
Then, we may determine the density of the parameters
for any complete network structure, simply by performing a change-of-variable operation. For example,
consider the complete belief-network structure x → y
for our two-variable domain. A parameter set for the
joint space is {θxy , θx̄y , θxȳ }; and a parameter set for
the network structure is {θx , θy|x , θy|x̄ }. These sets are
related by the following relations:
θxy = θx θy|x θx̄y = (1 − θx )(θy|x̄ ) θxȳ = θx (1 − θy|x )
Thus, given the density ρ(θxy , θx̄y , θxȳ |BSe C , ξ) for
the joint space, we may compute the density
ρ(θx , θy|x , θy|x̄ |BSe C , ξ) using the relation
ρ(θx , θy|x , θy|x̄ |BSe C , ξ) = J · ρ(θxy , θx̄y , θxȳ |BSe C , ξ)
where J is the Jacobian of the transformation
J

=
=

∂θxy /∂θx
∂θxy /∂θy|x
∂θxy /∂θy|x̄
θx (1 − θx )

∂θx̄y /∂θx
∂θx̄y /∂θy|x
∂θx̄y /∂θy|x̄

∂θxȳ /∂θx
∂θxȳ /∂θy|x
∂θxȳ /∂θy|x̄

change of
variable

Bx→ y : x

Bx←y : x

y

y

parameter
modularity

Bxy :

x

y

Figure 2: A method for obtaining the density for the
parameters of BS from the density of the joint space
of the domain.
dence, we may obtain the densities for θx and θy separately. To obtain the density for θx , we identify a
complete network structure BSC1 such that x has the
same parents (namely, none) in both BS and BSC1 .
Next, using the change-of-variable procedure described
in the previous paragraph, we determine the density
e
ρ(θx |BSC1
, ξ) from ρ(θxy , θx̄y , θxȳ |BSe C , ξ). Then, we
use the assumption of parameter modularity to obe
tain ρ(θx |BSe , ξ) = ρ(θx |BSC1
, ξ). In a similar manner, as illustrated in the figure, we obtain the density
ρ(θy |BSe , ξ).
Next, let us consider Assumption 5. By a similar argument to that given in the first part of this discussion, we know that given any two complete network
structures BSC1 and BSC2 , we may obtain the density ρ(ΘBSC1 |BSe C , ξ) from ρ(ΘBSC2 |BSe C , ξ), and vice
versa, through a change of variable. If we assume that
the densities for BSC1 are Dirichlet, however, it may
not be the case that the densities for BSC2 will be
Dirichlet. For example, in our two-variable domain,
suppose ρ(θx , θy|x , θy|x̄ |BSe C , ξ) is equal to a constant
(all Dirichlet exponents equal to 1). After a change of
variable, from Equation 11 we have
θ (1−θ )

(11)

ρ(θy , θx|y , θx|ȳ |BSe C , ξ) ∝ · θxy (1−θyx )
=

Given the assumption of parameter modularity,
this result extends to any belief-network structure.
Namely, in Heckerman et al. (1994, Theorem 2),
we show that, given the density of the multinomial
parameters for the joint space of U conditioned on
BSe C , we may determine the density of the parameters for any network structure. To understand this
result, consider the incomplete network structure containing no arc between x and y for our two variable domain. The method for determining the density for the parameters of BS is illustrated in Figure 2. Given the assumption of parameter indepen-

θy (1−θy )
(θy θx|y +(1−θy )θx|ȳ )(1−(θy θx|y +(1−θy )θx|ȳ ))

which is not Dirichlet. Consequently, the Dirichlet exponents must be constrained.
In Heckerman et al. (1994, Theorem 7), we show
that if ρ(ΘBSC |BSe C , ξ) is Dirichlet for every complete
belief-network structure BSC , then the density of the
parameters for the joint space (also conditioned on
BSe C ) must also have a Dirichlet distribution. Combining this result with our previous discussion, we see
0
that we may obtain all exponents Nijk
for all beliefnetwork structures, simply by assessing the Dirichlet
density for the joint space of U conditioned on BSe C .

For domains containing a small number of variables,
the user may assess this density directly. In larger
domains, however, we can use an assessment method
based on the notion of an equivalent sample size,
described by Winkler (1967). To understand this
method, let Θx1 ,...,xn denote the set of parameters
for the joint space of U = {x1 , . . . , xn }. Denote the
Dirichlet density for these parameters as follows:
Y
0
[θx1 ,...,xn ](Nx1 ,...,xn −1)
ρ(Θx1 ,...,xn |BSe C , ξ) ∝
x1 ,...,xn

(12)
Now, the expectation of Θx1 ,...,xn with respect to
ρ(Θx1 ,...,xn |BSe C , ξ) is equal to the user’s prior probability p(x1 , . . . , xn |BSe C , ξ) for the next instance of the
domain to be observed. Thus, using the formula for
the expectation of Θx1 ,...,xn given the Dirichlet density
in Equation 12, we obtain
p(x1 , . . . , xn |BSe C , ξ) =
where
N0 =

X

Nx0 1 ,...,xn
N0

Nx0 1 ,...,xn

(13)

(14)

x1 ,...,xn

Thus, we can determine all needed exponents by having a user assess p(x1 , . . . , xn |BSe C , ξ) and N 0 .

parameters for xi , given that we have observed the jth
instance of Πi . From Equation 15, we see that
Nij0 = N 0 · p(Πi = j|BSe , ξ)
That is, the equivalent sample size for Θij is just the
overall equivalent sample size N 0 times the probability
that we see Πi = j.
Substituting Equation 15 into the BD metric (Equation 10), we obtain the BDe metric, a score equivalent
metric for belief networks. We note that N 0 acts as a
gain control for learning—the smaller the value of N 0 ,
the more quickly the BDe metric will favor network
structures that differ from the prior belief-network
structure.
As an example, let Bx→y and By→x denote the beliefnetwork structures where x points to y and y points
to x, respectively, in our two-variable domain. Suppose that N 0 = 12 and that the user’s prior nete
work gives the joint distribution p(x, y|Bx→y
, ξ) =
e
e
1/4, p(x, ȳ|Bx→y , ξ) = 1/4, p(x̄, y|Bx→y , ξ) = 1/6, and
e
p(x̄, ȳ|Bx→y
, ξ) = 1/3. Using the BDe metric, if we
observe database D containing a single case with both
x and y true, we obatin

The user can assess the joint probability distribution
p(x1 , . . . , xn |BSe C , ξ) by constructing a belief network
for U , given BSe C . We call this network the user’s prior
belief network.4
The constant N 0 has a simple interpretation as the
equivalent number of cases that the user has seen since
he was completely ignorant about the domain. Winkler (1967) shows how a user may be trained to assess
N 0.

6

The BDe Metric

Given a prior belief network and the constant N 0 , it
0
is not difficult to show that the exponents Nijk
are
determined by the relation
0
Nijk
+ 1 = N 0 · p(xi = k, Πi = j|BSe C , ξ)

(15)

(see Heckerman et al. 1994 for a derivation). This constraint has a simple interpretation inPterms of equivi
0
alent sample sizes. Namely, Nij0 = rk=1
is the
Nijk
equivalent sample size for the parameter set Θij —the
4

At first glance, there seems to be a contradiction in
asking the user to construct such a belief network—which
may contain assertions of independence—under the assertion that BSe C is true. The assertions of independence in
the prior network, however, refer to independencies in the
next case to be observed. In contrast, the assertion of full
dependence BSe C refers to long-run fractions.

e
e
p(D, Bx→y
|ξ) = p(Bx→y
|ξ) ·

11! 6! 5! 3!
12! 5! 6! 2!

e
e
p(D, By→x
|ξ) = p(By→x
|ξ) ·

11! 5! 4! 3!
12! 4! 5! 2!

Thus, as required, the BDe metric exhibits the property of score equivalence.5

7

Causal Networks

People often have knowledge about the causal relationships among variables in addition to knowledge about
conditional independence. Such causal knowledge is
stronger than is conditional-independence knowledge,
because it allows us to derive beliefs about a domain
after we intervene. For example, most of us believe
that smoking causes lung cancer. From this belief, we
infer that if we stop smoking, then we decrease our
chances of getting lung cancer. In contrast, if we were
to believe that there is only a statistical correlation between smoking and lung cancer, perhaps because there
is a gene that causes both our desire to smoke and lung
5
We note that Buntine presented without derivation
the special case of the BDe metric obtained by letting
p(U |BSe C , ξ) be uniform, and noted the property of score
equivalence. Also, CH presented a special case of the
0
BD metric wherein each Nijk
is set to 1, yielding a uniform Dirichlet distribution on each density ρ(Θij |BSe , ξ).
This special case does not exhibit the property of score
equivalence.

cancer, then we would infer that giving up cigarettes
would not decrease our chances of getting lung cancer.
Causal networks, described—for example—by Spirtes
et al. (1993), Pearl and Verma (1991), and Heckerman
and Shachter (1994) represent such causal relationships among variables. In particular, a causal network
for U is a belief network for U , wherein it is asserted
that each nonroot node x is caused by its parents. The
precise meaning of cause and effect is not important for
our discussion. The interested reader should consult
the previous references.
More formally, we define a causal network to be a pair
(CS , CP ), where CS is a causal-network structure and
CP is a set of probability distributions corresponding
to that structure. The event CSe is the same as that for
a belief-network structure, except that we also include
in the event the assertion that each nonroot node is
caused by its parents.
In contrast to the case of belief networks, it is not appropriate to require the properties of event equivalence
or score equivalence. For example, in our two-variable
domain, both the causal network CS1 where x points to
y and the causal network CS2 where y points to x represent the assertion that x and y are dependent. The
network CS1 , however, in addition represents the assertion that x causes y, whereas the network CS2 represents the assertion that y causes x. Thus, the events
e
e
CS1
are CS2
are not equal. Indeed, it is reasonable to
assume that these events—and the events associated
with any two different causal-network structures—are
mutually exclusive.
Therefore, the consequences of event equivalence discussed in Section 5 do not apply to causal networks.
0
In particular, the exponents Nijk
have no theoretical
constraints, and we may use the BD metric to score
causal networks. Nonetheless, for practical reasons,
0
it is useful to constrain the parameters Nijk
. SDLC
describe one such approach. First, as we do, they
asses a prior network. Then, for each variable xi and
each instance j of Πi in the prior network, they allow the user to specify an equivalent sample size Nij0 .
From these assessments, SDLC compute equivalent
sample sizes Nij0 for other network structures using
an expansion–contraction procedure. This method has
several appealing theoretical properties, but is computationally expensive. CH’s specialization of the BD
0
metric, wherein they set each Nijk
to one is efficient,
but ignores the prior network. We have explored a
simple approach, wherein each Nij is equal to N 00 , a
constant. We call this metric the BDu metric (“u”
stands for uniform equivalent sample sizes). Of course,
the BDe metric may also be used to score causal networks.

Note that, in the context of causal networks, the assumption of parameter modularity (Assumption 4) has
an appealing justification. Namely, we can imagine
that a causal mechanism is responsible for the interaction between each node and its parents. The assumption of parameter modularity then follows from
the assumption that the causal mechanisms are independent.

8

Limitations of the BDe Metric

Let us again consider the scoring of belief networks.
Although our method for determining the exponents
0
Nijk
is simple, it is—in a sense—too simple. Namely,
it may be the case that a user has more knowledge
about some variables than others, and would like to
assess different equivalent sample sizes Nij0 for different values of i and j. If the network is causal, doing so
represents no problem. As we have seen in Section 5,
however, doing so in the case of belief networks requires that we abandon at least one of (1) score equivalence, (2) Dirichlet priors, or (3) the ability to score
all possible network structures.
We believe it is important to retain score equivalence
if at all possible. Furthermore, it is computationally
expensive to abandon the Dirichlet assumption. There
is promise, however, in avoiding the third assumption.
Namely, given a non-score-isomorphic metric that accommodates variable dependent sample sizes, we could
use it to score only one element from each equivalence
class of isomorphic network structures. To do so, we
need a method for designating exactly one network
structure from each equivalence class as the network
to be scored. A simple approach would be to ask the
user to specify a complete ordering over the domain
variables. For example, given the ordering (x, y, z) for
our three-variable domain, the equivalence class corresponding to the conditional independence of x and z
given y would be represented by the network structure
x → y → z; and we would score only this structure.
As a more subtle example, given the same ordering,
the equivalence class corresponding to the conditional
independence of x and y given z would be represented
by x → z → y, because among those network structures in this equivalence class, x occurs first only in
this network structure.

9

Priors for Network Structures

To complete the information needed to compute our
metrics, the user must assess the prior probabilities
for the network structures. These assessments are logically independent of the assessment of the prior network, except in the limit as equivalent sample size(s)

approach infinity, when the prior network structure
must receive a prior probability of one. Nonetheless,
structures that closely resemble the prior network tend
to have higher prior probabilities.

The lower the value of the cross entropy, the more accurate the algorithm. In Heckerman et al. (1994), we
describe a method for computing the cross entropy of
two networks that makes use of the network structures.

Here, we propose the following parametric formula for
p(BSe |ξ) that makes use of the prior network. Given
a network structure BS , let δi denote the number of
nodes in the symmetric difference of the parents of
xi in BS and the parents of xi in the prior network
structure.
Pn Then, BS and the prior network differ by
δ = i=1 δi arcs; and we penalize BS by a constant
factor 0 < κ ≤ 1 for each such arc. That is, we set

In our experiments, we construct prior networks by
adding noise to the gold-standard network. We control the amount of noise with a parameter η. When
η = 0, the prior network is identical to the goldstandard network, and as η increases, the prior network diverges from the gold-standard network. When
η is large enough, the prior network and gold-standard
networks are unrelated. To generate the prior network,
we first add 2η arcs to the gold-standard network, creating network structure BS1 . When we add an arc,
we copy the probabilities in BP 1 so as to maintain
the same joint probability distribution for U . Next,
we perturb each conditional probability in BP 1 with
noise. In particular, we convert each probability to
log odds, add to it a sample from a normal distribution with mean zero and standard deviation η, convert
the result back to a probability, and renormalize the
probabilities. Then, we create another network structure BS2 by deleting η arcs and reversing up to 2η
arcs (a reversal may create a directed cycle, in which
case, the reversal is not done). Next, we perform inference using the joint distribution determined by network (BS1 , BP 1 ) to populate the conditional probabilities for network (BS2 , BP 2 ), which we return as the
prior network.

p(BSe |ξ) = c κδ

(16)

where c is a normalization constant. This formula is
simple, as it requires only the assessment of a single
constant κ. Nonetheless, we can imagine generalizing
the formula by punishing different arc differences with
different weights, as suggested by Buntine. Although
this parametric form does not satisfy score equivalence,
we may recover this property, as described in the previous section, by designating within each event equivalence class the network structure to be scored.

10

Evaluation

In this section, we evaluate the BDe metric using the
36-node Alarm network for the domain of ICU ventilator management [Beinlich et al., 1989]. In our evaluations we start with the given network, which we
call the gold-standard network. Next, we generate a
database from the given network, using a Monte-Carlo
technique. Then, we use one of the scoring metrics and
a local search procedure similar to the one described in
Lam and Bacchus (1993) to identify a high-scoring network structure. Next, we use the database and prior
knowledge to populate the probabilities in the new network, called the learned network. In particular, we set
each probability p(xi = k|Πi = j) to be the posterior
mean of θijk , given the database. Finally, we compare the joint distributions of the gold-standard and
learned networks.
In this paper, we use the cross-entropy measure
for comparison. In particular, let q(xi , . . . , xn ) and
p(xi , . . . , xn ) denote the probability of an instance of
U obtained from the gold-standard and learned networks, respectively. Then we measure the accuracy of
a learning algorithm using the cross entropy H(q, p),
given by
H(q, p) =

X

x1 ,...,xn

q(xi , . . . , xn ) log

q(xi , . . . , xn )
p(xi , . . . , xn )
(17)

Figure 3 shows the cross entropy of learned networks
with respect to the Alarm network (inverse learning
accuracy) as a function of the deviation of the priornetwork from the gold- standard network (η) and the
user’s equivalent sample size (N 0 ) for the BDe metric.
In this experiment, we used 100-case databases generated from the Alarm network. For each value of η and
N 0 , the cross-entropy values shown in the figure represent an average over ten learning instances, where in
each instance we used a different database and prior
network. The databases and prior networks generated
for a given value of η were used for all values of N 0 . We
made the prior parameter κ a function of N 0 —namely,
κ = 1/(N 0 + 1)—so that it would take on reasonable
values at the extremes of N 0 . (When N 0 = 0, reflecting complete ignorance, all network structures receive
the same prior probability. Whereas, in the limit as
N 0 approaches infinity, reflecting complete confidence,
the prior network structure receives a prior probability
of one.)
The qualitative behavior of the curve is reasonable.
Namely, when η = 0—that is, when the prior network was identical to the Alarm network—learning
accuracy increased as the equivalent sample size N 0
increased. Also, learning accuracy decreased as the

91-1, Section of Medical Informatics, University of
Pittsburgh.

BDe Metric

[Cooper and Herskovits, 1992] Cooper, G. and Herskovits, E. (1992). Machine Learning, 9:309–347.

Cross Entropy

5
4

4-5
3-4

3

2
1.6
1.2
n
0.8

2
1

256

N'

64

16

4

1

2-3
1-2
0-1

0.4

0

[Dawid and Lauritzen, 1993] Dawid, A. and Lauritzen, S. (1993). Annals of Statistics, 21:1272–1317.

0

[Geiger and Heckerman, 1994] Geiger, D. and Heckerman, D. (1994). In this proceedings.
[Heckerman et al., 1994] Heckerman, D., Geiger, D.,
and Chickering, D. (March, 1994). Technical Report
MSR-TR-94-09, Microsoft.

Figure 3: Evaluation results.

[Heckerman and Shachter, 1994] Heckerman, D. and
Shachter, R. (1994). In this proceedings.

prior network deviated further from the gold-standard
network, demonstrating the expected result that prior
knowledge is useful. In addition, when η 6= 0, there
was a value of N 0 associated with optimal accuracy.
This result is not surprising. If N 0 is too large, then
the deviation between the true values of the parameters and their priors degrade performance. On the
other hand, if N 0 is too small, the metric is ignoring
useful prior knowledge. We speculate that results of
this kind can be used to calibrate users in the assessment of N 0 .

[Lam and Bacchus, 1993] Lam, W. and Bacchus, F.
(1993). In Proceedings of Ninth Conference on Uncertainty in Artificial Intelligence, Washington, DC,
pages 243–250. Morgan Kaufmann.

Also, the quantitative results are encouraging. To provide a scale for cross entropy in the Alarm domain,
note that the cross entropy of the Alarm network with
an empty network for the domain (i.e., a network
where all variables are independent) whose marginal
probabilities are determined from the Alarm network
is 13.6. Using only a 100 case database, and a prior
network with a significant amount of noise—η = 2,
the cross entropy for the BDe metric, at the optimum
value of N 0 (= 16), is only 1.6.

Acknowledgments
We thank Jack Breese, Wray Buntine, Greg Cooper,
Steffen Lauritzen, and anonymous reviewers for useful
suggestions.


