ions

Alejandro Isaza and Csaba Szepesvári and Vadim Bulitko and Russell Greiner
Department of Computing Science, University of Alberta
Edmonton, Alberta, T6G 2E8, CANADA
{isaza,szepesva,bulitko,greiner}@cs.ualberta.ca

Abstract
In this paper, we consider planning in stochastic
shortest path (SSP) problems, a subclass of
Markov Decision Problems (MDP). We focus on
medium-size problems whose state space can be
fully enumerated. This problem has numerous
important applications, such as navigation and
planning under uncertainty. We propose a new
approach for constructing a multi-level hierarchy
of progressively simpler abstractions of the
original problem. Once computed, the hierarchy
can be used to speed up planning by first finding
a policy for the most abstract level and then recursively refining it into a solution to the original
problem. This approach is fully automated and
delivers a speed-up of two orders of magnitude
over a state-of-the-art MDP solver on sample
problems while returning near-optimal solutions.
We also prove theoretical bounds on the loss
of solution optimality resulting from the use of
abstractions.

1

Introduction and Motivation

We focus on planning in stochastic shortest path problems
(the problem of reaching some goal state under uncertainty)
when planning time is critical — a situation that arises, for
instance, in path planning for agents in commercial video
games, where map congestions are modeled as uncertainty
of transitions. Another example is path planning for
multi-link robotic manipulators, where the uncertainty
comes from unmodeled dynamics as well as sensor and
actuator noise. More specifically, we consider the problem
of finding optimal policies in a sequence of stochastic
shortest-path problems (Bertsekas & Tsitsiklis, 1996),
where the problems share the same dynamics and transition
costs, and differ only in the location of the goal-state.
When the state space underlying the problems is sufficiently large, exact planning methods are unable to
deliver a solution within the required time, forcing the
user to resort to approximate methods in order to scale to
large domains. Exploiting the fact that multiple planning

problems share the same dynamics and transition costs, we
build an abstracted representation of the shared structure
where planning is faster, then map the individual planning
problem into the abstract space and derive a solution there.
The solution is then refined back into the original space.
In a related problem of path planning under real-time
constraints in deterministic environments (e.g., Sturtevant,
2007), a particularly successful approach is implemented
in the PR LRTS algorithm (Bulitko, Sturtevant, Lu, & Yau,
2007), which builds an abstract state space by partitioning
the set of states into cliques (i.e., each state within each
cluster is connected to each other state in that cluster
with a single action). Each such cluster becomes a single
abstract state. Two abstract states are connected by an
abstract transition if there is a pair of non-abstract states
(one from each abstract state) connected by a single
action. The resulting abstract space is smaller and simpler,
yet captures some of the structure of the original search
problem. Thus, an abstract solution can be used to guide
and constrain the search in the original problem, yielding
a significant speed-up. Further speed-ups can be obtained
by building abstractions on top of abstractions, which
creates a hierarchy of progressively smaller abstract search
spaces. PR LRTS can then be tuned to meet strict real-time
constraints while minimizing solution suboptimality.
Note that state cliques produced by PR LRTS make good
abstract states because landing anywhere in such a cluster
puts the agent a single action away from any other state in
the clique. This also means that the costs of the resulting
actions are similar, and that the cost of a single action
is negligible compared with the cost of a typical path.
Finally, any (optimal) path in the original problem can
be closely approximated at the abstract level, as an agent
following an (optimal) path has to traverse from cluster to
cluster. Since all neighboring clusters are connected in the
abstract problem, it is always possible to find a path in the
abstract problem that is “close” to the original path.
Given the attractive properties or PR LRTS, it is natural
to ask whether the ideas underlying it can be extended
to stochastic shortest path problems, with arbitrary cost
structures.
In a stochastic problem, the result of planning is a closed
loop policy that assigns actions to states. A successful ab-

straction must be suitable for approximating the execution
trace of an optimal policy. Imagine that clustering has been
done in some way. The idea is again to have abstract actions that connect neighboring clusters cheaply — that is,
the system should not produce expensive connections.
Intuitively, we want to connect one cluster to another if,
from any state of the first cluster, we can reliably get to
some state of the second cluster at roughly a fixed cost (the
same for any state in the first cluster). This way, “simulating” a policy of the original problem becomes possible
at a small additional cost (the meaning of simulation will
become clear later). This means that a connection between
clusters is implemented by a policy with a specific set
of initial states that brings the agent from any state of
the source cluster to some state of the target cluster. We
will use options (Sutton, Precup, & Singh, 1999) for such
policies, and choose clusters to allow such policies for any
two neighboring clusters. Thus, it is natural to look for
clusters of states that allow one to reliably simulate any
trajectory from any of the states to any other state.
Finally, we need an extra mechanism, the “goal approach”,
that deals with the challenge of reaching the base-level
goal itself from states that are close to the goal. Thus, our
planner first plans in the abstract space to reach the “goal
cluster”. After arriving at some state of the “goal approach
region”, the planner then uses the “goal-approach policy”
that, with high probability, moves the agent to the goal
state itself. These ideas form the core of our algorithm.

The MDP is undiscounted if γ = 1. An action
a ∈ ∪x∈X A(x) is called admissible in state x if a ∈ A(x).
Definition 2 A (generic) policy is a mapping that assigns
to each history (x0 , a0 , c0 , . . . , xt−1 , at−1 , ct−1 , xt ) an
action admissible in the most recent state xt . In general, a
mapping that maps possible histories to some set is called
a history dependent mapping.
Under mild conditions, it suffices to consider only stationary, deterministic policies (Bertsekas & Tsitsiklis, 1996),
on which we will focus:
Definition 3 A stationary and deterministic policy π is a
mapping of states to actions such that π(x) ∈ A(x) holds
for any state x ∈ X.
In what follows, we will use “policy” to mean stationary
and deterministic policies, unless otherwise mentioned.
The expected cost of policyPπ when the system starts
∞
in state x0 is vπ (x0 ) = E [ t=0 γ t c(Xt , π(Xt ), Xt+1 )]
where Xt is a Markov chain with P (Xt+1 = y|Xt = x) =
p(y|x, π(x)). The function vπ is called the value-function
underlying policy π.
One “solves” an MDP by finding a policy that minimizes
the cost from every state, simultaneously. In this paper
we deal only with stochastic shortest path problems, a
subclass of MDPs. In these MDPs the problem is to get to
a goal state with the least cost:

The three major contributions of the paper are: (i) a
novel theoretical analysis of option-based abstractions,
(ii) an effective algorithm for constructing high-quality
option-based abstractions, and (iii) experimental results
demonstrating that our algorithm performs effectively over
a range of problems of varying size and difficulty.

Definition 4 A finite stochastic shortest path (SSP)
problem is a finite undiscounted MDP that has a special
state, called the goal state g, such that ∀a ∈ A(g), we have
p(g|g, a) = 1 and c(g, a, g) = 0 and the immediate costs
for all the other transitions are positive.

Section 2 formally describes our problem, and provides
the theoretical underpinning of our approach. Section 3
then presents our algorithm for automatically building
options-based abstractions, and Section 4, our planning
algorithm that uses these abstractions. Section 5 empirically evaluates this approach, in terms of both efficiency
and effectiveness (suboptimality). Finally, Section 6
summarizes related work.

Consider a finite SSP (X, A, p, c). Let π be a stationary
policy. We say that this policy is proper if it reaches the
goal state g with probability one, regardless of the initial
state. Let Tπ : RX → RX be the policy’s evaluation
operator:
X
(Tπ v)(x) =
p(y|x, π(x)) [c(x, π(x), y) + v(y)] .
y∈X

2

Problem Formulation and Theory

This section formally defines stochastic shortest path
problems and the abstractions that we will consider. It
also presents a theoretical result that characterizes the
relationship between the performance of abstract policies
and policies of the original problem.
Definition 1 A Markov Decision Process (MDP) is defined by a finite state space X = {1, . . . , n}; a finite set of
actions A(x) for each state x ∈ X; transition probabilities
p(y|x, a) ∈ [0, 1] that correspond to the probability that
the next state is y when action a is applied in state x;
immediate cost c(x, a, y) ∈ ℜ for all x, y ∈ X and all
a ∈ A(x) and a discount factor γ ∈ (0, 1].

Bertsekas and Tsitsiklis (1996) prove that Tπ is a contraction with respect to a weighted maximum norm,
k·kw,∞ , with some positive weights, w ∈ RX
+ , where
kvkw,∞ = maxx |v(x)|/w(x). In particular, w(x) can be
chosen to be the expected number of steps until π reaches
the goal state when started from x. The contraction coefficient of Tπ , γπ , satisfies 1/(1 − γπ ) = maxx∈X w(x).
Thus, 1/(1 − γπ ) is the maximum of the expected number
of steps to reach the goal state, or in other words, the
maximum expected time policy π spends in the MDP (cf.
Prop 2.2 in Bertsekas & Tsitsiklis, 1996).
We adopt the notion of options from Sutton et al. (1999):
Definition 5 An option is a triple (π, I, ψ), where I ⊂ X
is the set of initial states, π is a (generic) policy that is

defined for histories that start with a state in I and ψ is
a history dependent mapping with range {0, 1}, called
the terminating condition. We say that the terminating
condition fires when ψ(ht ) = 1. Let T > 0 denote the
random time when the terminating condition fires for the
first time while following π. (Note that T = 0 is not
allowed.) We assume that P (T < +∞) = 1, independent
of the initial state when the policy π is started (i.e., the
option terminates in finite time with probability one).
As suggested in the introduction, an abstraction is a way to
group states and the abstract actions correspond to options:
Definition 6 We say that the MDP (X̃ , Ã, p̃, c̃) is an
option-based abstraction of (X, A, p, c), if there exists a
mapping, S : X̃ → 2X specifying the states S(x̃) ⊂ X
that correspond to an abstract state x̃ ∈ X̃ , a set Π of
options abstracting the actions of the MDP and a mapping
Ψ : ∪x̃∈X̃ Ã(x̃) → Π such that for any ã ∈ Ã(x̃), if
Ψ(ã) = (I, π, ψ) then S(x̃) ⊂ I.1
Henceforth we will use “abstraction” instead of “optionbased abstraction” and will call (X̃ , Ã, p̃, c̃) the “abstract
MDP”, X̃ the set of abstract states, Ã the set of abstract
actions, etc. Notationally, we call (X, A, p, c) the ground
level MDP, and we will identify quantities related to the
abstract MDP by using a tilde ( ˜ ). For simplicity, we
will identify the abstract actions with their corresponding
options. In particular, we will call ã both an abstract action
and an option, depending on the context.
In the following, we will assume that {S(x̃) | x̃ ∈ X̃ } is
a partition of X; we can then let x̃ : X → X̃ denote the
(unique) abstract state that includes x: x̃(x) ∈ X̃ such that
x ∈ S(x̃(x)), and say that (X̃ , S) is an aggregation of the
states in X. We also define S(x) = S(x̃(x)) as the set of
states in X that are in the same partition with x.
The restriction on Ψ in the above definition ensures that
the execution of any policy π̃ in the abstract MDP is
well-defined and proceeds as follows. Initially, there is
no active option. In general, whenever there is no active
option, we look up the abstract state x̃ = x̃(x) based on
the current state x and activate the option Ψ(π̃(x̃)). When
there is an active option, the option remains active until
the corresponding terminating condition fires. When an
option is active, the option’s policy selects the actions in
the ground level MDP. This way a policy π̃ in the abstract
MDP induces a policy in the ground level MDP.
Our goal now is to characterize what makes an abstraction
accurate. The following theoretical analysis is novel as it
considers abstractions where the action set is changed. In
particular, the action set can potentially be reduced and
the abstract actions can be options. To our knowledge,
such options-based abstractions have not been analyzed
previously; the closest results are probably Theorem 2 of
Kim and Dean (2003) and Theorem 4 of Dean, Givan, and
Leach (1997). The proof is rather technical and is given
1 X

2

denotes the power set of X: the set of all subsets of X.

in the extended version of our paper (Isaza, Szepesvári,
Bulitko, & Greiner, 2008).
Consider a proper policy π of the ground level MDP. We
want abstractions such that one can always find a policy in
the abstract MDP (X̃ , Ã, p̃, c̃) that approximates π well, no
matter how π was chosen. Clearly, this depends on how the
action set Ã and the corresponding transitions and costs
are defined in the abstract MDP. Quantifying this requires a
few definitions: Let p̃π (x̃, ỹ) be the probability of landing
in some state of S(ỹ) when following policy π until it
leaves the states of S(x̃), when the initial state is selected
at random from the states of S(x̃) based on the distribution
µS(x̃) . Let c̃π (x̃) denote the corresponding expected
“immediate” cost. Now pick a proper policy π̃ of the
abstract MDP. Let w̃ be the weight vector that makes Tπ̃ a
contraction in the abstract MDP.
P Further, define p̃π̃ (x̃, ỹ) =
p̃(ỹ|x̃, π̃(x̃)) and c̃π̃ (x̃) = ỹ∈X̃ p̃π̃ (x̃, ỹ)c̃π̃ (x̃, ỹ) and the
mixed ℓ1 /ℓ∞ norm k·kw̃,1/∞ :
kp̃1 − p̃2 kw̃,1/∞ = max
x̃∈X̃

X

ỹ∈X̃

|p̃1 (x̃, ỹ) − p̃2 (x̃, ỹ)|

w̃(ỹ)
.
w̃(x̃)

Let
επ,π̃ = kc̃π − c̃π̃ kw̃,∞ + cmax kp̃π − p̃π̃ kw̃,1/∞ ,

(1)

where cmax is the maximum of the immediate costs in the
ground level MDP. Hence, επ,π̃ measures how well the
costs and the transition probabilities induced by π “after
state aggregation” match those of π̃. Introduce c(x, π) as
the expected total cost incurred, conditioned on that policy
π starting in state x and stopping when it exits S(x). Further, introduce p(ỹ|x, π) as the probability that, given that
policy π is started in state x, when it exits S(x) it enters
S(ỹ) (ỹ 6= x̃(x)). Now fix an abstract state x̃ ∈ X̃ . If the
costs {c(x, π)}x∈S(x̃) and probabilities {p(ỹ|x, π)}x∈S(x̃) ,
ỹ 6= x̃, have a small range then we can model closely the
behavior of π locally at S(x̃) by introducing an option
with initial states in S(x̃) which mimics the “expected”
behavior of π as it leaves S(x̃), assuming, say, that the
initial state in S(x̃) is selected at random according to
the distribution µS(x̃) . If we do so for all abstract states
x̃ ∈ X̃ then we can make sure that minπ επ,π̃ is small. If
the above range conditions hold for all policies π of the
ground level MDP and all abstract states x̃ ∈ X̃ then by
introducing a sufficiently large number of abstract actions
it is possible to keep maxπ minπ̃ επ,π̃ small. Further,
notice that maxπ p(ỹ|x, π) is zero unless there exists a
transition from some state of S(x) to some state of S(ỹ),
in which case we say that S(x) is connected to S(ỹ).
Hence, no abstract action is needed “between” x̃ and ỹ,
unless S(x̃) is connected in the ground level MDP to S(ỹ).
Define T̃P
: B(X̃) → B(X̃), (T̃π ṽ)(x̃) =
π
c̃π (x̃) +
p̃
Since π is proper in the
ỹ π̃ (x̃, ỹ)ṽ(ỹ).
ground level MDP, it is not difficult to show that T̃π is
a contraction with respect to an appropriately defined
weighted supremum norm.
The next result gives a bound on the difference of value
functions of π and π̃ in terms of επ,π̃ :

Theorem 1 Let π be a proper policy in the ground level
MDP and let π̃ be a proper policy in the abstract MDP. Let
wπ (resp., w̃π ) be the weight vector that makes Tπ (resp.,
T̃π ) a contraction and let the corresponding contraction
factor be γπ (resp., γ̃π ). Let vπ be the value function of π
and ṽπ̃ be the value function of π̃. Then
kvπ − Eṽπ̃ kw,∞ ≤

kAvπ − vπ kw,∞
1 − γπ

+ λπ

επ,π̃
,
1 − γ̃π

where the operator E extends functions defined over X̃
to functions defined over X in a piecewise constant manner: E : B(X̃ ) → B(X), (Eṽ)(x) = ṽ(x̃(x)), and
A : B(X) → B(X) is the aggregation operator defined by
X
(AV )(x) =
µS(x) (z)V (z),
z∈S(x)

and λπ = maxx∈X w̃π (x̃(x))/wπ (x).
The factor λπ measures how many more steps are needed
to reach the goal if the execution of policy π is modified
such that, whenever the policy enters a new cluster x̃, the
state gets perturbed, by choosing a random state according
to µS(x̃) .
The theorem provides a bound on the difference between
the value function of a ground-level policy π and the value
function of an abstract policy when its value function is
extended to the ground-level states. The bound has two
terms: The first bounds the loss due to state abstraction,
while the second bounds the loss due to action abstraction.
When a similar range condition holds for the abstract
actions, too, then it is possible to bound the difference
between the value function of the policy induced in the
ground level MDP by π̃ and Eṽπ̃ , yielding a difference on
the value functions of π and the policy induced by π̃. Isaza
et al. (2008) provides further details.
If we apply this result to an optimal policy π ∗ of the
ground level MDP, we immediately get a bound on the
quality of the abstraction. We may conclude then that
the quality of abstraction is determined by the following
factors: (i) whether states with different optimal values are
aggregated; (ii) whether the random perturbation described
in the previous paragraph can increase the number of steps
to the goal substantially; and (iii) whether the immediate
costs c̃π∗ and transition probabilities p̃π∗ can be matched
in the abstract MDP.
Since we want to build abstractions that work independently of where the goal is placed, the knowledge
of the optimal policy with respect to a particular goal
cannot be exploited when constructing the abstractions.
In order to prevent large errors due to (i) and (ii), we
restrict aggregation such that only a few states are grouped
together. This makes the job of creating an aggregation
easier. Fortunately, we can achieve higher compression by
adding additional layers of abstractions. We can address
(iii) by creating a sufficiently large number of abstract
actions. Here, we use the simplifying assumption that we
only create abstract actions that bring the agent from some
cluster of states to some neighboring cluster. These can

serve as a “basis” for matching any complex next-state
distribution over the clusters by choosing an appropriate
stochastic policy in the abstract MDP. We also want to
ensure that the initial state within a cluster has a small
influence on the probability of transitioning to some
neighboring cluster and the associated costs. We use two
constants, ε and µ, to bound the amount of variation with
respect to initial states; note this allows us to control the
difference between the value function of a policy induced
in the ground level MDP by some abstract policy π̃ and the
extension of the value function of π̃ defined in the abstract
MDP to the ground level states, Eṽπ̃ . This is necessary to
ensure that a good policy in the abstract MDP produces a
good policy in the ground-level MDP, ultimately assuring
that the optimal policy of the abstract MDP will give rise
to a close to optimal policy in the ground-level MDP. The
resulting procedure is described in the next section.

3

Abstracting an SSP

This section describes our algorithm BuildAbstraction
for automatically building options-based abstractions.
These abstractions are goal-independent and thus apply
to a series of SSPs that share the state space and transition dynamics. The process consists of four main steps
(Figure 1): (1) Cluster proposes candidates for abstract
states; (2) GenerateLinkCandidates proposes candidates
for abstract actions (or “links”); (3) Repair validates and, if
necessary, repairs the links in order to satisfy the so-called
(ε, µ)-connectivity property (the formal definition is given
later) and Prune discards excessive links.
Once an abstraction is built, we use a special-purpose
planning procedure (described in Section 4) to solve
specific SSPs. The rest of this section describes the four
steps of our BuildAbstraction algorithm in detail.
Step 1: Cluster. A straightforward cluster-er will cluster a
state with some of its immediate neighbors. Unfortunately,
this approach may group states with diverging trajectories
(the trajectories from one state can differ from those of the
other state). By looking for the peers of a state (predecessors of its successors, line 2, Figure 1) we hope to find a
peer whose trajectories are similar to the trajectories of the
first state. Note that the clustering routine creates minimal
clusters. This is advantageous as it means the subsequent
steps, which connect clusters, is more likely to succeed.
Unfortunately, it also means relatively low reduction in the
number of states. Several layers of abstractions can help
increase this reduction.
Step 2: Generate Link Candidates. After forming the
initial clusters (i.e., the initial abstract states), BuildAbstraction generates candidates for abstract actions. One
approach is simply to propose abstract actions for all pairs
of abstract states, in the hope that only important ones will
remain after pruning. We use a less expensive strategy and
propose abstract action candidates only for “nearby” clusters (line 8). For each such pair we add two candidate links:
one in the forward and another in the backward direction —
this heuristic quickly generates reasonable link candidates.
We typically use k = 1. Our experiments confirm this is

BuildAbstraction(k, p, M ) // M – ground level MDP
–Cluster–
1 for each unmarked ground state x do
2
Find P (x), all the predecessors of successors of x
3
Find y ∈ P (x) that has the most successors
in common with x
4
Add x̃ to X̃ with S(x̃) = {x, y}
5
Mark states {x, y}
6 end for
–GenerateLinkCandidates–
7 repairQ ← ∅
8 for every x̃, ỹ ∈ X̃, where any state in S(ỹ) is
within k ground transitions of some state in S(x̃) do
9
repairQ ← repairQ ∪ {(x̃, ỹ), (ỹ, x̃)}
10 end for
–Repair–
11 while repairQ 6= ∅ do
12
(x̃, ỹ) ← pop an element from repairQ
13
set up an SSP, S, with domain R ⊂ X
where S(x̃)∪S(ỹ) ⊂ R with states in S(ỹ) as goals
14
attempt to find an optimal policy πS in S with IPS
15
if no policy found then
16
continue
17
else if πS does not meet the (ε, µ) conditions then
18
split the cluster adding both parts to repairQ
19
else
20
add ã to Ã(x̃) with Ψ(ã) = (S(x̃), πS , IS(ỹ) ) 2
21
set c̃(x̃, ã) to be the expected cost of
executing ã from a random state of x̃
22
set p̃(ỹ|x̃, ã) = 1, p̃(ỹ ′ |x̃, ã) = 0 for ỹ ′ 6= ỹ.
23
end if
24 end while
–Prune–
25 for each state x̃ do
26
find Ã∗ (x̃) = {ã1 , . . . , ãm }, all abstract actions
that connect clusters that are neighbors in M
27
order Ã(x̃) \ Ã∗ (x̃) to create [ãm+1 , . . . , ãn ] such
that c̃(x̃, ãi ) ≤ c̃(x̃, ãi+1 ), i = m + 1, . . . , n − 1
28
let Ã(x̃) = {ã1 , . . . , ãp }
29 end for
30 return (X̃, Ã, p̃, c̃).

Figure 1: The abstraction algorithm.
sufficient; increasing k results in slightly better quality, but
slower running times when solving the planning problems.
Step 3: Repair. For each candidate abstract action connecting abstract states x̃ and ỹ, we first need to derive an option
that, starting in any state in cluster x̃ leads the agent to some
state in cluster ỹ with a minimum total expected cost. We
derive this option by setting up a shortest path problem S,
whose domain includes S(x̃) and S(ỹ). We set the domain
of S to be sufficiently large that a policy within this domain
can reliably take the agent from any state of S(x̃) to some
state of S(ỹ). BuildAbstraction builds this domain by
performing a breadth-first search from S(ỹ), proceeding
backwards along the transitions, stopping at depth D + m,
where D is the search depth from S(ỹ) and m is the margin
to leave after all states of S(x̃) were added to the domain.
If there is any state of S(x̃) that was not included at depth
D, the Repair routine reports ‘no solution’. The transitions,
actions and costs of S are inherited from the MDP M . We
also add a new terminating state, which is the destination
2
Here IS is the characteristic function of S: IS (x) = 1 iff
x ∈ S and IS (x) = 0 otherwise.

of transitions leaving the region — i.e., those transitions
are redirected to this new terminal, with a transition cost
that exceeds the maximum of the total expected costs of the
ground level MDP. The high cost discourages the solutions
to enter the extra terminating state. The optimal solution
to S is obtained by using the Improved Prioritized Sweeping (IPS) algorithm of McMahan and Gordon (2005),
(line 14). We selected this algorithm based on its excellent
performance and known optimality properties (IPS reduces
to Dijkstra’s method in deterministic problems). The
resulting policy π is checked against (ε, µ)-connectivity,
defined as follows: we first compute the expected total
cost of reaching some states in S(ỹ) for all states of S(x̃);
let the resulting costs be c(x, π). Similarly, we compute
the probabilities p(S(ỹ)|x, π) for every x ∈ S(x̃). Then
we check if maxx,x′ ∈S(x̃) |c(x, π) − c(x′ , π)| ≤ ε and
maxx,x′ ∈S(x̃) |p(S(ỹ)|x, π) − p(S(ỹ)|x′ , π)| ≤ µ both
hold. If these constraints are met, a new abstract action
is created and is added to the set of admissible actions
at x̃ and the policy is stored as the option corresponding
to this new abstract action (lines 20–22). Otherwise, the
cluster is split (since every cluster has two states, this is
trivial) and the appropriate link candidates are added to the
repair queue so that no link between potentially connected
clusters is missed.
Step 4: Prune. After step 3, we have an abstract SSP whose
abstract states are (ε, µ)-connected. However, our abstract
action generation mechanism may produce too many
actions, which may slow down the planning algorithm (see
Section 4). We address this problem using a pruning step
that leaves only the “critical” and cheapest abstract actions.
An action is “critical” if it connects clusters that are
connected at the ground level with a single transition; these
actions are important to keep the structure of the ground
level MDP. We also keep the cheapest abstract actions
as they are likely to help achieve high quality solutions.
The “pruning parameter”, p, specifies the total number of
actions to keep. (If p is smaller or equal than the number of
ground actions, then only the “critical” actions are kept.)
BuildAbstraction runs in time linear in the size of the
input MDP, as every step is restricted to some fixed size
neighborhood of some state (i.e., every step is local). Further, employing a suitable data structure, the memory requirements can also be kept linear in the size of the input.
These properties are important when scaling up to realistic,
real-world problem sizes.

4

Planning with an Abstraction

After building an abstraction, we can use it to solve
particular SSP problems. When we specify a new goal,
our abstraction planner, AbsPlanner, then creates a
goal-approach region in the abstract MDP that includes
the goal and is large enough to include all states of the
cluster containing the goal. AbsPlanner builds this region
by starting with the ground goal and adding states and
transitions in a breadth-first fashion to a certain depth,
proceeding backwards along the transitions, stopping only
after adding all states of the goal-cluster. After building the
region, AbsPlanner produces an SSP. The domain of this

SSP includes the states found in the breadth-first search,
and also a new terminal state that becomes the destination
of transitions leaving the region — i.e., those transitions
are redirected to this new terminal, with a high transition
cost. All other costs and transitions of this SSP are inherited from the ground level MDP. AbsPlanner uses IPS to
solve the local MDP, and saves the resulting goal-approach
policy. It then solves the abstract MDP, where the goal
cluster is set as the goal. When executing the resulting
policy π̃, AbsPlanner proceeds normally until reaching a
state of the goal-approach region; it then switches to the
goal-approach policy, which it follows until reaching the
goal or leaving the region. When this latter event happens
and the state is x, execution switches to the option π̃(x̃(x)).
When using multiple levels of abstraction, AbsPlanner’s
execution follows a recursive, hierarchical strategy. Note
that the size of the goal-approach region is independent of
the size of the MDP. Thus, the planning time will depend
on the size of the top-level abstract MDP. For an MDP of
size n, by using log n levels of hierarchy, in theory it is then
possible to achieve planning times that scale with O(log n).
However, depending on the problem, it might be hard to
guarantee high quality solutions when using many levels
of abstraction. Furthermore, in practice (over the problems
used in our tests), the computation time is dominated by the
time needed to set up and solve the goal-approach SSPs,
which is required for even one layer of abstraction. This
is partly because our abstractions result in deterministic
shortest path problems, whose solutions can be found
significantly faster than those of stochastic problems.

5

Empirical Evaluation

This section summarizes our empirical evaluation of this
approach, in terms of the quality (suboptimality) of the
solutions and the solution times. Here we report the tradeoffs of using different levels of abstraction as well as the
dependence on the “stochasticity” of the transitions. (Note
that stochasticity makes it difficult to build abstractions.)
We also tested the performance of the algorithm on more
practical problems. In addition to the results presented
here, we conducted extensive experiments, studying the
trade off between solution quality and solution time as a
function of the various parameters of our algorithm (e.g.,
the values of p, k, or the number of abstraction levels), the
scaling behavior of our algorithm in terms of its resource
usage, the quality of solutions and the solution time. These
results, appearing in (Isaza et al., 2008), confirm that the
algorithm is robust to the choices of its parameters and
scales as expected by increasing problem sizes.
We run experiments over three domains: noisy gridworlds,
a “river” and congested game maps. The gridworlds are
empty and have four actions: up, down, left, and right,
each with cost 1. The probability that an action leaded to
the expected position (e.g., the action up moves the agent
up one cell) is 0.7, while the probability of reaching any of
the other three adjacent cells is 0.1.
The river is similar to the gridworld: its dimensions are
w × h, but there is a current flowing from left to right

and a fork corresponding to a line connecting the points
(w/2, h/2) and (w, h/2).3 The flow is represented by
modifying both the cost structure and the transition probabilities of the actions: action forward costs 1, backward
costs 5, diagonally-up-and-forward
and diagonally-down√
and-forward each cost 2. These actions are also stochastic: For the backward action, the probabilities are 0.7 for
going back and 0.1 for each of the other actions. For the
other three actions, the anticipated move occurs with probability 0.6 and the other moves except backwards occur
each with probability 0.2, and backwards has probability
0. We include the river domain to determine whether our
system can deal with non-uniform structures and because
the fork complicates the task of creating abstractions. We
empirically found the time to build abstractions for the
n-state gridworld was close to n/100 seconds, and around
n/50 for an n-state river domain. The build time for the
maps, using k = 1, was between 75 and 100 seconds.4
The congested game maps are again similar to gridworlds,
but with obstacles and with transitions probabilities that
depend on the congestion. The obstacle layout comes
from commercial game maps, and the stochastic dynamics
simulate what happens if multiple units traverse the same
map: in narrow passages, the units to become congested,
which means an agent trying to traverse such a passage
is likely to be blocked. We model this by modifying
each action by including a probability that the action will
“fail” and cause the agent to stay at the same position.
This “failure probability” depends on the position on the
game map, calculated by simulating many units randomly
traversing the game maps and measuring the average occupation of the individual cells, then turning the occupation
numbers into probabilities. The optimal policy of an agent
in a congested game map will then try to avoid narrow
passages, since the higher probability of traffic congestion
in such regions means an agent takes much longer to get
through those regions.
The baseline performance measures are obtained by
running the state-of-the-art SSP solver algorithm IPS. For
each study, we generate the abstraction and then use it
to solve 1,000 problems, whose start and goal locations
are selected uniformly at random. For each problem we
measure the solution time in seconds and the total solution
cost for both IPS and our method, then compute the
geometric average of the individual suboptimalities and
the individual solution time ratios.
5.1

Abstraction level trade-offs

We used a 100 × 100 gridworld to analyze the trade-offs of
different abstraction levels, with several different parameter configurations. We say a configuration is “dominant” if
it was a Pareto optimal — i.e., if no other configuration is
better in both time and suboptimality.
Figure 2 presents properties of the dominant configurations
3
See (Isaza et al., 2008) for more details, including relevant
pictures.
4
We ran all experiments on a 2GHz AMD Opteron(tm)
Processor with 4GB of RAM running Linux with kernel 2.6.18.

1.25
1.2
1.15

0.46
0.52

0.55
0.58
0.64

0.88

0.76

1.1

0.82

0.94

0.85

1.05
0

0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008
Solution time ratio

Figure 2: Subobtimality versus the solution time ratio as
compared to IPS for different parameter configurations.
The dominant configurations are shown for different levels
of abstraction.
for various abstraction levels. We see that using a smaller
number of abstractions required more time but produced
better solutions (i.e., lower suboptimality), and higher
levels of abstractions required less solution time but
produced inferior solutions (i.e., increased suboptimality).
Note that there are dominant configurations for every level
of abstraction, from 0 to 5.
We obtain a “level 0” abstraction by converting the given
ground-level SSP to deterministic shortest path problem
with the same states. (Recall that our abstraction process
abstracts the state space and produces a deterministic SSP;
here we just used the original state space.) Figure 2 shows
that this transformation provides solutions whose quality
is slightly inferior to the original problem, but it finds this
solution significantly faster (e.g., in 0.005 to 0.0073 of
the time). We also see that these “level 0” solutions are
superior to those based on higher abstraction levels, but
one can obtain these level-i solutions in yet less time.
5.2

0.40

1.3
Suboptimality

level 0
level 1
level 2
level 3
level 4
level 5

Suboptimality vs. speed-up on a 50x50 gridworld
1.35

0.02

Figure 3 plots the suboptimality and the speed-up of
finding a solution using our method, as compared to IPS,
for different values of P . We see that our method loses
optimality as the dynamics becomes noisier (i.e., when P
gets smaller). This is because our abstract actions, trying
to move the agent from one abstract state to the next will
fail with higher probability for noisier dynamics. Note

0.05

that the advantage of our method, in terms of planning
time, becomes larger with increased stochasticity. This is
because our abstractions are deterministic and planning in
a deterministic system is much faster than planning with a
stochastic system.
Figure 4, plotting the absolute values of cost and time
for both our method and IPS, provides another insight: It
shows that for increasing stochasticity both methods are
slowed down, but our method can cope better with this
situation. This figure also confirms that this leads to a loss
in solution quality.
For our method the typical parameters produce a suboptimality of around 1.4 for the river, and around 1.25 for
the gridworld domain. The speed-up for the gridworld is
around 30, while for the river it is around 800.

Sensitivity to Stochasticity of the Dynamics

As the environment becomes noisier, it becomes more difficult to construct a high quality abstraction. This section
quantifies how the solution quality and construction time
relate to noise in the dynamics. In general, we consider an
action “successful” if the agent moves to the appropriate
direction; our gridworld model set the success probability
to P = 0.7, leaving a probability of (1 − P )/3 to moving
in each of the other three directions. Here, we vary the
value of P . All of these experiments use a 50 × 50
gridworld with k = 2 and p = 4 (which means we keep
only the “critical” actions; see Section 3).

0.03
0.04
Solution time ratio

Figure 3: Subobtimality versus the solution time ratio as
compared to IPS for different values of P .

Cost

Suboptimality

Suboptimality vs. Speed-up on a 100x100 gridworld
1.7
1.65
1.6
1.55
1.5
1.45
1.4
1.35
1.3
1.25
1.2
1.15

Cost vs. solution time trade-off in a 50x50 gridworld
200
0.40
IPS
180
Abstraction
160
140
0.40
120
0.46
100
0.46
0.52
80
0.52
0.58
60
0.64
0.58
0.64
0.76
40
0.76
0.94
0.94
20
0.01
0.1
1
10
100
Solution time (s)

Figure 4: Cost versus solution time for IPS and abstraction
at different values of P .
5.3

Congested Game Maps

To test the performance of our approach in a more practical
application, we used maps modeled after game environments from commercial video games. We first created
simplified gridworlds that resemble some levels from a

Figure 5: A congested game map.
Darker/redder
color refers to high congestion. Dark blue regions are
impassable obstacles.
popular role-playing and real-time-strategy game. We then
converted the gridworlds into congested maps as described
earlier. This produced maps with state space sizes of
6176 (BG1), 5672 (BG2), 5852 (BG3), 20249 (WC1) and
9848 (WC2). Figure 5 provides one such map, where
each state’s color indicates the associated congestion:
warmer/redder colors indicates high congestion (i.e.,
low probability of success P ) while colder/bluer colors
indicates low congestion (i.e., high value of P ). Very dark
blue indicates impassable obstacles. We see that many
of the states in cluttered regions are highly congested and
should therefore be avoided.
Figure 6 shows the solution time and the solution suboptimalities for both our method and IPS, for two maps from
WarCraft (Blizzard Entertainment, 2002) and three maps
from Baldur’s Gate (BioWare Corp., 1998), including
Figure 5, using only a single layer of abstraction. We see
that our approach is indeed successful in speeding up the
planning process, while keeping the quality of the resulting
solutions high.

6

Related Work

Due to space constraints we review only the most relevant
work; references to other related works can be found in the
extensive bibliography lists of the cited works. Dean et al.
(1997) introduced the notion of ε-homogeneous partitions
and analyzed its properties, but without giving explicit
loss bounds. Kim and Dean (2003) developed some loss
bounds. Their Theorem 2 can be strengthened with our
proof method to kv ∗ − vP∗ k∞ ≤ kT vP∗ − vP∗ k∞ /(1 − γ)
(using our notation), basically dropping the first term
in their bound. Here v ∗ is the optimal value function
in the original MDP, vP∗ is the optimal value function
of the aggregated MDP extended back to the original
state space in a piecewise constant manner and T is the
Bellman-optimality operator in the original MDP. This
bound is problematic as it does not show how the quality of

partitions influences the loss. Our bound improves on this
bound in this respect, and also by extending it to the case
when the abstract actions correspond to options. While
Asadi and Huber (2004) also considered such optionsbased abstractions, they assume that the abstract actions
(options) are given externally (possibly by specifying goal
states for each of them) and they do not develop bounds.
In a number of subsequent papers, the authors refined
their methods. In particular, they became increasingly
focussed on learning problems. For example, in the recent
follow-up work, Asadi and Huber (2007) provide a method
to learn an abstract hierarchical representation that uses
state aggregation and options. Since they are interested
in skill transfer through a series of related problems that
can differ in their cost structure, they introduce a heuristic
to discover subgoals based on bottleneck states. They
learn options for achieving the discovered subgoals and
introduce a partitioning that respects the learned options
(in the clusters typically there are many states). The
success of the approach relies critically on the existence of
meaningful bottleneck states. This leads to a granularity
issue: identifying the bottleneck states requires computing
a statistic for each state visited, meaning bottlenecks
will not be pronounced if resolution is increased in
narrow pathways. Nevertheless, the approach has been
successfully tested in a non-trivial domain of 20,000 states.
Hauskrecht, Meuleau, Kaelbling, Dean, and Boutilier
(1998) introduce a method that also uses options, but the
abstract states correspond to boundary states of regions.
The regions are assumed to be given a priori. The idea is
similar to using bottleneck states. In contrast to that work,
we do not assume any prior knowledge, but construct
the abstractions completely autonomously. Further, we
deal with undiscounted SSPs, while Hauskrecht et al.
(1998) dealt with discounted MDPs (but this difference is
probably not crucial).

7

Discussion and Future Directions

In the approach presented, options serve as closed-loop
abstract actions. Another way to use an abstract solution
would be to use the abstract value function to guide local
search initiated from the current state. These ideas has
proven successful in pattern-database research where
the cost of an optimal solution of an abstract problem
is used as a powerful heuristic for the original problem.
Such a procedure has the potential to improve solution
quality, while keeping low the cost of the planning steps
interleaved with execution. Another idea is to use the abstraction to select the amount of such local search (i.e., the
depth of the rollouts); these ideas has proven successful in
deterministic environments (Bulitko, Björnsson, Luštrek,
Schaeffer, & Sigmundarson, 2007; Bulitko, Luštrek,
Schaeffer, Björnsson, & Sigmundarson, 2008).
Presently, our abstractions are deterministic. This suggests
two avenues for future work. First, applying advanced
heuristic search methods to such abstractions may lead to
performance gains. Second, in highly stochastic domains,
the abstraction’s determinism may lead to a poor quality
of solution, as the cost of ensuring arrival at an abstract

Solution time for different game maps

Solution suboptimality for different game maps
1.05

IPS
Abstraction

1.04
Suboptimality

Solution time (s)

10

1

0.1

1.03
1.02
1.01

0.01

1
WC 1

WC 2

BG 1

BG 2

BG 3

WC 1

Map

WC 2

BG 1

BG 2

BG 3

Map

Figure 6: Solution times (left) and suboptimalities (right) for several game maps.
state with certainty (or very high probability) can lead to
very conservative and costly paths. Thus, it would be of
interest to investigate stochastic abstractions. One idea
is to modify the way abstract actions are defined: When
planning to connect to abstract states after a solution of the
local SSP is found, with a little extra work we can compute
the probabilities of reaching various neighboring abstract
states under the policy found when the policy leaves the
region of interest.
Yet another avenue for future work would be to move
from a state-based problem formulation to a feature-based
one, assuming that the features describe the states. The
challenge is to design an algorithm that can construct an
abstraction without enumerating all the states, as ours
currently does. Although this paper has not attempted
to address this problem, we believe that the approach
proposed here (i.e., incremental clustering and defining
options by solving local planning problems) is applicable.
Finally, although the present paper dealt only with undiscounted, stochastic shortest path problems, the approach
can be extended to work for discounted problems. This
holds because a discounted problem can always be viewed
as an undiscounted stochastic shortest path problem where
every time step a transition is made to some terminal state
with probability 1 − γ, where 0 < γ < 1 is the discount
factor.

8

Conclusions

This paper has explored ways to speed up planning in
SSP problems via goal-independent state and action
abstraction. We strengthen existing theoretical results, then
provide an algorithm for building abstraction hierarchies
automatically. Finally, we empirically demonstrate the
advantages of this approach by showing that it works
effectively on SSPs of varying size and difficulty.

Acknowledgements
We gratefully acknowledge the insightful comments by the
reviewers. This research was funded in part by the National
Science and Engineering Research Council (NSERC), iCore and
the Alberta Ingenuity Fund.



A Bayesian belief network models a joint
distribution with an directed acyclic graph
representing dependencies among variables
and network parameters characterizing conditional distributions. The parameters are
viewed as random variables to quantify uncertainty about their values. Belief nets are
used to compute responses to queries; i.e.,
conditional probabilities of interest. A query
is a function of the parameters, hence a random variable. Van Allen et al. (2001, 2008)
showed how to quantify uncertainty about a
query via a delta method approximation of
its variance. We develop more accurate approximations for both query mean and variance. The key idea is to extend the query
mean approximation to a “doubled network”
involving two independent replicates. Our
method assumes complete data and can be
applied to discrete, continuous, and hybrid
networks (provided discrete variables have
only discrete parents). We analyze several
improvements, and provide empirical studies
to demonstrate their effectiveness.

1

INTRODUCTION

Consider a simple example. Suppose A represents
presence/absence of a medical condition while B and
Y are test results. Variables B and Y are conditionally independent given A, with A and B binary
and Y continuous. The conditional independence assumption is represented by the directed acyclic graph
structure in Figure 1(a). Let θa = P (A = a),
θb|a = P (B = b | A = a), and let p(y | βa , σa ) be the
conditional density of Y given A = a, assumed normal with mean βa and variance σa2 . We want to estimate the probability that condition A is present given

specified results from the two tests B and Y . Let Θ
represent all of the parameters. If Θ were known, we
would use the formula:
θa θb|a p(y | βa , σa )
.
a1 θa1 θb|a1 p(y | βa1 , σa1 )

q(Θ) = qa|b,y (Θ) = P

(1)

In the Bayesian paradigm, uncertainty about Θ is
quantified by modeling parameters as random variables. It follows that query probabilities such as (1)
are also random. A query response is usually estimated
by approximating its posterior mean. This approximation is similar to expression (1), but with θa and θb|a
replaced by their posterior means and with the normal
densities replaced by Student’s t densities.
One may want more than just a point estimate. Van
Allen et al. (2001, 2008) showed (for discrete networks)
how one can approximate the variance and posterior
distribution of a query. Their variance derivation employs the delta method; i.e., a first-order Taylor series
expansion of the function q(Θ) about the posterior
mean of Θ. They provide asymptotic theory and empirical experiments supporting this approach. They
also showed how these approximations can be used
to construct a Bayesian credible interval (error bars)
for q(Θ). Guo and Greiner (2005) applied this delta
method approximation as part of a mean squared error (i.e., squared bias + variance) measure designed
to estimate the quality of different belief net structures when seeking a best classifier. Lee et al. (2006)
provide a technique for combining independent belief
net classifiers that involves weighting their respective
mean probability values by their inverse variances, and
they show that this works well in practice.
We propose new approximations for the mean and
variance based on a simple trick. Suppose (A1 , B1 , Y1 )
and (A2 , B2 , Y2 ) are replicates of the network variables,
conditionally independent given Θ. We represent the
paired replicates as nodes in a “doubled network” with
the same structure; see Figure 1. The squared query
q(Θ)2 can be expressed as a query in this doubled net-

UAI 2009

θb1 |a1
θb1 |a2

θb2 |a1
θb2 |a2

HOOPER ET AL.
#$
θa1 θa2
A
!"
! #
!
#
!
#
!
#
!
#
θ 1 1θ 1 1
"
!
$
#
#$ b |a b |a
#$
θb1 |a1 θb1 |a2
(βa1 , σa21 )
Y
B
2
!" (βa2 , σa2 ) !" θb1 |a2 θb1 |a1
θb1 |a2 θb1 |a2

233

θa1 θa1

θb1 |a1 θb2 |a1
θb1 |a1 θb2 |a2
θb1 |a2 θb2 |a1
θb1 |a2 θb2 |a2

θb2 |a1 θb1 |a1
θb2 |a1 θb1 |a2
θb2 |a2 θb1 |a1
θb2 |a2 θb1 |a2

θa1 θa2

θb2 |a1 θb2 |a1
θb2 |a1 θb2 |a2
θb2 |a2 θb2 |a1
θb2 |a2 θb2 |a2

θa2 θa1

θa2 θa2

'(

A1 , A2
%%&
'
%
'
%
'
%
'
%
'(
'(
(βa1 , σa21 )(βa1 , σa21 ) '
&
%
(
'
(βa1 , σa21 )(βa2 , σa22 )
Y1 , Y2
B1 , B2
(βa2 , σ 22 )(βa1 , σ 21 )
%& (β 2 , σa2 )(β 2 , σa2 ) %&
2
2
a
a
a
a

Figure 1: (a) A simple Bayesian network. (b) The corresponding doubled network.
Figure 1: A simple Bayesian net.

work:
P (A1 = A2 = a | B1 = B2 = b, Y1 = Y2 = y, Θ) .
The method used to approximate the mean of q(Θ)
can be extended to the doubled network to approximate the mean of q(Θ)2 and hence to approximate
the variance. Unlike the delta method, our approach
does not rely on approximate local linearity of q(Θ).
It does involve the addition of two incomplete observations to the data set when calculating the posterior
mean of q(Θ)2 . In some situations, this addition results in under-estimation of the desired variance. This
deficiency is largely eliminated by a simple adjustment.
A similar adjustment substantially improves the usual
query mean approximation.
Section 2 reviews pertinent models and methods for
belief networks. The network doubling technique is
described in Section 3 for discrete, continuous, and hybrid networks. Proposed adjustments and numerical
results are presented in Sections 4 and 5 for discrete
networks. Corresponding work for continuous and hybrid networks is ongoing. Computational issues are
discussed in Section 6. Contributions and plans for
further work are summarized in Section 7.

2
2.1

BACKGROUND
NETWORK VARIABLES

We assume network structure is known. Let B denote
a discrete network variable taking values b ∈ DomB .
Let Y denote a continuous network variable taking values y on the real line. Vectors of variables are denoted
by boldface: A for discrete and X for continuous. Let
Θ be a random vector comprising all unknown network
parameters; i.e., Θ determines all conditional distributions of variables given their parents.
We assume that discrete variables have only discrete
parents. Suppose pa(B) = A; i.e., the parents of B are
the variables comprising the vector A. The conditional
probability that B = b given A = a is denoted
θb|a = θB=b|A=a = P {B = b | A = a, Θ}.

1

Variables associated with values will be clear from context. We employ similar abbreviations for other parameters and hyperparameters. The θb|a parameters
are often presented in conditional probability tables
(CPtables) with rows indexed by a and columns by
b; e.g., see Figure 1. Note that we use superscripts
b1 , b2 to list the distinct values in DomB . We use subscripts b1 , b2 to denote arbitrary values in DomB , often
related to replicated variables B1 , B2 .
Continuous variables can have both discrete and continuous parents. Suppose pa(Y ) = hA, Xi with X =
hX1 , . . . , Xd i. The conditional distribution of Y is

(Y | A = a, X = x, Θ) ∼ N (1, xT )β a , σa2 ; (2)
i.e., normally distributed, conditional mean related to
x by a linear regression model with coefficients depending on a. Here xT is the transpose of the ddimensional column vector x while β a is an (d + 1)dimensional column vector of regression coefficients
(the first entry is the constant term).
2.2

PRIOR AND POSTERIOR

The network parameters represented by Θ consist of
CPtable parameters θb|a , regression coefficient vectors
β a , and variances σa2 . We assume the prior distribution for Θ has the following form; e.g., see Gelman et
al. (2003).
• CPtable rows follow Dirichlet distributions:
θB|a := hθb|a , b ∈ DomB i ∼ Dir(αB|a ),
where αB|a := hαb|a , b ∈ DomB i .
• The regression coefficients and variance together
have a normal-(inverse chi-square) distribution:

(β a | σa2 ) ∼ Nd+1 µa , σa2 (νa Ψa )−1 ,
σa−2

∼ (τa2 νa )−1 χ2νa .

I.e., dropping subscripts for a moment, β conditioned on σ 2 is multivariate normal with mean

234

HOOPER ET AL.
vector µ and covariance matrix σ 2 (νΨ)−1 ; and
ντ 2 /σ 2 has a χ2ν distribution with ν > 0 (not necessarily an integer). Note that τ 2 /σ 2 has mean 1
and variance 2/ν.

• Parameters are assumed to be statistically independent except where joint distributions are specified above. In particular, we assume global independence: the parameters determining the conditional distribution of one variable given its parents
are independent of all other parameters.
The prior is conjugate: given a data set D consisting
of n independent replicates of complete tuples of network variables, the prior hyperparameter values are
updated as follows. Let nab and na be the number of
tuples in D with (A, B) = (a, b) and A = a, respectively. Let (xi , yi ) be the observations of (X, Y ) for
the na tuples with A = a. Let X a be the na × (d + 1)
matrix with rows (1, xTi ). Let y a be the column vector
with entries yi . In the five equations below, the prior
hyperparameter values appear on the right-hand side
and are identified with tildes (e.g., α̃).
αb|a = α̃b|a + nab
νa = ν̃a + na
νa Ψa = ν̃a Ψ̃a + X Ta X a
νa Ψa µa = ν̃a Ψ̃a µ̃a + X Ta y a
i
h
 2

νa τa + µTa Ψa µa = ν̃a τ̃a2 + µ̃Ta Ψ̃a µ̃a + y Ta y a
P
P
The values
a,b αb|a and
a νa are called the effective sample sizes for variables B and Y , respectively.
Our adjustments developed in Section 4 are motivated
by large m asymptotics, where m is proportional to
the effective sample size for each of the variables; i.e.,
0
αb|a = mαb|a
and νa = mνa0
0
with (αb|a
, νa0 , Ψa , µa , τa2 ) fixed.

(3)

Large m asymptotics are similar to but not the same
as large n asymptotics. As the sample size n increases,
the posterior mean E{θb|a | D} = αb|a /α·|a varies and
converges to some value. (Here and elsewhere,
the dot
P
subscript indicates summation: α·|a = b αb|a .) Under assumption (3), the posterior mean remains fixed
as m varies.
2.3

APPROXIMATING A QUERY MEAN

Consider a query involving outcomes of hypothesis
variables H given values for evidence variables E.
It is convenient to represent the query in terms of a
function w(H). E.g., suppose H = A, E = (B, Y ),
e = (b, y), and
q(Θ)

= P (A = a | B = b, Y = y, Θ)
= E{w(A) | B = b, Y = y, Θ} ,

UAI 2009

where w(A) = 1 for A = a and w(A) = 0 otherwise.
For discrete networks, query responses q(Θ) are usually estimated by q(Θ̂), where Θ̂ := E{Θ | D} is the
posterior mean of the parameter vector. This plugin estimate usually differs slightly from the posterior query mean E{q(Θ) | D}. Cooper and Herskovits
(1992, expression 19) showed that the plug-in estimate
equals E{q(Θ) | D, e}; i.e., the posterior query mean
given an augmented data set consisting of D and an
additional partial observation of the evidence variables
E = e. Cooper and Herskovits (1991) derived a formula for E{q(Θ) | D, e} that is valid for discrete, continuous, and hybrid networks. This formula provides a
useful approximation of the less tractable E{q(Θ | D}.
The plug-in estimate is a special case of this formula
for discrete networks. The formula is important for
our network doubling technique, so is reviewed here.
In the integral expression below, Z represents all variables not included in (H, E); dh and dz refer to product measures allowing both integration for continuous
variables (Lebesgue measure) and summation for discrete variables (counting measure). Some manipulation yields
E{q(Θ) | D, e} = E{w(H) | E = e, D}
= E [ E{w(H) | E = e, Θ} | D ]
(4)
RR
R
w(h) p(h, e, z | θ)p(θ | D)dθdhdz
RRR
.
=
p(h, e, z | θ)p(θ | D)dθdhdz
Now p(h, e, z | θ) factors as a product of conditional
probabilities and densities, one for each variable in
the network.
Due to global independence, the inteR
gral p(h, e, z | θ)p(θ | D)dθ factors into a product of
integrals, one for each variable. The result is a product
of probabilities and densities described in Section 2.4
below. It follows that E{q(Θ | D, e} can be calculated
in essentially the same manner as the function q(Θ),
but with two modifications.
• For discrete variables, parameters θb|a are replaced by their posterior means. If all network
variables are discrete, then we have the plug-in
estimate:
E{q(Θ) | D, e} = q(E{Θ | D}).

(5)

• For continuous variables, the normal densities are
replaced by the St1 (η, ω 2 , ν) densities described
below. Note that this is not the same as replacing
β and σ 2 parameters with their posterior means.
2.4

PREDICTIVE DISTRIBUTIONS

The predictive distribution of the network variables is
obtained by integrating out their joint conditional dis-

UAI 2009

HOOPER ET AL.

tribution given Θ with respect to the posterior distribution of Θ. Global independence allows this integration to be carried out separately for each conditional
distribution of a variable given its parents.
The predictive distribution for a discrete variable B is
πb|a := P (B = b | A = a, D) = E{θb|a | D} =

αb|a
.
α·|a

The predictive distribution for a continuous variable is
a location-scale version of the Student’s t distribution
with ν degrees of freedom. We need the multivariate
form of this distribution in Section 3, so we define it
here. Suppose
T = η + U −1/2 (Z − η),
where Z and U are independent, Z ∼ Np (η, Ω), U ∼
(1/ν)χ2ν , and Ω is a nonsingular covariance matrix.
It follows that T has the following density function
(Johnson and Kotz, 1972, page 134):

(νπ)p/2 |Ω|1/2

Γ[(ν + p)/2] / Γ(ν/2)

(ν+p)/2 .
1 + ν1 (t − η)T Ω−1 (t − η)

We refer to this as the Stp (η, Ω, ν) distribution. For
p = 1, we write St1 (η, ω 2 , ν). Note that St1 (0, 1, ν) is
Student’s t distribution.
We claim that (Y | A = a, X = x, D) ∼ St1 (η, ω 2 , ν)
with ν = νa , η = (1, xT )µa , and

ω 2 = τa2 (1, xT )(νa Ψa )−1 (1, xT )T + 1 . (6)
To see this, let us suppress subscripts for a moment.
Let Z1 ∼ N (0, 1) be independent of (β, σ). Put Z 2 :=
σ −1 (β − µ) ∼ Nm+1 0, (νΨ)−1 . We then have
(Y | a, x, D) ∼ (1, xT )β + σZ1

∼ η + (σ/τ )τ (1, xT )Z 2 + Z1 .

3

NETWORK DOUBLING

In Section 2.3 we noted that E{q(Θ) | D} is usually
approximated by the more tractable E{q(Θ) | D, e}.
Here we propose approximating Var{q(Θ) | D} by
Var{q(Θ) | D, e, e}; i.e., the posterior variance given
D and additional replicates E 1 and E 2 of the vector
of evidence variables, both having the same value e.
We develop a formula for this latter variance by imagining a doubled network; see Figure 1(b). These mean
and variance approximations can be improved by adjustments described in Section 4.
Consider two replicated tuples of network variables,
conditionally independent and identically distributed
given Θ. Use these to replace each variable in the

235

original network by a pair of variables; e.g., B is replaced by B ∗ := (B1 , B2 ) with possible values b∗ =
(b1 , b2 ) ∈ DomB ∗ = DomB × DomB . If pa(B) = A,
then pa(B ∗ ) = A∗ := (A1 , A2 ). Conditional distributions of doubled variables given parents are obtained
by multiplying probabilities or densities for single variables.
For discrete variables, we have
P (B ∗ = b∗ | A∗ = a∗ , Θ) = θb1 |a1 θb2 |a2 .
E.g., if A = A, DomA = {a1 , a2 }, and DomB =
{b1 , b2 }, then the CPtable for B ∗ is the 4 × 4 array
shown in Figure 1(b). More generally, if a CPtable
in the original network involves dr × dc parameters,
then corresponding table in the doubled network has
d2r × d2c entries. Note that CPtable rows in the doubled network are not independent (local independence
does not hold) and do not have Dirichlet distributions.
Fortunately, these properties are not needed for the
factorization described following (4).
For continuous variables, the conditional density of
Y ∗ = (Y1 , Y2 ) given (A∗ = a∗ , X ∗ = x∗ , Θ) is the
product of the densities for two normal distributions
of the form (2) with subscript i = 1, 2 on a and x.
Put H ∗ = (H 1 , H 2 ), w∗ (H ∗ ) = w(H 1 )w(H 2 ), E ∗ =
(E 1 , E 2 ), and e∗ = (e, e). Some manipulation using
conditional independence yields
q(Θ)2 = E{w∗ (H ∗ ) | E ∗ = e∗ , Θ} ,
q(Θ) = E{w(H1 ) | E ∗ = e∗ , Θ} .
We thus have
Var{q(Θ) | D, e, e}
(7)
2
2
= E{q(Θ) | D, e, e} − [E{q(Θ) | D, e, e}]
= E{w∗ (H ∗ ) | e∗ , D} − [E{w(H1 ) | e∗ , D}]2 .
The doubled network satisfies global independence assumptions, so we can follow the approach of Section
2.3 to evaluate the two expected values in (7). To
accomplish this task, we need bivariate predictive distributions for the doubled network.
For discrete variables, the calculation follows from the
means and covariances of a Dirichlet distribution. Let
δb1 b2 be the Kronecker delta function. We have
πb∗∗ |a∗

:= P {B ∗ = b∗ | A∗ = a∗ , D}
= E{θb1 |a1 θb2 |a2 | D}
=

πb1 |a1 πb2 |a2 + δa1 a2

πb1 |a1 (δb1 b2 − πb2 |a1 )
.
α·|a1 + 1

If all network variables are discrete, then we have an
identity corresponding to (5). Let Θ∗ be the vector

236

HOOPER ET AL.

of all CPtable entries in the doubled network; e.g.,
θb1 |a1 θb2 |a2 appears in row a∗ and column b∗ for the
CPtable of B ∗ . We then have
E{q ∗ (Θ∗ ) | D, e, e} = q ∗ (E{Θ∗ | D})

(8)

with the entries in E{Θ∗ | D} given by the πb∗∗ |a∗ values above. The two expected values in the variance approximation (7) are calculated by applying (8) twice:
with q ∗ (Θ∗ ) = q(Θ)2 and with q ∗ (Θ∗ ) = q(Θ).
For continuous variables, we need the density for
{(Y1 , Y2 ) | a1 , a2 , x1 , x2 , D}. There are two cases to
consider.
• If a1 6= a2 , then the parameters (β a1 , σa2 1 ) and
(β a2 , σa2 2 ) are mutually independent. Consequently, the joint distribution factors as a product
of two St1 (η, ω 2 , ν) densities; see expression (6).
• If a1 = a2 ( = a, say), then the joint distribution
is St2 (η, Ω, ν) with ν = νa , η = X 2 µa , and
o
n
Ω = τa2 X 2 (νa Ψa )−1 X T2 + I 2 ,
where X 2 is the 2 × (1 + d) matrix whose rows
are each (1, xTi ) and I 2 is the 2 × 2 identity matrix. The derivation is similar to that following
(6). Note that (β a , σa2 ) is the same for both Y1
and Y2 in this case.

4

UAI 2009

Table 1: Summary of approximations for µq and σqq .
Means
q̂1 = E{q(Θ) | D, e}
q̂2 = E{q(Θ) | D, e, e}
q̂3 = q̂1 − (q̂2 − q̂1 )
q̂4 = q̂1 − σ̂qr /µr

√
verify that the distribution of m(Q − µq , R − µr )
converges to bivariate normal by modifying the proof
of Theorem 2 in Van Allen et al. (2008). Asymptotic
normality implies that
2
σqqrr − 2σqr
− σqq σrr → 0 at rate m−5/2

T

v̂1 = g Cg ,

E{R − µr | Q} ≈ (Q − µq )

We use approximations for higher moments motivated
by large m asymptotics; i.e., a sequence of posterior
distributions of the form (3) with m → ∞. One may

2σqr σqq (1 − 2µq )
.
µq (1 − µq ) + σqq

(11)

Switching the roles of Q and R gives
σqrr ≈

(9)

For conciseness we suppress D in our expressions; i.e.,
we implicitly assume that expectations are conditioned
on D. Put Q = q(Θ) = P (H = h | E = e, Θ) and R =
P (E = e | Θ). Note that R is an unconditional query,
with hypothesis E = e and no evidence variables. Let
µq , µr , σqq , σrr , and σqr denote the means, variances,
and covariance for (Q, R). We extend this notation to
higher moments; e.g., σqqr = E{(Q − µq )2 (R − µr )}.

σqr
σqq

and hence σqqr ≈ σqqq σqr /σqq . Now σqqq = 0 for normal distributions; however, Van Allen et al. (2008) argue that query distributions are usually better approximated by beta distributions. Substituting the third
moment of a beta distribution for σqqq , we obtain
σqqr ≈

where g is the gradient vector of q(Θ) and C is the
covariance matrix of Θ, both evaluated at E{Θ | D}.
The second variance approximation v̂2 is the doubling
method introduced in Section 3. The simple adjustments (q̂3 , v̂3 ) and more complex adjustments (q̂4 , v̂4 )
are developed in this section.

(10)

while σqrr and σqqr converge to zero at rate m−2 . We
considered approximating σqqr and σqrr by zero but
found that more accurate approximations give better
results. Asymptotic bivariate normality suggests

ADJUSTMENTS

We now narrow our focus to discrete networks and
consider the four mean and variance approximations
in Table 1. The delta method approximation is

Variances
v̂1 = delta method (9)
v̂2 = Var{q(Θ) | D, e, e}
v̂3 = expression (18)
v̂4 = expression (17)

2σqr σrr (1 − 2µr )
.
µr (1 − µr ) + σrr

(12)

Before proceeding, we observe that µr and σrr can
be calculated exactly because R can be expressed as
a sum of products of independent terms. For queries
with this property, all approximations (except v̂1 ) are
exact; i.e., additional observations of evidence variables have no effect on the posterior mean or variance. E.g., given a discrete network
with structure
P
E → B → H, we have q(Θ) = b θh|b θb|e . Since parameters in each product are independent, it follows
that q̂2 = q̂1 = µq and v̂2 = σqq .
We begin with adjustments to improve q̂1 . Bayes rule
and some manipulation yields
q̂1

=

q̂2

=

E(QR)
σqr
= µq +
(13)
E(R)
µr
E(QR2 )
2µr σqr + σqrr
= µq +
.
2
E(R )
µ2r + σrr

We approximate σqqrr using (10), σqqr by (11), σqr by
(14), µq by q̂4 , and replace σqq by v̂4 . Rearranging
terms yields the identity: v̂4 =
2
(µ2r + σrr ){v̂2 + (q̂2 − q̂4 )2 } − 2σ̂qr
. (17)
µ2r + σrr + 4µr σ̂qr (1 − 2q̂4 )/{q̂4 (1 − q̂4 ) + v̂4 }

Notice that v̂4 appears in the denominator of (17). We
initially set this value to v̂2 , then iteratively solve for
v̂4 . The values converge in a few iterations.
We observe that replacing σrr by zero has negligible
effect on (17) as m → ∞. By also replacing q̂4 by q̂3
and σ̂qr /µr by q̂2 − q̂1 , we obtain a simpler identity:
v̂3 =

v̂2 + 2(q̂2 − q̂1 )2
. (18)
1 + 4(q̂2 − q̂1 )(1 − 2q̂3 )/{q̂3 (1 − q̂3 ) + v̂3 }

We again initialize by v̂2 , then iteratively solve for v̂3 .
The approximations q̂3 and v̂3 may be preferred to q̂4
and v̂4 since µr and σrr are not required.
Rates of convergence are summarized in Proposition 1
below. The proof of this result follows easily from Van
Allen et al. (2008) and the development above.
Proposition 1. Assume a discrete network satisfying
(3) and let m → ∞. The query mean µq remains constant while the variance σqq approaches zero at rate
m−1 . The mean approximations have errors q̂j − µq
approaching zero at rate m−1 for j = 1 and 2, and at
the faster rate m−3/2 for j = 3 and 4. All four variance approximations have relative errors (v̂j −σqq )/σqq
approaching zero at rate m−1 .

-0.1
-0.3

Scaled Error
q3

q4

q1

q4

0.2
-0.6

-0.2

Scaled Error

0.0

Scaled Error

q3

(b) Diamond & m = 20

0.5

(a) NB & m = 20

(15)

µ2r σqq + 2µr σqqr + σqqrr
E{(Q − µq )2 R2 }
=
. (16)
E(R2 )
µ2r + σrr

0.0

q1

In trying to improve v̂2 , we began with the idea of
replacing q̂2 with µq :

This suggests an approximation v̂2 +4(q̂2 − q̂1 )2 , which
does help to reduce the under-estimation problem;
however, a greater improvement is obtained by further
analysis of (15):

-0.5

The formula for q̂4 in Table 1 follows from (13). Now
recall that, under condition (3), µr remains fixed while
σrr → 0 as m → ∞. It follows that setting σrr = 0
in (14) will have negligible effect for large m. We thus
obtain σ̂qr ≈ (q̂2 − q̂1 )µr , leading to the simpler q̂3
approximation.

E{(Q − µq )2 | e, e} = v̂2 + (q̂2 − µq )2 .

-0.4
-0.8

(14)

-0.5

(q̂2 − q̂1 )µr (µ2r + σrr ){µr (1 − µr ) + σrr }
.
2
µ3r (1 − µr ) + µr (1 − 2µr )σrr − σrr

Scaled Error

If µr = 1, then set σ̂qr = 0. Otherwise, substituting
(12) for σqrr and solving yields σ̂qr =

237

0.1

HOOPER ET AL.

-1.0

UAI 2009

q1

q3

q4

(c) NB & m = 500

q1

q3

q4

(d) Diamond & m = 500

Figure 2: Boxplots of scaled errors m(q̂j − q̂0 ) for j ∈
{1, 3, 4}, m ∈ {20, 500}, and network structures NB
and Diamond. Each boxplot shows variation in errors
for a set of distinct queries, 22 +24 = 20 for NB and 108
for Diamond. Errors for q̂3 and q̂4 are nearly identical.
Errors for q̂1 are often much larger. Results for q̂2 are
not plotted since q̂2 − q̂0 ≈ 2(q̂1 − q̂0 ).

5

NUMERICAL RESULTS

We evaluated accuracy of approximations q̂j and v̂j using highly accurate empirical estimates of µq and σqq .
These estimates q̂0 and v̂0 were obtained by simulating k = 106 replicates of Θ from the posterior distribution, evaluating q(Θ) for each replicate, then calculating the sample mean and sample variance. Computational
costs
preclude using empirical variance estipaper/R
figures
mates/Users/peterhooper/Documents/Research/Doubling
in practice. When m is large, asymptotic normality of q(Θ) implies that the distribution of v̂0 /σqq
is approximately (1/k)χ2k with variance 2/k.p Consequently v̂0 /σqq varies over the interval 1 ± 2 2/k for
roughly 95% of samples. Since our variance approximations have relative errors of order m−1 , it follows
that k should be of order at least m2 for v̂0 to have
substantially smaller relative error. When comparing
approximate relative errors (v̂j − v̂0 )/v̂0 with k = 106 ,
variation in v̂0 has a noticeable effect for m = 500; see
Figure 3(f).
Our examples differ with respect to network structure, posterior distribution, and query. All variables
are binary. All posterior distributions satisfy BDe
constraints (e.g., see Hooper 2008), so all variables
have the same effective sample size m. Hyperparameters are thus determined by m and the poste-

238

HOOPER ET AL.
3

E = all children of H, e varies over all combinations (22 for NB-2, 24 for NB-4).

0

1

&
• Diamond network with 4 variables .
& . , all 108
distinct queries with one hypothesis variable.

-2

-1

Scaled Relative Error

2

10
5
0
-5

Scaled Relative Error

-3

-10

v1

v2

v3

v4

v1

v3

v4

0
-2
-6

-4

Scaled Relative Error

10
0
-10
-20
-30

Scaled Relative Error

v2

(b) Diamond & m = 20

2

(a) NB & m = 20

v1

v2

v3

v4

v1

v2

v3

v4

(d) Diamond & m = 100

COMPUTATIONAL ISSUES

2
0
-2
-6

-4

Scaled Relative Error

0
-10
-20

Approximations for means are compared in Figure 2
and for variances in Figure 3. The errors and relative errors are multiplied by m in these figures to facilitate comparisons across a range of effective sample sizes. Boxplots for m = 20, 100, and 500 are
shown. Plots for other values of m are similar. By
Proposition 1, relative errors (v̂j − σqq )/σqq should approach zero at rates cj /m, where cj depends implicitly on the network, E(Θ | D), and the query. This
theory is supported by Figure 3 and additional plots
(not shown) comparing the four methods for individual queries. Our results suggest that c3 ≈ c4 while c1
and c2 tend to be further from zero. Relative errors
can be interpreted in terms of variances or standard
deviations. If (v̂j − σqq )/σqq = cj /m, then we have
p
r
v̂j
cj
cj
cj
v̂j
=1+
and √
= 1+
≈1+
.
σqq
m
σqq
m
2m

6

-40

Scaled Relative Error

10

20

(c) NB & m = 100

-30

UAI 2009

v1

v2

v3

(e) NB & m = 500

v4

v1

v2

v3

v4

(f) Diamond & m = 500

Figure 3: Boxplots of relative errors m(v̂j − v̂0 )/v̂0 for
j ∈ {1, 2, 3, 4}, m ∈ {20, 100, 500}, and network structures NB and Diamond. Each boxplot shows variation
among values for a set of distinct queries, 20 for NB
and 108 for Diamond. We observe that: relative errors
tend to be larger for NB compared with Diamond; v̂3
and v̂4 tend to over-estimate σqq for NB and are more
accurate than v̂2 ; the three methods v̂2 , v̂3 ,and v̂4 have
similar accuracy for Diamond; v̂1 is less accurate than
the other methods. The four methods appear to have
paper/R figures
similar
accuracy in (f), but these plots are mislead/Users/peterhooper/Documents/Research/Doubling
ing. Many of the Diamond queries have the property
described following (12), where v̂2 = v̂3 = v̂4 = σqq .
We would therefore expect the Diamond results for
m = 500 to be similar to those for m = 100. It appears
that the variation among relative errors for m = 500
is due in large part to variation in v̂0 .
rior means E{Θ | D}. Our examples are from three
small networks, each with one vector E{Θ | D} and
m ∈ {20, 50, 100, 200, 500}:
• Two naı̈ve Bayes networks (NB-2 and NB-4 with
2 and 4 features plus the root variable); H = root,

Inference in Bayesian networks is in general an NPcomplete problem (Cooper, 1990). For instance, the
complexity of the Variable Elimination (VE) Algorithm is O(dr ), where d is an upper bound on the
number of values that a variable can take and r is
an upper bound on the size of a factor generated by
the VE Algorithm (Koller and Friedman, 2008). Network doubling uses essentially the same technique to
calculate a variance as that used to evaluate a query,
resulting in corresponding computational complexity.
The doubled CPtables are larger (squared number of
rows and columns), so the computational complexity
of VE is increased to O(d2r ). The delta method retains O(dr ) complexity (Van Allen et al., 2008), so is
typically faster in large networks; see Table 2 below.
In some cases, we can exploit the structure of the network or query to achieve a polynomial time inference
algorithm. For poly-tree Bayesian networks (i.e. networks with at most one undirected path between any
pair of nodes), there exist inference algorithms with
linear time complexity. Reduced complexity is also
available when the query can be expressed in terms of
probabilities of hypothesis and evidence nodes conditioned on their Markov blanket; i.e., the parents, the
children and the parents of the children. Once again,
we have a polynomial time inference algorithm. These
techniques translate directly to efficient algorithms for
computing all of the variance approximations in Table 1.

UAI 2009

HOOPER ET AL.

Table 2: Timing results in seconds.
Network
NB-4
Diamond
Alarm

# Queries
100,000
108,000
100

Delta
37.837
50.052
11.390

Doubling
3.969
12.660
282.342

239

Acknowledgements
We are grateful for helpful comments from the anonymous reviewers. We acknowledge support provided by
NSERC, iCORE, and the Alberta Ingenuity Centre for
Machine Learning.


A Bayesian net (BN) is more than a succinct

way to encode a probabilistic distribution; it
also corresponds to a function used to answer
queries. A BN can therefore be evaluated
by the accuracy of the answers it returns.
Many algorithms for learning BNs, however,
attempt to optimize another criterion (usu­
ally likelihood, possibly augmented with a
regularizing term) , which is independent of
the distribution of queries that are posed.
This paper takes the "performance criteria"
seriously, and considers the challenge of com­
puting the BN whose performance - read
"accuracy over the distribution of queries"
- is optimal. We show that many aspects
of this learning task are more difficult than
the corresponding subtasks in the standard
model.
INTRODUCTION

1

Many tasks require answering questions; this model
applies, for example, to both expert systems that iden­
tify the underlying fault from a given set of symp­
toms, and control systems that propose actions on the
basis of sensor readings. When there is not enough
information to answer a question with certainty, the
answering system might instead return a probabil­
ity value as its answer. Many systems that answer
such probabilistic queries represent the world using a
"Bayesian net" (BN), which succinctly encodes a dis­
tribution over a set of variables. Often the underlying
distribution, which should be used to map questions to
appropriate responses, is not known a priori. In such
cases, if we have access to training examples, we can
try to learn the model.
There are currently many algorithms for learning
BNs [Hec95, Bun96] . Each such learning algorithm
tries to determine which BN is optimal, usually based
•

Also: NEC Research Institute, Princeton, NJ

Dale Schuurmans*
Inst. for Research in Cognitive Science
University of Pennsylvania
Philadelphia, PA 19104-6228
daes@linc.cis.upenn.edu

on some measure such as log-likelihood (possibly aug­
mented with a "regularizing" term, leading to mea­
sures like MDL [LB94], and Bayesian Information Cri­
terion (BIC) [Sch78]) . However, these typical mea­
sures are independent of the queries that will be posed.
To understand the significance of this, note that we
may only care about certain queries (e.g., the prob­
ability of certain specific diseases given a set of ob­
served symptoms); and a BN with the best (say) log­
likelihood given the sample may not be the one which
produces the appropriate answers for the queries we
care about. This paper therefore argues that BN­
learning algorithms should consider the distribution
of queries, as well as the underlying distribution of
events, a.nd should therefore seek the BN with the best
performance over the query distribution, rather than
the one that appears closest to the underlying event
distribution.

To make this point more concrete, suppose we knew
that all queries will be of the form p( H I J, B) for some
assignments to these variables (e.g ., Hepatitis, given
the possible symptoms Jaundice=false and Blood test
=true). Given a set of examples, our learner has to de­
cide which BN (perhaps from some specified restricted
set) is best. Now imagine we had two candidates BNs
from this set: B1, which performs optimally on the
queries p( H I J, B), but does horribly on other queries
(e.g., incorrectly claims that J and E are conditionally
independent, has the completely wrong values for the
conditional probability of H to the treatment T ("Take
aspirin") , and so on); versus B2, which is slightly off
on the p( H I J, B) queries, but perfect on all other
queries. Here, most measures would prefer B2 over
B1, as they would penalize B1 for its errors on the
queries that will never occur!
Of course, if we re­
ally do only care about p( H I J, B ), this B2-over-B1
preference is wrong.
This assumes we have the correct distributions, of both
the real world events (e.g., quantities like p( H = 11 J ==
0, B = 1 ) = 0.42), and the queries that will be
posed (e.g., 48% of the queries will be of the form
"What is p(H
h/J = j, B =b)?"; 10% will be
"What is p( H = hI sl = VI, s4 = V4, s1 = V7 )?",
=

Learning Bayesian Nets that Perfonn Well

etc.). Another more subtle problem with the maximal­
likelihood-based measures arises when these distribu­
tions are not given expl icit ly, but must instead be es­
timated from examples. Here, we would, of course,
like to use the given examples to obtain good esti­
mates of the conditional probabilities P(HjJ, B). In
the general maximal-likelihood framework, however,
the examples would be used to fit all of the param­
eters within the entire BN, so we could conceivably
"waste" some examples or computational effort learn­
ing the value of irrel evant parameters. In general, it
seems better to focus the learner's resources on the
relevant queries (but see Section 4).
Our general challenge is to acquire a BN whose per­
formance is optimal, with respect to the distribution
of queries, and the underlying distribution of events.
Section 2 first lays out the framework by providing
the relevant definitions. Section 3 then addresses sev­
eral issues related to learning a BN whose accuracy
(by this measure) is optimal: presenting the computa­
tional/ sample complexities of first evaluating the qual­
ity of a given BN and then of finding the best BN of
a given structure. It then provides methods for hill­
climbing to a locally optimal BN. We will see that
these tasks are computationally difficult for general
classes of queries; Section 3 also presents a particular
class of queries for which these tasks are easy. Sec­
tion 4 then reflects on the general issue of how to best
use knowledge of the query distribution to improve
the efficiency of learni ng a good BN under our model.
Here we show situations where this information may
lead to ways of learning a BN (of a given structure)
th at are more sample-efficient than the standard ap­
proach. We first close this section by discussing how
our results compare with others.

Related Results: The framework closest to ours is

Friedman and Goldszmidt [FG96], as they also con­
sider finding th e BN that is best for some distribution
of queries, and also explain why the BN with (say)
maximal log-likelihood may not be the one with op­
timal performance on a specific task. In particular,
they note that evaluating a Bayesian net B, given a
set of training data D = { ci, ai , .. . , a�} � 1, under the
log-likelihood measure, amounts to using the formula
LL(B\D)
+

where B( x) is the probability that B assigns to the
event X· If all of the queries, however, ask for the
value of c given values of (a1, .. . an ) , then only the
first summation matters. This means that systems
that use LL(BID) to rank BNs could do poorly if the
contributions of second summation dominate those of
the first. The [FG96] paper, however, considers only
building BNs for classification, i.e., where every query
is of the specific form p( C = c I A1 =a1, . .. , An= an )
where C is the only "consequent" variable, and {Ai}

199

is the set of all other variables; their formulation also
implicitly assumes that all possible query-instances (of
complete tuples) will occur, and all are equally likely.
By contrast, we do not constrain the set of queries to
be of this single form, nor do we insist that all queries
occur equally often, nor that all variables be involved
in each query, Note in particular that we allow the
query's antecedents to not include the Markov blan­
ket of the consequent; we will see that this restriction
considerably simplifies the underlying computation.
Each of the queries we consider is of the form "p( X=
xI Y = y) = ?", where X, Y are subsets of the vari­
ables, and x, y are respective (ground) assignments
to these variables. As such, they resemble the stan­
dard class of "statistical queries", discussed by Kearns
and others [Kea93] in the context of noise-tolerant
learners.1 In that model, however, the learner is pos­
ing such queries to gather information about the un­
derlying distribution, and the learner's score depends
its accuracy with respect to some other specific set of
queries (here the same p( C = c I At = a1, . . . , An= an )
expression mentioned ab ove ). In our model, by con­
trast, the learner is observing which such queries are
posed by the "environment" , as it will be evaluated
based on its accuracy with respect to these queries.

Other researchers, including [FY96, Hi:if93], also com­
pute the sample complexity for learnin g good BNs.
They, however, deal with likelihood-based measures,
which (as we shall see) have some fundamental differ­
ences from our query-answering b ased model; hence,
our results are incomparable.
2

FRAMEWORK

As a quick synopsis: a Bayesian net is a directed
acyclic graph (V, E), whose nodes represent variables,
and whose arcs represent dependencies. Each node
also includes a conditional-probability-table that spec­
ifies how the node's values depends (stochastically) on
the values of its parents. (Readers unfamiliar with
these ideas are referred to [Pea88].)
In general, we assume there is a stationary un­
derlying distribution P over the N variables V =
{Vt, .. . , VN } · (I.e., p(Vt = V t , ... ,VN = VN ) � 0
and L,1, .. ,vN p( Vt = Vt, . . . , VN = VN) = 1). For
example, perhaps vl is the "disease" random variable,
whose value ranges over {healthy, cancer, flu, ... }; V2 is
"gender" E {male, female}, V3 is "body_temperature"
E [95 .. 105], etc. We will refer to this as the "underly­
ing distribution" or the "distribution over events".
A statistical query is a term of the form "p( X =x I Y =
y) = ?", where S, T C V are (possibly empty) subsets
of V, and x (resp,, y) is a legal assignment to the
elements of X (resp, Y). We let SQ be the set of all
10£ course, other groups have long been interested in
this idea; cf., the work in finding statistical answers from
database queries.

200

Greiner, Grove, and Schuunnans

possible legal statistical queries? and assume there is
a (stationary) distribution over SQ, written sq( X=
x; Y=y), where sq(X=x; Y=y) is the probability
that the query "What is the value of p( X = xI Y =
y )?" will be asked. We of course allow Y = {}; here
we are requesting the prior value of X, independent of
any conditioning. We also write sq( x; y) to refer to
the probability of the query "p( X= x I Y = y) == ?"
where the variable sets X and Y can be inferred from
the context.
While all of our results are based on such "ground"
statistical queries, we could also define sq( X; Y) to
refer to the probability that some query of the gen­
eral form "p( X = xI Y = y) = ?" will be asked,
for some assignments x, y; we could assume that all
assignments to these unspecified variables are equally
likely as queries. Finally, to simplify our notation, we
will often use a single variable, say q, to represent the
entire [X=x, Y=y] situation, and so will write sq( q)
to refer to sq( X=x; Y =y).
As a simple example, the claim sq( C ; A1,..., An )
1 states that our BN will only be used to find clas­
sifications C given the values of all of the variables
{Ai}· (Notice this is not asserting that p( C = c I A1 =
a1, ... , An = an ) = 1 for any set of assignments
{c, ai}.) If all variables are binary, this corresponds
to the claim that sq( C = c; A1 = a1, . . . , An = an) =
1/2n+l for each assignment. Alternatively, we can use
sq( C; A1, A2, A3) = 0.3, sq( C; A1, A2 ) 0.2, and
sq( D ; C = 1, A1
0, A3) = 0.25, and sq( D; C =
1, A1 = 1, A3 ) = 0.25, to state that 30% of the queries
involve seeking the conditional probability of C given
the observed values of the 3 attributes {A�, A2, A3};
20% involve seeking the probability of C given only the
2 attributes {A � , A2 }; 25% seek the probability of (the
different "consequent") D given that C
1, A1
0
and some observed value of A3, and the remaining 25%
seek the probability of D given that C = 1, A1 = 1
and some observed value of A3.

correlated (or at least, not in any simple way) with the
value of the conditional probabilityp( X=xI Y = y).
The fact that the underlying p( · ) is stationary sim­
ply means that the query sq( · ; . ) has a determi­
nate answer given by the true conditional probability
p( X= xI Y = y) E [0, 1]. In general, we call each
tuple (X=x; Y =y; p(X=x I Y=y)) a "labeled sta­
tistical query" .
Now fix a network (over V) B, and let B(xI y) =
B( X = xI Y = y) be the real-value (probability)
that B returns for this assignment. Given distribu­
tion sq( ·; ) over SQ, the "score" of B is
·

errsq,p ( B

) = L:sq(x;y)[B(xly)-p(xly)]2
x,y

{1)

where the sum is over all assignments x, y to all sub­
sets X, Y of variables. (We will often write this simply
err( B) when the distributions sq and pare clear from
the context.) Note this depends on both the underly­
ing distribution p( ) over V, and the sq( · ) distribution
over queries SQ.
·

=

=

=

=

=

If each of the N variables has domain {0, 1}, then the
SQ distribution has 0(5N) parameters, because each
variable Vi E V can play one of the following 5 roles in
a query:

{[

[ vv :; ] [ vv: r ]
[\:�],[���]

vv : ;c ]

,

,

,

}

Notice that we assume that sq( ; · ) can be, in general,
completely unrelated top( ·I·), because the probabil­
ity of being asked about sq( X= x ; Y = y ) need not be
·

sq(X=x; Y=y)

is "legal"

ifp(Y=y) >

0.

Note also that we use CAPITAL letters to represent single
variables, lowercase letters for that values that the vari­

ables might assume, and the boldface font when dealing

with sets of variables or values.

effJ(B) =

-1

L

IQI (X; Y; p)EQ

[B(xly) -p]2

be the "empirical score" of the Bayesian net.
For comparison, we will later use KL( B )
to refer to the Kullback-Liebler di­
Ld p( d) log
vergence between the correct distribution p( · ) and
the distribution represented by the Bayesian net B(·).
Given a set D of event tuples, we can approximate
fn-1 L:dED log
.
this score using KLD (B)
Note (1) that small KL divergence corresponds to the
large (log)likelihood, and (2) that neither KL( B) nor
-D
KL (B) depend on sq( · ).

;<(�)

�(�!

Finally, let SQ8 C SQ be the class of queries whose
"consequent" is single literal X = {V}, and whose
"antecedents" Y are a superset of V's Markov blanket,
with respect to the BN B; we will call these "Markov­
blanket queries".
3

(We avoid degeneracies by assuming Y n X ={}.)

2A query

Given a set of labeled statistical queries Q
{(xi; Yii Pi)}i we let

LEARNING ACCURATE
BAYES IAN NETS

Our overall goal is to learn the Bayesian Net with the
optimal performance, given examples of both the un­
derlying distribution, and of the queries that will be
posed (i.e., instances of {VI = VI, . .., VN = VN} tuples
and instances of SQ, possibly labeled).
1 Any Bayesian net B. that encodes
the underlying distribution p( ) , will in fact produce
the optimal performance; i.e., err( B.) will be optimal.
(However, the converse is not true: there could be nets

Observation

·

Learning Bayesian Nets that Perfonn Well

201

whose performance is perfect on the queries that inter­
est us, i.e., err( B*) = 0, but which are otherwise very
different from the underlying distribution.)
I

in the general SQ case, or to estimate KLD (B) from
incomplete tuples D [CH92, RBKK95]; as here the
Bayesian net computations are inherently intractabl e.
We will see these parallels again below.

From this observation we see that, if we have a learn­
ing algorithm that produces better and better approxi­
mations t o p( ) as it sees more training examples, then
in the limit the sq( ·) distribution becomes irrelevant.

Another challenge is computing the sample complexity
of gathering the information required to compute the
score for a network. It is easy to collect a sufficient
number of examples if we are considering learning from
labeled statistical queries. Here, a simple application
of Hoeffding's Inequality [Hoe63} shows4

·

Given a small set of examples, however, the sq( · ) dis­
tribution can play an important role in determining
which BN is optimal. This section considers both the
computational and sample complexity of this under­
lying task. Subsection 3.1 first considers the simple
task of evaluating a given network, as this informa­
tion is often essential to learning a good BN. Subsec­
tion 3.2 then analyses the task of filling in the optimal
CP-tables for a given graphical structure, and Subsec­
tion 3.3 discusses a hill-climbing algorithm for filling
these tables, to produce a BN whose accuracy is locally
optimal.

3.1

3

Theorem

Let

� ( B)

=:

L

1

MLSQ
(q,p)ESLsQ

(B( q) - p)2

be the empirical score of the Bayesian net B, based on
set SLSQ of

a

MLSQ

==

MLsQ(f., 8)

o:::

-D

It is easy to compute the estimate KL ( B) of KL( B ) ,
based on examples of complete tuples D drawn from
the p( ) distribution. In contrast, it is hard to com­
pute the estimate �( B) of err( B) from general
statistical queries - in fact, it is not even easy to
approximate this estimate.

2

2 ln 'J
21'.

labeled statistical queries, drawn randomly from the
sq( ) distribution and labeled by p( ) . Then, with
probability at least 1-6, e�LsQ (B) will be within <0 of
err( B); i.e., P[ !errS'LsQ (B)- err( B )I < t] � 1-o,
where this distribution is over all sets of MLsQ(e., J)
I
randomly drawn statistical queries.
·

COMPUTING err(B)

1

·

·

2 ([Rot96, DL93]) It is #P-hard3
to
compute er;Q ( B) over a set of general queries Q C
SQ. It is NP-hard to even estimate this quantity to
within an additive factor of 0.5.
I

Theorem

The reason is that evaluating the score for an arbi­
trary Bayesian network requires evaluating the poste­
rior probabilities of events in Q, which is known to
be difficult in general. In fact, this is hard even if we
know the distribution p( ) and consider only a single
(known) form for the query.
·

Note, however, that this computation is much easier
in the SQB case, because there is an trivial way to
evaluate a Bayesian net on any Markov-blanket query
[Pea88]; and hence to compute the score.
There is an obvious parallel between estimating

en=O' (B) when dealing with SQB queries Q1, and estimating KL D' ( B) from complete tuples D1 [Hec95]:

both tasks are quite straightforward, basically because
their respective Bayesian net computations are simple.
Similarly, it can be challenging to compute errQ ( B)
3

A more challenging question is: What if we only get
unlabeled queries, together with examples of the un­
derlying distribution? Fortunately, we again need only
a polynomial number of (unlabeled) query examples.
Unfortunately, we need more information before we
can bound on the number of event examples required.
To see this, imagine sq( ) puts all of the weight on a
single query, i.e., sq( X = 1 ; Y = 1 ) = 1. Hence,
a BN's accuracy depends completely on its perfor­
mance on this query, which in turn depends critically
on the true conditional probability p( X = 11 Y = 1 ) .
The only event examples relevant to estimating this
quantity are those with Y = 1; of course, these ex­
amples only occur with probability p( Y = 1 ). Un­
fortunately, this probability can be arbitrarily small.
Further, even if p( Y = 1) :::::: 0, the true value of
p(X = II Y = 1) can still be large (e.g., if X is
equal to Y, then p( X = II Y
1) = 1, even if
p( Y 1) = l/2n). Hence, we cannot simply ignore
such queries (as sq( X= x; Y y) can be high), nor
can we assume the resulting value will be near 0 ( as
p( X=x I Y =y) can be high).
·

=

=

=

We can still estimate the score of a BN, in the following
on-line fashion:
Theorem
of

to a satisfiability problem, and thus #P-hard problems are
at least as difficult

as

problems in NP.

First, let SsQ

MsQ(E, 8)

Roughly speaking, #P is the class of problems corre­

sponding to counting the number of satisfiable assignments

4

4

=

{ sq( Xi ;
2

=

2

E

ln

Yi

) }i be

a

set

4

J

Proofs for all new theorems, lemmas and corollaries

appear in

[GGS97].

202

Greiner, Grove, and Schuurmans

unlabeled statistical queries drawn randomly from the
sq( ·; ·) distributzon. Next, let Sn be the set of (com­
plete) examples sequentially drawn from the underlying
distribution p( ), until it includes at least
·

"
1 ( f,u
Mn
)

==

! In 2 MsQ
f2

cl

instances that match each Yi value; notice Sn may
require many more than Mb examples. (The "legal
query" requirement p( Yi) > 0 insures that this col­
lection process will terminate, with probability 1.) Fi­
nally, letp(So)(x; \y;), be the empirically observed es­
timate of p( Xi I Yi ) , based on this Sn set. Then, with
probability at least 1 - 8,

e'f'l3sq,Sn(B)

1_
= _

:L [

B( xly) -p(Sv}(xly)
\SsQ\ (x,y)E sq
S

f. of err( B);
err(B)\<f]2:: 1-J.

will be within

i.e.,

r

P[ \erP59 ,Sn (B) -

I

We can, moreover, get an a priori bound on the total
number of event examples if we can bound the proba­
bility of the query's conditioning events. That is,

Corollary 5 If we know that all queries encountered,
sq( x; y ), satisfy p( y) 2:: .\ for some .\ > 0, then we
need only gather
MD(<,o,>.)
max

{�[

M� + ln

4�59] , �

ln

4

�sg

}

complete event examples, along with

MsQ(f, 8)

2

==

�:2

4

ln "J

3.2

COMPUTING OPTIMAL CP-tables
FOR A GIVEN NETWORK
STRUCTURE

The structure of a Bayesian net, in essence, specifies
which variables are directly related to which others.
As people often know this "causal" information (at
least approximately), many EN-learners actually be­
gin with a given structure, and are expected to use
training examples to "complete" the BN, by filling in
the "strength" of these connections - i.e., to learn
the CP-table entrie s. To further motivate this task of
''fitting" good CP-tables to a given BN structure, note
that it is often the key sub-routine of the more general
EN-learning systems, which must also search through
the space of structures. This subsection addresses both
the computational, and sample, complexity of finding
this best (or near best) CP-table. Subsection 3.3 next
suggests a more practical, heuristic approach.
Stated more precisely, the structure of a specific
is a directed acyclic graph (V, E) with
nodes V and edges E C V x V. There are, of course,
(uncountably) many BNs with this structure, corre­
sponding to all ways of filling in the CP-tables. Let
LW(V, E) denote all such BNs.

B ayesi an net

We now address the task of finding a BN B E
whose score is, with high probability, (near)
minimal among this class ; i.e., find B such that

LW(V, E)

err( B)

<

E

+

min

B 'El3/II(V,E)

err( B1)

with probability at least 1- J, for small t:, J > 0. As in
Subsection 3.1, our learner has access to either labeled
statistical queries drawn from the query distribution
sq( ·) over SQ; or unlabeled queries from sq( ) , to­
gether with event examples drawn from p( ) .
·

·

example queries, to obtain an E-close estimate, with
probability at least 1 - J.
I
Of course, as.\ can be arbitrarily small (e.g., o (l/2n)
or worse), this Mn bound can be arbitrarily large,
in terms of the size of the Bayesian net. Note also
that the Friedman and Yakhini [FY96] bound similarly
depends on "skewness" of the distribution, which they
define as the smallest non-zero probability of an event,
over all atomic events.5
Two final comments: (1) Recall that these bounds de­
scribe only how many examples are required; not how
much work is required, given this information. Unfor­
tunately, using these examples to compute the score
of a BN requires solving a #P-hard problem; see The­
orem 2. (2) The sample complexity results hold for
estimating the accuracy of any system for represent­
ing arbitrary distributions; not just BNs.
5

Hoffgen [Hof93] was able to avoid this dependency, in
certain "log-loss" contexts, by "tilting" the empirical dis­
tribution to avoid 0-probability atomic events. That trick
does not apply to our query-based error measure.

Unfortunately this task - like most other other in­
teresting questions in the area - appears computa­
tionally difficult in the worst case. In fact, we prove
below the stronger result that finding the (truly) opti­
mal Bayesian net is not just NP-hard, but is actually
non-approximatable:

6 Assuming P ::j:. NP, no polynomial­
time algorithm (using only labeled queries) can com­
pute the CP-tables for a given Bayesian net structure
whose error score is within a sufficiently small addi­
tive constant of optimal. That is, given any structure
(V, E} and a set of labeled statistical queries Q, let
B(v,E),Q E BIV(V,E) have the minimal error over Q;

Theorem

i.e., 'VB'

E

/3N(V,E), e;::;Q(B(V,E),Q) ::; �(B1).

Then (assuming P =f. NP) there is some 1 > 0
such that no polynomial-time algorithm can always
find a solution within 1 of optimal, i.e., no poly­
time algorithm can always return a B(1v E),Q such that

er:rQ(B(�,E),Q)-�( B(V,E),Q)::;

,

1'·

I

In contrast, notice that the analogous task is trivial in

Learning Bayesian Nets that Perform Well
the log-likelihood framework: Given complete train­
ing examples (and some beni gn assumptions), the CP­
table that produces the optimal maximal-likelihood
BN corresponds simply to the observed frequency es­
timates [Hec95].
However, the news is not all bad in our case. Although
the problem may be computationally hard, the sample
complexity can be p oly nom ial . That is (under certain
conditions; see below), if we draw a polynomial num­
ber of labeled queries, and (somehow!) find the BN B
t ha t gives minimal error for tho se queries, then with
hi gh probability B will be within t of optimal over the
ful l distribution sq( ) .
·

We conjecture that the sample complexity result is
true in general. However, our results below uses the
following annoying, but extremely benign, technical
restriction. Let T
{y I sq( x; y) > 0} be th e set
of all conditioning events that might appe ar in queries
(often Twill simply be the set of all events). For any
=

c

>

1, define

J3NT�l/2<N (V, E)

=

N
{BE 13N(V,E) IVy E T,B(y) > l/2° }

to be the subset of BNs that assign, to each condi­
ti on in g event, a probability that is bounded below by
N
the doubly-exponcnttally small number l/2c . (Recall th at N = IV!, the number of variables.) We now
restrict our attention to these Bayesian nets. 6
•

Theorem 7 Consider any Bayesian net structure
(V, E), requiring the specification of K CP-table
entries CPT
{[q;lrdh=l..K.
Let B* E
I3.Af7�1;2cN (V, E) be the BN that has minimum em-

pirical score with respect to a sample of

M£sQ(t,o)

=

:b ( log � +

Klog2�

+

NKlog(2+c-logE))

labeled statistical queries from sq( ). Then, with prob­
ability at least 1 - J, B* will be no more than t worse
·

than Bopt, where Bopt is the BN with optimal score
among !3NT�l/2"N (V, E) with respect to the full dis­
tribv.tion sq ( ) .
I
·

This theorem is nontrivial to prove, and in particular
is not an immediate corollary to Theorem 3_ That
earlier result shows how to estimate the score for a
6Conceivably -

although we

conjecture

otherwise -

there could be some sets of queries and some graphs (V, E),
such that the best performance is obtained with extremely

small CP-table entries; e.g., of order

o(l/22

2�

).

(But note

that numbers this small can require doubly-exponential
precision just to write down, so such BNs would perhaps be

impractical anywayr)

Our result assumes that, even if such

BNs do allow improved performance, we are not interested
in them.

203

single fixed BN, allow in g 6 p robabilit y of error. But
since B* is chosen after the fact (i.e., to be opt imal on
the training set) we cannot have the same confidence

that we have estimated its score correctly. Instead,
we must use sufficiently many examples so that the
simultaneously estimated scores for all (uncountably
many) B' E BN T'?:l/2eN (V, E) are all within E of the
true values, with collective probability of at most J
that there is any error. Only then can we be confident
about B*'s accuracy. (See pro of in [GGS97] .)

As i n Sect ion 3.1, we can also consider the slightly
more complex task of lea rn in g the CP-table entries
f ro m unlabeled st ati sti cal queries sq( X = x; Y = y ) ,
augmented with examples of the underlying distribu­
tion p( ) . However, as above, this is a straightfor­
ward extension of th e "learning from labeled s tatist i­
cal query" case: one firs t draws a slightly larger sam­
ple of unlabeled statistical queries, and then uses a
sufficient sample of domain tuples to accurately esti­
mate the lab el s for each of these queries (h enc e sim­
ulating the effect of drawin g fewer - but still suffi­
ciently many - labeled statistical queries). Here we
encounter the s am e caveats that each of the unlabeled
statistical queries sq( X= x ; Y= y) must involve con­
ditioning events Y = y that occur with some nontrivial
probability p( Y y) > 0 (for otherwise one could not
put an no ntri vial upper bound on the number of tu­
ples needed to learn a good setting of the CP-table
entries). A detailed statement and proof of this r e­
sult is a straightforward exte nsio n of Theorem 7, so
we omit the details here. (See [GGS97].)
·

=

The point is that, from a sample complexity per sp ec ­
tive, it is fe asibl e to learn near optimal settings for the
CP-table entries in a fixed Bayesian network structure
under our mod el. The only difficult part is that actu­
ally computing these optimal entries from (a polyno­
mial number of) training samples is hard in g ener al;
cf., Theo rem 6. In fact, we will see, in Section 4, that
it is not correct to simply fill each CP-table entry with
the frequency es timat es.
3.3

HILL CLIMBING

It should not be surprising that finding the optimal
CP-tables was computationally hard, as this problem
has a lot in common with the challenge of l e arn in g the
KL( · ) - bes t network, given partially specified tuples;
a task for which people often use iterative steepest­
ascent climbing methods [RBKK95]. We now briefly
consider the analogous approach in our setting.

Given
a
single
labeled
statisti­
cal query "(x; y; p( xI y )}", consider how to change
the value of the CP-table entry [qlr], whose current
value is eqlr · We use th e follow in g lem ma :
8 Let B be a Bayesian net whose CP-table
includes the value eQ=qiR=r = eqjr E [0, 1] as the
value for the conditional probability of Q = q given
R = r. Let sq( X ; Y) be a statistical query, to which

Lemma

Greiner, Grove, and Schuurrnans

204

B assigns the probability B( X I Y). Then the deriva­

tive of B( X I Y), wrt the value

eq\r,

is

d err(X ,Y) ( B)
d eqlr
2(B(X[Y)-p)
B(X[Y) (1-B(X[Y)) (4)
eqlr

dB(X [Y)
deqlr
1

=-B(XfY) [B(q, r [X, Y)-B(q,rjY)]
eqlr
As

(2)

produced the score B(X I Y) here, the error for
this single query is ffi(X,Y) (B)
(B( X 1 Y) - p) 2 .
To compute the gradient of this error value, as a func­
tion of this single CP-table entry (using Equation 2),

B

=

am<XY
, )( B)
d eqlr

thus, letting

C

=

=

2(B(XjY) -p)

dB(XfY)
d eqlr

2(B ( X I Y)- p), we get

0dB (XjY)
deqlr
=

:::::

c

- [B(q,r, XJY)- B(XJY)B(q,r [Y)]
eqlr
c

-B(X J Y) [B( q,r I X, Y)-B(q,r I Y )] (3)
eqlr

We can t en ';lse �his derivati�e t� update the eq1r
.
value, by htll-chmbmg m the d1rect10n of the gradi­
ent (i.e., gradient ascent.). Of course, Equation 3 pro­
vides only that component of the gradient derived from
a single query; the overall gradient for eq\r will in­

�

volve summing these values of all queries (or perhaps
all queries in some sample). Furthermore, we must
constrain the gradient ascent to only move such that
eq' 1r remains as 1 (i.e., the sum of probabilities of

L:�

all possible values for Q, given a particular valuation
for Q's parents, must sum to 1). However, the tech­
niques involved are straightforward and well-known,
so we omit further analysis here.

Notice immediately from Equation 3 that we will
no t update eqlr (at least, not because of the query

sq( X; Y)) if the difference B( X I Y)- p is 0 (i.e., if
B (X I Y ) is correct) or if B(q, r I X, Y) - B( q, r I Y)
is 0 (i.e., if Y "d-separates" X and q, r); both of which
makes intuitive sense.

Unfortunately, we see that evaluating the gradient re­
quires computing conditional probabilities in a BN.
This is analogous to to th e known result in the tra­
ditional model [RBKK95]. It thus follows that it can
be #P-hard to evaluate this gradient in general (see
Theorem 2). However, in special cases- i.e., BNs for
which inference is tractable - efficient computation is
possible.
One demonstration of this is the class of "Markov­
bl anket queries" SQB (recall the definition at the end
of Section 2). Carrying out the gradient computation
is easy i� this case: Here when updatin� the [qlr] entry,
Y"e can 1gnore queries sq( X; Y) if [qJr) is outside of
1ts Markov blanket. We therefore need only consider
the queries sq( X; Y) where {Q} U R C XU Y and
moreover, when Q
q is consistent with X's assign­
ment; for these queries, the gradient is
=

which follows from Equation 3 using B( q, r I X, Y) :::::
q, r is consistent with X's claim (recall we ignore
sq(X; Y) otherwise), and observing that B(q,rj Y)
reduces to B( X I Y), as the part of { Q} U R already
in Y is irrelevant. Notice Equation 4 is simple to com­
pute, as it involves no non-trivial Bayesian net com­
putation; see the simple algorithms in [Pea88J.
1 as

4

HOW CAN THE QUERY
DISTRIBUTION HELP?

Our intuition throughout this paper is that having ac­
cess to the distribution of queries should allow us to
learn better and more efficiently than if we only get to
see domain tuples alone. Is this really true?
Note that the simplest and most standard approach
to learning CP-table entries is simply filling in each
CP-table entry with the observed frequency estimates
[OFE] obtained from p( ). Note that this ignores
any information about the query distribution. Unfor­
tunately, OFE is not necessarily a good idea in our
model, even if we have an arbitrary number of ex­
amples. This follows immediately from Theorem 6:
If the standard OFE algorithm was always success­
ful, then we would have a trivial polynomial time al­
gorithm for computing a near-optimal CP-table for a
fixed Bayesian net structure - which cannot be (un­
less P ==
Yes, Observation 1 does claim that the
optimal BN is a faithful model of the event distribu­
tion, meaning in particular that the value of each CP­
table entry [qlr] can be filled with the true probability
p(q I r ). However, this claim is not true in general
in the current context, where we are seeking the best
CP-table entries for a given network structure, as this
network structure might not correspond to the true
conditional independence structure of the underlying
distribution p( ).
·

NP).

·

In the case where the BN structure does not corre­
spond to the true conditional independence structure
of the underlying p( ) , ignoring the query distribution
and using straight OFE can lead to arbitrarily bad
·

results:

Example 4.1

Suppose the EN structure is simply
A -t X -t C, and the only labeled queries are
(C; A; 1.0) and (C; A; 0.0). (I.e., A = C with proba­
bility 11 and we are only interested in querying C given
A or A.) Suppose further that the intervening X is
completely independent of A and C -i.e., p( X I A)
p( X ]•A) p( C I X) p( C I•X) 0.5. (Note that
this BN structure is seriously wrong.)
=

=

=

=

205

Learning Bay esian Nets that Perfonn Well

In this situation, the EN that most faithfully follows

the event distribution, Bp, would have CP-table en­
e x 1 ;t = ec1x = e q x
0.51 with a per­
tries e x i A

s core of err( Bp ) = 0.25. (Recall that ex l A
�s the CP-table entry that specifies the probability of
X given that A holds; etc.) Now consider Bsq 1 whose
ent �ies are exiA = eCix
1.0 and e x 1 Ji = eqx = 0.0
=

=

formance
1

make X = A and C ;:::::: X . While Bsq clearly
has the X -dependencies completely wrong, its score is
=

- 1.e . ,

perfect, i.e. , err(

Bsq ) = 0.0. 7

I

Thus, filling CP-table entries with observed frequency
estimates - or using any other technique that con­
verges to Bp - leads to a bad solution in this case,
no matter how many training examples are used. On
the other hand, consider a learning procedure that
(knowing the query distribution!) ignores the X vari­
able completely and directly estimates the conditional
probabilities p( A I C ) and p( A 1 -.C ) before filling in
the CP-table entries (i. e., which is isomorphic to B89 ) .
This would eventually learn a perfect classifier. Of
course, such a procedure might have to be based on the
(impractical) learning techniques developed in Theo­
rem 7, or p erhaps (more realistically) use the heuristic
hill-climbing strategies presented in Section 3.3.
What about the case when the proposed network
structure is correct? Here we know that the standard
OFE approach eventually does converge to an optimal
CP-table setting for any query distribution ( Observa­
tion 1 ) . So, unlike the case of an incorrect structure,
there is no reason in the large-sample-size limit to con­
sider the query distribution. But what about the more
realistic situation, where the sample is finite? The
question then is:
Given that the known structure is correct,
can we exploit knowing the true query distribution?
There is one simple sense in which the answer can
certainly be yes. It is clearly safe to to restrict our
attention to those nodes of the BN that are not d­
separated from every query variable by the condition­
ing variables that appear in the queries. That is, if
a B N B contains the edge U � V, and the query
7The same issue is relevant to understanding the re­
striction in Theorem 7 to BNT >-l/2cN (V, E) . One might

consider removing this restriction by assuming that all con­
ditioning events y E T have significant probability accord­
ing to p( · ) ; i.e. , they are not too unlikely (a la Corol­
lary 5 ) . But Theorem 7 does not make this assumption,
for an important reason. Note that; if we are not directly
interested in queries about p( y ) , then the BN we use to
answer queries is not constrained to agree with p( y ) . In
particular, if it helps to get better answers on the queries
that do occur, the optimal BN Bopt could (conceivably)
"set" Bopt (y) to be extremely small; knowing that p( y )
is p erhaps large is j ust irrelevan t . Our theorem, which in­
s�ead assumes that the former quantity is not too small,
simply would not be helped by (what might seem to be
more natural) restrictions on p( y ) .

Figure 1 : Bayesian network structure for Example 4.2
("Naive Bayes" ) .
distribution sq( X = x ; Y = y) i s such that, for ev­
ery query "p( X = x I Y = y ) = ?" , both U and V are
d-separated from X by Y, then we know that the CP­
table entry e v l u cannot affect B's answer to the query,
B( X I Y ). Thus, it seems clear that we do not need
to bother estimating evJu here. Now suppose we have
a learning algorithm that uses a computed sample size
bound (which grows with the number of parameters
to be estimated) in order to provide certain perfor­
mance guarantees. Here, our knowledge of the query
distribution will reduce the effective size of the BN,
which will allow us to stop learning after fewer sam­
ples. Thus, using the query distribution can give an
advantage here, although only in a rather weak sense:
the basic learning technique might still amount to fill­
ing in the CP-table entries with frequency estimates
obtained from the underlying distribution p( ) - the
only win is that we will know that it is safe to stop
earlier because a small fragment of the network is rel­
evant.
·

Can one do better than simply filling in CP-table
entries with frequency estimates, given that the BN
structure is correct? As we now show, this question
does not seem to have a simple answer.
Motivated by Example 4.1, one might ignore the EN­
structure in general, and just directly estimate the con­
ditional probabilities for the queries of interest. Note
that this is guaranteed to converge to an optimal solu­
tion, eventually, even if the BN structure is incorrect.
However, it can be needlessly inefficient in some cases,
especially if the postulated BN structure is known to
be correct. This is because the BN structure can pro­
vide valuable knowledge about the distribution.
4 . 2 Consider the standard "Naive Bayes "
model with n + 1 binary attributes C, A1 , . . . , An such
that the Ai are conditionally independent given C;

Example

see Figure 1 .

Suppose the single query of interest is
= ? ".
If
= 0, A2 = 0, . . . , An = 0 )
we attempt to learn this probability directly, we must
wait for the (possibly very rare} event that A1 = A2
. . . = An = 0; it is easy to construct situations where
this will require an exponential (expected) number of
examples. However, if we use the EN-structure, we
can compute the required probability as soon as we
have learned the
+ 1 probabilities p(C = 0) and

''p( C

=

0 I A1

=

2n

206

Greiner, Grove, and Schuunnans
5

CONCLUSIONS

Remaining Challenges: There are of course several
other obvious open questions.

Figure 2: Bayesian network structure for Example 4.3
( "Reverse Naive Bayes" ) .

p(A; = O I C = O ) , p(A; = O I C = 1 ) for all i . If
p(C = 0) is near 1 /2, these probabilities will all be
learned accurately after relatively few samples.

I

This example might suggest that we should always try
to learn the B N as accurately as possible, and ignore
the query distribution (e.g., just use OFE ) . However,
there are other examples in which this approach would
hurt us:

First, the analyses above assume that we had the EN­
structure, and simply had to fill in the values of the
CP-tables. In general, of course, we may have to use
the examples to learn that structure as well. The obvi­
ous approach is to hill-climb in the discrete, but combi­
natorial space of BN structures, perhaps using a sys­
tem like PALO [Gre96] , after augmenting it to climb
from one structure S; to a "neighboring" structure
Si+b if Si+b filled with some CP-table entries, appears
better than S; with (near) optimal CP-table values,
over a distribution of queries. Notice we can often save
computation by observing that, for any query q, B1
and B2 will give the same error scores B1 ( q ) = B2 ( q )
if the only differences between B1 and B2 are outside
of q's Markov blanket.
The second challenge is how best to accommodate both
types of examples: queries (possibly labeled) , and do­
main tuples. As discussed above, sq( ) examples are
irrelevant given complete knowledge of p( ) (Obser­
vation 1). Similarly, given complete knowledge of the
query distribution, we only need p ( ) information to
provide the labels for the queries.
·

Example 4 . 3

Consider the "reverse " EN-structure
from Example 4 . 2, where the arrows are now directed
A; -t C instead of C -t Ai (Figure 2), and assume we
are only interested in queries of the form ''P( C I { } )
? . Here the strategy that estimates p( C = c ) di­
rectly (hence, ignoring the given EN-structure) domi­
nates the standard approach of estimating the CP-table
entries. To see this, note that for any reasonable train­
ing sample size N « 2 n , the frequency estimates for
most of the 2n CP-table entries eel a, , . . . ,an will be un­
defined. Even using Laplace adjustments to compen­
sate for this, the accuracy of the resulting EN estimate
B (C
cl { } ) will be poor unless p( C = c ) happens
to be near 0. 5. So, for example, if the true distribu­
tion p(- ) is such that C = A1 1\ parity(A2, ... , A n ) , and
p( A; = 1 ) = 0.5, then p( C = 1 ) will be equal to
0.25. But here the EN estimator (using Laplace ad­
justments) will give a value of B (C = I I { } ) :::::: 0.5 for
any training sample size N << 2 n , whereas the direct
strategy will quickly converge to an accurate estimate
P(C = 1) :::::: 0.25. (Note that the standard maximum
likelihood EN estimator will not even give a defined
value for E (C = I I { } ) in this case, since most of the
possible a 1 , . . . , a n patterns will remain unobserved.)

·

·

=

"

=

In general, the question
What should we actually do if the BN structure
is known (or assumed) to be correct, and we are
training on a (possibly small) sample of complete
instances?
remains open, and is an interesting direction for future
work
asking, in essence, should we trust the given
EN-structure, or the given query distribution? The
previous two examples suggest that the answer is not
a trivial one.
�

Of course, these extreme conditions are seldom met;
in general, we only have partial information of either
distribution. Further, Example 4 . 1 illustrates that
these two corpora of information may lead to differ­
ent BNs. We therefore need some measured way of
combining both types of information, to produce a B N
that i s both appropriate for the queries that have been
seen, and for other queries that have not
even if
this means degrading the performance on the observed
queries. (That is, the learner should not "overfit" the
learned BN to just the example queries it has seen; it
should be able to "extend" the BN based on the event
distribution.)
�

Contributions : As noted repeatedly in Machine
Learning and elsewhere, the goal of a learning algo­
rithm should be to produce a "performance element"
that will work well on its eventual performance task
[SMCB77, KR94] . This paper considers the task of
learning an effective Bayesian net within this frame­
work, and argues that the goal of a EN-learner should
be to produce a BN whose error, over the distribution
of queries, is minimal.

Our results show that many parts of this task are, un­
fortunately, often harder than the corresponding tasks
when producing a BN that is optimal in more familiar
contexts (e.g. , maximizing likelihood over the sampled
event data) � see in particular our hardness results for
evaluating a BN by our criterion (Theorem 2 ) , for fill­
ing in a EN-structure's CP-table (Theorem 6 ) , and for
the steps used by the obvious hill-climbing algorithm
trying to instantiate these tables. (Note, however, that

Learning Bayesian Nets that Perform Well
Learning
"Algorithm"

Structure is

OFE•

Correct
Incorrect

QDt

Correct
Incorrect

•

OFE

=

Computational
Efficiency

Correct convergence
(in the limit)

Small sample

"easy"

Yes [Obs 1)
No [Ex 4.1)

> 7 QD [Ex 4.2)"•

Yes

> 7 0FE [Ex 4.3) . .

N P-hard to approx. [Th

6)

207

Yest

Observed Frequency Estimates (or any other algorithm that tries to match the event distribution. )

QD uses the CP-table that i s "best" for given query distribution, using samples from the distrib�tion to

label queries.

QD will produce the BN that has minimum error, for this structure.
•• Our examples illustrate cases in which one "algorithm" (OFE, QD) is more sample efficient than the other.

+

Table 1: Issues when Learning from Distributional Samples
our approach is robustly guaranteed to converge to
a BN with optimal performance, while those alterna­
tive techniques are not.) Fortunately, we have found
that the sample requirements are not problematic for
our tasks (see Theorems 3, 4, 7 and Corollary 5),
given various obvious combination o f example types;
we also identify a significant subclass of queries (SQs)
in which some of these tasks are computationally easy.
We have also compared and contrasted our proposed
approach to filling in the CP-table-entries with the
standard "observed frequency estimate" method, and
found that there are many subtle issues in deciding
which of these "algorithms" works best, especially in
the small-sample situation. These results are summa­
rized in Table 1 . We p lan further analysis (both theo­
retical and empirical) towards determining when this
more standard measure, now seen to be computation­
ally simpler, is in fact an appropriate approximation
to our performance-based criteria.



These values, referred to as query responses, clearly de­
pend on the training sample used to instantiate the param­

A Bayesian Belief Network (BN) is a model of

eter values - i.e., different training samples will produce

a joint distribution over a finite set of variables,

different parameters and hence different responses.

with a DAG structure to represent the immedi­

This paper investigates how sampling variability in the

ate dependencies between the variables, and a
set of parameters (aka CPTables) to represent the

training data is related to uncertainty about a query re­

local conditional probabilities of a node, given

sponse. We follow the Bayesian paradigm, where uncer­

each assignment to its parents. In many situa­

tainty is quantified in terms of random variation, and we

tions, the parameters are themselves treated as

present a technique for computing Bayesian credible in­

random variables- reflecting the uncertainty re­

tervals (aka "error-bars") for query responses. Our algo­

maining after drawing on knowledge of domain

rithm takes as inputs a belief net structure (which we as­

experts and/or observing data generated by the

sume is correct- i.e., an accurate /-map of true distribu­

network. A distribution over the CPtable param­

tion [Pea88]); a data sample generated from the true belief

eters induces a distribution for the response the

net distribution; and a specific query of the form "'What is

BN will return to any "W hat is

Q =

Pr{ HIE} ?"

Pr{ H

=

hIE

= e } ?". After determining the

This paper investigates the distribution

conditional (posterior) distribution of the belief net param­

of this response, shows that it is asymptotically

eters given the sample, the algorithm produces an estimate

query.

(posterior mean value) of Q: e.g., estimate Q to be

asymptotic variance. We show that this compu­

To quantify uncertainty about this estimate, the algorithm

tation has the same complexity as simply com­

computes an approximate posterior variance for Q and uses

puting the

this variance to construct error-bars (a Bayesian credible in­

(mean value of the) response -i.e.,

O(n exp(w)),
ables and

w

where

n

terval) for Q; e.g., assert that Q is in the interval

is the number of vari­

is the effective tree width.

with 90% probability.

We

also provide empirical evidence showing that the

0.3 ± 0.1

There are several obvious applications for these error-bars.

error-bars computed from our estimates are fairly

First, error-bars can help a user make decisions, especially

accurate in practice, over a wide range of belief

in safety-critical situations - e.g., take action if we are

net structures and queries.

99%

sure that Q =

Pr{ H

=

hIE

= e

}

is on one

side of a decision boundary. Second, error-bars can

1

Introduction

can make appropriate guarantees about the answers to cer­

model of a joint probability distribution, are used in
an ever increasing range of applications [Hec95].
nets

are

typically

built by

Be­

first finding an ap­

propriate structure (either by interviewing an expert,
or by

selecting

a

good model

from training

data),

then using a training sample to fill in the parame­
ters

sug­

gest that more training data is needed before the system

Bayesian belief nets (BNs), which provide a succinct

lief

0.3.

normal, and derives expressions for its mean and

[Hec98]. T he resulting belief net is then used to an­

swer questions, e.g., compute the conditional probability

tain queries. This information is especially valuable when
additional training data, while available, is costly, and its
acquisition needs to be justified. Similarly, the user might
decide that more evidence is needed about a specific in­
stance, before he can render a meaningful decision.

Fi­

nally, if an expert is available and able to provide "correct
answers" to some specific questions, error-bars can be used
to validate the given belief net structure. E.g., if the expert
claims that Q

=

0.5 but our algorithm asserts that Q is in

UAJ 2001

VAN ALLEN ET AL.

0
�
a
I

0

91,110
0.400

523

91,0!0

0.600

G(�l
"x@4�
x4

Figure

1:

o

1
0

Simple Example: Diamond Graph

the interval 0.30 ± 0.04 with 99.9% probability, then we
may question whether the structure provided is correct (as­
suming we believe the expert). By contrast, we might not
question this structure if our algorithm instead asserted that
Q is in the interval 0.30 ± 0.25 with 99.9% probability.
Section 2 provides background results and notation con­
cerning belief nets and Dirichlet distributions for belief net
parameters. Section 3 presents the theoretical results under­
lying our error-bars: a derivation of an approximate poste­
rior variance for a query probability Q, and a proof that the
posterior distribution of Q is asymptotically normal. Com­
putational issues related to calculation of the variance are
briefly discussed. Section 4 presents the results of an em­
pirical study using Monte Carlo simulations to validate our
error-bar methodology over a wide range of belief net struc­
tures and queries. Section 5 briefly surveys related work,
placing our results in context.
2

1
0
0

Belief nets and Dirichlet distributions

We encode the joint distribution of a vector of discrete ran­
dom variables X = (Xv}vEV as a belief net (aka Bayesian
network, probability net). A belief net (V, A, 8) is a
directed acyclic graph whose nodes V index the random
variables and whose arcs A represent dependencies. Let
Pa(v) C V be the immediate parents of node v, and let
Fv = (Xw)wEPa(v) be the corresponding vector of parent
variables. In a belief net, a variable Xv is independent of
its nondescendents, given Fv. The elements of the vector
8 are the CPtable entries

Let Xv and Fv = TiwEPa(v)Xw be the domains of Xv and
Fv· We assume that the domains are finite. The CPtable
for Xv contains IXvl X IFvl entries 0v,xJf·
Figure 1 provides a simple example of a belief network
with specific CPtable entries. Here X1 has no parents, so
we write F1 = (}.We have F2 :::��: {Xt), Fa= (Xt), F 4 =
{X2, Xa); and for each value a, b, c, d, we have 01,al0 =
Pr{ xl = a IE>}, e2,bJa = Pr{ x2 = b I Xt = a, E> },
and e4,djb,c = Pr { x4 = d I x2 = b, X a = c, e }.

(Hence, using Figure 1, we have 81,110 = 0.4.) Note that
the values in each row add up to 1. In general, the variables
need not be binary, but can have larger (finite) domains.
The CPtable entries are estimated using training data and
(possibly) expert opinion. The latter information is incor­
porated using the Bayesian paradigm, where 8 is mod­
eled as a random variable and expert opinion is expressed
through an a priori distribution for 8. We adopt indepen­
dent Dirichlet priors1 for the various CPtable rows. Specit1cally, let 8vl/ = (9v,zl/)xEX. denote the CPtable row for
Fv = f- e.g., e4J(1,0) = (04,1](1,0), 04,0](1,0}) denotes
the entries for the X4 variable associated with the parental
assignment X2 = 1 and X3 = 0. We assume that, be­
fore observing the training data, the evil are independent
"Dir( a::,,11, x E Xv )"random vectors, where a:;,,11 > 0.
An absence of expert opinion is often expressed by setting
a;,xlf = 1 for all (v,x, f)- e.g., 84J(1,o) ""Dir( I, 1)
- which yields a uniform (flat) prior. Stronger opinion is
expressed through larger values of a:*v,x 11. Expressions for
the mean and variance of a Dirichlet distribution are given
below.
Now suppose that the training data consist of m indepen­
dent replicates of vectors X, generated using the given
structure and a fixed set of CPtable entries e. Let
mv,xlf denote the number of cases in the training set with
(Xv, Fv) = (x, f). Under the posterior distribution (the
conditional distribution given the training data), the E> vlf
are independent Vir( O:v,zl/• x E Xv) random vectors, with
O:v,xlf = a;,xlf + m v,xlf [BFH95]. This posterior distri­
bution underlies our derivation of Bayesian credible inter­
vals. Several properties of the Dirichlet distribution will be
needed.
Setting O:v,.J/ = l::xEXv O:v,xJ f• the posterior means and
(co)variances for CPtable entries are [BFH95]:

1

E{G v,xJf}

=

Cov{0v,zJ/• 0v,yJ/}

=

f.lv,xlf

O:v,zl/

= --

(1)

O:v,-J/
llv,xjt(c5xy- llv,yJf)
(2)
O:v,-J/ + 1

Readers unfamiliar with these assumptions, or with Dirichlet

distributions, are referred to [Hec98]. Note that a Dirichlet distri­
bution over a binary variable is a Beta distribution.

VAN ALLEN ET AL.

524

1 if x == y and c5,y = 0 otherwise. The ran­
Elvlf are asymptotically normal, in the limit
as min, av,xl! --t oo [Aki96]. More precisely, the nor­
malized variables ..;av,.1f(8v,x1J - 1-Lv,xJJ) converge in

where

c5xy =

dom vectors

distribution to jointly normal random variables with mean
zero and covariances

J.tv,xJJ(o,y- J.tv,yJJ).

This asymp­

totic framework is applicable as the amount of training data
increases

8v,xJJ

(m --t oo) provided all of the CPtable entries

are positive. This condition occurs with probability

one under a Dirichlet prior.

3

Bayesian Credible Intervals for Query Re­
sponses

It is well-known that the CPtable entries determine the

joint distribution of X:

Pr{ Xv = Xv, v

E

vIe}

=

I1vEV ev,x. II.] where(! vlvEV is determined by (xv ) vEV ;
see [Pea88]. Users are typically interested in one or more

specific "queries" asked of this joint distribution, where a
query is expressed as a conditional probability of the form
Q

=

q(B)

=

Pr{H=hiE=e,e},

(3 )

UAI2001

partial derivatives. Let Pv (h, x,fIe) denote the probabil­

ity

Pr{ H =h, Xv = x, Fv =fIE= e, e
and let pv (x , f

=

f1. },

le),Pv(h,f le),pv(f le), andp(h le) be

defined in a similar manner. Note that the subscript

v

is

Xv or F u is involved,
and all probabilities are evaluated at e
/-£. Let q� /
.zl
denote the partial derivative 8q(B)/8Bv,zlf evaluated at
0 := IL· We will use the following identity, derived by
needed to identify the node when

=

[GGS97, DarOO]:
1

qv,zlf

Pv(h,x,f le) - p(h le)pv(x,f le)
J.tv,zlf

_
-

We now derive an expression for

i7�,

·

(S)

and demonstrate

asymptotic validity of the credible interval (Equation

4)

given a sufficiently large training sample.

1 We assume that 8 is a random vector with
posterior Dirichlet distribution described in Section 2, and
approximate the variance of Q
q(8) by

Theorem

=

where H and E are subvectors of X, and h and e are legal

assignments to these subvectors. Note also the dependency
one.
In our Bayesian context, Q is a random variable with a (the­
oretically) known distribution determined by the posterior

ij�

the posterior mean J.LQ =

E{ Q}.

the iden tity [CH92]:
=

E{ q(8)}
Set It =

defined

E{E>}

where the components

by Equation

J.tv,zJ/

of 11- are

1.

W hile a point estimate 11-Q

=

q(�t)

can be useful, one

often requires some information concerning the potential
error in the estimate.

In the Bayesian context, this can

be achieved by plotting the posterior distribution of Q.
Alternatively, one may construct a 100(1

-

r5)%

cally not analytically tractable, but simple approximations
We

ill show that the distribution of Q is

w

approximately normal, and derive an approximation i7Q for

the standard deviation of Q. We then propose the following
interval as an approximate 100(1

J.LQ

±

-

r5)% credible interval:

zo/2 ifQ,

where zo/2 = <I>-1 (1 - 8/2) is the upper
standard normal distribution.

(4)

J/2

value of the

Our derivation is based on a first-order Taylor expansion of

q(E>)

about

q(�J-).

Some notation is needed to express the

-

Bvf )/(av,.Jf

+

1),

(6)

{Pv(h,x,!Je)-p(hle)pv(x,f]e)P
,
J.t v, zlf

Consider an asymptotic framework where the poste­
rior means J.tv,xlf are fixed, positive values, and
min { nv,zl/}
T hen the random variable
-+
oo.
(Q- J.LQ )/i7Q converges in distribution to the standard
nonnal distribution.
Proof. Our proof uses the Delta method

sider the Taylor expansion

credi­

ble interval for Q; i.e., an interval (L, U) defined so that
Pr{ L :::; Q :::; U} = 1- o. Exact calculations are typi­

are available.

L

T his value can be calcu­

q( E{E>} ).

vEV !E:Fv

where

distribution of e. For a point estimate of Q, one may use

lated using

L L (Avt

=

q(8)

=

q(fJ.) + D

+

[BFH95]. Con­

R,

where

D

L L L q�,l:l/ (E>v,zlf - /Jv,z!f ).

vEV /E:Fv zEXv

(7)

and the remainder term R can be expressed in terms of the
matrix of second derivatives of q( e) evaluated at a pointe
between Band J.t. Since the variances for E>u,zJ! in EqUCJ­
tion 2 are of order 1/av,zlf --t 0, and since the second
derivatives remain bounded in a neighbourhood of f.i, the
remainder R is asymptotically negligible compared with D.

&b

We define
to be the variance of D (Equation 7). As
the CPtable rows El vlf are statistically independent, but

UAI2001

VAN ALLEN ET AL.

e ntries within a row are correlated, the variance of
be expressed as

D

can

525

Table 1: Gold Standard for Validity Estimates
d

After substituting Equation 2 for the covariances and sim­
plifying, we obtain Equation 6 with

L (q�,xlt)21Lx,vlf'

Avf

A substitution of Equation 5 then yields the equivalent ex­
pressions/or Avt and Bvt within Equation 6.

Dfuq

10%
20%
30%
40%

Mean
2.38

3.15
3.63

3.88

Std.Dev.
1.86

2.41
2.79
2.96

q�,xlf in time O(n2w);

see [DarOO]. Given these deriva­
tives, the summations in Equation 6 can be performed with
one additional pass over the values, of time 0 (n).

The extended paper [VGHOl] describes an algorithm for
computing UQ. The main challenge, computing all of
the derivatives q� xlf' is accomplished by "back propagat­
ing" intermediate' results obtained by the Bucket Elimina­
tion [Dec98] algorithm.

We observe that
is a random variable with mean 0
and variance 1. It remains to show that D / ijQ is asymp­
totically normal. This result follows from the asymptotic
multivariate normality of the components of 8 (after suit­
able standardization- see Section 2), and the fact that D
0
is a linear function ofEl.

[VGHOl] also provides additional comments on the proper
interpretation and application of this theorem.

There are exceptional situations where
the posterior distribution of q(9) is analytically tractable
and exact credible intervals are available. In the degener­
ate situation where the network structure has arcs connect­
ing all pairs of nodes (and hence imposes no assumptions
about conditional independence), the assumption of inde­
pendent Dirichlet distributions for CPtable rows is equiva­
lent to an assumption of a single Dirichlet distribution over
unconditional probabilities Pr{ Xv = Xv, v E V}. It is
then straightforward to derive the distribution of the query
probability using properties of the Dirichlet distribution;
see [Mus93].2 Note that this exact approach is not cor­
rect in general
i.e., it does not hold for networks with
non-trivial structure.3

(8)

4

Empirical Study

Theorem 1 proves that the interval /-LQ ± z6;2uq is asymp­
totically valid. More precisely, let

Degenerate Case:

-

The computational problem of
computing J.LQ = q(p,) is known to be NP-hard [Coo90];
when all variables Xv are binary, the most effective ex­
act algorithms require time O(n2w), where n = lVI is the
number of nodes and w is the induced tree width of the
graph [Dec98, LS99]. The variance uq can also be com­
puted in time O(n2w). This result follows from the exis­
tence of algorithms that can compute all of the derivatives
Computational Issues:

Assuming a uniform prior and a sample of size m, we can
compute the posterior variance of Pr{ HIE} as P{HIE} x (1F{HIE} )/((m x P{E})+3), where F(x) is the expected value
of x, wrt the given belief net.
3This follows from a dimensionality argument: in a non-trivial
structure, the 2n-dimensional vector of unconditional probabili­
ties is constrained to lie in a lower-dimensional submanifold of
t e 2n -!-dimensional simplex. This cannot be represented by a
smgle Dirichlet distribution because, wpl, the constraints would
not be satisfied.
2

�

be the probability that the query response Q falls outside of
the credible interval, based on our UQ estimate of standard
deviation, Equation 6. The values 1 o and 1 � are the
nominal and actual coverage probabilities for the credible
interval. The value� is a function of o, the graph (V, A),
the query q, and the posterior distribution of 0. The pos­
terior distribution depends on the prior distribution and the
training sample. Thus� typically varies from one applica­
tion to the next. While Theorem 1 implies that� � o when
the training sample is sufficiently large, it does not tell us
whether this approximation is valid in practice, particularly
for small samples. In general, the validity of the approxi­
mation depends on all of the factors determining �. We
carried out a number of experiments to assess how these
factors affect validity.
-

-

Given a fixed set of factors, we estimate the correspond­
ing� by a simple Monte Carlo strategy. Using the (fixed)
posterior distribution of 0, calculate ILQ and uq. Simulate
r replicates ei from the posterior distribution, calculate
Qi = q(0;), then let.&. be the proportion of the {Qi} with
IQ; /-LQI > Zof2UQ. In our experiments, each � was
based on r 100 replicates.
-

=

To quantify the validity of the approximation�
employ average absolute differences:
validity estimate

=

average I.&- Jl.

�

o, we

(9)

The absolute differences are averaged as we vary one or
more of the the factors determining �- The validity es­
timates are presented as percentages in our tables. When

VAN ALLEN ET AL.

526

Diamood Gqaph Qu,riat wit'! lilO% E'rorBa,.·· E�e (r"' 100

m

UAI2001

=50}

Analyiic &r11 •
Qyery�II)OnM +
Mon'le Carlo 81.1'1 D
..•

11

•..

lo
.,
i

•..

�-1

..•

-2

01

QO

02

Figure 2:

Results for the Diamond Graph

We studied the following inferential patterns in the dia­
mond graph (Figure I):
Pr{Xl

=

1 IE>}

Q2 = Pr{X1

=

1IX2

=

1 IX2X3

Qi

=

=

=

el,ll()
11
=

E>}

=

1,

E>}

Q3

=

Pr{Xt

Q4
Qs

=

IIX1 = 1, E>}
Pr{X2X3
1 IX4
11 E>}
Pr{X1

Q6

=

=

=

-.2

1

Standard Normal Ouanmea

-1

0

2

(B) QQ-plot showing relation to Normal
J E {10%, 20%, 30%, 40% }. The resulting validity es­
timates are listed in Table 2. Each cell in the table is an
average of 30 values.

Figure 2(A) shows the error-bars returned by our approx­
imation, and also the Monte Carlo system, on a random
network posterior, for the error-bars for 90% credible inter­
vals. We see the two methods give similar answers.
Figure 2(B) uses a quantile-quantile (QQ) plot to address
the validity of the normality assumption, independently of
the linear approximation. Each "line" in this figure corre­
sponds to z-scores of the 100 query responses generated
by our Monte Carlo simulation, plotted against standard
normal quantiles. This figure shows six such lines, each
corresponding to a single query in { Q1, . .. , Q6}, given a
sample of size m 10. A straight-line would correspond
to data produced by a "perfect" normal distribution; we see
each dataset is close. (Of course, this is only suggestive; the
real proof comes first from Theorem 1, and then from the
data (e.g., Table 2) which demonstrates that our approach,
which assumes normality, produces reasonable results.)
=

8�·'1' 9'·'1°
La e2,lla e,,,,o
=

=

e2.'1' e3,qt e,,,IO

La e2,tla e3,t!a e,,,,o

82.111 83,111

=

=

Lb1c e4,t!b,c e2.bll es.c11 e,,,IO
L,.,b,c e4,llb,c e2,bla e3,cla e,,,,o

=

Pr{X4

=

-3

(A) Examples of Error Bars;

viewing these values, it is helpful to have a gold standard
for comparison. Consider the validity estimate 1.& - 51 for
a single ..:l. The minimum expected value is obtained when
..:l == S; i.e., when 100.& has the Binomial(lOO, S) distribu­
tion. Table 1 presents means and standard deviations under
these ideal circumstances. Now suppose a validity estimate
is obtained by averaging k independent terms. Its standard
deviation is typically greater than the value Std.Dev./ v'k
suggested by Table 1 because there is usually variation in
the underlying � values.
4.1

""""

1IXt

=

1, E>}

=

L:o,c 84,tlb,c 82,olt 83,cll

The six queries cover a range of different inferential pat­
terns. The first is basically a "sanity check", as it is a triv­
ial inference; the fourth is also straightforward, although it
does involve a multiplication. The sixth is slightly more
complex, but it is still only a summation of a set of prod­
ucts. The remaining queries involve divisions of increas­
ingly complicated expressions.
For each m E {10, 20, 30, 40} , we carried out 30 trials
of the form: (1) generate E> from a uniform Dirichlet
prior distribution, (2) generate a training sample of size m
based on E> and use the result to obtain a posterior dis­
tribution, (3) generate 100 Monte Carlo replicates from
the posterior distribution and use these to obtain an esti­
mate 6. for each pair (Q, J), for Q E {QI. ... , Q6} and

4.2

Results for Alarm Network

The Alarm network [BSCC89] is a benchmark network
based on a medical diagnosis domain, commonly used in
belief network studies. The network variables are all dis­
crete, but many range over 3 or more values. The network
includes a CPtable for each node; i.e., a particular 0 is spec­
ified.
Table 3 summarizes the results for experiments on the
Alarm network, where we varied both J and m. For each
m, we generated a single random sample of size m from
0, and used this to determine a posterior distribution (as­
suming a uniform prior). Validity estimates were obtained
by averaging over randomly chosen queries. The queries
Pr{H =hIE= e, 0} were chosen by determining an as­
signment H = h to one randomly chosen query variable,
and assignments E = e to five randomly chosen evidence
variables. (Here, we used [HC91] to determine which vari-

527

VAN ALLEN ET AL.

UAI2001

Table 4: Results for Random Networks
#E
#H
2
3
4
5

Table 2: Results for Diamond Graph
m

Ql

Q2

10

2.37

2.77

20

2.67

3.33

30
40

2.60
2.60

3.03
2.97

10
20
30

3.50
4.60
2.90

5.00
6.27
5.07

40

4.07

5.27

3.97
5.20
4.50
4.90

5.63

4.70

30
40

3 . 70
5.13

7.20
5.80

3.33
3.90

6 = 20%
4.87
4.70
7.03
5.13
5.97
4.43
4.93
4.93

6 = 40%
7.53
5.33
9.27
5.73
6.47
5.00
6.73
5.97

3.63

30
40

2.00

3.10

6.97

10

4.33
4 .73

6 = 10%
3.27
3.10
3.37
2.50
3.13
2.40

6 = 30%
7.23
6.10
11.13
6.83
7.20
5.47
6.63
6.03

20

10
20

Q4

Q3

Q5

Q6

2.20
2.90
2.77

3.93
3 .50
3.70
2.90

3.43
4.03

5.57
4.53

3.97
4;03

4.87
3.87

2.60

5.13
5.30

6.27

3.50
4.27

5.30
4.27

1
2
3
4
5

1
2

3
4
5

6.03

4.40

6.63

5.20
4.27
4.43

5.97
4.97
4.47

1
2
3
4

5

fJ
10%

20%

30%

40%

50

2.47

4.37

4.48

4.07

100
150

2.66
3.04

4.95
5.35

5.97
6.45

4.87
5.66

200

2.65

4.80

5.43

5.42

abies could be query as opposed to evidence variables.)
Some or all of the evidence variables might have had no
effect on the query variable, others might have had a pro­
found effect. Each cell in Table 3 represents an average
from 100 queries on a single posterior distribution.

4.3

Results for Random Networks

Although random networks tend not to reflect typical (or
natural) domains, they complement more focussed studies
by exposing methods to a wide range of inputs and help to
support claims of generality. We carried out experiments
on networks with 10 binary variables and 20 links, gener­
ating gold models from a uniform prior distribution on e.
and generating random queries of various types. Here we
used sample size m = 1 00 throughout, and varied the type
of query. Table 4 displays the results of our experiments.
Each query was of the form Pr{H =hIE= e, 9}, with
varying dimensionalities forE and H. Let #E and #H
denote the number of variables comprising E and H, re­
spectively. Each cell of Table 4 is based on 100 trials: I 0
queries on 10 networks, with both structure and posterior
generated randomly.

2.72

2.59
2.49

2.50
2.59

2.57
2.72

2.26
2.53

0

6
3.06
3.60
3.50
4.12

4.16
3.63
4.16
4.46

4.67

5.76

0

4.4

4.56
5.64

5.07
6.31

6.63

7.85

8.17

30%
4.75
5.11
5.02
5.13

5.97

=

5.61

5.69

7.63

7.43
9.45

5.15
4.95

4.39
4.96
5.78
7.14

4.98

4.20

2.79

4.71

5.14

4.43
4.11

2.56
2.88

4.38

4.74

2

5

20%
4.41
4.19

3.00
2.45

=

4.7 8
4.28

1
3
4

= 10%
2.84
3.20
3.13
2.93
2.62
2.38
2.S4
2.58
2.61
3.05

4.17
4.46
4.12

0

Table 3: Results for Alarm Network
m

2.16

6.39
8.71

6.33
7.25
10.81

12.51

13.99

4.90
4.24

40%
4.56
5.00
4.81
5.44

4.62
4.52

4.80
6.47

5.95
8.36

7.14

8.95

13.05

=

5.62
6.43
7.12
10.99
16.15

Discussion

Our hypothesis was that our Bayesian error-bars algorithm
would be accurate for essentially all cases. We tried to falsify our hypothesis by varying the following experimental
factors:
•

Network structure (V, A)

•

Credibility level 1

•

Query type (Diamond network, Alarm)

•

Number of evidence variables (Random networks)

•

Number of query variables (Random networks)

-

c5

In no case did we observe a result where averagelto
81
exceeded 20%. In most cases, the validity estimate was less
than 8/3. As noted in Table 1, even if our error-bars were
exact, we would still get positive validity estimates due to
the variance in to about �- We therefore believe that these
results comfortably bound the expected error of our method
under the experimental conditions . None of the factors that
we manipulated had a profound effect. T he strongest ef­
fect, observed in Table 4, was that increasing the number
of variables assigned in a query tended to increase the er­
ror I� 81; see also [Kle96]. One possible explanation is
that, as #E and #H increase, the query function q tends
to become more complex, and the local linear approxima­
tion of q becomes less reliable. Another possibility is that
-

-

VAN ALLEN ET AL.

528

the query probability

Q

tends to become very small, mak­

UAI2001

bution over CPtables, but for different purposes. For exam­

ing the normal approximation less accurate. Further exper­

ple, Cooper and Herskovits [CH92] use it to compute the

iments could address this issue.

expected response to a query; by contrast, we also approx­

We found these results very encouraging.

Our method

appears to give reasonable error-bars for a wide range of
queries and network types.

This makes the technique a

promising addition to the array of data-analysis tools re­
lated to belief networks, especially as the algorithm is rea­
sonably efficient, (only) roughly doubling the computation
time per inference. W hile there may be pathological cases

imate the posterior variance in that response. Similarly,
while many BN-learning algorithms compute the posterior
distribution over CPtables [Hec98], most of these systems
seek a single set of CPtable entries that maximize the like­
lihood, which again is different from our task;

e.g.,

their

task is not relative to a specific query (but see [GGS97]).
Many other projects consider sensitivity analyses, provid­

where our method will not give reasonable results - per­

ing mechanisms for propagating ranges of CPtable values

haps because the local linear approximation and the asymp­

to produce a range in the response; cf., [BKRK97, Las95,

totic normality are far off the truth-

we

did not find such

CNKE93, DarOO]. W hile these papers assume the user is
explicitly specifying the range of a local CPtable value, our

cases in our experiments.

work considers the source of these variances based on a
Other Experiments:

We also ran a number of other ex­

periments. One set computed the average{ A

8}

data sample. This also means our system must propagate

scores

all of the "ranges"; most other analyses consider only prop­

in each situation, to determine if there was any systematic

agating a single range. The [DarOO] system is an excep­

-

bias. (Note this score differs from Equation 9 by not tak­

tion, as it can simultaneously produce all of the derivatives.

ing absolute values.) We found that our bounds were typi­

However, Darwiche does not consider our error-bar appli­

cally a bit too wide for most queries- i.e., we often found
the

1

-

a-interval included slightly more than

1

-

8 of the

cases. We are currently investigating this, to see if there are
straight-forward refinements we can incorporate.

cation, and so does not include the additional optimizations
we could incorporate.
Excluding the [DarOO] result, none of the other projects
provides an efficient way to compute that information.

We also computed error-bars based on the (incorrect!)

Also, some of those other papers focus on properties of

"complete structure" assumption, which implies the re­

this derivative - e.g., when it is

sponse will have a simple Dirichlet distribution; see Foot­

able entry.

note

ately from our expression (Equation 6). Finally, our anal­

2. We found that, as anticipated, the approach de­

scribed in this paper, using Equation 6, consistently out­

0

for some specific CPt­

Note that this information falls out immedi­

ysis holds for arbitrary structures; by constrast some other

performed that case, in that our approach was consistently

results (e.g., [CNKE93]) deal only with singly connected

closer to the Monte Carlo estimates.

networks (trees).

[VGHOl] discusses these results in detail.

It also inves­

Lastly, our analysis also connects to work on abstractions,

tigates techniques for dealing with extreme values, where

which also involves determining how influential a CPtable

the normal distribution may be sub-optimal.

entry is, with respect to a query, towards deciding whether

5

typically computational efficiency in computing that re­

to include a specific node or arc [GDSOl]. Their goal is

Related Work

sponse. By contrast, our focus is in computing the error­
Our results provide a way to compute the variance of

bars around the response, independent of the time required

a BN's response to a query,

to determine that result.

which depends on the

posterior distribution over the space of CPtable entries,
based on a data sample.
method" [BFH95]:

This is done using the "Delta

first determine the variance of each

CPtable row, then propagate this variance using a sensi­
tivity analysis

(i.e., the partial derivatives); see Equation 6.

Kleiter [Kle96] performs a similar computation; parts of his
analysis are more general, in that he considers incomplete

(1) discuss how to deal
structures, (2) show how to deal

data. However, he does not

with

general graphical

with

the correlations encountered with general Dirichlet distri­
butions, nor

(3)

provide an efficient way to compute this

information. Moreover, our empirical data provide addi­
tional evidence that the approximations inherent in this ap­
proach are appropriate, even for small sample sizes.
Several other researchers also consider the posterior distri-

6

Conclusion

Further Extensions:

Our current system has been im­

plemented, and works very effectively. There are several
obvious ways to extend it.

One set of extensions corre­

spond to discharging assumptions underlying Theorem

1:

computing error bars when the data was used to learn the
structure, as well as the parameters; dealing with param­
eters that are drawn from a distribution other than inde­
pendent Dirichlets, perhaps even variables that have con­
tinuous domains; dealing with a training sample whose
instances are

not completely specified.

Our work deals

with fully-parameterized CP tables. It would be interesting
to investigate techniques capable of dealing with CPtables

UAI2001

VAN ALLEN ET AL.

represented as, say, decision tree functions [BFGK96], etc.
Contributions:
Many real-world systems work by rea­
soning probabilistically, based on a given belief net modeL
When knowledge concerning model parameters is condi­
tioned on a random training sample, it is useful to view
the parameters as random variables; this characterizes our
uncertainty concerning the responses generated to specific
queries in terms of random variation. Bayesian error-bars
provide a useful summary of our current knowledge about
questions of interest, and so provide valuable guidance for
decision-making or learning.

This paper addresses the challenge of computing the error­
bars around a belief net's response to a query, from a
Bayesian perspective. We first motivated and formally de­
fined this task- finding the 100(1 - o)% credible interval
for a query response with respect to its posterior distribu­
tion, conditioned on a training sample. We then investi­
gated an application of the "Delta method" to derive these
intervals. This required determining both the covariance
matrix interrelating all of the parameters, and the derivative
of the query response with respect to each parameter. We
produced an effective system that computes these quanti­
ties, and then combines them to produce the error-bars.
The fact that our approximation is guaranteed to be cor­
rect in the limit does not mean it will work well in practice.
We therefore empirically investigated these claims, by test­
ing our system across a variety of different belief nets and
queries, and over a range of sample sizes and credibility
levels. We found that the method works well throughout.

Acknowledgements
We are grateful for the many comments and suggestions we
received from Adnan Darwiche and the anonymous review­
ers, and for the fairness of the UAI'0 1 programme chairs.
All authors greatfutly acknowledge the generous support
provided by NSERC, iCORE and Siemens Corporate Re­
search. Most of this work was done while the first author
was a student at the University of Alberta.


Belief Propagation (BP) is one of the most
popular methods for inference in probabilistic graphical models. BP is guaranteed to
return the correct answer for tree structures,
but can be incorrect or non-convergent for
loopy graphical models. Recently, several
new approximate inference algorithms based
on cavity distribution have been proposed.
These methods can account for the effect of
loops by incorporating the dependency between BP messages. Alternatively, regionbased approximations (that lead to methods
such as Generalized Belief Propagation) improve upon BP by considering interactions
within small clusters of variables, thus taking small loops within these clusters into account. This paper introduces an approach,
Generalized Loop Correction (GLC), that
benefits from both of these types of loop correction. We show how GLC relates to these
two families of inference methods, then provide empirical evidence that GLC works effectively in general, and can be significantly
more accurate than both correction schemes.

1. Introduction
Many real-world applications require probabilistic inference from some known probabilistic model (Koller
& Friedman, 2009). This paper will use probabilistic
graphical models, focusing on factor graphs (Kschischang et al., 1998), that can represent both Markov
Networks and Bayesian Networks. The basic challenge of such inference is marginalization (or maxmarginalization) over a large number of variables. For
discrete variables, computing the exact solutions is
Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012.
Copyright 2012 by the author(s)/owner(s).

NP-hard, typically involving a computation that is exponential in the number of variables.
When the conditional dependencies of the variables
form a tree structure (i.e., no loops), this exact inference is tractable, and can be done by a message passing procedure, Belief Propagation (BP) (Pearl, 1988).
The Loopy Belief Propagation (LBP) system applies
BP repeatedly to graph structures that are not trees
(called “loopy graphs”); however, this provides only an
approximately correct solution (when it converges).
LBP is related to the Bethe approximation to free
energy (Heskes, 2003), which is the basis for minimization of more sophisticated energy approximations and provably convergent methods (Yedidia et al.,
2005; Heskes, 2006; Yuille, 2002). A representative
class of energy approximations is the region-graph
methods (Yedidia et al., 2005), which deal with a
set of connected variables (called “regions”); these
methods subsume both the Cluster Variation Method
(CVM) (Pelizzola, 2005; Kikuchi, 1951) and the Junction Graph Method (Aji & McEliece, 2001). Such
region-based methods deal with the short loops of the
graph by incorporating them into overlapping regions
(see Figure 1(a)), and perform exact inference over
each region. Note a valid region-based methods is exact if its region graph has no loops.
A different class of algorithms, loop correction methods, tackles the problem of inference in loopy graphical
models by considering the cavity distribution of variables. A cavity distribution is defined as the marginal
distribution on Markov blanket of a single (or a cluster
of) variable(s), after removing all factors that depend
on those initial variables. Figure 1(b) illustrates cavity distribution, and also shows that the cavity variables can interact. The key observation in these methods is that, by removing a variable xi in a graphical
model, we break all the loops that involve the variable xi , resulting in a simplified problem of finding

A Generalized Loop Correction Method

o

j
13

w
5

12

v

i

L

I

T

K
4

Y
u

W

j

8

w

t
Z

J

v

L

K
4

Y
u

j
w

t
Z

J

v

m

3
k

1
s

W

L

I

T
5

i

2

S
m

3
k

1
s

W

i
I

T
5

9

o

2

S

7
m

3
k

1
s

o

6

2

S

14

J

K
4

Y
u

t
Z

10
11

Figure 1. Part of a factor graph, where circles are variables (circle labeled “i” corresponding to variable “xi ”) and squares
(with CAPITAL letters) represent factors. Note variables {xi , xk , xs } form a loop, as do {xk , xu , xt }, etc.
(a) An example of absorbing short loops into overlapping regions. Here, a region includes factors around each hexagon
and all its variables. Factor I and the variables xi , xj , xk appear in the three regions r1 , r2 , r3 . (Figure just shows index
α for region rα .) Region-based methods provide a way to perform inference on overlapping regions. (In general, regions
do not have to involve exactly 3 variables and 3 factors.)
(b) Cavity variables for xs are {xw , xj , xk , xu , xv }, shown using dotted circles. We define the cavity distribution
for xs by removing all the factors around this variable, and marginalizing the remaining factor-graph on dotted circles.
Even after removing factors {T, Y, W }, the variables xv , xw , and xj , xk , xu still have higher-order interactions caused by
remaining factors, due to loops in the factor graph.
(c) Cavity region r1 = {j, s, k} includes variables shown in pale circles. Variables in dotted circles are the perimeter r1 .
Removing the “pale factors” and marginalizing the rest of network on r1 , gives the cavity distribution for r1 .

the cavity distribution. The marginals around xi can
then be recovered by considering the cavity distribution and its interaction with xi . This is the basis for
the loop correction schemes by Montanari & Rizzo’s
(2005) on pairwise dependencies over binary variables,
and also Mooij & Kappen’s (2007) extension to general
factor graphs – called Loop Corrected Belief Propagation (LCBP).
This paper defines a new algorithm for probabilistic
inference, called Generalized Loop Correction (GLC),
that uses a more general form of cavity distribution,
defined over regions, and also a novel message passing
scheme between these regions that uses cavity distributions to correct the types of loops that result from
exact inference over each region. GLC’s combination of loop corrections is well motivated, as regionbased methods can deal effectively with short loops in
the graph, and the approximate cavity distribution is
known to produce superior results when dealing with
long influencial loops (Mooij & Kappen, 2007).
In its simplest form, GLC produces update equations
similar to LCBP’s; indeed, under a mild assumption,
GLC reduces to LCBP for pairwise factors. In its general form, when not provided with information on cavity variable interactions, GLC produces results similar
to region-based methods. We theoretically establish
the relation between GLC and region-based approxi-

mations, for a limited setting.
Section 2 explains the notation, factor graph representation and preliminaries for GLC. Section 3 introduces
a simple version of GLC that works with regions that
partition the set of variables; followed by its extension to the more general algorithm. Section 4 presents
empirical results, comparing our GLC against other
approaches.

2. Framework
2.1. Notation
Let X = {X1 , X2 , . . . , XN } be a set of N discretevalued random variables, where Xi ∈ Xi . Suppose
their joint probability distribution factorizes into a
product of non-negative functions:
1 Y
P (X = x) :=
ψI (xI )
Z
I∈F

where each I ⊆ {1, 2, . . . , N } is a subset of the variable indices, and xI = {xi | i ∈ I} is the set of
values
Q in x indexed by the subset I. Each factor
ψI : i∈I Xi → [0, ∞) is a non-negative function, and
F is the collection of indexing subsets I for all the
factors ψI . Below we will use the term “factor” interchangeably with the function ψI and subset I, and the
term “variable” interchangeably for the value xi and
index i. Here Z is the partition function.

A Generalized Loop Correction Method

This model can be conveniently represented as a bipartite graph, called the factor graph (Kschischang et al.,
1998), which includes two sets of nodes: variable nodes
xi , and factor nodes ψI . A variable node xi is connected to a factor node ψI if and only if i ∈ I. We
use the notation N (i) to denote the neighbors of variable xi in the factor graph – i.e., the set of factors
defined by N (i) := {I ∈ F | i ∈ I}. To illustrate,
using Figure 1(a): N (j) = {I, T, S} and T = {j, s, w}.
Q
We use the shorthand ψA (x) :=
I∈A (xI ) to denote the product of factors in a set of factors A. For
marginalizing all possible values of x except the ith
variable,
notation:
X we define theX
f (x) :=
f (x).
x\i

xj ∈Xj ,j6=i

Similarly
for a set of variables r, we use the notation
P
to
denote
marginalization of all variables apart
x\r
from those in r.

r, is defined over
by r, as:
X Y
X the variables indexed
ψF \N (r) (x) =
ψI (xI )
P \r (x r ) ∝
x\

r

x\

r

I ∈N
/ (r)

Here the summation is over all variables but the ones
indexed by r.
In Figure 1(c), this is the distribution obtained by removing factors N (r1 ) = {I, T, Y, K, S, W } from the
factor gaph and marginalizing the rest over dotted circles, r1 .
The core idea to our approach is that each cavity region r can produce reliable probability distribution
over r, given an accurate cavity distribution estimate
over the surrounding variables r. Given the exact
cavity distribution P \r over r, we can recover the
exact joint distribution Pr over ⊕r by:
Y
Pr (x⊕r ) ∝ P \r (x r )ψN (r) (x) = P \r (x r )
ψI (xI ) .
I∈N (r)

2.2. Generalized Cavity Distribution
The notion of cavity distribution is borrowed from socalled cavity methods from statistical physics (Mézard
& Montanari, 2009), and has been used in analysis
and optimization of important combinatorial problems (Mézard et al., 2002; Braunstein et al., 2002).
The basic idea is to make a cavity by removing a variable xi along with all the factors around it, from the
factor graph (Figure 1(b)). We will use a more general
notion of regional cavity, around a region.
Definition A cavity region is a subset of variables r ⊆
{1, . . . , N } that are connected by a set of factors – i.e.,
the set of variable nodes r and the associated factors
N (r) := {N (i) | i ∈ r} forms a connected component
on the factor graph.
For example in Figure 1(a), the variables indexed
by r1 = {j, k, s} define a cavity region with factors
N (r1 ) = {I, T, Y, S, W, K}
Remark A “cavity region” is different from common
notion of region in region-graph methods, in that a
cavity region includes all factors in N (r) (and nothing
more), while common regions allow a factor I to be a
part of a region only if I ⊆ r.
The notation ⊕r := {i ∈ I | I ∈ N (r)} denotes
the cavity region r with its surrounding variables, and
r := ⊕r \ r denotes just the perimeter of the cavity
region r. In Figure 1(c), the dotted circles show the
indices r1 = {o, i, m, t, u, v, w} and their union with
the pale circles defines ⊕r1 .
Definition The Cavity Distribution, for cavity region

In practice, we can only obtain estimates P̂ \r (x r )
of the true cavity distribution P \r (x r ). However,
suppose we have multiple cavity regions r1 , r2 , . . . , rM
that collectively cover all the variables {x1 , . . . , xN }.
If rp intersects with rq , we can improve the estimate of P̂ \rp (x rp ) by enforcing marginal consistency
of P̂rp (x⊕rp ) with P̂rq (x⊕rq ) over the variables in
their intersection. This suggests an iterative correction scheme that is very similar to message passing.
In Figure 1(a), let each hexagon (over variables
and factors) define a cavity region, here r1 , . . . , r5 .
Note r1 can provide good estimates over {j, s, k},
given good approximation to cavity distribution over
{o, i, m, t, u, v, w}. This in turn can be improved by
neighboring regions; e.g., r2 gives a good approximation over {i, o}, and r3 over {i, m}. Starting from an
\r
initial cavity distribution P̂0 α , for each cavity region
α ∈ {1, . . . , 14}, We perform this improvement for all
cavity regions, in iterations until convergence.
\r

When we start with a uniform cavity distribution P̂0 p
for all regions, the results are very similar to those of
CVM. The accuracy of this approximation depends on
\r
the accuracy of the initial P̂0 p .
Following Mooij (2008), we use variable clamping to
estimate higher-order interactions in r: Here, we estimate the partition function Zx r after removing factors in N (r) and fixing x r to each possible assignment. Doing this calculation, we have P̂ \r (x r ) ∝
Zx r . In our experiments, we use the approximation
to the partition function provided using LBP. However
there are some alternatives to clamping: conditioning
scheme Rizzo et al. (2007) makes it possible to use

A Generalized Loop Correction Method

any method capable of marginalization for estimation
of cavity distribution (clamping requires estimation of
partition function). It is also possible to use techniques
in answering joint queries for this purpose (Koller &
Friedman (2009)).
Using clamping for this purpose also means that, if the
resulting network, after clamping, has no loops, then
P̂r (x⊕r ) is exact – hence GLC produces exact results
if for every cluster r, removing ⊕r results in a tree.

sistency condition:
X
X
P̂rp(x⊕rp)ψN(rp)∩N(rq) (x)−1= P̂rq(x⊕rq)ψN(rp)∩N(rq) (x)−1 ,
x\ rp,q

x\ rp,q

(2)

which we can use to derive update equations for mq→p .
Starting
X from the LHS of Eqn (2),

P̂rp (x⊕rp )ψN (rp )∩N (rq ) (x)−1

∝

rp,q
Xx\ \r
P̂0 p (x

rp )ψN (rp )\N (rq ) (x)

3.1. Simple Case: Partitioning Cavity Regions
To introduce our approach, first consider a simpler case where the cavity regions r1 , . . . , rM form
a (disjoint and exhaustive) partition of the variables
{1, . . . , N }.
Let rp,q := ( rp ) ∩ rq denote the intersection of the
perimeter rp of rp with another cavity region rq .
(Note rp,q 6= rq,p ). As r1 , . . . , rM is a partition,
each perimeter rp is a disjoint union of rp,q for
q = 1 . . . M (some of which might be empty if rp and rq
are not neighbors). Let N b(p) denote the set of regions
q with rp,q 6= ∅. We now consider how to improve the
cavity distribution estimate over rp through update
messages sent to each of the rp,q .
In Figure 1(a), the regions r2 , r4 , r5 , r7 , r11 , r14 form
a partitioning. Here, r2 with {m, k, s, w} ⊂ r2 , receives updates over r2,7 = {m} from r7 and updates over r2,4 = {k} from r4 . This last update
P
P
ensures x\{k} P̂r2 (x⊕r2 ) = x\{k} P̂r4 (x⊕r4 ). Towards enforcing this equality, we introduce a message
m4→2 (x r2,4 ) into distribution over ⊕r2 .
Here, the distribution over ⊕rp becomes: P̂rp (x⊕rp ) ∝
\rp

P̂0

(x

rp )ψN (rp ) (x⊕rp )

Y

mq→p (x

rp,q ),

(1)

q∈N b(p)

where P̂rp denotes our estimate of the true distribution Prp .
The messages mq→p can be recovered by considering marginalization constraints. When rp and rq
are neighbors, their distributions P̂rp (x⊕rp ) and
P̂rq (x⊕rq ) should satisfy
X

X

P̂rp (x⊕rp ) =

x\⊕rp ∩⊕rq

P̂rq (x⊕rq ).

x\⊕rp ∩⊕rq

As

∝ mq→p (x

rp,q)

X

\r

P̂0 p(x

rp)ψN(rp)\N(rq) (x)

Y

rp,q0 )

mq0 →p (x

rp,q0).

0

x\ rp,q

q ∈N b(p)
q 0 6=q

Setting this proportional to the RHS of Eqn (2), we
have the update equation
mnew
q→p (x

rp,q )

P̂rq (x⊕rq )ψN (rp )∩N (rq ) (x)−1

P
∝

x\ rp,q

P

\rp

P̂0

x\ rp,q

P
∝

(x

P̂rq (x⊕rq )ψN (rp )∩N (rq ) (x)−1

x\ rp,q

P

Q

mq→p (x rp,q0 )
rp )ψN (rp )\N (rq ) (x)
q 0 ∈N b(p)
q 0 6=q

P̂rp (x⊕rp )ψN (rp )∩N (rq ) (x)−1

mq→p (x

rp,q )

(3)

x\ rp,q

The last line follows from multiplying the numerator
and denominator by the current version of the message
mq→p . At convergence, when mq→p equals mnew
q→p , the
consistency constraints are satisfied. By repeating this
update in any order, after convergence, the P̂r (x⊕r )s
represent approximate marginals over each region.
The following theorem stablishes the relation between
GLC and CVM in a limited setting.
Theorem 1 If the cavity regions partition the variables and all the factors involve no more than 2 variables, then any GBP fixed point of a particular CVM
construction (details in Appendix A) is also a fixed
point for GLC, starting from uniform cavity distribu\r
tions P̂0 = 1. (Proof in Appendix A.)
Corollary 1 If the factors have size two and there are
no loops of size 4 in the factor graph, for single variable
partitioning with uniform cavity distribution, any fixed
points of LBP can be mapped to fixed points of GLC.

x\⊕rp ∩⊕rq

We can divide both sides by the factor product
ψN (rp )∩N (rq ) (x), as the domain of the factors in
N (rp ) ∩ N (rq ) is completely contained in ⊕rp ∩ ⊕rq
and independent of the summation. Hence we have
X

mq0 →p (x

q 0 ∈N b(p)

x\ rp,q

3. Generalized Loop Correction

Y

P̂rp(x⊕rp)
ψN(rp)∩N(rq) (x)

=

X
x\⊕rp ∩⊕rq

P̂rq(x⊕rq)
ψN(rp)∩N(rq) (x)

rp,q ⊂ ⊕rp ∩ ⊕rq , this implies the weaker con-

Proof If there are no loops of size 4 then no two factors have identical domain. Thus the factors are all
maximal and GBP applied to CVM with maximal factor domains, is the same as LBP. On the other hand
(refering to CVM construction of Appendix A) under
the given condition, GLC with single variable partitioning shares the fixed points of GBP applied to CVM

A Generalized Loop Correction Method

with maximal factors. Therefore GLC shares the fixed
points of LBP.

{ o, i}

{ i , m}

Theorem 2 If all factors have size two and no two
factors have the same domain, GLC is identical to
LCBP under single variable partitioning.
Proof Follows from comparison of two update equations – i.e., Eqn (3) and Eqn (5) in (Mooij & Kappen,
2007)– under the assumptions of the theorem.
3.2. General Cavity Regions
When cavity regions do not partition the set of variables, the updates are more involved. As the perimeter
rp is no longer partitioned, the rp,q ’s are no longer
disjoint.
For example in Figure 1, for r1 we have r1,2 = {o, i},
r1,3 = {i, m}, r1,4 = {t, u}, r1,5 = {v, w} and also
r1,6 = {i}, r1,7 = {m}, r1,8 = {m, t}, r1,9 = {t},
etc. This means xi appears in messages m2→1 , m3→1
and m6→1 .
Directly adopting the correction formula for P̂r in
Eqn (1) as a product of messsages over rp,q could
double-count variables. To avoid this problem, we
adopt a strategy similar to CVM to discount extra
contributions from overlapping variables in rp . For
each cavity region rp , we form a rp -region graph
(Figure 2) with the incoming messages forming the
distributions over top regions. For computational reasons, we only consider maximal rp,q domains.1 here,
this means dropping m6→1 as r1,6 ⊂ r1,2 and so on.
Our region-graph construction is similar to
CVM (Pelizzola, 2005) – i.e., we construct new
sub-regions as the intersection of rp,q ’s, and we
repeat this recursively until no new region can be
added. We then connect each sub-region to its
immediate parent. Figure 2 shows the r1 -region
graph for the example of Figure 1(a). If the cavity
regions are a partition, the rp -region graph includes
only the top regions. Below we use Rp to denote
the rp -region graph for rp ; RO
p to denote its top
(outer) regions; and brp (xρ ) to denote the belief over
region ρ in rp -region graph. For top-regions, the
initial belief is equal to the basic messages obtained
using Eqn (3).
Next we assign “counting numbers” to regions, in
a way similar to CVM: top regions are assigned
cn( rp,q ) = 1, and each sub-region ρ is assigned using
1

This does not noticably affect the accuracy in our experiments. When using uniform cavity distributions, the
results are identical.

{o}

{ m, t}

{ t, u}

{m}

{ v, w}

{t}

Figure 2. The r1 -region-graph consisting of all the messages to r1 . The variables in each region and its counting
number are shown. The upward and downward messages
are passed along the edges in this r1 -region-graph.

the Möbius formula:
X
cn(ρ) := 1 −

ρ0 ∈A(ρ)

cn(ρ0 )

where A(ρ) is the set of ancestors of ρ.
We can now define the belief over cavity regions rp as:
\rp

P̂rp (x⊕rp ) ∝ P̂0

rp )ψN (rp ) (x⊕rp )

(x

Y

brp (xρ )cn(ρ) (4)

ρ∈ Rp

This avoids any double-counting of variables, and reduces to Eqn (1) in the case of partitioning cavity regions.
To apply Eqn (4) effectively, we need to enforce
marginal consistency of the intersection regions with
their parents, which can be accomplished via message
passing in a downward pass, Each region ρ0 sends
to each of its child ρ, its marginal over the child’s
variables:
X
µρ0 →ρ (xρ ) :=
brp (xρ0 )
x\ ρ

Then set the belief over each child region to be the
geometric average Y
of the incoming messages:
1
brp (xρ ) :=
µρ0 →ρ (xρ ) |pr(ρ)|
0
ρ ∈pr(ρ)

The downward pass updates the child regions in Rp \
RO
p . We update the beliefs at the top regions using
a modified version of Eqn (3): brp (x rp,q ) ∝
P

P̂rq(x⊕rq)ψN(rq)∩N(rp)(x⊕rq)−1

x\ rp,q

P

P̂rp(x⊕rp)ψN(rp)∩N(rq)(x⊕rp)−1

f
bef
rp (x

rp,q)

cn(ρ)

,

(5)

x\ rp,q

rp,q ∈

for all top regions
f
Here bef
rp (x
rp,q :
f
bef
rp (x

rp,q )

rp,q )

RO
p.

is the effective old message over

=

X
x\ rp,q

Y
ρ∈ Rp

brp (xρ )

That is, in the update equation, we need the calculation of the new message to assume this value as the
old message from q to p. This marginalization is important because it allows the belief at the top region

A Generalized Loop Correction Method

brp (x rp,q ) to be influenced by the beliefs brp (xρ ) of
the sub-regions after a downward pass. It enforces
marginal consistency between the top regions, and at
f
convergence we have bef
rp (x rp,q ) = brp (x rp,q ). Notice also Eqn (5) is equivalent to the old update Eqn (3)
in the partitioning case.
To calculate this marginalization more efficiently,
GLC uses an upward pass in the rp -region-graph.
Starting from the parents of the lowest regions, we def
fine bef
rp (xρ ) as:
f
bef
rp (xρ0 ) := brp (xρ0 )

Y
ρ∈ch(ρ0 )

f
bef
r (xρ )
µρ→ρ0 (xρ )

Returning to the example, the previous text provides
a method to update P̂r1 (x⊕r1 ). GLC performs this
for the remaining regions as well, and then iterates
the entire process until convergence – i.e., until the
change in all distributions is less than a threshold.

4. Experiments
This section compares different variations of our
method against LBP as well as CVM, LCBP and
TreeEP (Minka & Qi, 2003) methods, each of which
performs some kind of loop correction. For CVM, we
use the double-loop algorithm of (Heskes, 2006), which
is slower than GBP but has better convergence properties. All methods are applied without any damping.
We stop each method after a maximum of 1E4 iterations or if the change in the probability distribution
(or messages) is less than 1E-9. We report the time
in seconds and the error for each method as the averagePof absolute error in single variable marginals –
i.e., xi ,v |P̂ (xi = v)−P (xi = v)|. For each setting, we
report the average results over 10 random instances of
the problem. We experimented with grids, 3-regular
random graphs, and the ALARM network as typical
benchmark problems.2
Both LCBP and GLC can be used with a uniform
initial cavity or with an initial cavity distribution estimated via clamping cavity variables. In the experiments, full and uniform refer to the kind of cavity
distribution used. We use GLC to denote the partitioning case, and GLC+ when overlapping clusters
of some form are used. For example, GLC+(Loop4,
full) refers to a setting with full cavity that contains
all overlapping loop clusters of length up to 4. If a
factor does not appear in any loops, it forms its own
cluster. The same form of clusters are used for CVM.
2
The evaluations are based on implementation in libdai
inference toolbox (Mooij, 2010).

Figure 4. Time vs error for 3-regular Ising models with local field and interactions sampled from a standard normal.
Each method in the graph has 10 points, each representing
an Ising model of different size (10 to 100 variables).

4.1. Grids
We experimented with periodic Ising grids in which
xi ∈ {−1, +1} is a binary variable and the probability distribution of a setting when xi and xj
are connected
inPthe graph is given by P (x) ∝
P
exp( i θi xi + 12 i,j∈I Ji,j xi xj ) where Ji,j controls
variable interactions and θi defines a single node potential – a.k.a. a local field. In general, smaller local
fields and larger variable interactions result in more
difficult problems. We sampled local fields independently from N (0, 1) and interactions from N (0, β 2 ).
Figure 3(left) summarize the results for 6x6 grids for
different values of β.
We also experimented with periodic grids of different
sizes, generated by sampling all factor entries independently from N (0, 1). Figure 3(middle) compares the
computation time and error of different methods for
grids of sizes that range from 4x4 to 10x10.
4.2. Regular Graphs
We generated two sets of experiments with random
3-regular graphs (all nodes have degree 3) over 40
variables. Here we used Ising model when both local
fields and couplings are independently sampled from
N (0, β 2 ). Figure 3(right) show the time and error for
different values of β. Figure 4 shows time versus error
for graph size between 10 to 100 nodes for β = 1. For
larger βs, few instances did not converge within allocated number of iterations. The results are for cases
in which all methods converged.

A Generalized Loop Correction Method

Figure 3. Average Run-time and accuracy for: (Left) 6x6 spinglass grids for different values of β. Variable interactions
are sampled from N (0, β 2 ), local fields are sampled from N (0, 1). (Middle) various grid-sizes: [5x5, . . . , 10x10]; Factors
are sampled from N (0, 1). (Right) 3-regular Ising models with local field and interactions sampled from N (0, β 2 ).

Table 1. Performance of varoius methods on Alarm
Method
Time(s) Avg. Error
LBP
3.00E-2
8.14E-3
TreeEP
1.00E-2
2.02E-1
CVM (Loop3)
5.80E-1
2.10E-3
CVM (Loop4) 7.47E+1
6.35E-3
CVM (Loop5) 1.22E+3
1.21E-2
CVM (Loop6) 5.30E+4
1.29E-2
LCBP (Full) 3.87E+1
1.07E-6
GLC+ (Factor, Uniform)
6.69E 0
3.26E-4
GLC+ (Loop3, Uniform)
6.71E 0
4.58E-4
GLC+ (Loop4, Uniform) 4.65E+1
3.35E-4
GLC+ (Factor, Full) 1.23E+3
1.00E-9
GLC+ (Loop3, Full) 1.36E+3
1.00E-9
GLC+ (Loop4, Full) 1.79E+3
1.00E-9

4.3. Alarm Network

lacking in general single-loop GBP implementations.
GLC’s time complexity (when using full cavity, and
using LBP to estimate the cavity distribution) is
O(τ M N |X |u + λM |X |v )), where λ is the number of
iterations of GLC, τ is the maximum number of iterations for LBP, M is the number of clusters, N
is the number of variables, u = maxp |
rp | and
v = maxp | ⊕ rp |. Here the first term is the cost of
estimating the cavity distributions and the second is
the cost of exact inference on clusters. This makes
GLC especially useful when regional Markov blankets
are not too large.

5. Conclusions

Alarm is a Bayesian network with 37 variables and
37 factors. Variables are discrete, but not all are binary, and most factors have more than two variables.
Table(1) compares the accuracy versus run-time of different methods. GLC with factor domains as regions –
i.e., rp = I for I ∈ F – and all loopy clusters produces
exact results up to the convergence threshold.

We introduced GLC, an inference method that provide accurate inference by utilizing the loop correction schemes of both region-based and recent cavitybased methods. Experimental results on benchmarks
support the claim that, for difficult problems, these
schemes are complementary and our GLC can successfully exploit both. We also believe that our scheme
motivates possible variations that can also deal with
graphical models with large Markov blankets.

4.4. Discussions

6. Acknowledgements

These results show that GLC consistently provides
more accurate results than both CVM and LCBP, although often at the cost of more computation time.
They also suggest that one may not achieve this tradeoff between time and accuracy simply by including
larger loops in CVM regions. When used with uniform
cavity, the performance of GLC (specifically GLC+)
is similar to CVM, and GLC appears stable, which is

We thank the anonymous reviewers for their excellent detailed comments. This research was partly funded by
NSERC, Alberta Innovates – Technology Futures (AICML)
and Alberta Advanced Education and Technology.



A new approach to maximum likelihood learning of discrete graphical models
and RBM in particular is introduced. Our method, Perturb and Descend (PD) is
inspired by two ideas (I) perturb and MAP method for sampling (II) learning by
Contrastive Divergence minimization. In contrast to perturb and MAP, PD leverages training data to learn the models that do not allow efficient MAP estimation.
During the learning, to produce a sample from the current model, we start from a
training data and descend in the energy landscape of the “perturbed model”, for a
fixed number of steps, or until a local optima is reached. For RBM, this involves
linear calculations and thresholding which can be very fast. Furthermore we show
that the amount of perturbation is closely related to the temperature parameter and
it can regularize the model by producing robust features resulting in sparse hidden
layer activation.

1 Introduction
The common procedure in learning a Probabilistic Graphical Model (PGM) is to maximize the likelihood of observed data, by updating the model parameters along the gradient of the likelihood. This
gradient step requires inference on the current model, which may be performed using deterministic
or a Markov Chain Monte Carlo (MCMC) procedure [1]. Intuitively, the gradient step attempts to
update the parameters to increase the unnormalized probability of the observation, while decreasing
the sum of unnormalized probabilities over all states –i.e., the partition function. The first part of
the update is known as positive phase and the second part is referred to as the negative phase. An
efficient alternative is Contrastive Divergence (CD) [2] training, in which the negative phase only
decreases the probability of the configurations that are in the vicinity of training data. In practice
these neighboring states are sampled by taking few steps on Markov chains that are initialized by
training data.
Recently Perturbation methods combined with efficient Maximum A Posteriori (MAP) solvers were
used to efficiently sample PGMs [3, 4, 5]. Here a basic idea from extreme value theory is used, which
states that the MAP assignments for particular perturbations of any Gibbs distribution can replace
unbiased samples from the unperturbed model [6]. In practice, however, models are not perturbed in
the ideal form and approximations are used [4]. Hazan et al. show that lower order approximations
provide an upper bound on the partition function [7]. This suggest that perturb and MAP sampling
procedure can be used in the negative phase to maximize a lower bound on the log-likelihood of
the data. However, this is feasible only if efficient MAP estimation is possible (e.g., PGMs with
submodular potentials [8]), and even so, repeated MAP estimation at each step of learning could be
prohibitively expensive.
Here we propose a new approach closely related to CD and perturb and MAP to sample the PGM
in the negative phase of learning. The basic idea is to perturb the model and starting at training
1

data, find lower perturbed-energy configurations. Then use these configurations as fantasy particles
in the negative phase of learning. Although this scheme may be used for arbitrary discrete PGMs
with and without hidden variables, here we consider its application to the task of training Restricted
Boltzmann Machine (RBM) [9].

2 Background
2.1 Restricted Boltzmann Machine
RBM is a bipartite Markov Random Field, where the variables x = {v, h} are partitioned into
visible v = [v1 , . . . , vn ] and hidden h = [h1 , . . . , hm ] units. Because of its representation power
([10]) and relative ease of training [11], RBM is increasing used in various applications. For example
as generative model for movie ratings [12], speech [13] and topic modeling [14]. Most importantly
it is used in construction of deep neural architectures [15, 16].
RBM models the joint distribution over hidden and visible units by
P(h, v|θ) =

1 −E(h,v,θ)
e
Z(θ)

P
where Z(θ) = h,v e−E(h,v,θ) is the normalization constant (a.k.a. partition function) and E is the
energy function. Due to its bipartite form, conditioned on the visible (hidden) variables the hidden
(visible) variables in an RBM are independent of each other:
Y
Y
P(h|v, θ) =
P(hj |v, θ) and P(v|h, θ) =
P(vi |h, θ)
(1)
1≤j≤m

1≤i≤n

Here we consider the energy function of binary RBM, where hj , vi ∈ {0, 1}



 X
X
X
T
T
T
E(v, h, θ) = −
vi Wi,j hj +
ai vi +
bj hj = − v Wh + a v + b h
1≤i≤n

1≤i≤n
1≤j≤m

1≤j≤m

The model parameter θ = (W, a, b), consists of the matrix of n × m real valued pairwise interactions W, and local fields (a.k.a. bias terms) a and b. The marginal over visible units is
1 X
P(v|θ) =
P(v, h|θ)
Z(θ)
h

Given a data-set D = {v(1) , . . . , v(N ) }, maximum-likelihood learning of the model seeks the maximum of the averaged log-likelihood:
1 X
ℓ(θ) =
log(P(v(k) |θ))
(2)
N (k)
v ∈D

Y 
X (k)
1 X
1 + exp(
(3)
vi Wi,j ) − log(Z(θ))
=−
N (k)
v

∈D 1≤j≤m

1≤i≤n

Simple calculations gives us the derivative of this objective wrt θ:
h
i
1 X
(k)
∂ℓ(θ)/∂Wi,j =
EP(hj |v(k) ,θ) vi hj − EP(vi ,hj |θ) [ vi hj ]
N (k)
v

1
∂ℓ(θ)/∂ai =
N
∂ℓ(θ)/∂bj =

1
N

∈D

X

vi

X

EP(hj |v(k) ,θ) [ hj ] − EP(hj |θ) [ hj ]

(k)

− EP(vi |θ) [ vi ]

v(k) ∈D

v(k) ∈D

2

where the first and the second terms in each line correspond to positive and negative phase respectively. It is easy to calculate P(hj |v(k) , θ), required in the positive phase. The negative phase,
however, requires unconditioned samples from the current model, which may require long mixing
of the Markov chain.
Note that the same form of update appears when learning any Markov Random Field, regardless of
the form of graph and presence of hidden variables. In general the gradient update has the following
form
∇θI ℓ(θ) = ED,θ [ φI (xI ) ] − Eθ [ φI (xI ) ]

(4)

where φI (xI ) is the sufficient statistics corresponding to parameter θI . For example the sufficient
statistics for variable interactions Wi,j in an RBM is φi,j (vi , hj ) = vi hj . Note that θ in calculating
the expectation of the first term appears only if hidden variables are present.
2.2 Contrastive Divergence Training
In estimating the second term in the update of eq(4), we can sample the model with the training data
in mind. To this end, CD samples the model by initializing the Markov chain to data points and
running it for K steps. This is repeated each time we calculate the gradient. At the limit of K → ∞,
this gives unbiased samples from the current model, however using only few steps, CD performs
very well in practice [2]. For RBM this Markov chain is simply a block Gibbs sampler with visible
and hidden units are sampled alternatively using eq(1).
It is also possible to initialize the chain to the training data at the beginning of learning and during
each calculation of gradient run the chain from its previous state. This is known as persistent CD
[17] or stochastic maximum likelihood [18].
2.3 Sampling by Perturb and MAP
Assuming that it is possible to efficiently obtain the MAP assignment in an MRF, it is possible to use
perturbation methods to produce unbiased samples. These samples then may be used in the negative
phase of learning.
e
Let E(x)
= E(x) − ǫ(x) denote the perturbed energy function, where the perturbation for each
x is a sample from standard Gumbel distribution ǫ(x) ∼ γ(ε) = exp(ε − exp(−ε)). Also let
e
e denote the perturbed distribution. Then the MAP assignment arg max P(x)
e
P(x)
∝ exp(−E)
is
x
an unbiased sample from P(x). This means we can sample P(x) by repeatedly perturbing it and
finding the MAP assignment. To obtain samples from a Gumbel distribution we transform samples
from uniform distribution u ∼ U(0, 1) by ǫ ← log(− log(u)).
The following lemma clarifies the connection between Gibbs distribution and the MAP assignment
in the perturbed model.
e
Lemma 1 ([6]). Let {E(x)}x∈X and E(x) ∈ ℜ. Define the perturbed values as E(x)
= E(x) −
ǫ(x), when ǫ(x) ∼ γ(ε) ∀x ∈ X are IID samples from standard Gumbel distribution. Then
exp(−E(b
x))
)
exp(−E(y))
y∈X

e
b) = P
P r(argx∈X max{−E(x)}
=x

(5)

Since the domain X , of joint assignments grows exponentially with the number of variables, we
can not find thePMAP assignment efficiently. As an approximation we may use fully decomposed
noise ǫ(x) =
i ǫ(xi ) [4]. This corresponds to adding a Gumbel noise to each assignment of
unary potentials. In the case of RBM’s parametrization, this corresponds to adding the difference
of two random samples from a standard Gumbel distribution (which is basically a sample from a
logistic distribution) to biases (e.g., e
ai = ai + ǫ(vi = 1) − ǫ(vi = 0)). Alternatively a second order
approximation may perturb a combination of binary and unary potentials such that each variable is
included once (Section 3.2)
3

3 Perturb and Descend Learning
Feasibility of sampling using perturb and MAP depends on availability of efficient optimization
procedures. However MAP estimation is in general NP-hard [19] and only a limited class of MRFs
allow efficient energy minimization [8]. We propose an alternative to perturb and MAP that is
suitable when inference is employed within the context of learning. Since first and second order
perturbations in perturb and MAP, upper bound the partition function [7], likelihood optimization
using this method is desirable (e.g., [20]). On the other hand since the model is trained on a data-set,
we may leverage the training data in sampling the model.
Similar to CD at each step of the gradient we start from training data. In order to produce fantasy
particles of the negative phase we perturb the current model and take several steps towards lower
energy configurations. We may take enough steps to reach a local optima or stop midway.
f e
e denote the perturbed model. For RBM, each step of this block coordinate descend
Let θe = (W,
a, b)
takes the following form
f >0
v ← e
a + Wh
(6)
(k)

e+W
fT v > 0
h ← b

(7)

where starting from v = v ∈ D, h and v are repeatedly updated for K steps or until the update
above has no effect (i.e., a local optima is reached). The final configuration is then used as the
fantasy particle in the negative phase of learning.
3.1 Amount of Perturbations
To see the effect of the amount of perturbations we simply multiplied the noise ǫ by a constant β
– i.e., β > 1 means we perturbed the model with larger noise values. Going back to Lemma1, we
see that any multiplication of noise can be compensated by a change of temperature of the energy
e
function – i.e., for β = T1 , the argx max E(x)
= argx max T1 E(x) − βǫ(x) remains the same.
However here we are only changing the noise without changing the energy.
Here we provide some intuition about the potential effect of increasing perturbations. Experimental
results seem to confirm this view. For β > 0, in the negative phase of learning, we are lowering
the probability of configurations that are at a “larger distance” from the training data, compared to
training with β = 1. This can make the model more robust as it puts more effort into removing false
valleys that are distant from the training data, while less effort is made to remove (false) valleys that
are closer to the training data.
3.2 Second Order Perturbations for RBM
As discussed in Section 2.3 a first order perturbation of θ, only injects noise to local potentials:
e
ai = ai + ǫ(vi = 1) − ǫ(vi = 0) and ebi = bj + ǫ(hi = 1) − ǫ(hi = 0)

In a second order perturbation we may perturb a subset of non-overlapping pairwise potentials as
well as unary potentials over the remaining variables. In doing so it is desirable to select the pairwise
potentials with higher influence – i.e., larger |Wi,j | values. With n visible and m hidden variables,
we can use Hungarian maximum bipartite matching algorithm to find min(m, n) most influential
interactions [21].
Once influential interactions are selected, we need to perturb the corresponding 2 × 2 factors with
Gumbel noise as well as the bias terms for all the variables that are not covered. A simple calculation
shows that perturbation of the 2 × 2 potentials in RBM corresponds to perturbing Wi,j as well as ai
and bj as follows
f i,j = Wi,j + ǫ(1, 1) − ǫ(0, 1) − ǫ(1, 0) + ǫ(0, 0)
W

e
ai = ai − ǫ(0, 0) + ǫ(0, 1)
ebj = bj − ǫ(0, 0) + ǫ(1, 0)
ǫ(0, 0), ǫ(0, 1), ǫ(1, 0), ǫ(1, 1) ∼ γ(ε)
where ǫ(y, z) is basically the injected noise to the pairwise potential assignment for vi = y and
hj = z.
4



