
We consider the problem belief-state monitoring
for the purposes of implementing a policy for
a partially-observable Markov decision process
(POMDP), specifically how one might approxi­
mate the belief state. Other schemes for belief­
state approximation (e.g., based on minimizing a
measure such as KL-divergence between the true
and estimated state) are not necessarily appropri­
ate for POMDPs. Instead we propose a frame­
work for analyzing value-directed approximation
schemes, where approximation quality is deter­
mined by the expected error in utility rather than
by the error in the belief state itself. We propose
heuristic methods for finding good projection
schemes for belief state estimation-exhibiting
anytime characteristics-given a POMDP value
function. We also describe several algorithms for
constructing bounds on the error in decision qual­
ity (expected utility) associated with acting in ac­
cordance with a given belief state approximation.

1

Introduction

Considerable attention has been devoted to partially­
observable Markov decision processes (POMDPs) [15, 17]
as a model for decision-theoretic planning. Their general­
ity allows one to seamlessly model sensor and action uncer­
tainty, uncertainty in the state of knowledge, and multiple
objectives [ 1 , 4]. Despite their attractiveness as a concep­
tual model, POMDPs are intractable and have found prac­
tical applicability in only limited special cases.
Much research in AI has been directed at exploiting cer­
tain types of problem structure to enable value functions for
POMDPs to be computed more effectively. These primar­
ily consist of methods that use the basic, explicit state-based
representation of planning problems [5]. There has, how­
ever, been work on the use of factored representations that
resemble classical AI representations, and algorithms for
solving POMDPs that exploit this structure [2, 8]. Repre­
sentations such as dynamic Bayes nets (DBNs) [7] are used
to represent actions and structured representations of value
functions are produced. Such models are important because

they allow one to deal (potentially) with problems involving
a large number of states (exponential in the number of vari­
ables) without explicitly manipulating states, instead rea­
soning directly with the factored representation.
Unfortunately, such representations do not automatically
translate into effective policy implementation: given a
POMDP value function, one must still maintain a belief
state (or distribution over system states) online in order to
implement the policy implicit in the value function. Belief
state maintenance, in the worst case, has complexity equal
to the size of the state space (exponential in the number of
variables), as well. This is typically the case even when
the system dynamics can be represented compactly using a
DBN, as demonstrated convincingly by Boyen and Koller
[3]. Because of this, Boyen and Koller develop an approx­
imation scheme for monitoring dynamical systems (as op­
posed to POMDP policy implementation); intuitively, they
show that one can decompose a process along lines sug­
gested by the DBN representation and maintain bounded er­
ror in the estimated belief state. Specifically, they approx­
imate the belief state by projection, breaking the joint dis­
tribution into smaller pieces by marginalization over sub­
sets of variables, effectively discounting certain dependen­
cies among variables.
In this paper, we consider approximate belief state moni­
toring for POMDPs. We assume that a POMDP has been
solved and that a value function has been provided to us in
a factored form (as we explain below). Our goal is to de­
termine a projection scheme, or decomposition, so that ap­
proximating the belief state using this scheme hinders the
ability to implement the optimal policy as little as possible.
Our scheme will be quite different from Boyen and Koller's
since our aim is not to keep the approximate belief state
as "close" to the true belief state as possible (as measured
by KL-divergence). Rather we want to ensure that decision
quality is sacrificed as little as possible.
In many circumstances, this means that small correlations
need to be accounted for, while large correlations can be
ignored completely. As an example, one might imagine a
process in which two parts are stamped from the same ma­
chine. If the machine has a certain fault, both parts have
a high probability of being faulty. Yet if the decisions for
subsequent processing of the parts are independent, the fact

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

498

that the fault probabilities for the parts are dependent is ir­
relevant. We can thus project our belief state into two inde­
pendent subprocesses with no loss in decision quality. As­
suming the faults are independent causes a large "error" in
the belief state; but this has no impact on subsequent deci­
sions or even expected utility assessment. Thus we need not
concern ourselves with this "error." In contrast, very small
dependencies, when marginalized, may lead to very small
"error" in the belief state; yet this small error can have se­
vere consequences on decision quality.
Because of this, while Boyen and Koller's notion of pro­
jection offers a very useful tool for belief state approxima­
tion, the model and analysis they provide cannot be applied
usefully to POMDPs. For example, in [ 1 4] this model is
integrated with a (sampling-based) search tree approach to
solving POMDPs. Because the error in decision quality is
determined as a function of the worst-case decision quality
with respect to actual belief state approximation error, the
bounds are unlikely to be useful in practice. We strongly
believe estimates of decision quality error should be based
on direct information about the value function.
In this paper we provide a theoretical framework for the
analysis of value-directed belief state approximation (VDA)
in POMDPs. The framework provides a novel view of ap­
proximation and the errors it induces in decision quality.
We use the value function itself to determine which cor­
relations can be "safely" ignored when monitoring one's
belief state. Our framework offers methods for bounding
(reasonably tightly) the error associated with a given pro­
jection scheme. W hile these methods are computationally
intensive-requiring in the worst case a quadratic increase
in the solution time of a POMDP-we argue that this of­
fline effort is worthwhile to enable fast online implemen­
tation of a policy with bounded loss in decision quality.
We also suggest a heuristic method for choosing good pro­
jection schemes given the value function associated with a
POMDP. Finally, we discuss how our techniques can also
be applied to approximation methods other than projection
(e.g., aggregation using density trees [ 1 3]).

2
2.1

POMDPs and Belief State Monitoring
Solving POMDPs

A partially-observable Markov decision process (POMDP)
is a general model for decision making under uncertainty.
Formally, we require the following components: a finite
state spaceS; a finite action space A; a finite observation
space Z; a transition function T : S x A -+ �(S); an
observation function 0 : S x A -+ �(Z); and a reward
function R : S -+ R.1 Intuitively, the transition function
T(s, a) determines a distribution over next states when an
agent takes action a in states-we write Pr(s, a, t) to de­
note the probability that state t is reached. This captures un­
certainty in action effects. The observation function reflects
the fact that an agent cannot generally determine the true
system state with certainty (e.g., due to sensor noise)-we
write Pr(s, a, z) to denote the probability that observation z

16-(X) denotes the set of distributions over finite set X.

- Optimal Value Function

Belief Space

Figure I: Geometric View of Value Function
is made at state s when action a is performed. Finally R(s)
denotes the immediate reward associated with s.2
The rewards obtained over time by an agent adopting a spe­
cific course of action can be viewed as random variables
R(t) . Our aim is to construct apolicythat maximizes the ex­
pected sum of discounted rewards E CL�o ··/ R( t)) (where
'Y is a discount factor less than one). It is well-known that
an optimal course of action can be determined by consid­
ering the fully-observable belief state MDP, where belief
states (distributions overS) form states, and a policy rr :
�(S) -+ A maps belief states into action choices. In prin­
ciple, dynamic programming algorithms for MDPs can be
used to solve this problem; but a practical difficulty emerges
when one considers that the belief space �(S) is an S
I I-1dimensional continuous space. A key result of Sondik [ I7]
showed that the value function V for a finite-horizon prob­
lem is piecewise-linear and convex and can be represented
as a finite collection of a-vectors.3 Specifically, one can
generate a collection N of a-vectors, each of dimension S
l I,
such that V(b) = ma:xaEN ba. Figure I illustrates a collec­
tion of a-vectors with the upper surface corresponding to
V. Furthermore, each a E N has a specific action associ­
ated with it; so given belief state b, the agent should choose
the action associated with the maximizing a-vector.
Insight into the nature of POMDP value functions, which
will prove critical in the methods we consider in the
next section, can be gained by examining Monahan's [15]
method for solving POMDPs. Monahan's algorithm pro­
ceeds by producing a sequence of k-stage-to-go value func­
tions Vk, each represented by a set of a-vectors Nk. Each
a E Nk denotes the value (as a function of the belief state)
of executing a k-step conditional plan. More precisely, let
the k-step observation strategies be the set oS< of map­
pings u : Z -+ Nk-l. Then each a-vector in Nk corre­
sponds to the value of executing some action a followed by
implementing some u E OSk; that is, it is the value of do­
ing a, and executing the k- 1-step plan associated with the
a-vector u(z) if z is observed. Using CP(a) to denote this
plan, we have that CP(a) = (a; ifz;, CP(u(z;))'v'z;). We
informally write this as (a; u). We write a( (a; u)) to de­
note the a-vector reflecting the value of this plan.
Given Nk, Nk+l is produced in two phases. First, the set
of vectors corresponding to all action-observation policies
2 Action

costs are ignored to keep the presentation simple.
3For infinite-horizon problems, a finite collection may not be
sufficient [1 8], but will generally offer a good approximation.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

is constructed (i.e., for each a E A and (J' E osk+l, the
vector a denoting the value of plan (a, CP((J'(z;) ) ) is added
to �k+l ). Second, this set is pruned by removing all domi­
nated vectors. This means that those vectors a such that b·a
is not maximal for any belief state b are removed from �k + 1.
In Figure 1 , a4 is dominated, playing no useful role in the
representation of V, and can be pruned. Pruning is imple­
mented by a series of linear programs. Refinements of this
approach are possible that eliminate (or reduce) the need for
pruning by directly identifying only a-vectors that are non­
dominated [17, 6, 4]. Other algorithms, such as incremen­
tal pruning [5], are similar in spirit to Monahan's approach,
but cleverly avoid enumerating all observation policies. A
finite k-stage POMDP can be solved optimally this way and
a finite representation of its value function is assured. For
infinite-horizon problems, a k-stage solution can be used to
approximate the true value function (error bounds can eas­
ily be derived based on the differences between successive
value functions).
One difficulty with these classical approaches is the fact
that the a-vectors may be difficult to manipulate. A sys­
tem characterized by n random variables has a state space
size that is exponential in n. Thus manipulating a single
a-vector may be intractable for complex systems.4 Fortu­
nately, it is often the case that an MDP or POMDP can be
specified very compactly by exploiting structure (such as
conditional independence among variables) in the system
dynamics and reward function [ 1 ]. Representations such as
dynamic Bayes nets (DBNs) [7 ] can be used to great effect;
and schemes have been proposed whereby the a-vectors are
computed directly in a factored form by exploiting this rep­
resentation.
Boutilier and Poole [2], for example, represent a-vectors
as decision trees in implementing Monahan's algorithm.
Hansen and Feng [8] use algebraic decision diagrams
(ADDs) as their representation in their version of incre­
mental pruning.5 The empirical results in [8] suggest that
such methods can make reasonably large problems solv­
able. Furthermore, factored representations will likely fa­
cilitate good approximation schemes. There is no reason
in principle that the other algorithms mentioned cannot be
adapted to factored representations as well.

2.2

Belief State Monitoring

Even if the value function can be constructed in a compact
way, the implementation of the optimal policy requires that
the agent maintains its belief state over time. The monitor­
ing problem itself is not generally tractable, since each be­
lief state is a vector of size jSj. Given a compact represen­
tation of system dynamics and sensors in the form of DBN,
one might expect that monitoring may become tractable us­
ing standard belief net inference schemes. Unfortunately,
this is generally not the case. Though variables may be ini4
The number of a-vectors can grow exponentially in the worst
case, as well; but for many problems the number remains manage­
able; and approximation schemes that simply bound their number
have been proposed [6].
5 ADDs, commonly used in verification, have been applied
very effectively to the solution of fully-observable MDPs [9].

499

tially independent (thus admitting a compact representation
of a distribution), and though at each time step only a small
number of variables become correlated, over time these cor­
relations "bleed through" the DBN, rendering most (if not
all) variables dependent after a time. Thus compact repre­
sentation of belief state is typically impossible.
Boyen and Koller [3] have devised a clever approximation
scheme for alleviating the computational burden of moni­
toring. In this work, no POMDP is used, but rather a sta­
tionary process, represented in a factored manner (e.g., us­
ing a DBN), is assumed. This might, for example, be the
process induced by adopting a fixed policy. Intuitively, they
consider projection schemes whereby the joint distribution
is approximated by projecting it onto a set of subsets of vari­
ables. It is assumed that these subsets partition the variable
set. For each subset, its marginal is computed; the approx­
imate belief state is formed by assuming the subsets are in­
dependent. Thus only variables within the same subset can
remain correlated in the approximate belief state. For in­
stance, if there are 4 variables A, B, C and D, the projection
scheme { AB, CD} will compute the marginal distributions
for AB and CD. The resulting approximate belief state,
P(ABCD) = P(AB)P(CD), has a compact, factored
representation given by the distribution of each marginal.
Formally, we say a projection scheme S is a set of subsets
of the set of state variables such that each state variable is in
some subset. This allows marginals with overlapping sub­
sets of variables (e.g., {ABC, BCD}). We view strict par­
titioning as a special type of projection. Some schemes with
overlapping subsets may not be computationally useful in
practice because it may not be possible to easily generate a
joint distribution from them by building a clique tree. We
therefore classify as practical those projection schemes for
which a joint distribution is easily obtained. Assuming that
belief state monitoring is performed using the DBN repre­
senting the system dynamics (see [ 1 0, 1 2] for details on in­
ference with DBNs), we obtain belief state bt+l from bt us­
ing the following steps: (a) construct a clique tree encod­
ing the variable dependencies of the system dynamics (for
a specific action and observation) and the correlations that
have been preserved by the marginals representing bt; (b)
initialize the clique tree with the transition probabilities, the
observation probabilities and the (approximate, factored)
joint distribution bt; (c) query the tree to obtain the distribu­
tion b� at the next time step; and (d) project b� according
to some practical projection scheme S to obtain the collec­
tion of marginals representing bt+l = S(b�) . The com­
plexity of belief state updating is now exponential only in
the size of the largest clique rather than the total number of
variables.
Boyen and Koller show how to compute a bound on the
KL-divergence of the true and approximate belief states,
exploiting the contraction properties of Markov processes
(under certain assumptions). But direct translation of these
bounds into decision quality error for POMDPs generally
yields weak bounds [ 1 4] . Furthermore, the suggestions
made by Boyen and Koller for choosing good projection
schemes are designed to minimize KL-divergence, not to
minimize error in expected value for a POMDP. For this rea-

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

500

S(b)

li

...... -- _ .. ;

Figure 2: Relevant belief states at stagek

Figure 3: The Switch Set Swk(a3) of a3

son,we are interested in new methods for choosing projec­
tions that are directly influenced by considerations of value
and decision quality.
Other belief state approximation schemes can be used for
belief state monitoring. For example, aggregation using
density trees can provide a means of representing a belief
state with many fewer parameters than the full joint. Our
model can be applied to such schemes as well.

with b, b and S(b) are a1,a2 and aa,respectively (see Fig­
ure 2). The approximation at stagek mistakenly induces the
choice of the action associated with a3 instead of a2 at b;
this incurs an error in decision quality of b · a2 - b · aa.
While the optimal choice is in fact a1, the unaccounted er­
ror b · a1 - b · a2 induced by the prior approximations will
be viewed as caused by the earlier approximations; our goal
at this point is simply to consider the error induced by the
current approximation.
In order to derive an error bound,we must identify,for each
a E �k,the set of vectors Swk(a) that the agent can switch
to by approximating its current belief state b given that b
identifies a as optimal. Formally,we define

3

Error Bounds on Approximation Schemes

In this section, we assume that a POMDP has been solved
and that its value function has been provided to us. We
also assume that some structured technique has been used
so that a-vectors representing the value function are struc­
tured [2, 8]. We begin by assuming that we have been given
an approximation scheme S for belief state monitoring in
a POMDP and derive error bounds associated with acting
according to that approximation scheme. We focus primar­
ily on projection, but we will mention how other types of
approximation can be fit into our model. We present two
techniques for bounding the error for a given approximation
scheme and show that the complexity of these algorithms is
similar to that of solving the POMDP, with a (multiplica­
tive) overhead factor of 1 �13.1

Plan Switching

Implementing the policy for an infinite-horizonPOMDP re­
quires that one maintains a belief state, plugging this into
the value function at each step,and executing the action as­
sociated with the maximizing a-vector. When the belief
state b is approximated using an approximation scheme S,
a suboptimal policy may be implemented since the maxi­
mizing vector for S(b) will be chosen rather than the max­
imizing vector for b. Furthermore this mistaken choice of
vectors (hence actions) can be compounded with each fur­
ther approximation at later stages of the process. To bound
such error, we first define the notion of plan switching. We
phrase our definitions in terms of finite-horizon value func­
tions,introducing the minor variations needed for infinite­
horizon problems later.
Suppose withk stages-to-go, the true belief state, had we
monitored accurately to that point, is b. However, due to
previous belief state approximations we take our current be­
lief state to be b. Now imagine our approximation scheme
has been ap�lied at timek to obtain S(b). Given �k, rep­
resenting V , suppose the maximizing vectors associated

Swk(a) = {a' E �k :3b'v'a(b·a 2: b·a,S(b)·a' 2: S(b)·a)}
Intuitively,this is the set of vectors we could choose as max­
imizing (thus implementing the corresponding conditional
plan) due to belief state approximation. In Figure 3, we see
that Swk(aa) = {a1, a2, a4}. The set Swk(a;) can be
identified readily by solving a series of O(l�k I) optimiza­
tion problems,each testing the possibility of switching to a
specific vector aj E �k, formulated as the following (pos­
sibly nonlinear) program:
max

s. t.

d
b . (a; - at) 2: d
'v'l -::j:: i
S(b) · (ai- at) 2: d 'v'l -::j:: j
Ls b(s) = 1
'v's
b(s) 2: 0

The solution to this program has a positive objective func­
tion value whenever there is a belief state b such that a; is
optimal at b, and aj is optimal at S(b). Note, in fact, that
we need only find a positive feasible solution,not an opti­
mal one, to identify aj as an element of Swk(a;). There
are I �k I switch sets to construct,so 0 (I �k12) optimization
problems need to be solved to determine all switch sets.
For linear approximation schemes (i.e., those in which the
constraints on S(b) are linear in the variables b; ), these
problems are easily solvable linear programs (LPs). We re­
turn to linear schemes in Section 6. Unfortunately,projec­
tion schemes are nonlinear, making optimization (or iden­
tification of feasible solutions) more difficult. On the other
hand, a projection scheme determines a set of linear con­
straints on the approximate belief state S(b). For instance,
consider the projection scheme S
{CD,DE} for

501

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

a POMDP with 3 binary variables. This projection im­
poses one linear constraint on S(b) for each subset of the
marginals in the projection:6

b(C)
b' (0)
b(E)
b'(D)
b'(CD) b(DE)

b(0)
b(D)
b(CD)

b'(C)
b'(E)
b'(DE)

Here b' denotes S(b) and b(XY) denotes the cumulative
probability (according to belief state b) of all states where
X and Y are true. These constraints define an LP that can
be used to construct a superset Swk(a;) of Swk(a;). Given
scheme S = {M1 , ... , Mn}, we define the following LP:
max d

s.t.

b ·(a; - at) 2': d
b' (aj- at) 2': d
b'(M) = b(M)
Ls b(s) = 1
Vs
b(s) 2': 0
Vs
b' (s) 2': 0
·

When a feasible positive solution exists,aj is added to the
set Swk(a;), though in fact, it may not properly be a member of Swk(a;). If no positive solution exists,we know a j
is not in Swk(a;) and it is not added to Swk(a;). This superset of the switch set can be used to derive an upper bound
on error.
While the number of constraints of the type b(M) = b' (M)
is exponential in the size of the largest marginal,we expect
that the number of variables in each marginal for a useful
projection scheme will be bounded by a small constant. In
this way, the number of constraints can be viewed as con­
stant (i.e., independent of state space size).
Though the above LPs (for both linear approximations and
projection schemes) look complex, they are in fact very
similar in size to the LPs used for dominance testing in
Monahan's pruning algorithm and the Witness algorithm,
involving O(ISI) variables and O(INkI) constraints. The
number of LP variables is exponential in the number of state
variables; however,the factored representation of a-vectors
allows LPs to be structured in such a way that the state
space need not be enumerated (i.e.,the variables represent­
ing the state probabilities can be clustered). Precisely the
same structuring is suggested in [2] and implemented in [8].
Thus solving an LP to test if the agent can switch from a; to
aj has the same complexity as a dominance test in the prun­
2
ing phase ofPOMDP solving. However,there are O(INk 1 )
pairs of a-vectors to test for plan switching whereas the
pruning phase may require as few as INkI dominance tests
if no vector is pruned. Hence, in the worst case, switch set
generation may increase the running time for solving the
POMDP by a factor of 0(INkI) at each stagek.
For ak-stage, finite-horizon POMDP, we can now bound
the error in decision quality due to approximationS. Define
the bound on the maximum error introduced at each stage j,
6
These equations can be generalized for POMDPs with non­
binary variables, though giving more than one equation per subset.

when a is viewed as optimal,as: 7

B1(a) = max
b

max b ·(a - a')
-J
a'ESW (a)

Since error at a belief state is simply the expectation of
the error at its component states, B1(a) can be determined
by comparing the vectors in S.Vj(a) with a component­

wise (with the maximum difference being B1(a)). L et
B1 = maxaENi B1(a) be the greatest error introduced
by a single approximation S at stage j . Then the total er­
ror fork successive approximations is bounded by ug =
2::::7=1 /i B1. For an infinite-horizon POMDP, assume we
have been given the infinite-horizon value function N* (i.e. ,
no stages are involved). Then we only need to compute
the switch sets Sw*(a) for this single N-set, and the max­
imum one-shot switching error B'S. The upper bound on
the loss incurred by applying s indefinitely is simply u; =
B'S /(1 - !). Computing the error u; is roughly equivalent
to performing 0( IN* I) dynamic programming backups on
N*.
The LP formulation used to construct switch sets is com­
putationally intensive. Other methods can be used how­
ever to construct these switch sets. We have, for example,
implemented a scheme whereby belief states are treated as
vectors in 3{181, and projection schemes are viewed as dis­
placing these vectors. The displacement vectors (vectors
which when added to a belief state b giveS(b)) induced by a
scheme S can be computed easily and can be used to deter­
mine the direction in which belief state approximation shifts
the true belief state. This is tum can be used to construct
overestimates of switch sets. While giving rise to looser er­
ror bounds, this method is much more efficient in practice.
Our emphasis, however, is on the analysis of error due to
approximation, so we do not dwell on this scheme in this
paper (see [ 16] for details).
3.2

Alternative Plans

The cumulative error induced by switching plans at cur­
rent and future stages can be bounded in a tighter way. The
idea is to generate the set of alternative plans that may be
executed as a result of both current and future approxima­
tions. Suppose that an agent,due to approximation at stage
k changes its belief state from b to S(b). This can induce
a change in the choice of optimal a-vector in Nk,say from
a1 to a2. However,even though the agent has switched and
chosen the first action associated with a2, it has not nec­
essarily committed to implementing the entire conditional
plan CP( a2) associated with a2. This is because further ap­
proximation at stagek - 1 may cause it to switch from the
continuation of CP( a2).
Suppose for instance that CP(a2) = (a;(]"), where (j ( z ) =
a3 E Nk-1. If z is observed,and the agent updates its (ap­
proximate) belief state S(b) accurately to obtain S(b)',then
7We use 'S;;,j instead of Sw' to emphasize the fact that we
use the approximate switch set generated for a projection scheme;
however, all definitions apply equally well to exact switch sets if
they are available.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

502

the maximizing vector at the next stage is necessarily a3.
But given that S(b)' will be approximated before the max­
imizing vector is chosen, the agent may adopt some other
continuation of the plan if a3 does not maximize value for
the (second) approximated belief state S(S(b)'). In fact,
the agent may implement CP(a4) at stage k - 1 for any
a4 ESwk -1 ( a3). Notice that the value of the plan actually
implemented-doing the first action of a2, followed by the
first action of a4, and so on-may not be represented by any
a-vector in �k.
We can actually construct the values of such plans, and thus
obtain much tighter error bounds, while we perform dy­
namic programming. We recursively define the set of al­
ternative sets, orA/t-sets for each vector at each stage.8 We
first define
That is, if a is optimal at stage 1 , then any vector in its
switch set can have its plan executed. The future alterna­
tive set for any a E �k, where CP(a) = (a, a"), is:

FA/t (a)

=

{a ((a, o-')) : ('v'z) o-' (z) EAlt-1 (o- (z))}

If a is in fact chosen to be executed at stagek, true expected
value may in fact be given by any vector in FAit (a), this
is due to future switching of policies at stages followingk.
Finally, define

Alt (a)

=

U{FA!t(a'): a' E Swk (a)}

If a is in fact optimal at stagek for a given belief state b, but
b is approximated currently and at every future stage, then
expected value might be reflected by any vector in Alt (a).
These vectors correspond to every possible course of ac­
tion that could be adopted because of approximation: if we
switch vectors at stage k, we could begin to execute (the
plan associated with) any a' ESwk (a); and if we begin ex­
ecuting a', we could end up executing (the plan associated
with) any a" E FAit (a').
Given these Alt-sets, the error associated with belief state
approximation can be given by the maximum difference in
value between any a and one of itsA/t-vectors. These FAit
and A/t-sets can be computed by dynamic programming
while a POMDP is being solved. The complexity of this al­
gorithm is virtually identical to that of generating �k from
�k-1, with the proviso that there are l�kl Alt-sets. How­
ever, these sets grow exponentially much like the sets �k
would if left unpruned. However, these sets can be pruned
in exactly the same way as �-sets, with the exception that
since we want to produce a worst-case bound on error, we
want to construct a lower surface for theAlt-sets rather than
an upper surface.
Given any Alt-set, we denote by Alt the collection of vectors
that are anti-dominating in Alt. For example, if the collec­
tion of vectors in Figure 4, form the set Alt (a), then the
vectors a1 and a4, making up the lower surface of this set,
8

This definition can be more concisely specified, but this for­
mat makes the computational implications clear.

-lower surface of

{a1,az,a3 � }

Figure 4: L ower surface

-k
-k
formAlt (a). FAit (a) is defined similarly. The set of antidominated vectors can be pruned in exactly the same way
that dominated a-vectors are pruned from a value function.
The same structuring techniques can be used to prevent ex­
�cit state enumeration as well. This pruning can keep the
AIt-sets very manageable in size. Assuming we have an approximation Altk(a) of Alt (a) for every a E �k, we con­
-k+1
struct Alt (a) as follows: (a) swk+1 (a) is constructed
-k+l
for each a E �k+1; (b) FAit
( a ) is constructed using
-k
Alt (a ) , and is then pruned to retain only anti-dominating
-k+l
vectors; and (c) Alt (a) is defined as the union of the
-k+l
FAit (a') sets for those a' E Swk+1 (a), and is then
pruned.
The following quantity bounds the error associated with ap­
proximating belief state using scheme S over the course of a
k-stage POMDP, when a represents optimal expected value
for the initial belief state:
-k
Ek5 (a) = maxmax{b ·(a - aI ): a' E Alt (a)}
b

This error can be computed using simple pointwise com­
parison of a with each such a'. It can also be restricted to
that region of belief space where a is optimal; maximizing
the difference only over belief states in that area to obtain a
tighter bound. Approximation error can be bounded glob­
ally using

E�

=

max{E�(a) :a E �k}

Furthermore, E� :::; U� since alternate vectors provide a
much tighter way to measure cumulative error.
For an infinite-horizon problem, we can compute switch
sets once as in the computation of Us. To compute a tighter
bound E'5, we can constructk-stages of Azt-sets, backing up
from �·. The bound E� is computed as above, and we set

E'5

=

E� +·-/Us

In this way, we can obtain fairly tight bounds on the error
induced by belief state approximation.
4

Value-Directed Approximations

The bounds Bk ( a ) and Ek described above can be used in
several ways to determine a good projection scheme. In or­
der to compute error bounds to guide our search for a good

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Figure 5: L attice of Projection Schemes
projection scheme, our "generic algorithm" will have to de­
termine the error associated with a different projectionS ap­
plied to each a-vector. Because of this, we will consider the
use of dijferent projection schemes So: for each a-vector (at
each stage if we have a finite-horizon problem). D espite �he
fact that we previously derived bounds on error assurrung
a uniform projection scheme, our algorithms work equally
well (i.e., provide legitimate bounds) if different projections
are used with each vector. The projection So: adopted for
vector a simply influences its switch set. Since the agent
knows which vector it is "implementing" at any point in
time we can record and easily apply the projection scheme
So: f�r that vector. This allows the agent to tailor its belief
state approximation to provide good results for its currently
anticipated course of action. This in tum will lead to much
better performance than using a uniform scheme.
4.1

Lattice of Projection Schemes

We can structure the search for a projection scheme by con­
sidering the lattice of projection schemes d�fined by �ub­
set inclusion. Specifically, we say St contams S2 (wntten
loosely S2 � St) if every subset of S2 is contai�ed �ithi�
.
.
some subset of S1. This means that S2 ts a finer parttt10n
than S1. The lattice of projections for three binary variables
is illustrated in Figure 5. Each node represents the set of
marginals defining some projection S. Above each node,
the subsets corresponding to its constraining equations are
listed (we refer to each such subset as a constraint). The
finest projections (which are the "most approximate" si�ce
they assume more independence) are at the top of the latttce.
Edges are labeled with the subset of variables correspond­
ing to the single constraining equation th�t �ust be a?ded
to the parent's constraints to obtain the chtld s constramts.
It should be clear that if S2 � St, then St offers (not neces­
sarily strictly) tighter bounds on error when used instead �f
S2 at any point. To see this, imagine that various arproxt­
mation schemes are used for different a-vectors at dtfferent
stages, and that S2 is used whenever a E Ni is chosen. If
we keep everything fixed but replace S2 with S1 at a, we
first observe that Sw�, (a) � Sw�, (a). This ensures that
B�, (a) :::; Bt (a) and Bt :::; B�, . If all other projection

503

.
operators are the same, then obvwusly
U5k, :::; uks, · s·trru-.
lar remarks apply to the infinite-horizon case. Furthermore,
given the definition of Alt-sets, reducing the switch set for
a at stagek by using S1 instead of S2 ensures that the Alt­
sets at all preceding stages are no larger (and may well be
smaller) than they would be if S2 were used. For this rea­
son, we have that E�, :::; E�, (and similarly E5, :::; E5,).
Consequently, as we move down the lattice, the bound on
approximation error gets smaller (i.e., our approximations
improve, at least in the worst case). Of course, the com­
putational effort of monitoring increases as well. The pre­
cise computational effort of monitoring will depend on the
structure of the D BN for the POMDP dynamics and its in­
teraction with the marginals given by the chosen projec­
tion scheme; however, the complexity of inference (i.e., the
dominant factors in the corresponding clique tree), can be
easily determined for any node in the lattice.
4.2

Search for a Good Projection Scheme

In a POMDP setting, the agent may have a bounded amount
of time to make an online decision at each time-step.
For this reason, efficient belief-state monitoring is crucial.
However, just as solving the POMDP is viewed as an offline
operation, so is the search for a good projection sch�me.
Thus it will generally pay to expend some computattonal
effort to search for a good projection scheme that makes
the appropriate tradeoff between decision quality and the
complexity of belief state maintenance. For instance, if any
scheme S with at most c constraints offers acceptable on­
line performance, then the agent need only search the row of
the lattice containing those projection schemes with c con­
straints. However, the size of this row is factorial in c. So
instead we use the structure of the lattice to direct our atten­
tion toward reasonable projections.
We describe here a generic, greedy, anytime algorithm for
finding a suitable projection scheme. We start with the root,
and evaluate each of its children. The child that looks most
"promising" is chosen as our current projection scheme. Its
children are then evaluated, and so on; this continues un­
til an approximation is found that incurs no err?r (specifi­
cally, each switch set is a singleton, as we descnbe below),
or a bound on the size of the projection is reached. We as­
sume for simplicity that at most c constraints will be al­
lowed. The search proceeds to depth c- n in the lattice and
at each node, at most n ( c - n) children are evaluated, so
a total of 0 ( nc2 - en2 ) nodes are examined. Since c must
be greater than n-the root node itself has n constraints.­
we assume 0 ( nc2 ) complexity. The structure of the lattice
ensures that decision quality (as measured by error bounds)
cannot decrease at any step. We note that practical and non­
practical projections are included in the lattice. In figure 5,
the only non-practical scheme is S = { AB, AC, B<:}.
During the search, it doesn't matter if a node correspondmg
to a non-practical scheme is traversed, as long as the final
node is practical. If it is not practical, then the best pr�c­
tical sibling of that node is picked or we backtrack �nt.tl a
.
practical scheme is found. We also note that smce t�Is �s a
greedy approach, we may not discover the bes� �roJectwn
with a fixed number of constraints. However, tt ts a well-

504

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

structured search space and other search methods for navi­
gating the lattice could be used.
We first describe one instantiation of this algorithm, the
finite-horizon U-bound search, for ak-stage, finite-horizon
POMDP. Given the collections of a-vectors N i, . . . , Nk, we
run the following search independently for each vector a E
N' for each i :::; k. The order does not matter; we will end
up with a projection scheme S for each a-vector, which is
�pplied whenever that a-vector is chosen as optimal at stage
z. We essentially minimize (over S) each term B� (a) in
�he bound Uk independently. For a given vector a at stage
z, the search proceeds from the root in a greedy fashion.
Each �hild S of the current node is evaluated by comput­
ing B5 (a), which basically requires that we compute the
switch set Sw5 (a), which in turn requires the solution of
IN' I LPs. Once the projection schemes Sa for each a are
fo�nd, the error bound Uk is given by the sum of the bounds
B' as described in the previous section. At each stage i,
the number of LPs that must be solved is O(nc2IN; 12) since
there are O(IN' I) a-vectors and for each a-vector, the lattice
search traverses 0 (nc2) nodes, each requiring the solution
of 0( IN' I) LPs. Since the solution of the original POMDP
requires the solution of at least IN I LPs, the overhead in­
curred is at most a factor of nc21N1.
The method above can be streamlined considerably. When
comparing two nodes, it is not always necessary to gener­
ate the entire switch set to determine which node has the
lowest bound W (a). Each vector a' in a's switch set intro­
duces an error of at most maJQ,{b(a- a')}. Since Bi(a) =
maxa'ESw'(a) {ma}Q, b(a - a')}, we can test vectors a'
in decreasing order of contributed error until one vector is
found to be in the switch set at one node but not the other.
The node that does not include this vector in its switch set
�as the lowest bound B� (a) (where S is that node's projec­
twn scheme). Instead of solving IN' I pairs of LPs, generally
only a few pairs of LPs will be solved.
When testing whether two different schemes S1 and S2
allow switching to some a-vector, the LPs to be solved
for each scheme are similar, differing only in the con­
straints dictated by each projection scheme. This similarity
can be exploited computationally by using techniques that
take advantage of the numerous common constraints if we
solv� similar LPs "concurrently" (for instance, by solving
a stnpped down LP that has only the common constraints
and using the dual simplex method to account for the extra
constraints). Though details are beyond the scope of this
paper, these techniques are faster in practice than solving
each LP from scratch. The greedy search can take full ad­
vantage of these speed-ups: each child has only one addi­
tional constraint (compared to its parent), so not only can
structure be shared across children, but the parent's solu­
tion can be exploited as well. We reiterate that these LPs
can also be structured, so state space enumeration is not re­
quired. Taken together, these computational tricks don't re­
duce the worst-case running time of O(nc2 IN I2) LPs; how­
ever in practice it is possible that only D(nciNI) LPs need be
solved, in which case, when integrated with the algorithm to
solve the POMDP, the overhead incurred would be a factor

proportional to nc. A thorough experimentation remains to
be done.
There are three variations of the algorithm above. The
infinite-horizon U-bound algorithm is much like the finite­
horizon version. However, we only have one set of a­
ve�tors, W, rather thank sets. Thus we compute far fewer
switch sets, and calculate the final bound using the equation
for U*. The finite-horizon E-bound algorithm is similar to
the above algorithm as well. The difference is that we compute A/_1-s�ts (or rather approximations to them, Alt5k(a))
to obtam tighter bounds on error. To do this requires that
we compute the projection schemes for the various stages
in order, from the last stage back to the first. Once a good
scheme has been found for the elements of Ni, the OOt-sets
can be computed for stage j + 1 without difficulty (this in­
volves simple DP backups). Then switch sets are computed
exactly as above, from whichAzt-sets, and error bounds, are
generated. Finally, the infinite-horizon E-bound algorithm
�roceeds by computing the switch sets for a given projec­
tion only once for each vector inN*; but additional DP back­
ups to compute Alt-sets (as described in the previous sec­
tion) are needed to derive tight error bounds.
5

Illustrative Example

We describe a very simple POMDP to illustrate the benefits
of value-directed approximation, with the aim of demon­
strati�g that minimizing belief state error is not always ap­
propnate when approximate monitoring is used to imple­
ment an optimal policy. The process involves only seven
stages with only one or two actions per stage (thus at some
stages no choice needs to be made), and no observations are
involved. Yet even such a simple system shows the benefits
of allowing the value function to influence the choice of ap­
proximation scheme.
We suppose there is a seven-stage manufacturing process
whereby four parts are produced using three machines, M,
Ml, and M2. Parts PI, P2, P3, and P4 are each stamped
in turn by machine M. Once stamped, parts P 1 and P2
are processed separately (in turn) on machine Ml, while
parts P3 and P4 are processed together on M2. Machine
M may be faulty (FM), with prior probability P r(FM).
When the parts are stamped by M, parts P 1 and P2 may be­
come faulty (F 1, F2), with higher probability of fault if FM
hol�s. �arts_P3 and P4 may also become faulty (F3, F4),
agam With higher probability if FM; but F3 and F4 are both
less sensitive to FM than Fl and F2 (e.g., P r(FIIFM) =
P r(F2IFM) > P r (F3IFM) = P r (F4IFM) ). If PI or P2 are
processed on machine M1 when faulty, a cost is incurred;
�f processed when OK, a gain is had; if not processed (re­
Jected), n� cost or gain is had. When P3 and P4 are pro­
.
cessed (jomtly)on M3, a greater gain is had if both parts are
OK, a lesser gain is had when one part is OK, and a drastic
�ost is incurred if both parts are faulty (e.g., machine M3
iS destroyed). The specific problem parameters are given in
Table 1.
Figure 6 shows the dependencies between variables for the

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Stages to go

7) StampPI

Actions
StampPI

Transitions

Rewards

only affects Fl

no reward

Correlation
FI(F2
F3/F4

if FM at previous step

6) StampP2

Stamp

5) StampP3

StampP3

4) SlampP4

P2

thenPr(FI)
only affects

I) Process/Reject P3P
, 4

F2

no reward

if FM at previous step

=

0.1

only affects F3:

if FM at previous step
then Pr(F3) = 0.1 elsePr(F3) =

=

0.1

else Pr(F4) =

if Fl then

4 for every state

Process

P2

Process P3,P4

all variables are persistant

if F2 then 0 else

all variables are persistant

4 for every state

all variables are persistant

if F3 & F4 then

RejectP3,P4

all variables are persistant

8

D

8

A

F

AA

2000

-

if -F3 & -F4 then
otherwise

c

0 else 8

all variables are persistant
all variables

Reject P2

1.0
0.0

Table 2: Comparison of different distance measures

no reward

0.05

Process PI

persistant

Loss

no reward

RejectPI

are

KL

0.7704 0.3092 0.4325
0.9451 0.3442 0.5599

0.05

only affects F4:

if FM at previous step
thenPr(F4)

2)Process/RejectP2

0.8 else Pr(F I) = 0.1

then Pr(F2) = 0.8 elsePr(F2)

StampP4

3) Process/Reject PI

=

505

0.025

16

E 0.01

A

0.1

3.3 for every state

0.09

0.15

Figure 7: An Example Density Tree

Table 1: POMDP specifications for the factory example

nal stage; and the former, Pr(FM,F3), is needed to accu­
rately assess Pr(F3,F4) at the subsequent stage. Thus we
maintain an approximate belief state with marginals involv­
ing no more than two variables, yet we are assured of acting
optimally.

Figure 6: DBN for the factory example

seven-stage DBN of the example.9 It is clear with three
stages to go, all the variables are correlated. If approximate
belief state monitoring is required for execution of the op­
timal policy (admittedly unlikely for such a simple prob­
lem!), a suitable projection scheme could be used.
Notice that the decisions to process P1 and P2 at stages-to­
go 3 and 2 are independent: they depend only on Pr(Fl)
and Pr(F2), respectively, but not on the correlation be­
tween the two variables. Thus, though these become quite
strongly correlated with five stages to go, this correlation
can be ignored without any impact on the decision one
would make at those points. Conversely,F3 andF4 become
much more weakly correlated with three stages to go; but
the optimal decision at the final stage does depend on their
joint probability. Were we to ignore this weak correlation,
we run the risk of acting suboptimally.
We ran the greedy search algorithm of Section 4.2 and, as
expected, it suggested projection schemes that break all cor­
relations except forFM andF3 with four stages to go, and
F3 andF4 with three, two, and one stage(s) to go. The lat­
ter, Pr(F3,F4), is clearly needed (at least for certain prior
probabilities onFM) to make the correct decision at the fi9
We have imposed certain constraints on actions to keep the
problem simple; with the addition of several variables, the prob­
lem could easily be formulated as a "true" DBN with identical dy­
namics and action choices at each time slice.

In contrast, if one chooses a projection scheme for this
problem by minimizing KL-divergence, L1-distance, or
Lz-distance, different correlations will generally be pre­
served. For instance, assuming a uniform prior overFM
(i.e., machine M is faulty with probability 0.5), Table 5
shows the approximation error that is incurred according
to each such measure when only the correlation between
F1 andF2 is maintained or when only the correlation be­
tweenF3 andF4 is maintained. All of these "direct" mea­
sures of belief state error prefer the former. However, the
loss in expected value due to the former belief state approx­
imation is 1 .0, whereas no loss is incurred using the lat­
ter. To test this further, we also compared the approxima­
tion preferred using these measures over 1 000 (uniformly)
randomly-generated prior distributions. If only theF1 jF2correlation is preserved at the first stage, then in 520 in­
stances a non-optimal action is executed with an average
loss of 0.6858. This clearly demonstrates the advantage of
using a value-directed method to choose good approxima­
tion schemes.

6

Framework Extensions

The methods described above provide means to analyze
value-directed approximations. Though we focused above
on projection schemes, approximate monitoring can be ef­
fected by other means. Our framework allows for the anal­
ysis of error of any linear approximation scheme S. In fact,
our analysis is better suited to linear approximations: the
constraints on the approximate belief state S (b), if linear,
allow us to construct exact switch sets Sw( a ) rather than ap­
proximations, providing still tighter bounds.
One linear approximation scheme involves the use of den­
sity trees [13]. A density tree represents a distribution by
aggregation: the tree splits on variables, and probabilities
labeling the leaves denote the probability of every state con­
sistent with the corresponding branch. For instance, the

506

UNCERTAINTY IN ARTI FICIAL I NTELLIGENCE PROCEEDINGS 2000

tree in Figure 7 d�notes a dist_!'ibution over four variables
in which states cdef and cdef both have probability 0 . 1 .
A tree that i s polynomially-sized i n the number of variables
offers an exponential reduction in the number of parameters
required to represent a distribution. A belief state can be ap­
proximated by forcing it to fit within a tree of a bounded size
(or satisfying other constraints). This approximation can be
reconstructed at each stage, just like projection. It is clear
that a density tree approximation is linear. Furthermore, the
number of constraints and required variables in the LP for
computing a switch set is small.
We also hope to extend this framework to analyze sampling
methods [ 1 1 , 1 3, 1 9] . While such schemes are generally an­
alyzed from the point of view of belief-state error, we would
like to consider the impact of sampling on decision quality
and develop value-directed sampling techniques that mini­
mize this impact.

7

Concluding Remarks

The value-directed approximation analysis we have pre­
sented takes a rather different view of belief state approxi­
mation than that adopted in previous work. Rather than try­
ing to ensure that the approximate belief state is as close as
possible to the true belief state, we try to make the approx­
imate belief state induce decisions that are as close as pos­
sible to optimal, given constraints on (say) the size of the
belief state clusters we wish to maintain. Our approach re­
mains tractable by exploiting recent results on factored rep­
resentations of value functions.
There are a number of directions in which this research
must be taken to verify its practicality. We are currently ex­
perimenting with the four bounding algorithms described
in section 4.2. Ultimately, although these algorithms pro­
vide worst-case bounds on the expected error, it is of in­
terest to gain some insight regarding the average error in­
curred in practice. We are also experimenting with other
heuristics, such as the the vector-space method mentioned
in Section 3 . I , that may provide a tradeoff between the qual­
ity of the error bounds and the efficiency of their compu­
tation. Other directions include the development of online,
dynamic choice of projection schemes for use in search-tree
approaches to POMDPs (see, e.g., [ 1 4]), as well as solving
POMDPs in a bounded-optimal way that takes into account
the fact that belief state monitoring will be approximate.
Acknowledgements Poupart was supported by NSERC
and carried our this research while visiting the University
of Toronto. Boutilier was supported by NSERC Research
Grant OGP0 1 2 1 843 and IRIS Phase 3 Project BAC.



Planning can often be simplified by decomposing the task into smaller tasks arranged
hierarchically. Charlin et al. [4] recently
showed that the hierarchy discovery problem
can be framed as a non-convex optimization
problem. However, the inherent computational difficulty of solving such an optimization problem makes it hard to scale to realworld problems. In another line of research,
Toussaint et al. [18] developed a method
to solve planning problems by maximumlikelihood estimation. In this paper, we show
how the hierarchy discovery problem in partially observable domains can be tackled using a similar maximum likelihood approach.
Our technique first transforms the problem
into a dynamic Bayesian network through
which a hierarchical structure can naturally
be discovered while optimizing the policy.
Experimental results demonstrate that this
approach scales better than previous techniques based on non-convex optimization.

1

Introduction

Planning in partially observable domains is notoriously
difficult. However, many planning tasks naturally decompose into subtasks that may be arranged hierarchically. For instance, the design of a soccer playing
robot is often decomposed into low-level skills such as
intercepting the ball, controlling the ball, passing the
ball, etc. [16]. Similarly, prompting systems that assist
older adults with activities of daily living (e.g., handwashing [8]) can be naturally decomposed into subtasks for each step of an activity. When a decomposition or hierarchy is known a priori, several approaches
have demonstrated that planning can be simplified and
performed faster [13, 7]. However, the hierarchy is

Pascal Poupart
Computer Science
University of Waterloo
Waterloo, Ontario, Canada
ppoupart@cs.uwaterloo.ca

not always known or easy to specify, and the optimal
policy may only decompose approximately. To that
effect, Charlin et al. [4] showed how a hierarchy can
be discovered automatically by formulating the planning problem as a non-convex quartically constrained
optimization problem with variables corresponding to
the parameters of the policy, including its hierarchical
structure. Unfortunately, the inherent computational
difficulty of solving this optimization problem prevents
the approach from scaling to real-world problems. Furthermore, it is not clear that automated hierarchy discovery simplifies planning since the space of policies
remains the same.
We propose an alternative approach that demonstrates
that hierarchy discovery (i) can be done efficiently
and (ii) performs a policy search with a different bias
than non-hierarchical approaches that is advantageous
when there exists good hierarchical policies. The approach combines Murphy and Paskin’s [10] factored
encoding of hierarchical structures (see also [17]) into
a dynamic Bayesian network (DBN) with Toussaint
et al.’s [18] maximum-likelihood estimation technique
for policy optimization. More precisely, we encode
POMDPs with hierarchical controllers into a DBN in
such a way that the policy and hierarchy parameters
are entries of some conditional probability tables. We
also consider factored policies that are more general
than hierarchical controllers. The policy and hierarchy parameters are optimized with the expectationmaximization (EM) algorithm [5]. Since each iteration
of EM essentially consists of inference queries, the approach scales easily.
Sect. 2 briefly introduces partially observable Markov
decision processes, controllers and policy optimization
by maximum likelihood estimation. Sect. 3 reviews
previous work on hierarchical modeling and how to use
a dynamic Bayesian network to encode a hierarchical
structure. Sect. 4 describes our proposed approach,
which combines a dynamic Bayesian network encoding with maximum likelihood estimation to simultane-

ously optimize a hierarchy and the controller. Sect. 5
demonstrates the scalability of the proposed approach
on benchmark problems. Finally, Sect. 6 summarizes
the paper and discusses future work.

2

Background

Throughout the paper we denote random variables by
upper case letters (e.g., X), values of random variables by their corresponding lower case letters (e.g.,
x ∈ dom(X)) and sets of values by upper case letters with math calligraphy (e.g., X = {x1 , x2 , x3 }).
We now review POMDPs (Sect. 2.1), how to represent
policies as finite state controllers (Sect. 2.2) and how
to optimize bounded controllers (Sect. 2.3).
2.1

POMDPs

Partially observable Markov decision processes
(POMDPs) provide a natural and principled framework for planning. A POMDP can be formally
defined by a tuple hS, A, O, ps , ps0 |as , po0 |s0 a , ras i
where S is the set of states s, A is the set of actions
a, O is the set of observations o, ps = Pr(S0 = s) is
the initial state distribution (a.k.a. initial belief),
ps0 |as = Pr(St+1 = s0 | At = a, St = s) is the transition
distribution, po0 |s0 a = Pr(Ot+1 = o0 | St+1 = s0 , At = a)
is the observation distribution and ras = R(At =
a, St = s) is the reward function. Throughout the
paper, it is assumed that S, A and O are finite and
discrete. The goal is to select actions to maximize
the rewards. At any point in time, the information
available to select the next action consists of the
history of past actions and observations. Hence a
policy π is defined as a mapping from histories to
actions. However, since histories grow with time,
it is common practice to summarize histories with
a fixed-length sufficient statistic such as the belief
distribution bs = Pr(S = s), which corresponds to the
state distribution (conditioned on the history of past
actions and observations). The belief distribution
b can be updated at each time step, based on the
0
action a taken and the observation
Po made according
ao0
to Bayes’ theorem: bs0 = k s bs ps0 |as po0 |s0 a (k
is a normalization constant). Policies can then be
defined as mappings from beliefs to actions (e.g.,
π(b) = a). The value V π (b) of a policy π starting
in belief b is measured byPthe discounted sum of
π
t
expected
P rewards: V (b) = t γ Ebt∗|π [rπ(bt )bt ] where
rab = s bs ras . An optimal policy π is a policy with
the highest value V ∗ for all beliefs: V ∗ (b) ≥ V π ∀π, b.
The optimal value function also satisfies
Bellman’s
P
∗ ao0
0 |ab V (b
equation: V ∗ (b)
=
max
r
+
p
)
0
a
ab
o
o
P
where po0 |ab = ss0 bs ps0 |as po0 |s0 a .

2.2

Finite State Controllers

A convenient representation for an important class of
policies consists of finite state controllers [6]. Instead
of using beliefs as sufficient statistics of histories, the
idea is to use a finite internal memory to retain relevant
bits of information from histories. Each configuration
of this memory can be thought of as a node in a finite
state controller, where nodes select actions to be executed and edges indicate how to update nodes based
on the observations received. A controller with a finite
set N of nodes n can encode a stochastic policy π with
three distributions: Pr(N0 = n) = pn (initial node distribution), Pr(At = a | Nt = n) = pa|n (action selection
distribution) and Pr(Nt+1 = n0 | Nt = n, Ot+1 = o0 ) =
pn0 |no0 (successor node distribution). Such a policy
can be executed by starting in a node n sampled from
pn , executing an action a sampled from pa|n , receiving
observation o0 , transitioning to node n0 sampled from
pn0 |no0 and so on. The value of a controller
Pcan be computed
by
solving
a
linear
system:
V
=
ns
a pa|n [ras +
P
0 ] ∀ns. The value at a
γ s0 o0 n0 ps0 |as po0 |s0 a pn0 |no0 Vn0 sP
P
given belief b is then V π (b) = n s bs pn Vns .
2.3

Policy Optimization

Several techniques have been proposed to optimize
controllers of a given size, including gradient ascent [9], stochastic local search [2], bounded policy iteration [14], non-convex quadratically constrained optimization [1] and likelihood maximization [18]. We
briefly describe the last technique since we will use it
in Sect. 4.
Toussaint et al. [18] recently proposed to convert
POMDPs into equivalent dynamic Bayesian networks
(DBNs) by normalizing the rewards and to optimize
a policy by maximizing the likelihood of the normalized rewards. Let R̃ be a binary variable corresponding to normalized rewards. The reward function ras is then replaced by a reward distribution
pr̃|sat = Pr(R̃ = r̃ | At = a, St = s, T = t) that assigns probability ras /(rmax − rmin ) to R̃ = 1 and
1 − ras /(rmax − rmin ) to R̃ = 0 (rmin = minas ras
and rmax = maxas ras ). An additional time variable
T is introduced to simulate the discount factor and the
summation of rewards. Since a reward is normally discounted by a factor γ t when earned t time steps in the
future, the prior pt = Pr(T = t)P
is set to γ t (1−γ) where
∞
the factor (1−γ) ensures that t=0 pt = 1. The resulting dynamic Bayesian network is illustrated in Fig. 1.
It can be thought of as a mixture of finite processes of
length t with a 0-1 reward R̃ earned at the end of the
process. The nodes Nt encode the internal memory of
the controller. Given the controller distributions pn ,
pa|n and pn0 |no0 , it is possible to evaluate the controller

N0

T =0

N0

T =1

A0

S0

R̂

N1

A0

S0

O1

A1

R̂

S1

...
N0

T = tmax

N1

A0

O1

Ntmax

A1

...

Otmax

we will empirically compare our approach to the nonconvex optimization techniques used to optimize recursive controllers. In another line of research, Murphy
and Paskin [10] proposed to model hierarchical hidden
Markov models (HMMs) with a dynamic Bayesian network (DBN). Theocharous et al. [17] also used DBNs
to model hierarchical POMDPs. We briefly review this
DBN encoding in Sect. 3.2 since we will use it in our
approach to model factored controllers.

Atmax

3.1
S0

S1

Stmax

Figure 1: POMDP represented as a mixture of finite
DBNs. For an infinite horizon, a large enough tmax
can be selected at runtime to ensure that the approximation error is small.
by computing the likelihood of R̃ = 1. More precisely,
V π (ps ) = (Pr(R̃ = 1) − rmin )/[(rmax − rmin )(1 − γ)].
Optimizing the policy can be framed as maximizing
the likelihood of R̃ = 1 by varying the distributions
pn , pa|n and pn0 |no0 encoding the policy. Toussaint et
al. use the expectation-maximization (EM) algorithm.
Since EM is guaranteed to increase the likelihood at
each iteration, the controller’s value increases monotonically. However, EM is not guaranteed to converge
to a global optimum. An important advantage of the
EM algorithm is its simplicity both conceptually and
computationally. In particular, the computation consists of inference queries that can be computed using
a variety of exact and approximate algorithms.

3

Recursive Controllers

R̂

Hierarchical Modeling

While optimizing a bounded controller allows an effective search in the space of bounded policies, such
an approach is clearly suboptimal since the optimal
controller of many problems grows doubly exponentially with the planning horizon and may be infinite
for infinite horizons. Alternatively, hierarchical representations permit the representation of structured
policies with exponentially fewer parameters. Several
approaches were recently explored to model and learn
hierarchical structures in POMDPs. Pineau et al. [13]
sped up planning by exploiting a user specified action hierarchy. Hansen et al. [7] proposed hierarchical controllers and an alternative planning technique
that also exploits a user specified hierarchy. Charlin et
al. [4] proposed recursive controllers (which subsume
hierarchical controllers) and an approach that discovers the hierarchy while optimizing a controller. We
briefly review recursive controllers in Sect. 3.1 since

A recursive controller [4] consists of a recursive automaton with concrete nodes n and abstract nodes n̄.
Abstract nodes call a subcontroller before selecting an
action. A controller is said to be recursive when it
can call itself, essentially encoding an infinite hierarchy. Formally, a recursive controller is parametrized
by an action selection distribution for each node (e.g.,
pa|n and pa|n̄ ), a successor node distributions for each
node (e.g., pn0 |no0 and pn0 |n̄o0 ) and a child node distribution for each abstract node (e.g., pn0 |n̄ )1 . Execution of a recursive controller is performed by executing the action selected by each node visited and
continuing to the successor node selected by the observation made. However, when an abstract node is
visited, before executing the action selected, its subcontroller is called and started in the child node selected by the child node distribution. A subcontroller
returns control to its parent node when a special end
node is reached. Charlin et al. [4] show that optimizing
a recursive controller with a fixed number of concrete
and abstract nodes can be framed as a non-convex
quartically constrained optimization problem. The hierarchical structure is discovered as the controller is
optimized since the variables of the optimization problem include the child node distributions which implicitly encode the hierarchy. Three techniques based on
a general non-linear solver, a mixed-integer non-linear
approximation and a form of bounded hierarchical policy iteration are experimented with, but do not scale
beyond toy problems. Furthermore, Charlin et al. do
not demonstrate whether searching in the space of hierarchical controllers can speed up planning. Although
it is clear that planning is simplified when a hierarchy
is given a priori since the policy space is reduced, it
is not clear that hierarchy discovery is beneficial since
the policy space remains the same while the parameter space changes. In Sect. 5, we demonstrate that
hierarchy discovery can be beneficial when a simple
hierarchical policy of high value exists.

1
The pa|n and pn0 |no0 distributions are combined in one
distribution pn0 a|no0 in [14]

S01

S21

S11

E0

...

E1

the tuple hpa|nl , pnl−1 |nl , pn0l |nl o0 i ∀l. The conditional
probability distributions of the mixture of DBNs (denoted by p̂) are:
• transition distribution: p̂s0 |as = ps0 |as

Figure 2:
HMM.

S00

S10

S21

O0

O1

O2

DBN encoding of a 2-level hierarchical

• observation distribution: p̂o0 |s0 a = po0 |s0 a
• reward distribution:
p̂r̃|as = (ras − rmin )/(rmax − rmin )
• mixture distribution: p̂t = (1 − γ)γ t
• action distribution: p̂a|n0 = pa|n0

3.2

Hierarchical HMMs

Murphy and Paskin [10] proposed to model hierarchical hidden Markov models (HMMs) as dynamic
Bayesian networks (DBNs). The idea is to convert a
hierarchical HMM of L levels into a dynamic Bayesian
network of L state variables, where each variable encodes abstract states at the corresponding level. Here,
abstract states can only call sub-HMMs at the previous level. Fig. 2 illustrates a two-level hierarchical
HMMs encoded as a DBN. The state variables Stl are
indexed by the time step t and the level l. The Et variables indicate when a base-level sub-HMM has ended,
returning its control to the top level HMM. The toplevel abstract state transitions according to the top
HMM, but only when the exit variable Et indicates
that the base-level concrete state is an exit state. The
base-level concrete state transitions according to the
base-level HMM. When an exit state is reached, the
next base-level state is determined by the next toplevel abstract state. Factored HMMs subsume hierarchical HMMs in the sense that there exists an equivalent factored HMM for every hierarchical HMM. In
Sect. 4.1, we will use a similar technique to convert
hierarchical controllers into factored controllers.

4

Factored Controllers

We propose to combine the DBN encoding techniques
of Murphy et al. [10] and Toussaint et al. [18] to convert a POMDP with a hierarchical controller into a
mixture of DBNs. The hierarchy and the controller
are simultaneously optimized by maximizing the reward likelihood of the DBN. We also consider factored
controllers which subsume hierarchical controllers.
4.1

DBN Encoding

Fig. 3a illustrates two consecutive slices of one DBN in
the mixture (rewards are omitted) for a three-level hierarchical controller. Consider a POMDP defined by
the tuple hS, A, O, ps , ps0 |as , po0 |s0 a , ras i and a threelevel hierarchical (non-recursive) controller defined by

• base
p̂n00 |n0 n01 o0 e0
 level node distribution:
0
if e = exit
pn00 |n01
=
pn00 |o0 n0 otherwise
• middle
p̂n01 |n1 n02 o0 e0 e1
 level node distribution:
if e1 = exit
 pn01 |n02
p 01 0 1 if e0 = exit and e1 6= exit
=
 n |o n
δn01 n1
otherwise
• toplevel node distribution: p̂n02 |o0 n2 e1
pn02 |o0 n2 if e1 = exit
=
δn02 n2
otherwise
• base-level
exit distribution: p̂e0 |n0

1 if n0 is an end node
=
0 otherwise
• middle-level
exit distribution: p̂e1 |n1 e0

1 if e0 = exit and n1 is an end node
=
0 otherwise
While the Etl variables help clarify when the end of
a sub-controller is reached, they are not necessary.
Eliminating them yields a simpler DBN illustrated in
Fig. 3b. The conditional probability distributions of
the Ntl variables become:
• base
p̂n00 |n0 n01 o0
 level node distribution:
pn00 |n01
if n0 is an end node
=
pn00 |o0 n0 otherwise
• middle
p̂n01 |n1 n02 o0
 level node distribution:
1
0
if n and n are end nodes
 pn01 |n02
pn01 |o0 n1 if n0 is an end node, but not n1
=

δn01 n1
otherwise
• toplevel node distribution: p̂n02 |n2 o0 e1
pn02 |n2 o0 if n1 and n0 are end nodes
=
δn02 n2
otherwise
Note that ignoring the above constraints in the conditional distributions yields a factored controller that
is more flexible than a hierarchical controller since the
conditional probability distributions of the Ntl variables do not have to follow the structure imposed by
a hierarchy

(a)

N 02

N2

(b)

N2

N 02

4.2.1

W.l.o.g. we initialize the start node N0top of the top
layer to be the first node (i.e., Pr(N0top = 1) = 1). The
node conditional distributions pn0l |φ(n0l ) are initialized
randomly as a mixture of three distributions:

E1
N 01

N1

N 01

N 00

N0

N 00

O0

O

S

S0

S

N2

N 02

N1
E0
N0

O

(c)

A

A

O0

S0

(d)
N 2 N 1 N 0 SS 0
N 2 N 1 N 0 S0

N1

N 01

Parameter initialization

pn0l |φ(n0l ) ∝ c1 + c2 Un0l φ(n0l ) + c3 δn0l nl
The mixture components are a uniform distribution, a
random distribution Uφ(n0l ) (an array of uniform random numbers in [0, 1]), and a term enforcing nl to stay
unchanged. For the node distributions at the base level
we choose c1 = 1, c2 = 1, c3 = 0 and for all other levels
we choose c1 = 1, c2 = 1, c3 = 10. Similarly we initialize
the action probabilities as

N 2 N 02 N 1 N 0 S 0

pa|nbase ∝ c1 + c2 Uanbase + c3 δa(nbase %a)

N 02 N 1 N 0 S 0

N0

N 00
N 02 N 1 N 01 N 0 S 0
N 02 N 01 N 0 S 0
N 02 N 01 N 0 N 00 S 0

S

S0

Figure 3: (a) Two slices of the DBN encoding the hierarchical POMDP controller. (b) A version where exit
variables are eliminated. (c) Variables O and A are
eliminated. (d) The corresponding junction tree (or
rather chain) for inference.

4.2

Maximum Likelihood Estimation

Following Toussaint et al.’s technique [18], we optimize
a factored controller by maximizing the reward likelihood. Since the policy parameters are conditional
probability distributions of the DBN, the EM algorithm can be used to optimize them. Computation
alternates between the E and M steps below. We denote by ntop and nbase the top and base nodes in a
given time slice. We also denote by φ(V ) and φ(v) the
parents of V and a configuration of the parents of V .
E-step: expected frequency of the hidden variables
top
top
1)
Entop = Pr(N
P 0 = n |R̃ =base
Eanbase = t Pr(At = a, Nt
= nbase |R̃ = 1)
En0l φ(n0l ) =
P
l
0l
l
l
t Pr(Nt+1 = n , φ(Nt+1 ) = φ(nt+1 )|R̃ = 1) ∀l
M-step: relativeP
frequency computation
pntop = Entop / ntop
PEntop
pa|nbase = Eanbase / a P
Eanbase
pn0l |φ(n0l ) = En0l φ(n0l ) / n0l En0l φ(n0l ) ∀l

with c1 = 1, c2 = 1, c3 = 100, where the last term enforces each node nbase = i to be associated with action
a = i%a.
4.2.2

E-step

To speed up the computation of the inference queries
in the E-step, we compute intermediate terms using a
forward-backward procedure. Let tmax be the largest
value of T , then a simple scheme that answers each
query separately takes O(t2max ) time since there are
O(tmax ) queries and each query takes O(tmax ) time
to run over the entire network. However, since part
of the computation is duplicated in several queries,
it is possible to compute intermediate terms α and β
in O(tmax ) time from which each expectation can be
computed in constant time (w.r.t. tmax ). To simplify
notation, N and n denote all the nodes and their joint
configuration in a given time slice.
t
Forward term: αns
= Pr(Nt = n, St = s)
0
αns = pnP
ps
t−1
αnt 0 s0 = n,s αns
pn0 s0 |ns
τ
Backward term: βns
= Pr(R̃ = 1|Nt−τ = n, St−τ =
s, T = t) P
0
βns
= a pa|n ras
P
τ
βns
= n0 ,s0 pn0 s0 |ns βnτ −1
0 s0

To fully take advantage of the structure of the DBN,
we first marginalize the DBN w.r.t. the observations
and actions to get the DBN in Fig. 3c. This 2-slice
DBN corresponds to the joint transition distribution
pn0 s0 |ns used in the above equations. Then we compile this 2-slice DBN into the junction tree (actually
junction chain) given in Fig. 3d.
P
P
τ
Let βns =
τ Pr(T = τ )βns and αns =
t Pr(T =
t
t)αns
, then the last two expectations of the E-step

can be computed as follows:2

P
Eanbase ∝ s,n−{nbase } αns pa|nbase ras +

P
0 0
γ s0P
,o0 ,n0 ps0 |as po0 |s0 a pn0 |o0 n βn s
En0l φ(n0l ) ∝ s,s0 ,a,n−φ(n0l ),n0−l αns pa|nbase ps0 |as


po0 |s0 a pn0 |o0 n ras + γβn0 s0 ∀l
4.2.3

M-step

The standard M-step adjusts each parameter pv|φ(v)
by normalizing the expectations computed in the Estep, i.e., pnew
v|φ(v) ∝ Evφ(v) . To speed up convergence,
we instead use a variant that performs a soften greedy
M-step. In the greedy M-step, each parameter pnew
v|φ(v)
is greedily set to 1 when v = argmaxv̄ fv̄φ(v̄) and 0
otherwise, where fvφ(v) = Evφ(v) /pold
v|φ(v) . The greedy
M-step can be thought of as the limit of an infinite
sequence of alternating partial E-step and standard
M-step where the partial E-step keeps f fixed. The
combination of a standard M-step with this specific
partial E-step updates pv|φ(v) by a multiplicative factor
proportional to fvφ(v) . In the limit, the largest fvφ(v)
ends up giving all the probability to the corresponding pv|φ(v) . EM variants with certain types of partial
E-steps ensure monotonic improvement of the likelihood when the hidden variables are independent [11].
This is not the case here, however by softening the
greedy M-step we can still obtain monotonic improvement most of the time while speeding up convergence.
We update pv|φ(v) as follows:
v ∗ = argmax fvφ(v)
pnew
v|φ(v)

∝

v
old
pv|φ(v) [δvv∗

# parameters
flat
|O||N |2 + |A||N |
fact. 2|O||N |1.5 + |A||N |0.5
forward-backward complexity
flat
O(tmax (|N ||S|2 + |N |2 |S|))
fact. O(tmax (|N ||S|2 + |N |1.5 |S|))
expectation complexity
flat
O(|N ||A|(|S|2 + |S||O|) + |N |2 |S||O|)
fact. O(|N ||A|(|S|2 + |S||O|) + |N |1.5 |O||S| + |N |2 |O|)

level have fewer parameters and a smaller complexity,
but also a smaller policy space due to the structure imposed by the hierarchy/factorization. While there is a
tradeoff between policy space and complexity, hierarchical and factored controllers are often advantageous
in practice since they can find more quickly a good
hierarchical/factored policy when there exists one.
A 2-level factored controller with |N |0.5 nodes at each
level has 2|O||N |1.5 parameters for pn0top |o0 nbase ntop
and pn0base |n0top o0 nbase , and |A||N |0.5 parameters for
The complexity of the forward (backpa|nbase .
ward) procedure is O(tmax (|N ||S|2 + |N |1.5 |S|)) and
the complexity of computing the expectations is
O(|N ||A|(|S|2 + |S||O|) + |N |1.5 |O||S| + |N |2 |O|). A
2-level hierarchical controller is further restricted and
therefore has fewer parameters, but the same time
complexity.

+ c + ] .

For c = 0 and  = 0 this is the greedy M-step. We
use c = 3 which softens (shortens) the step and improves convergence. Furthermore, adding small Gaussian noise  ∼ N (0, 10−3 ) helps to escape local minima.
4.2.4

Table 1: Number of parameters and computational
complexity for the flat controller with |N | nodes and
a 2-layer factored controller with |N top | = |N base | =
|N |0.5 nodes.

Complexity

For a flat controller, the number of parameters (neglecting normalization) is |O||N |2 for pn0 |o0 n and
|A||N | for pa|n . The complexity of the forward (backward) procedure is O(tmax (|N ||S|2 + |N |2 |S|)) where
the two terms correspond to the size of the two cliques
for inference in the 2-slice DBN after O and A are eliminated. The complexity of computing the expectations
from α and β is O(|N ||A|(|S|2 + |S||O|) + |N |2 |S||O|),
which corresponds to the clique sizes of the 2-slice
DBN including O and A.
In comparison, 2-level hierarchical and factored controllers with |N top | = |N base | = |N |0.5 nodes at each
2
The first expectation of the E-step does not need to be
computed since Pr(N0top = 1) = 1.

5

Experiments

We first compared the performance of the maximum
likelihood (ML) approach to previous optimizationbased approaches from [4]. Table 2 summarizes the results for 2-layer controllers with certain combinations
of |N base | and |N top |. The problems include paint,
shuttle and 4x4 maze (previously used in [4]) and
three additional problems: chain-of-chains (described
below), hand-washing (reduced version from [8]) and
cheese-taxi (variant from [12]). On the first three
problems, ML reaches the same values as the previous optimization-based approaches, but with larger
controllers. We attribute this to EM’s weaker ability to avoid local optima than the optimization-based
approaches. However, the optimization-based approaches run out of memory on the last three problems (memory needs exceed 2 Gb of RAM), while ML
scales gracefully (as analyzed in Sect. 4.2.4). ML approach demonstrates that hierarchy discovery can be
tackled with tractable algorithms. We also report the
values reached with a state of the art point-based value

Table 2: V ∗ denotes optimal values (with truncated trajectories) [3] except for handwashing and cheese-taxi
where we show the optimal value of the equivalent fully-observable problem. HSVI2 found a solution in less than
1s for every problem except handwashing where the algorithm was halted after 12 hours of computation. The
ML approach optimizes a factored controller for 200 EM iterations with a planning horizon of tmax = 100. (5,3)
nodes means |N base | = 5 and |N top | = 3. For cheese-taxi, we get a maximum value of 2.25. N/A indicates that
the solver did not complete successfully. All tests are done on a dual-core x64 processor @2.2GHz.
Problem
paint
shuttle
4x4 maze
chain-of-chains
handwashing
cheese-taxi

|S|, |A|, |O|

V∗

4, 4, 2
8, 3, 5
16, 4, 2
10, 4, 1
84, 7, 12
33, 7, 10

3.28
32.7
3.7
157.1
61052
65.3

HSVI2
V
3.29±0.04
32.9±0.8
3.75±0.1
157.1±0
N/A
2.53±0.3

Best results from [4]
nodes t(s)
V
(1,3)
<1
3.29
(1,3)
2
31.87
(1,2)
30
3.73
(3,3)
10
0.0
N/A
N/A

iteration method (HSVI2 [15]).
The next question is whether there are computational
savings when automatically discovering a hierarchy.
Recall that previous work has shown that policy optimization is simplified when a hierarchy is known a
priori since the space of policies is restricted. The
next experiment demonstrates that policy optimization while discovering a hierarchy can be done faster
and/or yield higher value when there exists good hierarchical policies. Table 3 compares the performance
when optimizing flat, hierarchical and factored controllers on chain-of-chains, hand-washing and cheesetaxi. Here, the factored and hierarchical controllers
have two levels and correspond respectively to the
DBNs in Fig. 3(a) and 3(b).3 The x-axis is the number of nodes for flat controllers and the product of the
number of nodes at each level for hierarchical and factored controllers. Taking the product is justified by the
fact that the equivalent flat controllers of some hierarchical/factored controllers require that many nodes.
The graphs in the right column of Table 3 demonstrate that hierarchical and factored controllers can
be optimized faster, confirming the analysis done in
Sect. 4.2.4. There is no difference in computational
complexity between the strictly hierarchical and unconstrained factored architectures. Recall however
that the efficiency gains of the hierarchical and factored controllers are obtained at the cost of a restricted
policy space. Nevertheless, the graphs in the left column of Table 3 suggest that hierarchical/factored controllers can still find equally good policies when there
exist one. Factored controllers are generally the most
robust. With a sufficient number of nodes, they find
the best policies on all three problems. Note that factored and hierarchical controllers need at least a number of nodes equal to the number of actions in the base
layer in order to represent a policy that uses all actions.
3
Factored controllers are hierarchical controllers where
the restrictions imposed by the Et variables are removed.

Level 1

ML approach (avg. over 10 runs)
nodes
t(s)
V
(5,3)
0.96±0.3
3.26±0.004
(5,3)
2.81±0.2
31.6±0.5
(3,3)
2.8±0.8
3.72±8e−5
(10,3)
6.4±0.2
151.6±2.6
(10,5)
655±2
984±1
(10,3)
311±14
−9±11(2.25∗ )

0
0.16

Level 0

0.84

A

1

2
B

3
D

C

Figure 4: Hierarchical controller learnt for the chainof-chains. The diamond indicates an exit node, for
which p̂e0 |n0 = 1.

This explains why hierarchical and factored controllers
with less than 4 base nodes (for chain-of-chains) and
7 base nodes (for hand-washing and cheese-taxi) do
poorly. The optimization of flat controllers tend to get
stuck in local optima if too many nodes are used. Comparing the unconstrained factored architecture versus
hierarchical, we find that the additional constraints
in the hierarchical controller make the optimization
problem harder although there are less parameters to
optimize. As a result, EM gets stuck more often in
local optima.
We also examine whether learnt hierarchies make intuitive sense. Good policies for the cheese-taxi and handwashing problems can often be represented hierarchically, however the hierarchical policies found didn’t
match hierarchies expected by the authors. Since these
are non-trivial problems for which there may be many
ways to represent good policies in a hierarchical fashion that is not intuitive, we designed the chain-ofchains problem, which is much simpler to analyze. The
optimal policy of this problem consists of executing n
times the same chain of n actions followed by a submit
action to earn the only reward. The optimal policy requires n2 + 1 nodes for flat controllers and n + 1 nodes
at each level for hierarchical controllers. For n = 3,
ML found a hierarchical controller of 4 nodes at each
level, illustrated in Fig. 4. The controller starts in
node 0. Nodes at level 1 are abstract and descend
into concrete nodes at level 0 by following the dashed

180
(7,3) (10,3)
(5,7)
(5,7)
(5,5)
(5,5) (7,5)

140

(10,5)

(10,7)

(5,10)
(7,7)

(7,10)

120
(5,3)

100
80

(5,3)

60
40

(7,3) (3,10)
(7,5)
(3,3)(3,5)(3,7)
(3,3)(3,5)(3,7) (3,10)

20

(7,7)

0
0

10

20

30

40

990
970

50

flat
(7,10)
factored
hierarchical
60

(7,5)
(7,3) (10,3)

(7,7)

(7,5)
(7,3)
(5,5)
(5,3)

940
930

(5,3)

(5,5)

(5,7)

(7,7)
(10,5)
(5,10)

(5,7)

(5,10)

920
900
0

10

20

30

90

100

(7,10)
(10,7)

(10,10)

40

50

60

70

80

flat
factored
hierarchical

10

(10,10)

(10,7)

0
-5
flat
factored
hierarchical

-10
-15
(7,7)
(5,5)
(7,5)
(5,7)
(3,3)(5,3)
(3,5)(7,3)
(3,7)
(3,10)
(5,5)
(7,5)
(5,7)
(3,3)(5,3)
(3,5)(7,3)
(3,7)
(3,10)

-20

(5,10)
(7,7)
(5,10)

40

50

60

70

80

90

100

50

60

70

80

90

100

50

60

70

80

90

100

3000
2500
2000
1500
1000
500

100

0

(7,10)
(7,10)

cheese-taxi: time (seconds)

(10,5)

30

flat
factored
hierarchical

3500

3500
(10,3)

20

0
90

5
cheese-taxi: best value

100
90
80
70
60
50
40
30
20
10
0
0
4000

(10,10)

flat
factored
hierarchical

(3,3)(3,5)(3,7) (3,10)
(3,3)(3,5)(3,7) (3,10)

910

80

(7,10)

960
950

70
(10,7)

(10,5)

(10,3)

980
handwashing: value

(10,10)

handwashing: time (seconds)

chain-of-chains: value

160

chain-of-chains: time (seconds)

Table 3: Left: The reached values depending on the number of nodes in the controller. For the factored
and hierarchical controller we indicate the number of nodes in both layers (e.g., (5,3) means |N base | = 5 and
|N top | = 3) and plot the data point at |N base ||N top | on the x-axis. For instance, in the case of handwashing
we see how the performance depends critically on |N base |. Right: The optimization time. In all cases, 200 EM
iterations are performed with a planning horizon of tmax = 100. The results for each controller are the average
of 10 runs with error bars of ±1 standard deviation.

-25

10

20

30

40

flat
factored
hierarchical

3000
2500
2000
1500
1000
500
0

0

10

20

30

40

50

60

70

80

90

100

0

10

20

nodes

edges. Control is returned to level 1 when an end node
(denoted by a diamond) is reached. Here, the optimal
policy is to do A-B-C three times followed by D. Hence
a natural hierarchy would abstract A-B-C and D into
separate subcontrollers. While the controller in Fig. 4
is not completely optimal (the vertical transition from
abstract node 0 should have probability 1 of reaching node A), it found an equivalent, but less intuitive
abstraction by having subcontrollers that do A-B-C
and D-A-B-C. This suggests that for real-world problems there will be many valid abstractions that are
not easily interpretable by humans and the odds that
an automated procedure finds an intuitive hierarchy
without any additional guidance are slim.

30

40

nodes

6

Conclusion

The key advantage of maximum likelihood is that it
can exploit the factored structure in a controller architecture. This facilitates hierarchy discovery when the
hierarchical structure of the controller is encoded into
a corresponding dynamic Bayesian network (DBN).
Our complexity analysis and the empirical run time
analysis confirm the favorable scaling. In particular,
we solved problems like handwashing and cheese-taxi
that could not be solved with the previous approaches
in [4]. Compared to flat controllers, factored controllers are faster to optimize and less sensitive to local
optima when they have many nodes. Our current implementation does not exploit any factored structure

in the state, action and observation space, however we
envision that a factored implementation would naturally scale to large factored POMDPs.
For the chain-of-chains problem, maximum likelihood
finds a valid hierarchy. For other problems like handwashing, there might be many hierarchies and the one
found by our algorithm is usually hard to interpret.
We cannot expect our method to find a hierarchy that
is human readable. Interestingly, although the strictly
hierarchical architectures have less parameters to optimize, they seem to be more susceptible to local optima
as compared to a factored but otherwise unconstrained
controller. Future work will investigate various heuristics to escape local optima during optimization.
In this paper we made explicit assumptions about the
structure – we prefixed the structure of the DBN to
mimic a strict hierarchy or a level-wise factorization
and we fixed the number of nodes in each level. However, the DBN framework allows us to build on existing
methods for structure learning of graphical models. A
promising extension would be to use such structure
learning techniques to optimize the factored structure
of the controller. Since the computational complexity
for evaluating (training) a single structure is reasonable, techniques like MCMC could sample and evaluate a variety of structures. This variety might also help
to circumvent local optima, which currently define the
most dominant limit of our approach.
Acknowledgments
Part of this work was completed while Charlin was at
the University of Waterloo. Toussaint acknowledges support by the German Research Foundation (DFG), Emmy
Noether fellowship TO 409/1-3. Poupart and Charlin were
supported by grants from the Natural Sciences and Engineering Research Council of Canada, the Canada Foundation for Innovation and the Ontario Innovation Trust.



Rollating walkers are popular mobility aids
used by older adults to improve balance control. There is a need to automatically recognize the activities performed by walker users
to better understand activity patterns, mobility issues and the context in which falls are
more likely to happen. We design and compare several techniques to recognize walker
related activities. A comprehensive evaluation with control subjects and walker users
from a retirement community is presented.

1

not only time consuming but may not be accurate due
to synchronization issues between various sensors. An
automated activity recognition system would enable
clinicians to gather statistics about the activity patterns of users, their level of mobility and the context
in which falls are more likely to occur. This will also be
useful for the development of smart walkers that can
assist users with navigation and braking while taking
into account their intended activity.
In this paper we describe a comparative analysis of activity recognition techniques based on hidden Markov
models (HMMs) and conditional random fields (CRFs)
trained by supervised and unsupervised learning for
rollating walkers instrumented with various sensors.
Our contributions are:

Introduction

Improving the quality of life of the ever increasing elderly population is one of the key concerns for health
care provision. Limitations to independent mobility
for these individuals have a significant impact on the
quality of their life. Devices such as rollating walkers
are often used to improve the independence and mobility of older adults. Our long-term goal is to improve
the utility of these devices, by enabling them to perceive their environment and actively provide assistance
to their users.
We are collaborating with a multidisciplinary group
that is studying the usage of rollating walkers. We
have access to a walker [9] that has been instrumented
with various sensors and cameras to monitor the user.
We are developing automated techniques to recognize
the activities performed by users with respect to their
walker (e.g., walking, standing, turning, etc.) based on
the non-video sensors. This problem is significant for
Kinesiologists who are studying the usage of walkers
by elderly people. Currently they have to hand label
the data by looking at video feeds of the user, which is
∗
Allan Caine is currently at Research in Motion, Waterloo

• the first fully automated system to automatically
recognize activities performed by walker users;
• design and training (supervised and unsupervised) of probabilistic models (HMMs and CRFs)
tailored to activity recognition with instrumented
walkers;
• comprehensive analysis of these techniques with
real data collected with control subjects and regular walker users living in a retirement community;
• comprehensive analysis of the ease/difficulty to
recognize common walker user activities with existing algorithms.
The paper is organized as follows. Section 2 summarizes related work. Section 3 describes the walker, the
experimental setup and our hypotheses regarding the
ease/difficulty of recognizing common user activities.
Section 4 describes the recognition models (HMMs and
CRFs) and their training procedures (supervised and
unsupervised). Sections 5 and 6 present the results of
the experiments and analyze each approach. Finally
Section 7 concludes and discusses future work.

2

Related Work

In [1], Alwan et al. describe a method that assesses basic walker-assisted gait characteristics, including heel
strikes, toe-off events, as well as stride time, double support and right and left single support phases.
These statistics are based on the measurement of
weight transfer between the user and the walker by
two load cells in the handles of the walker. A simple
thresholding approach is used to detect peaks and valleys in the load measurements, which are assumed to
be indicative of certain events in the gait cycle. This
work focuses on low level gait statistics where as we are
interested to recognize complex high level behaviours.
Hirata et al. [3] instrumented a walker with sensors
and actuators. They recognize three user states: walking, stopped and emergency (including falling). These
states are inferred based on the distance between the
user and the walker (measured by a laser range finder)
and the velocity of the walker. This work is limited
to the three states mentioned above and would not
be able to differentiate between activities that exhibit
roughly the same velocity and distance measurements
(e.g., walking, turning, going up a ramp).
A significant amount of work has been done on activity recognition in other contexts. In particular Liao et
al. [6] use a Hierarchical Markov Model to learn and
infer a user’s daily movements through an urban community. The model uses multiple levels of abstraction
in order to bridge the gap between raw GPS sensor
measurements and high level information such as a
user’s destination and mode of transportation. They
use Rao-Blackwellized particle filters for state estimation. In [5], they also recognize activities and places
from GPS traces by Hierarchical CRFs. This work assumes that the high level goals and routines of the user
as well as the map of the area are known and stable.

3

Experimental Setup

We have access to a walker developed by Tung et al. [9].
A picture of the walker is shown in Fig. 2. The walker
is equipped with various sensors including a 3-D accelerometer in the seat, a load-cell in each leg and a
wheel encoder, which measures the wheel’s displacement. The sensor readings vary between 0 and 216 − 1.
The data is channeled via blue-tooth to a PDA for acquisition at 50 Hz. In addition to these sensors, there
are two cameras on the walker. One is facing backwards and provides the video feed of the user’s legs.
The other is looking forwards and provides the video
feed of the environment. The video frame rate is approximately 30 frames/second. In order to collect data
for our models, we designed and conducted two exper-

Figure 1: Course used for healthy young subjects
Table 1: Behaviours in Experiment 1
Not Touching the Walker (NTW)
Stop/Standing (ST)
Walking Forward (WF)
Turn Left (TL)
Turn Right (TR)
Walking Backwards (WB)
Transfers (Sit to Stand/Stand to Sit) (TRS)

iments that are described below:
3.1

Experiment 1

17 healthy young subjects (age between 19 and 53)
were asked to go through the course shown in Figure
1 twice with the walker. The behaviours exhibited by
the participants are shown in Table 1.
3.2

Experiment 2

In a second experiment, we asked 8 regular walker
users (age 84 to 97) to follow the course shown in Figure 3. This experiment was conducted at the Village
of Winston Park (retirement community in Kitchener,
Ontario) and the participants were residents of that
facility. We also asked 12 adults (age 80 to 89) who
do not live in a retirement community and are not
regular walker users to follow the same course. The
behaviours exhibited during this experiment include
those of Tables 1 and 2.
3.3

Recognizing Behaviours

Our goal is to perform behaviour recognition based
on the non-video sensors.1 Since the accelerometers,
load cells and wheel encoder only measure indirectly
the activities of the person, it is not obvious a priori
which activities can easily recognized. We formulated
the following hypotheses.
1

The use of video data is subject to future work.

range of strategies used by people to lift or lower the
walker.

Figure 2: Smart Walker

Transfers (sit to stand or stand to sit) are also expected
to be difficult to recognize due to a wide range of
strategies. In theory, the load of the walker should decrease as the person goes from standing to sitting and
increase for sit to stand. However, some people leave
the walker to hold other supports such as the arms of
a chair. Some people also engage the walker’s brakes
as they do a transfer while others move the walker.
Another difficult behaviour is the reaching task. It
involves behaviours such as opening a door or picking something from ground. The wheel encoder is not
useful as people’s habits of engaging the brakes during
these behaviours are variable. In reaching tasks, however, the person usually keeps one hand on the walker
while he/she uses the other hand to reach for the object. This can be captured by the load cells as there
will be more weight on one side.
In the next section, we discuss various probabilistic
models to recognize activities as accurately as possible despite the wide range of strategies for some behaviours.

Figure 3: Course for older walker users

4
Not Touching Walker (NTW) should be easy to predict as there is no load on the walker and the walker
is not moving. Standing (ST) should be differentiable
from NTW based on load cell readings as load fluctuations are expected when the person touches the walker.
Walking forward (WF) and walking backward (WB)
should be differentiable from ST based on the wheel
encoder measurements, which increase with forward
movements and decrease with backward movements.
Sitting on walker (SW) should also be easily distinguishable since the value of the load sensors is much
higher than any other behaviour.
We expect turns to be more difficult to predict. We
hypothesize that the speed of the person is lower when
he/she is turning. We also expect a higher load on the
side of the turn and some mild acceleration in the opposite direction of the turn. However this will likely
vary with each person. Similarly, going up or down
ramps and curbs are not expected to be easy to detect.
We expect to see some fluctuations in the vertical acceleration when there is an immediate change of elevation and a sustained mild acceleration corresponding
to gravity in the forward or backward direction depending on the inclination of ramps. We hypothesize
that both these behaviours will be noticeable. However, they may be difficult to distinguish from WF in
general. Furthermore, going up and down curbs may
be particularly difficult to recognize due to the wide

Activity Recognition Models

We assume that the set of all possible behaviours is
B whose cardinality is m. The behaviour at time t is
represented by the random variable Bt . The reading
on sensor k at time t is given by the random variable
Stk where k ∈ {1, . . . , n}. For notational convenience,
we denote the sequence of behaviours from time i to
j by Bi:j and the observation sequence on sensor k
k
by Si:j
. The readings on all sensors observed between
1:n
time i and j is denoted by Si:j
and the actual observed sequence is denoted by s1:n
i:j . The total length
of a sequence is T . The actual behaviour at time t is
denoted by b̂t and the predicted behaviour at time t is
b̃t . Lower case letters denote the assignment of a value
to a random variable.

4.1

Hidden Markov Model

We construct a hidden Markov model in which the behaviour is the hidden variable and the sensor readings
are the observations (Fig. 4). The parameters include
θb0 ,b ≡ Pr(Bt = b0 |Bt−1 = b) (probability that the
behaviour at time t is b0 given that the behaviour at
time t−1 is b), φs,k,b ≡ Pr(Stk = s|Bt = b) (probability
that the value measured by the k th sensor at time t is
s given that the behaviour is b) and πb ≡ Pr(B0 = b)
(probability that the initial behaviour is b).

Table 2: Additional Behaviours in Experiment 2
Going up Ramp (GUR)
Going down Ramp (GDR)
Sitting on Walker (SW)
Reaching Task (RT)
Going up Curb (GUC)
Going down Curb (GDC)

Figure 4: HMM for behaviour recognition

4.1.1

Maximum Likelihood (ML) Learning

For supervised learning, we manually label the data
based on the video feed. We learn θb0 ,b by counting the
number of times behaviour b is followed by behaviour
b0 in the labeled data:
PT
δ (Bt =b0 &Bt−1 =b)
PT
θb0 ,b = t=1
∀b, b0 ∈ B
δ(B
=b)
t=1

t−1

Here δ(x) = 1 if x is true and 0 otherwise. However, in
some of our experiments, to avoid the bias introduced
by using a fixed course, we use a simple transition
model that reflects the fact that behaviours are τ times
more likely to persist than to change.
(
τ /m + τ − 1 if b = b0
∀b, b0 ∈ B
θb0 ,b =
1/m + τ − 1 otherwise
We can learn the prior πi from data using
PT
πb = t=1 δ (Bt = b)/T
Again, in some of our experiments, to avoid the bias
introduced by using a fixed course, we consider all behaviours to be equally likely initially. Hence
πb = 1/m ∀b ∈ B
We can model φs,k,b with a parametric density function such as a Gaussian. However, on close analysis of
the data, we found that the distribution of φs,i,j does
not follow a Gaussian. Therefore, we discretize the
sensor readings by dividing the range into D discrete
intervals. We learn φs,k,b using
PT

φs,k,b =

δ (Bt =b&Sti =s)
PT
t=1 δ(Bt =b)

t=1

∀b ∈ B, ∀s ∈ {1, . . . , D}
∀k ∈ {1, . . . , n}

ML with EM algorithm For unsupervised learning,
we use Expectation Maximization (EM) [2] to learn the
parameters. The algorithm alternates between com-

puting the expectations
E0 (b) = Pr(B0 = b|s1:n
1:t , π, θ, φ),
PT −1
0
E(b, b ) = t=1 Pr(Bt = b, Bt+1 = b0 |s1:n
1:t , π, θ, φ),
PT
Ek (b, s) = t=1 Pr(Bt = b, Stk = s|s1:n
,
1:t π, θ, φ)
and updating the parameters
πb = E0 (b),
P
0
θb0 ,b = E(b, b0 )/ P
b0 E(b, b ),
φs,k,b = Ek (b, s)/ s E(b, s).
Prediction: Note that since the sensor readings at
any given time are conditionally independent given
1:n
the
Q behaviour at that time, therefore, Pr(st |Bt ) =
iI φst ,i,b . We use maximum a posteriori filtering to
infer the most likely behaviour
 given past observations:
b̃t = maxb∈B Pr Bt = b|s1:n
1:t . This computation can
be performed online as only past observations are used.
4.1.2

Bayesian Learning

Bayesian learning is an alternative to maximum likelihood learning. We start from a prior distribution and
update it using Bayes rule to obtain the full posterior
distribution over the variables of interest. By considering the full posterior over the parameters, we hope to
avoid the over-fitting issues, often experienced by the
EM algorithm. Unfortunately, in most cases the posterior does not have a tractable form, so we cannot sample from it directly. Consequently, we resort to sampling techniques based on simulating a Markov chain
whose stationary distribution is the posterior distribution of interest.
A convenient choice of a prior distribution over
the parameters is a product of Dirichlet distributions
of the form Dir(θ1 , . . . , θk ; c1 , . . . , ck ) =
P
Q c1 c2
P
Γ(
Q ci )
θ1 θ2 . . . θkck where i θi = 1. Each DirichΓ(ci )
let is a prior for the corresponding multinomial distribution of transitions or emissions from some state.
The values ci can be understood as counts indicating
how many times a particular transition/emission has
been observed. The Dirichlet distribution is a conjugate prior for the multinomial distribution. Therefore, if the hidden states are known, the posterior distribution of the parameters will also be a product of
Dirichlets with the counts updated by the number of
observed transitions or emissions. If the hidden states
are not known, the posterior becomes a mixture of exponentially many products of Dirichlets, each product
corresponding to one possible state path.
We use Gibbs sampling to estimate the posterior over
hidden states and parameters. We repeatedly sample
each variable from the conditional distribution given
all the other variables. It can be shown that the resulting Markov chain converges to the joint distribution of

Table 3: HMM results for Experiment 2 using center of pressure (COP) features. Behaviour persistence parameter:
τ = 4000. Window size is 25. Observations are accelerometer measurements, speed, frontal and sagittal center of pressure
and total weight. Overall accuracy is 77%.
NTW

ST

WF

TL

TR

WB

RT

SW

GUR

GDR

GUC

GDC

NTW

821

7434

0

95

0

0

89

68

16

7

14

0

9.58

ST

1393

35203

360

3103

550

2165

6831

595

355

61

771

251

68.17

WF

239

2663

42297

6416

12486

763

3167

0

1283

657

618

828

59.23

TL

41

756

1076

12165

516

1095

705

51

141

152

149

20

72.12

TR

23

404

1743

784

10623

873

621

0

146

333

159

123

67.10

WB

0

0

0

174

58

447

251

0

25

3

54

33

42.78

RT

529

3837

516

1141

545

693

4227

25

221

88

263

202

34.40

SW

15

151

33

171

61

0

106

70769

14

0

4

10

99.21

GUR

0

0

62

20

18

0

52

0

2382

153

154

157

79.45

GDR

0

20

55

176

124

26

29

0

24

2389

81

270

74.80

GUC

0

17

19

40

18

0

49

0

5

0

2939

0

95.21

GDC

0

20

27

15

171

0

67

0

32

35

22

1311

77.12

all variables. Here, we use the collapsed Gibbs sampler
[7] which samples the hidden states and integrates out
the parameters to speed up the convergence.
Since we use a Dirichlet prior, the posterior probability
Pr(B1:T , s1:n
1:T ) can be computed analytically:
Pr(B1:T , s1:n
(1)
1:T )
ˆ
=
Pr(B1:T , s1:n
(2)
1:T |π, θ, φ) Pr(π, θ, φ)dπdθdφ
πθφ
Q
Y Q 0 Γ(αbb0 ) Y Q k Γ(βbsk )
B1 Γ(γB1 )
s
bP
P
(3)
=c P
Γ( B1 γB1 )
Γ( b0 αbb0 )
Γ( sk βbsk )

4.2

Conditional Random Field

Conditional Random Fields (CRFs) are probabilistic
models for segmenting and labeling sequential data
[4]. We consider the special case of linear-chain CRFs.
A CRF specifies the distribution of a sequence of labels (B1:t ) conditioned on a sequence of observations
1:n
(S1:t
). In our experiments, the labels are the behaviour of the user, and the observations are the sensor measurements at each time. The probability of
1:n
B1:t = b1:t conditioned on S1:t
= s1:n
1:t is given by
1:n
1:n
Pλ (B1:t = b1:t | S1:t
= s1:t
)=

b,k

b

T
Y

where α’s, β’s and γ’s are transition, emission and initial state counts. By taking Pr(Bt |B1:t−1 , Bt+1:T , s1:n
1:T ) =
Pr(Bt , B1:t−1 , Bt+1:T , s1:n
1:T )/

P

Bt

Pr(Bt , B1:t−1 , Bt+1:T , s1:n
1:T )

and simplifying, we get:
Pr(Bt |B1:t−1 , Bt+1:T , s1:n
1:T )
∝

Accuracy %

αB
B
P t−1 t
b αBt−1 bt
t

·

P

αBt Bt+1
αBt bt+1

bt+1

·

Qn

k=1

βB sk
P t t
s βBt s

By repeatedly sampling each hidden state according to
the distribution above, we are guaranteed to converge
to the posterior distribution Pr(B1:T |s1:n
1:T ).
Although we integrate out the parameters analytically,
we can sample efficiently from the distribution over the
parameters. For any assignment to B1:T , the conditional distribution Pr(π, θ, φ|B1:T , s1:n
1:T ) is a product
of Dirichlet distributions. Since the Gibbs sampler
provides us with a way to sample from Pr(B1:T |s1:n
1:T ),
sampling from Pr(π, θ, φ|s1:n
1:T ) can be done efficiently.
Gibbs sampling can be used both for learning and prediction. Here, we only use it for learning to simplify
the comparison to other methods.

1:n

eµ·f (st

t=1

,bt )



×

T
Y

1
×
Z(s1:n
1:t )


(4)

eν·g(bt−1 ,bt )

t=2

1:n
where Z(s1:t
) is a normalizing constant, f (s1:n
t , bt ) is
a state feature function (possibly vector-valued) with
the corresponding weights µ, and g(bt−1 , bt )) is a transition feature function with the corresponding weights
ν. Intuitively, the feature functions allow to incorporate which observations and labels are likely to occur
together. Usually, the feature functions are kept fixed
and the weights are learned from training data. Given
a sequence of labeled training data, the most common approach to find the model weights is minimizing
the negative log-likelihood. Writing λ for the stacked
weights (µ, ν), the objective function is given by

L (λ)

= −

T
X
t=1

T
X

µ · f s1:n
,
b
−
ν · g (bt−1 , bt )
t
t
t=2

 λT λ
1:n
+ log Z s1:T
+
2σ 2

(5)

where the last term on the right hand side is a shrinkage prior to penalize large weights. In our experiments

Table 4: CRF results for Experiment 2 using center of pressure (COP) features. Window size is 25. Observations are
accelerometer measurements, speed, frontal and sagittal center of pressure and total weight. Overall accuracy is 81%.
NTW

ST

WF

TL

TR

WB

RT

SW

GUR

GDR

GUC

GDC

Accuracy %

NTW

6674

1834

0

0

0

0

0

36

0

0

0

0

78

ST

1017

48677

1100

296

0

0

129

374

0

0

45

0

94

WF

0

2490

67738

744

270

0

25

112

0

0

38

0

95

TL

0

2081

5958

8528

97

0

72

117

0

12

2

0

51

TR

0

1517

10632

350

3129

0

62

34

0

69

39

0

20

WB

0

383

501

55

29

0

59

0

18

0

0

0

00

RT

387

7787

3071

327

24

0

634

46

11

0

0

0

5

SW

0

271

147

77

0

0

0

70839

0

0

0

0

99

GUR

0

173

1647

28

0

0

0

32

938

0

180

0

31

GDR

0

95

1753

0

575

0

0

0

0

720

41

10

23

GUC

0

436

375

0

89

0

122

0

0

0

2065

0

67

GDC

0

156

982

0

1

0

0

0

0

339

9

213

13

we chose σ 2 = 1, However, we found that scaling σ 2 by
factors up to 10 and 10−1 , does not yield big differences
in the accuracy of the resulting models. Since the objective function is convex, its unique minimum can be
found using gradient-based search. The term Z(s1:n
1:t ),
which also depends on λ, can be efficiently evaluated
using dynamic programming [8]. We use conjugate
gradients to minimize the negative log-likelihood and
stop training after 100 iterations.
To predict behaviours, we consider the speed of the
walker and the acceleration in x, y and z-direction.
Instead of using the raw data of the load sensors, we
consider the following measurements: the total load
(which is just the sum of the loads on each wheel), the
frontal plane center of pressure (which is the difference
between the loads on the left and on the right side
divided by the total load) and the sagittal plane center
of pressure (the difference between the loads on the
rear and front wheels divided by the total load).
Our state feature functions are based on thresholding:
for each pair of behaviours b, b0 and each sensor k, we
compare the actual observation to a fixed threshold
value. If the value is exceeded, we add some model
(e)
(n)
weight µbb0 k , otherwise, we add some weight µbb0 k . We
choose the threshold values manually by a visual inspection of the data. If the labels b, b0 cannot be well
discriminated by looking at the data from sensor k, the
threshold is chosen as the average value from sensor k;
later on, such “irrelevant” thresholds will be given very
low weights in the training of the model.
We use a very simple transition model to avoid a bias
towards certain transitions due to the design of the
walker course. In particular, the transition feature
function is given by g(bt−1 , bt ) = δ(bt−1 = bt ), hence
the corresponding weight ν is a scalar.

Given the model parameters and an observation sequence, the predicted behaviour sequence b̃1:T maximizes the a-posteriori probability of B1:T ,
b̃1:T

1:n
= arg max P (B1:T = b1:T | S1:T
= s1:n
1:T )
b1:T

where the maximization is over all label sequences of
length T . Note that b̃1:T can be computed efficiently
similar to the Viterbi algorithm for HMMs.

5

Results

We present results for the HMM and CRF for both
experiments. We use leave one out cross validation for
each round of training and prediction. Specifically, if
we want to predict the behaviour sequence for a certain participant in Experiment 2, we learn the parameters from all other participants in Experiment 2. For
Experiment 1, each person goes through the course
twice. We included one instance of the course in the
training data along with data from other participants
while testing for the same person.
Since the range of sensor readings is too large (all integers from 0 to 216 − 1), we divide the range into
20 intervals and set their length in such a way that
the same number of readings fall into each interval.
Since the participants’ weight varies, we also normalize
the load cell readings as follows: normalizedV alue =
(value − min)/(max − min).
Ground truth is established by hand labeling the data
based on the video. This process is not perfect since
the labeler can make mistakes while identifying behaviour transitions. For example, the labeler may interpret that a left turn started at time t, while the turn
may actually start some time before t, but it only becomes evident in the video at time t. Therefore, in
order to calculate our error, we introduce the concept

Table 5: Experiment 1 percentage accuracy for each behaviour. NL means the normalized load values are used. COP
implies that center of pressure feature is used instead of normalized load values. LOD means observation model is learnt
from data and transition model uses τ for behaviour persistence. LOTD means the observation, transition and prior
probabilities are learned from data. Window size is 25.
Learning

Accuracy
NTW

ST

WF

TL

TR

WB

TRS

Total

Supervised HMM NL

65

88

95

96

92

95

85

91

Supervised HMM COP

75

85

89

93

89

98

78

88

Supervised HMM LOTD NL

67

89

96

95

91

95

85

92

Supervised CRF

96

82

98

89

80

94

70

93

Unsupervised HMM EM

100

14

48

94

90

97

0

61

Unsupervised HMM Gibbs

100

72

95

66

72

44

17

83

Table 6: Experiment 2 percentage accuracy for each behaviour. NL means that normalized load values are used. COP
implies that center of pressure feature is used instead of normalized load values. LOD means that the observation model
is learnt from data and the transition model uses τ for behaviour persistence. LOTD means the observation, transition
and prior probabilities have been learned from data. Window size is 25
Learning

Accuracy
NTW

ST

WF

TL

TR

WB

RT

SW

GUR

GDR

GUC

GDC

Total

Supervised HMM NL

50

71

73

81

73

21

52

99

86

86

94

85

81

Supervised HMM COP

20

74

70

76

74

56

42

99

84

81

95

82

77

Supervised HMM LOTD NL

49

74

78

81

73

21

50

99

85

85

94

84

81

Supervised CRF

78

94

95

51

20

0

5

99

31

23

67

13

81

Unsupervised HMM EM

73

33

54

63

63

7

30

99

40

23

13

91

62

Unsupervised HMM Gibbs

100

42

77

69

81

0

62

91

57

52

46

2

69

of window size. If the window size is x, then for the
behaviour at time t, if we find the same behaviour in
the window between time t − x and time t + x in the
predicted sequence, we count it as a correct prediction.
We vary our window size from 0 to 50 in intervals of
5. If we make a correct prediction within a window
width of 25, then we are only off by half a second.
Since older people perform behaviours at a rate that
is much slower than half a second, this may still be
considered accurate.
Tables 3 and 4 show the results in the form of confusion matrices for Experiment 2 when performing supervised learning with an HMM and a CRF. In both
cases, accelerometer measurements, speed, frontal and
sagittal center of pressure (COP) and total weight are
used as observations instead of the raw measurements
(see Sect. 4.2 for more details). Each entry at row i
and column j indicates how many times behaviour i
was confused as behaviour j, assuming a window of
size 25.
Due to a lack of space, we did not include the confusion matrices for Experiment 1 and for the unsupervised learning algorithms, however Tables 5 and 6
summarize the recognition accuracy of all the algorithms 
for each experiment. We define accuracy as
PT
t=1 δ b̂t = b̃t /T . Note that random predictions

would yield an accuracy of 1/7 = 14% in Experiment
1 and 1/13 = 7% in Experiment 2.
In some situations, identifying transitions from
some behaviour to another is what really matters. Hence, Fig. 5 shows the precision and recall of behaviour transitions (in addition to recognition accuracy) as a function of the window size.
We calculate the number 
of actual transitions (i.e.,
PT
AT =
t=1 δ b̂t 6= b̂t−1 ), the number of pre
P|T | 
dicted transitions (i.e., P T =
t=1 δ b̃t 6= b̃t−1 )
and the number ofcorrectly predicted transitions (i.e.,

PT
CP T =
δ
b̃
=
6
b̃
&
b̂
=
b̃
&
b̂
=
b̃
).
t
t−1
t
t
t−1
t−1
t=1
Then precision = CP T /AT and recall = CP T /P T .

6

Discussion

In this section, we analyze the results presented in the
previous section.
Experiment 1 vs. Experiment 2: It is obvious
from Tables 5 and 6 that the overall accuracy is much
higher for Experiment 1. Difficult behaviours such as
TR, TL and TRS are also predicted accurately. In
fact, the accuracy for TRS is much higher than expected. One reason for the high accuracy may be that
in Experiment 1, each person executes the course twice

Figure 5: Accuracy, Precision and Recall for various algorithms in each experiment.
and one instance of the course execution is included in
the training data while testing for the same person.
Therefore, the training data may include information
that is more specific to the particular person e.g., how
people put load on the walker for different behaviours.
Secondly, for Experiment 1, behaviours such as NTW,
ST, WF and WB are easily distinguishable from each
other. Confusion is only likely between WF, TL and
TR and between ST and TRS. There is a larger number of behaviours in Experiment 2 that are difficult to
distinguish from each other based on sensor information e.g. WF, TL, TR, GUR, GDR, GUC and GDC.
Similarly it is difficult to distinguish between RT and
ST.
Center of Pressure (COP) vs. Normalized Load
Values: In addition to the accelerometer values and
the walker speed, we considered two alternative sets of
features for prediction. One set includes the normalized load cell measurements while the other includes
the frontal plane COP, the sagittal plane COP and the
total load on the walker. It is evident from Tables 5
and 6 that both sets of features predict different behaviours well. When the COP features are used, we
note that the prediction accuracy is lower for TR, TL,
GUR, GDR, GUC and GDC and higher for the other
behaviours.
CRF vs. HMM: Tables 5 and 6 show that the
total accuracy of the CRF is much higher than
that of the HMM. This is largely due to the fact
that the CRF models Pr(Behaviours|Observations)
directly and optimizes the parameters of this
distribution.
On the other hand, the HMM
models Pr(Observations|Behaviours) as well as
Pr(Behaviours), and then uses Bayes rule to calculate

Pr(Behaviours|Observations). Therefore, the HMM
model is more complex and the number of parameters
that we have to learn is larger. Also, the HMM makes
an explicit assumption about the conditional independence of sensor measurements over time. The CRF
avoids this (problematic) assumption since it does not
model any distribution over the observations. However, techniques for unsupervised learning are better
established for HMMs than CRFs. This becomes an
important advantage since we do not need to label data
in unsupervised learning.
We were surprised to see that the HMM was able to
predict certain behaviours better than the CRF. In
general, the HMM seems to favor behaviours that occur infrequently. For example, in Table 3, we can
see that the prediction accuracy of GUR and GUC
is higher than that of WF. We expected that it would
be difficult to accurately predict infrequent behaviours
such as GUR and GUC. Note also that WF is often
predicted as GUR and GUC. We suspect that this is
due to the assumption of conditional independence between different sensors.
Learning Transition Model from Data: As discussed in Sect. 3, the experiments include a pre-defined
walking course that biases the behaviour transitions.
This is why we considered two scenarios when learning an HMM: i) fixed transition model where each behaviour is τ times more likely to persist than to transition to some other behaviour with uniform probability
(denoted by LOD in Tables 5 and 6) and ii) learned
transition model (denoted by LOTD). Naturally, the
accuracy is higher when the transition model is learned
from data. In future work, we plan to collect data
with participants in their daily activities instead of a

scripted walking course, which will allow us to learn
realistic and personalized transition models.
ML vs. Bayesian Learning: As explained previously, manually labeling the data is a time consuming and error-prone. Unsupervised learning algorithms
avoid this problem. However, they take longer to converge and the solutions are usually approximations to
the optimal parameters. It is interesting to note from
Table 5 that for Experiment 1, the Gibbs sampling accuracy for NTW and ST is actually higher than other
algorithms. One important difficulty with unsupervised learning is state matching. On one hand, unsupervised learning algorithms may pick sub-behaviours
of composite behaviours as a state. For example, the
ramp transition can be broken up into getting on the
ramp (corresponding to a blip in the vertical acceleration), and walking on the ramp. The algorithm
may find a state that matches either one or both instead of the behaviours going up/down ramp. On the
other hand, if two behaviours are similar, the algorithm might treat them as the same state. We observe
that in Table 5, TRS is almost never predicted correctly. This may be due to the fact that TRS is very
similar to ST and hence they are merged into one state.
For EM and Gibbs sampling, once a model is learned,
we manually associate each latent state with the behaviour that is the most frequent. In general we used
a number of latent states equal to the number of behaviours, except for Gibbs sampling in Experiment 1
(Table 5) where we used more latent states (11) than
behaviours (7). As a result, the accuracy improved.
In future work, we would like to investigate more thoroughly whether using a larger number of latent states
generally improves the accuracy.
Since EM often gets stuck in local optima, we did 20
random restarts and showed the results of the model
with the highest likelihood. In contrast, Gibbs sampling does not suffer from this problem. It is evident
from Tables 5 and 6 as well as Fig. 5 that Gibbs sampling performs better than EM.

7

Conclusion and Future Work

This paper presented a novel and significant application of activity recognition in the context of instrumented walkers. We designed several algorithms based
on HMMs and CRFs, and tested them with real users
at the Village of Winston Park (retirement community in Kitchener, Ontario). A comprehensive analysis
of the results showed that behaviours associated with
walker usage tend to induce load, speed and acceleration patterns that are sufficient to distinguish them
with reasonable accuracy. In the future, we would like
to further improve the recognition accuracy, by using

the video data and exploring various feature extraction techniques. We are also working on improving
the battery life and memory capacity of the walker to
collect data over long periods of time. In particular,
this will allow us to loan the walker to users, record
their daily usage and learn realistic and personalized
behaviour transition models. Finally, we hope to turn
this work into a clinical tool that can be used to assess
the mobility patterns of walker users and the contexts
in which they are more likely to fall.
Acknowledgments
This work was supported by funds from CIHR,
NSERC, the Ontario Ministry of Research and Innovation (Pascal Poupart’s ERA) and the Canadian government (Mathieu Sinn’s postdoctoral fellowship). We
also thank the UW-Schlegel Research Institute for Aging and the Village of Winston Park as well as all the
volunteers who participated in the experiments.



We consider the problem of approximate belief-state
monitoring using particle filtering for the purposes
of implementing a policy for a partially observable
Markov decision process (POMDP). While particle fil­
tering has become a widely used tool in AI for monitor­
ing dynamical systems, rather scant attention has been
paid to their use in the context of decision making. As­
suming the existence of a value function, we derive er­
ror bounds on decision quality associated with filtering
using importance sampling. We also describe an adap­
tive procedure that can be used to dynamically deter­
mine the number of samples required to meet specific
error bounds. Empirical evidence is offered supporting
this technique as a profitable means of directing sam­
pling effort where it is needed to distinguish policies.

1

Ortiz

Computer Science Department

Introduction

Considerable attention has been devoted to partially observ­
able Markov decision processes (POMDPs) [19] as a model
for decision-theoretic planning. Their generality allows one
to seamlessly model sensor and action uncertainty, uncer­
tainty in the state of knowledge, and multiple objectives
[ 1, 4]. Despite their attractiveness as a conceptual model,
POMDPs are intractable and have found practical applica­
bility in only limited special cases.

The predominant approach to the solution of POMDPs in­

volves generating an optimal or approximate value func­
tion via dynamic programming: this value function maps
beliefstates (or distributions over system states) into opti­

University ofToronto
Toronto, ON M5S 3H5
cebly@cs. toronto. edu

are considerably more pressing.1
One important family of approximate belief state monitor­

ing methods is the particle filtering or sequential Monte
Carlo approach [6, 13]. A belief state is represented by a
random sample of system states, drawn from the true state
distribution . This set of particles is propagated through the

system dynamics and observation models to reflect the sys­
tem evolution. Such methods have proven quite effective,
and have been applied in many areas of AI such as vision

[11] and robotics [21].
While playing a large role in AI, the application of particle
filters to decision processes has been limited. While Thrun

[20] and McAllester and Singh [14] have considered the use
of sampling methods to solve POMDPs, we are unaware
of studies using particle filters in the implementation of a
POMDP policy. In this paper we examine just this, focus­
ing on the use of fairly standard importance sampling tech­
niques. Assuming a POMDP has been solved (i.e., a value
function constructed), we derive bounds on the error in de­
cision quality associated with particle filtering with a given

number of samples. These bounds can be used a priori to
determine an appropriate sample size, as well as forming

the basis of a post hoc error analysis. We also devise an
adaptive scheme for dynamic determination of sample size

based on the probability of making an (approximately) op­
timal action choice given the current set of samples at any

stage of the process. We note that similar notions have been
applied to the problem of influence diagram evaluation by
Ortiz and Kaelbling [15] with good results-our approach
draws much from this work, though with an emphasis on
the sequential nature of the decision problem.
A key motivation for taking a value-directed approach to
sampling lies in the fact that monitoring is an online pro­

mal expected value, and implicitly into an optimal choice
of action. Constructing such value functions is computa­
tionally intractable and much effort has been devoted to de­

cess that must be effected quickly. One might argue that
if the state space of a POMDP is large enough to require

veloping approximation methods or algorithms that exploit
specific problem structure. Potentially more troublesome is

sampling for monitoring, then its state space is too large to
hope to solve the POMDP. To counter this claim, we note

the problem of

first that recent algorithms [2, 9] based on factored repre­
sentations, such as dynamic Bayes nets (DBNs), can of­
ten solve POMDPs without explicit state space enumeration
and produce reasonably compact value function representa­

beliefstate monitoring-maintaining a be­

lief state over time as actions and observations occur so that

the optimal action choice can be made. This too is gen­
erally intractable, since a distribution must be maintained
over the set of system states, which has size exponential in
the number of system variables. While value function con­
struction is an offline problem, belief state monitoring must
be effected in real time, hence its computational demands

tions. Unfortunately, such representations do not generally
1While techniques exist for generating finite-state controllers
for POMDPs, there are still reasons for wanting to use value­
function-based approaches [ 17}.

454

POUPART ET AL.

UAI 2001

- ()ptimat Value Function

translate into effective (exact) belief monitoring schemes
[3]. Even in cases where a POMDP must be solved in a
traditional "fiat" fashion, we typically have the luxury of
compiling a value function offiine. Thus, even for large
POMOPs, we might reasonably expect to have value func·
tion information (either exact or approximate) available to
direct the monitoring process. The fact that one is able to
produce a value function ojfiine does not imply the ability

to monitor the process exactly in a timely

B<lief Spocc

online fashion.

We overview PO MOPs, structured solution techniques, and

Figure 1: Geometric View of Value Function

We also describe a dynamic sample generation scheme that
relies on ideas from group sequential sampling. We exam­

observation, and rr; is itself a conditional plan. Intuitively,

monitoring in Section 2. Section 3 describes a basic par­
ticle filtering scheme for POMDPs and analyzes its error.

ine this model empirically in Section 4, and conclude with

a discussion of future directions.

2

POMDPs and Belief State Monitoring

2.1

Solving POMDPs

A partially observable Markov decision process (POMDP)

is a general model for decision making under uncertainty.
Formally, we require the following components: a finite
state space S; a finite action space A; a finite observation

space Z; a transition function T

:

S

x

A

-t

�(S);2

an

observation function 0 : S x A -+ l!.(Z); and a reward
function
: S -t R. Intuitively, the transition function

T(s, a )

R

determines a distribution over next states when an
agent takes action a in states. This captures uncertainty in
action effects. The observation function reflects the fact that

an agent cannot generally determine the true system state

with certainty (e.g., due to sensor noise). Finally
notes the immediate reward associated with s.

R(s)

de­

The rewards obtained over time by an agent adopting a spe­
cific course of action can be viewed as random variables

R(t). Our aim is to construct apolicy that maximizes the ex­
E(L�o pt R(t))

pected sum of discounted rewards
(where
(J is a discount factor less than one). It is well-known that
an optimal course of action can be determined by consid·
ering the fully observable belief state MDP, where belief

states

l!. (S)

(distributions over
-t

S)

form states, and a policy

rr

:

A maps belief states into action choices. In prin­

ciple, dynamic progranuning algorithms for MDPs can be
used to solve this problem. A key result of Sondik [ 19]
showed that the value function V for a finite-horizon prob·
!em is piecewise-linear and convex and can be represented
as a finite collection of a-vectors.3 Specifically, one can

generate a collection N of a-vectors, each of dimension lSI,

V(b)

such that
= maXaeN ba. Figure 1 illustrates a collec­
tion of a-vectors with the upper surface corresponding to
We define ma( b) = arg max,.e �
to be the maximizing
a-vector for belief state

b.

ba

V.

Each a
E
� corresponds to the expected value of
executing an implicit conditional plan at a given be­
lief state.
This conditional plan, rr (a), has the form
(a; Ot, 71"1; oz, rrz; · · ·On, 7rn), where a is an action, Oi is an
2fl.(X) denotes the set of distributions over finite set X.
3

For infinite-horizon problems, a finite collection may not al­
ways be sufficient, but will generally offer a good approximation.

a plan of this form denotes the performance of action a fol­
lowed by execution of the remaining plan rr; in response
to observation oi. We denote by A(a) the (first) action
a of 1r(a). Given belief state b, the agent should execute

the action with the maximizing a-vector: A(ma(b)). In­
deed, if one has access to the entire plan 'IT(ma(b)) , this plan
should be executed to termination. We note, however, that
the plans 'IT (cr ) are rarely recorded explicitly.

One difficulty with these classical approaches is the fact
that the a-vectors may be difficult to manipulate. A sys­

tem characterized by

n

random variables has a state space

size that is exponential in

Thus manipulating a single

n.

a-vector may be intractable for complex systems.4 Fortu­

nately, it is often the case that an MOP or POMDP can be

specified very compactly by exploiting structure (such as
conditional independence among variables) in the system
dynamics and reward function

[I].

Representations such as

dynamic Bayes nets (DBNs) can be used, and schemes have
been proposed whereby the a-vectors are computed directly
in a factored form by exploiting this representation.
Boutilier and Poole

[2],

for example, represent a-vectors

as decision trees in implementing Monahan's algorithm.
Hansen and Feng [9] use algebraic decision diagrams
(ADDs) as their representation in their version of incre­
mental pruning. The empirical results in

[9]

suggest that

such methods can make reasonably sized problems solv­
able. Furthermore, factored representations will likely fa­
cilitate good approximation schemes.

2.2

Belief State Monitoring

Given a value function represented using a collection N
of a-vectors, implementation of an optimal policy requires
that one maintain a belief state over time in order to ap­

ply it to N. Given belief state bt at timet, we determine
at = A(ma(bt)), execute at, make a subsequent obser­
vation ot·f.l , then update our belief state to obtain bt+l.
The process is then repeated. Belief state monitoring is ef­
t
fected by computing bt+l = Pr(SW, a , ot+l , which in­
volves straightforward Bayesian updating. We denote by

)

T(b, a, o)

the update of any belief state

observation

o.

We inductively define

T(b,a1,011
, an, on)=
T(T
(T(b,
a1, 01 ) ,
_,_
·

·

4

___

·

_

·

·

b by

action

a

and

·

·

·

·

, an-11 On-l)an, on)

The number of a-vectors can grow exponentially in the worst
case, but can often be approximated.

UAI 2001

Even if the value function can be constructed in a com­
pact way, the monitoring problem itself is not generally
tractable, since each belief state is a vector of size IS 1. Un­
fortunately, even using DBNs does not alleviate the diffi­
culty, since correlations tend to "bleed through" the DBN,
rendering most (if not all) variables dependent after a time
[3]. Thus compact representation of the exact belief state
is typically impossible. Belief state approximation is there­
fore often required. At any point in time we have an ap­
proximation (;t of the true belief state bt, and must make our
decisions based on this approximate belief state. While sev­
eral methods for belief state approximation can be used (in­
cluding projection, aggregation, and variational methods),
and important class of techniques for dynamic problems is
sampling or simulation methods.
3

Particle Filtering for POMDPs

In this section we examine the impact of particle filtering
on decision quality in POMDPs. We first describe a typical
sequential importance sampling algorithm, and discuss the
use of partial evidence integration (EI) in the DBN to help
keep samples on track. We then analyze the error induced
by one stage of belief state approximation and show how
partial EI allows this analysis to be carried through multiple
stages (in a way that is not possible otherwise).
3.1

A

455

POUPART ET AL.

Basic Filtering Method for POMDPs

Assume we have been provided with the value function for
a specific POMDP M. This value function is represented
by a finite collection� of a-vectors. We assume an infinite­
horizon model so that we have a single set �. We also
assume that N is of a manageable size, and that the vec­
tors themselves are represented compactly (using ADDs,
decision trees, linear combinations of basis functions, or
some other representation). We emphasize, however, that
even if the value function is represented in standard state
form, approximate monitoring is often needed. We note that
our methods can be applied to approximate value functions,
though our analysis assumes an exact set�Implementation of the policy induced by this value function
requires that a belief state bt be maintained over all times
t. At any point in time we assume an approximation bt of
the true belief state bt, and make our decisions based on this
approximate belief state.
The basic procedure we consider is the use of a particle filter
for monitoring, with the approximate belief states so gener­
ated used for action selection in the POMDP. At any timet,
we have a collection bt ofnt weighted particles, or system
states, approximating the true distribution bt. Each particle
is a pair (s(i), w(;)). We often simply write s(;) to refer to

the it 11 particle ( i � nt). The total weight of the particle
set bt is wt ::: L: w(i)· The particle set b1 represents the
following distribution (which we also refer to as bt):

_
L:{w(i): s(i)::: s}
t
b(s):::
wt
Given this approximation bt of b1, action selection will take

0----8 0---- 8
1
��
�
8
(b)

(a)

Figure 2: Partial Evidence Integration
place in the POMDP as if b1 were the true distribution.
Thus, we let at = A(ma(b1)), execute action a1, and make
observation ot+l. Our new approximate belief state t;t+l
is generated by repeating the following steps until nt+I is
greater than some desired threshold:
1.

Draw a state s1 from the distribution b1•

a state s1+1 from the distribution
Pr(st+1ls1, a1).
3. Compute w::: Pr(o1+11st, at, st+l)
4. Add sample {s(i)\ w(i)1) ::: {st+l w) to bt+l and add
w to total weight w1•
2. Draw

1

This sequential importance sampling procedure induces a
consistent, though biased, estimate t;t+l of bt+1, and will
converge to the true distribution according to the usual con­
vergence results. The significance of this method lies in the
fact that, for a great many systems, it is easy to sample suc­
cessor states according to the system dynamics (i.e., sample
from the conditional distribution in Step 2), and to evaluate
the observation probabilities for given states (i.e., compute
the weights in Step 3). In contrast, direct computation of
Pr(S1+llb1, at, o1+1) is generally intractable.
3.2

Evidence Integration

One difficulty with the filtering algorithm above is that the
samples generated at time t + 1 are not influenced by ob­
servation o1+1, which often allows particles to drift from
the true belief state. Since we assume a DBN representa­
tion of dynamics, partial evidence integration (El) or arc
reversal [8] can be used to partially alleviate this problem
[13]. The generic structure of a DBN (assuming a fixed ac­
tion) is shown in Figure 2(a); reversing the arc from St+l to
ot+I results in a network shown in Figure 2(b). With this
structure, given a particle s( and observation ot+l, a par-

q

ticles(� 1 can be drawn directly. Of course, the reweighting

given ot+l must now be applied to the particles in b1. This
gives rise to the following particle fli tering procedure used
throughout the remainder of the paper:
(a) Given particle set b1, select action
A(ma(bt)), and observe ot+1;
(b) Reweight

samples

s(i)

at

according

to

Pr(o1+11st, a1) and normalize to produce
i/;

(c) Draw some number of particles s ( i) according to

l/;

456

POUPART ET AL.

(d) Sample particles

s(� 1

where R"' is the range of values that can be taken by a (i.e.,

given drawn prior particles

b- l
and a1+1 to produce t+ .

s1.(�)

Ra

// is an approxi­
, o1+1) in contrast to b1,
a1-1, ol, . . . , 01 ).

Note that the reweighted distribution
mation of Pr( st I a0,
which represents Pr(S1
•

·

·

at

, o1,

ia0,

•

•

•

.

UAI 2001

• •

=

maxs{a(s)}- mins{a(s)}).

Given a particular confidence threshold
of size

n

t

the accuracy of our estimate

When the DBN is factored, the arc reversal process can of­

tage of the structure in CPTs represented as, say, decision
trees or ADD. In this way, the usual exponential increase in

v�:

(1)

co:=

ten be fairly expensive, since it increases the connectivity of

the network. However, the reversal process can take advan­

c) and a sample set

we can produce a (one-sided) error bound c:,.. on

The required sample size given error tolerance c and confi­

dence threshold

table size with the number of added parents is often circum­

0 for the estimation of

v,; is:

vented [5]. We use structured arc reversal techniques in our

(2)

experiments.

3.3

We can also bound the simultaneous confidence that each of

One-Stage Analysis

As a precursor to bounding the error in decision quality
associated with particle filtering, we consider the error in­

duced by one stage of approximation only (and acting using

exact inference at all other stages). We first note the follow­

our es ti mates of each a W)

1- J.

has (one-sided) precision£ with

1�1 in Eq. 2 and maximiz­
ing over all a, we obtain the sample size Nt(c, 8):
probability

Decreasing J to

N t (t o) = r::ea;;N�(c, �l)
l
,

ing important fact regarding POMDPs:

(3)

Let bt, ll be two belief states s.t. maW) = ma(b1).
For any sequence of k observations and actions,
T(b1,a1,o1+1,···a1+k-l,at+k) and
let bt+k
/;t+k
T(l} ) at , ot+l ) . . ·at+k-1 ' ot+k) .
Then
maW+k) = ma(bHk).

with (arbitrary) nonoptimal behavior is bounded by h, then

way that b has the same maximizinga-vector as b1, then we

Theorem

Fact 1

This implies that, if we approximate

1

b1

at timet in such a

will: (a) choose the correct action at state t; and (b) choose

the optimal action at all subsequent stages if we monitor the
process exactly (w.r.t.

bt) at all subsequent stages.

Now, assume we have been able to exactly compute
have selected and executed action

at-l

bt-l,

we can sample directly from the distribution

T(bt-l, a1-1, o1)
an un bi as ed

b1

using the (arc-reversed) DBN to obtain

estimate b1 of bt.

We analyze the error associ­

ated with selecting ana-vector that has maximum expected
value w.r.t.

tion

is made with probability at least 1- o; if the error associated
the one-step approximation error is given by the following:

particles, with exact monitoring used at all other stages of
the process, then the error E (i.e., difference in expected
value of the policy implemented and the optimalpolicy) is
bounded by

Here the error incurred is discounted by
cess. Note that the error

easily bounded

b-t and executing its conditional plan to comple­

that point on).

s(i)} be a collection ofnt state samples drawn fromb1.

The value of any

a

E

N applied to true belief state b1

W J =a. b1 = Eb·[a(s)] = v�

is:

<
h-

s) denotes the value of a at state

s

such , each term

a (s(i))

(i.e., the s1h

bt) is V�.

As

v�

v�.

is a sample of this random variable

and the average of these is an unbiased estimate

of

We can apply (one-sided) Hoeffding bounds to determine
the accuracy of this estimate. Specifically:

Pr(V� ::; V� +c)
Pr(V� ;:::_ V�- c)

>
>

maxmaxa,•

...

f3 m in , { R ( s) }
1-

f3

though simple domain analysis will generally yield much
tighter bounds on

h.

post hoc analysis on the choice of
optimal choi ce has been made

a-vector to determine if an

component of a) and Eb' denotes expectation with respect
to distribution b1• Thus the value of a can be viewed as
a random variable whose expectation (w.r.t.

h on nonoptimal behavior can be

(rather loosely) using

One can also perform a

a
where a (

f3t+l to reflect the

fact that the approximation error occurs at stage t of the pro­

(or equivalently, acting using exact monitoring from

Let {

If beliefstate I/ is approximated with Nt ( E' o)

2

and made ob­

at. Furthermore, assume that we can com­
Pr(st-1la1-1, d) exactly. With these assumptions,

servation

pute

Choosing the maximizing a-vector using an approximate l/

with sample size N1 ( c, J) ensures that a 2t-optimal choice

1-

e-2n•,>;n;_

1-

e-2n',2;n;_

with high probability. Assuming n 1 samples have been gen­
erated, let£� be the error level determined by Eq. 1 using n t

(this is generally tighter than the c used to determine sample
size in Eq. 3 since we are looking at a specific vector).
Corollary

3

Let

btat -£�.

at = ma(bt) and suppose that
+

r

:2:

bta+t�,

Va EN\

{at}

Then with probability at least 1- oar-optimal policy will
be executed, and our error is bounded by:

UAI 2001

POUPART ET AL.

The parameter T represents the degree to which the value of
the second-best a-vector may exceed the value of the best
at b1 in the worst-case. Note that this relationship must hold
for some T :::; 2£. If the relationship holds forT = 0 (i.e.,
there is 2£-separation between the maximizing vector and
all other vectors at belief state b1) then we are executing the
optimal policy with probability at least 1 - o and our error
is bounded by j31+1oh.
3.4

•

the probability with which no mistake is made before stage
is at least ( 1 - oj! -t. Assuming a worst-case bound of
h on the performance of an incorrect choice (w.r.t. the opti­
mal policy) at any stage (which is thus independent of any
further mistakes being made), we have expected error E on
the sampling strategy where N (o, £) samples are generated
at each stage; E is bounded as follows:

t

Theorem

4

Multi-stage Analysis

The analysis above assumes that once an a-vector is cho­
sen, the plan corresponding to that vector will be imple­
mented over the problem's horizon. In fact, once the first
action A (a) is taken, the next action will be dictated by re­
peating the procedure on the subsequent approximate belief
state. Due to further sampling error, the next action cho­
sen may not be the "correct" continuation of the plan rr( a).
Thus we have no assurances that the 2£-optimal policy will
be implemented with high probability. In what follows, we
assume that our sample size and approximate belief state ll
are such that T = 0 at every point in time (i.e., our approx­
imate beliefs always give at least 2£-separation for the op­
timal vector). We discuss this assumption further below.
We make some preliminary observations and definitions be­
fore analyzing the accumulated error.
•

457

We first note that b1+ 1 is an unbiased estimate of the
distribution T(bt, at, o�+1). Though particle filtering
does not ensure that b1+1 is unbiased with respect to
the true belief state b1+ 1, our evidence integration pro­
cedure and reweighting scheme produce "locally" un­
biased estimates. To see this, notice that the distribu­
tion// obtained by reweighting b1 w.r.t. o1+1 corre­
sponds to exact inference assuming the distribution b1
is correct for St. (This exact computation is tractable
precisely because of the sparse nature of this approxi­
mate "prior" on 51.) Thus, the procedure for generat­
ing samples of st+l using b1 is a simple forward prop­
agation without reweighting, and thus provides an un­
biased sample of T (bt, at, o1+1 ) .
Let us say that a mistake is made at stage t if ma(b1+1)
is not optimal w.r.t. T(b1, a1, o1+1 ) . In other words,
due to sampling error, the approximate belief state
i/+1 differed from the "true" belief state one would
have generated using exact inference w.r.t. b1 in such
a way as to preclude an optimal policy choice.

We can now analyze the error in decision quality associated
with acting under the assumption that T = 0. Let stage t
be the first stage at which a mistake is made. If this is the
case, we have that ma ( b"+ 1 ) = ma (T ( b" , a", o"+1)) for
all k < t. By Fact I , this means that ma(b") = ma(b")
for all k < t (where b" is the true stage k belief state one
would obtain by exact monitoring). Thus, if stage t is the
first stage at which a mistake is made, we have acted ex­
actly as we would have using exact monitoring for the first t
stages of the process. Since our sampling process produces
an unbiased estimate b"+1 ofT(b", a", a"+1) at each stage,

The above reasoning assumes that T reaches zero at each
stage of the process, a fact which cannot be assumed a pri­
ori, since it depends crucially on the particular (approxi­
mate) belief states that emerge during the monitoring of the
process. Unfortunately, strong a priori bounds, as a simple
function of£ and J, are not possible if T > 0 at more than
one stage. The main reason for this is that the conditional
plans that one executes generally do not correspond to a­
vectors that make up the optimal value function. Specifi­
cally, when one chooses aT-optimal vector (for some 0 <
T $ 2e) at a specific stage, a (worst-case) error ofT is intro­
duced should this be the only stage at which a suboptimal
vector is chosen. If a T-optimal vector is chosen at some
later stage (T > 0), the corresponding policy is r-optimal
with respect to a vector that is itself only approximately op­
timal. Unfortunately, after this second "switch" to a subop­
timal vector, the error with respect to the original optimal
vector cannot be (usefully) bounded using the information
at hand.5
However, even without these a priori guarantees on deci­
sion quality, we expect that in practice, the following ap­
proximate error bound will work quite well, specifically as
a guide to determining appropriate sample complexity, as
discussed below:

E <

2£/3
-1 --{3

+

2e:hj3J
,.:-:-1---.:{3 +_{3=-J-=

(4)

Intuitively, at each stage of the process a 2e:-optimal vec­
tor will be chosen with high probability. Though we cannot
ensure this, in practice we expect that the cumulative error
over those stages where mistakes are not made can be use­
fully estimated by the first term. The second term accounts
for the possibility of mistakes, as in Theorem 4. Here a mis­
take refers to the probability 1-J event of choosing a vector
at a specific stage that is not 2£-optimal.
We also note that a post hoc analysis like that described for
one-stage analysis can be used to bound error:
Proposition 5 Let t be the first stage of the process at
which T > 0, and t + k be the second such stage. Then

hj3J
+ t+t2e:
t k+l h
p
+ {3 +
- 1- f3 + f38

E<

The first term in this bound denotes the error associated
with mistakes. The second term reflects the 2e: bound on er5In particular, it is not the case that the error is bounded by 2r

[ 17].

POUPART ET AL.

458

ror associated with the first switch to an approximately op­
timal vector at stage t, while the third reflects the second
switch. The main weakness in the bound again lies in this
last term and its reliance on h to bound error after a second
switch. One way in which these bounds can be strengthened
is through the use of switch set analysis, a technique de­
scribed in [17]. The set of constraints imposed by the sam­
pling scheme on the true belief state are linear and a priori
error bounds can be computed by

dynamic programming.

Details are beyond the scope of this paper.

3.5

Dynamic Sample Generation

The analysis above allows us to determine a priori the sam­
ple complexity required to achieve a certain error with a
specified probability. Our objective is ultimately to be rea­
sonably sure we choose the correct (maximizing) cr-vector
at each stage of the process. The method above ensures this
by requiring that
is estimated reasonably precisely for
each cr. The post hoc analysis of value separation suggests
that great precision is not needed if the vectors are widely
separated at the true belief state, specifically, if the best vec­
tor has value much greater than the second best. Draw­
ing on ideas from the literature on group sequential meth­
ods [ 12] and multiple-comparisons with the best (MCB)
[10] that analyze decision making from this perspective,
we describe a method that at each stage generates samples
dynamically, using a sampling plan whose termination de­
pends on results at earlier stages of the plan. The method is
inherently simple: we will take samples in batches until we
can select an cr-vector satisfying certain requirements. Our
method recalls the application of MCB results and group se­
quential methods by Ortiz and Kaelbling to influence dia­
grams (see [15] for details and further references).

V�

UAI2001

to Eq. 1 using 8 = 81/I�XI as the individual confidence pa­
rameter and nt = m1 as the number of samples. Defining
r1 as above, and combining a lower bound for o:i with an
upper bound for all the others, we have

Pr(V�.
�,

>
- max V! a;ta� �

-r1 ) > 1
-

- 81

If r = r1 is nonpositive, cri is the optimal vector with prob­
ability at least 1 - o1. In general, if we stop immediately
after processing the first batch and select cri, the error in­
curred will be at most max(O, r ) :::; 2C:t = 2 max01 c01[1].

If we are unsatisfied with the precision r achieved, we gen­
erate a second batch of m2 samples, and propose that

Pr(V�.

2

- amax
V�
a
;t ;

-r2 ) >

>
-

1

- 82

This bound holds if we insist beforehand that we will gen­
erate the second batch; but it ignores that fact that we gen­
erate this batch only after realizing our stopping condition
was not satisfied using the first batch. This dependence on
the bound resulting from the first batch-since these bounds
are random variables, this means we do not know a priori
whether we will generate a second batch-requires that we
correct for multiple looks at the data. We do this by insist­
ing that both bounds hold jointly, conjoining the bounds ob­
tained after two batches using the Bonferroni inequality and
letting T = mi n { j IJ ::,;2 , a; a ; } Tj:

=

V� � -r)
Pr(V�;- a�a�
raJ

� 1- (81 + a2)

Hence, if we stop after processing at most 2 batches, then
our error in selecting cr; will be at most max ( O , r ) with
probability at least 1 - (81 + o2). Applyi ng this argument
up to k batches, we obtain

Suppose we are trying to select the maximizing cr-vector

at stage t, using belief state 'bt. The basic structure of our
dynamic approach requires that we generate samples from

T(6t, at, at+1) in batches, each of some predetermined size.

To generate the jth batch:

(a) we determine a suitable confidence parameter 8j
(b) we generate the jth batch of mj samples from

T(bt, at' ot+l)

(c) we compute estimates v� [j] for all vectors cr
based on the samples in all j batches, correspond­
ing precisions E 01 (j], and let
be the vector with

greatest value

v� [j]

crj

(d) we compute threshold -7) =

maXa;ta� (V� [j]
J

V�. (j) - Ea• [j] J

J

+ E 01 (j]) and terminate if Tj

reaches a certain stopping criterion
We now elaborate on this procedure.
We use MCB results to obtain confidence lower bounds
(or one-sided confidence intervals) on the difference in true
value between that of the vector with largest value estimate
with respect to all the samples in the batches so far and the
best of the other vectors. Suppose m1 samples are gener­
ated in the first batch. Given simultaneous confidence pa­
rametero1, we obtain the one-sided boundsc:a(j] according

(5)

Pr(V�:- ��� v� � - r)
k

where

r =

k

� 1-

L:oi

j=l

min{jlj�l<,aj=a:} Tj.

The method

as described above will stop at

the first batch

:::; 0: at this point we are assured of select­
ing the optimal vector with high probability. If we insist
that we force -r to zero, the number of batches k cannot be

l such that Tj

bounded; thus, we must set the sequence of confidence pa­
rameters oj such that
1 OJ :::; o. For example, we might

:Lj:

set 8J = of(j j + 1)) and the individual confidence pa­
rameters as Oj /llX[. If there is separation between the value
of the optimal vector and the second best, the process will
stop after a finite number of batches. Hence, we can con­
tinue the process until -r ::o: 0. However, since the error
in the individual estimates decreases only proportionally to

(

ln j /j, termination might take longer than we wish, de­
pending on the amount of separation and the vector-value
variance. This problem is exacerbated by our use of loose
ranges in the computation of the precisions e: a [j].

J

If we impose a limit B on the number of batches, and want
to make sure that our assessment of r holds with proba­
bility at least 1
o, we need to set the sequence of con-

-

fidence parameters

OJ

such that

:Lf=1 OJ

:::;

o.

The easi­

est way to accomplish our global confidence requirements

UAI 2001

POUPART ET AL.

o /B.

o

Furthermore, if we want to be sure
is to set i =
that the method selects a vector with true value that is no
less than 2t: from the optimal with the same confidence,

Problem

j

to the

r(maxa R�/(2Be2)) In (BI�I/o)l.

State Space Size

o ee
Widget
Pavement

then one alternative is to set the number of samples mj in
each batch

459

If

we do not impose specific requirements then the setting of

32
32
128

Size ofN
maximum average
102
56
205
121
39
16

mj is arbitrary, but needs to be fixed in advance. This is

because for our analysis to hold, mj cannot depend on the
outcomes from the samples themselves. Although arbitrary,

in general, the setting of mi should take into consideration

a trade-off between reducing the expected total number of
samples before the method stop versus reducing the varia­
tion on the total number of samples.
In general, we expect this method to be effective when there

is sufficiently large gap between the best vector and the rest,
and/or the ranges in vector values are sufficiently small rel­
ative to the value separation and the error tolerance. By us­

ing loose upper bounds on the variances and accuracy pa­
rameters, the theoretical bounds can become very loose, and

hence do not reflect the potential gains we expect. The ver­
sion presented in this paper is very simple. Many variations

on the same idea are possible to try to bring the theoret­
ical bounds more in accordance with our belief about the

expected behavior of the method (for instance, using infor­
mation about range of differences in value between vector
pairs, allocating some samples to estimate variance, etc.),

but this is beyond the scope of this paper.

As before, unless we push the error tolerance

r

to zero at

each stage of the monitoring process, we cannot obtain tight
bounds on error after that point. However, we can assert:

Suppose beliefs are monitored according to
the dynamic procedure described above using global con­
fidence parameter o. Furthermore, suppose that r = 0 at
each stage. Then error E satisfies
Theorem 6

E<

hf3o

- 1- f3 + fJo

However, as noted above, the computational demands of

insisting that r = 0 can be severe if the belief state at
some time t is such that little separation exists between the

best vector and the second-best (that is, if

l}

lies close to a

"edge" of the value function, where two optimal a-vectors
intersect). If r s 0 at all stages up to timet, then the bound
described in Proposition 5 holds for this dynamic scheme.

4

Empirical Evaluation

Three test problems were used to carry out experiments test­
ing the efficacy of our sampling procedures (we refer to [ 16]

for the full specification of those problems; see also [18] for

a summary). Each of the three problems was solved using
Hansen and Feng's [9] ADD implementation of incremental
pruning (IP) to produce a set N of a-vectors using a compact

ADD representation.

In the following experiments, we report on the use of sam­

pling for approximate belief state monitoring on three test
problems. The goal of the experiments are twofold: to
evaluate (i) the impact on decision quality induced by sam­
pling techniques and (ii) the sample complexity necessary

Table 1: Statistics for the three test problems. T he maxi­
mum and average size of � are taken over a 15-stage pro­
cess.

to guarantee some level of decision quality. Note that the
experiments do not evaluate the running time of sampling
methods since that is not the focus of this paper and the ef­

ficiency gains of such methods have already been clearly

demonstrated [II, 21]. In theory, exact monitoring has time
complexity on the order ofO{ISI2) whereas sampling has a

time complexity in the order ofO(m log lSI) (m is the num­
ber of samples). T hus, a sampling strategy provides time
savings when m <

ISI2/log lSI.

The reader should also

be warned that the scope of the empirical evaluation was
limited to test problems for which a set of a-vectors cor­
responding to an optimal value function can be computed.
Hence, as shown in Table 1, lSI and INI are fairly small, and
consequently the following experiments should be consid­
ered preliminary.

The first experiment compares the expected loss incurred
by sampling methods to that of a random monitoring ap­

proach. More precisely, 5000 initial belief states are picked
uniformly at random and for each initial belief state, the op­

timal expected total reward is compared to the cumulative

rewards earned by an agent that approximately monitors its
belief state over 15 stages. The difference between the opti­

mal expected total return and the actual return is the loss due
to approximate monitoring. Table 2 shows the average loss
due to a single approximation at the first stage (assuming
exact monitoring for the remaining 14 stages), whereas Ta­

ble 3 shows the average cumulative loss due to approximate

monitoring at each of the 15 stages. When doing random

monitoring, the agent picks a belief state at random (uni­
formly) and executes the optimal action for this random be­

lief state. This random method can be viewed as a naive
strategy that any other approximation method should be

able to beat. The sampling methods implemented are basic
particle filtering (with partial evidence integration) where a

fixed number of particles (20, 40, 80 or 160) are sampled
for each approximate belief state. The column "worst" re­
ports the worst possible expected loss that can be achieved
by consistently choosing the worst actions.6 The worst ex­
pected loss is included to give some idea of the scale of po­
tential losses due to approximate monitoring.

As expected, the experiments show a gradual decrease in
average expected loss as the number of samples increases.

When compared to the random strategy (and considering
the range of values obtainable across the set of possible be­
haviors), sampling methods perform quite well. In Table 2,

6
This worst strategy can be computed by minimizing (in­
stead of maximizing) the expected total reward while solving the
POMDP.

460

POUPART ET AL.

P rob

Rand

Average Single Error
Sampling
20
160
80
40

1 .968

Table 2: Comparison of the average error due to a single
approximation at the first stage of a 15-stage process (exact
monitoring being performed for the remaining 14 stages).
Prob

CofiWidg
Pav

Rand
1.653
0.109
2 . 3 19

Average Cumulative Error
Sampling
20
160
40
80
0. 100 0.043 0.018 0.017
0.098 0.069 0.045 0.022
0 . 1 24 0.072 0.045 0.024

Prob

Worst

Worst
8.014
5.778
34.24

Table 3 : Comparison ofthe average cumulative error due to
approximate monitoring at each stage of a I S-stage process.

the first row of each problem indicates the actual error in­
curred and the second row indicates the upper bound 2c pre­
dicted by the theory (for £ == 0 . 1 ). This bound is loose when
compared to the actual error due to the worst-case nature of
the analysis. The bounds may still provide some guidance
regarding the amount of sampling desired to reduce the av­
erage expected loss to some suitable level (assuming a more
or less constant ratio between the bounds and the actual er­
ror).
In a second experiment, we evaluate the benefits of dynam­
ically determining the amount of sampling. For given o
and (, we evaluate the total number of samples necessary to
guarantee that the one-stage sampling error is bounded by
2c with confidence 1 - o. Table 4 shows how this total num­
ber ()f samples varies as we increase the maximum number
of batches. Once again, 5000 random initial belief states
are chosen and the average number of samples required to
decrease r below 2c is reported. The column for 1 batch
corresponds to the standard non-dynamic sampling proce­
dure. Table 4 reveals that for the widget and pavement prob­
lems, a dynamic sampling procedure can reduce the sam­
pling complexity quite dramatically for a well-chosen max­
imum number of batches. Unfortunately, the dynamic ap­
proach does not appear to have offered any savings in the
coffee problem. Further investigation is necessary to assess
the optimal (maximum) number of batches in general.
In a related paper [ 1 8], Poupart and Boutilier also tackle the
belief state monitoring problem, but using a vector space
method that exploits conditional independence. The idea
is to repeatedly approximate belief states using projections
as initially proposed by Boyen and Koller [3). Projec­
tion schemes and sampling approaches differ in many as­
pects including the properties of POMDPs for which they

UA1 2001

1
2
5
139 107
106 64

93
62

9
10
254 265
78 80
58
59

Table 4: Comparison of the average number of samples re­

quired for adaptive sampling at the first stage of a I S-stage
process (J = 0 . 1 and E = 2 for coffee and pavement,
J = 0 . 1 and c = 0.5 for widget).

are most suitable. Sampling methods exploit the sparsity
of belief distribution whereas projection schemes exploit
conditional independence. Given that the coffee, widget
and pavement problems are factored POMDPs, the vector
space methods tend to perform better than sampling with
respect to decision quality. For instance, average losses
due to single-stage approximation using the max VS-search
method are respectively 0.0013, 0.0082, 0.0014 for the cof­
fee, widget and pavement problems; similarly, the average
cumulative losses over 1 5 stages are respectively 0.01 54,
0.05 19 and 0.007 1 . However, the computational overhead
associated with sampling is minimal while the overhead as­
sociated with choosing good projection schemes is nontriv­
ial. We expect the two approaches can be combined in fruit­
ful ways (as we discuss below).
5

Concluding Remarks

Our value-directed sampling technique can be seen as ap­
plying methods from the MCB and group sequential sam­
pling fields to the problem of particle filtering for POMDPs.
We are able to derive (worst-case) error bounds on such an
approach, and use these bounds to suggest methods to direct
sampling in such a way as to choose optimal actions rather
than (necessarily) accurately estimate their values. Our ini­
tial empirical results are encouraging, though clearly much
more substantial testing is needed, a task in which we are
currently engaged.
This research can be extended in a number of ways in a
number of very interesting ways. One important challenge
is to provide a stronger analysis of error when the precision
parameter r > 0. One strategy to circumvent this diffi­
culty builds on the idea of constructing the set of alternative
conditional plans that may be executed when r > 0 [ 1 7] .
Another challenge is to provide an analysis i n the absence
of partial EI (which locally removes bias): one idea is to
use information from the DBN parameters to compute a pri­
ori error bounds; another is to use absolute approximation
bounds similar to those used in this paper or optimal rela­
tive approximation methods to obtain a posteriori bounds
on the error tolerance r.
We are very interested in adapting these techniques to other
value function representations (e.g., grid-based value func­
tions) and providing an error analysis of this method when
the value function is itself an approximation of the true
value function. Finally, previous work using value-directed
projection schemes [3, 1 7] has been used successful ly to ex-

POUPART ET AL.

UAI 2001

ploit the conditional independence present

in certain fac­

tored POMDPs to speed up belief monitoring. The sam­
pling approach described in this work does not exploit this
type of structure; however, one could sample the variables
defining the factored state space in a "stratified" fashion, or
apply Rao-Blackwellisation methods [6, 7].
Acknowledgements :

Boutilier and Poupart were sup­

ported by the Natural Sciences and Engineering Research
Council and the Institute for Robotics and Intelligent Sys­
tems.

Ortiz was supported by NSF IGERT award SBR

9870676.



We propose a new approach to value-directed be­
lief state approximation for POMDPs. The value­
directed model allows one to choose approxima­
tion methods for belief state monitoring that have
a small impact on decision quality. Using a vec­
tor space analysis of the problem, we devise two
new search procedures for selecting an approxi­
mation scheme that have much better computa­
tional properties than existing methods. Though
these provide looser error bounds, we show em­
pirically that they have a similar impact on deci­
sion quality in practice, and run up to two orders
of magnitude more quickly.
1

Introduction

Partially observable Markov decision processes (POMDPs)
have attracted considerable attention as a model for
decision-theoretic planning. Their generality allows one
to seamlessly model sensor and action uncertainty, uncer­
tainty in the state of knowledge, and multiple objectives
[1, 5]. Their computational intractability has, however,
limited their practical applicability [ 11, 13].
An important

approach to POMDPs involves constructing a
value function for a beliefstate MDP offline, and maintain­
ing a belief state (or distribution over system states) online,
which is used to implement an optimal policy [18]. Anum­
ber of approaches attacking the offline computational prob­
lems have been studied, including improved algorithms [6],
the use of factored representations [2, 8], as well as numer­
ous approximation schemes [9]. Little work has focused on
the online belief state monitoring problem. Because plan­
ning state spaces grow exponentially with the number of
variables, maintaining an explicit distribution over states
is generally impractical. Even when concise representa­
tions such as dynamic Bayes nets (DBNs) are used, moni­
toring is generally intractable, since the independencies ex­
ploited by DBNs vanish over time. Boyen and Koller [3]
proposed projection schemes for approximate monitoring,

cebly@cs.toronto.edu

essentially breaking weaker correlations among variables
to ensure tractability. Poupart and Boutilier [15] proposed
value-directed methods for approximation, allowing the an­
ticipated loss in expected utility guide the choice of approx­
imation scheme.
In this paper we pursue the value-directed approach since
its emphasis on minimizing impact on decision quality is
a critical factor in devising useful approximations. We use
the value function itself to determine which correlations can
be "safely" ignored when monitoring one's belief state. We
propose an alternative approach to choosing approximation
schemes for monitoring in POMDPs that overcomes many
of the computational bottlenecks of [15]. We introduce
a vector space formulation of the approximation problem
that allows one to construct approximation schemes with
looser error bounds, but much more quickly. Despite the
looser bounds, we show empirically that decision quality is
rarely worse than that obtained using the more intensive ap­
proaches. Our methods work in time roughly on order of
the time taken to solve a POMDP, and since they run of­
fline, they can be used with any POMDP technique that can
currently be applied. Furthermore, these methods take ad­
vantage of the factored (DBN) representations to avoid state
enumeration. The offline cost allows much faster (approxi­
mate) online policy implementation. Even in cases where
a POMDP must be solved in a traditional "flat" fashion,
we typically have the luxury of compiling a value function
offline. Thus, even for large POMDPs, we might reason­
ably expect to have value function information (either exact
or approximate) available to direct the monitoring process.
The fact that one is able to produce a value function offline
does not imply the ability to monitor the process exactly in
a timely online fashion.1 Finally, our model offers a novel
view of the approximation problem for belief state monitor­
ing for POMDPs.
We briefly overview POMDPs and value-directed approx­
imation in Section 2. We present our vector space formu­
lation in Section 3 and provide some suggestive empirical
1

While techniques exist for generating finite-state controllers
for POMDPs, there are still reasons for wanting to use value­
function-based approaches [14].

446

POUPART & BOUTILIER

UAI2001

results in Section 4.

2

POMDPs and Belief State Monitoring

The key components of a POMDP are: a finite state space

S; a finite action space A; a finite observation space Z; and
a reward function

R

:

S -+ R. Actions induce stochastic

state transitions with specified probabilities, and an agent is
provided with noisy observations of the system state (with
specified probabilities). A reward is received at each state

and an agent's objective is to control the system through ju­

dicious choice of action to maximize the expected reward
obtained over some horizon of interest.

1:

Figure

The rewards obtained over time by an agent adopting a spe­

The Switch Set

Sw(o:3)

of a3

cific course of action can be viewed as random variables

R(t l.

Our aim is to construct a policy that maximizes the ex­

pected sum of discounted rewards

E{l::�o 'l R(t)) (where

1 is a discount factor less than one). An optimal course

of action can be determined by considering the fully ob­

servable beliefstate MDP, where beliefstates (distributions

over S) form states, and a policy

1r

:

B

-+

A maps

belief states into action choices. A key result of Sondik
[18] showed that the value function V for a finite-horizon
problem is piecewise-linear and convex and can be rep­
resented as a finite collection of a-vectors; for infinite­

lief states can be maintained by standardBayesian methods;
but when lSI is large, the cost is prohibitive. This is espe­
cially true when S is determined by a set of variables X (and
lSI

::::

0(2IXI)).

In such cases, DBNs can be used to rep­

resent the dynamics ofPOMDPs and DBN inference tech­
niques that exploit conditional independence among vari­
ables can be applied to make monitoring more efficient. Un­
fortunately, as shown by Boyen and Koller [3}, in many
problems most if not all variables of DBNs tend to become

horizon problems, a finite collection generally offers a good

correlated over time so DBNs offer no significant savings.

approximation. Specifically, one can generate a collection

Boyen and Koller introduced projection schemes as a

N

of a-vectors, each of dimension lSI, such that

maxaEI:<

b ·a.

V(b)

:::;::

In Figure 1 the value function is given by

method to approximate belief states.

Given variables X

defining S, a projection is a setS of subsets of X with each

the upper surface of the five vectors shown. Each vector

variable in at least one subset. Correlations among vari­

is associated with a specific (course of) action. For finite

ables within a subset are preserved while the subsets are as­

horizon POMOPs, a set N

sumed to be independent. For instance, if X :::;::

k is generated for each stage k of

{A, B, C},
{ AB, C} approximates the exact be­
Pr(A,B,C) with b'
Pr(AB)Pr(C).

the process. Algorithms exist that construct efficient repre­

then projection S

sentations of a-vectors, such as decision trees or algebraic

lief state

decision diagrams (ADDs), when the POMDP is specified

The assumed independence allows more efficient monitor­

b :::;::

::::

::::

concisely using DBNs [2, 8].

ing using DBNs: at most, one maintains marginals over

Insight into the nature of POMOP value functions can be

each subset inS.

gained by examining Monahan's [12] method for solving

The choice of projection scheme (or any other approx­

POMDPs. Monahan's algorithm proceeds by producing a

imation) can have a dramatic impact on decision qual­

sequence of k-stage-to-go value functions Vk, each repre­
sented by a set of a-vectors Nk. Each

a

E

Nk denotes the

ity in a POMOP, since the approximate belief

b'

can lead

to the choice of a suboptimal course of action. Poupart

value (as a function of the belief state) of executing a k-step

andBoutilier [15] propose a value-directed approximation

conditional plan. More precisely, let the k-step observation
strategies be the set Oft of mappings u : Z -+ Nk-1.
Then each a-vector in Nk corresponds to the value of ex­

framework allowing computation of bounds on the loss in

ecuting some action

OS';

a followed by implementing some u

that is, it is the value of doing

a,

E

and executing the

u(z) if z is
CP(a) to denote this plan, we have that
CP(a) = (a;ifz;,CP(u(z;))'rlz;). We informally write
this as (a; u). We write a ( (a; u)) to denote the a-vector re­

expected utility for projection schemes, and search methods
for choosing projections that tradeoff decision quality with
monitoring efficiency. The techniques are computationally
intensive (potentially requiring time quadratic in the solu­

k - 1-step plan associated with the a-vector

tion time of thePOMOP); but this offline computation pro­

observed. Using

duces a projection scheme that improves online monitoring
efficiency with minimal sacrifice in decision quality. We
briefly outline this model.

flecting the value of this plan.

Assume a POMOP has been solved giving the set � of a­

The implementation of a policy requires that one monitor

vectors with a

belief state b over time so that it may be "plugged" into the

value function (or N) to make a suitable action choice. Be-

E N.

Let

R (a) be the optimal region for
b such that o: is maximal for

a (i.e., the set of belief states

b).

Given a projection schemeS, the switch set Sw(a) is

UAI2001

POUPART & BOUTILIER

447
,!-6,A,B,C)

the set of c/ such thatS(b) E R(a') for some b E R(a).
Thus,S could induce one to believe a' has maximum value
at the current belief state instead of a, thereby erroneously
"switching to" the plan corresponding to a' from a by using
S. Figure 1 illustrates a switch set Sw(a3)
{ a1, a2, a4 } .
Switch sets can be computed by solving a nonlinear pro­
gram for each a EN. Linear programs (LPs) can be used to
more effectively produce a superset of the switch set [15].
=

Given the switch sets (or supersets thereof), one can com­
pute an upper bound B� on the loss in expected value for a
single approximation using S at k stages to go:

B�

=

rnaxaeN�< maX(,

maxa'ES�(a) b ·(a- a')

W hen multistage approximations are applied, one can de­
vise an alternative set which is similar in spirit to the switch
set. The alternative set Alt ( a) is the set of all a-vectors cor­
responding to alternative plans that may be executed as a re­
sult of repeatedly approximating the belief state at all future
time steps (see [15] for a precise definition). Alt ( a ) is con­
structed with a dynamic programming procedure similar to
incremental pruning [6]. One can define an upper bound E�
on the loss in expected value due to successive belief state
approximations usingS for k stages to go:

E�

=

rnaxaeN�< max,

maxa'EAlt�(a) b ·(a- a')

These bounds can be extended to infinite-horizon problems.
Given the bounds B and E, one can search for an "opti­
mal" projection scheme by looking for the projection that
minimizes one of those bounds. The space of projection
schemes is very large (factorial in the number of variables),
but exhibits a nice lattice structure. Figure 2 illustrates the
lattice of projection schemes when the state space is defined
by the joint instantiation of variables A, B and C. Each
point denotes a projection scheme, with "descendents" of
any projection corresponding to more coarse-grained pro­
jections. As we move down the lattice, accuracy increases
since the number of correlations among the variables pre­
served in our belief state is increased (hence, error bounds
B and E monotonically decrease); but monitoring effi­
ciency decreases as we move downward for the same rea­
son. A number of search procedures can be used to traverse
the lattice, using the error bounds to guide the search. For
example, a simple (and incremental) greedy scheme is pro­
posed in [15]. The search is stopped when a suitable accu­
racy/efficiency tradeoff has been reached.
3

Vector Space Analysis

We now provide a vector space analysis of belief state ap­
proximation by projection, showing in Section 3 .I that pro­
jections allow movement of belief state only in certain di­
rections (defining a subspace). This allows us to view a­
vectors as determining gradients of value in different direc­
tions: approximations whose directions give similar value
gradients are less likely to cause switching (hence minimiz­
ing error). In Section 3.2 we use this to design faster switch

Figure 2: Lattice of Projection Schemes
test algorithms than those described above, though yield­
ing looser bounds. In Section 3.3 we devise a new vector­
space search algorithm to find projections without directly
trying to minimize these error bounds, instead relying on
value gradient similarity.
3.1

Vector space formulation

Given a projectionS over X, let b and b' = S(b) be points
in belief space. Define d = b' - b to be the displacement
vector from b to b'. Projection S determines a set of lin­
ear equations constraining b in terms of b'. For example,
if X = {X, Y} and S = {X, Y} (i.e., S treats X, Y as
independent), we have:

d(xy) + d(xy) + d(xy) + d(xy)
d(xy) + d(xy)
d(xy) + d(xy)

0
0
0

Geometrically, we interpret each equation as a hyperplane;
and their intersection (or solution space) is a line through
the origin representing a one-dimensional (in this example)
subspace. This subspace captures the set of all displace­
ment vectors resulting from the application of S (w.r.t. b').
Since all possible displacement vectors lie on the same line,
they must all have the same direction (vectors with opposite
orientation are assumed to have the same direction).
To illustrate, let b(x) = 0.3 and b(y) = 0.4. The approxi­
mate belief state using S above gives:

b'(xy)
b'(xy)
b'(xy)
b'(xfi)

b(x)b(y)
b(x)b(Y)
b(x)b(y)
b(x)b(ii)

0.12
0.18
0.28
0.42

Figure 3 shows a three-dimensional belief space for belief
states xy, xy, xy and xy.2 All belief states b with b(x) =
2

We omit dimension

b(xy) as probabilities sum to 1.

POUPART & BOUTILIER

448

UAI2001

b(xy)
max

x

s.t.

b(xy)+b(xy)-0.4

b·(a;-a;)>x
b'· (ai -a;)� x
b'(m') = b(m')

L b(s)

b(sj �

0

=

1

E S

\Is
\Is

b'(s) � 0

1 b(xyJ

Vm' � m, Vm

Table 1: Linear VS-switch test for projection schemes. This
LP has a strictly positive objective value iff there is some

b E R(a;)
Figure 3: Solution space of possible exact belief states

0.3lie in a hyperplane, and similarly for
intersection is the set { b

b'

:

=

b(y)

=

b

0.4. T heir

and all displace­

S (b)},

ment vectors forb' have the same direction. (For marginals
other than 0.3 and 0.4, the hyperplanes and their intersec­
tion shift, but remain parallel).
Let

Ds be the displacement subspace spanned by the set

of all displacement vectors induced by

S:

it is completely

characterized by its marginals (elements) and it describes

Ds

the directions of all displacements. In general,
is a
( 21XI - c)-dimensional subspace, where c is the number

of constraints, since it is the solution space of

c

linearly

independent equations, each corresponding to a constraint

d(m)

=

0. (c is the number of subsets of variables con­

tained in some subset

m E S,

as above.) This is obvious

when we re\\rrite the constraints as

Vm

· d = 0,

where Vm

is a boolean lSI-vector with l at states with all XE
and 0 at states with some

m true

false.3 In our example,

XE m

we have:
V0

vx
Vy

xy
(
(
(

1

xfj xy

1

1

1

1

1

0
1

0

xy

D� be t�e subspace spanned by the vectors mE S;
D5 IS. the null space of Ds (i.e., the set of vectors
perpendicular to each vector in Ds).
vm,

the space

3.2

We will see below that the subspaces

Ds and D�

allow

a nice characterization of a new switch test. We first con­
sider a simple relaxation of the switch test of

[15].

Recall

from Section 2 that approximation S could induce an agent

to switch from optimal vector ai to suboptimal vector aj if

S(b) E R(aj) for some bE R(o:i).

The idea behind the re­

laxed vector space (VS) switch test is to simply apply the
same technique ignoring the presence of other a-vectors.

The VS switch test asks whether there is some belief state b
3The

b·ajyetS(b)·ai

generalization

straightforward.

to

b(m')

=

b'(m')

for

O:j is in the VS-switch set of a;. This is equivalent to ask­
frj E Sw(a;) when all vectors except these two are

removed from l{. Note that the VS-switch set is a superset
of the true switch set.
Since the constraints relating bandS( b) are nonlinear, VS­
switch sets can be computed using nonlinear programs. We
can define a simpler linear VS-switch test as in Table 1

which produces a superset of the VS-switch set. This LP
is a relaxation of the LP switch test

Now define frij

[15}.

= a, - aj to be a vector representing the
difference in expected value for executing aj instead of a;.
We can show that the VS-switch test for a; and O:j is pos­
itive iff a;j
Consider a;j as a gradient that mea­

¢ D"§.

sures the error induced by an approximation when it causes

a switch from a; to aj. After an approximation, if this dif­

ference changes considerably, the agent is likely to choose
the wrong maximizing o:-vector. Define the relative error,
O;j, of this change in the relative assessment of a; with re­

O:j as:

Here

a;j

b(a;- aj)- S(b)(a,- O:j)
d. a;j

can be viewed as a gradient since approxima­

tions corresponding to displacement vectors

a;i

maximize the magnitude of

gle between

d · O:ij.

d parallel

to

In general, the an­

dand a;j is a good indicator of approximation

error. In particular, if they are perpendicular, their dot prod­

Vector space switch test

for whichb·a; >

such that

ing if

O;j

Let

R(aj )

s.

spect to

1
0
0

andb' E

any subset m' of variables contained in some marginal mE

<

S(b)·aj.

nonboolean

If so,we say

variables

is

uct is zero and the relative assessment of a; and aj remains
unchanged, preventing any switch. By definition, the sub­
space

Df is the set of vectors perpendicular to all displace­

ment vectors possibly induced by S, so when aij is a mem­

ber of

D"§, all possible displacement vectors are perpendic­

ular to

a;j and consequently there cannot be a switch from

a; to a j. Thus a;j

¢ D� iff the VS-switch test is positive.

This fact provides for a much more efficient method to com­
?ute switch sets than the LP of Table 1 . We decompose O:ij
m two orthogonal vectors corresponding to the projections
of a;j onto

D� and Ds:

a;j ::::: proj( a;j, D�)

+ proj( O.ij,

Ds)

UAI2001

POUPART & BOUTILIER

proJ(a, D) stands for the projection of a onto
a;J E D�, then pro} (a;i, D�)
a;J and,
consequently, proj(a;j, D s) is the zero-vector; otherwise,
proj(a;j,Ds) is nonzero. We can thus determine ifa;j E
D-§ by measuring the length of proj(O:ij, Ds). We have
0 when a;i E D�, and
that IJproj(a;j, Ds)ll2
a
>
0
when
O:ij
rf. D{ In particular, the
p
2
;j,
Ds
)
ll
ll roj(
squared length of proj(o:;j, Ds) can be computed by the
(where

D).

If

=

following equation:

=

·

·

(1)

tJE'Df

D-§.

Vf-because of its factored representation. For problems
involving binary variables, every vector in Vf consists of
a sequence of 1 's and -1 's (before normalization). The un­
normalized basis vector iim associated with subset m has a
1 in every component corresponding to a state with an even
number of true variables in m and -1 in every component
corresponding to a state with an odd number of true vari­
ables in m. For instance, projection S = { XY, Y Z} has
six marginals (0, X , Y, Z, XY andY Z), yielding the fol­
lowing basis vectors:4

1

-1

iiy

-1

-1

iiz

-1

vxy

=

-

-1

-1
-1

-1
-1

that using the original LP test. As in Section 2, these bounds

can be used to search the lattice of projection schemes for
making appropriate time-decision quality tradeoffs.

3.3

Vector space search

-1

based on the relative error expression

-1
-1

-1

-1
-1
-1
-1

-1

) t v'§
) I JiSi
) I J§
) I JiSi
) I JiSi
) I JiSi

With this orthonormal basis, we can implement VS-switch
tests very effectively, without recourse to the LP in Table 1.

We must simply compute Eq. 1 which requires O(c) dot

products. If unstructured, each dot product requires 0 (ISI)
elementary operations, for a total time ofO(ciSI). The use
of factored representations such as ADDs considerably im­
proves this rwming time. Each basis vector has only two
distinct values, and yields a very compact ADD representa­
tion. Assuming that the POMDP has been solved to pro­
duce ADD representations of the a-vectors, then the

a;j

will have compact representations, and the dot products will
be computed very efficiently: often a small constant inde­

pendent of the size of the state space. Hence, for sufficiently
structured POMDPs, the effective rwming time of a VS­
switch test is O(c).
By comparison, solving the linear program of an LP-switch
test [ 15] is polynomial in the number of constraints c and

the size of the state space. Furthermore, ADDs do not pro­

vide as useful a speed up for LPs since the effective state
4This definition can be generalized to non-binary variables.

O;j. We do not com­

pute switch sets at all, nor attempt to minimize worst-case

vector-space (VS) search

process instead seeks a projection S which defines a dis­
placement subspace

Ds that is as perpendicular as possible

to all gradients O:ij. This is motivated by the observation

that the more perpendicular the direction of an approxima­

tion with respect to

a;j, the smaller the magnitude of J;j

and, consequently, the less likely a switch will occur. Tech­

nically, this is done by minimizing the squared length of the

a;j on Ds (as in Eq. 1).
The length of proj(a;i, Ds) has a special interpretation: it
projection of each gradient

corresponds to the greatest (absolute)

relative error rate for

an approximation in some direction d E

Ds.

The relative

error rate corresponding to displacement vector dis the rel­

ative error induced by a unit displacement in the direction

ofd:

-1

=

vyz =

puted using the VS-switch test will generally be looser than

error bounds as above. This new

Here v-§ is some orthonormal basis spanning
The
spanning set of vectors Vm above can be used to generate
several orthonormal bases using the Gram-Schmidt orthog­
onalization process and normalizing. We consider a spe­
cific orthonormal basis in particular-which we refer to as

iie

space is the intersection of the abstract state space of all the

constraints. The price paid is that the B and E bounds com­

In this section we describe an alternative search method

llproj(a;j, Ds)ll� a;j O:ij- L (a;j vf

vx

449

d
lldll2 . Ojj

Hence, by choosing a projectionS that minimizes Eq. 1, we
are minimizing the (squared) worst relative error rate that
may result from projection S. When ignoring the distance
between the exact and approximate belief states, the rela­
tive error rate permits us to quantify how bad an approxi­
mation in some direction is likely to be. Each projection S

constrains approximations to directions within the subspace

Ds.

The directiond E

Ds with the highest (absolute) rel­

ative error rate has this worst relative error rate, which also
happens to be

llproj(O:ij, Ds) ll2· Thus, it is desirable to try

to minimize Expression

1.

Ideally we should choose an S that simultaneously mini­

mizes Eq. 1 for every gradient

a;J (J i= i). In the absence of

any prior information about the relative importance of each
gradient, we suggest two simple schemes: (a) minimize the
sum of squared lengths of each projection; or (b) minimize
the squared length of the greatest projection:

L llproj(a;j, Ds)ll�
j¢i
= L (a;j. D'jj- 2:::
#i

�J?'
:Jr-•

tJEDt

llproj(a;j, Ds)ll�

v .

a;j)

(2)

POUPART & BOUTILIER

450

We refer to these schemes as the sum and the max error es­
timators, respectively, for projection schemes. Of course,
many other schemes could be proposed.

Given a vector o:; E �. VS search uses eitherEq. 2 or Eq. 3
above to find a good projection

S as follows. Starting at

the root, we traverse the lattice of projection schemes (Fig­

ure 2) downward in a greedy manner. At each node, we pick
the most promising child by minimizing Eq. 2 or Eq. 3 The

computational complexity of a VS search is fairly low as it
avoids LPs. Its running time is

O(nc3J�I2ISI),

O(nc2) nodes in the lattice are tra­
versed, each requiring the evaluation ofEq. 2 orEq. 3 which
both take O(ci!XIISI) elementary operations.
For each region,

The VS search can also be streamlined. The constraints of a
node S are essentially the same as the constraints of its par­

ent node S' with one extra constraint corresponding to the
marginal

that labels the edge connecting the two nodes.

m

Problem

Since there is one basis vector per constraint, the following
equation holds:

Coffee

W idget

Pavement

2 and Eq. 3 can be computed in­

crementally as the lattice is traversed downward:

=

=

jt.i

·

and Weld

47
397
250

14

85

[7].

The third POMDP is inspired from the pave­

[ 17].

Since the analysis of the experiments doesn't require any
specific domain knowledge, the reader is referred to

[14] in

which the full specification of those problems is given.
Each of the three problems was solved using Hansen and
Feng's

[8]

ADD implementation of incremental pruning

(IP) to produce a set �

of a-vectors using a compact ADD

Each problem is run to 15 stages (dis­
counted). Table 2 shows, for each problem, its full state

representation.

lSI,

and its

effective size,

the largest intersec­

tion of abstract (ADD) states encountered during solution
(specifically, the LP-dominance test in IP). The effective
size is more relevant to solution time than

jSj.

We also

!X over the fifteen stages and the maximum

cally, six algorithms are tested: the B-bound and E-bound

O(nc2J�I2JSI)

since only

This running time is significantly

greedy search with LP-switch tests used in

[15].

As for

the B-bound or E-bound greedy search with VS-switch

O(nc3J�IJSI)

is comparable. The

VS search has an extra I� I factor, but one less

practice, I � I is usually larger than

c,

c

factor. In

so the VS search is ac­

tually slower. Again, the upper bounds on running times
are given in terms of lSI, but in practice, factored represen­
tations can drastically reduce the size of the effective state
space for structured POMDPs.

which computes switch sets using an LP

bounds; the VS analogs of these procedures which com­
putes weaker VS-switch sets using the algebraic formula­
tion of Section

3.2; and the VS search methods (sum and

max) of Section 3.3, which ignore these bounds, but instead
try to minimize Eq.

2

or Eq. 3. All search algorithms per­

form a lattice search within the set of projection schemes
that partition variables in disjoint subsets. Furthermore, as­
suming that marginals of at most two variables provide a

suitable efficiency/accuracy tradeoff, the lattice is traversed

until all children of

a node correspond to projections with a

marginal with 3 variables. This last node is the projection
scheme returned by the search.
We compare the time required to find a good projection us­
ing the different search procedures in Table

3. As expected,

the running time is much less when using VS-switch tests
(compared to LP-switch tests), since VS-switch tests do not

Empirical Evaluation

require the solution of LPs. As for VS search algorithms,

Three test problems were used to carry out the experiments.
The first POMDP is essentially the coffee problem intro­

[2].

[15],

and chooses a projection using either the B or E error

smaller thanO(nc2+ki�IISik) for the B-bound or E-bound

duced by Boutilier and Poole

56
121
16

ment maintenance problem described by Puterman

search of

one dot product needs to be computed instead of one for

4

102
205
39

each POMDP by minimizing different error bounds and/or

This incremental computation scheme for traversing the lat­

tests, the running time

12

using different switch tests, as described above. Specifi­

�J;<]]proj(a;j ,Ds,)Jj� -vm ·O:;j

c constraints.

time (s)

32
32
128

Once solved, we searched for a good projection scheme for

o:;j

Jr->

each of the

aver.

size set.

L jjproj(aij, Ds,)ll�- Vm

tice reduces the running time to

max

size of the sets

�Ji< jjproj(a;j, Ds)IJ�
J r- l

Solution

effective

show the solution time (in seconds) along with the average

I: Jlproj(o:;j, Ds)ll �
j#i

Size of !X

full

Table 2: Solution statistics for the three test problems

space size,
This means that both Eq.

State Space Size

since one

good projection must be found for each of the I� I regions

R(o:).

UAI2001

The second POMDP is a

variation of the widget problem described by Draper, Hanks

whether we minimize the sum of the relative error rates or
their maximum, the running time is roughly the same and
it is significantly faster than B-bound and E-bound search
algorithms that use LP-switch tests, but a bit slower

if VS

UAI2001

POUPART & BOUTILIER

Problem

Solut.

Coffee

LP

vs

47

1019

30

4379

2651

109
35

89605

48695

345

841

126

397

widget
Pavement

E-bd se arch
LP
vs

B-bd search

time

1 0 1 42

250

VS search
sum

max

151

154

707

703

97

96

Error

Single

B-bd search

E-bd search

LP

LP

vs

vs

4:

Error

stronger dependence on the number of a-vectors (compared

to VS-switch tes ts). The time to search for good projections

be much worse than that of solving POMDPs (though

this offline cost translates into online gains). In fact, only
search procedures that avoid solving LPs scale effectively
to larger problems. In some cases, these offer a decrease of
up to two orders of magnitude. T he running time ofVS pro­
cedures is roughly of the same order of magnitude as that of
the POMDP solution procedures.
We also compare the actual average error, as well as the for­
mal B and

E error bounds, obtained when applying the pro­

jection schemes found by various search algorithms (Tables

4, 5 and 6). T he average error is the average loss incurred
for 5000 random initial belief states generated from a uni­
form distribution. We see that the average error is essen­

tially the same whether the VS search procedure is used or
some error bound is minimized. As a result, the dramatic
computational savings associated with the VS procedures
has effectively no impact on solution quality. Note that the

E bounds are much larger than the average error

observed because the bounds are concerned with the worst
case scenario and, furthermore, they are not tight (supersets
of the switch sets are really computed).

5

Concluding Remarks

We have proposed a new approach to value-directed
belief state approximation for POMDPs. Our vector space
approach-using either VS-switch tests or direct VS
search-offers significant computational benefits over the
value-directed methods proposed by Poupart and Boutilier

[15].

vs

E-bd search
LP
vs

VS search
max

sum

Single

Aver. 0.0015 0.0015 0.0015 0.0015 0.0014 0.0014
Approx B-bd 5.3860 5.6900 5.3860 5.6900 5.3680 5.6160
Several Aver. 0.0066 0.0066 0.0066 0.0066 0.0071 0.0028
Approx E-bd 23.218 35.392 23.498 35.392 23.874 24.384

6:

Pavement problem: error comparisons

tests are used forB-bound search. This is because,

pared to LP-switch tests), but on the other hand, it has a

B and

VS search
max

Coffee problem: error comparisons

on the one hand, the VS search does not solve LPs (com­

can

B-bd search
LP

Table
switch

vs

Table 5: Widget problem: error comparisons

Aver. 0.0013 0.0063 0.0063 0.0063 0.0013 0.0014

Table

LP

vs

Several Aver. 0.0509 0.0508 0.0508 0.0508 0.0519 0.0517
Approx E-bd 8.3811 8.3811 8.3811 8.3811 8.3811 8.3811

VS search
sum

Several Aver. 0.0144 0.0161 0.01 6 1 0 . 01 6 1 0.0154 0.0107
Approx E-bd 13.085 13.085 13.085 13.085 13.085 13.085

E-bd search

LP

sum
Aver. 0.0352 0. 035 2 0.0352 0.0352 0.0082 0.0081
Approx B-bd 3.4080 3.6270 3.4080 3.6270 3.4080 3.4080

max

Approx B-bd 3.2840 5.9150 4.3910 5.9150 3.2840 3.2840

B-bd search

Single

Table 3: Search running time in seconds

Error

451

W hile the error bounds are looser, we have seen in

practice that our new schemes perform as well as the others

with respect to solution quality; thus the computational
savings are achieved with little impact on decision quality.
Furthermore, the vector space model provides new insights
into the belief state approximation problem and how
approximation impacts decision quality.

This novel view also gives us access to numerous tools from
linear algebra to design approximation methods that could
potentially offer better tradeoff's between decision quality
and monitoring efficiency.

For instance, it would be in­

teresting to investigate linear projectors since they allow
the design of linear approximation methods by specifying
(among other things) a displacement subspace Ds which
could be made as perpendicular as possible to the gradi­
ent vectors aij. Linear projectors are well-studied approx­
imation methods with numerous properties and therefore
they provide a promising alternative for improving value­
directed approximate belief state monitoring.
The success and scalability of our methods strongly de­
pends on the structure and compactness of the a-vectors.
Therefore, one could also analyze the dependency between
the a-vector structure and the conditional independence
structure of the transition and observation functions. From
a linear algebra perspective, the a-vectors can be viewed as
a discounted sum of reward vectors multiplied by transition
and observation matrices. T hus compact and structured a­
vectors could arise when the reward vectors fall into a small
invariant subspace of the transition and observation matri­
ces.

A

possible direction of research would then be to re­

late the conditional independence structure of the transition
and observation functions with their eigenvalue and eigen­
vector properties since they define the invariant subspaces.
This would allow us to better characterize the situations in
which our approach is suitable.
We are currently extending this approach, and its analysis,

452

POUPART & BOUTILIER

in a number of different directions. First, we motivated this

UAI2001

[6] A. R. Cassandra, M. L.

Littman, and N. L. Zhang. In­

work by focusing on infinite-horizon POMDPs, though our

cremental pruning: A simple, fast, exact method for

algorithms and analysis assume a finite set of a-vectors. Of­

POMDPs. In Proceedings of the Thirteenth Confer­

ten one is forced to approximate the value function (e.g., by

ence on Uncertainty in Artificial Intelligence, pages

54-6 1, Providence, RI, 1 997.

producing a finite set of vectors where an infinite set is re­
quired, or simply by reducing the number of vectors to keep
it manageable in size). Our algorithms can be applied di­

[7]

ning with information gathering and contingent exe­

rectly to approximate value functions, and we expect that

In Proceedings of the Second International
Conference on AI Planning Systems, pages 3 1-36,

cution.

the analysis can be extended with suitable modifications as
well. We are also interested in applying the idea of value­
directed monitoring to other representations of value func­
tions and other forms of approximate monitoring. The use
of grid-based value functions [4,

Chicago, 1994.

[8]

9, 1 0] provides a very at­

for which approximate monitoring will generally be neces­
tions can be used profitably to direct the choice of projection

E. A. Hansen and Z. Feng. Dynamic programming for

POMDPs using a factored state representation. In Pro ­

ceedings of the Fifth International Conference on AI
Planning Systems, Breckenridge, CO, 2000. 130- 139.

tractive method for producing approximate value functions
sary. We expect that information in grid-based value func­

D. Draper, S. Hanks, and D. Weld. Probabilistic plan­

[9] M. Hauskrecht.

Val ue-function approximations

for partially observable Markov decision processes.

(or other approximation) schemes. The use of value infor­

Journal ofArtificial Intelligence Researr:h, 1 3:33-94,

mation to guide other belief state approximation methods is

2000.

also of tremendous interest: we have recently developed a
sampling (particle filtering) algorithm that is influenced by
value function information

[ 1 6].

[ 10]

partially observed Markov decision processes. Annals

Finally, if it is taken for

granted that some form ofbeliefstate approximation will be

used, one might attempt to solve the POMDP to account for

of Operations Researr:h, 28:47-66, 1991.

[ 1 1] 0. Madani, S. Hanks, and A. Condon. On the undecid­
ability of probabilistic planning and infinite-horizon

this fact; that is, can we construct policies that are optimal

partially observable Markov decision problems.

subject to the resource constraints placed on the monitoring

Natural Sciences and Engineering Research Council and

[ 1 2] G.

[1]

[ 14]

processes.

[ 15]

1168- 1 175, Portland,

OR,

1996.

In Proceedings ofthe Four­
teenth Conference on Uncertainty in Artificial Intelli­
gence, pages 33-42, Madison, WI, 1 998.

[4]

R. I. Brafman.

[16)

[5]

Value­

certainty in Artificial Intelligence, Seattle, 2001. This
volume.

[ 17]

727-733, Providence, 1 997.

A. R. Cassandra, L. P. Kaelbling, and M. L. Littman.

P. Poupart, L. E. Ortiz, and C. Boutilier.

In Proceedings ofthe Seventeenth Conference on Un­

A heuristic variable-grid solution

teenth National Conference on Artificial Intelligence,

P. Poupart and C. Boutilier. Value-directed belief state

directed sampling methods for monitoring POMDPs.

method for POMDPs. In Proceedings of the Four­
pages

2000.

In Proceedings of the
Sixteenth Conference on Uncertainty in Artificial In­
telligence, pages 497-506, Stanford, 2000.

X. Boyen and D. Koller. Tractable inference for com­

plex stochastic processes.

Master's thesis, University of British

approximation for POMDPs.

teenth National Conference on Artificial Intelligence,
pages

Approximate value-directed belief state

Columbia, Vancouver,

cies for partially observable decision processes using

[3]

P. Poupart.

monitoring for partially observable Markov decision

C. Boutilier and D. Poole. Computing optimal poli­
compact representations. In Proceedings of the Thir­

C. H. Papadimitriou and J. N. Tsitsiklis. The complex­

erations Researr:h, 1 2(3):44 1 -450, 1987.

search, 1 1: 1 -94, 1 999.

[2]

28: 1- 1 6, 1982.

ity of Markov decision processes. Mathematics of Op­

C. Boutilier, T. Dean, and S. Hanks. Decision theo­
tional leverage. Journal of Artificial Intelligence Re­

A survey of partially observable

rithms. Management Science,

[13]

retic planning: Structural assumptions and computa­

E. Monahan.

Markov decision processes: Theory, models and algo­

the Institute for Robotics and Intelligent Systems.


