
Message-passing algorithms have emerged as
powerful techniques for approximate inference in graphical models. When these algorithms converge, they can be shown to
find local (or sometimes even global) optima
of variational formulations to the inference
problem. But many of the most popular
algorithms are not guaranteed to converge.
This has lead to recent interest in convergent
message-passing algorithms.
In this paper, we present a unified view of
convergent message-passing algorithms. We
present a simple derivation of an abstract
algorithm, tree-consistency bound optimization (TCBO) that is provably convergent in
both its sum and max product forms. We
then show that many of the existing convergent algorithms are instances of our TCBO
algorithm, and obtain novel convergent algorithms “for free” by exchanging maximizations and summations in existing algorithms.
In particular, we show that Wainwright’s
non-convergent sum-product algorithm for
tree based variational bounds, is actually
convergent with the right update order for
the case where trees are monotonic chains.

1

Introduction

Probabilistic inference in graphical models is a key
component in learning and using these models in practice. The two key inference problems are calculating
the marginals of a model and calculating its most likely
assignment (sometimes referred to as the MAP problem).
One approach to these generally intractable problems
is to use a variational formulation, where approximate

inference is cast as an optimization problem. For the
marginals case this usually corresponds to minimization of a free energy functional, and for the MAP problem it corresponds to solving a linear programming
(LP) relaxation [10].
A key challenge in both the MAP and marginals case
is to devise simple and scalable algorithms for solving the variational optimization problem. In recent
years numerous algorithms have been introduced for
both tasks. These algorithms typically have a “message passing like” structure.
Perhaps the most widely used message-passing algorithms are “belief propagation” and its generalizations [5, 8, 12, 14]. These algorithms typically have two
variants: sum-product which is used to approximate
the marginals, and max-product which is used to approximate the MAP. Fixed-points of these algorithms
can be shown to be local (or sometimes even global)
optima of the corresponding variational formulation.
Yet despite the spectacular empirical success of these
algorithms in real-world applications, they are not
guaranteed to converge, and variants of “dampening”
are often used to improve their convergence [8, 12, 14].
There has therefore been much recent work on convergent message passing algorithms [2, 5, 7, 13]. These algorithms are often very similar in structure to the nonconvergent algorithms and often include local maxproduct or sum-product operations. However, for each
of these specific algorithms it has been possible to
prove that the value of the variational problem (or its
dual) improves at each iteration. Perhaps the most intriguing example of this is Kolmogorov’s TRW-S algorithm [7] which is simply Wainwright’s tree-reweighted
max-product algorithm [11] with a different update
schedule.
Here we introduce a unifying framework which encompasses both marginals and MAP approximations,
by exploiting the mathematical similarities between
these approximations. Specifically, we provide an up-

394

MELTZER ET AL.

per bound on the optimum of the variational approximations, and give sufficient conditions that algorithms
need to satisfy in order to decrease this bound in a
monotone fashion. Any algorithm which satisfies these
conditions is guaranteed to decrease the upper bound
at every iteration. This property in turn guarantees
that such algorithms converge to a global optimum of
the variational problem in the marginals case and to a
local optimum in the MAP LP approximation case.
Our framework involves updating a subset of regions
which form a tree in the region graph. A related approach was recently suggested by Sontag et al. [9]
in the context of solving the MAP approximation.
Their work gives an explicit algorithm for optimizing
all edges corresponding to a tree in the original graph,
such that an upper bound on the LP optimum is decreased at every iteration. Our formalism does not give
an explicit update but rather conditions that guarantee an update to decrease the objective. However,
we show that these conditions are satisfied by several
known algorithms. Furthermore, since the condition
is similar in the marginals and MAP case (specifically
a condition of sum and max consistency respectively)
it is easy to obtain algorithms for both these cases simultaneously, and to use results in one problem for
obtaining algorithms for the other.
For instance, we consider the tree-reweighted (TRW)
free energy minimization problem [12]. Recently two
works have provided convergent algorithms for this
problem [1, 3], but these were more involved than standard message passing algorithms. Here we show the
surprising result that in fact the original algorithm
provided for TRW by Wainwright et al. is convergent, if run with an appropriate schedule of message
updates.

2

Bounds for MAP and Log-Partition

We consider a graphical model where the joint probability over variables p(x) factors
Q into a product over
clique potentials p(x) = Z1 α Ψα (xα ) or equivalently, the energy function
is a sum over clique enP
1
ergies p(x)
=
θ
(xα )). We also denote
exp(
α
α
Z
P
θ(x) = α θα (xα ).
The problem of calculating marginals and approximation of the partition function Z can be recast as the
following maximization problem of the function F (q)
(the negative of the free energy):
log Z = max F (q) = max (hθ(x)iq + H(q))
q

q

(1)

where q is the set of probability distributions over x,
hθ(x)iq is the average energy with respect to q and
H(q) is the entropy function. The maximizing argu-

UAI 2009

ment is then the distribution p(x).
This maximization is in general intractable, so approximate free energies are often used. A class of approximate free energies, discussed in [14], is based on the
concept of a region graph G whose nodes α are regions
of the original graph, and whose edges represent subregion relationships (i.e. an edge between α in β exists
only if β ⊂ α). The approximation replaces the joint
entropy H(q) with a linear combination of local region
entropies Hα (qα ) where each local entropy is weighted
by a “double-counting” number cα :
H̃G,c (q) =

X

cα Hα (qα ) ,

(2)

α

where the subscript G, c indicates the dependence of
the approximation on the region graph G and the
counting numbers cα .
With this approximation of H(q) the free energy now
only depends on local distributions, since the average energyPis a simple function of the qα , namely
hθ(x)iq =
α hθα (xα )iqα . To optimize only over local distributions qα , we need to consider only distributions such that there exists a q(x) that has these as
marginals. This set (called the marginal polytope [10])
cannot be expressed in a compact form, and is typically approximated. One popular approximation is the
local polytope of the region graph L(G) defined as the
set of local distributions that agree on the marginals
for any two regions in the region graph that are subsets
of each other:1
P
)
(
∀β ⊂ α, xβ
qα (xα ) = qβ (xβ )
xα \xβ
L(G) = q ≥ 0
P
∀α,
xα qα (xα ) = 1
Take together, this results in the following standard
variational approximation [10]:
X
max F̃ (q) = max
hθα (xα )iqα + H̃G,c (q) (3)
q∈L(G)

q∈L(G)

α

Similarly the MAP problem is approximated via
X
max F̃ (q) = max
hθα (xα )iqα
q∈L(G)

q∈L(G)

(4)

α

To obtain a unified formalism for MAP and marginals,
we use a temperature parameter T where T = 1 for
marginals and T = 0 for MAP and the optimization
is:
X
max F̃ (q) = max
hθα (xα )iqα + T H̃G,c (q) (5)
q∈L(G)

q∈L(G)

α

1
Note that this is the local polytope of the region graph
not the local polytope of the original graph

UAI 2009
2.1

MELTZER ET AL.

Positive Counting Numbers

The entropy approximation H̃G,c (q) in Eq. 2 is generally not a concave function of the local distributions q.
Thus maximization of F̃ (q) may result in local optima.
To avoid this undesirable property, several works (e.g.,
[5]) have focused on entropies which are obtained by
considering only concave H̃G,c (q) functions. We focus on approximations where all the double counting
numbers are non-negative. This is a strong restriction
but since we are working with a region graph formulation, many approximate free energies which have negative double counting numbers can be transformed into
ones with positive double counting numbers on a region graph. Perhaps the most important example are
tree-reweighted free energies inPwhich the entropy is
approximated as HT RBP (q) = τ ρτ Hτ (q) with ρτ a
probability distribution over trees in the graph and
Hτ (q) is the entropy of the distribution on τ with
marginals given by q (more precisely, the projection
of q on the tree τ ). If we consider a region graph
with trees and their intersection (Fig. 5) the double
counting numbers are non-negative.
ButP
HT RBP can
P
also be rewritten
H
=
ρ
H
+
T RBP
ij ij ij
i ci Hi with
P
ci = 1 − j ρij and ρij is the edge appearance probability of the edge ij under the distribution ρ. In this
case, the double-counting number for the singletons
ci may be negative. However, we will show that it is
sometimes advantageous to work in the representation
that uses a non-negative mixture of trees, since nonnegativity of the counting numbers allows a simpler
derivation of algorithms.
2.2

Optimization and Reparameterization

The vast majority of methods for solving the variational approximation are based on two classes of
constraints that local optima should satisfy. It is
easy to show using Lagrange multipliers, that local optima of F̃ should satisfy two types of constraints [5, 6, 12, 14, 15]
• Reparametrization Q
(or admissibility, or “e constraints”). P (x) ∝ α qα (xα )cα , for every x.

• sum-consistency
(or
“m
constraints”),
P
xα \xβ qα (xα ) = qβ (xβ ) for all β ⊂ α and
xβ .

By enforcing each of these constraints iteratively one
obtains many of the popular sum-product algorithms.
Replacing the sum-consistency constraint with maxconsistency gives many of the popular max-product
algorithms. A simple example is ordinary BP, which
maintains admissiblity at each iteration and a message
from i to j enforces consistency between bij and bj .

395

In general, simply iteratively enforcing constraints is
not guaranteed to give convergent algorithms. However, as we show in this paper, by iterating through
the constraints in a particular order, we obtain monotonically convergent algorithms.
2.3

Bound minimization and
reparameterizations

We begin by providing an upper bound on the logpartition function whose minimization is equivalent to
the maximization in Eq. 5.
We consider marginals bα of the exponential form:
bα (xα ; θ̃α ) =



1
exp θ̃α (xα )/cα
Zθ̃α

(6)

and require that these marginals will be admissible
(maintain the “e constraints”). We obtain admissibility by requiring that the variables θ̃ will satisfy the
following for each x:
X
X
θα (xα ) =
θ̃α (xα )
(7)
α

α

The algorithms we propose will optimize over the variables θ̃ while keeping the constraint in Eq. 7 satisfied
at all times. Moreover, they will monotonically decrease an upper bound on the optimum of Eq. 5. In
the following two lemmas we provide this bound for
the sum and max cases.
Lemma 1 The approximation to the log partition
function is bounded above by:
X
boundsum (θ̃) =
(8)
cα ln Zθ̃α
α

for any reparameterization θ̃ (i.e., any θ̃ satisfying
Eq. 7).
Minimizing boundsum (θ̃) over the set of reparameterizations θ̃ would give the approximated log-partition
function:
min boundsum (θ̃) = max F̃ (q)
θ̃

q∈L(G)

(9)

This is the optimum of Eq. 5 with T = 1.
Proof: Kolmogorov [7] showed that if θ̃ is a reparameterization (i.e. keeping the constraint in Eq. 7), it also
holds that hθ̃iq = hθiq for any q ∈ L(G). Using this
property, we can see that the log-partition function is
constant under reparameterization: F̃ (q; θ) = F̃ (q; θ̃)
for any q ∈ L(G), and in particular the maximum value

396

MELTZER ET AL.

will remain the same. Now, using the new variables θ̃
we have a trivial bound on the log-partition:

X
hθ̃α iqα + cα Hα (qα )
max F̃ (q; θ̃) = max
q∈L(G)

q∈L(G)

≤

X
α

Algorithm 1 The tree consistency bound optimization (TCBO) algorithm
Iterate over sub-graphs T of the region graph that have
a tree structure:

α

1. Choose a tree T



max hθ̃α iqα + cα Hα (qα )
qα

2. Update the values of θ̃αt+1 for all α ∈ T such that:

Since the counting numbers cα are non-negative, the
marginals defined in Eq. 6 maximize each local functional Fα (qα ; θ̃α , cα ) = hθ̃α iqα + cα Hα (qα ), and the
optimal value is cα ln Zθ̃α . Thus,
max F̃ (q; θ̃) ≤

q∈L(G)

X

cα ln Zθ̃α

• re-parameterization is maintained:
X
X
θ̃αt+1 (xα ) +
θ(x) =
θ̃αt (xα )
• tree-consistency is enforced:
Define the beliefs

(10)

α

t+1
bt+1
α (xα ; θ̃α ) =

or optimize the bound to the MAP by enforcing max-consistency:

q∈L(G)

t+1
t+1
t+1
max bt+1
α (xα ; θ̃α ) = bβ (xβ ; θ̃β )
xα\β

xα

Minimizing boundmax (θ̃) over the set of reparameterizations θ̃ would give the optimal value for the regiongraph LP relaxation of the MAP:
X
min boundmax (θ̃) = max
(12)
hθα (xα )iqα
α

This is the optimum of Eq. 5 with T = 0.
Proof: The bound follows directly P
from the admissiblity constraint so that maxx α θ̃α (xα ) ≤
P
α maxxα θ̃α (xα ). The fact that the tightest bound
coincides with the LP relaxation was proven in [13].
We note that the above two lemmas may also be
viewed as an outcome of convex duality. In other
words, the original variational maximization problem
and the equivalent bound minimization problem are
convex duals of each other.
In the following sections we provide a framework for
deriving minimization algorithms for the above two
bounds.

Zαt+1



exp θ̃αt+1 (xα )/cα

xα\β

Lemma 2 The value of the MAP is bounded above by:
X
boundmax (θ̃) =
(11)
max θ̃α (xα )
for any reparameterization θ̃ (i.e., any θ̃ satisfying
Eq. 7).

1

For each α ∈ T, β ∈ T, β ⊂ α and xβ , optimize the bound to the log-partition function
by enforcing sum-consistency:
X
t+1
t+1
t+1
bt+1
α (xα ; θ̃α ) = bβ (xβ ; θ̃β )

A similar result may be obtained for the MAP case
(this result or variants of it appeared in previous
works, e.g., [7, 9, 13]).

α

α6∈T

α∈T

The bound is tight if there exists a reparameterization θ̃ such that the marginals bα (xα ; θ̃α ) are sumconsistent (i.e. b ∈ L(G)). The existence of such a
re-parameterization is guaranteed if the maximum of
the approximated negative free energy F̃ (q; θ) does not
happen at an extreme point [14].

θ̃

UAI 2009

3

Bound optimization and consistency

We propose the tree consistency bound optimization
(TCBO) algorithm as a general framework for minimizing the bounds in Sec. 2.3 for the approximated
log-partition and for the MAP, within a region graph
with positive counting numbers cα .
The idea is to perform updates on trees that are subgraphs of the region-graph. The θ̃ corresponding to
each such tree will be updated simultaneously in a way
that will achieve a monotone decrease in the bound.
The method we propose, as described in Algorithm 1
keeps the beliefs admissible with the positive counting numbers cα (or equivalently, always maintains θ̃(x)
that reparameterize the original energy θ(x)). The corresponding θ̃ thus satisfy the conditions of the bound
in Sec. 2.3. Furthermore, at each iteration, max or sum
consistency of the beliefs is enforced for the subtree T .
As mentioned earlier, maintaining consistency on subsets does not generally result in convergent algorithms.
However, as the following lemmas show, in our case
enforcing consistency is equivalent to block coordinate

UAI 2009

MELTZER ET AL.

397

descent on the bound.
Lemma 3 The sum-consistency lemma: Consider the bound minimization problem for the logpartition function with positive counting numbers
(Lemma 1), defined on a subset of regions and intersections T . The part of the bound which is influenced
by the beliefs of the subset is:
X
P BT (θ̃T ) =
cα ln Zθ̃α
α∈T

The problem is to find {θ̃α } for all α ∈ T that minimize
P BT (θ̃T ) subject to θ̃ being reparameterizations of the
energy {θα }. If for some
θ̃∗ the be
 reparameterization

Figure 1: A simple 2x2 grid, with pair regions.
Proof: The part of the bound which is dependent on
θ̃T is bounded below:
P BT (θ̃T ) ≥ max θ̃T (xT )
xT

where

liefs b(xα ; θ̃α∗ ) ∝ exp θ̃α∗ /cα are sum-consistent, then
it minimizes the bound.

Proof: The part of the bound which is dependent on
θ̃T the is bounded below:
X
P BT (θ̃T ) =
max Fα (qα ; θ̃α , cα )
α∈T

≥

qα

max

qT ∈L(G)

X

Fα (qα ; θ̃α , cα )

α∈T

Now, if we find variables θ̃α for all α ∈ T such
that they provide global re-parameterization θ̃(x) =
θ(x) (so we can have a bound), and the marginals
bα (xα ; θ̃α ) ∝ exp(θ̃α (xα )/cα ) which maximize each
term Fα (qα ; θ̃α , cα ) separately are also sum-consistent
(bT ∈ L(G)), then P BT (θ̃T ) will achieve its optimal
value, and thus we perform block coordinate descent
on the bound.
Note that for optimizing the bound to the logpartition, the subset T does not have to form a tree,
and the sum-consistency of the beliefs is enough. Yet,
it may be easier in practice to enforce sum-consistency
on trees.
Lemma 4 The max-consistency lemma: Consider the bound minimization problem for MAP with
positive counting numbers (Lemma 2), defined on a
subset of regions and intersections that form a tree T .
The part of the bound which is influenced by the beliefs
of the tree is:
X
P BT (θ̃T ) =
max θ̃α (xα )
α∈T

xα

The problem is to find {θ̃α } for all α ∈ T that minimize
P BT (θ̃T ) subject to θ̃ being reparameterizations of the
energy {θα }. If for some
θ̃∗ the be
 reparameterization
liefs b(xα ; θ̃α∗ ) ∝ exp θ̃α∗ /cα are max-consistent, then
it minimizes the bound.

. X
θ̃α (xα )
θ̃T (xT ) =
α∈T

so if we can find an assignment x∗T whose cost θ̃T (x∗T )
equals P BT (θ̃T ), that means we have the tightest
bound. Now, if for some
θ̃T∗ the be
 reparameterization

liefs bα (xα ; θ̃α∗ ) ∝ exp θ̃α∗ /cα are max-marginalizable
then we can always find an assignment x∗T that sits on
the maxima of θ̃α∗ because the subgraph is a tree (so
there cannot be any frustrations). Hence, we obtain
θ̃T (x∗T ) = P BT (θ̃T∗ ), and the bound achieves its optimal value for the coordinates in T .
The above two lemmas show that the TCBO algorithm
monotonically decreases the bound after each update.
In the log-partition case, the bound is strictly convex
and thus this strategy finds the global minimum of
the bound which is the global maximum of Eq. 5. In
the MAP case, the function is not strictly convex and
the algorithm may converge to values that are not its
global optimum. This phenomenon is shared by most
dual descent algorithms (e.g., [2, 7, 13]).
TCBO is a general scheme and can be implemented
for different choices of tree sub-graphs. In the next
section we illustrate some possible choices and their
relation to known algorithms.

4

Existing bound minimizing
algorithms

We identify some existing convergent algorithms as instances of TCBO: Heskes’ algorithm [5] for approximating the marginals, and MPLP [2],TRW-S [7], maxsum diffusion (MSD) [13] for approximating MAP.
Figures 2-5 show the reparametrization, region graph
and the tree sub-graph updated at each iteration of
these algorithms, for the simple example of a 2x2
grid shown in Fig. 1. Note that all algorithms use
a reparameterization with positive double counting
numbers. Furthermore, they update only a subtree at

398

MELTZER ET AL.

Figure 2: Illustration of the max-sum-diffusion (MSD)
algorithm as an instance of the TCBO formalism.
MSD operates on a region graph containing pairs and
singletons (here corresponding to a 2x2 grid). The subgraph T corresponding to the TCBO update is shown
in the blue dashed line.

UAI 2009

Figure 3: Heskes’ sum-product algorithm may be
viewed as a TCBO algorithm updating the subtree
shown in the blue dashed line (a star graph centered
on a singleton node).
Algorithm 3 Heskes’ sum-product algorithm
Iterate over intersection regions β:

Algorithm 2 The max sum diffusion (MSD) algorithm
Iterate over edges between regions < α, β >:

1. ∀α ⊃ β set the message from α to β:

1. Set the message from α to β:

t+1
mα→β (xβ )

=

t
mα→β (xβ )

t+1
mα→β (xβ )

v
u
u maxxα\β btα (xα )
·t
btβ (xβ )

=

t
xα\β bα (xα )
mtβ→α (xβ )

P

2. Update the belief of the intersection region:
t+1

bβ

2. Update the beliefs:

(xβ ) ∝

”cα /ĉ
Y “ t+1
β
mα→β (xβ )

α⊃β
t+1

bβ

(xβ )

∝

t+1

mα→β (xβ ) ·

Y

t

mα′ →β (xβ )

α′ 6=α
t+1
bα (xα )

∝

Ψα (xα )
Q
t
mt+1
β ′ 6=β mα→β ′ (xβ )
α→β (xβ ) ·

3. ∀α ⊃ β set the messages to the parent regions and their beliefs:
t+1

mβ→α (xβ )

=

t+1
bα (xα )

∝

bt+1
(xβ )
β
mt+1
α→β (xβ )
1/cα

Ψα

t+1

(xα ) · mβ→α (xα )

Y

t

′

mβ ′ →α (xβ )

β ′ 6=β

a time. What remains to be shown is that each iteration achieves consistency among the beliefs (in other
words, it satisfies the conditions of TCBO framework
and thus monotonically decreases the corresponding
upper bound).
Heskes’ algorithm can be shown to be an instance of
TCBO using direct substitution. The update rules are
shown in algorithm 3. MPLP (algorithm 4) does not
appear at first sight to use the region graph illustrated
in Fig. 4, but rather works with edges and singletons.
However, as we show in the appendix, there is a way
to transform the messages used in the max-product
version of Heskes’s algorithm into messages of MPLP
using the MPLP region graph. The max-consistency
achieved by MSD (algorithm 2) can again be shown
directly.
It is also possible to use tree graphs (or forests) as
regions, and various existing methods indeed use this
approach. We may consider a TCBO algorithm which
iterates through all edges and nodes, and for each edge
or node enforces consistency between it and all trees
that contain it. This is illustrated in Fig. 5. A naive

implementation of such a scheme is costly, as it requires multiple tree updates for every edge. However,
Kolmogorov [7] showed that there exists an efficient
implementation (which he called TRW-S) of such a
scheme in the MAP case. This implementation may
only be applied if the trees are monotonic chains, defined as follows: given an ordering of the nodes in a
graph, a set of chains is monotonic if the order of nodes
in the chain respects the given ordering. This structure
allows one to reuse messages in a way that simultaneously implements operations on multiple trees. The
scheduling of messages is important for guaranteeing
convergence in this case. It turns out that one needs to
scan nodes along the pre-specified order, first forward
and then backward.
In the marginals case, the TRW algorithm of Wainwright [12] corresponds to optimizing over tree regions
but is not provably convergent. In the next section we
show how to derive a convergent algorithm for this case
using our formalism.

UAI 2009

MELTZER ET AL.

399

Figure 4: MPLP for pairs and singletons is equivalent
to Heskes’ algorithm with stars and pairs.
Algorithm 4 The max product linear programming
(MPLP) algorithm
Iterate over pairs of neighbouring nodes < ij >:
1. Set the message from i to < ij >:
t+1

mi→ij (xi ) =

t

Y

mik→i (xi )

k∈N (i)\j

and equivalently from j to < ij >
2. Update the pairwise beliefs of < ij >:
t+1

bij (xi , xj ) ∝

q
t+1
Ψij (xi , xj ) · mt+1
i→ij (xi ) · mj→ij (xj )

3. Set the messages from < ij > to i (and equivalently from
< ij > to j):
v
“
”
u
u max
Ψij (xi , xj ) · mt+1
xj
u
j→ij (xj )
t+1
mij→i (xi ) ∝ t
mt+1
i→ij (xi )
4. Set the beliefs of i (and same for j):
t+1

bi

t+1

(xi ) ∝ mij→i (xi ) ·

Y

t

mik→i (xi )

k∈N (i)\j

5

Figure 5: A region graph with chains, their pairwise
and singleton components. Such graphs are used by
the TRW-S algorithms. In the above example there
are two chains, which are also monotonic chains since
they agree with the node ordering {1, 2, 3, 4}. TRW-S
may be viewed as a TCBO algorithm on the subgraph
shown in the blue dashed line (and an additional subgraph corresponding to a pairwise component and all
the chains that contain it).
since it is an instance of a TCBO algorithm for the
sum case. Furthermore, this TRW-S variant differs
from the algorithm in [12] only in the scheduling of
messages.
An additional algorithm that can be easily shown to
be convergent is two way GBP [14] with all double
counting numbers cα = 1, both in the sum and in
the max versions. At each iteration, two-way GBP
updates only the beliefs of a region and one of its subregions, which is trivially a tree. The fact that it maintains reparameterization and enforces consistency can
be shown directly. In fact, it can be shown that two
way GBP with cα = 1 is identical to MSD.

New bound minimizing algorithms
6

By replacing the max with a sum (or vice versa) in
the algorithms discussed in the previous section, we
obtain algorithms that enforce a different type of consistency, and keep the same reparameterization and
region graph as shown in the figures. Thus, the maxproduct version of Heskes’ algorithm and the sumproduct versions of TRW-S, MPLP and MSD are convergent with respect to the relevant bound.
The TRW-S sum-product case is especially interesting. In this case the relevant bound becomes the treereweighted log-partition function bound introduced in
[12]. The message passing algorithm suggested in [12]
does not generally converge. In contrast, the TRWS sum-product algorithm is guaranteed to converge,

Experiments

We present two experiments to illustrate the convergence of our sum and max algorithms. All algorithms
were applied to an instance of a 10x10 “spin glass”
with pairwise terms drawn randomly from [−9, 9] and
field from [−1, 1]. In each case we tested the new
TCBO algorithms.
For estimating the log-partition, we ran TRW and considered a uniform distribution over 2 spanning forests
in the graph: all horizontal and all vertical chains.
These chains are monotone with respect to the node
ordering {1, 2, ..., 100}. We ran TRW-S by following
the nodes order first forward and then backward, updating each time only the messages in the direction of

400

MELTZER ET AL.

UAI 2009

Algorithm 5 The sequential tree reweighted BP (TRW-

940

S) algorithm

1/ρij

mi→j (xj ) ∝ max Ψi (xi )Ψij
xi

(xi , xj )

Q

ρik
k∈N (i)\j mk→i (xi )
1−ρ
mj→iij (xi )

2. After each iteration over all edges, update all singleton and
pairwise beliefs:
bi (xi )

∝

Ψi (xi )

Y

∝
·

ρ

ik (x )
mk→i
i

k∈N (i)\j
1−ρ
mj→iij (xi )

Q

ρ

jk
k∈N (j)\i mk→j (xj )

900
890
880

860
0

10

20

30
Iterations

40

50

60

1−ρ

mi→jij (xj )

the scan. Fig. 6 shows a comparison of this schedule to
TRW where the node ordering is followed in a forward
manner, and all outgoing messages are updated from
each node. Both schedules keep a re-parameterization
and provide a bound to the log-partition, yet only
TRW-S monotonically decreases it at each iteration.
For the MAP case, we ran the max-product version of
Heskes’ algorithm using a region graph of pairs (with
double counting numbers 1) and singletons (with double counting numbers 0). We also ran MPLP and MSD
on the same problem. Fig. 7 shows the bounds obtained after each iteration. As can be seen, all three
algorithms monotonically converged to the same value.

7

sum−TRW−S
sum−TRBP

910

870

1/ρ
Ψi (xi )Ψj (xj )Ψij ij (xi , xj )

Q

920

ρ

ij
mj→i
(xi )

j∈N (i)

bij (xi , xj )

Bound to Log−Partition

930

1. Iterate over edges i → j in a certain updating order, and set
the message from i to j:

Discussion

Despite the empirical success of max-product and sumproduct algorithms in applications, the original algorithms are not guaranteed to converge. Much research
in recent years has therefore been devoted to devising convergent algorithms. Typically these recent algorithms are either max-product or sum-product and
their proof of convergence is specific to the algorithm.
Here we have presented a unified framework for convergent message passing algorithms and showed the importance of enforcing consistency in both sum-product
and max-product algorithms. Not only does this analogy allow us to give a unified derivation for existing
algorithms, it also gives an easy way to derive novel algorithms from existing ones by exchanging maximizations and summations.
Although many convergent algorithms are instances
of our framework, it is worth pointing out two convergent algorithms that are not. The first is Hazan and
Shashua’s recent algorithm [3, 4] which works for provably convex double counting numbers (not necessarily

Figure 6: The bound on the log-partition in a
10x10 “spin-glass”, obtained by sum-product TRW
and TRW-S with edge probability appearances of 1/2.
Note that the two algorithms differ only in the order
of updates they perform. TRW-S follows the node ordering {1, 2, ..., 100} that agrees with the monotonic
chains, first forward and then backward. In the TRW
implementation we followed the same nodes order in a
forward manner.

positive as we are assuming). The second is ordinary
BP on a single cycle, which can be shown to be convergent in both its sum and max product forms. We
emphasize that even negative counting numbers can be
handled by us in some cases, by using larger regions.
All the algorithms we discussed here in fact only updated star graphs in the region graph. Our conditions
for monotonicity apply to general tree updates. However, it seems less straightforward to obtain general
(non-star) tree updates that achieve (max or sum) consistency and reparameterization simultaneously. Interestingly, the tree based updates in [9] do monotonically
decrease an upper bound but seem not to satisfy maxconsistency. Thus, it remains an interesting challenge
to find general tree updates that satisfy the consistency
constraints, as these could be easily used interchangeably for MAP and marginals.
Perhaps the most intriguing result of our analysis is
the importance of update schedule for obtaining convergence – a non-convergent algorithm becomes convergent when the right update schedule is used. It
will be interesting to see whether convergent update
schedules can be derived for an even larger class of
message-passing algorithms.

UAI 2009

MELTZER ET AL.

clusters and “hyper stars” respectively, in which the
centers are the intersections of the clusters.

890
885



Linear Programming (LP) relaxations have
become powerful tools for finding the most
probable (MAP) configuration in graphical
models. These relaxations can be solved
efficiently using message-passing algorithms
such as belief propagation and, when the relaxation is tight, provably find the MAP configuration. The standard LP relaxation is
not tight enough in many real-world problems, however, and this has lead to the use
of higher order cluster-based LP relaxations.
The computational cost increases exponentially with the size of the clusters and limits the number and type of clusters we can
use. We propose to solve the cluster selection problem monotonically in the dual LP,
iteratively selecting clusters with guaranteed
improvement, and quickly re-solving with the
added clusters by reusing the existing solution. Our dual message-passing algorithm
finds the MAP configuration in protein sidechain placement, protein design, and stereo
problems, in cases where the standard LP relaxation fails.

1

Introduction

The task of finding the maximum aposteriori assignment (or MAP) in a graphical model comes up in a
wide range of applications. For an arbitrary graph,
this problem is known to be NP hard [11] and various
approximation algorithms have been proposed.
Linear Programming (LP) relaxations are commonly
used to solve combinatorial optimization problems in
computer science, and have a long history of being
used to approximate the MAP problem in general
graphical models (e.g., see [9]). LP relaxations have an
advantage over other approximate inference schemes in

Tommi Jaakkola
CSAIL, MIT
Cambridge, MA

Yair Weiss
Hebrew University
Jerusalem, Israel

that they come with an optimality guarantee – if the
solution to the linear program is integral, then it is
guaranteed to give the global optimum of the MAP
problem.
An additional attractive quality of LP relaxations is
that they can be solved efficiently using messagepassing algorithms such as belief propagation and its
generalizations [3, 13, 15]. In particular, by using
message-passing algorithms, we can now use LP relaxations for large-scale problems where standard, offthe-shelf LP solvers could not be used [18].
Despite the success of LP relaxations, there are many
real-world problems for which the basic LP relaxation
is of limited utility in solving the MAP problem. For
example, in a database of 97 protein design problems
studied in [18], the standard LP relaxation allowed
finding the MAP in only 2 cases.
One way to obtain tighter relaxations is to use clusterbased LP relaxations, where local consistency is enforced between cluster marginals. As the size of the
clusters grow, this leads to tighter and tighter relaxations. Furthermore, message-passing algorithms can
still be used to solve these cluster-based relaxations,
with messages now being sent between clusters and not
individual nodes. Unfortunately, the computational
cost increases exponentially with the size of the clusters, and for many real-world problems this severely
limits the number of large clusters that can be feasibly
incorporated into the approximation. For example, in
the protein design database studied in [18], each node
has around 100 states, so even a cluster of only 3 variables would have 106 states. Clearly we cannot use too
many such clusters in our approximation.
In this paper we propose a cluster-pursuit method
where clusters are incrementally added to the relaxation, and where we only add clusters that are guaranteed to improve the approximation. Similar to the
work of [16] who worked on region-pursuit for sumproduct generalized belief propagation [19], we show

how to use the messages from a given cluster-based
approximation to decide which cluster to add next. In
addition, by working with a message-passing algorithm
based on dual coordinate descent, we monotonically
decrease an upper bound on the MAP value.

2

MAP and its LP Relaxation

We consider functions over n discrete variables x =
{x1 , . . . , xn } defined as follows. Given a graph G =
(V, E) with n vertices, and potentials θij (xi , xj ) for all
edges ij ∈ E, define the function
f (x; θ) =

X

θij (xi , xj ) +

ij∈E

X

θi (xi ) .

(1)

i∈V

Our goal is to find the MAP assignment, xM , that
maximizes the function f (x; θ).
The MAP problem can be formulated as a linear program as follows. Let µ be a vector of marginal probabilities that includes {µij (xi , xj )}ij∈E over variables
corresponding to edges and {µi (xi )}i∈V associated
with the nodes. The set of µ that arise from some joint
distribution is known as the marginal polytope [14],

M(G) =

µ | ∃p(x) s.t.

p(xi , xj ) = µij (xi , xj )
p(xi ) = µi (xi )


.

The MAP problem can then be shown to be equivalent
to the following LP,
max f (x, θ) = max µ · θ ,
x

(2)

µ∈M(G)

refer to the P
marginal of τc (xc ) for the edge (i, j), i.e.
τc (xi , xj ) = xc\i,j τc (xc ). Define MC (G) as




µij (xi , xj ) = µi (xi )

τP
c (xi , xj ) = µij (xi , xj ) ∀c, (i, j) ⊆ c

xc τc (xc ) = 1
P

∃τ ≥ 0
 µ≥0

xj

It is easy to see that MC (G) is an outer bound on
M(G), namely MC (G) ⊇ M(G). As we add more
clusters to C the relaxation of the marginal polytope
becomes tighter. Note that similar constraints should
be imposed on the cluster marginals, i.e., they themselves should arise as marginals from some joint distribution. To exactly represent the marginal polytope,
such a hierarchy of auxiliary clusters would require
clusters of size equal to the treewidth of the graph.
For the purposes of this paper, we will not generate
such a hierarchy but instead use the clusters to constrain only the associated edge marginals.
2.1

Choosing Clusters in the LP Relaxation

Adding a cluster to the relaxation MC (G) requires
computations that scale with the number of possible
cluster states. The choice of clusters should therefore
be guided by both how much we are able to constrain
the marginal polytope, as well as the computational
cost of handling larger clusters. We will consider a
specific scenario where the clusters are selected from
a pre-defined set of possible clusters C0 such as triplet
clusters. However, we will ideally not want to use all
of the clusters in C0 , but instead add them gradually
based on some ranking criterion.

P
P
where µ · θ =
ij∈E
xi ,xj θij (xi , xj )µij (xi , xj ) +
P P
µ
(x
)θ
(x
).
There
always exists a maximizi
i
i
i
i
xi
ing µ that is integral – a vertex of the marginal polytope – and which corresponds to xM . Although the
number of variables in this LP is only O(|E|+|V |), the
difficulty comes from an exponential number of linear
inequalities typically required to describe the marginal
polytope M(G).

The best ranking of clusters is problem dependent. In
other words, we would like to choose the subset of clusters which will give us the best possible approximation
to a particular MAP problem. We seek to iteratively
improve the approximation, using our current beliefs
to guide which clusters to add. The advantage of iteratively selecting the clusters is that we add them only
up to the point that the relaxed LP has an integral
solution.

The idea in LP relaxations is to relax the difficult
global constraint that the marginals in µ arise from
some common joint distribution. Instead, we enforce
this only over some subsets of variables that we refer
to as clusters. More precisely, we introduce auxiliary
distributions over clusters of variables and constrain
the edge distributions µij (xi , xj ) associated with each
cluster to arise as marginals from the cluster distribution.1 Let C be a set of clusters such that each c ∈ C is
a subset of {1, . . . , n}, and let τc (xc ) be any distribution over the variables in c. We also use τc (xi , xj ) to

Recently, Sontag and Jaakkola [12] suggested an approach for incrementally adding constraints to the
marginal polytope using a cutting-plane algorithm. A
similar approach may in principle be applied to adding
clusters to the primal problem. One shortcoming of
this approach is that it requires solving the primal LP
after every cluster added, and even solving the primal LP once is infeasible for large problems involving
hundreds of variables and large state spaces.

1

Each edge may participate in multiple clusters.

In the next section we present a method that incrementally adds clusters, but which works exclusively
within the dual LP. The key idea is that the dual LP

provides an upper bound on the MAP value, and we
seek to choose clusters to most effectively minimize
this bound. Note that an analogous bound minimization strategy is problematic in the primal where we
would have to assess how much less the maximum
is due to including additional constraints. In other
words, obtaining a certificate for improvement is difficult in the primal. Moreover, unlike the dual, the
primal algorithm might not give an upper bound on
the MAP prior to convergence.
Finally, we can “warm start” our optimization scheme
after each cluster addition in order to avoid re-solving
the dual LP. We do this by reusing the dual variables calculated in the previous iterations which did
not have the new clusters.

3

Dual LP Relaxation

The obstacles to working in the primal LP lead us
to consider the dual of the LP relaxation. Different
formulations of the primal LP have lead to different
dual LPs, each with efficient message-passing algorithms for solving them [3, 6, 13, 15]. In this paper
we focus on a particular dual formulation by Globerson and Jaakkola [3] which has the advantage that the
message-passing algorithm corresponds to performing
coordinate-descent in the dual LP. Our dual algorithm
will address many of the problems that were inherent
in the primal approaches, giving us:

we show in our experiments that MAP assignments
can be found for nearly all of the problems we consider.
We next describe the generalized MPLP algorithm for
the special case of clusters comprised of three nodes.
Although the algorithm applies to general clusters, we
focus on triplets for simplicity, and because these are
the clusters used in the current paper.
MPLP passes the following types of messages:
• Edge to Node: For every edge e ∈ E (e denotes
two indices in V ) and every node i ∈ e, we have a
message λe→i (xi ).
• Edge to Edge: For every edge e ∈ E, we have
a message λe→e (xe ) (where xe is shorthand for
xi , xj , and i and j are the nodes in the edge).
• Triplet to Edge: For every triplet cluster c ∈ C,
and every edge e ∈ c, we have a message λc→e (xe ).
The updates for these messages are given in Figure
1. To guarantee that the dual objective decreases, all
messages from a given edge must be sent simultaneously, as well as all messages from a triplet to its three
edges.
The dual objective that is decreased in every iteration
is given by


X
X
g(λ) =
max θi (xi ) +
λki→i (xi )
i∈V

3. Simple “warm start” of tighter relaxation.
4. An efficient algorithm that scales to very large
problems.
3.1

The Generalized MPLP Algorithm

The generalized Max-Product LP (MPLP) messagepassing algorithm, introduced in [3], decreases the dual
objective of the cluster-based LP relaxation at every
iteration. This monotone property makes it ideal for
adding clusters since we can initialize the new messages such that the dual value is monotonically decreased.
Another key advantage of working in the dual is that
the dual objective gives us a certificate of optimality.
Namely, if we find an assignment x such that f (x; θ) is
equal to the dual objective, we are guaranteed that x
is the MAP assignment (since the dual objective upper
bounds the MAP value). Indeed, using this property

k∈N (i)

#

"

1. Monotonically decreasing upper bound on MAP.
2. Choosing clusters which give a guaranteed bound
improvement.

xi

+

X
e∈E

max λe→e (xe ) +
xe

X

λc→e (xe )

c:e∈c

It should be noted, however, that not all λ are dual
feasible. Rather, λ needs to result from a reparameterization of the underlying potentials (see [3]). However,
it turns out that after updating all the MPLP messages
once, all subsequent λ will be dual feasible, regardless
of how λ is initialized.2
By LP duality, there exists a value of λ such that g(λ)
is equal to the optimum of the corresponding primal
LP. Although the MPLP updates decrease the objective at every iteration, they may converge to a λ that
is not dual optimal, as discussed in [3]. However, as
we will show in the experiments, our procedure often finds the exact MAP solution, and therefore also
achieves the primal optimum in these cases.
3.2

Choosing Clusters in the Dual LP
Relaxation

In this section we provide a very simple procedure that
allows adding clusters to MPLP, while satisfying the
2

In our experiments, we initialize all messages to zero.

• Edge to Node: For every edge ij ∈ E and node i (or j) in the edge:
λij→i (xi )← −

 1
hX
i
2  −j
λi (xi ) + θi (xi ) + max
λc→ij (xi , xj ) + λ−i
(x
)
+
θ
(x
,
x
)
+
θ
(x
)
j
ij
i
j
j
j
j
3
3 xj c:ij∈c

−j
where
λ−j
i (xi ) is the sum of edge-to-node messages into i that are not from edge ij, namely: λi (xi ) =
P
k∈N (i)\j λik→i (xi ).

• Edge to Edge: For every edge ij ∈ E:
λij→ij (xi , xj )← −

i
1h
2 X
−j
(x
)
+
λ
(x
)
+
θ
(x
,
x
)
+
θ
(x
)
+
θ
(x
)
λc→ij (xi , xj ) + λ−i
j
i
ij
i
j
i i
j
j
i
3 c:ij∈c
3 j

• Triplet to Edge: For every triplet c ∈ C and every edge e ∈ c:
λc→e (xe ) ←

−

 1
h X 
X
2
λe→e (xe ) +
λc0 →e (xe ) + max
λe0 →e0 (xe0 ) +
3
3 xc\e 0
0
e ∈c\e

c =
6 c
e ∈ c0

X

i
λc0 →e0 (xe0 )

c0 =
6 c
e0 ∈ c0

Figure 1: The generalized MPLP updates for an LP relaxation with three node clusters.
algorithmic properties in the beginning of Section 3.
Assume we have a set of triplet clusters C and now
wish to add a new triplet. Denote the messages before
adding the new triplet by λt . Two questions naturally
arise. The first is: assuming we decide to add a given
triplet, how do we set λt+1 such that the dual objective
retains its previous value g(λt ). The second question
is how to choose the new triplet to add.
The initialization problem is straightforward. Simply
set λt+1 to equal λt for all messages from triplets and
edges in the previous run, and set λt+1 for the messages from the new triplet to its edges to zero.3 This
clearly results in g(λt+1 ) = g(λt ).
In order to choose a good triplet, one strategy would be
to add different triplets and run MPLP until convergence to find the one that decreases the objective the
most. However, this may be computationally costly
and, as we show in the experiments, is not necessary.
Instead, the criterion we use is to consider the decrease
in value that results from just sending messages from
the triplet c to its edges (while keeping all other messages fixed).
The decrease in g(λ) resulting from such an update
has a simple form, as we show next. Assume we are
considering adding a triplet c. For every edge e ∈ c,
define be (xe ) to be
X
be (xe ) = λe→e (xe ) +
λc0 →e (xe ) ,
(3)
c0 :e∈c0
3

It is straightforward to show that λt+1 is dual feasible.

where the summation over clusters c0 does not include
c (those messages are initially zero). The decrease in
g(λ) corresponding to updating only messages from c
to the edges e ∈ c can be shown to be
"
d(c) =

X
e∈c

max be (xe ) − max
xe

xc

#
X

be (xe )

.

(4)

e∈c

The above corresponds to the difference between independently maximizing each edge and jointly maximizing over the three edges. Thus d(c) is a lower bound
on the improvement in the dual objective if we were
to add triplet c. Our algorithm will therefore add the
triplet c that maximizes d(c).
3.3

The Dual Algorithm

We now present the complete algorithm for adding
clusters and optimizing over them. Let C0 be the
predefined set of triplet clusters that we will consider
adding to our relaxation, and let CL be the initial relaxation consisting of only edge clusters (pairwise local
consistency).
1. Run MPLP until convergence using the CL clusters.
2. Find an integral solution x by locally maximizing
the
P single node beliefs bi (xi ), where bi (xi ) = θi (xi ) +
k∈N (i) λki→i (xi ). Ties are broken arbitrarily.
3. If the dual objective g(λt ) is sufficiently close to
the primal objective f (x; θ), terminate (since x is approximately the MAP).

4. Add the cluster c ∈ C0 with the largest guaranteed
bound improvement, d(c), to the relaxation.
5. Construct “warm start” messages λt+1 from λt .
6. Run MPLP for N iterations, and return to 2.
Note that we obtain (at least) the promised bound improvement d(c) within the first iteration of step 6. By
allowing MPLP to run for N iterations, the effect of
adding the cluster will be propagated throughout the
model, obtaining an additional decrease in the bound.
Since the MPLP updates correspond to coordinatedescent in the dual LP, every step of the algorithm
decreases the upper bound on the MAP. The monotonicity property holds even if MPLP does not converge in step 6, giving us the flexibility to choose the
number of iterations N . In Section 5 we show results
corresponding to two different choices of N .
In the case where we run MPLP to convergence before
choosing the next cluster, we can show that the greedy
bound minimization corresponds to a cutting-plane algorithm, as stated below.
Theorem 1. Given a dual optimal solution, if we find
a cluster for which we can guarantee a bound decrease,
all primal optimal solutions were inconsistent with respect to this cluster.
Proof. By duality both the dual optimum and the primal optimum will decrease. Suppose for contradiction
that in the previous iteration there was a primal feasible point that was cluster consistent and achieved the
LP optimum. Since we are maximizing the LP, after
adding the cluster consistency constraint, this point is
still feasible and the optimal value of the primal LP
will not change, giving our contradiction.
This theorem does not tell us how much the given
cluster consistency constraint was violated, and the
distinction remains that a typical cutting-plane algorithm would attempt to find the constraint which is
most violated.

4

Related Work

Since MPLP is closely related to the max-product generalized belief propagation (GBP) algorithm, our work
can be thought of as a region-pursuit method for GBP.
This is closely related to the work of Welling [16] who
suggested a region-pursuit method for sum-product
GBP. Similar to our work, he suggested greedily
adding from a candidate set of possible clusters. At
each iteration, the cluster that results in the largest
change in the GBP free energy is added. He showed
excellent results for 2D grids, but on fully connected
graphs the performance actually started deteriorating

with additional clusters. In [17], a heuristic related to
maxent normality [19] was used as a stopping criterion for region-pursuit to avoid this behavior. In our
work, in contrast, since we are working with the dual
function of the LP, we can guarantee monotonic improvement throughout the running of the algorithm.
Our work is also similar to Welling’s in that we focus
on criteria for determining the utility of adding a cluster, not on finding these clusters efficiently. We found
in our experiments that a simple enumeration over
small clusters proved extremely effective. For problems where triplet clusters alone would not suffice to
find the MAP, we could triangulate the graph and consider larger clusters. This approach is reminiscent of
the bounded join-graphs described in [1].
There is a large body of recent work describing the
relationship between message-passing algorithms such
as belief propagation, and LP relaxations [7, 15, 18].
Although we have focused here on using one particular message-passing algorithm, MPLP, we emphasize
that similar region-pursuit algorithms can be derived
for other message-passing algorithms as well. In particular, for all the convex max-product BP algorithms
described in [15], it is easy to design region-pursuit
methods. The main advantage of using MPLP is its
guaranteed decrease of the dual value at each iteration,
a guarantee that does not exist for general convex BP
algorithms.
Region-pursuit algorithms are also conceptually related to the question of message scheduling in BP, as
in the work of Elidan et al. [2]. One way to think
of region-pursuit is to consider a graph where all the
clusters are present all the time, but send and receive
non-informative messages. The question of which cluster to add to an approximation, is thus analogous to
the question of which message to update next.

5

Experiments

Due to the scalable nature of our message-passing algorithm, we can apply it to cases where standard LP
solvers cannot be applied to the primal LP (see also
[18]). Here we report applications to problems in computational biology and machine vision.4
We use the algorithm from Section 3.3 for all of our experiments. We first run MPLP with edge clusters until
convergence or for at most 1000 iterations, whichever
comes first. All of our experiments, except those intended to show the difference between schedules, use
N = 20 for the number of MPLP iterations run after
adding a cluster. While running MPLP we use the
messages to decode an integral solution, and compare
4

Graphical models for these are given in [18].

!1063.5
!1064

Objective

!1064.5
!1065
!1065.5
!1066

MPLP for 20 iterations
MPLP until convergence
MAP

!1066.5
!1067
!1067.5
1000

1500
2000
MPLP iterations

2500

Figure 2: Comparison of different schedules for adding
clusters to relaxation on a side-chain prediction problem.

the dual objective to the value of the integral solution.
If these are equal, we have found the MAP solution.5
Otherwise, we keep adding triplets.
Our results will show that we often find the MAP solution to these hard problems by using only a small number of triplet clusters. This indicates both that triplets
are sufficient for characterizing M(G) near the MAP
solution of these problems, and that our algorithm can
efficiently find the informative triplets.
5.1

Side-Chain Prediction

The side-chain prediction problem involves finding the
three-dimensional configuration of rotamers given the
backbone structure of a protein [18]. This problem
can be posed as finding the MAP configuration of a
pairwise model, and in [18] the TRBP algorithm [13]
was used to find the MAP solution for most of the
models studied. However, for 30 of the models, TRBP
could not find the MAP solution.
In earlier work [12] we used a cutting-plane algorithm
to solve these side-chain problems and found the MAP
solution for all 30 models. Here, we applied our dual
algorithm to the same 30 models and found that it also
results in the MAP solution for all of them (up to a
10−4 integrality gap). This required adding between
1 and 27 triplets per model. The running time was
between 1 minute and 1 hour to solve each problem,
with over half solved in under 9 minutes. On average
we added only 7 triplets (median was 4.5), another
indication of the relative ease with which these techniques can solve the side-chain prediction problem.
5
In practice, we terminate when the dual objective is
within 10−4 of the decoded assignment, so these are approximate MAP solutions. Note that the objective values
are significantly larger than this threshold.

We also used these models to study different update
schedules. One schedule (which gave the results in
the previous paragraph) was to first run a pairwise
model for 1000 iterations, and then alternate between
adding triplets and running MPLP for 20 more iterations. In the second schedule, we run MPLP to convergence after adding each triplet. Figure 2 shows the two
schedules for the side-chain protein ‘1gsk’, one of the
side-chain proteins which took us the longest to solve
(30 minutes). Running MPLP to convergence results
in a much larger number of overall MPLP iterations
compared to using only 20 iterations. This highlights
one of the advantages of our method: adding a new
cluster does not require solving the earlier problem to
convergence.
5.2

Protein Design

The protein design problem is the inverse of the protein
folding problem. Given a particular 3D shape, we wish
to find a sequence of amino-acids that will be as stable
as possible in that 3D shape. Typically this is done by
finding a set of amino-acids and rotamer configurations
that minimizes an approximate energy.
While the problem is quite different from side-chain
prediction, it can be solved using the same graph structure, as shown in [18]. The only difference is that now
the nodes do not just denote rotamers, but also the
identity of the amino-acid at that location. Thus, the
state-space here is significantly larger than in the sidechain prediction problem (up to 180 states per variable
for most variables).
In contrast to the side-chain prediction problems,
which are often easily solved by general purpose integer linear programming packages such as CPLEX’s
branch-and-cut algorithm [5], the sheer size of the
protein design problems immediately limits the techniques by which we can attempt to solve them. Algorithms such as our earlier cutting-plane algorithm [12]
or CPLEX’s branch-and-cut algorithm require solving
the primal LP relaxation at least once, but solving the
primal LP on all but the smallest of the design problems is intractable [18]. Branch and bound schemes
have been recently used in conjunction with a message passing algorithm [4] and applied to similar protein design problems, although not the ones we solve
here.
We applied our method to the 97 protein design problems described in [18], adding 5 triplets at a time to
the relaxation. The key striking result of these experiments is that our method found the exact MAP
configuration for all but one of the proteins6 (up to a
precision of 10−4 in the integrality gap). This is es6

We could not solve ‘1fpo’, the largest protein.

pecially impressive since, as reported in [18], only 2
of these problems were solvable using TRBP, and the
primal problem was too big for commercial LP solvers
such as CPLEX. For the problem where we did not
find the MAP, we did not reach a point where all the
triplets in the graph were included, since we ran out
of memory beforehand.
Among the problems that were solved exactly, the
mean running time was 9.7 hours with a maximum
of 11 days and a minimum of a few minutes. We
note again that most of these problems could not be
solved using LP solvers, and when LP solvers could
be used, they were typically at least 10 times slower
than message-passing algorithms similar to ours (see
[18] for detailed timing comparisons).
Note that the main computational burden in the algorithm is processing triplet messages. Since each variable has roughly 100 states, passing a triplet message
requires 106 operations. Thus the number of triplets
added is the key algorithmic complexity issue. For the
models that were solved exactly, the median number
of triplets added was 145 (min: 5, max: 735). As
mentioned earlier, for the unsolved model this number
grew until the machine’s memory was exhausted. We
believe however, that by optimizing our code for speed
and memory we will be able to accommodate a larger
number of triplets, and possibly solve the remaining
model as well. Our current code is written mostly in
Matlab, so significant optimization may be possible.
5.3

Stereo Vision

Given a stereo pair of images, the stereo problem is to
find the disparity of each pixel in a reference image.
This disparity can be straightforwardly translated into
depth from the camera. The best algorithms currently
known for the stereo problem are those that minimize
a global energy function [10], which is equivalent to
finding a MAP configuration in a pairwise model.
For our experiments we use the pairwise model described in [18], and apply our procedure to the
“Tsukuba” sequence from the standard Middlebury
stereo benchmark set [10], reduced in size to contain
116x154 pixels.
Since there are no connected triplets in the grid graph,
we use our method with square clusters. We calculate
the bound decrease using square clusters, but rather
than add them directly, we triangulate the cycle and
add two triplet clusters. This results in an equivalent
relaxation, but has the consequence that we may have
to wait until MPLP convergence to achieve the guaranteed bound improvement.
In the first experiment, we varied the parameters of the

5

9.237

x 10

9.236
9.235
9.234

Objective
Integer solution

9.233
9.232
9.231
9.23
9.229
1000

1050

1100

1150

1200

1250

1300

MPLP iterations

Figure 3: Dual objective and value of decoded integer
solution for one of the reduced “Tsukuba” stereo models,
as a function of MPLP iterations. It can be seen that both
curves converge to the same value, indicating that the MAP
solution was found.

energy function to create several different instances.
We tried to find the MAP using TRBP, resolving ties
using the methods proposed in [8]. In 4 out of 10
cases those methods failed. Using our algorithm, we
managed to find the MAP for all 4 cases.7
Figure 3 shows the dual objective and the decoded
integer solution after each MPLP iteration, for one set
of parameters.
In the results above, we added 20 squares at a time to
the relaxation. We next contrasted it with two strategies: one where we pick 20 random squares (not using our bound improvement criterion) and one where
we pick the single best square according to the bound
criterion. Figure 4 shows the resulting bound per iteration for one of the models. It can be seen that the
random method is much slower than the bound criterion based one, and that adding 20 squares at a time is
better than just one. We ended up adding 1060 squares
when adding 20 at a time, and 83 squares when adding
just one. Overall, adding 20 squares at a time turned
out to be faster.
We also tried running MPLP with all of the square
clusters.
Although fewer MPLP iterations were
needed, the cost of using all squares resulted in an
overall running time of about four times longer.

7

For one of these models, a few single node beliefs at
convergence were tied, and we used the junction tree algorithm to decode the tied nodes (see [8]).




Inference problems in graphical models are
often approximated by casting them as constrained optimization problems. Message
passing algorithms, such as belief propagation, have previously been suggested as methods for solving these optimization problems.
However, there are few convergence guarantees for such algorithms, and the algorithms
are therefore not guaranteed to solve the corresponding optimization problem. Here we
present an oriented tree decomposition algorithm that is guaranteed to converge to
the global optimum of the Tree-Reweighted
(TRW) variational problem. Our algorithm
performs local updates in the convex dual
of the TRW problem – an unconstrained
generalized geometric program. Primal updates, also local, correspond to oriented
reparametrization operations that leave the
distribution intact.

1

Introduction

The problem of probabilistic inference in graphical
models refers to the task of calculating marginal distributions or the most likely assignment variables. Both
these problems are generally NP hard, requiring approximate methods.
Many approximate inference methods, including message passing algorithms, can be viewed as trying to
solve a variational formulation of the inference problem. The idea in variational approaches is to cast approximate inference as a constrained minimization of
a free energy function (see [14] for a recent review).
Two key questions arise in this context. The first is
how to choose the free energy, and the second is how to
design efficient algorithms that minimize it. When the
Bethe free energy is used, it has been shown [16] that

Tommi Jaakkola
CSAIL
Massachusetts Institute of Technology
Cambridge, MA 02139

fixed-points of the belief propagation (BP) algorithm
correspond to local minima of the free energy. However, BP is not generally guaranteed to converge to a
fixed-point. Although there do exist algorithms that
are guaranteed to converge to a local minimum of the
Bethe free energy [15, 17], its global minimization is
still a hard non-convex problem for which no efficient
algorithms are known.
The difficulties with the Bethe free energy derive
from its non-convexity and corresponding local minima problem. To avoid this difficulty, several authors
have recently studied convex free energies [6, 7, 13].
The associated convex optimization problems can in
principle be solved using generic convex optimization
procedures [1] with guarantees of finding the global optimum in polynomial time. Although this presents a
significant improvement over the non-convex case, the
generic optimization route may be very costly in large
practical problems. For example, when using a generic
convex solver, every update of the variables has complexity O(n), where n is the number of variables. In
contrast, the optimization using message passing algorithms can be reduced to local updates with O(1)
operations. Interestingly, even in the convex setting,
the convergence of these message passing algorithms
is typically not guaranteed, and damping heuristics
are required to ensure convergence in practice [13]. A
prominent exception is [7] where the author provides
a provably convergent message passing algorithm for
free energies where the entropy term is a non-negative
combination of joint entropies.
Here we provide a provably convergent message passing algorithm for a specific variational setup, namely
the Tree-Reweighted (TRW) optimization problem of
Wainwright et al. [13]. The algorithm we propose is
guaranteed to converge to the global optimum of the
free energy, and does not require additional parameters such as the damping ratio. A key step in obtaining the updates is deriving the convex-dual of TRW,
which we show to be an unconstrained instance of a

134

GLOBERSON & JAAKKOLA
generalized geometric program (GP) [3]. We derive a
message passing algorithm, which we call TRW Geometric Programming (TRW-GP), that yields monotone improvement of the dual GP. We demonstrate the
utility of our TRW-GP algorithm by providing an example where the TRW message passing algorithm in
[13] does not converge, but TRW-GP does.

2

where H(Xi ) is the entropy of µi (xi ) and I(Xi ; Xj )
is the mutual information calculated from µij (xi , xj ).
Note that this expression is independent of the direction of the edges in the tree. We will make use of the
directed edges in the next section.
Define the following variational free energy function
F (µ; ρ, θ)

The Tree-Reweighting Formulation

We consider pairwise Markov random fields (MRF)
over a set of variables x = x1 , . . . , xn . Given a graph
G with n vertices V and a set of edges E, an MRF is
a distribution over x defined by
p(x; θ) =

1 Pij∈E θij (xi ,xj )+Pi∈V
e
Z(θ)

θi (xi )

Our focus here is on approximating singleton
marginals of p(x; θ), namely p(xi ; θ). This problem is
closely related to that of evaluating the partition function Z(θ). We focus on the TRW variational problem
which yields an upper bound on Z(θ) as well as a set of
approximate marginals obtained from the minimizing
solution.
We begin by briefly reviewing the TRW formalism.
Consider a set of k spanning trees on G denoted by
T 1 , . . . , Tk , P
and a distribution ρi over these trees where
ρi ≥ 0 and ρi = 1. To avoid overloading notation in
subsequent analysis, we assume here that the trees are
directed, so that the same tree structure may appear
multiple times with different edge orientations. This
differs from the presentation in [13] though the distinction is immaterial in the remainder of this section.
We also introduce the notion of pseudomarginals
defined as the singleton and pairwise marginals
µi (xi ), µij (xi , xj ) associated with the edges and nodes
of G. We use µ to denote the set of all these marginals
and C(G) the set of µ’s that are pairwise consistent
P
P
µij (xi , xj ) = µi (xi ) ,
µij (xi , xj ) = µj (xj )
xj
xi
P
µi (xi ) = 1 , µij (xi , xj ) ≥ 0 .
xi

For a given tree T and µ ∈ C(G), define the entropy
H(µ; T ) to be the entropy of an MRF on the tree T
with marginals given by µ. Note that only a subset
of the pairwise distributions in µ will be used for each
tree, namely µij (xi , xj ) such that ij is an edge in T .
The tree entropy may be written in closed form as (cf.
[13])
X
X
H(µ; T ) =
H(Xi ) −
I(Xi ; Xj )
(2)
ij∈T

k
X

ρi H(µ; Ti ) .

(3)

i=1

In [13] it is shown that minimizing F (µ; ρ, θ) results
in an upper bound on the log-partition function
log Z(θ) ≤ − min F (µ; ρ, θ) .
µ∈C(G)

(1)

where θij (xi , xj ) and θi (xi ) are parameters, θ denotes
all the parameters, and Z(θ) is the partition function.

i

F (µ; ρ, θ) = −µ · θ −

(4)

The minimization also results in an optimal (minimizing) µ, which is used to approximate the marginals
of p(x; θ). Empirical results in [13] show that TRW
usually performs as well as, and often better than the
standard Bethe free energy approximations, especially
in regimes where BP fails to converge.

3

Conditional Entropies and Directed
Edge Probabilities

Our goal is to use convex duality to obtain the dual
problem of Eq. (4). To achieve this, we first seek a
representation of F (µ; ρ, θ) that is a convex function of
µ for all values of µ, and not just within the consistent
set µ ∈ C(G). For example, the entropy term in Eq. (2)
is concave only for µ ∈ C(G) but not for a general µ.
We therefore seek an alternative expression for the tree
entropy.
Let r(T ) be the root node of T (recall that the trees
are directed). We write the entropy associated with
the tree as
X
H(µ; T ) = H(Xr(T ) ) +
H(Xi |Xj )
(5)
j→i∈T

where j → i ∈ T implies that there is a directed edge
from vertex j to vertex i in the directed tree T . The
conditional entropy H(Xi |Xj ) is assumed to be calculated only on the basis of the joint marginal µij (xi , xj ),
and does not involve µi (xi ). The entropy H(Xr(T ) )
is calculated via the singleton marginal µr(T ) (xr(T ) ).
The expressions in Eq. (5) and Eq. (2) will agree whenever µ ∈ C(G). However, they will yield different results when µ ∈
/ C(G).
The advantage of Eq. (5) is that H(µ; T ) is now a concave function of the set of marginals µ. The concavity
follows immediately from the concavity of H(Xi ) as a
function of µi (xi ) and the concavity of the conditional
entropy H(Xi |Xj ) as a function of µij (xi , xj ) [6].

GLOBERSON & JAAKKOLA
The function F (µ; ρ, θ) involves a summation over a
potentially large number of tree entropies. To express
this compactly while maintaining directionality, we define ρi|j as the probability that the directed edge j → i
is present in a tree drawn according to the distribution
ρ over trees. Similarly, we define ρ◦i as the probability that node i appears as a root. We note that it
is possible to find such edge probabilities for distributions (e.g. uniform) over the set of all spanning trees
by employing a variant of the matrix tree theorem for
directed trees (see [12] p. 141 and [11]).
The function F (µ; ρ, θ) can now be written as
−µ · θ −

X

i∈V

ρ◦i H(Xi ) −

X

ij∈Ē

ρi|j H(Xi |Xj )

now on) and refer to the consistency constraints by
~
C(G).
The TRW primal problem is then
P T RW :

min F (µ; ρ, θ) .

(6)

The TRW Convex Dual

(8)

~
µ∈C(G)

The convex dual of P T RW is derived in App. A. and
is in fact a convex unconstrained minimization problem. In what follows we describe this dual. The dual
variables will be denoted by βij (xi , xj ) for ij ∈ E, and
are not constrained.2 The dual objective is given by
X
X ρ−1 (θ (x )−P
k∈N (i) λk|i (xi ;β))
FD (β; ρ, θ) =
ρ◦i log
e ◦i i i
xi

i

where the edge set Ē contains edges in both directions.
In other words, if ij ∈ Ē then ji is also in Ē. The new
function F (µ; ρ, θ) is convex in µ without assuming
consistency of the marginals.

4

135

where λj|i (xi ; β) is a function of the β variables:
λj|i (xi ; β) = −ρj|i log

X

e

ρ−1
(θij (xi ,xj )+δj|i βij (xi ,xj ))
j|i

xj

and δj|i is defined as

1 ji ∈ E
δj|i =
−1 ij ∈ E .
The dual TRW optimization problem is then

The TRW primal problem is given by
min F (µ; ρ, θ) .

µ∈C(G)

DT RW :
(7)

Since the function F (µ; ρ, θ) is now convex for all µ
and the set of constraints is linear, this optimization
problem is convex and thus has an equivalent convex
dual [1].1 However, it is not immediately clear how
to derive this dual in closed form. The main difficulty is that two terms in the objective F (µ; ρ, θ) depend on µij (xi , xj ), namely H(Xi |Xj ) and H(Xj |Xi ).
To get around this problem we introduce additional
variables to the primal problem. Specifically, we replace µij (xi , xj ) by two copies which we denote by
µi|j (xi , xj ) and µj|i (xi , xj ), and require that these
two copies are identical. The entropy H(Xi |Xj ) is
then evaluated via the variables µi|j (xi , xj ). We shall
also find it convenient to replace the consistency constraints in C(G) by the following equivalent directed
consistency constraints
µi|j (xi , xj ) = µj|i (xi , xj )
P
P
µj|i (xi , xj ) = µi (xi ) ,
µi|j (xi , xj ) = µj (xj )
xj
xi
P
µi (xi ) = 1 , µi|j (xi , xj ) ≥ 0 , µj|i (xi , xj ) ≥ 0 .
xi

For simplicity we will continue to denote the new extended variable set by µ (as we will be using it from
1
Strict duality follows from Slater’s conditions, which
are satisfied in this case.

min FD (β; ρ, θ) .
β

(9)

We re-emphasize the fact the DTRW is an unconstrained minimization of a function of β. The variables λj|i (xi ; β) are introduced merely for the purpose
of notational convenience. The mapping between dual
and primal variables can be shown to be
−1

µi (xi ) ∝ eρ◦i (θi (xi )−
µj|i (xj |xi ) ∝ e

P

k∈N (i)

λk|i (xi ;β))

ρ−1
(θij (xi ,xj )+δj|i βij (xi ,xj ))
j|i

.

(10)

This relation maps the optimal β to the optimal µ,
but we shall also use it for non-optimal values.
The dual objective FD (β; ρ, θ) is a convex function
(see App. B) and therefore has no local minima.

5

Dual Gradient and Optimum

The DTRW problem presented above is unconstrained
and can thus be solved using a variety of gradient
based algorithms, such as conjugate gradient or BFGS
[10]. The gradient of FD (β; ρ, θ) w.r.t. β is
∂FD (β; ρ, θ)
= µi|j (xi |xj )µj (xj ) − µj|i (xj |xi )µi (xi )
∂βij (xi , xj )
where the distributions are given by the dual to primal
mapping in Eq. (10). The gradient is thus a measure
2
Note that β variables are not directed, i.e., there is one
variable βij per edge.

136

GLOBERSON & JAAKKOLA
of the discrepancy between two ways of calculating
the joint pairwise marginal, based on the two different
orientations of the edge ij.
To characterize the optimum of DTRW we set the gradient to zero, yielding the following simple dual optimality criterion
µi|j (xi |xj )µj (xj ) = µj|i (xj |xi )µi (xi ) .

(11)

Thus at the optimum the two alternative ways of estimating µij (xi , xj ) will yield the same result.
Calculating the gradient w.r.t a given βij (xi , xj ) has
complexity O(1), and relies only on βij (xi , xj ) for
edges containing i or j. Thus the gradient can be calculated locally, and gradient descent algorithms can
be implemented efficiently. One drawback of gradient based algorithms is their reliance on line-search
modules for finding a step size that decreases the objective. In the next section we consider updates that
are parameter-free.

6

Local Marginal Updates

The gradient updates described in the previous section
use the difference between two joint distributions. We
will now focus on updates relying on the ratio between
these distributions. Consider
t+1
t
βij
(xi , xj ) = βij
(xi , xj )+ǫ log

µtj|i (xj |xi )µti (xi )

µti|j (xi |xj )µtj (xj )

(12)

where µtj|i (xj |xi ) and µti (xi ) are functions of β as in
Eq. (10) and ǫ is a step size whose value will be discussed in the next section. As a ratio of two expected
values, the update is reminiscent of Generalized Iterative Scaling [5]. We shall assume for simplicity that
only one edge is updated at each time step t.

Lemma 6.1 : For 0 < ǫ < min(ρ◦i , ρ◦j , ρi|j , ρj|i ) the
dual objective is decreased at every iteration so that
∆D (µt ) ≥ 0 for all t. Furthermore, ∆D (µt ) = 0 holds
if and only if the optimum condition of Eq. (11) is
satisfied.
Any choice of ǫ that is smaller than
min(ρ◦i , ρ◦j , ρi|j , ρj|i ) will result in monotone improvement of the objective. In the current implementation we use ǫ = 12 min(ρ◦i , ρ◦j , ρi|j , ρj|i ). This value
turns out to minimize a first order approximation of
the improvement in the objective, and was found to
work well in practice. The convergence to the global
optimum now follows from Lemma 6.1.
Lemma 6.2 : The updates in Eq. (12) with ǫ as in
Lemma 6.1 converge to the joint optimum of PTRW
and DTRW.
Proof: Denote the mapping from µt to µt+1 by
R(µt ) = µt+1 . The mapping is clearly continuous.
By Lemma 6.1 the sequence FD (β t ; ρ, θ) is monotonically decreasing. It is also bounded since FD (β; ρ, θ)
is bounded and thus the difference series ∆D (µt ) converges to zero. Taking t to infinity then implies that
µt has a convergent subsequence that converges to
some µ∗ . This µ∗ will then satisfy FD (µ∗ ; ρ, θ) =
FD (R(µ∗ ); ρ, θ). We know from the Lemma 6.1 that
such a point necessarily satisfies the zero gradient condition in Eq. (11), and thus µ∗ (or more precisely, the
corresponding β) minimizes the dual objective.3

7

Tree Re-parametrization View

The TRW problem can be interpreted in terms of iterating through different re-parametrizations of the distribution p(x; θ) [13]. Here we present a related view
of our algorithm.

The update in Eq. (12) is performed on the β variables.
An equivalent, and somewhat simpler update may be
derived in terms of the variables µti|j (xi |xj ) and µtj (xj ).
The resulting updates and algorithm are described in
Figure 1. We call the resulting algorithm TRW-GP
(TRW Geometric Programming).

We wish to show that the marginal variables obtained
by the algorithm can always be used to obtain the
original distribution via
Y
Y
µtj|i (xj |xi )ρj|i . (13)
p(x; θ) = ct
µti (xi )ρ◦i

6.1

For t = 0 this is clearly true. We proceed by induction.
Assume that at iteration t we have a reparametrization with constant ct . Substituting the update rule in
Figure 1 and using simple algebra shows that we again
have a reparametrization, only with

Convergence Proof

To analyze the convergence of the update in the previous section, we need to consider the resulting change
in the objective FD (β; ρ, θ), namely FD (β t ; ρ, θ) −
FD (β t+1 ; ρ, θ). It can be shown (see App. D) that
this difference only depends on the µ variables in the
TRW-GP algorithm, and thus we denote it by ∆D (µt ).
Since FD (β t ; ρ, θ) should be minimized, this difference
needs to be non-negative. This is indeed guaranteed
by the following lemma (see App. D):

i

ct+1 = ct eFD (β

ij∈Ē

t+1

;ρ,θ)−FD (β t ;ρ,θ)

= ct e−∆D (µ

t

)

.

3
To carefully account for the possibility that some of
the converging marginals would involve zero probabilities,
the updates in the primal form, along with the objective,
can be written in a form without any ratios.

GLOBERSON & JAAKKOLA

137

Inputs: A graph G = (E, V ), parameter vector θ on G, root probabilities ρ◦i and directed edge probabilities
ρi|j for (ij), (ji) ∈ E.
−1

Initialization: Set µ0i (xi ) ∝ eρ◦i θi (xi ) and µ0i|j (xi |xj ) ∝ eρi|j θij (xi ,xj )
−1

Algorithm: Iterate until small enough change in marginals:
• Set ǫ =

1
2

min(ρ◦i , ρ◦j , ρi|j , ρj|i ), and update

µt+1
(xi ) ∝ µti (xi )
i
µt+1
i|j (xi |xj )

∝

P
xj



µtj|i (xj |xi )

−1
µti|j (xi |xj )1−ǫρi|j



!ρj|i ρ◦i
ǫρ−1
j|i

−1

µti|j (xi |xj )µtj (xj )
µtj|i (xj |xi )µti (xi )

µtj|i (xj |xi )µti (xi )
µtj (xj )

ǫρ−1
i|j

Output: Final values of marginals.
Figure 1: The TRW-GP algorithm expressed in terms of conditional and singleton marginals.
In other words the multiplicative constant turns
out to be related to the improvement in the dual
function. This creates an interesting link between
reparametrization and minimization, and may be used
to study message passing algorithms where a dual is
more difficult to characterize.

8

Relation to Previous Work

Heskes [7] recently presented a detailed study of convex free energies. When the entropy term is a positive
combination of joint and singleton entropies (and is
therefore concave), he provides a local update algorithm that is monotone in the convex dual, and converges to the global optimum. He then discusses the
application of the same algorithm to the case where the
singleton entropies all have negative weight, and the
overall entropy is convex over the set of constraints.4
In this case, the dual is generally not given in closed
form and it is not known if the algorithm decreases it
at every step. However, Heskes argues that with sufficient damping the algorithm can be shown to converge,
although the exact form of damping is not given.
Since the TRW entropy can be shown to decompose
into positively weighted pairwise entropies and negatively weighted singleton entropies, it satisfies the
above condition in Heskes’ work. Our analysis provides several advantages over the algorithm in [7].
First, we derive a closed form solution of dual. Second, the dual is unconstrained, and thus allows unconstrained minimization methods to be applied. Third,
unlike most belief propagation variants, our algorithm
4

The discussion in [7] is in terms of general regions,
not just pairs. We present his argument for the simpler
pairwise case.

is shown to provide a monotone improvement of an
objective function5 , and thus diverges from the standard fixed point analysis used in message passing algorithms.
Finally, another algorithm which is guaranteed to converge to a global minimum of convexified free energies
is the double loop CCCP algorithm of Yuille [17]. The
main disadvantage of CCCP is that each iteration requires solving an optimization problem. This usually
results in slower convergence, and furthermore it is
not clear what precision is required for the inner loop
optimization, and how this affects convergence guarantees. The algorithm we present here is essentially a
single loop method, and is thus easier to analyze.

9

Empirical Demonstration

The original TRW message passing (TRW-MP) algorithm presented in [13] is not generally guaranteed to
converge. However, we observed empirically that when
damping of α = 0.5 is applied to the log-messages,
convergence is always achieved.6 To compare TRWMP to TRW-GP, we use the pseudomarginals generated by TRW-MP7 as marginals in the primal objective F (µ; ρ, θ) in Eq. (3). This value is not expected
to be an upper or lower bound on the optimum of
F (µ; ρ, θ), since the TRW-MP pseudomarginals are
5

As mentioned above, Heskes presents such an algorithm for positively weighted singleton and pairwise entropies. It is however not clear that such entropies are
useful in practice
6
This observation is in line with Heskes’ argument that
sufficiently damped messages will converge for the case of
the TRW free energy.
7
See Equations (58) and (59) in [13].

138

GLOBERSON & JAAKKOLA
TRW−GP
TRW−MP
TRW−MP(damped)

880

870

860

850
0

100

Iteration

200

300

140

Primal−Dual Value

Primal−Dual Value

890

TRW−GP
TRW−MP
TRW−MP(damped)

135

130

125

120
0

100

Iteration

200

300

Figure 2:

Illustration of the dual message passing algorithm for a 10 × 10 Ising model. The TRW-GP curve
shows the dual objective value FD (β; ρ, θ) obtained by the
TRW-GP algorithm. The TRW-MP curves show the primal objective values F(µ; ρ, θ) obtained by TRW message
passing algorithms. The damped TRW-MP used a damping of 0.5 in the log domain. The MRF parameters were
set as follows: αF = 1, αI = 9 for the left figure, and
αF = 1, αI = 1 for the right figure.

not guaranteed to be pairwise consistent, except at
the optimum. However, since the TRW-MP pseudomarginals converge to the optimal primal marginals,
the value F (µ; ρ, θ) will converge to the primal optimum. The progress of TRW-GP may be monitored by
evaluating FD (β; ρ, θ) at every iteration. This value is
guaranteed to decrease and converge to the optimum
of FD (β; ρ, θ) which is identical to the optimum of
F (µ; ρ, θ). We can thus observe the rate at which the
different algorithms converge to their joint optimum.
To study the convergence rate of the two algorithms, we used an Ising model on a 10 × 10 grid
with interaction parameters θij drawn uniformly from
[−αI , αI ] and field parameters θi drawn uniformly
from
[−αF , αFP]. The MRF is given by p(x; θ) ∝
P
θij xi xj + i∈V θi xi
ij∈E
e
where xi ∈ {+1, −1}. We
used a uniform distribution over directed spanning
trees calculated as in [11].
Figure 2 (left) shows an example run where the undamped TRW-MP algorithm does not converge, but
the TRW-GP and the damped TRW-MP do converge,
and do so roughly at the same rate. Figure 2 (right)
shows an example where both TRW-MP algorithms
converge and do so at a faster rate than TRW-GP.
We experimented with various values of αF and αI
and have observed that at lower interaction levels (e.g.,
αI ≤ 4 for αF = 1) the TRW-MP algorithms outperform TRW-GP, whereas for higher interaction levels
the undamped TRW-MP does not converge, but the
damped version converges at roughly the same rate as
TRW-GP. We also experimented with conjugate gradient minimization of FD (β; ρ, θ), but these did not
yield better rates than TRW-GP.

10

Conclusions

We presented a novel message passing algorithm whose
updates yield a monotone improvement on the dual of
the TRW free energy minimization problem. In order
to obtain a closed form dual we used two tricks. The
first was to decouple different entropies that depend on
the same marginals by introducing multiple copies of
these marginals. The second was to use uni-directional
consistency constraints, so that every copy of a joint
marginal appears in a single consistency constraint.
Although we presented the method in the context of
tree decompositions, the algorithm itself still applies
as long as ρi|j and ρ◦i are non-negative (although the
upper bound on the log partition function may not be
guaranteed in this case).
The TRW-GP algorithm resolves the convergence
problems with the undamped TRW-MP algorithm.
However, we observed empirically that the damped
TRW-MP algorithm always converges, and typically at
a better rate than TRW-GP. Thus, the main contribution of the current paper is in introducing a dual framework for message passing algorithms, which could be
used to analyze existing algorithms, and possibly develop faster variants in the future.
Free energies may be defined using marginals of more
than two variables [13, 16]. In a recent paper [6]
we study the relation between such free energies and
GP. It will be worthwhile to study generalizations of
TRW-GP to this case. Another interesting extension is
to the MAP problem, where the corresponding variational problem is a linear program. Global convergence
results for MAP message passing algorithms such as
max-product are also hard to obtain in the general
case. It turns out that an approach similar to the one
presented here may be used to obtained convergent
algorithms to solve the MAP linear program. These
algorithms will be presented elsewhere.



Discriminative linear models are a popular
tool in machine learning. These can be generally divided into two types: linear classifiers,
such as support vector machines (SVMs),
which are well studied and provide stateof-the-art results, and probabilistic models
such as logistic regression. One shortcoming of SVMs is that their output (known
as the ”margin”) is not calibrated, so that
it is difficult to incorporate such models as
components of larger systems. This problem is solved in the probabilistic approach.
We combine these two approaches above
by constructing a model which is both linear in the model parameters and probabilistic, thus allowing maximum margin training with calibrated outputs. Our model assumes that classes correspond to linear subspaces (rather than to half spaces), a view
which is closely related to concepts in quantum detection theory. The corresponding
optimization problems are semidefinite programs which can be solved efficiently. We illustrate the performance of our algorithm on
real world datasets, and show that it outperforms second-order kernel methods.

1

Introduction

Support Vector Machines (SVM) [12] are commonly
regarded as the state of the art in supervised learning.
One of their key advantages is their maximization of
the margin, a property which also guarantees certain
generalization bounds [1]. A different class of supervised learning algorithms is the one based on probabilistic models, such as logistic regression [9]. Such
models are parameterizations of the class conditional
distributions, which are trained to maximize the like-

Amir Globerson
CSAIL
Massachusetts Institute of Technology
Cambridge, MA 02139

lihood of the observed data. One advantage of probabilistic models is that the probabilities they generate
may be used as a calibrated measure of certainty about
class prediction. Such a measure may be used in systems which incorporate classifiers as modules, and is
generally useful in balancing different types of errors.
The confidence measure in SVMs is commonly taken
to be the margin of an example, which is a geometric quantity and is not naturally calibrated or even
bounded.
While there have been previous attempts on assigning
probabilistic outputs to SVMs [11], they have been
based on transforming the margin into the [0, 1] range,
and not on a complete probabilistic model.
Another integral property of SVMs is of course the
half-space structure of classes (in the binary case). An
equivalent statement is that SVMs assume there is a
transformation of inputs into the real line such that
positive and negative points correspond to different
classes. Moreover, by using kernels, linear separation
need only be assumed for a nonlinear transformation
of the variables. However, geometric intuition is often
lost as a result of the kernel transformation, and the
resulting separators are not easily interpretable.
In this work, we present a different view of class separation, which incorporates both the concepts of margin maximization and probabilistic modeling. Our approach assumes that classes correspond to orthogonal
linear subspaces in feature space. This assumption is
reasonable in many domains where the existence or
absence of a feature is the key predictor of its class
identity, rather than its exact value or its relation to
values of other features. For example, in document
classification there may be subsets of words (or linear
combinations of word counts) whose appearance indicates the document topic. In image classification, a
set of pixels may be indicative of image content regardless of their exact intensity ratios. An alternative
statement of the problem is that there exists a linear transformation of feature space such that a unique

subset of coordinates is active in each class.
In order to measure the degree to which a given input
point belongs to a given subspace we use a projection
operator which measures what fraction of the point’s
norm lies in a subspace. Such projection operators
correspond to matrices with eigenvalues in the discrete set {0, 1}. We relax this assumption to the [0, 1]
range, which makes the model tractable. It also turns
out that the output of the projection operators have a
natural interpretation as probabilities, and these probabilities are linear functions of the model parameters
(the projection matrices).
Because the model is both a linear and a probabilistic
model, we can efficiently implement both methods that
rely on margin maximization, and those that maximize
probability related measures such as log likelihood or
optimal Bayes errors. All these problems are convex
and two of them are Semidefinite Programs (SDP) [16]
for which efficient algorithms exist.
Our model is closely related to ideas in quantum detection and estimation, where semidefinite matrices are
used to generate probabilities. A simple view is that
the class conditional models are represented by SDP
matrices with a unit trace and the detectors are represented using PSD matrices.
We compare the performance of our method to the
closely related second order kernels SVM, and show
that it achieves improved performance on a handwritten digit classification task, while providing meaningful probabilistic output.

2

The Probabilistic Model

Consider a classification task where x ∈ Rd are the feature vectors, and classes are y ∈ {1, . . . , k}. Denote the
classification rule by f (x) = y. We assume that classes
reside in linear subspaces Sy (i.e., x ∈ Sy ⇔ f (x) = y)
such that Si ∩ Sj = {0} (∀i 6= j), Si and Sj are orthogonal spaces, and the spaces Sy span the entire space:
S1 ⊕ S2 . . . ⊕ Sk = Rd . This corresponds to the assumption that there exists a linear transformation in
feature space such that a subset of coordinates is active
for a given class, and these coordinates are mutually
exclusive.
The projection operator on the space Sy is a matrix Ay
such that Ay is idempotent (thus, for every x ∈ Sy we
have Ay x = x) and symmetric (A2y = Ay ,Ay = ATy ).
This implies that if x ∈ Sy then kAy xk2 = kxk2 , and
if x ∈
/ Sy then kAy xk2 ≤ kxk2 . The above suggests
that kAy xk2 may be taken as a measure of the degree
to which x belongs to class y.
Now, note that kAy xk2 = xT Ay x so that this measure

is in fact a quadratic function of x, and importantly is
linear in Ay .
Since we are interested in the multiclass setting, it is
natural to define kAy xk2 as the probability of class y
given the point x:
p(y|x) =

xT (

1
P

y

Ay )x

xT A y x .

(1)

This implies the probability is invariant to the norm
of x and we can thus always normalize x such that
kxk2 = 1.
The normalization factor in Eq. (1) makes the probability a nonlinear function of Ay . However, our assumption on
P the structure of the classes in fact implies that
y Ay = I. To see this, denote the orthogonal basis of Sy by Vy . The
S assumption about
the structure of Sy implies that y Vy = {v1 , . . . , vd }
yields an orthogonal basis of the entire space. Denote
by
matrix whose columns are {vi }ni=1 . Then
P V the P
T
T
= I by the assumption of
i vi vi = V V
y Ay =
orthogonality and the fact that Vy is orthogonal to Vy0
for y 6= y 0 .
P
Since y Ay = I the probabilistic model of Eq. (1)
reduces to
p(y|x) = xT Ay x .

(2)

Optimization over the set of idempotent matrices is
an integer optimization problem, which seems to be
hard to solve. We therefore relax this assumption,
and only constrain
Ay to be positive semidefinite, and
P
to satisfy
A
=
I. These two constraints imply
y y
that the eigenvalues of Ay lie between zero and one.
Since idempotent matrices are characterized by eigenvalues in λ ∈ {0, 1}, we can interpret our relaxation as
relaxing this eigenvalues constraint by the constraint
λ ∈ [0, 1] (see e.g. [14]).

3

The Learning Problem

We now turn to the problem of learning a classifier
using the probabilistic model defined in Eq. (2). Given
a labeled sample (xi , yi ), i = 1, . . . , n, we seek a set of
parameters Ay which result in a good classifier. Below
we present two approaches to this problem. The first
is related to margin based methods, and the second to
likelihood based ones.
3.1

Margin based approaches

A desired property in a classifier is that the probability
it assigns to the correct class is higher that those assigned to incorrect classes. In other words, we wish to

maximize the margin between the correct probability
p(yi |xi ) and the incorrect ones p(z|xi ) where z 6= yi .
Define the margin of a point xi by,
mi = p(yi |xi ) − max p(z|xi ) .
z6=yi

Then, as in other margin based classifiers, we wish
to maximize the minimum margin. In the separable
case (i.e., there exists a classifier such that all margins
on the training set are positive), the margin maximization problem is given by the following semidefinite program:
max
s.t

η
p(y
P i |xi ) − p(z|xi ) ≥ η
y Ay = I
Ay º 0

∀i,

z 6= yi

If the data is not separable, we add a slack variable ξi
for each sample point
P
max η − β i ξi
s.t
p(y
P i |xi ) − p(z|xi ) ≥ η − ξi ∀i, z 6= yi (3)
y Ay = I
Ay º 0, ξi ≥ 0
where β ≥ 0 is a tradeoff parameter.
We call this method the MaxMargin approach since it
seeks a maximum margin model.
3.2

Likelihood based approaches

Since our model is a parametric family of distributions,
one way to optimize it is via standard maximum likelihood. This yields the following optimization problem
P
max Pi log p(yi |xi )
s.t
(4)
y Ay = I
Ay º 0
Note that since p(yi |xi ) is a linear function of the parameters, its log is concave, and the optimization problem is thus concave, although it is not a standard SDP,
since the objective is nonlinear. We do not study this
approach further in this manuscript, since we prefer to
focus on problems for which standard solvers exist.
A related approach is obtained if we consider the measure of success of the predictor to be the probability it
assigns to the correct class. This view implies that we
should perform the following maximization
P
max Pi p(yi |xi )
s.t
(5)
y Ay = I
Ay º 0
This optimization is very similar to the maximum likelihood one, but without the log function. The objective can also be viewed as the optimal Bayes loss in

prediction given that the true distribution is p(y|x).
We therefore denote this optimization by Bayes.
Note that for logistic models such as Conditional Random Field (CRF) [9], this problem is not convex since
CRF probabilities are not convex functions of the parameters. Interestingly, this problem may be solved
analytically for the binary case as we now show.
Denote the two matrices by A1 and A2 = I − A1 .
The constraints imply that the eigenvalues of A1 are
between zero and one. The objective function then
becomes,
X
X
tr(A1 xi xTi ) +
tr((I − A1 )xi xTi ) .
i:yi =1

i:yi =2

Omitting the constant term which does not affect the
solution we get,

 
X
X
xi xTi  .
tr A1 
xi xTi −
i:yi =1

i:yi =2

Let vi and λi be the eigenvectors and the eigenvalues
of the constant matrix,
X
X
xi xTi −
xi xTi .
i:yi =1

i:yi =2

Since the objective is linear in the matrix A1 we get
that A1 has the same eigenvectors vi . Let di be the
eigenvalues of A1 . We have that the objective function
is given by

 
X
X
X
λ i di .
xi xTi  =
tr A1 
xi xTi −
i:yi =1

i:yi =2

i

Therefore, to maximize the objective function one
should set di = sgn(λi ), where we define sgn(0) = 0.5.
To conclude, we showed that the solution of Eq. (5)
for two classes can be obtained by computing the difference between the (normalized) covariances matrices
per class, and assigning each of the eigenvectors to
one of the matrices Ay in accordance with the sign of
the corresponding eigenvalue. A similar algorithm was
proposed in the context of quantum detection theory
where more information is assumed. See the book of
Helstrom [8] for more details.

4

Convex bounds on the zero-one loss

A common approach to choosing an optimal classifier is to find the one which minimizes a convex upper
bound on the zero one loss. In conditional log-linear
models [9], the function − log2 p(y|x) is such a convex
upper bound (convex in the model parameters). In

7

For β = 1 the factor in the parenthesis may be interpreted as a lower bound on the probability of correct classification. Thus the max-margin method may
be viewed as optimizing a (multiplicative) tradeoff between correct classification and margin maximization.
Different values of β reflect the weight that should be
attributed to classification rate compared to margin.

Zero−One
ML
Bayes
Margin

6

Convex Bound

5

4

3

2

5

Duality

1

0
0

0.1

0.2

0.3

0.4

0.5

p(y=1|x)

0.6

0.7

0.8

0.9

1

Figure 1: Convex upper bounds on the zero one loss in
the binary case. Curves are shown for the case where
y = 1 is the correct class. For the margin bound, a
value of η = 0.2 is used.
Support Vector Machines [12] the hinge loss serves as
a bound.

lzo (x, y, p) = Θ[p(y|x) − 0.5]

(6)

To illustrate the bounds in our models, we focus on the
binary class case. Figure 1 shows the bounds discussed
below and their relation to the zero-one loss.
The simplest linear upper bound on the zero-one loss
is (see Figure 1)
lBayes (x, y, p) = 2(1 − p(y|x))

(7)

As its name suggests, it is minimized by the Bayes
optimization problem in Eq. (5).
The ML problem in Eq. (4) corresponds to minimizing
the bound (see Figure 1)
lML (x, y, p) = − log2 (p(y|x))

(8)

The interpretation of the maximum margin formulation is slightly more complex. Consider the function
1 2
− p(y|x)}
η η

(9)

The function lMarg is also an upper bound on the zeroone loss, as can be seen in Figure 1 and is similar to
the hinge loss, with the exception that the former is
parameterized by η.
The objective in Eq. (3) in the binary case can then
be written as a sum of elements
η(1 − βlMarg (xi , yi , p, η))

Standard duality transformation yields the dual
semidefinite program
min
s.t.

The zero-one loss is given by

lMarg (x, y, p, η) = max{0, 1 +

As in the case of SVM, convex duality may be used to
gain important insights into the problem. We obtain
the convex dual of Eq. (3) by introducing two sets
of dual parameters. The first (corresponding to the
normalization constraint) is λ, a matrix of size d × d.
The second (corresponding to the margin constraints)
is qyi (y = 1, . . . , k and i = 1, . . . , n) where we force
qyi i = 0.

(10)

−tr(λ)
P
q =1
Pi,z zi
q
Pz zi ≤ β ∀iT = 1,
P. . . , n P
T
q
x
x
−
i:yi 6=y yi i i
i:yi =y (
z qzi )xi xi º λ

(11)

where the last constraint is true for all y.
We can use the dual to further interpret the meaning
of the β parameter. Assume that β = 1/(νn) where n
is the number of examples and ν ∈ [0, 1]. We say that
the ith example is not a support vector if the solution
of Eq. (11) satisfies qzi = 0 for all z. Intuitively, an
example which is not a support vector does not change
the solution of the optimization problem and thus can
be omitted, without affecting the solution. Note that
this definition is somewhat weaker than the standard
definition in support vector machines, since we do not
have a representer theorem that links the primal and
dual solutions. We also say that the ith example is a
margin error if ξi > 0. The following lemma links
the value of ν to both margin errors and the number
of support vectors and is analogous the ν-property in
[13] (proposition 12).
Lemma 5.1 : Let (η, ξi , Ay ) be the solution of the
primal optimization problem and let (qyi , λ) be the solution of the dual. Then,
1. ν is an upper bound on the fraction of margin errors.
2. ν is a lower bound on the fraction of support vectors.

Proof:
At most a fraction of ν examples
P
Pcan satisfy
q
=
β
=
1/(νn).
This
is
because
zi
z
Pi,z qzi = 1.
But from KKT conditions we know that z qzi = β if
ξi > 0. Hence the first part of the lemma. Any support
vector
Pcan contribute at most a mass of 1/(νm) to the
sum i,z qzi = 1. Thus, there are at least νn examples
which are support vectors.

6

Implementation Issues

The semidefinite programs discussed above can be
solved using existing solvers such as CSDP [3]. This
package was used in the experiments discussed below.
However, for large n or d this approach becomes impractical. An alternative approach, which yielded similar results, is to use a projected sub-gradient algorithm [2]. The projected sub-gradient algorithm takes
small steps along the sub-gradient of the objective,
followed by Euclidean projection on the set of constraints.
To see how it may be applied, note that Eq. (3) may
be written as
P
max η − β i [η − p(yi |xi ) + maxz6=yi p(z|xi )]+
P
s.t
y Ay = I
Ay º 0
Thus the objective is a non-differentiable function, and
the only constraints are positivity of Ay and the normalization constraints. It is straightforward to obtain
the sub-gradient of the objective. We now turn to the
Euclidean projection part of the algorithm.
Here the set of constraints is given by
n
o
X
Snorm =
Ay :
Ay = I
y

Spos
S

=

n

Ay : A y º 0

o

= Snorm ∩ Spos

of the sets Snorm , Spos is straightforward, one can use
Dykstra’s alternating projection algorithm [6] to obtain the Euclidean projection on S.

7

Relation to 2nd order kernel
methods

Our probabilistic model is closely related to SVM
with second order kernels. To see this, note that
xT Ay x = tr(Ay xxT ) which may be interpreted as a
dot product between the elements of Ay and xxT . This
is precisely the form of the predictor obtained for SVM
with a second order kernel. There are however several
key differences between our approach and the SVM
one. The first is that the outputs in our case are automatically normalized probabilities, whereas the SVM
need not even be positive. The second is that the
bound on the zero-one loss used in our learning algorithm is significantly different from that used in SVM.
Clearly, the class of models we learn are a subclass of
those available to second order SVMs, due to the constraints on the matrices Ay . To gain more insight into
the constraints, consider the case where Ay is constrained to be diagonal. The resulting classification
rule will be based on the dot product between diag(Ay )
and the element-wise square of x. Since the diagonal
elements will then be constrained to be in the range
[0, 1], this case corresponds to a linear SVM on the
squared x with box constraints on the weight vectors.
This creates an interesting link between our method
and linear separators with positive weights such as the
Winnow algorithm [10].
The relation between the log-likelihood formulation
(Eq. (4)) and logistic-regression is not direct as the
relation between the margin formulation (Eq. (3)) and
SVMs. This is because in our model, probabilities
are linear in the parameters, while for logistic regression they are obtained through exponentiation of linear
terms.

Define the Euclidean projection of the parameters Ay
on S by
X
{Apy } = arg min
kAy − Ây k2
(12)

In the experimental evaluations below, we compared
our method to second order SVMs, and found that the
former achieved better performance. We elaborate on
possible reasons for this result in what follows.

In the binary class case, this projection can be found
analytically. Define the matrix C = (A1 − A2 + I)/2,
and denote by vi , λi its eigenvectors and eigenvalues.
Then it can be shown that the projection is given by
X
min(1, max(0, λi ))vi viT , Ap2 = I − A1
Ap1 =

8

Ây ∈S

y

i

In the multiclass case, there does not seem to be an
analytic solution. However, since projection on each

Quantum Mechanics

The formulation we proposed, and especially the Likelihood based approach, are related to analogous detection problems in the quantum mechanics literature.
We begin with some definitions. A density operator ρ is a positive semi-definite matrix, with a unit
trace, tr(ρ) = 1 . We can think of the density operator as defining a distribution over the eigenvec-

7

In the machine-learning formulation given in the current work, we do not assume to be given either the
prior probabilities ζi nor the density operators ρi , but
only a finite sample from both. Specifically, we assume
to have only pairs of a vector x and a label y. Where
the label y was drawn in accordance to the prior ζi
and the vectors x in accordance to the class conditional probabilities ρi .

6

Error SVM

5
4
3
2

9

1
0
0

1

2

3
4
Error MaxMargin PM

5

6

7

Figure 2: Test error (in percentages) of the MaxMargin
algorithm (x-axis) vs. test error (in percentages) of
SVM (y-axis) for all the 45 label-pairs of the USPS
dataset. A point above the line y = x indicates better
performance for the MaxMargin algorithm.
tors of the operators, with a weight proportional to
the corresponding
eigenvalue. Specifically, denote by
P
ρ = i νi vi viT where kvi k2 = 1 . Then,
Pr [vi ] = νi = viT ρvi = tr(ρvi viT ) .
We can also use the density operator to define a probability measure over every normalized vector x using
the same algebraic form and have, Pr [x] = tr(ρxxT ) .
We now turn our attention to the problem of quantum
hypothesis testing [8]. Assume that there are given k
density-operators ρi for i = 1 . . . k. Our goal is to find
a set of k operators Πi which we shall call detection
operators. These operators
Pare positive semi-definite
whose sum is the identity, i Πi = I . These detection
operators are used to define the conditional detection
probabilities,
Pr [state j | state i] = tr(ρi Πj )
that the detectors choose the jth state when the ith
state is true. Let us denote by ζj the prior probability
of being in jth state. Then, the average detection error
is given by,
k X
k
X
j

ζj (1 − δi,j )tr(ρj Πi ) ,

i

where δi,j = 1 if i = j and δi,j = 0 if i 6= j. The
goal of the system designer is to find a set of detection
operators Πi that will minimize the average error. Eldar [7] proposed a few formulations of the problem as
semi definite-programs. Note that the above problem
is similar to our Likelihood formulation in Eq. (4).

Related Work

A few attempts were made to combine large margin
classifiers with probabilistic outputs. The most notable example is the work of Platt [11]. This work
suggests using a sigmoid on the outputs of the support vector machine, and provides ways to calibrate
the parameters of this sigmoid.
An alternative approach was discussed by CesaBianchi et al in [4, 5]. They suggested to force the
output of a linear classifier to the [0, 1] range (and thus
have a probabilistic interpretation) by assuming that
both the input vector x and the weight vector v lie in
a ball of radius one. Thus the value of the inner product between the weight vector and the input vector
is always in the range [−1, 1], which is mapped linearly into the range [0, 1]. Note that there is no simple
and direct extension of this approach into multi-class
problems.
Several directions which relate machine learning and
quantum mechanics were proposed recently. Warmuth [17] presents a generalization of the Bayes rule to
the case when the prior is a density matrix. Wolf [18]
provides interesting relations between spectral clustering and other algorithms based on Euclidean distance,
and the Born rule. Our likelihood based approach is
related to some of the many detection algorithms presented by Eldar [7]. Note that unlike Eldar, we do
not assume direct knowledge of a probabilistic model
(prior probabilities or density operators) but only a
finite sample from it.

10

Experimental Evaluation

To illustrate how our method extracts subspaces from
data, we first apply it to a simple two dimensional
XOR problem shown in Figure 3. The resulting PSD
matrices Ay turn out to be projection matrices (i.e.
eigenvalues in {0, 1}) although their eigenvalues could
be non-integers in principle. Furthermore, we have
p(yi |xi ) = 1 for all sample points.
We next evaluated our algorithm using the USPS
handwritten digits dataset. The training set contains
7, 291 training examples and the test set has 2, 007 ex-

1

0.6

Bayes
MaxMargin

0.9

0.4
Fraction of Test Examples

0.8

0.2
0
−0.2
−0.4

0.7
0.6
0.5
0.4
0.3
0.2
0.1

−0.6
−0.5

0

0.5

Figure 3: An example of subspace learning in two dimensions. The classes in this case are the two one dimensional subspaces (i.e., lines) corresponding to the
vectors v1 = [1, 1], v2 = [−1, 1]. The sample points (5
points per class) are drawn randomly from these lines.
Applying our max-margin algorithm with β = 0.1 to
this sample results in matrices A1 , A2 with spectra
[0, 1], [1, 0] respectively. The lines corresponding to the
dominant directions in each Ay are shown in the figure.

amples. Originally, each instance represents an image
of a size 16 × 16 of a digit. There are ten possible
digits. Since our current implementation (CSDP) is
still limited in the data size it can handle, we reduced
the dimensionality of the data by replacing each four
adjacent pixels with their mean, resulting in image of
size 8 × 8. Thus, the dimensionality was reduced from
256 to 64. We enumerated over all 45 pairs of digits
and repeated the following process 10 times. For each
pair we randomly chose 300 examples which were associated with one of the two digits of the current pair.
The remaining training examples associated with this
pair were used as a validation set. The test set was
the standard USPS test set (restricted to the relevant
two digits).
We trained three algorithms: support vector machines
(SVMs), our maximum-margin formulation in Eq. (3)
(denoted by MaxMargin) and our optimal Bayes formulation in Eq. (5) (denoted by Bayes). For SVMs we
used 9 values for the regularization parameter β and
for the MaxMargin method we tried 6 values for the
regularization parameter. We trained each of the algorithms using all the values of the parameter and picked
the one model which achieved minimal error over the
validation set. We then used this model to compute
the error over the test set. We averaged the results
over the 10 repeats.
Figure 2 summarizes the results for both SVMs and the
MaxMargin approach. Each point corresponds to one
of the 45 binary classification problems. A point above

0
0

0.2

0.4
0.6
Threshold

0.8

1

Figure 4: Fraction of examples in test set which the
difference in probability |p(3|x) − p(5|x)| is below a
threshold set by a value in the x-axis.

the line y = x corresponds to a pair where MaxMargin
performs better than SVMs, and vise-versa. Clearly,
MaxMargin outperforms SVMs, as most of the points
are above the line y = x. We computed a similar plot
for the Bayes algorithm which turned out to be worse
than both MaxMargin and SVMs.
To better understand the performance of Bayes and
MaxMargin we focus our attention on one of the 45 binary problems. Specifically, we chose the hard task of
discriminating between the digits three and five. This
is the hardest task for SVMs. We picked one of the
partitions of the data into training-set and validationset and computed the absolute difference in probability for each of the test examples: |p(5|x) − p(3|x)|.
We then enumerated over several possible threshold
values of this difference, and recorded the fraction of
test examples for which this difference is higher than
the value of the threshold. The results are summarized in Figure 4 for the MaxMargin and Bayes algorithms. As one can observe from the figure, for the
MaxMargin algorithm all of the examples have a difference in probability that is less than 0.6. While for
the Bayes algorithm the difference of the probabilities
is even as high as 0.8. This result can be explained by
the following observation: The goal of the MaxMargin algorithm is to maximize the number of correct
predictions. For this task, there is no need to have a
high-difference in probabilities, only high-enough difference (of about 0.5). On the other hand, the Bayes
algorithm optimizes the expected error when drawing
a label using the probability model p(y|x). It thus
tries to push the probabilities apart from each other,
even at the cost of making some prediction error. Indeed, this is the case here, since Bayes generally yields
worse generalization error than MaxMargin. However,
its probabilities seem to better calibrated, suggesting

that Bayes should in some cases be the preferred algorithmic choice.

[2] D.P. Bertsekas. Nonlinear Programming. Athena
Scientific, Belmont, MA, 1995.

11

[3] B. Borchers. CSDP, a C library for semidefinite
programming. Optimization Methods and Software, 11(1):613–623, 1999.

Discussion

We presented algorithms for learning subspaces using
probabilistic models. This resulted in semidefinite optimization, and allowed both max-margin and likelihood objectives.
Note that although our presentation referred to the
case of orthogonal subspaces, a much wider class of
subspaces are separable under our classification rule
(intuitively, subspaces such that the angle between
them is above 45 degrees in the binary case).
The empirical results presented above show that our
method compares favorably with second order SVM.
Since our model is effectively a subclass of the latter,
it is not immediately clear why this should be the case.
There are two differences between our method and
SVMs which could shed light on these results. The first
is that since we optimize over a constrained parameter set for the weights, generalization error variance is
reduced, albeit at the cost of possibly increased bias.
It will be very interesting to obtain theoretic results
in this respect. While it does not seem like the VC dimension of our class is smaller than the corresponding
SVM, there still may be theoretical guarantees which
result from our constraints (positive semidefiniteness
and normalization) on parameter space. Another possible explanation for the empirical results is the difference in the objective function, and related convex
bounds on the zero-one loss. While SVM uses a hinge
loss to bound the zero-one loss, our method effectively
uses the bounds discussed in Section 3.2. One difference between these two bounds, is that the hinge loss
heavily penalizes points with negative margin, whereas
in our case this penalty is upper bounded.

[4] N. Cesa-Bianchi, A. Conconi, and C. Gentile.
Learning probabilistic linear-threshold classifiers
via selective sampling. In COLT 16, 2003.
[5] N. Cesa-Bianchi, A. Conconi, and C. Gentile.
Margin-based algorithms for information filtering.
In NIPS 15, 2003.
[6] R. L. Dykstra. An algorithm for restricted least
squares regression. J. Amer. Stat. Assoc., 78:837–
842, 1983.
[7] Y. C. Eldar. Quantum Signal Processing. PhD
thesis, MIT, 2001.
[8] C.W. Helstorm. Quantum Detection and Estimation Theorey. Academic Press, San Francisco,
CA, 1976.
[9] A. McCallum J. Lafferty and F. Pereira. Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In ICML
18, pages 282–289, 2001.
[10] N. Littlestone. Mistake bounds and logarithmic
linear-threshold learning algorithms. PhD thesis,
U. C. Santa Cruz, March 1989.
[11] J.C. Platt. Probabilities for SV machines. In
A. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers, pages 61–74. MIT press, 1999.
[12] B. Schölkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.
[13] B. Schölkopf, A.J. Smola, R. Williamson, and
P. Bartlett. New support vector algorithms. Neural Computation, 12:1083–1121, 2000.

An interesting extension of our method is to model
local interactions via semidefinite matrices. This
would correspond to Taskar’s extension of SVM to
the multi-label case [15], and would hopefully share
the probabilistic interpretation of Conditional Random Fields [9].

[14] J. Shi and J. Malik. Normalized cuts and image
segmentation. IEEE Trans. on Pattern Analysis
and Machine Intelligence, 22(8):888–905, 2000.

Acknowledgments The authors thank the Rothschild

[15] B. Taskar, C. Guestrin, and D. Koller. Max margin markov networks. In NIPS 15, 2003.

Foundation - Yad Hanadiv for their generous support.



We address the problem of learning the parameters in graphical models when inference
is intractable. A common strategy in this
case is to replace the partition function with
its Bethe approximation. We show that there
exists a regime of empirical marginals where
such Bethe learning will fail. By failure we
mean that the empirical marginals cannot be
recovered from the approximated maximum
likelihood parameters (i.e., moment matching
is not achieved). We provide several conditions on empirical marginals that yield outer
and inner bounds on the set of Bethe learnable marginals. An interesting implication of
our results is that there exists a large class of
marginals that cannot be obtained as stable
fixed points of belief propagation. Taken together our results provide a novel approach
to analyzing learning with Bethe approximations and highlight when it can be expected
to work or fail.
Probabilistic graphical models [8, 23] are a powerful
tool for describing complex multivariate distributions.
They have been used successfully in a wide range of
fields, from computational biology to machine vision
and natural language processing. To use such a model
in practice, one typically needs to solve two related
tasks. The first is the inference task which involves
calculating probabilities of events under the model.
The second task involves learning the parameters of
the model from empirical data.
Unfortunately, in many models of interest the inference
problem is computationally hard, and cannot be solved
exactly in practice. This has motivated extensive research into approximate inference schemes, some of
which have been quite successful empirically. Perhaps
the most well known of these is the belief propaga-

tion (BP) algorithm, which is closely related to variational approximations based on Bethe free energies
[26]. Another variational approach, which uses convex free energies is the tree-reweighted (TRW) method
[22]. Although the TRW approach results in convex optimization problems for inference, it sometimes
yields marginals that are inferior to those obtained by
BP (e.g., see [9]).
How should one learn the parameters of a model when
inference is intractable? The typical approach to parameter learning is likelihood maximization, but when
inference is intractable it is also hard to maximize the
likelihood.1 Because of this difficulty, many methods
have been devised to approximate the learning problem. One elegant approach is to approximate the likelihood using the same variational approximation that
is employed during inference [5, 14, 16, 19].
Analyzing the performance of approximate learning
schemes is challenging, since even the accuracy of the
underlying variational approximations is hard to analyze. Furthermore, we do not generally expect the
learned model to be similar to the one obtained using exact maximum likelihood. One approach, which
has recently been introduced by Wainwright [19] is to
use the notion of moment matching. In exact maximum likelihood learning, the learned model has a nice
property: some if its marginals are guaranteed to be
identical to those of the empirical data. This property
is often referred to as moment matching. Wainwright
[19, 21] has shown that when using convex variational
approximations such as TRW, the learned model also
has the moment matching property in the following
sense: if one applies approximate inference to it (using the same variational approach that was used during learning), the resulting marginals will be equal to
1

When the data are known to be generated by a graphical model of the same structure, pseudo-likelihood [1] can
be used and is consistent. However, this assumption is
rarely met in practice, and pseudo-likelihood often does
not perform well in these cases.

the empirical ones. However, these results cannot be
applied to learning with Bethe approximations, since
the latter are not convex. Because of the success of
Bethe approximations in a wide array of applications,
it is important to understand the advantages and limitations of learning with those. This is precisely the
goal of our work.
It may initially seem like learning with Bethe approximations would also result in a moment matching property. In other words, if we use Bethe approximations
during both learning and inference, our learned model
will agree with the empirical marginals. However, as
we show here, the situation is considerably more complex. In the current work we provide some surprising
results with respect to moment matching and Bethe
approximations, that shed light on the performance of
learning with such approximations, and on properties
of the BP algorithm. Our main results are:
• We show that there exist empirical distributions for
which Bethe approximations cannot perform moment matching. In other words, if we run BP on
the optimal Bethe parameters, we will not recover
the empirical marginals. Such empirical distributions are thus bad inputs for Bethe approximations,
since the learned parameters cannot be used to reconstruct the original marginals.
• We provide inner and outer bounds on the set of
marginals for which Bethe moment matching is possible, and show that they agree with empirical behavior of Bethe learning. Surprisingly, we show that
binary attractive models cannot be learned with
Bethe approximations for certain graphs.
• Our results also provide a novel characterization of
BP fixed points. Specifically, we show that there is
a large class of marginals that cannot be obtained
as stable fixed points of BP.
Taken together, our results provide a novel way of analyzing learning with Bethe approximations.

1

Maximum Likelihood in Graphical
Models

We focus on pairwise Markov random fields for
simplicity. That is, we consider random variables
X1 , . . . , XNV and pairwise functions θij (xi , xj ) corresponding to edges E in a graph G with NV nodes. The
MRF corresponding to these parameters is given by:


NV
X
X
1
exp 
θij (xi , xj ) +
θi (xi )
p(x; θ) =
Z(θ)
i=1
ij∈E

(1)

where Z(θ) is the partition function and x corresponds
to a complete assignment to the NV variables.
We wish to learn the parameters θ from a sample of
size M given by x(1) , . . . , x(M ) . A standard approach
to parameter learning is to maximize the log likelihood
given by:2
`(θ) =

1 X
log p(x(m) ; θ) = µ̄ · θ − log Z(θ)
M m

(2)

where µ̄ are the empirical marginals given by:
µ̄ij (xi , xj )

=

µ̄i (xi )

=

1 X
δxm ,x δxm ,x
M m i i j j
1 X
δxm ,x
M m i i

∀ij ∈ E
i = 1, . . . , N

and θ is the corresponding vector with the parameters
θij (xi , xj ), θi (xi ) in appropriate coordinates.
The likelihood function `(θ) is concave in θ and thus
does not have local optima. Finding its global maximum is possible when the function, as well as its gradient, can be calculated efficiently. In these cases a
variety of methods can be used, such as gradient ascent, iterative proportional fitting or any other general
purpose first order convex optimization procedure.
A key property of the optimal parameters θ M L (µ̄) is
that they satisfy the so called moment matching condition, described next. Define the vector µθ to be the
set of marginals of the model p(x; θ) given by:
µθij (xi , xj )
µθi (xi )

= p(xi , xj ; θ) ∀ij ∈ E
= p(xi ; θ)

i = 1, . . . , N

The moment matching condition for maximum likelihood optimality is then simply:
µθM L = µ̄

(3)

The condition states the following simple fact: the optimal parameters θ M L are such that the optimal model
p(x; θ M L ) has the given empirical marginals. This is
a desirable property since the empirical marginals are
often a good approximation of the true marginals.3
1.1

Bethe Approximations of the Likelihood

The problem of maximizing the likelihood in Eq. 2 is
hard due to the general intractability of the partition
function and marginals inference problems. We shall
2

Dependence on the sample is implicit throughout.
When not enough data is available for estimating
marginals reliably from data, it is possible to use smoothing or regularization. We do not address this here, but our
results can be generalized to these cases.
3

focus on a common approach to this problem, which is
to replace log Z(θ) with an approximation F (θ). We
shall specifically be interested in the Bethe approximation [26] defined as follows. First define the negative
Bethe free energy as:

The subdifferential of F (θ) is defined as follows. For
a given θ define M (θ) to be the set of vectors µ that
maximize F (µ; θ). Namely:

F (µ; θ) = µ · θ + HB (µ)

The subdifferential of F (θ) is then Conv {M (θ)}, the
convex hull of the vectors in M (θ). The subdifferential
of `B (θ; µ̄) is thus µ̄ − Conv {M (θ)}. Taken together
we have that the optimality condition for θ B (µ̄) is:

(4)

Where the function HB (µ) is the Bethe entropy given
by:
X
X
HB (µ) =
(di − 1)
µi (xi ) log µi (xi )
xi

i∈V

−

X X

µij (xi , xj ) log µij (xi , xj )

ij∈E xi ,xj

and di is the degree of node i in the graph G.
The Bethe approximation for the log partition function
log Z(θ) is then given by:
F (θ) = max F (µ; θ)
µ∈ML

µ∈ML

µ̄ ∈ Conv {M (θ B (µ̄))}

(8)

(9)

Whenever there is a single maximizer µ(θ) in M (θ)
the condition becomes µ̄ = µ(θ), resembling the moment matching condition in Eq. 3. However, generally
the above condition is more subtle and actually means
that when there are multiple maximizers, µ̄ is not necessarily equal to any of them. In Section 2 we consider
the strong implications that this has on learning with
Bethe approximations.

(5)
1.3

Eq. 5 uses the following definitions: ML is the local
marginal polytope, which is the set of locally consistent
pseudo-marginals µ defined as:
P


µij (xi , xj ) = µi (xi ) ij ∈ E 





xj


P
µ
(x
,
x
)
=
µ
(x
)
ij
∈
E
ij
i
j
j
j
ML = µ ≥ 0
.
xi


P






µi (xi ) = 1
xi

(6)
We are thus interested in maximizing the Bethe likelihood:
`B (θ; µ̄) = µ̄ · θ − F (θ) = µ̄ · θ − max F (µ; θ) (7)
µ∈ML

One interesting property of the function `B (θ; µ̄),
which is typically overlooked, is that it is in fact a
concave function of θ and thus it does not have local
maxima. This is a simple outcome of the fact that
F (θ) is a pointwise maximum over functions that are
convex (in fact linear) in θ and is thus convex [2], so
that its negation is concave.
In what follows, we characterize the global maximizer
of the Bethe likelihood and discuss several ways of approximating it. We denote the maximizer by θ B (µ̄).
1.2

M (θ) = arg max F (µ; θ)

Optimality Conditions for Bethe
Learning

The Bethe likelihood `B (θ; µ̄) is a concave but nonsmooth function. Thus, a necessary and sufficient condition for θ B (µ̄) to maximize it is that the subdifferential of `B (θ; µ̄) at θ B (µ̄) contains the all zero vector 0
[12]. In what follows we use this to characterize θ B (µ̄).

Maximizing the Bethe Likelihood in
Practice

Although `B (θ; µ̄) is concave, finding its maximum is
still hard and there is no known closed form solution
or polynomial time algorithm for it. The key difficulty is that both evaluating `B (θ; µ̄) and calculating
a subgradient for it involve maximizing F (µ; θ) over
µ.4 Since F (µ; θ) is not concave in the general case,
this appears to be a hard problem.5
The common practice in this case is to find a stationary point of F (µ; θ) using BP [26] and using it as a
subgradient in subgradient ascent. Although it is hard
to obtain theoretical guarantees for such an approach,
it sometimes provides good empirical results [14, 16].
Another common approach comes from studying the
tree graph case. In that case, the maximum likelihood
parameters for µ̄ are given in closed form by θ c (µ̄)
defined as [21]:
θic (xi ; µ̄)
c
θij
(xi , xj ; µ̄)

=

log µ̄i (xi )
µ̄ij (xi , xj )
= log
µ̄i (xi )µ̄j (xj )

When the graph is not a tree, θ c (µ̄) is not necessarily
the maximum likelihood parameter, as we also show
in what follows. However, it is always true that µ̄
is a stationary point of F (µ; θ c (µ̄)) [21]. Specifically,
if one initializes BP on the MRF θ c (µ̄) with uniform
messages, µ̄ will be obtained as a fixed point (although
this fixed point may be unstable as we show later).
4

This maximization is subject to constraint µ ∈ ML .
In what follows we do not state this explicitly for brevity.
5
We are not aware of complexity results that prove this
fact.

2

Bethe Learnable Parameters

Our main goal in the current work is to understand when Bethe learning achieves moment matching. By moment matching we mean that inference
on the learned parameter will result in the empirical marginals (as in Eq. 3). In the context of Bethe
learning, it is natural to define inference as returning the maximizer of F (µ; θ).6 One difficulty with
this is that F is hard to maximize due to its nonconvexity.7 However, there is a more fundamental
difficulty: F (µ; θ B (µ̄)) might have multiple distinct
global maximizers. In this case, the outcome of the
inference step is not well defined, and hence moment
matching cannot be achieved. Furthermore, in this
case the empirical marginals µ̄ will generally not correspond to any of the maximizers of F (µ; θ B (µ̄)), but
will rather lie in their convex hull (see Section 1.2).
On the other hand, if F (µ; θ B (µ̄)) has a single global
maximum then by Eq. 9 this maximizer is exactly µ̄
and we have moment matching.
This brings us to our key question: for which values
of µ̄ will Bethe achieve moment matching? Given the
above discussion, these are µ̄ for which the optimal
θ B (µ̄) has a single maximum.
Definition 1. µ̄ is Bethe learnable if there exists a
θ such that µ̄ is the unique maximizer of F (µ; θ). In
this case θ maximizes the Bethe likelihood (i.e., θ =
θ B (µ̄)), and µ̄ can be recovered from θ so that moment
matching is achieved. Denote the set of such µ̄ by BL .
The definition of BL is quite implicit and it is thus
not immediately clear what can be said about this set.
For example, are there parameters that are not Bethe
learnable? Clearly there are parameters that are Bethe
learnable since Bethe is exact for trees (see Section
1.3). In what follows we show that there are in fact
µ̄ that are not in BL and characterize those in some
cases.

3

Characterizing Unlearnable
Marginals

The naive approach to testing if µ̄ ∈ BL would be
to scan all parameters values θ, and for each of those
test if µ̄ is the unique maximizer of F (µ; θ). In what
follows we provide several simpler approaches, some in
closed form. Specifically, Sections 3.1 and 3.2 provide
6

Using the same inference approximation in learning
and inference is also motivated by the results in [19, 21]
where a similar approach for convex free energies results in
moment matching.
7
In practice, maximization will be approximated by
running BP on the MRF given by θ.

outer bounds on BL and Section 3.3 provides an inner
bound.
3.1

A Condition Using Canonical Parameters

Our first observation in terms of characterizing BL is a
lemma that provides a sufficient condition for µ̄ ∈
/ BL .
In other words, we obtain an outer bound on BL .
Lemma 1. Let µ̄ ∈ ML and let θ c (µ̄) be its canonical parameter. If µ̄ is not a global maximizer of
F (µ; θ c (µ̄)) then µ̄ ∈
/ BL .
Proof. We are given the fact that:
µ̄ ∈
/ arg max F (µ; θ c (µ̄))
µ∈ML

(10)

Asssume by contradiction that µ̄ ∈ BL so that there
is a parameter θ̃ for which µ̄ ∈ arg maxµ∈ML F (µ; θ̃).
As mentioned in Section 1.3 µ̄ is a fixed point of BP for
the parameters θ c (µ̄). Since µ̄ maximizes the negative
Bethe free energy for θ̃, it is also a BP fixed point for θ̃
[26, Theorem 3]. Since BP fixed points correspond to
re-parameterizations of the MRF [20], and since both
θ c (µ̄) and θ̃ have the same BP fixed point µ̄, it follows
that θ c (µ̄) and θ̃ are re-parametrizations of the same
MRF. It is then easy to show that two parameters that
are re-parametrizations of each other share the same
BP fixed points with the same values up to a constant.
This implies that µ̄ does in fact maximize F (µ; θ c (µ̄)),
contradicting our assumption in Eq. 10.
Lemma 1 provides an easy procedure for excluding
points from BL : Given µ̄ generate the canonical parameter θ c (µ̄). Run BP with this parameter multiple
times (e.g., by initializing messages randomly). If a
fixed point with value F (µ; θ c (µ̄))) > F (µ̄; θ c (µ̄)) is
found then µ̄ ∈
/ BL .
Another interesting implication of Lemma 1 (by negation) is that if µ̄ is learnable then its maximum likelihood parameter is necessarily the canonical one θ c (µ̄).
Thus, the canonical parameters are in some sense the
best choice for maximum likelihood with a Bethe approximation. Namely, they are the correct maximum
likelihood parameters when µ̄ is learnable. When µ̄ is
not learnable using Bethe approximation in learning is
apparently not a good approach in any case.
3.2

Closed Form Bounds via the Hessian

Here we provide another outer bound on BL (i.e., a
criterion for µ ∈
/ BL ). The key idea is to find marginals
µ̄ which can never be a maximum (local or global) of
F (µ; θ) regardless of the value of θ. Clearly, such µ̄
will not be in BL .

For this purpose we will focus on binary pairwise models. Following [24, 10] we will use a minimal parameterization. In this representation we denote µi (xi = 1)
as µi and µij (xi = 1, xj = 1) as µij . The other singleton and pairwise marginals can be obtained from
these via:
µij (xi = 1, xj = 0)

= µi − µij

µij (xi = 0, xj = 1)

= µj − µij

µij (xi = 0, xj = 0)
µi (xi = 0)

1 − µi

It can be verified that a point is in ML (G) if and
only if all the above marginals are non-negative (e.g.,
see [3, 17]). The function F (µ; θ) in this case can be
expressed as a function of µij , µi alone. Furthermore,
there is no need to add non-negativity constraints since
these would result in log of a negative number in the
objective. For this unconstrained F (µ; θ), a sufficient
condition for µ̄ not to be a maximum for any θ is
that the Hessian of F (µ; θ) at µ̄ is not negative-semidefinite regardless of the value of θ. Luckily, the Hessian at F (µ; θ) does not depend on θ (since F is linear in θ) and equals the Hessian of the Bethe entropy
HB ({µij , µi }). Thus we have:
Lemma 2. For binary variables, if the Hessian of
HB ({µij , µi }) at µ = µ̄ is not negative-semi-definite
then µ̄ ∈
/ BL .
Lemma 2 provides a sufficient condition for µ̄ ∈
/ BL .
It can be easily tested for any given µ̄ by calculating
the Hessian at this point and checking its eigenvalues.
To better understand the condition on the Hessian,
we turn to a more specific scenario which results in a
closed form expression on µ̄.
We focus on marginals µ̄ that are homogenous in the
sense that all pairwise µij = µe for a constant µe and
all µi = µv for a constant µv .8 We make no restrictions
on the graph structure. The following lemma states a
simple sufficient condition for guaranteeing that µ̄ ∈
/
BL . We denote by NV the number of variables and
NE the number of edges.
Lemma 3. Assume µ̄ corresponds to homogenous binary marginals in minimal representation, and µ̄ satisfies the following condition:
µ̄e >

(1 −

NV
NV
2
NE )µ̄v + 2NE µ̄v
NV
1 − 2N
E

• The condition is independent of the graph structure. It only depends on the number of variables
and edges.
• For tree graphs and single loop graphs there are no µ̄
that satisfy the conditions. This is consistent with
the fact that the Bethe free energy has a unique
optimum in these cases and all marginals are Bethe
learnable.

= µij − µi − µj + 1
=

The proof of the lemma is given in Appendix A. This
result has several interesting implications:

(11)

Then the Hessian of HB (µ) at µ̄ is not negative-semidefinite, and hence µ̄ ∈
/ BL .
8
Note that ML (G) in this case reduces to the constraints: 2µv − 1 ≤ µe ≤ µv and µe ≥ 0.

V
• As N
NE → 0 (e.g., for complete graphs with NV →
∞) the condition becomes µ̄e ≥ µ̄2v . Perhaps surprisingly, this is a condition that is satisfied by any
ferromagnetic distribution [15, 4].9 This implies
that if µ̄ are generated from a ferromagnetic disV
/ BL . In other
tribution with N
NE → 0, then µ̄ ∈
words, ferromagnets are not Bethe learnable in this
asymptotic regime.

3.3

Inner Bounds on BL

The results in the previous sections provide outer
bounds on BL . In the current section we show how
previous results on BP convergence can be utilized
to obtain inner bounds on BL . The key idea is to
check, given some µ̄, whether the canonical parameters θ c (µ̄) yield moment matching. We know that µ̄
is a BP fixed point for θ c (µ̄). Thus a sufficient condition for moment matching to take place is that µ̄
is the unique stable fixed point of BP.10 This would
imply that at θ c (µ) we have moment matching for µ̄
and thus µ̄ ∈ BL .
Fortunately there are various results that provide sufficient conditions on a parameter θ having a unique
stable fixed point. This has been addressed by multiple works in the past (e.g., [11, 13]).11 Each of these
works provides a different condition on θ. We are not
concerned with which one is better since all can equally
well be applied to our case and we can simply take the
union of the conditions to obtain a tighter bound.
Thus, our condition for µ̄ ∈ BL is calculated as follows:
for a given µ̄, calculate θ c (µ̄). Now check whether
θ c (µ̄) has a unique BP fixed point (by using one of
the conditions in the papers above). If it does, then
µ̄ ∈ BL . In the experiments section we will specifically
look at the condition from [13].
9

The non-homogenous form is µij ≥ µi µj .
Stability is needed to guarantee that this point is in
fact a global maximum as our analysis requires [6].
11
Note also [7] which analyzes unique fixed points, but
not necessarily stable.
10

4

Related Work

1
0.9

Several works have analyzed the behavior of Bethe approximations in graphical models, and their relation to
BP. The first key work is [26], which showed that the
fixed points of BP are local optima of the Bethe free
energy. Later this result was refined in [6] to show that
the stable fixed points of BP are local minima of the
Bethe free energy.
Another line of work focuses on conditions under which
the BP has a unique fixed point (implying no local
minima via [6]). Such works (e.g., [18, 11, 13, 7]) typically provide sufficient conditions on the model structure and parameters for BP to have a unique fixed
point. As we show in Section 3.3, such results can be
used to obtain inner bounds on BL , although they were
not originally developed for this purpose.
There has been less work on understanding learning
with Bethe approximations. The canonical parameters (see Section 1.3) have been suggested in several
works [21, 23, 25]. As we show here, these parameters
are generally not the ones optimizing the Bethe likelihood, but when µ̄ is learnable, they are in fact optimal.
Much stronger theory is available for learning with
convex free energies such as tree-reweighted variants.
In [19] it is shown that such methods have desirable
stability and asymptotic properties. The performance
of Bethe learning is not analyzed in this context, since
it does not fall under the convex approximations.
Our work proposes a novel view of learning with Bethe,
which is to focus on the marginals that Bethe can
and cannot match during learning. This highlights the
regimes where Bethe cannot be expected to work well,
and those where it might work, as we show further
in our empirical results. Our results provide initial
characterization of the set BL in terms of inner and
outer bounds. We expect that these can be tightened
further.

5

Illustrating the Bounds

In this section we provide several graphical illustrations of the bounds on BL that we presented earlier. We focus on the case of a two dimensional 3 × 3
toroidal grid graph, and on homogenous parameters as
described in Section 3.2. In this case, µ̄ can be conveniently represented in two dimensions (i.e., µv , µe ).
We begin by showing a case where the empirical
marginals µ̄ are unlearnable. In this case, the maximum Bethe likelihood parameter θB (µ̄) results in a
function F (µ; θB (µ̄)) which is not maximized by µ̄
(i.e., moment matching is not achieved). Rather, the
function F has two other maxima, and µ̄ lies in their

0.8
0.7

µv

0.6
0.5
0.4
Empirical marginals

0.3
0.2

Bethe maximizers

0.1
0.1

0.2

0.3

0.4

0.5
µe

0.6

0.7

0.8

0.9

1

Figure 1: Illustration of a case where µ̄ (denoted by
empirical marginals in the figure) does not maximize
F (µ; θ B (µ̄)). However, µ̄ is obtained as a convex combination of the two global maxima of F (µ; θ B (µ̄)). The
colormap corresponds to the values of F (µ; θ B (µ̄)). In
this case θ B (µ̄) was found by exhaustive search.
convex hull, as described in Section 1.2. This is shown
in Figure 1.
Figure 2 depicts several results regarding the set BL .
The colored regions correspond to marginals which
can be obtained via some empirical distribution (i.e.,
the marginal polytope [23]). The blue region indicates marginals that are learnable according to the
inner bound in Section 3.3. The red region indicates
marginals that are unlearnable according to the outer
bounds in Sections 3.1 and 3.2. In this case, lemmas
1, 2 and 3 yield identical outer bounds. The region in
black is not covered by any of our bounds. However,
there is an easy way to check empirically whether it
might be learnable: run gradient descent on the Bethe
likelihood (using BP to approximate the objective and
gradient12 ), and check whether the parameters at convergence satisfy moment matching. It turns out that
for the region in black, moment matching is achieved.
Note that due to the potential suboptimality of BP,
this is not a theoretical guarantee that this region is indeed learnable (i.e., it could be that we have reached a
suboptimal parameter, or that the reported marginals
are not the optimal ones for Bethe). Taken together,
the results in Figure 2 suggest that our outer bounds
are tight for the given graph.

6

Discussion

This work presents an analysis of when learning with
Bethe is guaranteed to fail, in the sense of not match12
This is an approximation since BP does not necessarily
find the global optimum of the Bethe free energy.

by checking one of our outer bounds). Additionally, we
show that in the binary homogenous models, there are
graphs where ferromagnetic models are unlearnable.

1
0.9
0.8

Our analysis of the Hessian in Section 3.2 shows that
there are marginals µ̄ that cannot be local minima of
the Bethe free energy. This in turn implies that they
will not be stable fixed points of BP. This highlights a
very interesting limitation of using BP to approximate
marginals. Namely, that there regions of marginals
space which will never be obtained as a result of running BP. It will be interesting to study the practical
implications of this observations.

0.7

µv

0.6
0.5
0.4
0.3
0.2
Unlearnable
Learnable (Inner Bound)
Learnable (Grad. Opt.)

0.1
0.1

0.2

0.3

0.4

0.5
µe

0.6

0.7

0.8

0.9

1

Figure 2: Illustration of bounds on BL space. Each point
in the figure corresponds to an empirical marginal µ̄. The
blue region shows marginals that are learnable according
to Section 3.3. The red region shows marginals that are
unlearnable according to Sections 1.3 and 3.2. The black
region is not covered by any of our bounds. However,
when running gradient ascent on the Bethe likelihood with
these marginals, it turns out that moment matching is
achieved at convergence (we decide that moment matching
is achieved when the absolute difference between empirical
and Bethe marginals is less than 0.01).

ing the moments of the training data. One of the key
difficulties of the Bethe approximation of marginals is
the non-convexity of the Bethe free energy, resulting in
local optima during inference. Our results show that
when using Bethe within learning, another problem
surfaces, even in the case where exact optimization
of the Bethe free energy is possible (and hence learning is tractable since the Bethe likelihood is concave).
Namely, that for particular empirical marginals (those
outside BL ) moment matching will not be achieved.
Characterizing the set BL is a challenge, and we have
presented initial steps in this direction by showing how
to obtain inner and outer bounds on it, some in closed
form. Our analysis also highlights the fact that the
canonical parameters often used in learning are inadequate in some cases. On the other hand, they can be
used to obtain outer bounds on BL as in Lemma 1.
Many interesting questions arise from our analysis and
deserve further study: e.g., when are the inner and
outer bounds we presented tight, and whether tighter
bounds exist? When are the canonical parameters optimal and in BL ?
On a practical level our results imply that there are
regimes when Bethe learning is bound to fail, and that
in some cases they can be inferred from the empirical
marginals without running a learning algorithm (e.g.,

Finally, our results do not imply that one should not
use Bethe approximations within learning. It has
been previously shown that Bethe approximations of
marginals outperform convex variants across a wide
range of parameter settings [9]. It is thus certainly
possible that for data such that µ̄ ∈ BL the learned
parameters will perform well. We intend to address
this issue as well as other theoretical questions in future work.

A

Proof of Lemma 3

We first note that the Hessian of HB (µ) has a particularly simple form in the homogeneous case. Denote
this Hessian matrix by A. Then its elements correspond to only NV +3 unique numbers ai (i = 1, . . . , V ),
b, c , d, which are given by:
1
1
+
) − di c
µv
1 − µv
1
b = −
1 − 2µv + µe
1
1
+
c =
(µv − µe ) (1 − 2µv + µe )
1
1
d = − −
−c
µe
µv − µe

ai

=

(di − 1)(

The Hessian depends on these values via:

ak k ∈ V, l ∈ V
l=k




b
k
∈
V,
l
∈
V
l
∈
N (k)



c k ∈ V, l ∈ E
k∈l
A[k,l] =
c k ∈ E, l ∈ V
l∈k




d
k,
l
∈
E
k
=l



0
otherwise

(12)

where N (k) is the set of neighbors of node k. The
indexing scheme is as follows: by k ∈ V we mean k is
the coordinate corresponding to µk and k ∈ E means
k is the coordinate corresponding to some edge in E.
The Hessian A is negative-semi-definite iff for all z
it holds that z T Az ≤ 0. We will show that if the

condition in Eq. 11 is satisfied then we can find a z
such that z T Az > 0 and therefore A is not negative
definite, and the lemma follows. We will define such a
z in the following way: zi = 1 if i ∈ V and zi = z (for
1
we
some scalar z) if i ∈ E. Denoting â = µ1v + 1−µ
v
obtain after some algebra that:13

[10] J. Mooij and H. Kappen. On the properties of the
Bethe approximation and loopy belief propagation on
binary networks. Journal of Statistical Mechanics:
Theory and Experiment, page P11012, 2005.

z T Az = (2E − V )â − 2NE c + 2NE b + NE z 2 d + 4NE zc

[12] R. T. Rockafellar. Convex Analysis (Princeton Mathematical Series). Princeton University Press, 1970.

The above is a quadratic concave function in z. When
its discriminant is greater than zero, it will attain positive values, and A will not be negative definite. The
condition on the discriminant corresponds to:
1
NV
0 < c2 − d (â − c + b) +
dâ
2
4NE

(13)

Assigning the values of the Hessian and some more
algebra leads to the equation:
0 < (µe − µ2v )(1 −

NV
NV
)−
µv (1 − µv )
2NE
2NE

(14)

Switching sides we get the condition in Eq. 11.



Online sequence prediction is the problem of
predicting the next element of a sequence
given previous elements. This problem has
been extensively studied in the context of individual sequence prediction, where no prior
assumptions are made on the origin of the
sequence. Individual sequence prediction algorithms work quite well for long sequences,
where the algorithm has enough time to learn
the temporal structure of the sequence. However, they might give poor predictions for
short sequences. A possible remedy is to rely
on the general model of prediction with expert advice, where the learner has access to a
set of r experts, each of which makes its own
predictions on the sequence. It is well known
that it is possible to predict almost as well as
the best expert if the sequence length is order
of log(r). But, without ﬁrm prior knowledge
on the problem, it is not clear how to choose
a small set of good experts. In this paper we
describe and analyze a new algorithm that
learns a good set of experts using a training set of previously observed sequences. We
demonstrate the merits of our approach by
applying it on the task of click prediction on
the web.

1. Introduction
Sequence prediction is a key task in machine learning and statistics. It involves predicting the next element in a sequence given the previous elements. Typical applications include stock market prediction, click
prediction in web browsing and consumption predicAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012.
Copyright 2012 by the author(s)/owner(s).

tion in smart grids. Although the sequence prediction
problem has been well studied, current solutions either
work for long sequences or require strong prior knowledge. In this work we provide a method that uses
training data to learn how to predict a novel sequence.
As we shall show, we use the training sequences to obtain the prior knowledge needed for predicting novel
sequences.
Sequence prediction is most naturally cast as an online
prediction problem (Cesa-Bianchi and Lugosi, 2006),
where at every step we predict the next element, and
then receive the true value of the element while suffering a loss if we made a prediction error. We are
then allowed to improve the model, and predict the
next step. The online formulation is natural in most
applications since the new element’s true value unfolds
in real time and we are interested in minimizing the
prediction loss of this process.
One classical approach to the problem is the so called
universal sequence prediction class of methods (Feder
et al., 1992; Hutter, 2006). Such methods guarantee
that asymptotically (with sequence size) the model
will achieve optimal prediction error. However, the
price we pay for universality is that good performance
will be reached only after seeing long sequences. Intuitively, the reason for this is that no prior knowledge
about the sequence is used, so it may take a while until
we have a good model of it.
An alternative approach which does introduce prior
knowledge is predicting with expert advice (Littlestone
and Warmuth, 1994; Vovk, 1990). Here one has a
set of r experts, where each expert is a sequence predictor. The Weighted-Majority algorithm (Littlestone
and Warmuth, 1994) uses such experts to do online
prediction, and is guaranteed to perform almost as well
as the best expert. More formally, for any sequence of
length T , the average number of prediction mistakes of
Weighted-Majority (WM ) is bounded above by the average number of prediction mistakes made by the best

Learning the Experts for Online Sequence Prediction

√
expert plus log(r)/T . Thus, the WM algorithm will
perform well when for every sequence there exists an
expert that performs well on it and when the sequence
is long enough.
Given the above, it’s clear that learning from experts
will work when the experts fit the sequences we want to
predict. Thus a key question, which we address here,
is how to choose a good set of experts. We propose to
learn these experts from a set of training sequences.
In the spirit of empirical risk minimization we shall
seek a set of experts that perform well on our training
set. This is a highly non-trivial task in our case due
to several reasons. First, the performance of a pool of
experts is measured by the performance of an online
algorithm whose parameters are the experts, and it’s
not clear how to optimize this function. We shall see
that the hindsight loss is a simpler function to optimize
and results in comparable theoretical guarantees. Second, we would like our experts to use arbitrarily long
histories in making predictions, but do so without over
ﬁtting. We shall show that this can be done by using a
variant of context trees (a.k.a. prediction suﬃx trees).
Finally, it’s not clear what generalization guarantees,
if any, can be expected from such a scheme. We perform a detailed generalization analysis, providing theoretical bounds on the sample complexity of learning
a good set of experts.
Our learning task is thus as follows: given a training
set of sequences, learn a set of experts that will work
well for online sequence prediction. In a sense, this
can be viewed as a collaborative version of sequence
prediction. We provide an objective that corresponds
to this discriminative setting and analyze the generalization error of its minimizer. Our theoretical analysis
provides generalization bounds that show no over ﬁtting for longer histories, and quantify the advantage
of learning in the collaborative setting.
We apply our model to synthetic and real-world problems and show that it outperforms methods which do
not use temporal and collaborative approaches.

2. Problem Formulation
Let Σ be a ﬁnite alphabet. A sequence of symbols is
a member of Σ∗ and is denoted by x = (x1 , . . . , xT ).
Online sequence prediction takes place in consecutive
rounds. On round t, the forecaster observes the preﬁx
x1:t−1 = (x1 , . . . , xt−1 ) and predicts x̂t ∈ Σ. Then, the
next symbol, xt , is revealed and the forecaster pays
1[xt ̸=x̂t ] . That is, it pays 1 if xt ̸= x̂t and 0 otherwise.
An “expert” for sequence prediction is a function f :
Σ∗ → Σ. Such an expert can be used for predicting

the t’th symbol by setting x̂t = f (x1:t−1 ).
Given a set of r such experts, the Weighted-Majority
(WM) algorithm (Littlestone and Warmuth, 1994) (see
pseudocode below), can be used for predicting almost
as well as the best expert.
A performance guarantee for WM is provided in the
following theorem (Littlestone and Warmuth, 1994).

Weighted Majority (WM)
parameter: η > 0
initialize: w1 = (1/r, . . . , 1/r)
for t = 1, 2, . . . , T
choose i ∼ wt at random
predict x̂t = fi (x1:t−1 )
update rule ∀i, wt+1 [i] ∝ wt [i]e−η1[fi (x1:t−1 )̸=xt ]
Theorem
1 The Weighted Majority algorithm (with
√
η = log(r)/T ) obtains the following regret bound:
√
T
T
1∑
1∑
4 log(r)
P[x̂t ̸= xt ] ≤ min
1[fi (x1:t−1 )̸=xt ] +
.
i T
T t=1
T
t=1
It follows that we can predict the sequence reasonably
well if two conditions hold:
1. log(r) is suﬃciently small compared to T .
2. At least one of the experts makes a small number
of mistakes on the sequence.
Therefore, when choosing a set of experts we face the
classical bias-complexity tradeoﬀ: On one hand we
want
r to be small enough so that the regret term
√
log(r)/T will be small. On the other hand, diﬀerent experts will work well on diﬀerent sequences, and
since we do not know the type of sequence we are going to get, we would like to increase r so that the set
of experts will be rich enough to explain many types
of sequences.
In this paper we propose to learn a good set of experts
based on a sample of sequences. Formally, let H be a
hypothesis class of experts. It is convenient to allow
experts to output predictions from a set Y , where we
have some way to convert an element from Y into a
prediction in Σ. For example, we can use Y = R|Σ| ,
where we interpret the prediction y ∈ Y as a score
for each of the symbols in Σ. The mapping from a
score vector in Y to an actual prediction in Σ is via
arg maxσ∈Σ yσ . Therefore, each f ∈ H is a function
from Σ∗ to Y . The loss of a prediction f (x1:t−1 ) is
measured by a loss function ℓ : Y × Σ → R. The loss

Learning the Experts for Online Sequence Prediction

function can be the 0-1 loss 1[xt ̸=x̂t ] . Later, we use
other loss measures that are convex surrogates of the
zero-one loss.
The problem that we consider in this paper can be formalized as follows: We are given a sample of sequences,
S = (x(1) , . . . , x(m) ), where each x(i) is assumed to be
sampled i.i.d. from an unknown distribution D over
Σ∗ . Our goal is to use S for learning a set of experts,
F ⊂ H, of size |F | ≤ r, where r is a parameter of the
learning problem (which should depend on the typical
size of T ). We wish to learn F such that when running
WM on a new sequence with the set F it will have a
small number of mistakes.
Given an expert f and a sequence x, we denote by
L(f, x) the average loss of f on x, speciﬁcally:

principle, namely, to solve the optimization problem
1 ∑
WM(F, x(i) ) .
m i=1
m

min

F ⊂H:|F |=r

This problem might be diﬃcult to optimize since the
objective function involves the activation of an algorithm and does not have a simple mathematical formulation. To overcome this diﬃculty, we show how a
simpler objective may be used. In light of Theorem 1
(generalized to convex surrogate losses) we know that
for any sequence x,
√
4 log(|F |)
WM(F, x) ≤ min L(f, x) +
.
(1)
f ∈F
T
Let us slightly overload notation and denote

L(f, x) =

1
T

T
∑

L(F, x) = min L(f, x) .

ℓ(f (x1:t−1 ), xt ) .

f ∈F

t=1

Given a set of experts, F ⊂ H, we denote by WM(F, x)
the averaged loss of applying the WM algorithm on
the sequence x with the set of experts F . Therefore,
our ultimate goal is to learn a set of experts F which
(approximately) minimizes
E [WM(F, x)] .

x∼D

Before we describe how we learn F , let us ﬁrst consider
two extreme situations. First, for r = 1, i.e. F = {f },
then WM(F, x) = L(f, x). That is, at prediction time,
we simply follow the predictions of the single expert f .
This is exactly the standard traditional setting of statistical batch learning, where we would like to learn a
model f from a hypothesis class H whose expected loss
over a randomly chosen example (in our case x ∼ D) is
as small as possible. The problem with this approach
is that it might be the case that the sequences are of
diﬀerent types, where no single expert from H is able
to accurately predict all of the sequences. On the other
extreme, if we set r = ∞, i.e. F = H, then we revert
to the problem of online learning with a hypothesis
class H. The problem with this approach is that if
H is “complex”,1 then the sequence length required
in order to guarantee good performance of the online
learning might be very large.

Thus L(F, x) is the hindsight loss when learning the
sequence x with experts F . Taking expectation of both
sides of Eq. 1 we obtain that
√
4 log(|F |)
E [WM(F, x)] ≤ E [L(F, x)] + E
.
x∼D
x∼D
x∼D
T
(2)
The second summand only depends on F via its size.
Therefore, for a ﬁxed size of F , we can follow a standard bound minimization approach and aim at minimizing Ex∼D [L(F, x)] instead of Ex∼D [WM(F, x)]. In
other words, we minimize the hindsight loss instead of
the online loss. An ERM approach to this minimization yields the following minimization problem on the
training set of sequences:
1 ∑
L(F, x(i) ) .
m i=1
m

min

F ⊂H:|F |=r

(3)

By deﬁnition of L(F, x), this can be written equivalently as
min

f1 ,...,fr ∈H

m
r
∑
1 ∑
minr
wj L(fj , x(i) )
m i=1 w∈∆ j=1

(4)

where ∆r = {w ∈ Rr : w ≥ 0, ∥w∥1 = 1}.

A straightforward approach for learning F when r > 1
is to follow the empirical risk minimization (ERM)

Assuming that H can be encoded as a convex set and
L(f, x) is a convex function,2 we obtain that the objective of Eq. 4 is convex in f1 , . . . , fr and w(1) , . . . , w(r)
individually but not jointly. This suggests an alternating optimization scheme where one alternates between

1
As measured, for example, by its Littlestone dimension
(Ben-David et al., 2009).

2
This will be the case for the class H and loss function
we use in Section 3.1

3. The Learning Algorithm

Learning the Experts for Online Sequence Prediction

optimizing over w’s and over f ’s. This scheme is especially attractive since minimizing over w for ﬁxed
F is straightforward: for each sequence x(i) ﬁnd the
best expert and set w(i) to 1 for that expert and 0
otherwise. Optimizing fi for ﬁxed w can be done via
gradient descent when using a smooth loss as we do
here (see Sections 3.1 and 3.2).

As mentioned before, any function f : Σ∗ → Σ can
be described by a context tree (as long as we allow
its depth to be large enough). Therefore, without additional constraints, learning the class of all context
trees from a ﬁnite sample will lead to over-ﬁtting. To
overcome this, one can constrain the depth of the tree.
Alternatively, we can allow any depth but carefully
discount long histories as described next.

3.1. The class of bounded norm context trees

Following (Dekel et al., 2010), we aim to balance between long histories (can be very informative but are
rare in the data hence are hard to learn) and short histories (less informative but easier to learn). This can
be done by deﬁning a norm over matrices corresponding to context trees, where longer histories are penalized more. Formally, for each column i of a context
tree matrix U, let d(i) be the depth of its corresponding node in∑the tree. Let a1 ≥ a2 ≥ . . . be a sequence
∞
such that i=1 ai ≤ 1.3 Then, we deﬁne a norm of
vectors to be such that
∑
∥u∥2 =
(5)
ad(i) u2i ,

Thus far we have given a general scheme and have
not described the particular set of experts we will use.
In what follows we specify those. Any function f :
Σ∗ → Σ can be described using a multiclass context
tree. For our experts, we will be using a generalization
of multiclass context trees following Dekel et al. (2010),
described below.
To simplify notation, denote Σ = [k] = {1, . . . , k}. A
multiclass context-tree is a k-ary rooted tree, where
each node of the tree is associated with a vector z ∈
Rk . The prediction of the tree on a sequence x1:t−1
is determined as follows. We initially start with the
vector z = 0 ∈ Rk , and set the current node to be
the root of the tree. We then add to z the vector
associated with the current node and traverse to its
xt−1 child, which becomes the current node. We add
again the vector associated with the current node and
traverse to its xt−2 child. This process is repeated until
we arrive either to x1 or to a leaf of the tree. The ﬁnal
value of z gives a score value to each of the elements
in Σ, and the actual prediction is arg maxi zi .
It is convenient to represent a context tree as a matrix
with k rows as follows. Let us order the nodes of a full
k-ary tree in a breadth ﬁrst manner. For simplicity,
we restrict ourselves to trees of bounded depth (which
can be very large, so this is not a serious limitation).
To represent a context tree as a matrix, we set column
i of the matrix to be the vector associated with the
i’th node of the tree (where if the node does not exist
in the tree we simply set the column to be the all zeros
vector). Similarly, we can map a sequence x1:t−1 to a
∗
vector ψ(x1:t−1 ) ∈ {0, 1}|Σ | as follows. Suppose that
we traverse from the root of a full k-ary tree according to the symbols xt−1 , xt−2 , . . . , x1 , as we described
before. Then, we set all the coordinates of ψ(x1:t−1 )
corresponding to nodes we visited in this path to be 1,
and set all the rest of the coordinates to be zero.
It is easy to verify that the vector z constructed by
a context tree for the history x1:t−1 is U ψ(x1:t−1 ),
where U is the matrix describing the context tree (the
size of U is thus |Σ| × |Σ∗ | and the columns correspond
to the vectors z at each node).

i

∑
and a norm over matrices to be ∥U∥2 = j ∥Uj ∥2 ,
where Uj is the j’th row of U. Put another way, the
squared norm of U is a weighted sum of the squared
Euclidean norms of columns of U, where the weight of
column i is ad(i) . Thus, we assign a higher penalty to
columns corresponding to deep nodes of the trees.
Consequently, we deﬁne the hypothesis class of
bounded norm context trees to be
HB = {U : ∥U∥ ≤ B} .

(6)

Finally, we also need to deﬁne scale sensitive loss functions. A common choice is the multiclass log-loss:
(∑
)
(
)
ℓ(z, y) = log
exp 1[y′ ̸=y] − zy + zy′
.
y ′ ∈Σ

This loss function has the advantages of being a convex
surrogate loss for the zero-one loss.
3.2. The LEX algorithm
We are now ready to describe our algorithm, which
we call LEX (for Learning Experts). Our goal is to
minimize the loss in Eq. 4 with respect to the vectors
wi and the parameters of the experts. As described
in Section 3.1, we parameterize each expert by a context tree matrix U ∈ Rk × |Σ∗ |. As mentioned earlier,
we can minimize Eq. 4 via alternating optimization
where minimizing over w can be done in closed form
3

Here we take ai = i−2 .

Learning the Experts for Online Sequence Prediction

and minimizing over U can be done with gradient descent. Calculating the gradient w.r.t. U is easy for
the log loss. In our implementation we use stochastic
gradient descent, where an update is performed after
each training sequence is processed.

4. Analysis
Deﬁne the generalization loss for the set of experts F :
LD (F ) = E L(F, x) .
x∼D

In light of Eq. 2, in order to bound Ex [WM(F, x)] it
suﬃces to bound LD (F ). In this section we derive
bounds on LD (F ). Our bounds depend on the following measures: the number of experts |F |, a complexity measure of the hypothesis class H, the number of
training examples, and the training loss:
LS (F ) =

1 ∑
L(F, x) .
|S|
x∈S

We ﬁrst deﬁne a complexity measure for a hypothesis
class H with respect to a loss function ℓ.
Definition 1 Let H be a class of functions from Z to
Q, let Y be a target set, and let ℓ : Q×Y → R be a loss
function. We say that the complexity of H is C(H) if
for any sequence (z1 , y1 ), . . . , (zq , yq ) ∈ (Z × Q)q and
for any ϵ > 0, there exists H′ ⊂ H of size |H′ | ≤
2
(2q)C(H)/ϵ , such that for all h ∈ H exists h′ ∈ H′
that satisfies

have LD (F ) ≤ LS (F ) + ϵ is order of r C(H)/ϵ2 . In
particular, the sample complexity of learning a set of
r experts is r times larger than the sample complexity
of learning a single expert.
The proof of the theorem is given in the long version
of this article. The main ideas of the proof are as
follows. First, we construct a cover for the loss class
{x 7→ L(F, x) : F ⊂ H, |F | = r}. Then, we bound the
Rademacher complexity of this class using a generalization of Dudley’s chaining technique, which is similar to a technique recently proposed in Srebro et al.
(2010).
Next, we turn our attention to the speciﬁc class of context trees with bounded norm. The following lemma
bounds its complexity.
Lemma 1 Let HB be the class of multiclass context
trees which maps Σ∗ into R|Σ| as defined in Section
3.1. Let ℓ : R|Σ| × Σ → R be a loss function such that
∀l ∈ Σ, u, v ∈ R|Σ| , |ℓ(u, l) − ℓ(v, l)| ≤ ∥u − v∥∞ .
Then: C(HB ) ≤ O(B 2 log(k)).
The proof of the lemma is given in the long version
of this article. The main idea is a nice trick showing how to bound the cover of a linear class based on
known bounds on the convergence rate of sub-gradient
mirror descent algorithms (e.g., see Nemirovski and
Yudin, 1978). This is similar to a method due to Zhang
(2002), although our bound is slightly better.

∀i ∈ [q], |ℓ(h(zi ), yi ) − ℓ(h′ (zi ), yi )| ≤ ϵ .

The multiclass log-loss function satisﬁes the conditions
of the above lemma, hence:

The reader familiar with covering number bounds can
easily recognize C(H) as determining the size of a cover
of H. It is also easy to verify that if H is a class of
binary classiﬁers then C(H) is upper bounded by the
VC dimension of H (this follows directly from Sauer’s
lemma). We will later show that the class of bounded
norm context trees has a bounded C(H) as well.

Corollary 1 Let HB be the class of multiclass context
trees and let ℓ be the multiclass log-loss. Let D be a
probability over Σ∗ such that there exists some constant
T with P[len(x) ≤ T ] = 1. Then, with probability of at
least 1−δ over S ∼ Dm , for all F ⊂ HB , with |F | = r,
we have
(√
)
r B2
LD (F ) ≤ LS (F ) + Õ
.
m

Theorem 2 Let D be a probability over Σ∗ such that
there exists some constant T with P[len(x) ≤ T ] =
1. Assume also
√ that for all x and F ⊂ H we have
L(F, x) ∈ [0, C(H)]. Then, with probability of at
least 1 − δ over S ∼ Dm , for all F ⊂ H, with |F | = r,
we have
)
(√
r C(H)
.
LD (F ) ≤ LS (F ) + Õ
m
The above theorem tells us that if H is of bounded
complexity, then the number of samples required to

In summary, if we manage to ﬁnd a set F ⊂ HB of size
r that achieves a small hindsight training loss, then it
will also achieve a small hindsight generalization loss.
Combining this with Eq. 2 yields
(√
)
√
r B2
4 log(r)
E[WM(F, x)] ≤ LS (F ) + Õ
+E
.
m
T
Therefore, the performance of the Weighted-Majority
algorithm is upper bounded by three terms: The training loss of F (which can decrease when increasing r),

Learning the Experts for Online Sequence Prediction

the estimation error term (which increases with r), and
the online regret term (which also increases with r).

5. Related Work
The problem of sequence prediction has a fairly long
history and has received much attention from game
theorists (Robbins, 1951; Blackwell, 1956; Hannan,
1957), information theorists (Cover and Hart, 1967;
Cover and Shenhar, 1977; Feder et al., 1992; Willems
et al., 1995), and machine learning researchers (Helmbold and Schapire, 1997; Pereira and Singer, 1999;
Cesa-Bianchi and Lugosi, 2006; Dekel et al., 2010).
One of the most useful tools is context trees, which
store informative histories and the probability of the
next symbol given these. However, all of these works
consider predicting a sequence from a single source.
Indeed, our work extends these single sequence predictions to the collaborative setting where we model different sequences, but constrain the predictors to share
some common structure (i.e., the experts used in prediction).
Another related line of work is multitask prediction
(e.g., see Ando and Zhang, 2005; Abernethy et al.,
2007), in which one considers several diﬀerent multiclass prediction problems and seeks a common feature
space for those. This setting is diﬀerent from ours
in several ways. First, in the multitask setting one receives a set of training instances from each task, where
it is known which sample belongs to each class. In our
case, we receive only a set of individual sequences. Furthermore, in the multitask setting, the test data comes
from one of the known tasks, whereas we again receive
a novel sequence from an unknown source.
A more recent approach to sequence modeling is the
“sequence memoizer”, which is based on nonparametric Bayesian models (Wood et al., 2009). So far these
have been applied to a single type model (e.g., language modeling), and not for multiple distinct models as we have here. It is conceivable that a fully
Bayesian model for collaborative sequence prediction
can be built using these models, and it would be interesting to contrast it with our approach.
Another possible approach to the problem is to use
probabilistic latent variable models (Hofmann, 1999)
or their discriminative counterparts (Felzenszwalb
et al., 2008; Yu and Joachims, 2009). Here each sequence will be mapped to a latent variable corresponding to the best expert. Next, given the class and
the previous history, a probabilistic suﬃx tree will be
used to generate the next action. However, such a
model will not handle long histories appropriately and

is likely to result in overﬁtting (as our empirical results
also show). While it may be possible to add history
discounting to such a model, it will be considerably
more complex than what we suggest here.
In our formulation, the state space Σ is unstructured.
There are cases of interest, where Σ has structure. For
example, it may correspond to the items in an online
shopping basket. Prediction in such a setting was recently addressed in Rendle et al. (2010). Unlike in our
case, they have access to multiple training sequences
from particular users, and prediction is done on these
users. Furthermore, the temporal model itself is only
ﬁrst order and thus very diﬀerent from ours. Note
that we can easily extend our approach to structured
state spaces by using structured prediction instead of
multiclass as we do here.

6. Experiments
In what follows, we evaluate the performance of the
LEX algorithm (see Section 3.2) on two datasets: synthetic and real-world. We compare it to the baselines
described below.
6.1. Baselines Models
We consider three diﬀerent baselines models. The ﬁrst
is our LEX algorithm with r = 1 (we denote this baseline by 1-LEX), which is in fact a batch trained PST
(where training uses the log loss). In this approach
all training sequences are modeled via a single PST
corresponding to one expert. It thus does not directly
model multiple temporal behaviors of the sequences in
the data.
Our second baseline is an online PST model which is
evaluated on each test sequence individually. Training
is done using the algorithm in (Dekel et al., 2010). Being an online algorithm, it does not use the training
data. However, given long enough sequences it will be
able to model any deterministic temporal behavior optimally. In other words, this algorithm has the beneﬁt
of adaptation but its performance crucially depends
on the length of the sequence. We denote this baseline
by Online PST.
Finally, we consider a generative latent variable model
(denoted by LMM) which is a mixture of Markov
chains. An order d Markov chain is a basic yet powerful tool for modeling sequences. In LMM we generalize Markov chains by allowing each sequence to be generated by one of r regular Markov models. We think
of these r models as diﬀerent chain types similarly
to the r experts of LEX. Speciﬁcally, for a sequence
x1 , . . . xt−1 the r-LMM model of order d is deﬁned by:

Learning the Experts for Online Sequence Prediction

6.2. Synthetic Data
We begin by considering sequences that follow one of
two temporal patterns. The sequences are generated
as follows: First randomly select j ∈ {1, 2} then draw
T samples according
to the (independent) distribution:
{
2−1
if x = j
Pr(xt = x) =
. We used
−1
(2(|Σ| − 1))
otherwise
|Σ| = 200 and generated a set of m = 1000 sequences,
each of length T = 250 (these parameters where selected to resemble the browsing data characteristics).
We note that by construction, the maximal possible
generalization accuracy on this data is 0.5. We evaluate the accuracy of online prediction on 400 test sequences.
0.5
0.45

Accuracy

0.4
0.35
0.3
1-LEX
LEX
LMM
Online PST

0.25
0.2
0

100

200
300
Sample Size

400

500

Figure 1. Test accuracy of online prediction on the synthetic data. See Section 6.2. The four algorithms are described in Section 6.1.

In Fig. 1 we show the accuracy (on test data) of
LEX and the three baselines. We notice that LEX
approaches 0.5 accuracy using about 50 sequences,
1-LEX and LMM require substantially more samples
in order to approach this performance (over 500 sequences for 0.45 accuracy). In other words, in agreement with our theoretical analysis, the sample complexity of LEX is smaller than both 1-LEX and
LMM. The accuracy of online PST is much lower

0.7
0.65

Accuracy

def ∑r
Pr(xt |x1:t−1 ) =
q=1 P(xt |xt−d:t−1 , z = q) P(z = q)
Where z is the latent (unobserved) variable which ”assigns“ a chain type to a sequence. Note that the standard Markov chain is simply a 1-LMM. We learn the
parameters of a LMM from training data using EM.
The (online) prediction using this model is done by
the maximum a-posteriori assignment at each point in
time. Since LMM does not discount long histories, it
is not expected to perform well when d is large and
not enough training data is available. Parameters for
all algorithms (i.e., r and d) were tuned using cross
validation.

0.6
0.55
0.5

1-LEX
LEX
LMM
Online PST

0.45
0

100

200
300
Sample Size

400

500

Figure 2. Test accuracy of online prediction on the click
prediction task. See Section 6.3. The four algorithms are
described in Section 6.1.

due to the conservative training of this algorithm.
6.3. Click Prediction Data
Here we consider a challenging task of predicting the
browsing pattern of web users. Speciﬁcally, we use
browsing logs for users in an intra-net site. For each
session the sequence of url s visited by every user was
recorded by the web server. The dataset contains 2000
such sequences of length 70-150. The domain of the
prediction problem, is of distinct url s and its magnitude is |Σ| = 189. The data was split into train,
validation and test sets, the sizes of the training sets
vary, while the validation and test set sizes were ﬁxed
at 200 and 800 sequences respectively. We applied
the three baseline models, and compared their performance to LEX. In this experiment the r experts
learned by LEX were combined with an additional
expert obtained from training a 1-LEX algorithm, resulting in a pool of r+1 learned experts. This addition
smoothes performance on short sequences where the
WM algorithm might not have enough time to decide
which of the r experts to follow.
Results are shown in Fig. 2. It can be seen that LEX
outperforms the other methods. When considering the
diﬀerence in accuracy between LEX and 1-LEX we
notice that the added accuracy from multiple experts
shrinks as training size increases. This trend agrees
with theory, since as more data is available to 1-LEX,
it can use longer histories and eventually will be able to
model any temporal behavior. However, as we show in
the synthetic experiments, the gap for small data sizes
can be considerable.

Learning the Experts for Online Sequence Prediction

7. Discussion
We have described and analyzed a method for learning
the experts for online sequence prediction. In particular, we speciﬁed it to the class of prediction suﬃx
trees. Thus, our experts can capture dependencies on
arbitrarily long histories. This is achieved by mapping context trees into a vector space and designing
a norm on this space which discounts long histories.
As our generalization results show, the complexity of
the model is not penalized by the maximal possible
length of histories (dimensionality of the matrix U)
but rather by the eﬀective needed context based history (captured by the norm of U). Our empirical results show that temporal user speciﬁc structure can
indeed be used to improve prediction accuracy.
The proposed approach can be extended in several
ways. First, we can consider diﬀerent prediction goals:
instead of predicting the next symbol in the sequence,
corresponding to the next URL, we can have a binary
classiﬁer that returns one if a user is likely to take a
given action and zero otherwise. Alternatively, we can
consider a ranking task where we want to sort actions
according to their interest to the user. To use such
objectives we will just need to replace our multiclass
log loss with the corresponding loss.
Finally, we note that our model can be applied to
a wide array of practical problems. Some examples
are ad placements, course enrollment systems, and enhanced user interface automation.
Acknowledgements: This research is supported by
the HP Labs Innovation Research Program.



The introduction of loopy belief propagation
(LBP) revitalized the application of graphical
models in many domains. Many recent works
present improvements on the basic LBP algorithm in an attempt to overcome convergence and
local optima problems. Notable among these
are convexified free energy approximations that
lead to inference procedures with provable convergence and quality properties. However, empirically LBP still outperforms most of its convex variants in a variety of settings, as we also
demonstrate here. Motivated by this fact we
seek convexified free energies that directly approximate the Bethe free energy. We show that
the proposed approximations compare favorably
with state-of-the art convex free energy approximations.

1

Introduction

Computing likelihoods and marginal probabilities is a critical subtask in applications of graphical models. As computing the exact answers is often infeasible, there is a growing need for approximate inference methods. An important
class of approximations are variational methods [7] that
pose inference in terms of minimizing the free energy functional. In the last decade, loopy belief propagation (LBP), a
simple local message passing procedure, proved to be empirically successful and was used in a variety of applications [10]. The seminal work of Yedidia et al. [20] merged
these lines of work by formulating loopy belief propagation
in terms of optimizing the Bethe free energy, an approximate free energy functional.
LBP suffers from two inherent problems: it fails to converge in some cases, and may converge to local optima due
to the non-convexity of the Bethe free energy. Several approaches have been introduced to fix the non-convergence
issue, so that LBP provably converges to a local optimum
of the Bethe free energy [17, 22]. However, this still leaves

the problem of local optima, and therefore the dependence
of the solution on initial conditions. To alleviate this problem, several works [2, 6, 13, 15] construct convex free energy approximations, for which there is a single global optimum. Convexity also paved the way for the introduction of
provably convergent message-passing algorithms for calculating likelihood and marginal probabilities [3, 4]. Moreover, some of these approximations provide upper bounds
on the partition function [2, 13].
Despite their algorithmic elegance and convergence
properties, convex variants often do not provide better empirical results than LBP. While this observation is shared
by many practitioners, it does not have firm theoretical justification. Motivated by this observation, our goal in this
work is to construct approximations that are both convex
and directly approximate the Bethe free energy. We show
how to approximate the Bethe in L2 norm and how to find
the best upper bound on it for a given random field. We
then illustrate the utility of our proposed approximations
by comparing them to previously suggested ones across a
variety of models and parameterizations.

2

Free Energy Approximations

Probabilistic graphical models provide a succinct language
to specify complex joint probabilities over many variables.
This is done by factorizing the distribution into a product
over local potentials. Let x ∈ X n denote a vector of n discrete random variables. Here we focus on Markov Random
Fields where the joint probability is given by:
(
)
X
X
1
p(x; θ ) =
exp
θα (xα ) +
θi (xi )
(1)
Z(θθ )
α
i
where α correspond to subsets of parameters (or factors),
and Z(θθ ) is the partition function that serves to normalize the distribution. We denote marginal distributions over
variables and factors by µi (xi ) and µα (xα ), respectively,
and the vector of all marginals by µ.
Given such a model, we are interested in computing the
marginal probabilities µ , as well as the partition function

UAI 2009

MESHI ET AL.

Z(θθ ), which is required for calculating the likelihood of evidence, especially in the context of parameter estimation.
Finding exact answers for these tasks is theoretically and
practically hard and thus many works often resort to approximate inference.
A class of popular approximate inference approaches
are the variational methods that rely on the following exact formulation of log Z(θθ ) [14]:
n
o
µ)
(2)
log Z(θθ ) =
max θ T µ + H(µ
µ ∈M(G)

The set M(G) is known as the marginal polytope associated with a graph G [14]. A vector µ is in M(G) if it
corresponds to the marginals of some distribution p(x):


µi (xi ) = p(xi )
M(G) = µ ∃p(x) s.t.
(3)
µα (xα ) = p(xα )
µ) is defined as the entropy of the unique exponential
H(µ
distribution of the form in Eq. (1) consistent with marginals
µ . Finally, the objective in Eq. (2) is the negative of the free
µ, θ ].
energy functional, denoted F [µ
The solution to the optimization problem in Eq. (2) is
precisely the desired vector of marginals of the distribution
p(x; θ ).
In itself, this observation is not sufficient to provide an
efficient algorithm, since the maximization in Eq. (2) is as
hard as the original inference task. Specifically, M(G) is
µ) is
difficult to characterize and the computation of H(µ
also intractable, so both need to be approximated. First,
one can relax the optimization problem to be over an outer
bound on the marginal polytope. In particular, it is natural
to require that the resulting pseudo-marginals obey some
local normalization and marginalization constraints. These
constraints define the local polytope
P


xi µi (xi ) = 1
P
µ
L(G) =
≥0
(4)
xα \xi µα (xα ) = µi (xi )
As for the entropy term, a family of entropy approximations with a long history in statistical physicsPis based on a
µ) = r cr Hr (µr ),
weighted sum of local entropies Hc (µ
where r are subsets of variables (regions) and the coefficients cr are called counting numbers [21]. The approximate optimization problem then takes the form:
n
o
µ)
log Z̃(θθ ) = max θ T µ + Hc (µ
(5)
µ ∈L(G)

The entropy approximation is defined both by the choice
of regions and by the choice of counting numbers. This
poses two complementary challenges: defining the regions,
and assigning counting numbers for these regions. Here we
focus on the second problem, which arises for any choice
of regions. For simplicity, we limit ourselves to a common
choice of regions — over variables and factors, although

403
the results to follow can be generalized to more elaborate
region choices (e.g., [16, 18]). In this case the approximate
entropy takes the form:
µ) =
Hc (µ

X

ci Hi (µi ) +

X

cα Hα (µα )

(6)

α

i

where ci and cα are the counting numbers for variables and
factors, respectively.
Each set of counting numbers will result in a different
µ)
approximation. The Bethe entropy approximation Hb (µ
is defined by choosing cα = 1, ci = 1 − di (where di =
|{α : i ∈ α}|) [21].
Concave Entropy Approximations
One shortcoming of the Bethe entropy is that it is not concave, and thus Eq. (5) may have local optima. It is possible
to consider instead entropy approximations that are provably concave. Such approximations have been studied extensively in recent years, along with provable convergent
algorithms for solving Eq. (5) in these cases. One of the
first concave entropy approximations introduced was the
tree-reweighting (TRW) method of Wainwright et al.[13].
The TRW entropy approximation is a convex combination
of tree entropies and is concave. Furthermore, it is an upµ) so that the optimum of Eq. (5)
per bound on the true H(µ
yields an upper bound on log Z(θθ ).
More recently, Heskes [5] derived a set of sufficient condition for cα , ci to yield a concave function. He showed
µ) is provably concave
that an entropy approximation Hc (µ
for µ ∈ L(G) if there exist auxiliary counting numbers
cαα , cii , ciα ≥ 0 such that
cα

= cαα +

X

ciα

∀α

(7)

ciα

∀i

(8)

i:i∈α

ci

= cii −

X
α:i∈α

3

Message Passing Algorithms

The optimization problem in Eq. (5) can be solved using
generic optimization tools. However, message passing algorithms have proved especially useful for this task. Starting with the work of Yedidia et al.[20] many message passing algorithms have been proposed for optimizing variational approximations. Although not all these algorithms
are provably convergent, if they do converge, it is to a fixed
µ) is conpoint of Eq. (5). Furthermore if the entropy Hc (µ
cave, this is the global optimum of Eq. (5).
Most existing algorithms make the assumption that
cα = 1. Since in this work we want to explore a broader
range of approximations, we derive message passing updates for the more general case (for any cα 6= 0). Our
derivation, which follows closely the one of Yedidia et al.,
results with the following update rules:

404

MESHI ET AL.

UAI 2009
that these models are optimized correctly? It is easy to see
that if c satisfies:
X
ci +
cα = 1
∀i
(11)
α,i∈α

µ) will solve the factorized
then the corresponding Hc (µ
model exactly. We call c values that obey Eq. (11) variableµ) [21].
valid, as the variables are counted correctly in Hc (µ
In a similar way we can define factor-valid approximations that satisfy:
Figure 1: Illustration of the counting number space with different
regions and point of interest labeled (see text).

mα→i (xi )
ni→α (xi )
Where qi =

cα

qi −cα

qi −1
cα −qi +1

1
cα −qi +1

= m0α→i (xi ) cα −qi +1 n0i→α (xi ) cα −qi +1
= m0α→i (xi )
1−ci
di

m0α→i (xi )

n0i→α (xi )

(10)

and:

=

X

1

e cα θα (xα )

xα \xi

n0i→α (xi )

(9)

= eθi (xi )

Y

nj→α (xj )

j ∈α
j 6= i

Y

mβ→i (xi )

β :i∈β
β 6= α

Note that by plugging in the Bethe counting numbers,
where cα = 1 and ci = 1 − di , this reduces back to the
standard BP messages. Furthermore, if we set cα = 1 as in
Yedidia et al. [21], then Eq. (9) and Eq. (10) reduce to the
two-way algorithm defined there.
The above updates are not guaranteed to converge even
µ) is concave. However, we have found that with
if Hc (µ
dampening of messages it did converge for all the cases we
studied.

4

Properties of Counting Number Space

Given a specific model, different choices of counting numbers lead to different entropy approximations. But what
are good counting numbers and how can we find those for
a specific model?
One desirable property of counting numbers is that they
result in concave entropies, as discussed in Section 2.
There are several rationales for choosing those. One is
clearly that optimization is globally optimal. The other is
µ) is itself concave [14].
that the true entropy H(µ
Another approach to deriving good counting numbers is
µ)
to restrict ourselves to c such that optimization with Hc (µ
is exact for at least some values of θ . For example, suppose
we have a model where θα (xα ) = 0 for all α (i.e., a completely factorized model). How should we constrain c such

cα = 1

∀α

(12)

Intuitively, approximations that satisfy both Eq. (11) and
Eq. (12) are valid [21] as they have the appealing property of not over- or under-counting variables and factors
µ). Furthermore, for tree
in the approximate entropy Hc (µ
structured distributions it has been shown that only valid
counting numbers can yield exact results [11].
Figure 1 illustrates the structure of the above constraints
in the space of counting numbers. Note that the Bethe approximation is the single choice of counting numbers that
is both factor- and variable-valid. The TRW approximation
is, by definition, always variable-valid, and any distribution
over spanning trees results in a different value of cα . Finally, we note that for different model structures the counting number space looks different. For example, the Bethe
approximation for tree structured distributions is convex.
To better understand the properties of different counting
numbers, we perform an experiment where the counting
number space is two dimensional and can be visualized.
For this we use 5 × 5 toroidal grids in which each variable
is connected to four neighboring variables by pairwise factors. The joint probability n
distribution of the model is given
o
P
P
1
by: p(x; θ ) = Z(θ
exp
i θ i xi +
(i,j)∈G θi,j xi xj
θ)
with xi , xj ∈ {±1}. The field parameters θi were drawn
uniformly from the range [−ωF , ωF ], and the interaction parameters θi,j were drawn either from the range
[−ωI , ωI ] or from [0, ωI ] to obtain mixed or attractive potentials respectively [2, 13]. This model structure has inherent symmetry as all factors are pairwise and each variable
appears in exactly four factors. Hence, if we choose the
counting numbers of the approximation based solely on the
structure of the model, we get the same ci for all variables
and the same cα for all factors.
Figure 2 shows the performance of various approximations on two models. The first model is sampled in an easy
regime with relatively weak interaction parameters while
the second model is sampled from a more difficult regime
with stronger interaction parameters.
We observe that most convex free energy approximations have large errors both in the estimate of the logpartition and in that of the marginal beliefs. When looking at subspaces that tend to empirically perform better

UAI 2009

MESHI ET AL.

405

Figure 2: Quality of different approximations for two instances of the 5 × 5 toroidal grid from different parametric scenarios. In each
matrix the x-axis denotes the counting number for nodes (ci ) and the y-axis denotes the counting number for edges of the grid (cα ).
Each pixel in the matrix is the result of running belief propagation with these counting numbers. The subspace of provably convex
approximations is bound by solid lines, and the variable-valid subspace is marked by a dotted line. The left column shows the error in
approximate log-partition function (| log Z(θθ ) − log Z̃(θθ ) |), the middle column shows the average L1 error in approximate marginals
over factors and variables. The rightmost column shows in more detail the approximation quality in the variable-valid subspace. The
colored stars show various approximations (see Figure 1 and text).

than others, the convex subspace does not seem to generally give good approximations. However, we notice that
variable-valid approximations stand out as the main region of relatively low error. In fact, we note that to the
best of our knowledge all free energy approximations suggested in the literature obey this variable-valid constraint
[2, 4, 13, 19, 21].
The rightmost column of Figure 2 shows performance
of variable-valid approximations. We notice that for almost all models tested the approximation improves as the
counting numbers get closer to the Bethe counting numbers. However, in most cases the Bethe approximation
outperforms its convex counterparts. We obtained similar
results for fully connected graphs and other non-pairwise
models (not shown).

of the model, and adaptive approximations that also take
the model parameters into account.
5.1

Static Approximations

Following the insights of the previous section, it is natural
to try to combine the convexity constraints of Eq. (7) and
Eq. (8) with the validity constraints defined by Eq. (11).
Inside this subspace, a straightforward choice is to find the
counting numbers that are closest in terms of Euclidean distance to the Bethe counting numbers. For clarity we denote
by b the vector of Bethe counting numbers with bi = 1−di
and bα = 1. We define the convexBethe-c approximation
as the solution to the optimization problem
2

argmin kc − bk

(13)

c

5

Approximating Bethe

The experiments in the previous section demonstrate that
the Bethe free energy performs well across a variety of parameter settings. Since we would like to work with convex
free energies, a key question is which of the convex free
energies comes closest to the performance of Bethe.
In what follows, we describe several approaches to obtaining counting numbers that satisfy the above requirements. We divide these into static approximations that determine the counting numbers based only on the structure

s.t. ci , cα satisfy Eq. (7,8,11).
This constrained optimization problem can be formulated
as a quadratic program and solved using standard solvers.
A similar approach was recently studied by Hazan and
Shashua [4].
However, it is not clear that the L2 metric in counting
number space is adequate for approximating Bethe. In principle we would like to approximate the Bethe entropy itself
µ) is a funcrather than its counting numbers. Since Hb (µ
µ) that is
tion of µ we would like to find a function Hc (µ

406

MESHI ET AL.

UAI 2009

Figure 3: Comparison of estimation errors the partition function (left column) and the marginal probabilities (right column) for several
approximation schemes. In the first row we take ωF = 0.05 and attractive potentials, and in the second row ωF = 1 and mixed
potentials. In each graph the x-axis corresponds to the interaction meta-parameters ωI , and the y-axis shows the error in the log partition
estimation (left column) and marginal estimation (right column). Each line describes the average errors over 20 5 × 5 grid models
sampled with the corresponding meta-parameters. The TRW approximation in this graph corresponds to the uniform distribution over
four spanning trees.

closest to it when integrating over all µ values. We can put
this formally as:
Z
µ) − Hc (µ
µ))2 dµ
µ
argmin
(Hb (µ
(14)
c

L(G)

s.t. ci , cα satisfy Eq. (7,8).
We integrate over L(G) since this is the optimization range
in Eq. (5) and thus the relevant domain of approximation.
Although this integration seems daunting, we can simplify
µ) and Hc (µ
µ) can be
the problem by noticing that Hb (µ
written as bT Hµ and cT Hµ respectively, where Hµ is
the vector of local entropies. This results in the following
quadratic optimization problem:
argmin(b − c)T A(b − c)

(15)

c

s.t. ci , cα satisfy Eq. (7,8).
where

Z
A=

µ
Hµ HµT dµ

L(G)

is the matrix of integration of all pairwise products of local entropy terms. Exact calculation of A is intractable
and so we resort to MCMC methods1 , by performing a random walk inside L(G). Starting with a random point inside
1
Volume computations over such polytopes are generally difficult, but in some special cases may be solved in closed form [8].

L(G) (i.e., a set of consistent marginals µ ) we sample a legal direction, find the two boundaries along this direction,
and then sample uniformly a new point from within the
bounded interval. A straightforward argument shows that
the stationary distribution of this walk is uniform within
L(G). To determine when the random walk is close to the
stationary distribution, we apply a heuristic convergence
test by running in parallel several chains from different random starting points and comparing their statistics [1]. Once
we determine convergence, we then use samples from the
different runs to estimate A. Finally, we solve the optimization problem in Eq. (15) with and without enforcing
the variable-valid constraints of Eq. (11), and term these
approximations convexBethe-µvv and convexBethe-µ , respectively.
We evaluate the quality of these approximations for calculating the marginals and partition function in 5 × 5 nontoroidal grids with various parameterizations. We compare their performance with the Bethe approximation and
the TRW approximation using a uniform distribution over
four spanning trees (see [13]) 2 . Following Wainwright et
al. [13], in each trial we set ωF to a fixed value and gradually increase the interaction strength ωI . For each com2
We also used uniform TRW weights over all spanning trees
and got similar results (not shown).

UAI 2009

MESHI ET AL.

407

Figure 4: Comparison of estimation errors in partition function and marginals of the adaptive free energy approximations. The experimental setting is similar to that of Figure 3.

bination of ωF and ωI we sample 20 random models and
measure the estimation errors of each approximation (see
Figure 3).
We first observe that the Bethe approximation outperforms its convex counterparts in terms of partition function approximation under all settings. However we see
that when the field meta-parameter is small (ωF = 0.05)
and the interaction meta-parameter is large (ωI ≥ 0.8),
the convex approximations do better than Bethe in terms
of marginal probabilities estimates. These results are consistent with previous studies [2, 4, 13].
Among the convex free energies optimal L2 approximation of the Bethe free energy (convexBethe-µ ) does better
than optimal L2 approximation of the Bethe counting numbers (convexBethe-c ) in most of the range of parameters.
convexBethe-µ does not perform well in the low interaction regime (small ωI ). This is presumably due to the fact
that it is not forced to be variable-valid, and thus will not
be exact for independent (or close to independent) models.
When averaging across all parameter settings,
convexBethe-µ yields the best performance among
the convex approximations. We conclude that if one seeks
a convex c that performs across a range of parameters, it is
advantageous to approximate the Bethe entropy function
rather than its counting numbers. However, this comes at
a price of lower approximation quality in some regimes.
Regarding computation time, both heuristics require extra
calculations. We note however, that as the computation
does not depend on the model parameters, these extra cal-

culations need to be performed only once for each model
structure. Once the counting numbers are determined, the
optimization of the approximate free energy is exactly the
same as in standard BP. In addition, the performance of the
convexBethe-µ approximation depends on the quality of
the estimation of the matrix A in Eq. (15). This introduces
a trade-off between the cost of the MCMC simulation and
the quality of the approximation, which can be controlled
by the MCMC convergence threshold.
5.2

Adaptive Approximations

The approximations we examined so far were based on the
structure of the model alone, and were not tuned to its parameters. Intuitively, a good approximation should assign
more weight to “stronger” interactions than to weaker ones.
Indeed, Wainwright et al. [13], introduce a method for finding the optimal weights in TRW (denoted TRW-opt). The
µ) and
TRW entropy upper bounds the true entropy H(µ
thus the corresponding variational approximation in Eq. (5)
results in an upper-bound on the true partition function.
Wainwright et al. thus seek counting numbers that minimize this upper bound.
Here we present a different and simpler approach for
adaptively setting the counting numbers. As in the previous section, our motivation is to approximate the performance of the Bethe approximation via convex free energies. One strategy for doing so is to consider only c where
µ) ≥ Hb (µ
µ). This implies that the optimum in Eq. (5)
Hc (µ
will always upper bound the Bethe optimum. To come as
close as possible to the Bethe optimum, we can then min-

408

MESHI ET AL.

UAI 2009

Figure 5: Comparison of partition function and marginal probabilities estimates for several free energy approximations. In each panel
the x-axis spans values of ωF and the y-axis spans values of ωI (attractive setting). Each pixel shows the difference between average
absolute errors over 20 random trials for these meta-parameters. That is, red pixels show parameterizations where the error of the first
approximation is larger, and blue pixels show parameterizations where the error of the second approximation is larger.

imize the optimum of Eq. (5) over the counting numbers
c.
How can we constrain c to upper bound the Bethe free
energy? It turns out that there is a simple condition on c
that achieves this, as we show next.

µ, θ ] be free energy approximation
Proposition 5.2: Let F̃c [µ
µ, θ ] is:
with c ∈ Cvv . The subgradient of maxµ F̃c [µ

Proposition 5.1: If c ∈ Cvv then the difference between the
approximate free energies can be written as:

µ, θ ].
where µ ∗ maximizes F̃c [µ

µ) − Hb (µ
µ) =
Hc (µ

X

(1 − cα )Iα (µα )

α

Where
Cvv is the variable-valid subspace and Iα (µα ) =
P
H
i∈α i (µi )−Hα (µα ) is the multi-information of the distribution µα (see also [9]).
Since Iα (µα ) ≥ 0 always holds, we get that if cα ≤ 1 for
µ) is an upper bound on the Bethe entropy.
all α, then Hc (µ
This property is not only sufficient but also necessary if we
want to find counting numbers for which the bound holds
regardless of µ . Note that the TRW counting numbers satisfy this constraint and thus the TRW approximation is also
an upper bound on the Bethe free energy. Finally, we notice
that due to the convexity constraints (Eq. (7) and Eq. (8))
the resulting entropy approximation is always non-negative
(unlike Bethe).
We now show how to minimize the upper bound on the
Bethe free energy. For this we generalize a result of Wainwright et al.[13]:

µ, θ ]
∂ maxµ F̃c [µ
= −Iα (µ∗α )
∂cα

Given this gradient, we can use a conditional gradient alµ, θ ] over the set
gorithm as in [13] to minimize maxµ F̃c [µ
of c that gives an upper bound on Bethe. Our algorithm is
identical to TRW-opt except for the choice of search direction within the conditional gradient algorithm. In TRW-opt
this involves finding a maximum weighted spanning tree
while in our case it involves solving a LP (argminc −cT I
subject to the constraints). We denote the result of this optimization process by convexBethe-u . Empirically we find
that this method is faster than the TRW iterative optimization algorithm and requires less calls to the inference procedure. More importantly, while finding the optimal counting
numbers in TRW is computationally hard for non-pairwise
models [14], our method is naturally applicable in the more
general setting.
To evaluate the above adaptive strategy, we compare it
with the convexBethe-c approximation and with the Bethe
approximation in the same setting we used for the static approximations (see Figure 4). In addition, Since the choice
of the field meta-parameter ωF greatly influences the relative performance of the approximation we conduct experiments to better understand its role. Instead of fixing the

UAI 2009

MESHI ET AL.

field meta-parameter and varying the coupling strength we
plot a two-dimensional map where both meta-parameters
are free to change (see Figure 5).
As we can see in Figure 4 both adaptive heuristics improve on the static ones. Moreover, our convexBethe-u
procedure is often more accurate than TRW-opt. Yet, both
adaptive methods are still inferior to Bethe approximation
for most models we tested, except for the particular regions
we discuss above. More extensive comparison for different choices of parameters (Figure 5) reinforces the observation that the accuracy of the approximations differ under various model parameters. This suggests that given the
model structure, there is no single “best” choice of counting numbers which is better under all parametric settings.
We do see, however, that the Bethe approximation gives
better or equivalent estimates of the log-partition function
compared to the convex approximations (negative values
in the map) except for the region with a very weak field
meta-parameter (ωF ≈ 0) and a strong interaction metaparameter (ωI > 1.5). Furthermore, we see that the advantage of convex approximations over Bethe in marginals
estimation is also in the region where ωF is weak and ωI is
strong.
To examine the generality of these observations, we repeated the experiments described here for models with a
structure of fully connected graph over 10 nodes (see Appendix A). These dense models differ from the sparse grid
structured models, yet we get very similar results. We also
conducted similar experiments for smaller and larger grids
(see Appendix A) and for models with non-pairwise potentials (not shown), again with very similar results. We therefore believe that the conclusions we draw here are valid for
a wide range of models.

6

Discussion

The study of convex free energies was originally motivated
by the realization that loopy belief propagation was optimizing the non-convex Bethe free energy. It thus set out to
alleviate the non-convexity problem in the Bethe optimization procedure, and indeed has resulted in elegant algorithmic message-passing solutions for convex free energies.
Another interesting application of convex free-energies was
for optimizing Bethe (or Kikuchi) free energies via sequences of local convex approximations [6]. Although this
resulted in faster optimization, it still inherited the localoptima problem of the Bethe optimization. More recently,
convex free energy variants were shown to be particularly
useful in the context of model selection [12].
Despite these merits, in terms of quality of the approximation, convex free energies are still often not competitive
with Bethe and in fact result in poorer performance over
a wide range of parameter settings, as we also show here.
This leads to the natural question, which we address in this
work: what is the best convex approximation to the Bethe

409
free energy?
As we have shown, there are several approaches to this
problem, depending on whether we seek a set of counting
numbers that is independent of the model parameters, or
one that can be tuned adaptively. Our results show that
convex Bethe approximations often work better than other
schemes. For example, the counting numbers that approximate the Bethe entropy in L2 norm across all µ values often
work better than other choices. Furthermore, our adaptive
strategy for choosing the best counting numbers for a given
model often works better than other methods such as TRW.
Our adaptive procedures are also easily extendible to nonpairwise regions, unlike TRW which becomes intractable
in these cases.
One might argue that it is more reasonable to directly
µ) rather than the Bethe
approximate the true entropy H(µ
entropy. The main difficulty with this approach is that
µ) is not generally known, and thus its approximations
H(µ
are typically quite loose. For example, it is not clear how
to go about approximating it in L2 norm, as we do for
Bethe here. The Bethe free energy, on the other hand, is
tractable and as we show can be approximated in various
ways. Thus, even though we lose by not approximating the
true entropy, we gain by obtaining tighter approximations
to the Bethe entropy, which typically provides good performance.
Another conclusion from our results is that variablevalid counting numbers usually outperform non-valid ones.
One possible explanation for this fact is that they are guaranteed to give exact results for independent models. An
interesting open question is what other constraints we can
pose on counting numbers to enforce exactness in different
scenarios, and whether we can optimize over the set of such
numbers.
Acknowledgements
We thank Talya Meltzer, Shai Shalev-Shwartz, Raanan Fatal and
Yair Weiss for helpful remarks. This research was supported in
part by a grant from the Israel Science Foundation.


