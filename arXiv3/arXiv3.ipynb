{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests as rq\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import re\n",
    "from time import sleep\n",
    "import os.path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize\n",
    "def remover_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII').replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>community</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plausible reasoning from spatial observations</td>\n",
       "      <td>6</td>\n",
       "      <td>1301.2285v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Monte-Carlo Algorithm for Dempster-Shafer Be...</td>\n",
       "      <td>6</td>\n",
       "      <td>1303.5757v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Computational Complexity of Dominance and ...</td>\n",
       "      <td>6</td>\n",
       "      <td>1401.3453v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Assumptions Behind Dempster's Rule</td>\n",
       "      <td>6</td>\n",
       "      <td>1303.1518v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Generating Graphoids from Generalised Conditio...</td>\n",
       "      <td>6</td>\n",
       "      <td>1302.6852v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  community           id\n",
       "0      Plausible reasoning from spatial observations          6  1301.2285v1\n",
       "1  A Monte-Carlo Algorithm for Dempster-Shafer Be...          6  1303.5757v1\n",
       "2  The Computational Complexity of Dominance and ...          6  1401.3453v1\n",
       "3             The Assumptions Behind Dempster's Rule          6  1303.1518v1\n",
       "4  Generating Graphoids from Generalised Conditio...          6  1302.6852v1"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titulos_com_autores = pd.read_csv('titulo_doc_comunidade.csv',sep=';')\n",
    "titulos_com_autores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(571, 3)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titulos_com_autores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CoffeeLake_01\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>community</th>\n",
       "      <th>adi_author</th>\n",
       "      <th>titles_papers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>58553</td>\n",
       "      <td>Nonparametric Bayesian Logic;Constraint Proces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>6474</td>\n",
       "      <td>Learning Belief Networks in Domains with Recur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>58554</td>\n",
       "      <td>Inter-causal Independence and Heterogeneous Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>31206</td>\n",
       "      <td>A Monte-Carlo Algorithm for Dempster-Shafer Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>78646</td>\n",
       "      <td>Possibilistic Constraint Satisfaction Problems...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  community  adi_author                                      titles_papers\n",
       "0         6       58553  Nonparametric Bayesian Logic;Constraint Proces...\n",
       "1         6        6474  Learning Belief Networks in Domains with Recur...\n",
       "2         6       58554  Inter-causal Independence and Heterogeneous Fa...\n",
       "3         6       31206  A Monte-Carlo Algorithm for Dempster-Shafer Be...\n",
       "4         6       78646  Possibilistic Constraint Satisfaction Problems..."
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_idAutores_totulo = pd.read_csv('TitlesAutores_grau.csv',sep=';;')\n",
    "com_idAutores_totulo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_idAutores_totulo['titles_papers'] = com_idAutores_totulo['titles_papers'].str.split(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>community</th>\n",
       "      <th>adi_author</th>\n",
       "      <th>titles_papers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>58553</td>\n",
       "      <td>[Nonparametric Bayesian Logic, Constraint Proc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>6474</td>\n",
       "      <td>[Learning Belief Networks in Domains with Recu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>58554</td>\n",
       "      <td>[Inter-causal Independence and Heterogeneous F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>31206</td>\n",
       "      <td>[A Monte-Carlo Algorithm for Dempster-Shafer B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>78646</td>\n",
       "      <td>[Possibilistic Constraint Satisfaction Problem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  community  adi_author                                      titles_papers\n",
       "0         6       58553  [Nonparametric Bayesian Logic, Constraint Proc...\n",
       "1         6        6474  [Learning Belief Networks in Domains with Recu...\n",
       "2         6       58554  [Inter-causal Independence and Heterogeneous F...\n",
       "3         6       31206  [A Monte-Carlo Algorithm for Dempster-Shafer B...\n",
       "4         6       78646  [Possibilistic Constraint Satisfaction Problem..."
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_idAutores_totulo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando dicionário que mapeia título do artigo com identificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionario = {'titulo':'id'}\n",
    "dicionario2 = {'titulo':'id'}\n",
    "for index, row in com_idAutores_totulo.iterrows():\n",
    "    com, id_autor, titulos = row\n",
    "    for t in titulos:\n",
    "        t = remover_acentos(t)\n",
    "        dicionario2[t] = '' #teste\n",
    "        for i, r in titulos_com_autores.iterrows():\n",
    "            titulo2, com2, ids_titulo2 = r\n",
    "            titulo2 = remover_acentos(titulo2)\n",
    "            if t == titulo2:\n",
    "                dicionario[t] = ids_titulo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dicionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dicionario2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dicionario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferença entre len(dicionario) e len(dicionario2) mostra que alguns títulos vão se perder, mas que eles serão poucos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salvando em txt_autores_id\\58553_6.txt\n",
      "NonparametricBayesianLogic\n",
      "1207.1375v1:NonparametricBayesianLogic\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1375v1.txt\n",
      "ConstraintProcessinginLiftedProbabilisticInference\n",
      "1205.2635v1:ConstraintProcessinginLiftedProbabilisticInference\n",
      "abrindo arquivo txt_semRefTitulo\\1205.2635v1.txt\n",
      "salvando em txt_autores_id\\6474_6.txt\n",
      "LearningBeliefNetworksinDomainswithRecursivelyEmbeddedPseudoIndependentSubmodels\n",
      "1302.1549v1:LearningBeliefNetworksinDomainswithRecursivelyEmbeddedPseudoIndependentSubmodels\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1549v1.txt\n",
      "ARobustQuantumRandomAccessMemory\n",
      "1201.2250v1:ARobustQuantumRandomAccessMemory\n",
      "abrindo arquivo txt_semRefTitulo\\1201.2250v1.txt\n",
      "ExploringParallelisminLearningBeliefNetworks\n",
      "1302.1529v1:ExploringParallelisminLearningBeliefNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1529v1.txt\n",
      "ExploringLocalizationinBayesianNetworksforLargeExpertSystems\n",
      "1303.5438v1:ExploringLocalizationinBayesianNetworksforLargeExpertSystems\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5438v1.txt\n",
      "salvando em txt_autores_id\\58554_6.txt\n",
      "Inter-causalIndependenceandHeterogeneousFactorization\n",
      "1302.6855v1:Inter-causalIndependenceandHeterogeneousFactorization\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6855v1.txt\n",
      "ProbabilisticSemanticsandDefaults\n",
      "1304.2370v1:ProbabilisticSemanticsandDefaults\n",
      "abrindo arquivo txt_semRefTitulo\\1304.2370v1.txt\n",
      "SymmetricCollaborativeFilteringUsingtheNoisySensorModel\n",
      "1301.2309v1:SymmetricCollaborativeFilteringUsingtheNoisySensorModel\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2309v1.txt\n",
      "NonparametricBayesianLogic\n",
      "1207.1375v1:NonparametricBayesianLogic\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1375v1.txt\n",
      "ConstraintProcessinginLiftedProbabilisticInference\n",
      "1205.2635v1:ConstraintProcessinginLiftedProbabilisticInference\n",
      "abrindo arquivo txt_semRefTitulo\\1205.2635v1.txt\n",
      "SeeingtheForestDespitetheTrees:LargeScaleSpatial-TemporalDecisionMaking\n",
      "1205.2651v1:SeeingtheForestDespitetheTrees:LargeScaleSpatial-TemporalDecisionMaking\n",
      "abrindo arquivo txt_semRefTitulo\\1205.2651v1.txt\n",
      "salvando em txt_autores_id\\31206_6.txt\n",
      "AMonte-CarloAlgorithmforDempster-ShaferBelief\n",
      "1303.5757v1:AMonte-CarloAlgorithmforDempster-ShaferBelief\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5757v1.txt\n",
      "TheComputationalComplexityofDominanceandConsistencyinCP-Nets\n",
      "1401.3453v1:TheComputationalComplexityofDominanceandConsistencyinCP-Nets\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3453v1.txt\n",
      "TheAssumptionsBehindDempster'sRule\n",
      "1303.1518v1:TheAssumptionsBehindDempster'sRule\n",
      "abrindo arquivo txt_semRefTitulo\\1303.1518v1.txt\n",
      "GeneratingGraphoidsfromGeneralisedConditionalProbability\n",
      "1302.6852v1:GeneratingGraphoidsfromGeneralisedConditionalProbability\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6852v1.txt\n",
      "Rules,BeliefFunctionsandDefaultLogic\n",
      "1304.1134v1:Rules,BeliefFunctionsandDefaultLogic\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1134v1.txt\n",
      "Order-of-MagnitudeInfluenceDiagrams\n",
      "1202.3745v1:Order-of-MagnitudeInfluenceDiagrams\n",
      "abrindo arquivo txt_semRefTitulo\\1202.3745v1.txt\n",
      "salvando em txt_autores_id\\78646_6.txt\n",
      "PossibilisticConstraintSatisfactionProblemsor\"Howtohandlesoftconstraints?\"\n",
      "1303.5427v1:PossibilisticConstraintSatisfactionProblemsor\"Howtohandlesoftconstraints?\"\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5427v1.txt\n",
      "BoundsArcConsistencyforWeightedCSPs\n",
      "1401.3481v1:BoundsArcConsistencyforWeightedCSPs\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3481v1.txt\n",
      "Frominfluencediagramstomulti-operatorclusterDAGs\n",
      "1206.6844v1:Frominfluencediagramstomulti-operatorclusterDAGs\n",
      "abrindo arquivo txt_semRefTitulo\\1206.6844v1.txt\n",
      "PenaltylogicanditsLinkwithDempster-ShaferTheory\n",
      "1302.6804v1:PenaltylogicanditsLinkwithDempster-ShaferTheory\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6804v1.txt\n",
      "salvando em txt_autores_id\\81173_6.txt\n",
      "NonparametricBayesianLogic\n",
      "1207.1375v1:NonparametricBayesianLogic\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1375v1.txt\n",
      "salvando em txt_autores_id\\100228_6.txt\n",
      "NewResultsonEquilibriainStrategicCandidacy\n",
      "1306.1849v2:NewResultsonEquilibriainStrategicCandidacy\n",
      "abrindo arquivo txt_semRefTitulo\\1306.1849v2.txt\n",
      "PenaltylogicanditsLinkwithDempster-ShaferTheory\n",
      "1302.6804v1:PenaltylogicanditsLinkwithDempster-ShaferTheory\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6804v1.txt\n",
      "Syntax-basedDefaultReasoningasProbabilisticModel-basedDiagnosis\n",
      "1302.6827v1:Syntax-basedDefaultReasoningasProbabilisticModel-basedDiagnosis\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6827v1.txt\n",
      "TheComputationalComplexityofDominanceandConsistencyinCP-Nets\n",
      "1401.3453v1:TheComputationalComplexityofDominanceandConsistencyinCP-Nets\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3453v1.txt\n",
      "Possibilisticdecreasingpersistence\n",
      "1303.1510v1:Possibilisticdecreasingpersistence\n",
      "abrindo arquivo txt_semRefTitulo\\1303.1510v1.txt\n",
      "GroupActivitySelectionProblem\n",
      "1401.8151v1:GroupActivitySelectionProblem\n",
      "abrindo arquivo txt_semRefTitulo\\1401.8151v1.txt\n",
      "Plausiblereasoningfromspatialobservations\n",
      "1301.2285v1:Plausiblereasoningfromspatialobservations\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2285v1.txt\n",
      "ALogicofGradedPossibilityandCertaintyCopingwithPartialInconsistency\n",
      "1303.5727v1:ALogicofGradedPossibilityandCertaintyCopingwithPartialInconsistency\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5727v1.txt\n",
      "PossibilityandNecessityFunctionsoverNon-classicalLogics\n",
      "1302.6788v1:PossibilityandNecessityFunctionsoverNon-classicalLogics\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6788v1.txt\n",
      "salvando em txt_autores_id\\103920_6.txt\n",
      "SidesteppingtheTriangulationProbleminBayesianNetComputations\n",
      "1303.5440v1:SidesteppingtheTriangulationProbleminBayesianNetComputations\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5440v1.txt\n",
      "AMethodforSpeedingUpValueIterationinPartiallyObservableMarkovDecisionProcesses\n",
      "1301.6751v1:AMethodforSpeedingUpValueIterationinPartiallyObservableMarkovDecisionProcesses\n",
      "abrindo arquivo txt_semRefTitulo\\1301.6751v1.txt\n",
      "FastValueIterationforGoal-DirectedMarkovDecisionProcesses\n",
      "1302.1575v1:FastValueIterationforGoal-DirectedMarkovDecisionProcesses\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1575v1.txt\n",
      "IncrementalPruning:ASimple,Fast,ExactMethodforPartiallyObservableMarkovDecisionProcesses\n",
      "1302.1525v1:IncrementalPruning:ASimple,Fast,ExactMethodforPartiallyObservableMarkovDecisionProcesses\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1525v1.txt\n",
      "Incrementalcomputationofthevalueofperfectinformationinstepwise-decomposableinfluencediagrams\n",
      "1303.1502v1:Incrementalcomputationofthevalueofperfectinformationinstepwise-decomposableinfluencediagrams\n",
      "abrindo arquivo txt_semRefTitulo\\1303.1502v1.txt\n",
      "Region-BasedApproximationsforPlanninginStochasticDomains\n",
      "1302.1573v1:Region-BasedApproximationsforPlanninginStochasticDomains\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1573v1.txt\n",
      "IndependenceofCausalInfluenceandCliqueTreePropagation\n",
      "1302.1574v1:IndependenceofCausalInfluenceandCliqueTreePropagation\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1574v1.txt\n",
      "Inter-causalIndependenceandHeterogeneousFactorization\n",
      "1302.6855v1:Inter-causalIndependenceandHeterogeneousFactorization\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6855v1.txt\n",
      "SolvingAsymmetricDecisionProblemswithInfluenceDiagrams\n",
      "1302.6840v1:SolvingAsymmetricDecisionProblemswithInfluenceDiagrams\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6840v1.txt\n",
      "salvando em txt_autores_id\\135504_6.txt\n",
      "TheComplexityofPlanExistenceandEvaluationinProbabilisticDomains\n",
      "1302.1540v1:TheComplexityofPlanExistenceandEvaluationinProbabilisticDomains\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1540v1.txt\n",
      "TheComputationalComplexityofDominanceandConsistencyinCP-Nets\n",
      "1401.3453v1:TheComputationalComplexityofDominanceandConsistencyinCP-Nets\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3453v1.txt\n",
      "salvando em txt_autores_id\\140321_6.txt\n",
      "Argumentsusingontologicalandcausalknowledge\n",
      "1401.4144v1:Argumentsusingontologicalandcausalknowledge\n",
      "abrindo arquivo txt_semRefTitulo\\1401.4144v1.txt\n",
      "PossibilityandNecessityFunctionsoverNon-classicalLogics\n",
      "1302.6788v1:PossibilityandNecessityFunctionsoverNon-classicalLogics\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6788v1.txt\n",
      "salvando em txt_autores_id\\31116_80.txt\n",
      "NodeSplitting:ASchemeforGeneratingUpperBoundsinBayesianNetworks\n",
      "1206.5251v1:NodeSplitting:ASchemeforGeneratingUpperBoundsinBayesianNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5251v1.txt\n",
      "AVariationalApproachforApproximatingBayesianNetworksbyEdgeDeletion\n",
      "1206.6817v1:AVariationalApproachforApproximatingBayesianNetworksbyEdgeDeletion\n",
      "abrindo arquivo txt_semRefTitulo\\1206.6817v1.txt\n",
      "ApproximatingthePartitionFunctionbyDeletingandthenCorrectingforModelEdges\n",
      "1206.3241v1:ApproximatingthePartitionFunctionbyDeletingandthenCorrectingforModelEdges\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3241v1.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDML:AMethodforLearningParametersinBayesianNetworks\n",
      "1202.3709v1:EDML:AMethodforLearningParametersinBayesianNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1202.3709v1.txt\n",
      "OnBayesianNetworkApproximationbyEdgeDeletion\n",
      "1207.1370v1:OnBayesianNetworkApproximationbyEdgeDeletion\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1370v1.txt\n",
      "salvando em txt_autores_id\\46339_80.txt\n",
      "LearningWhyThingsChange:TheDifference-BasedCausalityLearner\n",
      "1203.3525v1:LearningWhyThingsChange:TheDifference-BasedCausalityLearner\n",
      "abrindo arquivo txt_semRefTitulo\\1203.3525v1.txt\n",
      "EfficientinferenceinpersistentDynamicBayesianNetworks\n",
      "1206.3289v1:EfficientinferenceinpersistentDynamicBayesianNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3289v1.txt\n",
      "salvando em txt_autores_id\\75914_80.txt\n",
      "ExploitingEvidenceinProbabilisticInference\n",
      "1207.1372v1:ExploitingEvidenceinProbabilisticInference\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1372v1.txt\n",
      "NodeSplitting:ASchemeforGeneratingUpperBoundsinBayesianNetworks\n",
      "1206.5251v1:NodeSplitting:ASchemeforGeneratingUpperBoundsinBayesianNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5251v1.txt\n",
      "salvando em txt_autores_id\\105236_80.txt\n",
      "QualitativePropagationandScenario-basedExplanationofProbabilisticReasoning\n",
      "1304.1082v1:QualitativePropagationandScenario-basedExplanationofProbabilisticReasoning\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1082v1.txt\n",
      "KnowledgeEngineeringforLargeBeliefNetworks\n",
      "1302.6839v1:KnowledgeEngineeringforLargeBeliefNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6839v1.txt\n",
      "IntercausalReasoningwithUninstantiatedAncestorNodes\n",
      "1303.1492v1:IntercausalReasoningwithUninstantiatedAncestorNodes\n",
      "abrindo arquivo txt_semRefTitulo\\1303.1492v1.txt\n",
      "AComparisonofDecisionAnalysisandExpertRulesforSequentialDiagnosis\n",
      "1304.2362v1:AComparisonofDecisionAnalysisandExpertRulesforSequentialDiagnosis\n",
      "abrindo arquivo txt_semRefTitulo\\1304.2362v1.txt\n",
      "EfficientEstimationoftheValueofInformationinMonteCarloModels\n",
      "1302.6794v1:EfficientEstimationoftheValueofInformationinMonteCarloModels\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6794v1.txt\n",
      "Search-basedMethodstoBoundDiagnosticProbabilitiesinVeryLargeBeliefNets\n",
      "1303.5721v1:Search-basedMethodstoBoundDiagnosticProbabilitiesinVeryLargeBeliefNets\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5721v1.txt\n",
      "AnExperimentalComparisonofNumericalandQualitativeProbabilisticReasoning\n",
      "1302.6818v1:AnExperimentalComparisonofNumericalandQualitativeProbabilisticReasoning\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6818v1.txt\n",
      "salvando em txt_autores_id\\105241_80.txt\n",
      "TradeoffsinConstructingandEvaluatingTemporalInfluenceDiagrams\n",
      "1303.1458v1:TradeoffsinConstructingandEvaluatingTemporalInfluenceDiagrams\n",
      "abrindo arquivo txt_semRefTitulo\\1303.1458v1.txt\n",
      "DynamicNetworkUpdatingTechniquesForDiagnosticReasoning\n",
      "1303.5739v1:DynamicNetworkUpdatingTechniquesForDiagnosticReasoning\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5739v1.txt\n",
      "AnExperimentalComparisonofNumericalandQualitativeProbabilisticReasoning\n",
      "1302.6818v1:AnExperimentalComparisonofNumericalandQualitativeProbabilisticReasoning\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6818v1.txt\n",
      "AStandardApproachforOptimizingBeliefNetworkInferenceusingQueryDAGs\n",
      "1302.1532v1:AStandardApproachforOptimizingBeliefNetworkInferenceusingQueryDAGs\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1532v1.txt\n",
      "KnowledgeEngineeringforLargeBeliefNetworks\n",
      "1302.6839v1:KnowledgeEngineeringforLargeBeliefNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6839v1.txt\n",
      "WhatisanOptimalDiagnosis?\n",
      "1304.1087v1:WhatisanOptimalDiagnosis?\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1087v1.txt\n",
      "salvando em txt_autores_id\\105296_80.txt\n",
      "SymbolicProbabilisticInferencewithEvidencePotential\n",
      "1303.5713v1:SymbolicProbabilisticInferencewithEvidencePotential\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5713v1.txt\n",
      "SymbolicProbabilisticInferencewithContinuousVariables\n",
      "1303.5712v1:SymbolicProbabilisticInferencewithContinuousVariables\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5712v1.txt\n",
      "AnArchitectureforProbabilisticConcept-BasedInformationRetrieval\n",
      "1304.1128v1:AnArchitectureforProbabilisticConcept-BasedInformationRetrieval\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1128v1.txt\n",
      "RefinementandCoarseningofBayesianNetworks\n",
      "1304.1138v1:RefinementandCoarseningofBayesianNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1138v1.txt\n",
      "BackwardSimulationinBayesianNetworks\n",
      "1302.6807v1:BackwardSimulationinBayesianNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6807v1.txt\n",
      "salvando em txt_autores_id\\140336_80.txt\n",
      "AnExperimentalComparisonofNumericalandQualitativeProbabilisticReasoning\n",
      "1302.6818v1:AnExperimentalComparisonofNumericalandQualitativeProbabilisticReasoning\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6818v1.txt\n",
      "BackwardSimulationinBayesianNetworks\n",
      "1302.6807v1:BackwardSimulationinBayesianNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6807v1.txt\n",
      "salvando em txt_autores_id\\105297_80.txt\n",
      "AnArchitectureforProbabilisticConcept-BasedInformationRetrieval\n",
      "1304.1128v1:AnArchitectureforProbabilisticConcept-BasedInformationRetrieval\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1128v1.txt\n",
      "salvando em txt_autores_id\\105298_80.txt\n",
      "AnArchitectureforProbabilisticConcept-BasedInformationRetrieval\n",
      "1304.1128v1:AnArchitectureforProbabilisticConcept-BasedInformationRetrieval\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1128v1.txt\n",
      "salvando em txt_autores_id\\140360_80.txt\n",
      "AnApproximateNonmyopicComputationforValueofInformation\n",
      "1303.5720v2:AnApproximateNonmyopicComputationforValueofInformation\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5720v2.txt\n",
      "KnowledgeEngineeringforLargeBeliefNetworks\n",
      "1302.6839v1:KnowledgeEngineeringforLargeBeliefNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6839v1.txt\n",
      "salvando em txt_autores_id\\39288_104.txt\n",
      "Alogicforreasoningaboutambiguity\n",
      "1401.2011v1:Alogicforreasoningaboutambiguity\n",
      "abrindo arquivo txt_semRefTitulo\\1401.2011v1.txt\n",
      "GeneratingNewBeliefsFromOld\n",
      "1302.6783v1:GeneratingNewBeliefsFromOld\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6783v1.txt\n",
      "WeightedSetsofProbabilitiesandMinimaxWeightedExpectedRegret:NewApproachesforRepresentingUncertaintyandMakingDecisions\n",
      "1302.5681v1:WeightedSetsofProbabilitiesandMinimaxWeightedExpectedRegret:NewApproachesforRepresentingUncertaintyandMakingDecisions\n",
      "abrindo arquivo txt_semRefTitulo\\1302.5681v1.txt\n",
      "ProbabilityUpdate:Conditioningvs.Cross-Entropy\n",
      "1302.1543v1:ProbabilityUpdate:Conditioningvs.Cross-Entropy\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1543v1.txt\n",
      "DefiningExplanationinProbabilisticSystems\n",
      "1302.1526v1:DefiningExplanationinProbabilisticSystems\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1526v1.txt\n",
      "ANewApproachtoUpdatingBeliefs\n",
      "1304.1119v1:ANewApproachtoUpdatingBeliefs\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1119v1.txt\n",
      "AmbiguousLanguageandDifferencesinBeliefs\n",
      "1203.0699v1:AmbiguousLanguageandDifferencesinBeliefs\n",
      "abrindo arquivo txt_semRefTitulo\\1203.0699v1.txt\n",
      "CausesandExplanations:AStructural-ModelApproach---Part1:Causes\n",
      "1301.2275v1:CausesandExplanations:AStructural-ModelApproach---Part1:Causes\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2275v1.txt\n",
      "salvando em txt_autores_id\\46282_104.txt\n",
      "Value-DirectedBeliefStateApproximationforPOMDPs\n",
      "1301.3887v1:Value-DirectedBeliefStateApproximationforPOMDPs\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3887v1.txt\n",
      "HierarchicalPOMDPControllerOptimizationbyLikelihoodMaximization\n",
      "1206.3291v1:HierarchicalPOMDPControllerOptimizationbyLikelihoodMaximization\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3291v1.txt\n",
      "ComparativeAnalysisofProbabilisticModelsforActivityRecognitionwithanInstrumentedWalker\n",
      "1203.3500v1:ComparativeAnalysisofProbabilisticModelsforActivityRecognitionwithanInstrumentedWalker\n",
      "abrindo arquivo txt_semRefTitulo\\1203.3500v1.txt\n",
      "Value-DirectedSamplingMethodsforPOMDPs\n",
      "1301.2305v1:Value-DirectedSamplingMethodsforPOMDPs\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2305v1.txt\n",
      "Vector-spaceAnalysisofBelief-stateApproximationforPOMDPs\n",
      "1301.2304v1:Vector-spaceAnalysisofBelief-stateApproximationforPOMDPs\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2304v1.txt\n",
      "salvando em txt_autores_id\\100240_104.txt\n",
      "ACaseStudyinKnowledgeDiscoveryandElicitationinanIntelligentTutoringApplication\n",
      "1301.2297v1:ACaseStudyinKnowledgeDiscoveryandElicitationinanIntelligentTutoringApplication\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2297v1.txt\n",
      "DeliberationSchedulingforTime-CriticalSequentialDecisionMaking\n",
      "1303.1491v1:DeliberationSchedulingforTime-CriticalSequentialDecisionMaking\n",
      "abrindo arquivo txt_semRefTitulo\\1303.1491v1.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SensorValidationUsingDynamicBeliefNetworks\n",
      "1303.5419v1:SensorValidationUsingDynamicBeliefNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5419v1.txt\n",
      "salvando em txt_autores_id\\75909_104.txt\n",
      "SPOOK:ASystemforProbabilisticObject-OrientedKnowledgeRepresentation\n",
      "1301.6733v1:SPOOK:ASystemforProbabilisticObject-OrientedKnowledgeRepresentation\n",
      "abrindo arquivo txt_semRefTitulo\\1301.6733v1.txt\n",
      "General-PurposeMCMCInferenceoverRelationalStructures\n",
      "1206.6849v1:General-PurposeMCMCInferenceoverRelationalStructures\n",
      "abrindo arquivo txt_semRefTitulo\\1206.6849v1.txt\n",
      "LearningProbabilisticRelationalDynamicsforMultipleTasks\n",
      "1206.5249v1:LearningProbabilisticRelationalDynamicsforMultipleTasks\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5249v1.txt\n",
      "ProbabilisticModelsforAgents'BeliefsandDecisions\n",
      "1301.3876v1:ProbabilisticModelsforAgents'BeliefsandDecisions\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3876v1.txt\n",
      "salvando em txt_autores_id\\75911_104.txt\n",
      "AcceleratingEM:AnEmpiricalStudy\n",
      "1301.6730v1:AcceleratingEM:AnEmpiricalStudy\n",
      "abrindo arquivo txt_semRefTitulo\\1301.6730v1.txt\n",
      "CAPIR:CollaborativeActionPlanningwithIntentionRecognition\n",
      "1206.5928v1:CAPIR:CollaborativeActionPlanningwithIntentionRecognition\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5928v1.txt\n",
      "LearningProbabilisticRelationalDynamicsforMultipleTasks\n",
      "1206.5249v1:LearningProbabilisticRelationalDynamicsforMultipleTasks\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5249v1.txt\n",
      "AdaptiveImportanceSamplingforEstimationinStructuredDomains\n",
      "1301.3882v1:AdaptiveImportanceSamplingforEstimationinStructuredDomains\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3882v1.txt\n",
      "DeliberationSchedulingforTime-CriticalSequentialDecisionMaking\n",
      "1303.1491v1:DeliberationSchedulingforTime-CriticalSequentialDecisionMaking\n",
      "abrindo arquivo txt_semRefTitulo\\1303.1491v1.txt\n",
      "salvando em txt_autores_id\\77029_104.txt\n",
      "BootstrappingMonteCarloTreeSearchwithanImperfectHeuristic\n",
      "1206.5940v1:BootstrappingMonteCarloTreeSearchwithanImperfectHeuristic\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5940v1.txt\n",
      "CAPIR:CollaborativeActionPlanningwithIntentionRecognition\n",
      "1206.5928v1:CAPIR:CollaborativeActionPlanningwithIntentionRecognition\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5928v1.txt\n",
      "salvando em txt_autores_id\\77030_104.txt\n",
      "MonteCarloBayesianReinforcementLearning\n",
      "1206.6449v1:MonteCarloBayesianReinforcementLearning\n",
      "abrindo arquivo txt_semRefTitulo\\1206.6449v1.txt\n",
      "CAPIR:CollaborativeActionPlanningwithIntentionRecognition\n",
      "1206.5928v1:CAPIR:CollaborativeActionPlanningwithIntentionRecognition\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5928v1.txt\n",
      "salvando em txt_autores_id\\77031_104.txt\n",
      "BootstrappingMonteCarloTreeSearchwithanImperfectHeuristic\n",
      "1206.5940v1:BootstrappingMonteCarloTreeSearchwithanImperfectHeuristic\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5940v1.txt\n",
      "CAPIR:CollaborativeActionPlanningwithIntentionRecognition\n",
      "1206.5928v1:CAPIR:CollaborativeActionPlanningwithIntentionRecognition\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5928v1.txt\n",
      "salvando em txt_autores_id\\77032_104.txt\n",
      "CAPIR:CollaborativeActionPlanningwithIntentionRecognition\n",
      "1206.5928v1:CAPIR:CollaborativeActionPlanningwithIntentionRecognition\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5928v1.txt\n",
      "RepresentationRequirementsforSupportingDecisionModelFormulation\n",
      "1303.5730v1:RepresentationRequirementsforSupportingDecisionModelFormulation\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5730v1.txt\n",
      "RepresentingContext-SensitiveKnowledgeinaNetworkFormalism:APreliminaryReport\n",
      "1303.5414v1:RepresentingContext-SensitiveKnowledgeinaNetworkFormalism:APreliminaryReport\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5414v1.txt\n",
      "CausalMechanism-basedModelConstruction\n",
      "1301.3872v1:CausalMechanism-basedModelConstruction\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3872v1.txt\n",
      "BootstrappingMonteCarloTreeSearchwithanImperfectHeuristic\n",
      "1206.5940v1:BootstrappingMonteCarloTreeSearchwithanImperfectHeuristic\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5940v1.txt\n",
      "salvando em txt_autores_id\\100199_104.txt\n",
      "UsingFirst-OrderProbabilityLogicfortheConstructionofBayesianNetworks\n",
      "1303.1480v1:UsingFirst-OrderProbabilityLogicfortheConstructionofBayesianNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1303.1480v1.txt\n",
      "UsingNewDatatoRefineaBayesianNetwork\n",
      "1302.6826v1:UsingNewDatatoRefineaBayesianNetwork\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6826v1.txt\n",
      "UsingCausalInformationandLocalMeasurestoLearnBayesianNetworks\n",
      "1303.1483v1:UsingCausalInformationandLocalMeasurestoLearnBayesianNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1303.1483v1.txt\n",
      "GeneratingNewBeliefsFromOld\n",
      "1302.6783v1:GeneratingNewBeliefsFromOld\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6783v1.txt\n",
      "UCP-Networks:ADirectedGraphicalRepresentationofConditionalUtilities\n",
      "1301.2259v1:UCP-Networks:ADirectedGraphicalRepresentationofConditionalUtilities\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2259v1.txt\n",
      "ProbabilityDistributionsOverPossibleWorlds\n",
      "1304.2341v1:ProbabilityDistributionsOverPossibleWorlds\n",
      "abrindo arquivo txt_semRefTitulo\\1304.2341v1.txt\n",
      "Solving#SATandBayesianInferencewithBacktrackingSearch\n",
      "1401.3458v1:Solving#SATandBayesianInferencewithBacktrackingSearch\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3458v1.txt\n",
      "salvando em txt_autores_id\\31138_116.txt\n",
      "OntheComplexityofDecisionMakinginPossibilisticDecisionTrees\n",
      "1202.3718v1:OntheComplexityofDecisionMakinginPossibilisticDecisionTrees\n",
      "abrindo arquivo txt_semRefTitulo\\1202.3718v1.txt\n",
      "CompilingPossibilisticNetworks:AlternativeApproachestoPossibilisticInference\n",
      "1203.3465v1:CompilingPossibilisticNetworks:AlternativeApproachestoPossibilisticInference\n",
      "abrindo arquivo txt_semRefTitulo\\1203.3465v1.txt\n",
      "salvando em txt_autores_id\\8824_116.txt\n",
      "ASymbolicApproachtoReasoningwithLinguisticQuantifiers\n",
      "1303.5401v1:ASymbolicApproachtoReasoningwithLinguisticQuantifiers\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5401v1.txt\n",
      "IncrementalMapGenerationbyLowCostRobotsBasedonPossibility/NecessityGrids\n",
      "1302.1559v1:IncrementalMapGenerationbyLowCostRobotsBasedonPossibility/NecessityGrids\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1559v1.txt\n",
      "EvolutionofIdeas:ANovelMemeticAlgorithmBasedonSemanticNetworks\n",
      "1201.2706v2:EvolutionofIdeas:ANovelMemeticAlgorithmBasedonSemanticNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1201.2706v2.txt\n",
      "salvando em txt_autores_id\\46213_116.txt\n",
      "CompilingPossibilisticNetworks:AlternativeApproachestoPossibilisticInference\n",
      "1203.3465v1:CompilingPossibilisticNetworks:AlternativeApproachestoPossibilisticInference\n",
      "abrindo arquivo txt_semRefTitulo\\1203.3465v1.txt\n",
      "salvando em txt_autores_id\\31137_116.txt\n",
      "OntheQualitativeComparisonofDecisionsHavingPositiveandNegativeFeatures\n",
      "1401.3444v1:OntheQualitativeComparisonofDecisionsHavingPositiveandNegativeFeatures\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3444v1.txt\n",
      "OntheComplexityofDecisionMakinginPossibilisticDecisionTrees\n",
      "1202.3718v1:OntheComplexityofDecisionMakinginPossibilisticDecisionTrees\n",
      "abrindo arquivo txt_semRefTitulo\\1202.3718v1.txt\n",
      "Decision-makingUnderOrdinalPreferencesandComparativeUncertainty\n",
      "1302.1537v1:Decision-makingUnderOrdinalPreferencesandComparativeUncertainty\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1537v1.txt\n",
      "salvando em txt_autores_id\\46214_116.txt\n",
      "CompilingPossibilisticNetworks:AlternativeApproachestoPossibilisticInference\n",
      "1203.3465v1:CompilingPossibilisticNetworks:AlternativeApproachestoPossibilisticInference\n",
      "abrindo arquivo txt_semRefTitulo\\1203.3465v1.txt\n",
      "LoopyBeliefPropagationinBayesianNetworks:originandpossibilisticperspectives\n",
      "1206.0976v1:LoopyBeliefPropagationinBayesianNetworks:originandpossibilisticperspectives\n",
      "abrindo arquivo txt_semRefTitulo\\1206.0976v1.txt\n",
      "Argumentativeinferenceinuncertainandinconsistentknowledgebases\n",
      "1303.1503v1:Argumentativeinferenceinuncertainandinconsistentknowledgebases\n",
      "abrindo arquivo txt_semRefTitulo\\1303.1503v1.txt\n",
      "APrincipledAnalysisofMergingOperationsinPossibilisticLogic\n",
      "1301.3835v1:APrincipledAnalysisofMergingOperationsinPossibilisticLogic\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3835v1.txt\n",
      "Graphicalreadingsofpossibilisticlogicbases\n",
      "1301.2255v1:Graphicalreadingsofpossibilisticlogicbases\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2255v1.txt\n",
      "salvando em txt_autores_id\\101882_116.txt\n",
      "OnModalLogicsforQualitativePossibilityinaFuzzySetting\n",
      "1302.6813v1:OnModalLogicsforQualitativePossibilityinaFuzzySetting\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6813v1.txt\n",
      "CombiningMultiple-ValuedLogicsinModularExpertSystems\n",
      "1303.5705v1:CombiningMultiple-ValuedLogicsinModularExpertSystems\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5705v1.txt\n",
      "ACompleteCalculusforPossibilisticLogicProgrammingwithFuzzyPropositionalVariables\n",
      "1301.3832v1:ACompleteCalculusforPossibilisticLogicProgrammingwithFuzzyPropositionalVariables\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3832v1.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASymbolicApproachtoReasoningwithLinguisticQuantifiers\n",
      "1303.5401v1:ASymbolicApproachtoReasoningwithLinguisticQuantifiers\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5401v1.txt\n",
      "salvando em txt_autores_id\\135514_116.txt\n",
      "CombiningMultiple-ValuedLogicsinModularExpertSystems\n",
      "1303.5705v1:CombiningMultiple-ValuedLogicsinModularExpertSystems\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5705v1.txt\n",
      "IncrementalMapGenerationbyLowCostRobotsBasedonPossibility/NecessityGrids\n",
      "1302.1559v1:IncrementalMapGenerationbyLowCostRobotsBasedonPossibility/NecessityGrids\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1559v1.txt\n",
      "salvando em txt_autores_id\\140340_116.txt\n",
      "CombiningMultiple-ValuedLogicsinModularExpertSystems\n",
      "1303.5705v1:CombiningMultiple-ValuedLogicsinModularExpertSystems\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5705v1.txt\n",
      "OnModalLogicsforQualitativePossibilityinaFuzzySetting\n",
      "1302.6813v1:OnModalLogicsforQualitativePossibilityinaFuzzySetting\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6813v1.txt\n",
      "salvando em txt_autores_id\\140341_116.txt\n",
      "CombiningMultiple-ValuedLogicsinModularExpertSystems\n",
      "1303.5705v1:CombiningMultiple-ValuedLogicsinModularExpertSystems\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5705v1.txt\n",
      "OnModalLogicsforQualitativePossibilityinaFuzzySetting\n",
      "1302.6813v1:OnModalLogicsforQualitativePossibilityinaFuzzySetting\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6813v1.txt\n",
      "salvando em txt_autores_id\\147023_116.txt\n",
      "CombiningMultiple-ValuedLogicsinModularExpertSystems\n",
      "1303.5705v1:CombiningMultiple-ValuedLogicsinModularExpertSystems\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5705v1.txt\n",
      "salvando em txt_autores_id\\31203_134.txt\n",
      "ImprovingtheScalabilityofOptimalBayesianNetworkLearningwithExternal-MemoryFrontierBreadth-FirstBranchandBoundSearch\n",
      "1202.3744v1:ImprovingtheScalabilityofOptimalBayesianNetworkLearningwithExternal-MemoryFrontierBreadth-FirstBranchandBoundSearch\n",
      "abrindo arquivo txt_semRefTitulo\\1202.3744v1.txt\n",
      "MostRelevantExplanation:Properties,Algorithms,andEvaluations\n",
      "1205.2601v1:MostRelevantExplanation:Properties,Algorithms,andEvaluations\n",
      "abrindo arquivo txt_semRefTitulo\\1205.2601v1.txt\n",
      "SolvingMultistageInfluenceDiagramsusingBranch-and-BoundSearch\n",
      "1203.3531v1:SolvingMultistageInfluenceDiagramsusingBranch-and-BoundSearch\n",
      "abrindo arquivo txt_semRefTitulo\\1203.3531v1.txt\n",
      "ImportanceSamplinginBayesianNetworks:AnInfluence-BasedApproximationStrategyforImportanceFunctions\n",
      "1207.1422v1:ImportanceSamplinginBayesianNetworks:AnInfluence-BasedApproximationStrategyforImportanceFunctions\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1422v1.txt\n",
      "salvando em txt_autores_id\\31089_134.txt\n",
      "ExtendedLiftedInferencewithJointFormulas\n",
      "1202.3698v1:ExtendedLiftedInferencewithJointFormulas\n",
      "abrindo arquivo txt_semRefTitulo\\1202.3698v1.txt\n",
      "GenericPreferencesoverSubsetsofStructuredObjects\n",
      "1401.3459v1:GenericPreferencesoverSubsetsofStructuredObjects\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3459v1.txt\n",
      "UCP-Networks:ADirectedGraphicalRepresentationofConditionalUtilities\n",
      "1301.2259v1:UCP-Networks:ADirectedGraphicalRepresentationofConditionalUtilities\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2259v1.txt\n",
      "AHeuristicSearchApproachtoPlanningwithContinuousResourcesinStochasticDomains\n",
      "1401.3428v1:AHeuristicSearchApproachtoPlanningwithContinuousResourcesinStochasticDomains\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3428v1.txt\n",
      "salvando em txt_autores_id\\31164_134.txt\n",
      "SuboptimalityBoundsforStochasticShortestPathProblems\n",
      "1202.3729v1:SuboptimalityBoundsforStochasticShortestPathProblems\n",
      "abrindo arquivo txt_semRefTitulo\\1202.3729v1.txt\n",
      "PolicyIterationforDecentralizedControlofMarkovDecisionProcesses\n",
      "1401.3460v1:PolicyIterationforDecentralizedControlofMarkovDecisionProcesses\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3460v1.txt\n",
      "AHeuristicSearchApproachtoPlanningwithContinuousResourcesinStochasticDomains\n",
      "1401.3428v1:AHeuristicSearchApproachtoPlanningwithContinuousResourcesinStochasticDomains\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3428v1.txt\n",
      "ImprovingtheScalabilityofOptimalBayesianNetworkLearningwithExternal-MemoryFrontierBreadth-FirstBranchandBoundSearch\n",
      "1202.3744v1:ImprovingtheScalabilityofOptimalBayesianNetworkLearningwithExternal-MemoryFrontierBreadth-FirstBranchandBoundSearch\n",
      "abrindo arquivo txt_semRefTitulo\\1202.3744v1.txt\n",
      "SolvingMultistageInfluenceDiagramsusingBranch-and-BoundSearch\n",
      "1203.3531v1:SolvingMultistageInfluenceDiagramsusingBranch-and-BoundSearch\n",
      "abrindo arquivo txt_semRefTitulo\\1203.3531v1.txt\n",
      "SparseStochasticFinite-StateControllersforPOMDPs\n",
      "1206.3263v1:SparseStochasticFinite-StateControllersforPOMDPs\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3263v1.txt\n",
      "salvando em txt_autores_id\\31191_134.txt\n",
      "ABilinearProgrammingApproachforMultiagentPlanning\n",
      "1401.3461v1:ABilinearProgrammingApproachforMultiagentPlanning\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3461v1.txt\n",
      "PolicyIterationforDecentralizedControlofMarkovDecisionProcesses\n",
      "1401.3460v1:PolicyIterationforDecentralizedControlofMarkovDecisionProcesses\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3460v1.txt\n",
      "TheComplexityofDecentralizedControlofMarkovDecisionProcesses\n",
      "1301.3836v1:TheComplexityofDecentralizedControlofMarkovDecisionProcesses\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3836v1.txt\n",
      "OptimizingMemory-BoundedControllersforDecentralizedPOMDPs\n",
      "1206.5258v1:OptimizingMemory-BoundedControllersforDecentralizedPOMDPs\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5258v1.txt\n",
      "RolloutSamplingPolicyIterationforDecentralizedPOMDPs\n",
      "1203.3528v1:RolloutSamplingPolicyIterationforDecentralizedPOMDPs\n",
      "abrindo arquivo txt_semRefTitulo\\1203.3528v1.txt\n",
      "AnytimePlanningforDecentralizedPOMDPsusingExpectationMaximization\n",
      "1203.3490v1:AnytimePlanningforDecentralizedPOMDPsusingExpectationMaximization\n",
      "abrindo arquivo txt_semRefTitulo\\1203.3490v1.txt\n",
      "Message-PassingAlgorithmsforQuadraticProgrammingFormulationsofMAPEstimation\n",
      "1202.3739v1:Message-PassingAlgorithmsforQuadraticProgrammingFormulationsofMAPEstimation\n",
      "abrindo arquivo txt_semRefTitulo\\1202.3739v1.txt\n",
      "salvando em txt_autores_id\\58500_134.txt\n",
      "CausalMechanism-basedModelConstruction\n",
      "1301.3872v1:CausalMechanism-basedModelConstruction\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3872v1.txt\n",
      "MostRelevantExplanation:Properties,Algorithms,andEvaluations\n",
      "1205.2601v1:MostRelevantExplanation:Properties,Algorithms,andEvaluations\n",
      "abrindo arquivo txt_semRefTitulo\\1205.2601v1.txt\n",
      "salvando em txt_autores_id\\72745_134.txt\n",
      "GenericPreferencesoverSubsetsofStructuredObjects\n",
      "1401.3459v1:GenericPreferencesoverSubsetsofStructuredObjects\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3459v1.txt\n",
      "SimpleRegretOptimizationinOnlinePlanningforMarkovDecisionProcesses\n",
      "1206.3382v2:SimpleRegretOptimizationinOnlinePlanningforMarkovDecisionProcesses\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3382v2.txt\n",
      "UnstructuringUserPreferences:EfficientNon-ParametricUtilityRevelation\n",
      "1207.1390v1:UnstructuringUserPreferences:EfficientNon-ParametricUtilityRevelation\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1390v1.txt\n",
      "Cost-SharinginBayesianKnowledgeBases\n",
      "1302.1567v1:Cost-SharinginBayesianKnowledgeBases\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1567v1.txt\n",
      "OnlineSpeedupLearningforOptimalPlanning\n",
      "1401.5861v1:OnlineSpeedupLearningforOptimalPlanning\n",
      "abrindo arquivo txt_semRefTitulo\\1401.5861v1.txt\n",
      "salvando em txt_autores_id\\75921_134.txt\n",
      "OptimizingMemory-BoundedControllersforDecentralizedPOMDPs\n",
      "1206.5258v1:OptimizingMemory-BoundedControllersforDecentralizedPOMDPs\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5258v1.txt\n",
      "IncrementalClusteringandExpansionforFasterOptimalPlanninginDec-POMDPs\n",
      "1402.0566v1:IncrementalClusteringandExpansionforFasterOptimalPlanninginDec-POMDPs\n",
      "abrindo arquivo txt_semRefTitulo\\1402.0566v1.txt\n",
      "ScalablePlanningandLearningforMultiagentPOMDPs:ExtendedVersion\n",
      "1404.1140v2:ScalablePlanningandLearningforMultiagentPOMDPs:ExtendedVersion\n",
      "abrindo arquivo txt_semRefTitulo\\1404.1140v2.txt\n",
      "PolicyIterationforDecentralizedControlofMarkovDecisionProcesses\n",
      "1401.3460v1:PolicyIterationforDecentralizedControlofMarkovDecisionProcesses\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3460v1.txt\n",
      "salvando em txt_autores_id\\105243_134.txt\n",
      "IDEAL:ASoftwarePackageforAnalysisofInfluenceDiagrams\n",
      "1304.1107v1:IDEAL:ASoftwarePackageforAnalysisofInfluenceDiagrams\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1107v1.txt\n",
      "IntegratingModelConstructionandEvaluation\n",
      "1303.5405v1:IntegratingModelConstructionandEvaluation\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5405v1.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision-TheoreticControlofProblemSolving:PrinciplesandArchitecture\n",
      "1304.2343v1:Decision-TheoreticControlofProblemSolving:PrinciplesandArchitecture\n",
      "abrindo arquivo txt_semRefTitulo\\1304.2343v1.txt\n",
      "DecisionMakingwithIntervalInfluenceDiagrams\n",
      "1304.1096v1:DecisionMakingwithIntervalInfluenceDiagrams\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1096v1.txt\n",
      "IdealReformulationofBeliefNetworks\n",
      "1304.1089v1:IdealReformulationofBeliefNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1089v1.txt\n",
      "ANewLookatCausalIndependence\n",
      "1302.6814v2:ANewLookatCausalIndependence\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6814v2.txt\n",
      "salvando em txt_autores_id\\105245_134.txt\n",
      "Decision-TheoreticControlofProblemSolving:PrinciplesandArchitecture\n",
      "1304.2343v1:Decision-TheoreticControlofProblemSolving:PrinciplesandArchitecture\n",
      "abrindo arquivo txt_semRefTitulo\\1304.2343v1.txt\n",
      "Computationally-OptimalReal-ResourceStrategies\n",
      "1304.1090v1:Computationally-OptimalReal-ResourceStrategies\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1090v1.txt\n",
      "ProbabilisticConceptualNetwork:ABeliefRepresentationSchemeforUtility-BasedCategorization\n",
      "1303.1474v1:ProbabilisticConceptualNetwork:ABeliefRepresentationSchemeforUtility-BasedCategorization\n",
      "abrindo arquivo txt_semRefTitulo\\1303.1474v1.txt\n",
      "AStructured,ProbabilisticRepresentationofAction\n",
      "1302.6798v1:AStructured,ProbabilisticRepresentationofAction\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6798v1.txt\n",
      "DecisionMethodsforAdaptiveTask-SharinginAssociateSystems\n",
      "1303.5423v1:DecisionMethodsforAdaptiveTask-SharinginAssociateSystems\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5423v1.txt\n",
      "salvando em txt_autores_id\\105267_134.txt\n",
      "AProbabilisticApproachtoHierarchicalModel-basedDiagnosis\n",
      "1302.6846v1:AProbabilisticApproachtoHierarchicalModel-basedDiagnosis\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6846v1.txt\n",
      "IDEAL:ASoftwarePackageforAnalysisofInfluenceDiagrams\n",
      "1304.1107v1:IDEAL:ASoftwarePackageforAnalysisofInfluenceDiagrams\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1107v1.txt\n",
      "AGeneralizationoftheNoisy-OrModel\n",
      "1303.1479v1:AGeneralizationoftheNoisy-OrModel\n",
      "abrindo arquivo txt_semRefTitulo\\1303.1479v1.txt\n",
      "salvando em txt_autores_id\\31136_145.txt\n",
      "PAC-BayesianPolicyEvaluationforReinforcementLearning\n",
      "1202.3717v1:PAC-BayesianPolicyEvaluationforReinforcementLearning\n",
      "abrindo arquivo txt_semRefTitulo\\1202.3717v1.txt\n",
      "OnlineLearningunderDelayedFeedback\n",
      "1306.0686v2:OnlineLearningunderDelayedFeedback\n",
      "abrindo arquivo txt_semRefTitulo\\1306.0686v2.txt\n",
      "OnlineLearninginMarkovDecisionProcesseswithAdversariallyChosenTransitionProbabilityDistributions\n",
      "1303.3055v1:OnlineLearninginMarkovDecisionProcesseswithAdversariallyChosenTransitionProbabilityDistributions\n",
      "abrindo arquivo txt_semRefTitulo\\1303.3055v1.txt\n",
      "ARandomizedMirrorDescentAlgorithmforLargeScaleMultipleKernelLearning\n",
      "1205.0288v2:ARandomizedMirrorDescentAlgorithmforLargeScaleMultipleKernelLearning\n",
      "abrindo arquivo txt_semRefTitulo\\1205.0288v2.txt\n",
      "StatisticalLinearEstimationwithPenalizedEstimators:anApplicationtoReinforcementLearning\n",
      "1206.6444v1:StatisticalLinearEstimationwithPenalizedEstimators:anApplicationtoReinforcementLearning\n",
      "abrindo arquivo txt_semRefTitulo\\1206.6444v1.txt\n",
      "AnalysisofKernelMeanMatchingunderCovariateShift\n",
      "1206.4650v1:AnalysisofKernelMeanMatchingunderCovariateShift\n",
      "abrindo arquivo txt_semRefTitulo\\1206.4650v1.txt\n",
      "ApprenticeshipLearningusingInverseReinforcementLearningandGradientMethods\n",
      "1206.5264v1:ApprenticeshipLearningusingInverseReinforcementLearningandGradientMethods\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5264v1.txt\n",
      "AnAdaptiveAlgorithmforFiniteStochasticPartialMonitoring\n",
      "1206.6487v1:AnAdaptiveAlgorithmforFiniteStochasticPartialMonitoring\n",
      "abrindo arquivo txt_semRefTitulo\\1206.6487v1.txt\n",
      "Dyna-StylePlanningwithLinearFunctionApproximationandPrioritizedSweeping\n",
      "1206.3285v1:Dyna-StylePlanningwithLinearFunctionApproximationandPrioritizedSweeping\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3285v1.txt\n",
      "SpeedingUpPlanninginMarkovDecisionProcessesviaAutomaticallyConstructedAbstractions\n",
      "1206.3233v1:SpeedingUpPlanninginMarkovDecisionProcessesviaAutomaticallyConstructedAbstractions\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3233v1.txt\n",
      "salvando em txt_autores_id\\58563_145.txt\n",
      "BayesianError-BarsforBeliefNetInference\n",
      "1301.2313v1:BayesianError-BarsforBeliefNetInference\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2313v1.txt\n",
      "ImprovedMeanandVarianceApproximationsforBeliefNetResponsesviaNetworkDoubling\n",
      "1205.2642v1:ImprovedMeanandVarianceApproximationsforBeliefNetResponsesviaNetworkDoubling\n",
      "abrindo arquivo txt_semRefTitulo\\1205.2642v1.txt\n",
      "salvando em txt_autores_id\\58564_145.txt\n",
      "ImprovedMeanandVarianceApproximationsforBeliefNetResponsesviaNetworkDoubling\n",
      "1205.2642v1:ImprovedMeanandVarianceApproximationsforBeliefNetResponsesviaNetworkDoubling\n",
      "abrindo arquivo txt_semRefTitulo\\1205.2642v1.txt\n",
      "OnlineLearninginMarkovDecisionProcesseswithAdversariallyChosenTransitionProbabilityDistributions\n",
      "1303.3055v1:OnlineLearninginMarkovDecisionProcesseswithAdversariallyChosenTransitionProbabilityDistributions\n",
      "abrindo arquivo txt_semRefTitulo\\1303.3055v1.txt\n",
      "LinearProgrammingforLarge-ScaleMarkovDecisionProblems\n",
      "1402.6763v1:LinearProgrammingforLarge-ScaleMarkovDecisionProblems\n",
      "abrindo arquivo txt_semRefTitulo\\1402.6763v1.txt\n",
      "salvando em txt_autores_id\\58565_145.txt\n",
      "SpeedingUpPlanninginMarkovDecisionProcessesviaAutomaticallyConstructedAbstractions\n",
      "1206.3233v1:SpeedingUpPlanninginMarkovDecisionProcessesviaAutomaticallyConstructedAbstractions\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3233v1.txt\n",
      "ImprovedMeanandVarianceApproximationsforBeliefNetResponsesviaNetworkDoubling\n",
      "1205.2642v1:ImprovedMeanandVarianceApproximationsforBeliefNetResponsesviaNetworkDoubling\n",
      "abrindo arquivo txt_semRefTitulo\\1205.2642v1.txt\n",
      "LearningBayesianNetsthatPerformWell\n",
      "1302.1542v1:LearningBayesianNetsthatPerformWell\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1542v1.txt\n",
      "BayesianError-BarsforBeliefNetInference\n",
      "1301.2313v1:BayesianError-BarsforBeliefNetInference\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2313v1.txt\n",
      "AGeneralizedLoopCorrectionMethodforApproximateInferenceinGraphicalModels\n",
      "1206.4654v1:AGeneralizedLoopCorrectionMethodforApproximateInferenceinGraphicalModels\n",
      "abrindo arquivo txt_semRefTitulo\\1206.4654v1.txt\n",
      "TrainingRestrictedBoltzmannMachinebyPerturbation\n",
      "1405.1436v1:TrainingRestrictedBoltzmannMachinebyPerturbation\n",
      "abrindo arquivo txt_semRefTitulo\\1405.1436v1.txt\n",
      "salvando em txt_autores_id\\58566_145.txt\n",
      "ImprovedMeanandVarianceApproximationsforBeliefNetResponsesviaNetworkDoubling\n",
      "1205.2642v1:ImprovedMeanandVarianceApproximationsforBeliefNetResponsesviaNetworkDoubling\n",
      "abrindo arquivo txt_semRefTitulo\\1205.2642v1.txt\n",
      "salvando em txt_autores_id\\63480_145.txt\n",
      "PlanningbyPrioritizedSweepingwithSmallBackups\n",
      "1301.2343v1:PlanningbyPrioritizedSweepingwithSmallBackups\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2343v1.txt\n",
      "Dyna-StylePlanningwithLinearFunctionApproximationandPrioritizedSweeping\n",
      "1206.3285v1:Dyna-StylePlanningwithLinearFunctionApproximationandPrioritizedSweeping\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3285v1.txt\n",
      "Off-PolicyActor-Critic\n",
      "1205.4839v5:Off-PolicyActor-Critic\n",
      "abrindo arquivo txt_semRefTitulo\\1205.4839v5.txt\n",
      "salvando em txt_autores_id\\72512_145.txt\n",
      "SpeedingUpPlanninginMarkovDecisionProcessesviaAutomaticallyConstructedAbstractions\n",
      "1206.3233v1:SpeedingUpPlanninginMarkovDecisionProcessesviaAutomaticallyConstructedAbstractions\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3233v1.txt\n",
      "salvando em txt_autores_id\\72513_145.txt\n",
      "SpeedingUpPlanninginMarkovDecisionProcessesviaAutomaticallyConstructedAbstractions\n",
      "1206.3233v1:SpeedingUpPlanninginMarkovDecisionProcessesviaAutomaticallyConstructedAbstractions\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3233v1.txt\n",
      "salvando em txt_autores_id\\72582_145.txt\n",
      "Dyna-StylePlanningwithLinearFunctionApproximationandPrioritizedSweeping\n",
      "1206.3285v1:Dyna-StylePlanningwithLinearFunctionApproximationandPrioritizedSweeping\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3285v1.txt\n",
      "salvando em txt_autores_id\\72583_145.txt\n",
      "Bayes'Bluff:OpponentModellinginPoker\n",
      "1207.1411v1:Bayes'Bluff:OpponentModellinginPoker\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1411v1.txt\n",
      "Dyna-StylePlanningwithLinearFunctionApproximationandPrioritizedSweeping\n",
      "1206.3285v1:Dyna-StylePlanningwithLinearFunctionApproximationandPrioritizedSweeping\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3285v1.txt\n",
      "salvando em txt_autores_id\\21015_151.txt\n",
      "ResourcefulContextualBandits\n",
      "1402.6779v6:ResourcefulContextualBandits\n",
      "abrindo arquivo txt_semRefTitulo\\1402.6779v6.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TamingtheMonster:AFastandSimpleAlgorithmforContextualBandits\n",
      "1402.0555v2:TamingtheMonster:AFastandSimpleAlgorithmforContextualBandits\n",
      "abrindo arquivo txt_semRefTitulo\\1402.0555v2.txt\n",
      "LogarithmicTimeOnlineMulticlassprediction\n",
      "1406.1822v13:LogarithmicTimeOnlineMulticlassprediction\n",
      "abrindo arquivo txt_semRefTitulo\\1406.1822v13.txt\n",
      "Proceedingsofthe29thInternationalConferenceonMachineLearning(ICML-12)\n",
      "1207.4676v2:Proceedingsofthe29thInternationalConferenceonMachineLearning(ICML-12)\n",
      "abrindo arquivo txt_semRefTitulo\\1207.4676v2.txt\n",
      "PredictingConditionalQuantilesviaReductiontoClassification\n",
      "1206.6860v1:PredictingConditionalQuantilesviaReductiontoClassification\n",
      "abrindo arquivo txt_semRefTitulo\\1206.6860v1.txt\n",
      "LearningPerformanceofPredictionMarketswithKellyBettors\n",
      "1201.6655v1:LearningPerformanceofPredictionMarketswithKellyBettors\n",
      "abrindo arquivo txt_semRefTitulo\\1201.6655v1.txt\n",
      "Efficientprogrammablelearningtosearch\n",
      "1406.1837v1:Efficientprogrammablelearningtosearch\n",
      "abrindo arquivo txt_semRefTitulo\\1406.1837v1.txt\n",
      "ContextualBanditLearningwithPredictableRewards\n",
      "1202.1334v2:ContextualBanditLearningwithPredictableRewards\n",
      "abrindo arquivo txt_semRefTitulo\\1202.1334v2.txt\n",
      "salvando em txt_autores_id\\21014_151.txt\n",
      "EfficientTestSelectioninActiveDiagnosisviaEntropyApproximation\n",
      "1207.1418v1:EfficientTestSelectioninActiveDiagnosisviaEntropyApproximation\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1418v1.txt\n",
      "LearningPerformanceofPredictionMarketswithKellyBettors\n",
      "1201.6655v1:LearningPerformanceofPredictionMarketswithKellyBettors\n",
      "abrindo arquivo txt_semRefTitulo\\1201.6655v1.txt\n",
      "salvando em txt_autores_id\\75901_151.txt\n",
      "StudiesinLowerBoundingProbabilitiesofEvidenceusingtheMarkovInequality\n",
      "1206.5242v1:StudiesinLowerBoundingProbabilitiesofEvidenceusingtheMarkovInequality\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5242v1.txt\n",
      "CutsetSamplingwithLikelihoodWeighting\n",
      "1206.6822v1:CutsetSamplingwithLikelihoodWeighting\n",
      "abrindo arquivo txt_semRefTitulo\\1206.6822v1.txt\n",
      "ModelingTransportationRoutinesusingHybridDynamicMixedNetworks\n",
      "1207.1384v1:ModelingTransportationRoutinesusingHybridDynamicMixedNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1384v1.txt\n",
      "salvando em txt_autores_id\\31151_151.txt\n",
      "StudiesinLowerBoundingProbabilitiesofEvidenceusingtheMarkovInequality\n",
      "1206.5242v1:StudiesinLowerBoundingProbabilitiesofEvidenceusingtheMarkovInequality\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5242v1.txt\n",
      "AND/ORImportanceSampling\n",
      "1206.3232v1:AND/ORImportanceSampling\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3232v1.txt\n",
      "ProbabilisticTheoremProving\n",
      "1202.3724v1:ProbabilisticTheoremProving\n",
      "abrindo arquivo txt_semRefTitulo\\1202.3724v1.txt\n",
      "Join-GraphPropagationAlgorithms\n",
      "1401.3489v1:Join-GraphPropagationAlgorithms\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3489v1.txt\n",
      "ApproximateInferenceAlgorithmsforHybridBayesianNetworkswithDiscreteConstraints\n",
      "1207.1385v1:ApproximateInferenceAlgorithmsforHybridBayesianNetworkswithDiscreteConstraints\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1385v1.txt\n",
      "ApproximationbyQuantization\n",
      "1202.3723v1:ApproximationbyQuantization\n",
      "abrindo arquivo txt_semRefTitulo\\1202.3723v1.txt\n",
      "Formula-BasedProbabilisticInference\n",
      "1203.3482v1:Formula-BasedProbabilisticInference\n",
      "abrindo arquivo txt_semRefTitulo\\1203.3482v1.txt\n",
      "ModelingTransportationRoutinesusingHybridDynamicMixedNetworks\n",
      "1207.1384v1:ModelingTransportationRoutinesusingHybridDynamicMixedNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1384v1.txt\n",
      "salvando em txt_autores_id\\31205_151.txt\n",
      "AND/ORMulti-ValuedDecisionDiagrams(AOMDDs)forGraphicalModels\n",
      "1401.3448v1:AND/ORMulti-ValuedDecisionDiagrams(AOMDDs)forGraphicalModels\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3448v1.txt\n",
      "Best-FirstAND/ORSearchforMostProbableExplanations\n",
      "1206.5268v1:Best-FirstAND/ORSearchforMostProbableExplanations\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5268v1.txt\n",
      "Order-of-MagnitudeInfluenceDiagrams\n",
      "1202.3745v1:Order-of-MagnitudeInfluenceDiagrams\n",
      "abrindo arquivo txt_semRefTitulo\\1202.3745v1.txt\n",
      "salvando em txt_autores_id\\46259_151.txt\n",
      "Join-GraphPropagationAlgorithms\n",
      "1401.3489v1:Join-GraphPropagationAlgorithms\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3489v1.txt\n",
      "BEEM:BucketEliminationwithExternalMemory\n",
      "1203.3487v1:BEEM:BucketEliminationwithExternalMemory\n",
      "abrindo arquivo txt_semRefTitulo\\1203.3487v1.txt\n",
      "salvando em txt_autores_id\\75933_151.txt\n",
      "AND/ORMulti-ValuedDecisionDiagrams(AOMDDs)forWeightedGraphicalModels\n",
      "1206.5266v1:AND/ORMulti-ValuedDecisionDiagrams(AOMDDs)forWeightedGraphicalModels\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5266v1.txt\n",
      "TheRelationshipBetweenAND/ORSearchandVariableElimination\n",
      "1207.1407v1:TheRelationshipBetweenAND/ORSearchandVariableElimination\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1407v1.txt\n",
      "AND/ORMulti-ValuedDecisionDiagrams(AOMDDs)forGraphicalModels\n",
      "1401.3448v1:AND/ORMulti-ValuedDecisionDiagrams(AOMDDs)forGraphicalModels\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3448v1.txt\n",
      "Join-GraphPropagationAlgorithms\n",
      "1401.3489v1:Join-GraphPropagationAlgorithms\n",
      "abrindo arquivo txt_semRefTitulo\\1401.3489v1.txt\n",
      "salvando em txt_autores_id\\81187_151.txt\n",
      "ModelingTransportationRoutinesusingHybridDynamicMixedNetworks\n",
      "1207.1384v1:ModelingTransportationRoutinesusingHybridDynamicMixedNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1384v1.txt\n",
      "salvando em txt_autores_id\\81188_151.txt\n",
      "ModelingTransportationRoutinesusingHybridDynamicMixedNetworks\n",
      "1207.1384v1:ModelingTransportationRoutinesusingHybridDynamicMixedNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1384v1.txt\n",
      "salvando em txt_autores_id\\81222_151.txt\n",
      "ASchemeforApproximatingProbabilisticInference\n",
      "1302.1534v1:ASchemeforApproximatingProbabilisticInference\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1534v1.txt\n",
      "EfficientTestSelectioninActiveDiagnosisviaEntropyApproximation\n",
      "1207.1418v1:EfficientTestSelectioninActiveDiagnosisviaEntropyApproximation\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1418v1.txt\n",
      "salvando em txt_autores_id\\101895_153.txt\n",
      "DependencyNetworksforCollaborativeFilteringandDataVisualization\n",
      "1301.3862v1:DependencyNetworksforCollaborativeFilteringandDataVisualization\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3862v1.txt\n",
      "salvando em txt_autores_id\\46245_153.txt\n",
      "Playinggamesagainstnature:optimalpoliciesforrenewableresourceallocation\n",
      "1203.3478v1:Playinggamesagainstnature:optimalpoliciesforrenewableresourceallocation\n",
      "abrindo arquivo txt_semRefTitulo\\1203.3478v1.txt\n",
      "OntheErdosDiscrepancyProblem\n",
      "1407.2510v1:OntheErdosDiscrepancyProblem\n",
      "abrindo arquivo txt_semRefTitulo\\1407.2510v1.txt\n",
      "UnderstandingSamplingStyleAdversarialSearchMethods\n",
      "1203.4011v1:UnderstandingSamplingStyleAdversarialSearchMethods\n",
      "abrindo arquivo txt_semRefTitulo\\1203.4011v1.txt\n",
      "SynthesizingManipulationSequencesforUnder-SpecifiedTasksusingUnrolledMarkovRandomFields\n",
      "1306.5707v2:SynthesizingManipulationSequencesforUnder-SpecifiedTasksusingUnrolledMarkovRandomFields\n",
      "abrindo arquivo txt_semRefTitulo\\1306.5707v2.txt\n",
      "ABayesianApproachtoTacklingHardComputationalProblems\n",
      "1301.2279v1:ABayesianApproachtoTacklingHardComputationalProblems\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2279v1.txt\n",
      "AlgorithmPortfolioDesign:Theoryvs.Practice\n",
      "1302.1541v1:AlgorithmPortfolioDesign:Theoryvs.Practice\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1541v1.txt\n",
      "SurveyPropagationRevisited\n",
      "1206.5273v1:SurveyPropagationRevisited\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5273v1.txt\n",
      "salvando em txt_autores_id\\72553_153.txt\n",
      "Adjacency-FaithfulnessandConservativeCausalInference\n",
      "1206.6843v1:Adjacency-FaithfulnessandConservativeCausalInference\n",
      "abrindo arquivo txt_semRefTitulo\\1206.6843v1.txt\n",
      "DetectingCausalRelationsinthePresenceofUnmeasuredVariables\n",
      "1303.5754v1:DetectingCausalRelationsinthePresenceofUnmeasuredVariables\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5754v1.txt\n",
      "ATransformationalCharacterizationofMarkovEquivalenceforDirectedAcyclicGraphswithLatentVariables\n",
      "1207.1419v1:ATransformationalCharacterizationofMarkovEquivalenceforDirectedAcyclicGraphswithLatentVariables\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1419v1.txt\n",
      "Semi-InstrumentalVariables:ATestforInstrumentAdmissibility\n",
      "1301.2261v1:Semi-InstrumentalVariables:ATestforInstrumentAdmissibility\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2261v1.txt\n",
      "Causaldiscoveryoflinearacyclicmodelswitharbitrarydistributions\n",
      "1206.3260v1:Causaldiscoveryoflinearacyclicmodelswitharbitrarydistributions\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3260v1.txt\n",
      "AtheoreticalstudyofYstructuresforcausaldiscovery\n",
      "1206.6853v1:AtheoreticalstudyofYstructuresforcausaldiscovery\n",
      "abrindo arquivo txt_semRefTitulo\\1206.6853v1.txt\n",
      "DiscoveringCyclicCausalModelsbyIndependentComponentsAnalysis\n",
      "1206.3273v1:DiscoveringCyclicCausalModelsbyIndependentComponentsAnalysis\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3273v1.txt\n",
      "salvando em txt_autores_id\\72554_153.txt\n",
      "Adjacency-FaithfulnessandConservativeCausalInference\n",
      "1206.6843v1:Adjacency-FaithfulnessandConservativeCausalInference\n",
      "abrindo arquivo txt_semRefTitulo\\1206.6843v1.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causaldiscoveryoflinearacyclicmodelswitharbitrarydistributions\n",
      "1206.3260v1:Causaldiscoveryoflinearacyclicmodelswitharbitrarydistributions\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3260v1.txt\n",
      "DiscoveringCyclicCausalModelsbyIndependentComponentsAnalysis\n",
      "1206.3273v1:DiscoveringCyclicCausalModelsbyIndependentComponentsAnalysis\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3273v1.txt\n",
      "salvando em txt_autores_id\\103919_153.txt\n",
      "ReasoningabouttheValueofDecision-ModelRefinement:MethodsandApplication\n",
      "1303.1475v1:ReasoningabouttheValueofDecision-ModelRefinement:MethodsandApplication\n",
      "abrindo arquivo txt_semRefTitulo\\1303.1475v1.txt\n",
      "ProbabilisticConceptualNetwork:ABeliefRepresentationSchemeforUtility-BasedCategorization\n",
      "1303.1474v1:ProbabilisticConceptualNetwork:ABeliefRepresentationSchemeforUtility-BasedCategorization\n",
      "abrindo arquivo txt_semRefTitulo\\1303.1474v1.txt\n",
      "DirectedReductionAlgorithmsandDecomposableGraphs\n",
      "1304.1110v1:DirectedReductionAlgorithmsandDecomposableGraphs\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1110v1.txt\n",
      "Time-CriticalDynamicDecisionMaking\n",
      "1301.6750v1:Time-CriticalDynamicDecisionMaking\n",
      "abrindo arquivo txt_semRefTitulo\\1301.6750v1.txt\n",
      "salvando em txt_autores_id\\46244_153.txt\n",
      "OntheErdosDiscrepancyProblem\n",
      "1407.2510v1:OntheErdosDiscrepancyProblem\n",
      "abrindo arquivo txt_semRefTitulo\\1407.2510v1.txt\n",
      "AlgorithmPortfolioDesign:Theoryvs.Practice\n",
      "1302.1541v1:AlgorithmPortfolioDesign:Theoryvs.Practice\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1541v1.txt\n",
      "ABayesianApproachtoTacklingHardComputationalProblems\n",
      "1301.2279v1:ABayesianApproachtoTacklingHardComputationalProblems\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2279v1.txt\n",
      "MaximizingtheSpreadofCascadesUsingNetworkDesign\n",
      "1203.3514v1:MaximizingtheSpreadofCascadesUsingNetworkDesign\n",
      "abrindo arquivo txt_semRefTitulo\\1203.3514v1.txt\n",
      "Playinggamesagainstnature:optimalpoliciesforrenewableresourceallocation\n",
      "1203.3478v1:Playinggamesagainstnature:optimalpoliciesforrenewableresourceallocation\n",
      "abrindo arquivo txt_semRefTitulo\\1203.3478v1.txt\n",
      "salvando em txt_autores_id\\72596_153.txt\n",
      "InferenceforMultiplicativeModels\n",
      "1206.3296v1:InferenceforMultiplicativeModels\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3296v1.txt\n",
      "ABayesianApproachtoLearningBayesianNetworkswithLocalStructure\n",
      "1302.1528v2:ABayesianApproachtoLearningBayesianNetworkswithLocalStructure\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1528v2.txt\n",
      "UsingTemporalDataforMakingRecommendations\n",
      "1301.2320v1:UsingTemporalDataforMakingRecommendations\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2320v1.txt\n",
      "StructureandParameterLearningforCausalIndependenceandCausalInteractionModels\n",
      "1302.1561v2:StructureandParameterLearningforCausalIndependenceandCausalInteractionModels\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1561v2.txt\n",
      "ModelsandSelectionCriteriaforRegressionandClassification\n",
      "1302.1545v1:ModelsandSelectionCriteriaforRegressionandClassification\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1545v1.txt\n",
      "PerfectTree-LikeMarkovianDistributions\n",
      "1301.3834v1:PerfectTree-LikeMarkovianDistributions\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3834v1.txt\n",
      "DependencyNetworksforCollaborativeFilteringandDataVisualization\n",
      "1301.3862v1:DependencyNetworksforCollaborativeFilteringandDataVisualization\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3862v1.txt\n",
      "salvando em txt_autores_id\\100219_153.txt\n",
      "ABayesianApproachtoTacklingHardComputationalProblems\n",
      "1301.2279v1:ABayesianApproachtoTacklingHardComputationalProblems\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2279v1.txt\n",
      "salvando em txt_autores_id\\100220_153.txt\n",
      "Location-BasedReasoningaboutComplexMulti-AgentBehavior\n",
      "1401.4593v1:Location-BasedReasoningaboutComplexMulti-AgentBehavior\n",
      "abrindo arquivo txt_semRefTitulo\\1401.4593v1.txt\n",
      "ABayesianApproachtoTacklingHardComputationalProblems\n",
      "1301.2279v1:ABayesianApproachtoTacklingHardComputationalProblems\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2279v1.txt\n",
      "salvando em txt_autores_id\\100221_153.txt\n",
      "UsingTemporalDataforMakingRecommendations\n",
      "1301.2320v1:UsingTemporalDataforMakingRecommendations\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2320v1.txt\n",
      "ABayesianApproachtoTacklingHardComputationalProblems\n",
      "1301.2279v1:ABayesianApproachtoTacklingHardComputationalProblems\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2279v1.txt\n",
      "ADecisionTheoreticApproachtoTargetedAdvertising\n",
      "1301.3842v1:ADecisionTheoreticApproachtoTargetedAdvertising\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3842v1.txt\n",
      "DependencyNetworksforCollaborativeFilteringandDataVisualization\n",
      "1301.3862v1:DependencyNetworksforCollaborativeFilteringandDataVisualization\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3862v1.txt\n",
      "ABayesianApproachtoLearningBayesianNetworkswithLocalStructure\n",
      "1302.1528v2:ABayesianApproachtoLearningBayesianNetworkswithLocalStructure\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1528v2.txt\n",
      "LearningBayesianNetworks:TheCombinationofKnowledgeandStatisticalData\n",
      "1302.6815v2:LearningBayesianNetworks:TheCombinationofKnowledgeandStatisticalData\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6815v2.txt\n",
      "salvando em txt_autores_id\\31168_156.txt\n",
      "Convergentmessagepassingalgorithms-aunifyingview\n",
      "1205.2625v1:Convergentmessagepassingalgorithms-aunifyingview\n",
      "abrindo arquivo txt_semRefTitulo\\1205.2625v1.txt\n",
      "TighteningLPRelaxationsforMAPusingMessagePassing\n",
      "1206.3288v1:TighteningLPRelaxationsforMAPusingMessagePassing\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3288v1.txt\n",
      "ConvergentPropagationAlgorithmsviaOrientedTrees\n",
      "1206.5243v1:ConvergentPropagationAlgorithmsviaOrientedTrees\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5243v1.txt\n",
      "DiscriminativeLearningviaSemidefiniteProbabilisticModels\n",
      "1206.6815v1:DiscriminativeLearningviaSemidefiniteProbabilisticModels\n",
      "abrindo arquivo txt_semRefTitulo\\1206.6815v1.txt\n",
      "WhatCannotbeLearnedwithBetheApproximations\n",
      "1202.3731v1:WhatCannotbeLearnedwithBetheApproximations\n",
      "abrindo arquivo txt_semRefTitulo\\1202.3731v1.txt\n",
      "LearningtheExpertsforOnlineSequencePrediction\n",
      "1206.4604v1:LearningtheExpertsforOnlineSequencePrediction\n",
      "abrindo arquivo txt_semRefTitulo\\1206.4604v1.txt\n",
      "ConvexifyingtheBetheFreeEnergy\n",
      "1205.2624v1:ConvexifyingtheBetheFreeEnergy\n",
      "abrindo arquivo txt_semRefTitulo\\1205.2624v1.txt\n",
      "salvando em txt_autores_id\\46317_156.txt\n",
      "ModelingEventswithCascadesofPoissonProcesses\n",
      "1203.3516v1:ModelingEventswithCascadesofPoissonProcesses\n",
      "abrindo arquivo txt_semRefTitulo\\1203.3516v1.txt\n",
      "CT-NOR:RepresentingandReasoningAboutEventsinContinuousTime\n",
      "1206.3280v1:CT-NOR:RepresentingandReasoningAboutEventsinContinuousTime\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3280v1.txt\n",
      "salvando em txt_autores_id\\58541_156.txt\n",
      "TheFactoredFrontierAlgorithmforApproximateInferenceinDBNs\n",
      "1301.2296v1:TheFactoredFrontierAlgorithmforApproximateInferenceinDBNs\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2296v1.txt\n",
      "LatentTopicModelsforHypertext\n",
      "1206.3254v1:LatentTopicModelsforHypertext\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3254v1.txt\n",
      "MAPEstimation,LinearProgrammingandBeliefPropagationwithConvexFreeEnergies\n",
      "1206.5286v1:MAPEstimation,LinearProgrammingandBeliefPropagationwithConvexFreeEnergies\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5286v1.txt\n",
      "TighteningLPRelaxationsforMAPusingMessagePassing\n",
      "1206.3288v1:TighteningLPRelaxationsforMAPusingMessagePassing\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3288v1.txt\n",
      "Convergentmessagepassingalgorithms-aunifyingview\n",
      "1205.2625v1:Convergentmessagepassingalgorithms-aunifyingview\n",
      "abrindo arquivo txt_semRefTitulo\\1205.2625v1.txt\n",
      "salvando em txt_autores_id\\58586_156.txt\n",
      "MeanFieldVariationalApproximationforContinuous-TimeBayesianNetworks\n",
      "1205.2655v1:MeanFieldVariationalApproximationforContinuous-TimeBayesianNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1205.2655v1.txt\n",
      "ContinuousTimeMarkovNetworks\n",
      "1206.6838v1:ContinuousTimeMarkovNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1206.6838v1.txt\n",
      "GibbsSamplinginFactorizedContinuous-TimeMarkovProcesses\n",
      "1206.3251v1:GibbsSamplinginFactorizedContinuous-TimeMarkovProcesses\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3251v1.txt\n",
      "IncorporatingExpressiveGraphicalModelsinVariationalApproximations:Chain-GraphsandHiddenVariables\n",
      "1301.2268v1:IncorporatingExpressiveGraphicalModelsinVariationalApproximations:Chain-GraphsandHiddenVariables\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2268v1.txt\n",
      "salvando em txt_autores_id\\72573_156.txt\n",
      "CT-NOR:RepresentingandReasoningAboutEventsinContinuousTime\n",
      "1206.3280v1:CT-NOR:RepresentingandReasoningAboutEventsinContinuousTime\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3280v1.txt\n",
      "SequentialUpdateofBayesianNetworkStructure\n",
      "1302.1538v1:SequentialUpdateofBayesianNetworkStructure\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1538v1.txt\n",
      "OntheRelationbetweenKappaCalculusandProbabilisticReasoning\n",
      "1302.6797v1:OntheRelationbetweenKappaCalculusandProbabilisticReasoning\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6797v1.txt\n",
      "ActionNetworks:AFrameworkforReasoningaboutActionsandChangeunderUncertainty\n",
      "1302.6796v1:ActionNetworks:AFrameworkforReasoningaboutActionsandChangeunderUncertainty\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6796v1.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReasoningWithQualitativeProbabilitiesCanBeTractable\n",
      "1303.5406v1:ReasoningWithQualitativeProbabilitiesCanBeTractable\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5406v1.txt\n",
      "Makinglifebetteronelargesystematatime:ChallengesforUAIresearch\n",
      "1206.5279v1:Makinglifebetteronelargesystematatime:ChallengesforUAIresearch\n",
      "abrindo arquivo txt_semRefTitulo\\1206.5279v1.txt\n",
      "salvando em txt_autores_id\\72574_156.txt\n",
      "CT-NOR:RepresentingandReasoningAboutEventsinContinuousTime\n",
      "1206.3280v1:CT-NOR:RepresentingandReasoningAboutEventsinContinuousTime\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3280v1.txt\n",
      "salvando em txt_autores_id\\72575_156.txt\n",
      "CT-NOR:RepresentingandReasoningAboutEventsinContinuousTime\n",
      "1206.3280v1:CT-NOR:RepresentingandReasoningAboutEventsinContinuousTime\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3280v1.txt\n",
      "salvando em txt_autores_id\\72576_156.txt\n",
      "CT-NOR:RepresentingandReasoningAboutEventsinContinuousTime\n",
      "1206.3280v1:CT-NOR:RepresentingandReasoningAboutEventsinContinuousTime\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3280v1.txt\n",
      "salvando em txt_autores_id\\72577_156.txt\n",
      "CT-NOR:RepresentingandReasoningAboutEventsinContinuousTime\n",
      "1206.3280v1:CT-NOR:RepresentingandReasoningAboutEventsinContinuousTime\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3280v1.txt\n",
      "salvando em txt_autores_id\\72578_156.txt\n",
      "CT-NOR:RepresentingandReasoningAboutEventsinContinuousTime\n",
      "1206.3280v1:CT-NOR:RepresentingandReasoningAboutEventsinContinuousTime\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3280v1.txt\n",
      "salvando em txt_autores_id\\72561_]256.txt\n",
      "SensitivityAnalysisforThresholdDecisionMakingwithDynamicNetworks\n",
      "1206.6818v1:SensitivityAnalysisforThresholdDecisionMakingwithDynamicNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1206.6818v1.txt\n",
      "PivotalPruningofTrade-offsinQPNs\n",
      "1301.3889v1:PivotalPruningofTrade-offsinQPNs\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3889v1.txt\n",
      "EnhancingQPNsforTrade-offResolution\n",
      "1301.6735v1:EnhancingQPNsforTrade-offResolution\n",
      "abrindo arquivo txt_semRefTitulo\\1301.6735v1.txt\n",
      "ComputingProbabilityIntervalsUnderIndependencyConstraints\n",
      "1304.1140v1:ComputingProbabilityIntervalsUnderIndependencyConstraints\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1140v1.txt\n",
      "HowtoElicitManyProbabilities\n",
      "1301.6745v1:HowtoElicitManyProbabilities\n",
      "abrindo arquivo txt_semRefTitulo\\1301.6745v1.txt\n",
      "TheComputationalComplexityofSensitivityAnalysisandParameterTuning\n",
      "1206.3265v1:TheComputationalComplexityofSensitivityAnalysisandParameterTuning\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3265v1.txt\n",
      "LearningBayesianNetworkParameterswithPriorKnowledgeaboutContext-SpecificQualitativeInfluences\n",
      "1207.1387v1:LearningBayesianNetworkParameterswithPriorKnowledgeaboutContext-SpecificQualitativeInfluences\n",
      "abrindo arquivo txt_semRefTitulo\\1207.1387v1.txt\n",
      "Pre-processingforTriangulationofProbabilisticNetworks\n",
      "1301.2256v1:Pre-processingforTriangulationofProbabilisticNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2256v1.txt\n",
      "MakingSensitivityAnalysisComputationallyEfficient\n",
      "1301.3868v1:MakingSensitivityAnalysisComputationallyEfficient\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3868v1.txt\n",
      "AnalysingSensitivityDatafromProbabilisticNetworks\n",
      "1301.2314v1:AnalysingSensitivityDatafromProbabilisticNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2314v1.txt\n",
      "salvando em txt_autores_id\\46287_256.txt\n",
      "aHUGIN:ASystemCreatingAdaptiveCausalProbabilisticNetworks\n",
      "1303.5421v1:aHUGIN:ASystemCreatingAdaptiveCausalProbabilisticNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5421v1.txt\n",
      "InferenceinMultiplySectionedBayesianNetworkswithExtendedShafer-ShenoyandLazyPropagation\n",
      "1301.6749v1:InferenceinMultiplySectionedBayesianNetworkswithExtendedShafer-ShenoyandLazyPropagation\n",
      "abrindo arquivo txt_semRefTitulo\\1301.6749v1.txt\n",
      "MyopicValueofInformationinInfluenceDiagrams\n",
      "1302.1535v1:MyopicValueofInformationinInfluenceDiagrams\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1535v1.txt\n",
      "UsingROBDDsforInferenceinBayesianNetworkswithTroubleshootingasanExample\n",
      "1301.3880v1:UsingROBDDsforInferenceinBayesianNetworkswithTroubleshootingasanExample\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3880v1.txt\n",
      "RepresentingandSolvingAsymmetricBayesianDecisionProblems\n",
      "1301.3879v1:RepresentingandSolvingAsymmetricBayesianDecisionProblems\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3879v1.txt\n",
      "WelldefinedDecisionScenarios\n",
      "1301.6729v1:WelldefinedDecisionScenarios\n",
      "abrindo arquivo txt_semRefTitulo\\1301.6729v1.txt\n",
      "AnalysisinHUGINofDataConflict\n",
      "1304.1146v1:AnalysisinHUGINofDataConflict\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1146v1.txt\n",
      "OptimalJunctionTrees\n",
      "1302.6823v1:OptimalJunctionTrees\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6823v1.txt\n",
      "TheCostofTroubleshootingCostClusterswithInsideInformation\n",
      "1203.3502v1:TheCostofTroubleshootingCostClusterswithInsideInformation\n",
      "abrindo arquivo txt_semRefTitulo\\1203.3502v1.txt\n",
      "FromInfluenceDiagramstoJunctionTrees\n",
      "1302.6824v1:FromInfluenceDiagramstoJunctionTrees\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6824v1.txt\n",
      "salvando em txt_autores_id\\100259_256.txt\n",
      "EnhancingQPNsforTrade-offResolution\n",
      "1301.6735v1:EnhancingQPNsforTrade-offResolution\n",
      "abrindo arquivo txt_semRefTitulo\\1301.6735v1.txt\n",
      "PivotalPruningofTrade-offsinQPNs\n",
      "1301.3889v1:PivotalPruningofTrade-offsinQPNs\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3889v1.txt\n",
      "AnalysingSensitivityDatafromProbabilisticNetworks\n",
      "1301.2314v1:AnalysingSensitivityDatafromProbabilisticNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2314v1.txt\n",
      "HowtoElicitManyProbabilities\n",
      "1301.6745v1:HowtoElicitManyProbabilities\n",
      "abrindo arquivo txt_semRefTitulo\\1301.6745v1.txt\n",
      "salvando em txt_autores_id\\72593_256.txt\n",
      "MAIES:AToolforDNAMixtureAnalysis\n",
      "1206.6816v1:MAIES:AToolforDNAMixtureAnalysis\n",
      "abrindo arquivo txt_semRefTitulo\\1206.6816v1.txt\n",
      "ConditionsUnderWhichConditionalIndependenceandScoringMethodsLeadtoIdenticalSelectionofBayesianNetworkModels\n",
      "1301.2262v1:ConditionsUnderWhichConditionalIndependenceandScoringMethodsLeadtoIdenticalSelectionofBayesianNetworkModels\n",
      "abrindo arquivo txt_semRefTitulo\\1301.2262v1.txt\n",
      "PropagationusingChainEventGraphs\n",
      "1206.3293v1:PropagationusingChainEventGraphs\n",
      "abrindo arquivo txt_semRefTitulo\\1206.3293v1.txt\n",
      "salvando em txt_autores_id\\78621_256.txt\n",
      "aHUGIN:ASystemCreatingAdaptiveCausalProbabilisticNetworks\n",
      "1303.5421v1:aHUGIN:ASystemCreatingAdaptiveCausalProbabilisticNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5421v1.txt\n",
      "EvaluatingInfluenceDiagramsusingLIMIDs\n",
      "1301.3881v1:EvaluatingInfluenceDiagramsusingLIMIDs\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3881v1.txt\n",
      "MAIES:AToolforDNAMixtureAnalysis\n",
      "1206.6816v1:MAIES:AToolforDNAMixtureAnalysis\n",
      "abrindo arquivo txt_semRefTitulo\\1206.6816v1.txt\n",
      "salvando em txt_autores_id\\101534_256.txt\n",
      "Onreasoninginnetworkswithqualitativeuncertainty\n",
      "1303.1506v1:Onreasoninginnetworkswithqualitativeuncertainty\n",
      "abrindo arquivo txt_semRefTitulo\\1303.1506v1.txt\n",
      "Re-entrantStructuralPhaseTransitioninaFrustratedKagomeMagnet,Rb2SnCu3F12\n",
      "1308.4519v1:Re-entrantStructuralPhaseTransitioninaFrustratedKagomeMagnet,Rb2SnCu3F12\n",
      "abrindo arquivo txt_semRefTitulo\\1308.4519v1.txt\n",
      "PivotalPruningofTrade-offsinQPNs\n",
      "1301.3889v1:PivotalPruningofTrade-offsinQPNs\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3889v1.txt\n",
      "RiskAgoras:DialecticalArgumentationforScientificReasoning\n",
      "1301.3874v1:RiskAgoras:DialecticalArgumentationforScientificReasoning\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3874v1.txt\n",
      "Localstructurecorrelationsinplasticcyclohexane-aReverseMonteCarlostudy\n",
      "1301.3312v1:Localstructurecorrelationsinplasticcyclohexane-aReverseMonteCarlostudy\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3312v1.txt\n",
      "salvando em txt_autores_id\\101901_256.txt\n",
      "UsingROBDDsforInferenceinBayesianNetworkswithTroubleshootingasanExample\n",
      "1301.3880v1:UsingROBDDsforInferenceinBayesianNetworkswithTroubleshootingasanExample\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3880v1.txt\n",
      "AcomputationalschemeforReasoninginDynamicProbabilisticNetworks\n",
      "1303.5407v1:AcomputationalschemeforReasoninginDynamicProbabilisticNetworks\n",
      "abrindo arquivo txt_semRefTitulo\\1303.5407v1.txt\n",
      "MakingSensitivityAnalysisComputationallyEfficient\n",
      "1301.3868v1:MakingSensitivityAnalysisComputationallyEfficient\n",
      "abrindo arquivo txt_semRefTitulo\\1301.3868v1.txt\n",
      "NestedJunctionTrees\n",
      "1302.1553v1:NestedJunctionTrees\n",
      "abrindo arquivo txt_semRefTitulo\\1302.1553v1.txt\n",
      "ReductionofComputationalComplexityinBayesianNetworksthroughRemovalofWeakDependencies\n",
      "1302.6825v1:ReductionofComputationalComplexityinBayesianNetworksthroughRemovalofWeakDependencies\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6825v1.txt\n",
      "salvando em txt_autores_id\\103911_256.txt\n",
      "HowtoElicitManyProbabilities\n",
      "1301.6745v1:HowtoElicitManyProbabilities\n",
      "abrindo arquivo txt_semRefTitulo\\1301.6745v1.txt\n",
      "salvando em txt_autores_id\\103912_256.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HowtoElicitManyProbabilities\n",
      "1301.6745v1:HowtoElicitManyProbabilities\n",
      "abrindo arquivo txt_semRefTitulo\\1301.6745v1.txt\n",
      "salvando em txt_autores_id\\105255_256.txt\n",
      "OptimalJunctionTrees\n",
      "1302.6823v1:OptimalJunctionTrees\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6823v1.txt\n",
      "AnalysisinHUGINofDataConflict\n",
      "1304.1146v1:AnalysisinHUGINofDataConflict\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1146v1.txt\n",
      "ApproximationsinBayesianBeliefUniverseforKnowledgeBasedSystems\n",
      "1304.1101v1:ApproximationsinBayesianBeliefUniverseforKnowledgeBasedSystems\n",
      "abrindo arquivo txt_semRefTitulo\\1304.1101v1.txt\n",
      "FromInfluenceDiagramstoJunctionTrees\n",
      "1302.6824v1:FromInfluenceDiagramstoJunctionTrees\n",
      "abrindo arquivo txt_semRefTitulo\\1302.6824v1.txt\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for index, row in com_idAutores_totulo.iterrows():\n",
    "    com, id_autor, titulos = row\n",
    "    \n",
    "    nome = 'txt_autores_id\\\\' + str(id_autor) + '_' + str(com) + '.txt'\n",
    "    print('salvando em ' + nome)\n",
    "    fileAutor = open(nome,\"w+\",encoding='utf-8')\n",
    "    for t in titulos:\n",
    "        t = remover_acentos(t)\n",
    "        print(t)\n",
    "        idArtigo = dicionario[t]\n",
    "        print(idArtigo + ':' + t)\n",
    "        #capturando conteúdo do artigo\n",
    "        artigo = 'txt_semRefTitulo\\\\' + str(idArtigo) + '.txt' \n",
    "        print('abrindo arquivo ' + artigo)\n",
    "        file = open(artigo,\"r\",encoding='utf-8')\n",
    "        texto = file.read()\n",
    "        fileAutor.write(texto)\n",
    "        fileAutor.write('\\n')\n",
    "        file.close()\n",
    "        sleep(0.05)\n",
    "    fileAutor.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Renato a partir dos arquivos .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\CoffeeLake_01\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\CoffeeLake_01\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(re.sub('[-\\xad][\\s\\n]','',text)) \\\n",
    "                          for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]{3,}', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "    #return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37.7 s\n",
      "(100, 20000)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(input='filename',max_df=0.95, max_features=20000,\n",
    "                                 min_df=0.05, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "import glob\n",
    "t = [f for f in glob.glob(\"txt_autores_id/*.txt\", recursive=False)]\n",
    "#t = !ls txt2/*.txt\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(t)\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\x0f-greedi',\n",
       " '-.fli',\n",
       " '-logic',\n",
       " '-that',\n",
       " '-valu',\n",
       " '/eft-devi',\n",
       " '0and',\n",
       " '0|input',\n",
       " '3-sat',\n",
       " '4-tupl']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('terms.txt','w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\u000f-greedi</th>\n",
       "      <th>-.fli</th>\n",
       "      <th>-logic</th>\n",
       "      <th>-that</th>\n",
       "      <th>-valu</th>\n",
       "      <th>/eft-devi</th>\n",
       "      <th>0and</th>\n",
       "      <th>0|input</th>\n",
       "      <th>3-sat</th>\n",
       "      <th>4-tupl</th>\n",
       "      <th>...</th>\n",
       "      <th>zero weight</th>\n",
       "      <th>zhang</th>\n",
       "      <th>zhang pool</th>\n",
       "      <th>zilberstein</th>\n",
       "      <th>zkl</th>\n",
       "      <th>zkl fix</th>\n",
       "      <th>zkl log</th>\n",
       "      <th>zml</th>\n",
       "      <th>zoeter</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>txt_autores_id\\100199_104.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_autores_id\\100219_153.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_autores_id\\100220_153.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_autores_id\\100221_153.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_autores_id\\100228_6.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               \u000f-greedi     -.fli  -logic     -that  -valu  \\\n",
       "txt_autores_id\\100199_104.txt       0.0  0.004630     0.0  0.002006    0.0   \n",
       "txt_autores_id\\100219_153.txt       0.0  0.000000     0.0  0.000000    0.0   \n",
       "txt_autores_id\\100220_153.txt       0.0  0.000000     0.0  0.000000    0.0   \n",
       "txt_autores_id\\100221_153.txt       0.0  0.000000     0.0  0.005412    0.0   \n",
       "txt_autores_id\\100228_6.txt         0.0  0.002243     0.0  0.000000    0.0   \n",
       "\n",
       "                               /eft-devi  0and  0|input     3-sat  4-tupl  \\\n",
       "txt_autores_id\\100199_104.txt        0.0   0.0      0.0  0.000000     0.0   \n",
       "txt_autores_id\\100219_153.txt        0.0   0.0      0.0  0.009351     0.0   \n",
       "txt_autores_id\\100220_153.txt        0.0   0.0      0.0  0.003108     0.0   \n",
       "txt_autores_id\\100221_153.txt        0.0   0.0      0.0  0.002792     0.0   \n",
       "txt_autores_id\\100228_6.txt          0.0   0.0      0.0  0.002005     0.0   \n",
       "\n",
       "                               ...  zero weight     zhang  zhang pool  \\\n",
       "txt_autores_id\\100199_104.txt  ...     0.000000  0.002809         0.0   \n",
       "txt_autores_id\\100219_153.txt  ...     0.000000  0.000000         0.0   \n",
       "txt_autores_id\\100220_153.txt  ...     0.003477  0.000000         0.0   \n",
       "txt_autores_id\\100221_153.txt  ...     0.000000  0.000000         0.0   \n",
       "txt_autores_id\\100228_6.txt    ...     0.000000  0.000000         0.0   \n",
       "\n",
       "                               zilberstein  zkl  zkl fix  zkl log  zml  \\\n",
       "txt_autores_id\\100199_104.txt          0.0  0.0      0.0      0.0  0.0   \n",
       "txt_autores_id\\100219_153.txt          0.0  0.0      0.0      0.0  0.0   \n",
       "txt_autores_id\\100220_153.txt          0.0  0.0      0.0      0.0  0.0   \n",
       "txt_autores_id\\100221_153.txt          0.0  0.0      0.0      0.0  0.0   \n",
       "txt_autores_id\\100228_6.txt            0.0  0.0      0.0      0.0  0.0   \n",
       "\n",
       "                               zoeter      zone  \n",
       "txt_autores_id\\100199_104.txt     0.0  0.000000  \n",
       "txt_autores_id\\100219_153.txt     0.0  0.000000  \n",
       "txt_autores_id\\100220_153.txt     0.0  0.003108  \n",
       "txt_autores_id\\100221_153.txt     0.0  0.000000  \n",
       "txt_autores_id\\100228_6.txt       0.0  0.000000  \n",
       "\n",
       "[5 rows x 20000 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(x,columns=terms,index=[re.sub('txt?/(\\d+.?\\d+v\\d+)\\.txt','\\\\1',a) for a in t])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\u000f-greedi</th>\n",
       "      <th>-.fli</th>\n",
       "      <th>-logic</th>\n",
       "      <th>-that</th>\n",
       "      <th>-valu</th>\n",
       "      <th>/eft-devi</th>\n",
       "      <th>0and</th>\n",
       "      <th>0|input</th>\n",
       "      <th>3-sat</th>\n",
       "      <th>4-tupl</th>\n",
       "      <th>...</th>\n",
       "      <th>zhang pool</th>\n",
       "      <th>zilberstein</th>\n",
       "      <th>zkl</th>\n",
       "      <th>zkl fix</th>\n",
       "      <th>zkl log</th>\n",
       "      <th>zml</th>\n",
       "      <th>zoeter</th>\n",
       "      <th>zone</th>\n",
       "      <th>comunidade</th>\n",
       "      <th>id_autor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>txt_autores_id\\100199_104.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>104</td>\n",
       "      <td>100199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_autores_id\\100219_153.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>153</td>\n",
       "      <td>100219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_autores_id\\100220_153.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>153</td>\n",
       "      <td>100220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_autores_id\\100221_153.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>153</td>\n",
       "      <td>100221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_autores_id\\100228_6.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>100228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               \u000f-greedi     -.fli  -logic     -that  -valu  \\\n",
       "txt_autores_id\\100199_104.txt       0.0  0.004630     0.0  0.002006    0.0   \n",
       "txt_autores_id\\100219_153.txt       0.0  0.000000     0.0  0.000000    0.0   \n",
       "txt_autores_id\\100220_153.txt       0.0  0.000000     0.0  0.000000    0.0   \n",
       "txt_autores_id\\100221_153.txt       0.0  0.000000     0.0  0.005412    0.0   \n",
       "txt_autores_id\\100228_6.txt         0.0  0.002243     0.0  0.000000    0.0   \n",
       "\n",
       "                               /eft-devi  0and  0|input     3-sat  4-tupl  \\\n",
       "txt_autores_id\\100199_104.txt        0.0   0.0      0.0  0.000000     0.0   \n",
       "txt_autores_id\\100219_153.txt        0.0   0.0      0.0  0.009351     0.0   \n",
       "txt_autores_id\\100220_153.txt        0.0   0.0      0.0  0.003108     0.0   \n",
       "txt_autores_id\\100221_153.txt        0.0   0.0      0.0  0.002792     0.0   \n",
       "txt_autores_id\\100228_6.txt          0.0   0.0      0.0  0.002005     0.0   \n",
       "\n",
       "                               ...  zhang pool  zilberstein  zkl  zkl fix  \\\n",
       "txt_autores_id\\100199_104.txt  ...         0.0          0.0  0.0      0.0   \n",
       "txt_autores_id\\100219_153.txt  ...         0.0          0.0  0.0      0.0   \n",
       "txt_autores_id\\100220_153.txt  ...         0.0          0.0  0.0      0.0   \n",
       "txt_autores_id\\100221_153.txt  ...         0.0          0.0  0.0      0.0   \n",
       "txt_autores_id\\100228_6.txt    ...         0.0          0.0  0.0      0.0   \n",
       "\n",
       "                               zkl log  zml  zoeter      zone  comunidade  \\\n",
       "txt_autores_id\\100199_104.txt      0.0  0.0     0.0  0.000000         104   \n",
       "txt_autores_id\\100219_153.txt      0.0  0.0     0.0  0.000000         153   \n",
       "txt_autores_id\\100220_153.txt      0.0  0.0     0.0  0.003108         153   \n",
       "txt_autores_id\\100221_153.txt      0.0  0.0     0.0  0.000000         153   \n",
       "txt_autores_id\\100228_6.txt        0.0  0.0     0.0  0.000000           6   \n",
       "\n",
       "                               id_autor  \n",
       "txt_autores_id\\100199_104.txt    100199  \n",
       "txt_autores_id\\100219_153.txt    100219  \n",
       "txt_autores_id\\100220_153.txt    100220  \n",
       "txt_autores_id\\100221_153.txt    100221  \n",
       "txt_autores_id\\100228_6.txt      100228  \n",
       "\n",
       "[5 rows x 20002 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comunidade'] = df.index\n",
    "df['comunidade'] = df['comunidade'].str.replace('txt_autores_id','')\n",
    "df['comunidade'] = df['comunidade'].str.replace('\\\\','')\n",
    "df['comunidade'] = df['comunidade'].str.replace('.txt','')\n",
    "df[['id_autor', 'comunidade']] = df['comunidade'].str.split(\"_\", n = 1, expand = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo base de dados sem medida de centralidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "(\"Bin edges must be unique: array([0.00685062, 0.01926059, 0.01926059, 0.01926059]).\\nYou can drop duplicate edges by setting the 'duplicates' kwarg\", 'occurred at index 0|input')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-204-72079a954493>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mautorWord_community\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id_autor'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'comunidade'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m                             \u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_replace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNaN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                             \u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'baixo'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'medio'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'alto'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mautorWord_community\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautorWord_community\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_categories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'zero'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mautorWord_community\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comunidade'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomunidade\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   6485\u001b[0m                          \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6486\u001b[0m                          kwds=kwds)\n\u001b[1;32m-> 6487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;31m# compute the result using the series generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-204-72079a954493>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mautorWord_community\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id_autor'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'comunidade'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m                             \u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_replace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNaN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                             \u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'baixo'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'medio'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'alto'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mautorWord_community\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautorWord_community\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_categories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'zero'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mautorWord_community\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comunidade'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomunidade\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\tile.py\u001b[0m in \u001b[0;36mqcut\u001b[1;34m(x, q, labels, retbins, precision, duplicates)\u001b[0m\n\u001b[0;32m    311\u001b[0m     fac, bins = _bins_to_cuts(x, bins, labels=labels,\n\u001b[0;32m    312\u001b[0m                               \u001b[0mprecision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_lowest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m                               dtype=dtype, duplicates=duplicates)\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     return _postprocess_for_cut(fac, bins, retbins, x_is_series,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\tile.py\u001b[0m in \u001b[0;36m_bins_to_cuts\u001b[1;34m(x, bins, right, labels, precision, include_lowest, dtype, duplicates)\u001b[0m\n\u001b[0;32m    337\u001b[0m             raise ValueError(\"Bin edges must be unique: {bins!r}.\\nYou \"\n\u001b[0;32m    338\u001b[0m                              \u001b[1;34m\"can drop duplicate edges by setting \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m                              \"the 'duplicates' kwarg\".format(bins=bins))\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m             \u001b[0mbins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_bins\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: (\"Bin edges must be unique: array([0.00685062, 0.01926059, 0.01926059, 0.01926059]).\\nYou can drop duplicate edges by setting the 'duplicates' kwarg\", 'occurred at index 0|input')"
     ]
    }
   ],
   "source": [
    "autorWord_community = df.drop(['id_autor','comunidade'],axis=1).\\\n",
    "                            replace(to_replace=0,value=np.NaN).\\\n",
    "                            apply(lambda a: pd.qcut(a,3,labels=['baixo','medio','alto']))\n",
    "autorWord_community = autorWord_community.apply(lambda a: a.cat.add_categories(['zero']))\n",
    "autorWord_community['comunidade'] = df.comunidade\n",
    "autorWord_community['id'] = df.id\n",
    "autorWord_community.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(372, 581)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autorWord_community.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['116', '256', '104', '80', '151', '153', '6', '145', '134', '156'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autorWord_community.comunidade.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo base de dados COM medida de centralidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Label</th>\n",
       "      <th>community</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6474</td>\n",
       "      <td>6474</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8824</td>\n",
       "      <td>8824</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21014</td>\n",
       "      <td>21014</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21015</td>\n",
       "      <td>21015</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31089</td>\n",
       "      <td>31089</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31116</td>\n",
       "      <td>31116</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31136</td>\n",
       "      <td>31136</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31137</td>\n",
       "      <td>31137</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31138</td>\n",
       "      <td>31138</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31151</td>\n",
       "      <td>31151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  Label  community\n",
       "0   6474   6474          6\n",
       "1   8824   8824        116\n",
       "2  21014  21014        151\n",
       "3  21015  21015        151\n",
       "4  31089  31089        134\n",
       "5  31116  31116         80\n",
       "6  31136  31136        145\n",
       "7  31137  31137        116\n",
       "8  31138  31138        116\n",
       "9  31151  31151        151"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nosRede_centralidade_grau = pd.read_csv('C:\\\\Users\\\\CoffeeLake_01\\\\Desktop\\\\ASOC-MGP\\\\arXiv\\\\NosRede_ARXIV-IA_GRAU.csv',\n",
    "                  sep = ',')\n",
    "nosRede_centralidade_grau.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Label</th>\n",
       "      <th>community</th>\n",
       "      <th>abstract</th>\n",
       "      <th>account</th>\n",
       "      <th>accur</th>\n",
       "      <th>accuraci</th>\n",
       "      <th>achiev</th>\n",
       "      <th>action</th>\n",
       "      <th>activ</th>\n",
       "      <th>...</th>\n",
       "      <th>version</th>\n",
       "      <th>view</th>\n",
       "      <th>weak</th>\n",
       "      <th>weight</th>\n",
       "      <th>well-known</th>\n",
       "      <th>wide</th>\n",
       "      <th>world</th>\n",
       "      <th>yield</th>\n",
       "      <th>comunidade</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6474</td>\n",
       "      <td>6474</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>6474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8824</td>\n",
       "      <td>8824</td>\n",
       "      <td>116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116</td>\n",
       "      <td>8824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21014</td>\n",
       "      <td>21014</td>\n",
       "      <td>151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>151</td>\n",
       "      <td>21014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21015</td>\n",
       "      <td>21015</td>\n",
       "      <td>151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>alto</td>\n",
       "      <td>alto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>151</td>\n",
       "      <td>21015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31089</td>\n",
       "      <td>31089</td>\n",
       "      <td>134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>134</td>\n",
       "      <td>31089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31116</td>\n",
       "      <td>31116</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>80</td>\n",
       "      <td>31116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31136</td>\n",
       "      <td>31136</td>\n",
       "      <td>145</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>alto</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145</td>\n",
       "      <td>31136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31137</td>\n",
       "      <td>31137</td>\n",
       "      <td>116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116</td>\n",
       "      <td>31137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31138</td>\n",
       "      <td>31138</td>\n",
       "      <td>116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116</td>\n",
       "      <td>31138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31151</td>\n",
       "      <td>31151</td>\n",
       "      <td>151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>baixo</td>\n",
       "      <td>baixo</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>151</td>\n",
       "      <td>31151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 584 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  Label  community abstract account  accur accuraci achiev action  \\\n",
       "0   6474   6474          6      NaN     NaN    NaN      NaN    NaN    NaN   \n",
       "1   8824   8824        116      NaN     NaN    NaN      NaN    NaN    NaN   \n",
       "2  21014  21014        151      NaN     NaN    NaN      NaN    NaN    NaN   \n",
       "3  21015  21015        151      NaN     NaN    NaN    baixo   alto   alto   \n",
       "4  31089  31089        134      NaN     NaN    NaN      NaN    NaN  baixo   \n",
       "5  31116  31116         80      NaN     NaN    NaN      NaN    NaN    NaN   \n",
       "6  31136  31136        145    medio     NaN    NaN    baixo   alto  baixo   \n",
       "7  31137  31137        116      NaN     NaN    NaN      NaN    NaN    NaN   \n",
       "8  31138  31138        116      NaN     NaN    NaN      NaN    NaN    NaN   \n",
       "9  31151  31151        151      NaN   baixo  baixo    baixo  baixo    NaN   \n",
       "\n",
       "   activ  ... version   view weak weight well-known   wide  world  yield  \\\n",
       "0    NaN  ...     NaN    NaN  NaN    NaN        NaN    NaN    NaN    NaN   \n",
       "1    NaN  ...   medio    NaN  NaN    NaN        NaN    NaN    NaN    NaN   \n",
       "2  baixo  ...     NaN    NaN  NaN  medio        NaN  medio    NaN    NaN   \n",
       "3    NaN  ...     NaN    NaN  NaN  baixo        NaN  medio    NaN    NaN   \n",
       "4    NaN  ...     NaN  medio  NaN  baixo        NaN    NaN    NaN    NaN   \n",
       "5    NaN  ...     NaN    NaN  NaN    NaN        NaN    NaN    NaN  medio   \n",
       "6    NaN  ...   medio    NaN  NaN    NaN      baixo    NaN  baixo    NaN   \n",
       "7    NaN  ...     NaN  medio  NaN    NaN        NaN    NaN  baixo    NaN   \n",
       "8    NaN  ...     NaN    NaN  NaN    NaN        NaN    NaN    NaN    NaN   \n",
       "9  baixo  ...     NaN    NaN  NaN  medio        NaN    NaN    NaN  baixo   \n",
       "\n",
       "  comunidade     id  \n",
       "0          6   6474  \n",
       "1        116   8824  \n",
       "2        151  21014  \n",
       "3        151  21015  \n",
       "4        134  31089  \n",
       "5         80  31116  \n",
       "6        145  31136  \n",
       "7        116  31137  \n",
       "8        116  31138  \n",
       "9        151  31151  \n",
       "\n",
       "[10 rows x 584 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autorWord_community['id'] = autorWord_community.id.astype('int64', copy=False)\n",
    "autorWord_community_c = pd.merge(nosRede_centralidade_grau, autorWord_community, left_on='Id', right_on='id')\n",
    "autorWord_community_c.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 584)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autorWord_community_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['6', '116', '151', '134', '80', '145', '156', '104', '153', '256'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autorWord_community_c.comunidade.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando bases em arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>account</th>\n",
       "      <th>accur</th>\n",
       "      <th>accuraci</th>\n",
       "      <th>achiev</th>\n",
       "      <th>action</th>\n",
       "      <th>activ</th>\n",
       "      <th>acycl</th>\n",
       "      <th>adapt</th>\n",
       "      <th>addit</th>\n",
       "      <th>...</th>\n",
       "      <th>version</th>\n",
       "      <th>view</th>\n",
       "      <th>weak</th>\n",
       "      <th>weight</th>\n",
       "      <th>well-known</th>\n",
       "      <th>wide</th>\n",
       "      <th>world</th>\n",
       "      <th>yield</th>\n",
       "      <th>comunidade</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>txt\\100194_116.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alto</td>\n",
       "      <td>baixo</td>\n",
       "      <td>medio</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116</td>\n",
       "      <td>100194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt\\100195_116.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116</td>\n",
       "      <td>100195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt\\100196_116.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alto</td>\n",
       "      <td>baixo</td>\n",
       "      <td>medio</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116</td>\n",
       "      <td>100196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt\\100197_256.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>256</td>\n",
       "      <td>100197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt\\100198_256.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>256</td>\n",
       "      <td>100198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt\\100199_104.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>alto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>alto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>alto</td>\n",
       "      <td>...</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>medio</td>\n",
       "      <td>medio</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104</td>\n",
       "      <td>100199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt\\100200_80.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>medio</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>100200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt\\100204_151.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>151</td>\n",
       "      <td>100204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt\\100219_153.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>153</td>\n",
       "      <td>100219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt\\100220_153.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>medio</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>alto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>153</td>\n",
       "      <td>100220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 581 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   abstract account  accur accuraci achiev action activ acycl  \\\n",
       "txt\\100194_116.txt      NaN   baixo    NaN      NaN    NaN    NaN   NaN   NaN   \n",
       "txt\\100195_116.txt      NaN     NaN    NaN      NaN    NaN    NaN   NaN   NaN   \n",
       "txt\\100196_116.txt      NaN   baixo    NaN      NaN    NaN    NaN   NaN   NaN   \n",
       "txt\\100197_256.txt      NaN     NaN    NaN      NaN    NaN    NaN   NaN   NaN   \n",
       "txt\\100198_256.txt      NaN     NaN    NaN      NaN    NaN    NaN   NaN   NaN   \n",
       "txt\\100199_104.txt      NaN    alto    NaN    baixo   alto    NaN   NaN   NaN   \n",
       "txt\\100200_80.txt       NaN     NaN    NaN      NaN    NaN    NaN   NaN   NaN   \n",
       "txt\\100204_151.txt      NaN     NaN    NaN      NaN    NaN    NaN   NaN   NaN   \n",
       "txt\\100219_153.txt      NaN     NaN    NaN      NaN    NaN    NaN   NaN   NaN   \n",
       "txt\\100220_153.txt      NaN   medio  medio    medio    NaN  baixo  alto   NaN   \n",
       "\n",
       "                    adapt  addit  ... version  view   weak weight well-known  \\\n",
       "txt\\100194_116.txt  baixo    NaN  ...     NaN  alto  baixo  medio      baixo   \n",
       "txt\\100195_116.txt  medio    NaN  ...     NaN   NaN    NaN  medio        NaN   \n",
       "txt\\100196_116.txt  baixo    NaN  ...     NaN  alto  baixo  medio      baixo   \n",
       "txt\\100197_256.txt    NaN    NaN  ...     NaN   NaN    NaN    NaN       alto   \n",
       "txt\\100198_256.txt    NaN    NaN  ...     NaN   NaN    NaN    NaN       alto   \n",
       "txt\\100199_104.txt  medio   alto  ...   baixo   NaN    NaN  baixo      medio   \n",
       "txt\\100200_80.txt   medio  medio  ...     NaN   NaN    NaN    NaN        NaN   \n",
       "txt\\100204_151.txt    NaN    NaN  ...     NaN   NaN    NaN    NaN        NaN   \n",
       "txt\\100219_153.txt    NaN    NaN  ...     NaN   NaN    NaN    NaN        NaN   \n",
       "txt\\100220_153.txt    NaN    NaN  ...     NaN   NaN    NaN    NaN        NaN   \n",
       "\n",
       "                     wide  world yield comunidade      id  \n",
       "txt\\100194_116.txt    NaN  baixo   NaN        116  100194  \n",
       "txt\\100195_116.txt    NaN    NaN   NaN        116  100195  \n",
       "txt\\100196_116.txt    NaN  baixo   NaN        116  100196  \n",
       "txt\\100197_256.txt    NaN    NaN   NaN        256  100197  \n",
       "txt\\100198_256.txt    NaN    NaN   NaN        256  100198  \n",
       "txt\\100199_104.txt  medio  medio   NaN        104  100199  \n",
       "txt\\100200_80.txt     NaN    NaN   NaN         80  100200  \n",
       "txt\\100204_151.txt    NaN    NaN   NaN        151  100204  \n",
       "txt\\100219_153.txt    NaN    NaN   NaN        153  100219  \n",
       "txt\\100220_153.txt    NaN    NaN   NaN        153  100220  \n",
       "\n",
       "[10 rows x 581 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autorWord_community.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Label</th>\n",
       "      <th>community</th>\n",
       "      <th>abstract</th>\n",
       "      <th>account</th>\n",
       "      <th>accur</th>\n",
       "      <th>accuraci</th>\n",
       "      <th>achiev</th>\n",
       "      <th>action</th>\n",
       "      <th>activ</th>\n",
       "      <th>...</th>\n",
       "      <th>version</th>\n",
       "      <th>view</th>\n",
       "      <th>weak</th>\n",
       "      <th>weight</th>\n",
       "      <th>well-known</th>\n",
       "      <th>wide</th>\n",
       "      <th>world</th>\n",
       "      <th>yield</th>\n",
       "      <th>comunidade</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6474</td>\n",
       "      <td>6474</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>6474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8824</td>\n",
       "      <td>8824</td>\n",
       "      <td>116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116</td>\n",
       "      <td>8824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21014</td>\n",
       "      <td>21014</td>\n",
       "      <td>151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>151</td>\n",
       "      <td>21014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21015</td>\n",
       "      <td>21015</td>\n",
       "      <td>151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>alto</td>\n",
       "      <td>alto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>151</td>\n",
       "      <td>21015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31089</td>\n",
       "      <td>31089</td>\n",
       "      <td>134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>134</td>\n",
       "      <td>31089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31116</td>\n",
       "      <td>31116</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>80</td>\n",
       "      <td>31116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31136</td>\n",
       "      <td>31136</td>\n",
       "      <td>145</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>alto</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145</td>\n",
       "      <td>31136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31137</td>\n",
       "      <td>31137</td>\n",
       "      <td>116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116</td>\n",
       "      <td>31137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31138</td>\n",
       "      <td>31138</td>\n",
       "      <td>116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116</td>\n",
       "      <td>31138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31151</td>\n",
       "      <td>31151</td>\n",
       "      <td>151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>baixo</td>\n",
       "      <td>baixo</td>\n",
       "      <td>baixo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baixo</td>\n",
       "      <td>151</td>\n",
       "      <td>31151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 584 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  Label  community abstract account  accur accuraci achiev action  \\\n",
       "0   6474   6474          6      NaN     NaN    NaN      NaN    NaN    NaN   \n",
       "1   8824   8824        116      NaN     NaN    NaN      NaN    NaN    NaN   \n",
       "2  21014  21014        151      NaN     NaN    NaN      NaN    NaN    NaN   \n",
       "3  21015  21015        151      NaN     NaN    NaN    baixo   alto   alto   \n",
       "4  31089  31089        134      NaN     NaN    NaN      NaN    NaN  baixo   \n",
       "5  31116  31116         80      NaN     NaN    NaN      NaN    NaN    NaN   \n",
       "6  31136  31136        145    medio     NaN    NaN    baixo   alto  baixo   \n",
       "7  31137  31137        116      NaN     NaN    NaN      NaN    NaN    NaN   \n",
       "8  31138  31138        116      NaN     NaN    NaN      NaN    NaN    NaN   \n",
       "9  31151  31151        151      NaN   baixo  baixo    baixo  baixo    NaN   \n",
       "\n",
       "   activ  ... version   view weak weight well-known   wide  world  yield  \\\n",
       "0    NaN  ...     NaN    NaN  NaN    NaN        NaN    NaN    NaN    NaN   \n",
       "1    NaN  ...   medio    NaN  NaN    NaN        NaN    NaN    NaN    NaN   \n",
       "2  baixo  ...     NaN    NaN  NaN  medio        NaN  medio    NaN    NaN   \n",
       "3    NaN  ...     NaN    NaN  NaN  baixo        NaN  medio    NaN    NaN   \n",
       "4    NaN  ...     NaN  medio  NaN  baixo        NaN    NaN    NaN    NaN   \n",
       "5    NaN  ...     NaN    NaN  NaN    NaN        NaN    NaN    NaN  medio   \n",
       "6    NaN  ...   medio    NaN  NaN    NaN      baixo    NaN  baixo    NaN   \n",
       "7    NaN  ...     NaN  medio  NaN    NaN        NaN    NaN  baixo    NaN   \n",
       "8    NaN  ...     NaN    NaN  NaN    NaN        NaN    NaN    NaN    NaN   \n",
       "9  baixo  ...     NaN    NaN  NaN  medio        NaN    NaN    NaN  baixo   \n",
       "\n",
       "  comunidade     id  \n",
       "0          6   6474  \n",
       "1        116   8824  \n",
       "2        151  21014  \n",
       "3        151  21015  \n",
       "4        134  31089  \n",
       "5         80  31116  \n",
       "6        145  31136  \n",
       "7        116  31137  \n",
       "8        116  31138  \n",
       "9        151  31151  \n",
       "\n",
       "[10 rows x 584 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autorWord_community_c.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "autorWord_community = autorWord_community.drop(['id'], axis = 1)\n",
    "autorWord_community.fillna('zero').to_csv('arXiv_resumoTitulo.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "autorWord_community_c = autorWord_community_c.drop(['Id','Label','community','id'], axis = 1)\n",
    "autorWord_community_c.fillna('zero').to_csv('arXiv_resumoTitulo_centralidadeGrau.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
