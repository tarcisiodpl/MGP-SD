
We present a class of inequality constraints on
the set of distributions induced by local interventions on variables governed by a causal Bayesian
network, in which some of the variables remain
unmeasured. We derive bounds on causal effects that are not directly measured in randomized experiments. We derive instrumental inequality type of constraints on nonexperimental
distributions. The results have applications in
testing causal models with observational or experimental data.

1

Introduction

The use of graphical models for encoding distributional and causal information is now fairly standard
[Heckerman and Shachter, 1995,
Lauritzen, 2000,
Pearl, 2000, Spirtes et al., 2001].
The most common
such representation involves a causal Bayesian network
(BN), namely, a directed acyclic graph (DAG) G which, in
addition to the usual conditional independence interpretation, is also given a causal interpretation. This additional
feature permits one to infer the effects of interventions
or actions, such as those encountered in policy analysis,
treatment management, or planning. Specifically, if an
external intervention fixes any set T of variables to some
constants t, the DAG permits us to infer the resulting
post-intervention distribution, denoted by Pt (v),1 from
the pre-intervention distribution P (v). The quantity
Pt (y), often called the “causal effect” of T on Y , is what
we normally assess in a controlled experiment with T
randomized, in which the distribution of Y is estimated
for each level t of T . We will call a post-intervention
distribution an interventional distribution, and call the
distribution P (v) nonexperimental distribution.
1

[Pearl, 1995a, Pearl, 2000] used the notation P (v|set(t)),
P (v|do(t)), or P (v|t̂) for the post-intervention distribution,
while [Lauritzen, 2000] used P (v||t).

Jin Tian
Department of Computer Science
Iowa State University
Ames, IA 50011
jtian@cs.iastate.edu

The validity of a causal model can be tested only if it has
empirical implications, that is, it must impose constraints
on the statistics of the data collected. A causal BN not
only imposes constraints on the nonexperimental distribution but also on the interventional distributions that can
be induced by the network. Therefore a causal BN can
be tested and falsified by using two types of data, observational, which are passively observed, and experimental,
which are produced by manipulating (randomly) some variables and observing the states of other variables. The ability to use a mixture of observational and experimental data
will greatly increase our power of causal reasoning and
learning. The use of a mixture of experimental and observational data in learning causal BN is demonstrated in
[Cooper and Yoo, 1999, Heckerman, 1995]. In this paper
we consider using combined observational and experimental data for causal model testing.
There has been much research on identifying observational implications of BNs. It is well-known that
the observational implications of a BN are completely
captured by conditional independence relationships
among the variables when all the variables are observed
[Pearl et al., 1990]. When a BN invokes unobserved
variables, called hidden or latent variables, the network
structure may impose other equality and/or inequality
constraints on the distribution of the observed variables
[Verma and Pearl, 1990,
Robins and Wasserman, 1997,
Desjardins, 1999, Spirtes et al., 2001].
Methods
for identifying equality constraints were given in
[Geiger and Meek, 1998,
Tian and Pearl, 2002b].
[Pearl, 1995b] gave an example of inequality constraints
in the model shown in Figure 1. The model imposes the
following inequality, called the instrumental inequality by
Pearl, for discrete variables X, Y , and Z,
X
max
max P (xy|z) ≤ 1.
(1)
x

y

z

This model has been further analysed using convex analysis approach in [Bonet, 2001]. In principle, all (equality and inequality) constraints implied by BNs with hidden variables can be derived by the quantifier elimination

U
Z

X

Y

all its non-descendants given its direct parents in the graph.
These restrictions imply that the joint probability function
P (v) = P (v1 , . . . , vn ) factorizes according to the product
Y
P (v) =
P (vi |pai )
(2)
i

Figure 1: U is a hidden variable.
method presented in [Geiger and Meek, 1999]. However,
due to high computational demand (doubly exponential in
the number of probabilistic parameters), in practice, quantifier elimination is limited to BNs with few number of probabilistic parameters. For example, the current quantifier
elimination algorithms cannot deal with the simple model
in Figure 1 for X, Y , and Z being binary variables.
When all variables are observed, a complete characterization of constraints on interventional distributions imposed by a given causal BN has been given in [Pearl, 2000,
pp.23-4]. When a causal BN contains unobserved variables, there may be inequality constraints on interventional distributions [Tian and Pearl, 2002a]. For the model
in Figure 1, bounds on causal effects Px (y) in terms of
the nonexperimental distribution P (x, y, z) was derived in
[Balke and Pearl, 1994, Chickering and Pearl, 1996] using
linear programming method for X, Y , and Z being binary
variables.
In this paper, we seek the constraints imposed by a causal
BN with hidden variables on both nonexperimental and interventional distributions. We present a type of inequality constraints on interventional distributions. We derive
bounds on causal effects in terms of nonexperimental distributions and given interventional distributions. We derive
instrumental inequality type of constraints upon nonexperimental distributions. Although the constraints we give are
not complete, they constitute necessary conditions for a hypothesized model to be compatible with the data. The constraints also provide information (bounds) on the effects of
interventions that have not been tried experimentally, from
observational data and given experimental data.

2

Causal Bayesian Networks and
Interventions

A causal Bayesian network, also known as a Markovian
model, consists of two mathematical objects: (i) a DAG
G, called a causal graph, over a set V = {V1 , . . . , Vn }
of vertices, and (ii) a probability distribution P (v), over
the set V of discrete variables that correspond to the vertices in G.2 The interpretation of such a graph has two
components, probabilistic and causal. The probabilistic interpretation views G as representing conditional independence restrictions on P : Each variable is independent of
2

We only consider discrete random variables in this paper.

where pai are (values of) the parents of variable Vi in G.
The causal interpretation views the arrows in G as representing causal influences between the corresponding variables. In this interpretation, the factorization of (2) still
holds, but the factors are further assumed to represent autonomous data-generation processes, that is, each conditional probability P (vi |pai ) represents a stochastic process by which the values of Vi are assigned in response
to the values pai (previously chosen for Vi ’s parents), and
the stochastic variation of this assignment is assumed independent of the variations in all other assignments in the
model. Moreover, each assignment process remains invariant to possible changes in the assignment processes that
govern other variables in the system. This modularity assumption enables us to predict the effects of interventions,
whenever interventions are described as specific modifications of some factors in the product of (2). The simplest
such intervention, called atomic, involves fixing a set T of
variables to some constants T = t, which yields the postintervention distribution
 Q
{i|Vi 6∈T } P (vi |pai ) v consistent with t.
Pt (v) =
0
v inconsistent with t.
(3)
Eq. (3) represents a truncated factorization of (2), with
factors corresponding to the manipulated variables removed. This truncation follows immediately from (2)
since, assuming modularity, the post-intervention probabilities P (vi |pai ) corresponding to variables in T are either
1 or 0, while those corresponding to unmanipulated variables remain unaltered. If T stands for a set of treatment
variables and Y for an outcome variable in V \ T , then Eq.
(3) permits us to calculate the probability Pt (y) that event
Y = y would occur if treatment condition T = t were
enforced uniformly over the population.
When some variables in a Markovian model are unobserved, the probability distribution over the observed variables may no longer be decomposed as in Eq. (2). Let V =
{V1 , . . . , Vn } and U = {U1 , . . . , Un′ } stand for the sets of
observed and unobserved variables respectively. If no U
variable is a descendant of any V variable, then the corresponding model is called a semi-Markovian model. In this
paper, we only consider semi-Markovian models. However, the results can be generalized to models with arbitrary
unobserved variables as shown in [Tian and Pearl, 2002b].
In a semi-Markovian model, the observed probability distribution, P (v), becomes a mixture of products:
XY
P (v) =
P (vi |pai , ui )P (u)
(4)
u

i

U1

where P Ai and U i stand for the sets of the observed and
unobserved parents of Vi , and the summation ranges over
all the U variables. The post-intervention distribution, likewise, will be given as a mixture of truncated products
Pt (v)
8 X Y
<
P (vi |pai , ui )P (u)
=
u {i|Vi 6∈T }
:
0

W1

X

Y

v inconsistent with t.
(5)

Z

W2

U3

v consistent with t.

U2

Figure 2: U1 ,U2 and U3 are hidden variables.

Assuming that v is consistent with t, we can write
Pt (v) = Pt (v \ t)

(6)

In the rest of the paper, we will use Pt (v) and Pt (v \ t)
interchangeably, always assuming v being consistent with
t.

3

j = 1, . . . , l, is computable from Pv\h (v) and is given
by
Y
Pv\h(i) (v)
Pv\hj (v) =
,
(9)
Pv\h(i−1) (v)
{i|Vhi ∈Hj }

where each Pv\h(i) (v), i = 0, 1, . . . , k, is given by

Constraints on Interventional
Distributions

Pv\h(i) (v) =

X

Pv\h (v).

(10)

h\h(i)

Let P∗ denote the set of all interventional distributions induced by a given semi-Markovian model,
P∗ = {Pt (v)|T ⊆ V, t ∈ Dm(T ), v ∈ Dm(V )}

(7)

where Dm(T ) represents the domain of T . What are the
constraints imposed by the model on the interventional distributions in P∗ ? The structure of the causal graph G will
play an important role in finding these constraints. A ccomponent is a maximal set of vertices such that any two
vertices in the set are connected by a path on which every
edge is of the form L99 U 99K where U is a hidden variable. The set of variables V is then partitioned into a set of
c-components. For example, the causal graph G in Figure 2
consists of two c-components {X, Y, Z} and {W1 , W2 }.
Let G(H) denote the subgraph of G composed only of the
variables in H and the hidden variables that are ancestors
of H. In general, equality constraints on the set of interventional distributions can be found using the following three
lemmas.
Lemma 1 [Tian and Pearl, 2002b] Let H ⊆ V , and assume that H is partitioned into c-components H1 , . . . , Hl
in the subgraph G(H). Then we have

A special case of Lemma 1 is when H = V , and we have
the following Lemma.
Lemma 2 [Tian and Pearl, 2002b] Assuming that V is
partitioned into c-components S1 , . . . , Sk , we have
(i) P (v) =

Q

i

Pv\si (v).

(ii) Let a topological order over V be V1 < . . . < Vn , and
let V (i) = {V1 , . . . , Vi }, i = 1, . . . , n, and V (0) = ∅.
Then each Pv\sj (v), j = 1, . . . , k, is computable from
P (v) and is given by
Pv\sj (v) =

Y

P (vi |v (i−1) ).

(11)

{i|Vi ∈Sj }

The next lemma provides a condition under which we can
compute Pv\w (w) from Pv\c (c) where W is a subset of C,
by simply summing Pv\c (c) over other variables C \ W .

(8)

Lemma 3 [Tian and Pearl, 2002b] Let W ⊆ C ⊆ V , and
W ′ = C \ W . If W contains its own observed ancestors in
G(C), then
X
Pv\c (v) = Pv\w (v).
(12)

(ii) Let k be the number of variables in H, and let a topological order of the variables in H be Vh1 < . . . <
Vhk in G(H). Let H (i) = {Vh1 , . . . , Vhi } be the set
of variables in H ordered before Vhi (including Vhi ),
i = 1, . . . , k, and H (0) = ∅. Then each Pv\hj (v),

The set of equality constraints implied by these three lemmas can be systematically listed by slightly modifying the
procedure in [Tian and Pearl, 2002b] for listing equality
constraints on nonexperimental distributions. We will not
show the details of the procedure here since the focus of
this paper is on inequality constraints.

(i) Pv\h (v) decomposes as
Pv\h (v) =

Y

Pv\hi (v).

w′

i

For example, the model in Figure 1 imposes the following
equality constraints.
Pz (xy) = P (xy|z)
Pyz (x) = P (x|z)

(13)
(14)

Pxz (y) = Px (y)

(15)

since for all Vi ∈ V
0 ≤ P (vi |pai , ui ) ≤ 1.

(29)


′

The model in Figure 2 imposes the following equality constraints.

For a fixed S1′ set, there are 2|S1 | number of Eq. (26) type of
inequalities. For different S1′ sets, some of those inequalities may imply others as shown in the following lemma.

Pw1 w2 (xyz) = P (z|w1 xw2 y)P (y|w1 xw2 )P (x|w1 )
′′

(16)
Pw1 w2 z (xy) = P (y|w1 xw2 )P (x|w1 )
Pw1 w2 y (xz) = Pw1 y (xz)

(17)
(18)

Pw1 w2 x (yz) = Pw2 x (yz)
Pw1 w2 yz (x) = P (x|w1 )

(19)
(20)

Pw1 w2 xz (y) = Pw2 x (y)
Pw1 w2 xy (z) = Py (z)
Pxyz (w1 w2 ) = P (w2 |w1 x)P (w1 )

(21)
(22)
(23)

Pxyzw2 (w1 ) = P (w1 )
X
P (w2 |w1 x)P (w1 )
Pxyzw1 (w2 ) =

(24)
(25)

Lemma 5 If S1′ ⊂ S1′′ , then the set of 2|S1 | inequalities,
∀S1 ⊆ S1′′ ,
X

(−1)|S2 | Pv\(s1 ∪s2 ) (v) ≥ 0,

∀v ∈ Dm(V )

S2 ⊆S1′′ \S1

(30)
′

imply the set of 2|S1 | inequalities, ∀S1 ⊆ S1′ ,
X

(−1)|S2 | Pv\(s1 ∪s2 ) (v) ≥ 0,

∀v ∈ Dm(V )

S2 ⊆S1′ \S1

(31)

w1

3.1

Inequality Constraints

The proof is omitted due to space limitation.

In this paper, we are concerned with inequality constraints
imposed by a model. The P∗ set induced from a semiMarkovian model must satisfy the following inequality
constraints.
Lemma 4 For any S1 ⊆ V and any superset S1′ ⊆ V of
S1 , we have
X
(−1)|S2 | Pv\(s1 ∪s2 ) (v) ≥ 0, ∀v ∈ Dm(V )
S2 ⊆S1′ \S1

(26)

Assume that the set of variables V in the model is partitioned into c-components T1 , . . . , Tk . Due to the equality constraints given in Lemma 1, instead of listing 2|V |
Eq. (26) type of inequalities, we only need to give 2|Ti |
Eq. (26) type of inequalities for each c-component Ti .
Proposition 1 Let the set of variables V in a semiMarkovian model be partitioned into c-components
T1 , . . . , Tk . The P∗ set must satisfy the following inequality
constraints: for i = 1, . . . , k, ∀S1 ⊆ Ti ,

where |S2 | represents the number of variables in S2 .
X

Proof: We use the following equation.
k
Y

(−1)|S2 | Pv\(s1 ∪s2 ) (v) ≥ 0,

∀v ∈ Dm(V )

S2 ⊆Ti \S1

(32)

(1 − ai )

i=1

=1−

X

ai +

i

X

ai aj − . . . + (−1)k a1 . . . ak . (27)

i,j

Take aj = P (vj |paj , uj ), we have that
X Y
P (vi |pai , ui )
u {i|Vi ∈S1 }

Y

(1 − P (vj |paj , uj ))P (u)

{j|Vj ∈S1′ \S1 }

=

X

S2 ⊆S1′ \S1

(−1)|S2 | Pv\(s1 ∪s2 ) (v) ≥ 0

(28)

For example, Proposition 1 gives the following inequality
constraints for the model in Figure 1,
1 − Pyz (x) − Pxz (y) + Pz (xy) ≥ 0
Pyz (x) − Pz (xy) ≥ 0
Pxz (y) − Pz (xy) ≥ 0

(33)
(34)
(35)

Pz (xy) ≥ 0,

(36)

in which (36) is trivial, and (34) becomes trivial because of
equality constraints (13) and (14).

For the model in Figure 2, Proposition 1 gives the following
inequality constraints for the c-component {X, Y, Z},
1 − Pw1 w2 yz (x) − Pw1 w2 xz (y) − Pw1 w2 xy (z)
+ Pw1 w2 z (xy) + Pw1 w2 y (xz) + Pw1 w2 x (yz)
(37)
− Pw1 w2 (xyz) ≥ 0
Pw1 w2 yz (x) − Pw1 w2 z (xy) − Pw1 w2 y (xz)
+ Pw1 w2 (xyz) ≥ 0
Pw1 w2 xz (y) − Pw1 w2 z (xy) − Pw1 w2 x (yz)

(38)

+ Pw1 w2 (xyz) ≥ 0
Pw1 w2 xy (z) − Pw1 w2 y (xz) − Pw1 w2 x (yz)

(39)

+ Pw1 w2 (xyz) ≥ 0
Pw1 w2 z (xy) − Pw1 w2 (xyz) ≥ 0

(40)
(41)

Pw1 w2 y (xz) − Pw1 w2 (xyz) ≥ 0
Pw1 w2 x (yz) − Pw1 w2 (xyz) ≥ 0

(42)
(43)

Pw1 w2 (xyz) ≥ 0,

(44)

some of which are implied by the set of equality constraints
(16)-(25). It can be shown that all inequality constraints
for c-component {W1 , W2 } are implied by equality constraints.
Note that in general, the inequality constraints given in
this section are not the complete set of constraints that are
implied by a given model. For example, for the model
given in Figure 1, the sharp bounds on Px (y) given in
[Balke and Pearl, 1994] for X, Y , and Z being binary variables are not implied by (33)-(36).

4

Inequality Constraints On a Subset of
Interventional Distributions

Proposition 1 gives a set of inequality constraints on the set
of interventional distributions in P∗ . In practical situations,
we may be interested in constraints involving only a certain
subset of interventional distributions. For example, (i) We
have done some experiments, and obtained Ps (v) for some
sets S. We want to know whether these data are compatible with the given model. For this purpose, we would like
inequality constraints involving only those known interventional distributions; (ii) A special case of (i) is that we only
have the nonexperimental distribution P (v). We want inequality constraints on P (v) imposed by the model; (iii) In
practice, certain experiments may be difficult or expensive
to perform. Still, we want some information about a particular causal effect, given some known interventional distributions and nonexperimental distribution. We may provide
bounds on concerned causal effect that can be derived from
those inequality constraints (if this causal effect is not computable from given quantity through equality constraints).
To restrict the set of inequality constraints given in Proposition 1 to constraints involving only certain subset of interventional distributions, in principle, we can treat each

Ps (v) for an instantiation of v ∈ Dm(V ) as a variable,
and solve the inequalities to eliminate unwanted variables
using methods like Fourier-Motzkin elimination or quantifier elimination. However, this is typically only practical
for small number of binary variables due to high computational complexity. In this paper, we show some inequality constraints involving only interventional distributions of
interests that can be derived from those in Proposition 1.
In general, these constraints may not include all the possible constraints that could be derived from Proposition 1 in
principle.
Instead of directly solving the inequality constraints given
in Proposition 1, we consider the inequality in Eq. (26) for
every S1′ ⊆ Ti . We keep every inequality that involves only
the interventional distributions of interests. Those inequalities that contain unwanted interventional distributions may
lead to some new inequalities. For example, in the model
in Figure 2, consider the following inequality that follows
from (26) with S1 = {Z} and S1′ = {Y, Z},
Pw1 w2 xy (z) − Pw1 w2 x (yz) ≥ 0.

(45)

Suppose we want constraints on Pw1 w2 x (yz) and get rid
of unknown quantity Pw1 w2 xy (z). First we have equality
constraints (19) and (22), and Eq. (45) becomes
Pw2 x (yz) ≤ Py (z)

(46)

Pw2 x (yz) is a function of W2 and X but Py (z) is not,
which leads to
max Pw2 x (yz) ≤ Py (z)
w2 ,x
X
max Pw2 x (yz) ≤ 1
z

w2 ,x

(47)
(48)

Eq. (48) is a nontrivial inequality constraint on
Pw1 w2 x (yz) = Pw2 x (yz), which can also be represented as
Pw2 x (yz0 ) + Pw2′ x′ (yz1 ) ≤ 1

(49)

for any w2 ∈ Dm(W2 ), x ∈ Dm(X), w2′ ∈ Dm(W2 ) and
x′ ∈ Dm(X) when Z is binary (Dm(Z) = {z0 , z1 }).
From the above considerations, we give a procedure in
Figure 3 that lists the inequality constraints on the interventional distributions of interest. The procedure has a
complexity of 32|Ti | . Note that A will always contain the
nonexperimental distribution and all interventional distributions that can be computed from P (v) (via equality constraints).
In Step 1, we list the inequalities that do not involve unwanted quantities (i.e., interventional distributions not included in A). Note that we remove some redundant inequalities based on the following lemma.

procedure FindIneqs(G,A)
INPUT: a causal graph G, interventional distributions of
interest A, equality constraints implied by G
OUTPUT: inequalities of interests, IETi for each ccomponent Ti , i = 1, . . . , k
Step 1:
For each c-component Ti , i = 1, . . . , k
For each S1 ⊆ Ti (small to large)
For each S1′ ⊆ Ti such that S1 ⊆ S1′ (small to large)
Study the P
inequality
eS1 ,S1′ = S2 ⊆S ′ \S1 (−1)|S2 | Pv\(s1 ∪s2 ) (v) ≥ 0
1
If every interventional distribution in eS1 ,S1′ is in A
IETi = IETi ∪ {eS1 ,S1′ ≥ 0};
Remove any eS1 ,R in IETi such that R ⊂ S1′ ;
Step 2:
For each c-component Ti , i = 1, . . . , k
For each S1 ⊆ Ti (small to large)
For each S1′ ⊆ Ti such that S1 ⊆ S1′ (small to large)
Study the P
inequality
eS1 ,S1′ = S2 ⊆S ′ \S1 (−1)|S2 | Pv\(s1 ∪s2 ) (v) ≥ 0
1
If some interventional distribution in eS1 ,S1′ is
not in A
IETi = IETi ∪ {eS1 ,S1′ ≥ 0 reformulated
in the form of (55)};

Eq. (26) as eS1 ,S1′ ≥ 0, with
eS1 ,S1′
X
X
(−1)|R|−|S1 | Pv\r (v)
(−1)|R|−|S1 | Pv\r (v) +
=
(52)
where W1 = {S1 ∪ S2 |S2 ⊆ S1′ \ S1 , Pv\(s1 ∪s2 ) (v) is in
A} and W2 = {S1 ∪ S2 |S2 ⊆ S1′ \ S1 , Pv\(s1 ∪s2 ) (v) is not
in A}. We have
X
X
(−1)|R|−|S1 | Pv\r (v).
(−1)|R|−|S1 | Pv\r (v) ≥ −
R∈W2

R∈W1

(53)
Suppose the left-hand side is a function of variables E1 and
the right-hand side is a function of variables E2 . Then,
X
(−1)|R|−|S1 | Pv\r (v)
min
E1 \E2

≥−

R∈W1

X

(−1)|R|−|S1 | Pv\r (r).

(54)

R∈W2

S

Let Q =
X
Q

Figure 3: A Procedure for Listing Inequality Constraints
On a Subset of Interventional Distributions

R∈W2

R∈W1

R. We obtain,
X
(−1)|R|−|S1 | Pv\r (v)
min
R∈W2

E1 \E2

≥−

X

R∈W1

R∈W2

(−1)|R|−|S1 |

Y

|Dm(Vi )|.

(55)

{i|Vi ∈Q\R}

Note that if E1 \ E2 = ∅, then we do not need minE1 \E2 .
Lemma 6 Let Sup(S1 ) denote the set of supersets of S1 such that S1′ ∈ Sup(S1 ) if and only
if every interventional distribution in eS1 ,S1′
=
P
|S2 |
Pv\(s1 ∪s2 ) (v) ≥ 0 is in A. For
S2 ⊆S1′ \S1 (−1)
a set of sets W , let M ax(W ) = {S|S ∈ W, there is no
S ′ ∈ W such that S ⊂ S ′ } denote the set of maximal sets
in W . Then, the set of inequalities
∀S1 ⊆ Ti , ∀S1′ ∈ M ax(Sup(S1 )),
X
(−1)|S2 | Pv\(s1 ∪s2 ) (v) ≥ 0, ∀v ∈ Dm(V ) (50)
S2 ⊆S1′ \S1

imply the inequalities
∀S1 ⊆ Ti , ∀S1′ ∈ Sup(S1 )
X
(−1)|S2 | Pv\(s1 ∪s2 ) (v) ≥ 0, ∀v ∈ Dm(V ).
S2 ⊆S1′ \S1

(51)

See the Appendix for the proof.
In Step 2, we deal with the inequalities that contain unwanted quantities as follows. We rewrite the inequality in

To illustrate the procedure, suppose we want to get the inequality constraints on the two interventional distributions
Pw1 w2 xy (z) and Pw1 w2 x (yz) and we are given a tried interventional distribution Pw1 w2 y (xz).
In Step 1, consider the loop in which Ti = {X, Y, Z}
and S1 = {∅}. The procedure first adds e∅,{X} and
e∅,{Z} . When it adds e∅,{X,Z} , it will remove e∅,{X} and
e∅,{Z} from IETi and keep e∅,{X,Z} which turns out to be
M ax(Sup(∅)). This repeats for every S1 ⊆ Ti .
In Step 2, consider the loop where Ti = {X, Y, Z}
and S1 = {Y }. The procedure studies eS1 ,S1′ for each
S1′ ∈ {{Y }, {X, Y }, {Y, Z}, {X, Y, Z}}. For example,
for S1′ = {X, Y, Z}, we have the inequality (39). From
(16), (17), (19) and (21), we obtain

max P (y|w1 xw2 )P (x|w1 ) + Pw2 x (yz)
w1 ,z

− P (z|w1 xw2 y)P (y|w1 xw2 )P (x|w1 ) ≤ Pw2 x (y).

(56)

Summing both sides over Y gives

X
max P (y|w1 xw2 )P (x|w1 ) + Pw2 x (yz)
y

w1 ,z


− P (z|w1 xw2 y)P (y|w1 xw2 )P (x|w1 ) ≤ 1.

(57)

4.1

5

Bounds on Causal Effects

Suppose that our goal is to bound a particular interventional
distribution. For this case, A in the procedure FindIneqs
consists of the particular interventional distribution that we
want to bound, the nonexperimental distribution P (v), and
all interventional distributions that are computable from
P (v).
For example, consider the graph in Figure 2. Suppose that we want to bound the interventional distribution Pw1 w2 xy (z) and that the interventional distribution
Pw1 w2 y (xz) is available from experiments. FindIneqs will
list the following bounds for Pw1 w2 xy (z) in Step 1.
1 − P (x|w1 ) − Pw1 w2 xy (z) + Pw1 w2 y (xz) ≥ 0
Pw1 w2 xy (z) − Pw1 w2 y (xz) ≥ 0

(58)
(59)

which provides a lower and upper bound for Pw1 w2 xy (z)
respectively.
4.2

Inequality Constraints on Nonexperimental
Distribution

Conclusion

We present a class of inequality constraints imposed by a
given causal BN with hidden variables on the set of interventional distributions that can be induced from the network. We show a method to restrict these inequality constraints on to that only involving interventional distributions of interests. These inequality constraints can be used
as necessary test for a causal model to be compatible with
given observational and experimental data. Another application permits us to bound the effects of untried interventions from experiments involving auxiliary interventions
that are easier or cheaper to implement.
We derive a type of inequality constraints upon the nonexperimental distribution in a complexity of 32m where m is
the number of variables in the largest c-component. These
constraints are imposed by the network structure, regardless of the number of states of the (observed or hidden)
variables involved. These constraints can be used to test a
model or distinguish between models. How to test these
inequality constraints in practice and use them for model
selection would be interesting future research.

Now assume that we want to find inequality constraints on
nonexperimental distribution. For this case, A in the procedure FindIneqs consists of the nonexperimental distribution P (v) and all interventional distributions that are computable from P (v).

Acknowledgments

The inequality constraints produced by FindIneqs in this
case include the instrumental inequality type of constraints.
Consider the graph in Figure 1. For the c-component
{X, Y }, FindIneqs will produce the inequality (35). From
(13) and (15), we have

Appendix : Proof of Lemma 6

max P (xy|z) ≤ Px (y)
z

and summing both sides over Y gives
X
max P (xy|z) ≤ 1.
y

z

(60)

This research was partly supported by NSF grant IIS0347846.

We will show that if the inequalities in (50) hold, then for
any n ≤ |V | we have
∀S1 ⊆ Ti , ∀S1′ ∈ M axn (Sup(S1 )),
X
(−1)|S2 | Pv\(s1 ∪s2 ) (v) ≥ 0, ∀v ∈ Dm(V ) (64)
S2 ⊆S1′ \S1

(61)

Since this must hold for all X, we obtain the instrumental
inequality (1).
To illustrate more general instrumental inequality type of
constraints, consider the graph in Figure 2. For S1 =
{Y, Z} and S1′ = {X, Y, Z}, FindIneqs produces the inequality (43). From (16) and (19), we have

where M axn (S) = M ax(S \ {R|R ∈ S, |R| > n}). (51)
will follow from (64) if we let n be the size of the set S1′ in
(51). Assuming (50), we prove (64) by induction on n.
Base: n = |V |. (64) is equivalent to (50).
Hypothesis: Assume that
∀S1 ⊆ Ti , ∀S1′ ∈ M axn (Sup(S1 )),
X
(−1)|S2 | Pv\(s1 ∪s2 ) (v) ≥ 0, ∀v ∈ Dm(V ).
S2 ⊆S1′ \S1

(65)

max P (z|w1 xw2 y)P (y|w1 xw2 )P (x|w1 ) ≤ Pw2 x (yz).
w1

(62)
Summing both sides over Y and Z gives
X
max P (z|w1 xw2 y)P (y|w1 xw2 )P (x|w1 ) ≤ 1. (63)
yz

w1

Induction step: We show that
∀S1 ⊆ Ti , ∀S1′ ∈ M axn−1 (Sup(S1 )),
X
(−1)|S2 | Pv\(s1 ∪s2 ) (v) ≥ 0, ∀v ∈ Dm(V ).
S2 ⊆S1′ \S1

(66)

If |S1′ | < n − 1, then S1′ is in M axn (Sup(S1 )). Thus,
(66) follows from (65). If |S1′ | = n − 1, then one of the
followings should hold.
Case 1: S1′ is in M axn (Sup(S1 )).
Case 2: There exists a variable α such that S1′ ∪ {α} is in
M axn (Sup(S1 )).
In Case 1, (66) follows from (65). In Case 2, we have
X
(−1)|S2 | Pv\(s1 ∪s2 ) (v) ≥ 0, ∀v ∈ Dm(V )
S2 ⊆(S1′ ∪{α})\S1

(67)
and
X

(−1)|S2 | Pv\(s1 ∪{α}∪s2 ) (v) ≥ 0, ∀v ∈ Dm(V ).

S2 ⊆S1′ \S1

(68)
(68) follows from (65) since S1′ ∪ {α} is in
M axn (Sup(S1 ∪ {α})).
Summing (67) and (68)
gives (66). 

