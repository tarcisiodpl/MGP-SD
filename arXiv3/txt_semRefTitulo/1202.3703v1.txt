
We consider filtering for a continuous-time,
or asynchronous, stochastic system where the
full distribution over states is too large to
be stored or calculated. We assume that
the rate matrix of the system can be compactly represented and that the belief distribution is to be approximated as a product of
marginals. The essential computation is the
matrix exponential. We look at two different methods for its computation: ODE integration and uniformization of the Taylor expansion. For both we consider approximations in which only a factored belief state
is maintained. For factored uniformization
we demonstrate that the KL-divergence of
the filtering is bounded. Our experimental
results confirm our factored uniformization
performs better than previously suggested
uniformization methods and the mean field
algorithm.

1

Continuous-Time Markov Systems

We are interested in monitoring (alternatively tracking or filtering) a continuous-time finite-state homogeneous Markovian stochastic system. This implies that
evidence and events can arrive at any real-valued time
(asynchronously) and be either instantaneous or have
real-valued time durations. The system is stationary
and has discrete states which means that a sample
(trajectory) for the system consists of a series of times
at which the system jumped from one state to another.
Such models are common in the queueing theory and
verification literatures. The algorithms developed
there almost exclusively focus on steady-state properties of the system. A singular exception is the work
of Sutton and Jordan (2008) which applied Gibbs sampling to queueing models. By contrast, the continuous

William Lam
University of California, Irvine
willmlam@ics.uci.edu

time Bayesian network (Nodelman et al., 2002) literature has focused on finite-time properties, so we compare against those algorithms.
In verification, continuous-time models can be used
to compute the probability a system will function by
a given time, such as in continuous stochastic logic
(Baier et al., 2003). Other problems of interest include
state estimation in asynchronous systems, such as distributed computer systems (Xu and Shelton, 2010),
robotics (Ng et al., 2005), social networks (Fan and
Shelton, 2009), or phylogenetic trees (Cohn et al.,
2009).
1.1

Parameterization

Such a Markovian system is described by an initial
distribution over the state space and an intensity (or
rate) matrix, often denoted Q. The diagonal element
qii = −qi where qi ≥ 0 is the rate of leaving state i.
This means that the density of the duration of the process remaining in state i for exactly a duration ∆t is an
exponential distribution: qi eqi ∆t . The non-diagonal
elements, qij ≥ 0 are the P
rates of transitioning from
state i to state j. qi =
j qij in a closed system.
(The rate of leaving a state is equal to the sum of the
rates of moving to any other state.) The probability
of transitioning to state j immediately upon leaving
state i is qij /qi . Note that the diagonal elements of
Q are non-positive and the non-diagonal elements are
non-negative. The sum of each row is 0.
Monitoring such a system consists of keeping track of
the probability distribution over the state at the current time t, given all evidence prior to t. We would like
a recursive solution in which evidence before t can be
discarded once the distribution (or its estimate) at t
has been computed. As an initial problem, we will be
interested in tracking in the absence of any evidence.
We will cover how to incorporate evidence in Section 5.
To simplify notation, we will assume that we have the
distribution at time 0 and wish to propagate this dis-

tribution to time t. If p is the distribution at time 0,
represented as a row vector, then p0 , the distribution
at t also represented as a row vector, is

nential. We concentrate here on two of the principal
methods for calculating peQt .
2.1

p0 = peQt

ODE

where eQt is the matrix exponential of Qt. For a few
hundred states, this computation is tractable. Yet,
the number of states of a system grows exponentially
with the number of properties required to describe the
system’s state. Very quickly it becomes impossible to
represent the matrix Q or even the result p0 exactly. It
should be noted that almost any useful structure that
might be imposed on Q is destroyed by the matrix
exponential. Kronecker-sum constructions of Q are
the sole exception, but they represent processes with
completely independent sub-processes and are therefore not interesting on their own.

One method is to rewrite f (t) = peQt as the ordinary
differential equation

Previous results have looked at sparse representations
of p and p0 (Sidje et al., 2007). These work well for systems with very tightly coupled components in which
there are only a few joint assignments to the system
properties that have large probability at any one time.
By contrast, in this work we are interested in systems with many (relatively) weakly interacting components. Therefore, instead of approximating p and
p0 with sparse vectors, we will approximate them with
factored representations.

2.2

In order to have a factored representation, any state
must be able to be described as a set of properties or
variables. We let Xi represent variable i (out of n); an
assignment to Xi is denoted xi ; and therefore a joint
assignment (a complete state) x = (x1 , . . . , xn ). We
will, therefore, be considering the case of calculating
Q
p0 = peQt when p is approximated as p̂(x) =Q i p̂i (xi )
0
0
and p is similarly approximated as p̂ (x) = i p̂0i (xi ).
Example. Our running example will use a 4-state system described by the rate matrix
3
−4 1 3 0
6 2 −7 0 5 7
Q=4
.
4 0 −5 1 5
0 6 2 −8
2


If the distribution at t = 0 is p = 0.4 0.1
the marginal distribution at t = 0.5 is
0.4313
60.2958
= p4
0.4106
0.2886
2

p0 = pe0.5Q

= [0.3708

2

0.1443
0.2648
0.1406
0.2622
0.1910

0.3098
0.2221
0.3305
0.2293

0.2


0.3 ,

3
0.1147
0.21737
0.11835
0.2199

0.2810

0.1572] .

Matrix Exponential Calculations

Moler and Loan (2003) give an excellent description of
the numeric difficulties in calculating the matrix expo-

f˙(t) = f (t)Q

f (0) = p

and solve using ODE integration methods. The most
common ODE solver is the Runge-Kutta-Fehlberg
(RKF) method which which adapts the step size to
the current error, thereby allowing for quick progress
at times of slow system change. The fundamental calculation is the multiplication of a current estimate of
the distribution by Q to calculate the time derivative.
Uniformization

Uniformization is a transformation of a continuoustime Markovian system into a discrete-time one. However, it does not correspond to constructing either the
embedded Markov chain of the continuous-time process, nor to time-slicing the system at regular intervals. It is equivalent to sampling the intervals between
potential state changes from an exponential with rate
α and then sampling a suitable Markov chain just at
these time points (with stochastic matrix M ), so that
the resulting distribution over trajectories matches the
original continuous-time Markov system.
Mathematically we can construct α and M as follows.
We express the rate matrix Q as
Q = α(M − I)
so that—provided α ≥ maxi qi —M is a stochastic matrix: all elements are on [0, 1] and the rows sum to 1.
Ideally α should be as small as possible as it represents the rate of the process sampling the intervals.
Note that while the continuous-time process never has
a “self-transition” explicitly, M does have non-zero diagonal elements (corresponding to those states whose
rates of leaving are not maximal).
This can be applied to the matrix exponential calculation as
eQt = eα(M −I)t = eαtM e−αt .
In the context of the matrix exponential, this transformation is usually performed to remove the negative
elements in the resulting Taylor expansion and thereby
stabilize the calculation. We wish to calculate
peQt = pe−αt eαtM = e−αt

∞
X
(αt)k
k=0

k!

pM k .

(1)

Given that M is a stochastic matrix, p0 is a infinite mixture of distributions p, pM, pM 2 , . . . with
weights e−αt , e−αt αt, e−αt αt
2 , . . . . Each distribution
corresponds to k steps of the Markov chain of M and
the weights are the probabilities that exactly k steps
happen in a time period of t. These distributions are
calculated recursively, so the important subcalculation
is the multiplication of a state distribution by M .
Example. Uniformization on the Q matrix results in
α = 8 and
0.5
60.25
M =4
0.5
0
2

2.3

0.125
0.125
0
0.75

0.375
0
0.375
0.25

3
0
0.6257
.
0.1255
0

Approximate Versions

Both of these methods involve the calculation of state
distributions and their multiplication by either Q or
M . We will use v to denote any state distribution vector generated during the course of such a calculation.
For the ODE method, v is a particular point and vQ is
its time derivative. For uniformization, v is an element
of the sum and vM is the next element.
While we might assume that the problem specification
is compact and thereby assume that p and Q have
compact representations, for a system with many variables, it is usually not possible to express intermediate
v values exactly, as structure that may exist in p and Q
does not exist in v. Therefore, the most direct method
for constructing an approximate filtering algorithm is
to keep v restricted to a smaller representation.
In the case for uniformization, Sidje et al. (2007) proposed keeping a sparse representation for v (by dropping elements less than a given threshold). If Q has
a sparse representation (only a small fraction of any
row are non-zero) and any given row can be generated as needed, this results in an algorithm with nonexponential running time (in n, the number of variables). We call this method sparse uniformization.
Sparse uniformization works well for problems in which
the distribution to be tracked is highly peaked. However, in problems in which the variables may be more
loosely coupled (and therefore an assignment to one
does not necessarily dictate a joint assignment to all),
this approximation will either be poor, or it will require a large number of states to be tracked, thereby
defeating the quick runtime.
For such systems, a factored representation of v (as
suggested above) is more suitable. This naturally suggests two approximate algorithms. First, we might
force the RKF integrator to project v to the space
of factored distributions. Second, we might force the

uniformization method to project v to the space of factored distributions. We call these factored RKF and
factored uniformization.
2.4

Notation

We will concentrate on approximate calculations of
vM (vQ is similar). We consider one subcalculation in
the form v 0 = vM . Let v̂ be theQcurrent approximation
of v in factored form: v̂(x) = i v̂i (xi ). Similarly, let
v̂ 0 = v̂M , the result of multiplying v̂ by M , which is
not necessarily completely factored. Finally let ṽ be
the projection of v̂ 0 to the set of factored distributions.
Let M⊥ be the operator that both multiplies by M and
projects to the space of factored distributions. Thus
ṽ = v̂M⊥ and is a factored distribution (v̂ 0 is not).
We abuse notation and let any vector also stand for
the distribution it embodies. Further, for any vector v, we let vi be the marginal distribution of v over
the variable xi (even if v is not factored), v−i be the
marginal distribution over all variables except xi , vui
be the marginal distribution over all of xi ’s parents,
and vi|−i be the distribution over xi conditioned on
all the other variables.

3

Factored Rate Matrix

For either factored approximate method, we need to
be able to calculate the projection of vM (or vQ) onto
the space of factored distributions without explicitly
generating vM as an exponentially large vector.
While our methods are applicable to Petri nets (Petri,
1962), edge-valued decision diagrams (Wan et al.,
2011), and other compact representation of the rate
matrix, we focus on using a continuous time Bayesian
network (CTBN) to represent Q. We note that a correspondence between CTBNs and Petri nets has already
been established (Raiteri and Portinale, 2009).
3.1

Continuous Time Bayesian Network

A continuous time Bayesian network (CTBN) (Nodelman et al., 2002) has an initial distribution described
by a Bayesian network that is not of direct importance
to this work. It has a factored representation of the
matrix Q. Each variable’s dynamics depend only on a
subset of the other variables (which we denote as parents). Let U i be the parents of Xi . Then for every
assignment ui to U i there exists an intensity matrix
QXi |ui of dimension equal to the number of states of
Xi . It describes the rates of change for Xi when its
parents equal ui . If we let δ(x, x0 ) be the set of variable indexes for which the assignments in x and x0

differ, the complete Q matrix is
P
0
0

 k QXk |uk (xk , xk ) if x = x
0
Q(x, x ) = QXj |uj (xj , x0j )
if δ(x, x0 ) = {j}


0
otherwise.
So Q is sparse (mostly zeros) with non-zero elements
on the diagonal and where only one variable changes.
It has the form of a sum of factors (compared with a
Bayesian network’s product of factors).
Example. The Q matrix of our example can be represented by a CTBN of two variables: A (with values
a0 and a1 ) and B (with values b0 and b1 ). A has no
parents and is the parent of B. If we let
»
QA =

»
–
»
–
–
−5 5
−3 3
−1 1
, QB|a1 =
, QB|a0 =
6 −6
4 −4
2 −2

then the full system has the previous Q matrix if the
global states are ordered a0 b0 , a1 b0 , a0 b1 , a1 b1 .
To construct the uniformized matrix M , we would like
to select α to be the maximally negative diagonal element. In practice this could be a difficult optimizaPn
tion problem. So, instead we select α =
i=1 αi
where αi = − minui minxi QXi |ui (xi , xi ), the maximally negative diagonal element for Xi for any ui . We
let MXi |ui = QXi |ui /αi + I be the stochastic matrix
we obtain for Xi with parent assignment ui using uniformization constant αi . The stochastic matrix M can
then be described as
P
0
0

 k M̃Xk |uk (xk , xk ) if x = x
0
0
M (x, x ) = M̃Xj |uj (xj , xj )
if δ(x, x0 ) = {j}


0
otherwise.
i
where M̃Xi |ui = ααi MXi ui + α−α
α I. The stochastic
matrix M cannot be described as a dynamic Bayesian
network (DBN): it has an additive structure, not a
multiplicative one, and does not allow the transition of
multiple variables. However, M can be described as a
mixture
P of particularly simple DBN transition models:
M = i ααi Mi where


0
0

MXi |ui (xi , xi ) if x = x
Mi (x, x0 ) = MXi |ui (xi , x0i ) if δ(x, x0 ) = {i}


0
otherwise.
In particular, it describes a mixture in which with
probability ααi variable i transitions according to
MXi |U i and all other variables remain the same. The
ith mixture DBN has a structure in which all variables
x0j , where j 6= i, have only xj (the same node in the
previous slice) as a parent. Variable x0i has the same
parents as in the CTBN, with the addition of xi . No
intra-slice arcs exist.

Example. αA = 2, αB = 6 and so α = 8 (as before).
We can decompose the same M as before into
»
MA =

»
»
–
–
–
0.5 0.5
0.5 0.5
0.17 0.83
, MB|a0 =
, MB|a1 =
1 0
0.67 0.33
1
0

which can be viewed as a Markov chain in which with
probability 82 A transitions according to MA , otherwise
B transitions according to MB|a0 or MB|a1 .
3.2

Calculating Factored vM

Given the mixture-of-DBN interpretation above of M ,
it is not surprising that ṽ = v̂M⊥ can be calculated in
one step without constructing v̂M . As Q has the same
structure, the same method works for it too.
As projection onto a particular variable, xj , is linear,
if M⊥j is the composition
P of M and the projection
onto xj , then M⊥j = i ααi Mi⊥j where Mi⊥j is the
composition of Mi and projection operation onto xj .
Furthermore, in Mi only variable i can change:
X αi
αj
αj
ṽj = v̂M⊥j =
v̂Mi⊥j = (1 −
)v̂j +
v̂Mj⊥j .
α
α
α
i
Mj⊥j is the marginal over xj after a transition according to Mj . No other variable changes, so this is the
expectation of MXj |U j over v̂’s distribution over U j :
h
i
αj P
αj
)I +
v̂
(u
)M
ṽj = v̂j (1 −
u
j
X
|u
j
j
j
α uji
hP α
(2)
= v̂j
uj v̂uj (uj )M̃Xj |uj
To calculate v̂Q and project onto xj , Equation 2 holds
if we change M̃Xj |uj to QXj |uj .
Example.

 Starting distribution
 p marginals v̂A =
0.6 0.4 and v̂B = 0.5 0.5 . Multiplying this factored approximation by M and then projecting can be
done by multiplying vA by M̃A and vB by 0.6M̃B|a0 +
0.4M̃B|a1 :
»
»
»
–
–
–
.875 .125
.625 .375
.375 .625
M̃A=
, M̃B|a0 =
, M̃B|a1 =
.
.25 .75
.5 .5
.75 .25

4

Bounds for Factored Uniformization

While we have not found a suitable way of bounding the approximation error for factored RKF, we can
derive bounds similar to those of the BK algorithm
(Boyen and Koller, 1998) for discrete-time stochastic
processes to bound the error in propagation and projection through a single M matrix. However, because
the process is a mixture of processes in which only
a single component changes, the BK result for compound processes does not carry over.

We then use this bound to bound the error of the entire
Taylor expansion and thereby the factored uniformization method.
4.1

Divergence Bound for Single Step

unfortunate, but the next section demonstrates that it
is not a problem for the contraction rate of the entire
Taylor expansion.
We upper-bound the increase from projection:
DKL (vM kv̂M⊥ ) − DKL (vM kv̂M ) ≤  .

We first concentrate on bounding the error of a single multiplication by M . We wish to show that the
KL-divergence between v 0 = vM and v̂ 0 = v̂M is no
greater than that between v and v̂.
We begin with a simple property of the KL-divergence.
The proof is omitted, but is a consequence of the fact
that entropy does not increase upon conditioning.
Q
Lemma 1. If q(x) = i q(xi ) is a factored distribution and p(x) is a (non-factored) distribution over the
same sample space, and x−i is the set of all variables
except xi ,
P
i DKL (p(xi | x−i )kq(xi )) ≥ DKL (p(x)kq(x)) .
We require a mixing rate definition from Boyen and
Koller (1998):
Definition 2. The mixing rate
P of a stochastic matrix
M is defined as γ , mini1 ,i2 j min(Mi1 ,j , Mi2 ,j ).
We can then state that M is a contraction mapping
with respect to the KL-divergence:
Theorem 3. Let γi be the minimum (over ui ) mixing
rate of the stochastic matrix MXi |ui and γ = mini αiαγi .
Then,
DKL (v 0 kv̂ 0 ) ≤ (1 − γ)DKL (vkv̂) .
Proof. If we let v 0(i) = vMi and v̂ 0(i) = v̂Mi , then
P
P
DKL (vM kv̂M ) = DKL ( i ααi v 0(i) k i ααi v̂ 0(i) )
P
≤ i ααi DKL (v 0(i) kv̂ 0(i) )
P
0(i)
0(i)
0(i)
0(i) 
= i ααi DKL (v−i kv̂−i ) + DKL (vi|−i kv̂i|−i )

P
≤ i ααi DKL (v−i kv̂−i ) + (1−γi )DKL (vi|−i kv̂i|−i )

P
= i ααi DKL (vkv̂) − γi DKL (vi|−i kv̂i|−i )
P
≤ DKL (vkv̂) − γ i DKL (vi|−i kv̂i )
≤ DKL (vkv̂) − γDKL (vkv̂)
The first inequality is from the convexity of the
KL-divergence (Cover and Thomas, 1991, Theorem
2.7.2). The next inequality holds because Mi does
not change any variables except xi and the conditional
KL-divergence of xi contracts by γi (Boyen and Koller,
1998, Theorem 3). The final inequality is due to the
lemma above.
Note that unlike in BK, the global contraction rate
(γ) does not depend on the in- or out-degree of model,
but it is inversely proportional to n. This appears

As a crude upper bound,  ≤ −(n−1) ln η where η is
the smallest marginal probability. As shown by Boyen
and Koller (1999), better bounds can be placed by
more careful analysis or considering the average case.
Taken together this means that after a single multiplication and projection, the KL-divergence error of our
estimate can be bounded as
DKL (vM kv̂M⊥ ) ≤ (1 − γ)DKL (vkv̂) +  .

(3)

Example. We have γA = 0.5, γB|a0 = 0.5, γB|a1 =
0.17. Therefore γ = min(0.5× 28 , 0.17× 68 ) = 0.125.
Therefore the KL-divergence between the true answer and the factored approximation contracts by 1 −
0.125 = 0.825 for each multiplication by M .
4.2

Bound on Approximate Taylor Expansion

Our goal is not to bound the error on a single step,
but rather the error of our entire approximation to
the matrix exponential. Equation 3 implies that
k
DKL (vM k kv̂M⊥
) ≤ (1−γ)k DKL (vkv̂) + 

k−1
X

(1−γ)i

i=0
k

= (1−γ) DKL (vkv̂) +

k
 1−(1−γ)
γ

(4)

If we combine the bound from Equation 4 with the
Taylor expansion of Equation 1, we can obtain a bound
on the KL-divergence between the true matrix exponential, and an approximation of the Taylor expansion
in which each vector of probabilities is approximated
by a factored form and only the first l terms are evaluated (and then renormalized):
Theorem 4. Let α, , and γ be as defined above.
Let p be an arbitrary distribution over the state space
of the process. Let p̂ be an arbitrary factored distribution over the same. Further, let p0 = peQt be
the distribution at time t in the future, and let p̂0 =
k
Pl
1
−αt (αt)
k
0
k=0 e
1−Rl
k! p̂M⊥ be the approximation of p
constructed by uniformization of a Taylor expansion
truncated to l terms in which each matrix multiplication is projected back to the space of factored distribuk
P∞
tions. Rl = k=l+1 e−αt (αt)
k! . Then


DKL (p0 kp̂0 ) ≤ e−γαt DKL (pkp̂)+ (1−e−γαt )+Rl (δ+ )
γ
γ
where δ = maxx − ln p̂0x , the maximum negative log
probability over any joint assignment in the final approximate calculation.

Proof. For compactness of presentation, let βk =
k
e−αt (αt)
k! , γ̄ = (1 − γ), and R̄l = (1 − Rl ). Then
Pl
P∞
k
DKL (p0 kp̂0 ) = DKL ( k=0 βk pM k k R̄1l k=0 βk p̂M⊥
)
P
P
l
l
k
≤ R̄l DKL ( R̄1l k=0 βk pM k k R̄1l k=0 βk p̂M⊥
)
P
P
l
∞
k
−αt
k
1
1
+ Rl DKL ( Rl k=l+1 e βk pM k R̄l k=0 βk p̂M⊥
)
Pl
k
k
≤ k=0 βk DKL (pM kp̂M⊥ ) + Rl δ
Pl
≤ k=0 βk [γ̄ k DKL (pkp̂) + (1 − γ̄ k )/γ] + Rl δ
≤ e−γαt DKL (pkp̂) + γ (1 − e−γαt ) + Rl (δ +

γ̄ l
γ )

.

The first inequality is due to the convexity of the KLdivergence in only the first argument. The second is
due to the convexity of the KL-divergence in both arguments, and that the KL-divergence is bounded by
the negative log of smallest probability of the second
argument. The third is due to Theorem 3. The final is due to bounding a finite Taylor expansion of an
exponential by the exponential.
Note that Rl goes to zero as l grows. In the remaining
terms, γ almost always appears multiplied by α. In the
previous section, we commented on how γ is inversely
proportional to n. However, α is proportional to n and
they exactly cancel out. Thus if l is large enough, we
can let γ 0 = mini αi γi and conclude
0

DKL (p0 kp̂0 ) ≤ e−γ t DKL (pkp̂) +

0
α
(1 − e−γ t ) . (5)
0
γ

Thus the contraction rate for the full approximation is
not a function of n. It is unclear how α scales with n.
We are then left with two error terms. The first decays
with t and the second grows with t (but is bounded).
The resulting distribution p̂0 is a mixture of factored
distributions. So, if filtering is to continue past time
t, p̂0 must be projected back to the space of factored
distributions, introducing another similar additive error. This may happen either because evidence arrives
at t and requires conditioning, or because we may
wish to subdivide a propagation from 0 to t into m
propagations of length t/m. Equation 5 implies this
is not helpful for accuracy (if one considers applying
the bound recursively m times over intervals of length
t/m). However, our experimental results show that
some interval subdivision is helpful.
0

Example. γ = γα = 1. For large l, the KLDivergence between the true distribution and the factored approximation before (DKL ) and after propaga0
tion to t = 0.5 (DKL
) is related by
0
DKL
≤ 0.61DKL + (8/1)(1 − 0.61)

where we note that e−1×0.5 ≈ 0.61. If the smallest possible marginal probability is 0.01, then a crude upperbound on  is − ln 0.01 = 4.6.

5

Adding Evidence

So far, we have discussed only how to filter without evidence. In a continuous-time process, evidence can take
on two forms. First, it can be point evidence. That
is, at time t we observe the values of certain variables,
but for no duration. We propagate to the time of the
evidence, condition the distribution on the evidence,
and then continue. In this case, conditioning on the
evidence in expectation reduces the error (Boyen and
Koller, 1998, Fact 1).
The second form of evidence is interval evidence: a
variable remains in a particular state from t1 to t2 . In
this case, at t1 we condition on the evidence (same as
for point evidence, above). From t1 until t2 , we monitor using a modified Q matrix in which all transitions
where the evidence variable changes are set to 0 (but
the diagonal elements remain unchanged). The resulting p̂ sums to the probability of the interval evidence,
but it can be normalized to yield the conditional distribution of the state. The normalization makes the
analysis difficult. However, we conjecture that interval
evidence will also not increase the error in expectation.
Finally we note that the sparse uniformization method
can have serious difficulties with evidence if none of the
maintained states are consistent with the evidence.
Example. Given the previous distribution p for t = 0,
and B = b0 on t = [0.5, 1), we propagate p to t = 0.5
using Equation 1 (bounded), with Equation 2 to calculate the projected multiplications.

 We get the factored

distribution p̂A = 0.65 0.35 , p̂B =  0.56  0.44 .
We condition on B = b0 by setting p̂B = 1 0 . Then
we similarly propagate to t = 1, but using
»
QA =

–
»
–
»
–
−5 0
−3 0
−1 1
, QB|a1 =
, QB|a0 =
0 0
0 0
2 −2

for which αA = 2, αB = 5, and α = 7:
»
MA =

6

»
»
–
–
–
0.5 0.5
0.4 0
0 0
, MB|a0 =
, MB|a1 =
.
1 0
0 1
0 1

Experimental Results

We employed two synthetic networks and a network
built from a real data set in our evaluations. Since exact filtering is intractable for large models, we limited
the number of variables to allow for calculations of the
true approximation errors.
The synthetic networks we use are the ring and toroid
dynamic Ising networks of 20 binary variables from ElHay et al. (2010). The ring network is bidirectional,
while the toroid is directed. Nodes try to track their
parents with a coupling parameter, β, indicating the

0.4

0.16

M arried

Children

0.35

0.14
Employment
Smoking
Married
Children

HS

HM

HC

DKL(exact | approx)

0.12
0.1

DKL(exact | approx)

Smoking

0.08
0.06

Employment

2

4

6

8

10

t (years)

(a) BHPS network

0.25

X2

X3

X4

X

16

X5

X6

X7

X8

X9

X10

X11

X12

X13

X14

X15

X16

X17

X18

X19

0.2
0.15

0.05

0.02
0
0

X1

X9

0.1

0.04

HE

X0

0.3

(b) BHPS results

0
0

0.2

0.4

0.6

0.8

1

t

(c) Toroid results

(d) Toroid network

Figure 1: Accuracy versus t (interval width) for the BHPS and toroid networks, for factored uniformization.
strength of the influence, and a rate parameter τ that
is inversely proportional to the expected time between
switching. We set τ = 4 and β = 1. We set both
networks to have a deterministic starting distribution.
For the toroid the first 5 variables are in state 0 and the
remaining variables are in state 1. The ring network’s
initial distribution is the reverse.
The real network we used was constructed from the
British Household Panel Survey (BHPS) data set (Economic & Social Research Council, 2003). The data set
records major life changes from a set of roughly 8,000
British citizens in areas including household organization, employment, income, wealth and health. We use
the same network model as in Fan et al. (2010) and
Nodelman et al. (2005) which chooses 4 variables: employment (student, employed, unemployed), children
(0, 1, ≥2), married (not married, married) and smoking (non-smoker, smoker) and adds a hidden binary
variable for each (Figure 1a). The structure and parameters of both the initial distribution and the dynamics were learned by the structural EM algorithm
(Nodelman et al., 2005) and we used the learned network model for our experiments.
6.1

KL-Divergence Bound

Our first experiment tested the theoretic error bound.
Figure 1 shows the KL-divergence between the true
marginals and the marginals computed by factored
uniformization for the BHPS network and the toroid
network. The bound on the KL-divergence of the full
distribution is also a bound on any marginal, and experimentally the errors on the marginals grow initially
and then asymptote, as per Theorem 4.
6.2

Approximation Comparison

We then compared our factored uniformization
method (UF ) to other approximations. In particular,
we compared to factored RKF (RKFF ), as explained in
Section 2.1, sparse uniformization (US ), and the mean
field (MF) approach of Cohn et al. (2009) for CTBNs.
For the purpose of comparison, the MF method was

extended to accept evidence on subsets of variables.
We varied an error tolerance parameter for each
method to map the trade-off between error and runtime. For UF , we varied the uniformization-specific
parameter θ that determines the number of intervals
of propagation (see Sidje et al., 2007). We fixed the
number of terms of the Taylor series expansion (l)
to a value that performed reasonably. Similarly, for
sparse uniformization we varied θ and fixed l to a wellperforming value. MF and RKFF both use RKF for
integration, so we varied the error tolerance of RKF.
The evidence for the ring and toroid networks was set
to be relatively unexpected: for t ∈ [0.5, 1.0) x0 = 1
and x1 = 0. For the ring, we queried the distribution
of x10 (far from the evidence) and x19 (adjacent to the
evidence) at time t = 1. For the toroid, we queried
the distribution of x6 (adjacent to the evidence) and
x13 (a node more the in middle) at t = 1. For the
BHPS network, we chose evidence where employment
is observed to be in the student state continuously from
year 1 to year 5. We queried the marginal distribution
of the variables smoking and children at t = 5 years.
Figure 2 shows the KL-divergence between the true
and approximated marginals of the query variable for
each of the three networks for each query. These results are typical of other query marginals. In general,
factored uniformization (UF ) performs the best, especially for smaller running times (with US occasionally
being the method of choice for longer running times).
However, when the query node is close to the evidence,
RKFF performs better (x10 for the ring and x13 for the
toroid network are examples). The advantage of US for
longer running times is predictable as it approaches the
true value as more states are retained. Finally, note
that for the BHPS query over the children variable, US
has infinite KL-divergence (and hence does not appear
on the plot). This is because all of the retained states
have this variable equal to 0. While this is certainly
the most common value at t = 5, it does not have
probability 1 under the true distribution and thus the
KL-divergence between the true distribution and the
approximation of US is infinite.

−2

10

−1

−4

10

UF

−6

10

RKF

UF

UF

RKFF

RKFF

MF
US

DKL(exact | approx)

DKL(exact | approx)

DKL(exact | approx)

10

−2

10

MF
US

−1

10

F

MF
U

−2

S

10

−8

10 −2
10

−1

10

0

10
Runtime (sec)

1

−2

−1

10

2

10

10

10

(a) BHPS, smoking

0

10
Runtime (sec)

1

10

2

10

−2

0

10

2

10

10

4

10

Runtime (sec)

(b) Ring, x10

(c) Toroid, x6

−1

10

0

U

10

F

RKFF
−1

−3

10

UF

−4

10

RKFF

10

DKL(exact | approx)

DKL(exact | approx)

DKL(exact | approx)

−2

10

−2

10

UF
RKFF
MF
US

MF
−5

MF
U
S

−1

10

−2

10

−3

10 −2
10

−1

10

0

10
Runtime (sec)

1

10

(d) BHPS, children

2

10

10 −2
10

−1

10

0

10
Runtime (sec)

(e) Ring, x19

1

10

2

10

−2

0

10

2

10

10

4

10

Runtime (sec)

(f) Toroid, x13

Figure 2: Computation time versus accuracy comparisons.

7

Conclusion

We have demonstrated approximate continuous-time
filtering based on uniformization. It is simple to implement, and we have proven bounds on the KLdivergence of its error. The approximation can be
made more accurate by lumping together variables into
joint marginals. The bounds are similar in style to
those of BK and also depend on the mixing time of the
individual components (adjusted for the continuoustime nature of the system). Our experimental results
demonstrate that the theoretic bounded error holds
in practice. Furthermore, our method gives superior time-accuracy trade-offs for most of the examples
tested in this paper.

