
Many successful approaches to semantic
parsing build on top of the syntactic analysis of text, and make use of distributional representations or statistical models to match parses to ontology-specific
queries. This paper presents a novel deep
learning architecture which provides a semantic parsing system through the union
of two neural models of language semantics. It allows for the generation of
ontology-specific queries from natural language statements and questions without
the need for parsing, which makes it especially suitable to grammatically malformed or syntactically atypical text, such
as tweets, as well as permitting the development of semantic parsers for resourcepoor languages.

1

Introduction

The ubiquity of always-online computers in the
form of smartphones, tablets, and notebooks has
boosted the demand for effective question answering systems. This is exemplified by the growing popularity of products like Apple’s Siri or
Google’s Google Now services. In turn, this creates the need for increasingly sophisticated methods for semantic parsing. Recent work (Artzi and
Zettlemoyer, 2013; Kwiatkowski et al., 2013; Matuszek et al., 2012; Liang et al., 2011, inter alia)
has answered this call by progressively moving
away from strictly rule-based semantic parsing, towards the use of distributed representations in conjunction with traditional grammatically-motivated
re-write rules. This paper seeks to extend this line
of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed
neural semantic generative parsing model. It does
so by adapting deep learning methods from related

work in sentiment analysis (Socher et al., 2012;
Hermann and Blunsom, 2013), document classification (Yih et al., 2011; Lauly et al., 2014; Hermann and Blunsom, 2014a), frame-semantic parsing (Hermann et al., 2014), and machine translation (Mikolov et al., 2010; Kalchbrenner and
Blunsom, 2013a), inter alia, combining two empirically successful deep learning models to form
a new architecture for semantic parsing.
The structure of this short paper is as follows.
We first provide a brief overview of the background literature this model builds on in §2. In §3,
we begin by introducing two deep learning models
with different aims, namely the joint learning of
embeddings in parallel corpora, and the generation
of strings of a language conditioned on a latent
variable, respectively. We then discuss how both
models can be combined and jointly trained to
form a deep learning model supporting the generation of knowledgebase queries from natural language questions. Finally, in §4 we conclude by
discussing planned experiments and the data requirements to effectively train this model.

2

Background

Semantic parsing describes a task within the larger
field of natural language understanding. Within
computational linguistics, semantic parsing is typically understood to be the task of mapping natural language sentences to formal representations
of their underlying meaning. This semantic representation varies significantly depending on the
task context. For instance, semantic parsing has
been applied to interpreting movement instructions (Artzi and Zettlemoyer, 2013) or robot control (Matuszek et al., 2012), where the underlying
representation would consist of actions.
Within the context of question answering—the
focus of this paper—semantic parsing typically
aims to map natural language to database queries
that would answer a given question. Kwiatkowski

et al. (2013) approach this problem using a multistep model. First, they use a CCG-like parser
to convert natural language into an underspecified
logical form (ULF). Second, the ULF is converted
into a specified form (here a FreeBase query),
which can be used to lookup the answer to the
given natural language question.

3

Model Description

We describe a semantic-parsing model that learns
to derive quasi-logical database queries from natural language. The model follows the structure of
Kwiatkowski et al. (2013), but relies on a series of
neural networks and distributed representations in
lieu of the CCG and λ-Calculus based representations used in that paper.
The model described here borrows heavily from
two approaches in the deep learning literature.
First, a noise-contrastive neural network similar to
that of Hermann and Blunsom (2014a, 2014b) is
used to learn a joint latent representation for natural language and database queries (§3.1). Second, we employ a structured conditional neural
language model in §3.2 to generate queries given
such latent representations. Below we provide the
necessary background on these two components,
before introducing the combined model and describing its learning setup.
3.1

Bilingual Compositional Sentence Models

The bilingual compositional sentence model
(BiCVM) of Hermann and Blunsom (2014a) provides a state-of-the-art method for learning semantically informative distributed representations
for sentences of language pairs from parallel corpora. Through the joint production of a shared latent representation for semantically aligned sentence pairs, it optimises sentence embeddings
so that the respective representations of dissimilar cross-lingual sentence pairs will be weakly
aligned, while those of similar sentence pairs will
be strongly aligned. Both the ability to jointly
learn sentence embeddings, and to produce latent
shared representations, will be relevant to our semantic parsing pipeline.
The BiCVM model shown in Fig. 1 assumes
vector composition functions g and h, which map
an ordered set of vectors (here, word embeddings from DA , DB ) onto a single vector in Rn .
As stated above, for semantically equivalent sentences a, b across languages LA , LB , the model

aims to minimise the distance between these composed representations:
Ebi (a, b) = kg(a) − h(b)k2
In order to avoid strong alignment between dissimilar cross-lingual sentence pairs, this error
is combined with a noise-contrastive hinge loss,
where n ∈ LB is a randomly sampled sentence,
dissimilar to the parallel pair {a, b}, and m denotes some margin:
Ehl (a, b, n) = [m + Ebi (a, b) − Ebi (a, n)]+ ,
where [x]+ = max(0, x). The resulting objective
function is as follows
!
k
X
X
λ
J(θ) =
Ehl (a, b, ni ) + kθk2 ,
2
(a,b)∈C

i=1

with λ2 kθk2 as the L2 regularization term and
θ={g, h, DA , DB } as the set of model variables.

L2 word embeddings

h
L2 sentence embedding
contrastive estimation

L1 sentence embedding
g

L1 word embeddings

Figure 1:
BiCVM.

...

Diagrammatic representation of a

While Hermann and Blunsom (2014a) applied
this model only to parallel corpora of sentences,
it is important to note that the model is agnostic
concerning the inputs of functions g and h. In this
paper we will discuss how this model can be applied to non-sentential inputs.

3.2

Conditional Neural Language Models

Neural language models (Bengio et al., 2006) provide a distributed alternative to n-gram language
models, permitting the joint learning of a prediction function for the next word in a sequence
given the distributed representations of a subset
of the last n−1 words alongside the representations themselves. Recent work in dialogue act labelling (Kalchbrenner and Blunsom, 2013b) and
in machine translation (Kalchbrenner and Blunsom, 2013a) has demonstrated that a particular
kind of neural language model based on recurrent
neural networks (Mikolov et al., 2010; Sutskever
et al., 2011) could be extended so that the next
word in a sequence is jointly generated by the
word history and the distributed representation for
a conditioning element, such as the dialogue class
of a previous sentence, or the vector representation
of a source sentence. In this section, we briefly describe a general formulation of conditional neural
language models, based on the log-bilinear models of Mnih and Hinton (2007) due to their relative
simplicity.
A log-bilinear language model is a neural network modelling a probability distribution over the
next word in a sequence given the previous n−1,
i.e. p(wn |w1:n−1 ). Let |V | be the size of our vocabulary, and R be a |V | × d vocabulary matrix
where the Rwi demnotes the row containing the
word embedding in Rd of a word wi , with d being a hyper-parameter indicating embedding size.
Let Ci be the context transform matrix in Rd×d
which modifies the representation of the ith word
in the word history. Let bwi be a scalar bias associated with a word wi , and bR be a bias vector
in Rd associated with the model. A log-bilinear
model expressed the probability of wn given a history of n−1 words as a function of the energy of
the network:

−

!
T
Rw
C
i i

Rwn −

bTR Rwn

− bwn

i=1

From this, the probability distribution over the
next word is obtained:
p(wn |w1:n−1 ) = P

e−E(wn ;w1:n−1 )
−E(wn ;w1:n−1 )
wn e

To reframe a log-bilinear language model as a
conditional language model (CNLM), illustrated

wn-1

wn

β

Figure 2: Diagrammatic representation of a Conditional Neural Language Model.
in Fig. 2, let us suppose that we wish to jointly
condition the next word on its history and some
variable β, for which an embedding rβ has been
obtained through a previous step, in order to compute p(wn |w1:n−1 , β). The simplest way to do this
additively, which allows us to treat the contribution of the embedding for β as similar to that of an
extra word in the history. We define a new energy
function:
E(wn ; w1:n−1 , β) =
!
!
n−1
X
T
−
Rw
C + rβT Cβ Rwn − bTR Rwn − bwn
i i
i=1

to obtain the probability
e−E(wn ;w1:n−1 ,β)
−E(wn ;w1:n−1 ,β)
wn e

p(wn |w1:n−1 , β) = P

Log-bilinear language models and their conditional variants alike are typically trained by maximising the log-probability of observed sequences.
3.3

E(wn ; w1:n−1 ) =
n−1
X

wn-3 wn-2

A Combined Semantic Parsing Model

The models in §§3.1–3.2 can be combined to form
a model capable of jointly learning a shared latent representation for question/query pairs using
a BiCVM, and using this latent representation to
learn a conditional log-bilinear CNLM. The full
model is shown in Fig. 3. Here, we explain the
final model architecture both for training and for
subsequent use as a generative model. The details
of the training procedure will be discussed in §3.4.
The combination is fairly straightforward, and
happens in two steps at training time. For the

Knowledgebase query
Relation/object
embeddings

Conditional
Log-bilinear
Language Model

Generated Query

h
Query embedding
Question embedding
Latent
representation

g

Word embeddings

...
Question

Figure 3: Diagrammatic representation of the full
model. First the mappings for obtaining latent
forms of questions and queries are jointly learned
through a BiCVM. The latent form for questions
then serves as conditioning element in a logbilinear CNLM.
first step, shown in the left hand side of Fig. 3,
a BiCVM is trained against a parallel corpora
of natural language question and knowledgebase
query pairs. Optionally, the embeddings for the
query symbol representations and question words
are initialised and/or fine-tuned during training,
as discussed in §3.4. For the natural language
side of the model, the composition function g can
be a simple additive model as in Hermann and
Blunsom (2014a), although the semantic information required for the task proposed here would
probably benefit from a more complex composition function such as a convolution neural network. Function h, which maps the knowledgebase
queries into the shared space could also rely on
convolution, although the structure of the database
queries might favour a setup relying primarily on
bi-gram composition.
Using function g and the original training data,
the training data for the second stage is created
by obtaining the latent representation for the questions of the original dataset. We thereby obtain
pairs of aligned latent question representations and
knowledgebase queries. This data allows us to
train a log-bilinear CNLM as shown on the right
side of Fig. 3.
Once trained, the models can be fully joined to
produce a generative neural network as shown in
Fig. 4. The network modelling g from the BiCVM

g

...
Question
Figure 4: Diagrammatic representation of the final
network. The question-compositional segment of
the BiCVM produces a latent representation, conditioning a CNLM generating a query.
takes the distributed representations of question
words from unseen questions, and produces a latent representation. The latent representation is
then passed to the log-bilinear CNLM, which conditionally generates a knowledgebase query corresponding to the question.
3.4

Learning Model Parameters

We propose training the model of §3.3 in a two
stage process, in line with the symbolic model of
Kwiatkowski et al. (2013).
First, a BiCVM is trained on a parallel corpus
C of question-query pairs hQ, Ri ∈ C, using composition functions g for natural language questions
and h for database queries. While functions g and
h may differ from those discussed in Hermann and
Blunsom (2014a), the basic noise-contrastive optimisation function remains the same. It is possible to initialise the model fully randomly, in which

case the model parameters θ learned at this stage
include the two distributed representation lexica
for questions and queries, DQ and DR respectively, as well as all parameters for g and h.
Alternatively, word embeddings in DQ could be
initialised with representations learned separately,
for instance with a neural language model or a
similar system (Mikolov et al., 2010; Turian et al.,
2010; Collobert et al., 2011, inter alia). Likewise,
the relation and object embeddings in DR could be
initialised with representations learned from distributed relation extraction schemas such as that
of Riedel et al. (2013).
Having learned representations for queries in
DR as well as function g, the second training phase
of the model uses a new parallel corpus consisting
of pairs hg(Q), Ri ∈ C 0 to train the CNLM as presented in §3.3.
The two training steps can be applied iteratively,
and further, it is trivial to modify the learning
procedure to use composition function h as another input for the CNLM training phrase in an
autoencoder-like setup.

4

Experimental Requirements and
Further Work

The particular training procedure for the model
described in this paper requires aligned question/knowledgebase query pairs. There exist some
small corpora that could be used for this task
(Zelle and Mooney, 1996; Cai and Yates, 2013). In
order to scale training beyond these small corpora,
we hypothesise that larger amounts of (potentially
noisy) training data could be obtained using a
boot-strapping technique similar to Kwiatkowski
et al. (2013).
To evaluate this model, we will follow the experimental setup of Kwiatkowski et al. (2013).
With the provisio that the model can generate
freebase queries correctly, further work will seek
to determine whether this architecture can generate other structured formal language expressions,
such as lambda expressions for use in textual entailement tasks.

Acknowledgements
This work was supported by a Xerox Foundation
Award, EPSRC grants number EP/I03808X/1 and
EP/K036580/1, and the Canadian Institute for Advanced Research (CIFAR) Program on Adaptive
Perception and Neural Computation.

