

of the logic, and the set consists of all unique pos­
sible worlds. A probability distribution is placed
over this set. Probabilities are then assigned to
the sentences by giving each sentence a proba­
bility equal to the probability of the subset of
possible worlds in which it is true.
Although this approach is unproblematic when
applied to propositional languages, certain diffi­
culties arise when dealing with first order lan­
guages. By taking a different tack these difficul­
ties can be overcome, and indeed, it has already
been demonstrated that probabilities can be co­
herently assigned to the sentences of any first
order language ( Gaifman [2] , Scott and Krauss
[3] ).
While the method of assigning probabilities to
logical formulas is capable of representing prob­
abilistic degrees of belief, it is incapable of ef­
fectively representing statistical assertions. It is
argued that many types of defaults have a natu­
ral statistical interpretation, but cannot be rep­
resented by probabilities over logical formulas,
because of this limitation. Some authors have
attempted to represent default.s by ( conditional )
probabilities over logical formulas ( Geffner and
Pearl [4] , Pearl [5]), and the difficulties this
causes can be demonstrated.
It is pointed out that although probabilities
over logical formulas fails to do the job, statis­
tical assertions can be efficiently represented in
other types of probability logics, logics which go
beyond the simple device of assigning probabili-

In Probabilistic Logic Nilsson uses the device of
a probability distribution over a set of possible
worlds to assign probabilities to the sentences of
a logical language. In his paper Nilsson concen­
trated on inference and associated computational
issues. This paper, on the other hand, exam­
ines the probabilistic semantics in more detail,
.particularly for the case of first order languages,
and attempts to explain some of the features and
limitations of this form of probability logic. It is
pointed out that the device of assigning proba­
bilities to logical sentences has certain expressive
limitations. In particular, statistical assertions
are not easily expressed by such a device. This
leads to certain difficulties with attempts to give
probabilistic semantics to default reasoning us­
ing probabilities assigned to logical sentences.
1

Introduction

Nilsson [1] describes a method of assigning prob­
abilities to the sentences of a logic through a
probability distribution over a set of possible
worlds. Each possible world in this set is a consis­
tent assignment of truth values to the sentences
•This research was supported by a Post-Doctoral fel­
lowship funded by the U.S. Army Signals Warfare Labo­
ratory, while the author was a researcher at the University
of Rochester.

I
I

I;

15

I
I
Given a probability distribution over the set
of possible worlds it is possible to assign a prob­
ability to each sentence of the language. Each
sentence is given a probability equal to the prob­
ability of the set of worlds in which it is true. So,
for example, the sentence A v B is true in worlds
1, 2, and 3. Hence, its probability will be equal
to the probability of the set of worlds { 1, 2, 3}.
Equivalently, a probability distribution can be
placed directly ov-er the sentences of the logic,
more precisely over the Lindenbaum-Tarski alge­
bra of the language. This algebra is generated by
grouping the sentences into equivalence classes.
Two sentences, a and /3, are in the same equiv­
alence class iff f-o a +-+ f3, where f-0 indicates
deducible from the propositional axioms.
This technique is not limited to languages with
a finite number of atomic symbols.. However,
when the language is finite the atoms will be
sentences of the language, and the probability
distribution can be completely specified by the
probabilities of the atoms (the e-classes of). Any
sentence can be written as a disjunction of a
unique set of atoms, and its probability will be
the sum of the probabilities of these atoms. For
example, if we specify the probabilities {A 1\ B =
.5, Af\-,B= .1, -,Af\B= . 2, -,Af\...,B= 2 } , then
the sentence A VB will have probability 0.8 as it
can be written as (AI\ B) V (AI\ -,B) v (-,AI\ B).

ties to first order sentences.
2

The Propositional Case

A natural semantic model for a propositional lan­
guage is simply a subset of the set of atomic
symbols (Chang and Keisler [6]). This subset
is the set of atomic symbols which are assigned
the truth value true (t). In the propositional
case Nilsson's concept of possible worlds, i.e., a
set of consistent truth value assignments, has a
natural correspondence with the set of semantic
models. Each possible world is completely de­
termined by its truth value assignments to the
atomic symbols of the language, and the truth
value assignments to the atomic symbols can be
viewed as being the characteristic function of a
semantic model (with t= 1, f= 0).
For example, in a propositional language with
two atomic symbols {A, B} there are four possi­
ble worlds with corresponding semantic models
(u is used to indicate the truth function).
1. {Au= t, Bu= t} or {A, B}.
2. {Au= t, Bu= f} or {A}�
3. {Au= f, Bu= t} or {B}.
4.

.

{Au= f, Bu= f} or {}, i.e., the empty set.

Another way of looking at possible worlds,
which will turn out to be more useful when we
move to first order languages, is to consider the
atoms1 of the language. When the language has
a finite number of atomic symbols each possible
world can be represented as a single sentence: a
sentence formed by conjoining each atomic sym­
bol or its negation, such a sentence is called an
atom. Corresponding to the four worlds above
we have the four atoms A 1\ B ' A 1\ -,B ' -,A 1\ B '
and ...,A 1\ -,B.

3

First Order Languages

When the move is made to first order languages
certain problems arise. The first problem is that
we lose the nice correspondence between possible
worlds and semantic morlf'll'l 'T'hf' normal �Pman­
tic model for a first order language is consider­
ably more complex than the model for a proposi­
tional language, and the truth values of the sen­
tences in a first order "language are determined
both by the model and by an interpretation (i.e.,
the mapping from the symbols to the semantic
entities). For a given truth value assignment

1 An atom in a Boolean algebra is a minimal non-zero
element ( Bell and Machover, [71).

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

16

I

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

guage. What this means is that each existentially
quantified sentence is equal to the supremum of
all its instantiations. This implies that the prob­
ability of any existentially quantified sentence
must be greater than or equal to the probability
of any of its instances. Similarly, the probability
of any universally quantified sentence must be
less than or equal to the proba bility of any of its
instances.
This interpretation also makes sense in terms
of Nilsson's possible worlds. In any single possi­
ble world the existential must be true if any of
its instantiations are. Hence, the set of possible
worlds in which the existential is true includes
the set of possible worlds in which any instanti­
ation is true, and thus the existential must have
a probability greater than or equal to the prob­
ability of any of its instances.

to the sentences (possible world) there will be
many different (in fact an infinite number) of
model/interpretation pairs which will yield the
same truth values. Hence, the semantic struc­
ture of the possible worlds is unclear.
Another difficulty, which Nilsson is aware of, is
that Nilsson's techniques depend on being able
to generate consistent truth value assignments
for a set of sentences. These are used as 0/ 1
column vectors in his V matrix. This technique
is limited to languages in which the consistency
of a finite set of sentences can be established.
The consistency of a set of first order sentences
is not decidable, except in special cases (see Ack­
ermann [8] for an interesting survey).
These difficulties can be avoided if instead
of probability distributions over possible worlds
we consider probability distributions over the
Lindenbaum-Tarski (L-T) algebra of the lan­
guage. It has already been demonstrated by
Gaifman [2] that a probability measure can be
defined over the L-T algebra of sentences of a
first order language. Every sentence in the lan­
guage will have a probability equal to the prob­
ability of its equivalence class, and furthermore,
the probabilities will satisfy the condition

4

tistical Know ledge
Probabilities attached to logical sentences can be

interpreted as degrees of belief in those sentences.
Instead of either asserting a sentence or its nega­
tion, as in ordinary logic, one can attach some
intermediate degree to it, a degree of belief. So,
for example, one could represent a degree of be­
lief of greater than 0.9 in the assertion "Tweety
can fly" by assigning the sentence Fly(Tweety) a
probability > 0.9. However, it is not easy to rep­
resent statistical information, for example, the
assertion "More than 90% of all birds fly." 2
First, propositional languages do not seem to
possess sufficient power to represent these kinds
of statements. This particular statistical stat.e-

-.(a 1\ (3) then p[a v (3J p[aJ + p[(3],
where 1- indicates deducible from the first order
axioms. This means that the probability mea­
sure preserves the partial order of the algebra.
In this partial order we have a < f3 iff a 1\ f3 a;
hence, p[a] ::; p[a 1\ f3] + p[-.a 1\ /3]
p[,B] (by
the above condition). Under this partial order
the conjunction and disjunction operators gener­
ate the greatest lower bound (infimum) and least
upper bound (supremum).
To examine what happens to quantified sen­
tences under such a probability measure it is suf­
ficient to note that for L-T algebras we have that
If

1-

=

=

=

( * ) l3xal

=

·- ·

·-

.

�It is the case that first order logic is in some sense uni­
versally expressive. That is, set theory can be constructed
in first order lo gi c , and thus sufficient mathematics can
be built up inside the hl.nguage to represent statements
of this form. This is not, however, :tn efficient repre� en­
tation, nor is there any direct reflection in the s emantics
of the statistical information. I am concerned here with
efficient representations.

VtET la(x/t)l,

where I • I indicates the equivalence class of the
formula, and T is the set of terms of the Ian-

I
I

The Representation of Sta­

17

I
I
formula of a first order language (unconditional
probability assertions like "p[fly(x)]" are not
first order formulas either ) . This statement is
intended to assert that for every term, t, in the
first order language the conditional probability of
the sentence Fly(x/t), with the variable x sub­
stituted by the term t, given Bird(x/t) is > 0.9.
However, this formulation also falls prey to any
know exception. Say that there is some individ­
ual, denoted by the constant c, who is thought
to be a bird, i.e., p[Bird(c)] is high, and for some
reason or the other is also believed to be unable
to fly, i.e., p[Fly(c)] is low, then clearly this state­
ment cannot be true for the instance when x is
c; hence, the meta-level universal statement can­
not be true: it is not true for the instance c. It is
important to note that it does not matter what
other things are known about the individual c.
For example, c could be known to be an ostrich,
and thus there may be a good reason why c is
unable to fly. However, it will still be the case
that the conditional probability of Fly(c) given
just Bird(c) will not be > 0.9. The universal
statement will fail for c. That is, this problem is
not resolved by conditioning on more knowledge
as claimed by Cheeseman [9].
There is no way that the statistical statement
"More than 90% of all birds fly" can be repre­
sented by the assertion that the conditional prob­
ability is greater than 0.9 for all substitutions of
x: this assertion will be false for certain substi­
tutions. The problem here is that the statistical
statement implies that p[Fly(x)IBird(x)] > 0.9
for a random x , but a universally quantified x
is not the same as a random variable x; further­
more, the simple device of assiJ?;ninl!; prnhahilit.ies
to sentences of a logical language does not give
you access to random variables. This point has
also been raised by Schubert [10].
One can choose to interpret the variable 'x' as
being a random variable. However, simply choos­
ing such an interpretation is not sufficient; it does
not provide any formal meaning. That is, the se­
mantics and behaviour of such a random x must

ment is an assertion which indicates some rela­
tionship between the properties of being a bird
and being able to fly, but it is not an assertion
about any particular bird. This indicates that
some sort of variable is required. Propositionaf
languages do not have variables, and so are in­
adequate for this task even when they are gener­
alized to take on probabilities instead of just 1/0
truth values.
When we move to first order languages we do
get access to variables, variables which can range
over the set of individuals. A seemingly reason­
able way to represent this statement is to con­
sider the probabilistic generalization of the uni­
versal sentence 'VxB£rd(x) � Fly(x). The uni­
versal in 1/0 first order logic says that all birds
fly, so if we attach a probability of > 0.9 per­
haps we will get what we need. Unfortunately,
this does not work. If there is single bird who is
thought to be unable to fly, this universal will
be forced to have a probability close to zero.
That is, the probability of this universal must
be 1- p[3xB£rd(x) !v-.,Fly(x)]. Hence, if one be­
lieves to degree greater that 0.1 that a non-flying
bird exists, then the probability of the universal
must be < .9.
Since universal quantification or its dual exis­
tential quantification are the only ones available
in a first order language, it does not seem that
moving to first order languages allows us to rep­
resent statistical assertions. There is, however,
one more avenue available: conditional proba­
bilities. We have probabilities attached to sen­
tences hence with two sentences we can form
conditional probabilities. It has been suggested
(Cheeseman [9]) that meta-quantified statements
of the following form can be used to capture sta­
tistical statements, in particular for the state­
ment about birds:
('Vx)p[Fly(x) IB£rd(x)]

>

0.9.

The reason that this is a meta-quantification is
that the universal quantifier is quantifying over
a formula "p[Fly(x)IBird(x)]" which is not a

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

18

I

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

have universal statements like 'rlx Penguin(x) Bird(x). The probability of these universals is
one; thus, as discussed above, every instantia­
tion must also have probability one.
To examine the difficulties which arise from
this approach consider the following exam­
Say that we have a logical language
ple.
with the predicates Bird, Fly, and Penguin,
some set of terms { ti}, and a probability
distribution over the sentences of the lan­
guage which satisfies the default rules, i.e.,
for all terms ti, p[Fly(ti)JBird(ti)J � 1 and
p[---.Fly(ti)jPengut'n(ti)] � 1, and in which the
universal 'rlx Penguin(x) --+ Bt'rd(x), has proba­
bility one. Some simple facts which follow from
the universal having probability one are that for
all terms ti, p[Bird(ti)] 2: p[Penguin(t;)], and
p[B£rd(ti) /\ Penguin(ti)] p[Penguin(ti)]).
Consider the derivation in figure 1.
The constraints imply that for any term ti that

also be specified. It is possible to formalize such
an interpretation. A logic can be constructed
with random variables which have a formal se­
mantics and a proof theory which specifies their
behaviour. Such a logic has been constructed
(Bacchus [11,12]). However, its structure is quite
different from probability logics which attach
probabilities to first order sentences.
5

The Representation of De­
faults

There are many different defaults which have a
natural statistical justification, the famous ex­
ample of "Birds fly" being one of them. A nat­
ural reason for assuming by default that a par­
ticular bird can fly is simply the fact that, in a
statistical sense, most birds do fly. This is not to
say that all defaults have a statistical interpreta­
tion: there are many different notions of typical­
ity which do not have a straightforward statis­
tical interpretation, e.g., "Dogs give live birth"
(Carlson [13] , Nutter [14], also see Brachman [15]
for a discussion of different notions of typicality) .
Since probabilities attached to the sentences
of a logic do not offer any easy way of represent­
ing statistical assertions, it is not surprising that
attempts to use this formalism to give meaning
to defaults leads to certain difficulties.
Recently Geffner and Pearl [4] have pro­
posed giving semantics to defaults through meta­
quantified conditional probability statements
( also Pearl [5]3}.
For example, the default
"Birds fly" is given meaning through the meta­
quantified statement Vx p[Fly(x)jB£rd(x)! �
1 In order to allow penguins to be non­
flying birds they have the separate default rule:
Vx p[---.Fly(x)/Pengut'n(x)J � 1.
They also

=

p[Pengut'n(t;) :::; (�)p[---.Penguin(t;)!;

hence p[Pengut' n (ti ) J cannot be much greater
than 0.5. Since � 0.5 is an upper bound on
the probability of all instances of the formula
Penguin(x), it must also be the case that it is an
upper bound on the probability of the sentence
3x Penguin(x), by equation*·
That is, if we accept the defaults we are must
reject any sort of high level of belief in the exis­
tence of penguins.
To be fair to Geffner and Pearl their system
does provide a calcu.lus for reasoning with de·
faults. However, this result indicates that se­
mantically their particular probabilistic interpre­
tation of defaults causes anomalies. It does not
capture the statistical notion that birds usually
fly.

3Pearl uses a slightly different notion of probabilities
within € of one. The technical d ifferences between this
approach and that of Geffner and Pearl do not make any
difference to the following d iscussion; the anomalies pre­
sented also appear in Pearl's system.

6

It has been demonstrated that although proba­
bilities can be assigned to the sentences of any

I
I

Conclusions

19

I
I
1

::::;

I

p[Fly(ti)IBird(ti)]
p[Fly(ti) 1\ Bird(ti) 1\ --,Peng(ti)] p[Fly(ti) 1\ Bird(ti) 1\ Peng(ti)]
+
p[Bird(ti)]
p[Bird(ti)]
p[Fly(ti) 1\ Bird(ti) 1\ ....,Peng(ti)] p[Fly(ti) 1\ Bird(ti) 1\ Peng(ti)]
+
p[Penguin(t;)]
p[Penguin(ti)]
p[-,Penguin(ti)] p[Fly(ti) 1\ Penguin(ti)]
+
p[Penguin(ti)J
p[Penguin(ti)]
p[...., Penguin(ti)]
�--����+ ::::; 0
p[Penguin(ti)]

I

.

<

<

I
I
I

Figure 1: Conditional Probabilities close to one.
feasible conclusions, conclusions which can be
defeated by new information. Kyburg uses an
object language/meta-language formalism, and
has explored the inductive formation of defeasi­
ble conclusions in greater detail [17].

first order language, the resulting probability
logics are not powerful enough to efficiently rep­
resent statistical assertions. It h�s also been
demonstrated that attempts to give defaults a
probabilistic semantics using these types of prob­
ability logics leads to certain semantic anomalies.
Statistical facts, it has been argued, give a
natural justification to many default inferences.
This implies that probabilities might still be use­
ful for giving semantics to default rules and a jus­
tification to default inferences. For example, the
default rule "Birds fly" could be represented as
a statistical assertion that some large percentage
of birds fly, and the default inference "Tweety
flies" could be given the justification that Tweety
probably does fly if to the best of our knowledge
Tweety was a randomly selected bird.
Probability logics which accomplish this have
already been developed (Bacchus [11,12], Kyburg
[16]), but these logics go beyond the simple de­
vice of assigning probabilities to the sentences
of a logical language. Bacchus uses a logic which
has random variables as well as universally quan­
tified variables, this logic is capable of express­
ing statistical informa-tion, and possesses a sound
and complete proof theory capable of reasoning
with statistical facts. Default inferences are han­
dled by an inductive mechanism which forms de-

1

I
I
I

Acknowledgement

The author is thankful to Henry Kyburg for his
helpful suggestions.

I

