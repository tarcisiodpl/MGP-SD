
The Gibbs sampler is one of the most popular algorithms for inference in statistical
models. In this paper, we introduce a herding variant of this algorithm, called
herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an
O(1/T ) convergence rate for models with independent variables and for fully
connected probabilistic graphical models. Herded Gibbs is shown to outperform
Gibbs in the tasks of image denoising with MRFs and named entity recognition
with CRFs. However, the convergence for herded Gibbs for sparsely connected
probabilistic graphical models is still an open problem.

1

Introduction

Over the last 60 years, we have witnessed great progress in the design of randomized sampling
algorithms; see for example [16, 9, 3, 22] and the references therein. In contrast, the design of deterministic algorithms for “sampling” from distributions is still in its inception [8, 13, 7, 20]. There are,
however, many important reasons for pursuing this line of attack on the problem. From a theoretical perspective, this is a well defined mathematical challenge whose solution might have important
consequences. It also brings us closer to reconciling the fact that we typically use pseudo-random
number generators to run Monte Carlo algorithms on classical, Von Neumann architecture, computers. Moreover, the theory for some of the recently proposed deterministic sampling algorithms has
taught us that they can achieve O(1/T
√ ) convergence rates [8, 13], which are much faster than the
standard Monte Carlo rates of O(1/ T ) for computing ergodic averages. From a practical perspective, the design of deterministic sampling algorithms creates an opportunity for researchers to apply
a great body of knowledge on optimization to the problem of sampling; see for example [4] for an
early example of this.
The domain of application of currently existing deterministic sampling algorithms is still very narrow. Importantly, there do not exist deterministic tools for sampling from unnormalized multivariate
probability distributions. This is very limiting because the problem of sampling from unnormalized
distributions is at the heart of the field of Bayesian inference and the probabilistic programming
approach to artificial intelligence [17, 6, 18, 11]. At the same time, despite great progress in Monte
Carlo simulation, the celebrated Gibbs sampler continues to be one of the most widely-used algorithms. For, example it is the inference engine behind popular statistics packages [17], several tools
for text analysis [21], and Boltzmann machines [2, 12]. The popularity of Gibbs stems from its
simplicity of implementation and the fact that it is a very generic algorithm.
Without any doubt, it would be remarkable if we could design generic deterministic Gibbs samplers with fast (theoretical and empirical) rates of convergence. In this paper, we take steps toward
Authors are listed in alphabetical order.

1

achieving this goal by capitalizing on a recent idea for deterministic simulation known as herding.
Herding [24, 23, 10] is a deterministic procedure for generating samples x ∈ X ⊆ Rn , such that the
empirical moments µ of the data are matched. The herding procedure, at iteration t, is as follows:
x(t)

=

arg maxhw(t−1) , φ(x)i
x∈X

w

(t)

= w

(t−1)

+ µ − φ(x(t) ),

(1)

where φ : X → H is a feature map (statistic) from X to a Hilbert space H with inner product h·, ·i,
w ∈ H is the vector of parameters, and µ ∈ H is the moment vector (expected value of φ over
the data) that we want to match. If we choose normalized features by making kφ(x)k constant for
all x, then the update to generate samples x(t) for t = 1, 2, . . . , T in Equation 1 is equivalent to
minimizing the objective
2

T
1X
J(x1 , . . . , xT ) = µ −
(2)
φ(x(t) ) ,
T t=1
p
where T may have no prior known value and k · k = h·, ·i is the naturally defined norm based
upon the inner product of the space H [8, 4].

Herding can be used to produce samples from normalized probability distributions. This is done
as follows. Let µ denote a discrete, normalized probability distribution, with µi ∈ [0, 1] and
P
n
i=1 µi = 1. A natural feature in this case is the vector φ(x) that has all entries equal to zero,
except for the entry at the position indicated by x. For instance, if x = 2 and n = 5, we have
P
b = T −1 Tt=1 φ(x(t) ) is an empirical estimate of the distribution.
φ(x) = (0, 1, 0, 0, 0)T . Hence, µ
In this case, one step of the herding algorithm involves finding the largest component of the weight
(t−1)
vector (i? = arg maxi∈{1,2,...,n} wi
), setting x(t) = i? , fixing the i? -entry of φ(x(t) ) to one
and all other entries to zero, and updating the weight vector: w(t) = w(t−1) + (µ − φ(x(t) )). The
b converges on the
output is a set of samples {x(1) , . . . , x(T ) } for which the empirical estimate µ
target distribution µ as O(1/T ).
The herding method, as described thus far, only applies to normalized distributions or to problems
where the objective is not to guarantee that the samples come from the right target, but to ensure that
some moments are matched. An interpretation of herding in terms of Bayesian quadrature has been
put forward recently by [14].
In this paper, we will show that it is possible to use herding to generate samples from more complex
unnormalized probability distributions. In particular, we introduce a deterministic variant of the
popular Gibbs sampling algorithm, which we refer to as herded Gibbs. While Gibbs relies on drawing samples from the full-conditionals at random, herded Gibbs generates the samples by matching
the full-conditionals. That is, one simply applies herding to all the full-conditional distributions.
The experiments will demonstrate that the new algorithm outperforms Gibbs sampling and mean
field methods in the domain of sampling from sparsely connected probabilistic graphical models,
such as grid-lattice Markov random fields (MRFs) for image denoising and conditional random
fields (CRFs) for natural language processing.
We advance the theory by proving that the deterministic Gibbs algorithm converges for distributions
of independent variables and fully-connected probabilistic graphical models. However, a proof establishing suitable conditions that ensure convergence of herded Gibbs sampling for sparsely connected probabilistic graphical models is still unavailable.

2

Herded Gibbs Sampling

For a graph of discrete nodes G = (V, E), where the set of nodes are the random variables V =
{Xi }N
i=1 , Xi ∈ X , let π denote the target distribution defined on G.
Gibbs sampling is one of the most popular methods to draw samples from π. Gibbs alternates (either
systematically or randomly) the sampling of each variable Xi given XN (i) = xN (i) , where i is the
index of the node, and N (i) denotes the neighbors of node i. That is, Gibbs generates each sample
from its full-conditional distribution p(Xi |xN (i) ).
2

Algorithm 1 Herded Gibbs Sampling.
Input: T .
(0)
Step 1: Set t = 0. Initialize X(0) in the support of π and wi,xN (i) in (π(Xi = 1|xN (i) ) −
1, π(Xi = 1|xN (i) )).
for t = 1 → T do
(t−1)
Step 2: Pick a node i according to some policy. Denote w = w (t−1) .
i,xN (i)

(t)
xi

(t)
xi

Step 3: If w > 0, set
= 1, otherwise set
= 0.
(t)
(t−1)
(t−1)
(t)
Step 4: Update weight w (t) = w (t−1) + π(Xi = 1|xN (i) ) − xi
i,xN (i)

i,xN (i)

(t)

(t−1)

Step 5: Keep the values of all the other nodes xj = xj
(t)
wj,xN (j)

=

(t−1)
wj,xN (j) , ∀j

6= i or xN (j) 6=

, ∀j 6= i and all the other weights

(t−1)
xN (i) .

end for
Output: x(1) , . . . , x(T )

Herded Gibbs replaces the sampling from full-conditionals with herding at the level of the fullconditionals. That is, it alternates a process of matching the full-conditional distributions p(Xi =
xi |XN (i) ). To do this, herded Gibbs defines a set of auxiliary weights {wi,xN (i) } for any value of
Xi = xi and XN (i) = xN (i) . For ease of presentation, we assume the domain of Xi is binary,
X = {0, 1}, and we use one weight for every i and assignment to the neighbors xN (i) . Herded
Gibbs can be trivially generalized to the multivariate setting by employing weight vectors in R|X |
instead of scalars.
If the binary variable Xi has four binary neighbors XN (i) , we must maintain 24 = 16 weight vectors.
Only the weight vector corresponding to the current instantiation of the neighbors is updated, as
illustrated in Algorithm 1. The memory complexity of herded Gibbs is exponential in the maximum
node degree. Note the algorithm is a deterministic Markov process with state (X, W).
The initialization in step 1 guarantees that X(t) always remains in the support of π. For a deterministic scan policy in step 2, we take the value of variables x(tN ) , t ∈ N as a sample sequence.
Throughout the paper all experiments employ a fixed variable traversal for sample generation. We
call one such traversal of the variables a sweep.

3

Analysis

As herded Gibbs sampling is a deterministic algorithm, there is no stationary probability distribution
of states. Instead, we examine the average of the sample states over time and hypothesize that it
converges to the joint distribution, our target distribution, π. To make the treatment precise, we need
the following definition:
Definition 1. For a graph of discrete nodes G = (V, E), where the set of nodes V = {Xi }N
i=1 ,
(τ )
Xi ∈ X , PT is the empirical estimate of the joint distribution obtained by averaging over T
(τ )
samples acquired from G. PT is derived from T samples, collected at the end of every sweep over
N variables, starting from the τ th sweep:
(τ )

PT (X = x) =

1
T

τ +T
X−1

I(X(kN ) = x)

(3)

k=τ

Our goal is to prove that the limiting average sample distribution over time converges to the target
distribution π. Specifically, we want to show the following:
(τ )

lim PT (x) = π(x), ∀τ ≥ 0

T →∞

If this holds, we also want to know what the convergence rate is.
3

(4)

We begin the theoretical analysis with a graph of one binary variable. For this graph, there is only
one weight w. Denote π(X = 1) as π for notational simplicity. The sequence of X is determined
by the dynamics of w (shown in Figure 1):

1 if w(t−1) > 0
w(t) = w(t−1) + π − I(w(t−1) > 0), X (t) =
(5)
0 otherwise
Lemma 3 in the appendix shows that (π − 1, π] is the invariant interval of the dynamics, and the
state X = 1 is visited at a frequency close to π with an error:
1
(6)
T
This is known as the fast moment matching property in [24, 23, 10]. We will show in the next two
theorems that the fast moment matching property also holds for two special types of graphs, with
proofs provided in the appendix.
(τ )

|PT (X = 1) − π| ≤

Figure 1: Herding dynamics for a single variable.
In an empty graph, all the variables are independent of each other and herded Gibbs reduces to
running N one-variable chains in parallel. Denote the marginal distribution πi := π(Xi = 1).
Examples of failing convergence in the presence of rational ratios between the πi s were observed in
[4]. There the need for further theoretical research on this matter was pointed out. The following
theorem provides formal conditions for convergence in the restricted domain of empty graphs.
Theorem 1. For an empty graph, when herded Gibbs has a fixed scanning order, and
(τ )
{1, π1 , . . . , πN } are rationally independent, the empirical distribution PT converges to the target distribution π as T → ∞ for any τ ≥ 0.
A set of n real numbers, x1 , x2 , . . P
. , xn , is said to be rationally independent if for any set of rational
n
numbers, a1 , a2 , . . . , an , we have i=1 ai xi = 0 ⇔ ai = 0, ∀1 ≤ i ≤ n. The proof of Theorem 1
consists of first formulating the dynamics of the weight vector as a constant translation mapping in
a circular unit cube, and then proving that the weights are uniformly distributed by making use of
Kronecker-Weyl’s theorem [25].
For fully-connected (complete) graphs, convergence is guaranteed even with rational ratios. In fact,
herded Gibbs converges to the target joint distribution at a rate of O(1/T ) with a O(log(T )) burn-in
period. This statement is formalized in Theorem 2.
Theorem 2. For a fully-connected graph, when herded Gibbs has a fixed scanning order and a
Dobrushin coefficient of the corresponding Gibbs sampler η < 1, there exist constants l > 0, and
B > 0 such that
λ
, ∀T ≥ T ∗ , τ > τ ∗ (T )
T


(1+η)
(1−η)lT
2B
∗
∗
2
where λ = 2N
, and dv (δπ) := 12 ||δπ||1 .
l(1−η) , T = l , τ (T ) = log 1+η
4N
(τ )

dv (PT

− π) ≤

(7)

The constants l and B are defined in Equation 31 for Proposition 4 in the appendix. If we ignore the
burn-in period and start collecting samples simply from the beginning, we achieve a convergence rate
)
of O( log(T
) as stated in Corollary 10 in the appendix. The constant l in the convergence rate has an
T
exponential term, with N in the exponent. An exponentially large constant seems to be unavoidable
for any sampling algorithm when considering the convergence to a joint distribution with 2N states.
As for the marginal distributions, it is obvious that the convergence rate of herded Gibbs is also
4

O(1/T ) because marginal probabilities are linear functions of the joint distribution. However, in
practice, we observe very rapid convergence results for the marginals, so stronger theoretical results
about the convergence of the marginal distributions seem plausible.
The proof proceeds by first bounding the discrepancy between the chain of empirical estimates of
(s)
the joint obtained by averaging over T herded Gibbs samples, {PT }, s ≥ τ , and a Gibbs chain
(τ )
initialized at PT . After one iteration, this discrepancy is bounded above by O(1/T ).
The Gibbs chain has geometric convergence to π and the distance between the Gibbs and herded
Gibbs chains is bounded by O(1/T ). The geometric convergence rate to π dominates the discrep(τ )
ancy of herded Gibbs and thus we infer that PT converges to π geometrically. To round-off the
proof, we must find a limiting value for τ . The proof concludes with an O(log(T )) burn-in for τ .
However, for a generic graph we have no mathematical guarantees on the convergence rate of herded
Gibbs. In fact, one can easily construct synthetic examples for which herded Gibbs does not seem
to converge to the true marginals and joint distribution. For the examples covered by our theorems
and for examples with real data, herded Gibbs demonstrates good behaviour. The exact conditions
under which herded Gibbs converges for sparsely connected graphs are still unknown.

4
4.1

Experiments
Simple Complete Graph

We begin with an illustration of how herded Gibbs substantially outperforms Gibbs on a simple
complete graph. In particular, we consider a fully-connected model of two variables, X1 and X2 ,
as shown in Figure 2; the joint distribution of these variables is shown in Table 1. Figure 3 shows
the marginal distribution P (X1 = 1) approximated by both Gibbs and herded Gibbs for different
. As  decreases, both approaches require more iterations to converge, but herded Gibbs clearly
outperforms Gibbs. The figure also shows that Herding does indeed exhibit a linear convergence
rate.
X1 = 0 X1 = 1 P(X2 )
X2 = 0 1/4 − 

1/4
X1
X2

3/4 − 
3/4
X2 = 1
P(X1 )
1/4
3/4
1
Figure 2: Two-variable model.
4.2

Table 1: Joint distribution of the two-variable model.

MRF for Image Denoising

Next, we consider the standard setting of a grid-lattice MRF for image denoising. Let us assume
that we have a binary image corrupted by noise, and that we want to infer the original clean image.
Let Xi ∈ {−1, +1} denote the unknown true value of pixel i, and yi the observed, noise-corrupted
value of this pixel. We take advantage of the fact that neighboring pixels are likely to have the same
label by defining an MRF with an Ising prior. That is, we specify a rectangular 2D lattice with the
following pair-wise clique potentials:
 J

e ij e−Jij
ψij (xi , xj ) = −Jij
(8)
e
eJij
and joint distribution:

p(x|J) =


1X

1
1
ψij (xi , xj ) =
exp 
Z(J) i∼j
Z(J)
2
Y

Jij xi xj  ,

(9)

i∼j

where i ∼ j is used to indicate that nodes i and j are connected. The known parameters Jij establish
the coupling strength between nodes i and j. Note that the matrix J is symmetric. If all the Jij > 0,
then neighboring pixels are likely to be in the same state.
5

(a) Approximate marginals obtained via Gibbs (blue) and herded Gibbs (red).

(b) Log-log plot of marginal approximation errors obtained via Gibbs (blue) and herded Gibbs (red).

(c) Inverse of marginal approximation errors obtained via Gibbs (blue) and herded Gibbs (red).

Figure 3: (a) Approximating a marginal distribution with Gibbs (blue) and herded Gibbs (red) for an
MRF of two variables, constructed so as to make the move from state (0, 0) to (1, 1) progressively
more difficult as  decreases. The four columns, from left to right, are for  = 0.1,  = 0.01,
 = 0.001 and  = 0.0001. Table 1 provides the joint distribution for these variables. The error
bars for Gibbs correspond to one standard deviation. Rows (b) and (c) illustrate that the empirical
convergence rate of herded Gibbs matches the expected theoretical rate. In the plots of rows (b)
and (c), the upper-bound in the error of herded Gibbs was used to remove the oscillations so as to
illustrate the behaviour of the algorithm more clearly.
The MRF model combines the Ising prior with a likelihood model as follows:
 "

#
Y
Y
1
p(x, y) = p(x)p(y|x) = 
ψij (xi , xj ) .
p(yi |xi )
Z i∼j
i

(10)

The potentials ψij encourage label smoothness. The likelihood terms p(yi |xi ) are conditionally
independent (e.g. Gaussians with known variance σ 2 and mean µ centered at each value of xi ,
denoted µxi ). In more precise terms,


X
X
1
1
1
p(x, y|J, µ, σ) =
exp 
(11)
Jij xi xj − 2
(yi − µxi )2  .
Z(J, µ, σ)
2 i∼j
2σ i
6

Figure 4: Original image (left) and its corrupted version (right), with noise parameter σ = 4.

Figure 5: Reconstruction errors for the image denoising task. The results are averaged across
10 corrupted images with Gaussian noise N (0, 16). The error bars correspond to one standard
deviation. Mean field requires the specification of the damping factor D.

P
When the coupling parameters Jij are identical, say Jij = J, we have
ij Jij f (xi , xj ) =
P
J ij f (xi , xj ).
Hence, different neighbor configurations result in the same value of
P
J ij f (xi , xj ). If we store the conditionals for configurations with the same sum together, we
only need to store as many conditionals as different possible values that the sum could take. This
enables us to develop a shared version of herded Gibbs that is more memory efficient where we only
maintain and update weights for distinct states of the Markov blanket of each variable.
In this exemplary image denoising experiment, noisy versions of the binary image, seen in Figure 4
(left), were created through the addition of Gaussian noise, with varying σ. Figure 4 (right) shows a
corrupted image with σ = 4. The L2 reconstruction errors as a function of the number of iterations,
for this example, are shown in Figure 5. The plot compares the herded Gibbs method against Gibbs
and two versions of mean field with different damping factors [19]. The results demonstrate that the
herded Gibbs techiques are among the best methods for solving this task.
A comparison for different values σ is presented in Table 2. As expected mean field does well in the
low-noise scenario, but the performance of the shared version of herded Gibbs as the noise increases
is significantly better.
7

Table 2: Errors of image denoising example after 30 iterations (all measurements have been scaled
by ×10−3 ). We use an Ising prior with Jij = 1 and four Gaussian noise models with different
σ’s. For each σ, we generated 10 corrupted images by adding Gaussian noise. The final results
shown here are averages and standard deviations (in parentheses) across the 10 corrupted images. D
denotes the damping factor in mean field.
σ
MethodPPP

2

4

6

8

Herded Gibbs
Herded Gibbs - shared
Gibbs
Mean field (D=0.5)
Mean field (D=1)

21.58(0.26)
22.24(0.29)
21.63(0.28)
15.52(0.30)
17.67(0.40)

32.07(0.98)
31.40(0.59)
37.20(1.23)
41.76(0.71)
32.04(0.76)

47.52(1.64)
42.62(1.98)
63.78(2.41)
76.24(1.65)
51.19(1.44)

67.93(2.78)
58.49(2.86)
90.27(3.48)
104.08(1.93)
74.74(2.21)

PP
P

Person

Other

Other

Other

Other

Other

Person

Other

Other

Other

Other

Zed

’

s

dead

baby

.

Zed

’

s

dead

.

Figure 6: Typical skip-chain CRF model for named entity recognition.

4.3

CRF for Named Entity Recognition

Named Entity Recognition (NER) involves the identification of entities, such as people and locations, within a text sample. A conditional random fied (CRF) for NER models the relationship
between entity labels and sentences with a conditional probability distribution: P (Y |X, θ), where
X is a sentence, Y is a labeling, and θ is a vector of coupling parameters. The parameters, θ, are feature weights and model relationships between variables Yi and Xj or Yi and Yj . A chain CRF only
employs relationships between adjacent variables, whereas a skip-chain CRF can employ relationships between variables where subscripts i and j differ dramatically. Skip-chain CRFs are important
in language tasks, such as NER and semantic role labeling, because they allow us to model long
dependencies in a stream of words, see Figure 6.
Once the parameters have been learned, the CRF can be used for inference; a labeling for some
sentence X is found by maximizing the above probability. Inference for CRF models in the NER
domain is typically carried out with the Viterbi algorithm. However, if we want to accommodate long
term dependencies, thus resulting in the so called skip-chain CRFs, Viterbi becomes prohibitively
expensive. To surmount this problem, the Stanford named entity recognizer [15] makes use of
annealed Gibbs sampling.
To demonstrate herded Gibbs on a practical application of great interest in text mining, we modify
the standard inference procedure of the Stanford named entity recognizer by replacing the annealed
Gibbs sampler with the herded Gibbs sampler. The herded Gibbs sampler in not annealed. To find
the maximum a posteriori sequence Y , we simply choose the sample with highest joint discrete
probability. In order to be able to compare against Viterbi, we have purposely chosen to use singlechain CRFs. We remind the reader, however, that the herded Gibbs algorithm could be used in cases
where Viterbi inference is not possible.
We used the pre-trained 3-class CRF model in the Stanford NER package [15]. This model is a
linear chain CRF with pre-defined features and pre-trained feature weights, θ. For the test set, we
used
the corpus for theNIST 1999 IE-ER Evaluation. Performance is measured in per-entity F1

precision·recall
. For all the methods, except Viterbi, we show F1 scores after 100, 400 and
F1 = 2 · precision+recall
800 iterations in Table 3. For Gibbs, the results shown are the averages and standard deviations
over 5 random runs. We used a linear annealing schedule for Gibbs. As the results illustrate,
8

”Pumpkin” (Tim Roth) and ”Honey Bunny” (Amanda Plummer) are having breakfast in a
diner. They decide to rob it after realizing they could make money off the customers as
well as the business, as they did during their previous heist. Moments after they initiate
the hold-up, the scene breaks off and the title credits roll. As Jules Winnfield (Samuel L.
Jackson) drives, Vincent Vega (John Travolta) talks about his experiences in Europe, from
where he has just returned: the hash bars in Amsterdam, the French McDonald’s and its
”Royale with Cheese”.

Figure 7: Results for the application of the NER CRF to a random Wikipedia sample [1]. Entities
are automatically classified as Person, Location and Organization.

herded Gibbs attains the same accuracy as Viterbi and it is faster than annealed Gibbs. Unlike
Viterbi, herded Gibbs can be easily applied to skip-chain CRFs. After only 400 iterations (90.5
seconds), herded Gibbs already achieves an F1 score of 84.75, while Gibbs, even after 800 iterations
(115.9 seconds) only achieves an F1 score of 84.61. The experiment thus clearly demonstrates that
(i) herded Gibbs does no worse than the optimal solution, Viterbi, and (ii) herded Gibbs yields
more accurate results for the same amount of computation. Figure 7 provides a representative NER
example of the performance of Gibbs, herded Gibbs and Viterbi (all methods produced the same
annotation for this short example).
Table 3: Gibbs, herded Gibbs and Viterbi for the NER task. The average computational time each
approach took to do inference for the entire test set is listed (in square brackets). After only 400
iterations (90.48 seconds), herded Gibbs already achieves an F1 score of 84.75, while Gibbs, even
after 800 iterations (115.92 seconds) only achieves an F1 score of 84.61. For the same computation,
herded Gibbs is more accurate than Gibbs.
```
```Iterations 100
```
Method
`
Annealed Gibbs
Herded Gibbs
Viterbi

5

84.36(0.16) [55.73s]
84.70 [59.08s]

400

800

84.51(0.10) [83.49s]
84.75 [90.48s]

84.61(0.05) [115.92s]
84.81 [132.00s]
84.81[46.74s]

Conclusions and Future Work

In this paper, we introduced herded Gibbs, a deterministic variant of the popular Gibbs sampling algorithm. While Gibbs relies on drawing samples from the full-conditionals at random, herded Gibbs
generates the samples by matching the full-conditionals. Importantly, the herded Gibbs algorithm is
very close to the Gibbs algorithm and hence retains its simplicity of implementation.
The synthetic, denoising and named entity recognition experiments provided evidence that herded
Gibbs outperforms Gibbs sampling. However, as discussed, herded Gibbs requires storage of the
conditional distributions for all instantiations of the neighbors in the worst case. This storage requirement indicates that it is more suitable for sparse probabilistic graphical models, such as the
CRFs used in information extraction. At the other extreme, the paper advanced the theory of deterministic sampling by showing that herded Gibbs converges with rate O(1/T ) for models with
independent variables and fully-connected models. Thus, there is gap between theory and practice
that needs to be narrowed. We do not anticipate that this will be an easy task, but it is certainly a key
direction for future work.
We should mention that it is also possible to design parallel versions of herded Gibbs in a Jacobi
fashion. We have indeed studied this and found that these are less efficient than the Gauss-Seidel
version of herded Gibbs discussed in this paper. However, if many cores are available, we strongly
recommend the Jacobi (asynchronous) implementation as it will likely outperform the Gauss-Seidel
(synchronous) implementation.
9

The design of efficient herding algorithms for densely connected probabilistic graphical models
remains an important area for future research. Such algorithms, in conjunction with Rao Blackwellization, would enable us to attack many statistical inference tasks, including Bayesian variable
selection and Dirichlet processes.
There are also interesting connections with other algorithms to explore. If, for a fully connected
graphical model, we build a new graph where every state is a node and directed connections exist
between nodes that can be reached with a single herded Gibbs update, then herded Gibbs becomes
equivalent to the Rotor-Router model of Alex Holroyd and Jim Propp1 [13]. This deterministic analogue of a random walk has provably superior concentration rates for quantities such as normalized
hitting frequencies, hitting times and occupation frequencies. In line with our own convergence
results,
√ it is shown that discrepancies in these quantities decrease as O(1/T ) instead of the usual
O(1/ T ). We expect that many of the results from this literature apply to herded Gibbs as well.
The connection with the work of Art Owen and colleagues, see for example [7], also needs to
be explored further. Their work uses completely uniformly distributed (CUD) sequences to drive
Markov chain Monte Carlo schemes. It is not clear, following discussions with Art Owen, that CUD
sequences can be constructed in a greedy way as in herding.

