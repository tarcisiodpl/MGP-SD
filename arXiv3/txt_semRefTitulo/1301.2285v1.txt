

While plausible reasoning has been applied to a vari­

This article deals with plausible reasoning

about spatial information. On the other hand, it has

ety of domains, it has rarely been applied to reasoning
been applied to reasoning about

from incomplete knowledge about large-scale

tial information.

consisting of a set of pointwise observations,

fluents1 do not change and therefore that their value

belief functions to represent the influence of

persist from one time point to the subsequent one, un­

the knowledge at a given point to another

less the contrary is known (from an observation, for

point; the quantitative strength of this in­

instance) or inferred; this implies some minimization

fluence decreases when the distance between

of change. Now, the latter persistence paradigm can

These influences are

be transposed from temporal to spatial reasoning. In

aggregated using a variant of Dempster's rule

the very same line of reasoning, when reasoning about

of combination taking into account the rela­

properties in space, it is (often) intuitively satisfactory

tive dependence between observations.

to assume that, knowing from an observation that a
given property r.p holds at a given point

Introduction

as well at points "close enough" to

This article aims at handling knowledge about large­
scale spatial properties (e.g.

soil type, weather), in

contexts where this knowledge is only partial; i.e. some
piece of information is known only at some given lo­

cations of space. We have investigated some means to
perform plausible reasoning on this kind of informa­
tion at any point in the considered space.
Several studies can be related to the question of
usually consider the question of representing incom­
plete knowledge about the location of spatial ob­
jects (using relational theories, more or less related
to the seminal work of [Randell

et al., 1992], or us­
2000]), or about vague re­
1996], rather than about static

ing fuzzy locations [Bloch,
gions [Cohn and Gotts,

properties and their distribution over a given space.

et al., 2000]

apply revision strategies to in­

consistency removing in geographical information sys­
tems.

A completely different line of work, in the

robotics literature, deals with map building using oc­
cupancy grids (see e.g.

[Iyengar and Elfes,

will be briefly discussed in Section

5.2.

x,

then it holds

x.

What we precisely mean by "close enough" depends
on the nature of the region as well as on the property
r.p involved. Moreover, it is clear that the belief that
r.p "persists" from point

x

to point y is gradually de­

creasing: the closer y to

x,

the more likely r.p observed

at

x

is still true at y. This graduality can be modelled

by order relations or by quantitative measures such as
probability. However, as we explain in Section 3, pure

imprecise knowledge in spatial databases, but they

[Wiirbel

Plausible reasoning about systems

that evolve over time usually consists in assuming that

is extrapolated to neighbour points. We use

both points increases.

temporal information,

which gives some hints about how to do it for spa­

spatial properties. The available information,

1

France

1991]);

it

probabilistic reasoning is not well-suited to this kind of

reasoning, unless very specific assumptions are made.
We therefore model persistence with the help of the

belief function theory,

also known as the Dempster­

Shafer theory. Belief functions (and their duals, plau­
sibility functions) generalize probability measures and
enable a clear distinction between randomness and ig­
norance that probability measures fail to do.
After giving some background on belief functions, we
show how to infer plausible conclusions, weighted by
belief degrees, from spatial observations.

Then, we

relate computational experiments, evoke information­
theoretic and decision-theoretic issues, and conclude.
1A

fluent is

a

proposition

which

evolves over time.

LANG & MULLER

286

Background on belief functions

2

UAI2001

well-suited to the combination of information from dif­
ferent sources. The

The

Dempster�Shafer

[Dempster,

1967]

theory

[Shafer,

1976]

of

evidence

is a generalization of

Dempster combination m1 ffi m2 of

two (normalized) mass functions
defined by

m1

probability theory enabling an explicit distinction
between randomness and ignorance.
Let

S

mt

ffi m2(A) =

be a finite set of possible states of the world

(taken to be the set for possible values for a given vari­
able, for the sake of simplicity),

one of which corre­

L
X,Y�S,XnY=A

and

m2

on S is

m1(X)m2(Y)
R(m1, m2)

where

sponds to the real world. A (normalized)

mass assign­
m(0) = 0
and LACS m(A) = 1. The condition that m(0) = 0 is
sometimes omitted (see [Smets and Kennes, 1994]): if
m(0) > 0 then we say that m is an unnormalized mass
assignment. The int erest of having a mass assignment
ment

is a mapping

m: 25

-t [0,

1]

such that

unnormalized is the ability to keep track of a degree
of conflict.
The subsets of S with a nonempty mass are called the

focal elements of m. m

simple support function iff
it there is a nonempty subset A of S and a a E [0, 1]
such that m(A) =a and m (S) = 1-a (by convention,
when specifying

is a

ssignmen t we omit subsets

a ma.ss a

with an empty mass).
A mass assignment

Plm

Plm(B)

from 25 to [0,

=

Belrn (B)

m

induces two set functions

Belm
1]: the belief function Belm
and the plausibility function Plm are defined respec­
tively by: '</B � S, Belm(B) = L ACB m (A) and
and

LAnB;t.0 m(A).

When

m is

normalized,

represents the probability of existence of at

least one true piece of evidence for A, while

Plm(B)

represents the probability of existence of at least one
true piece of evidence which does not contradict A.
When all focal elements are singletons,

m

can be

viewed as a probability distribution on S; in this case

X,Y<::;S,XnY=0

Importantly, this operation is associative, which en­

m1 ffi m2 ffi . .. ffi
of mass assignments.

ables its extension to the combination

mn of an

arbi t r ary number n

When unnormalization is allowed, we define the t.m ­

normalized Dempster combination
or not) mass assignments

m1

and

of two (normalized

m2

on

S

by

X,Y<;:S,XnY=A

The resulting

m1 ffiu m2 (0)
m1 and m2.

measures the degree of

conflict between

Lastly, in some cases it is needed to transform a mass
assignment into a probability distribution. This is the
case for instance when performing decision-theoretic
tasks.
place

Importantly, this transformation should take

after combination has been performed

and not

before, as argued in [Smets and Kennes, 1994] who in­
troduce the

pignistic transform T(m)

of a normalized

m, being the probability distribution
by: Vs E S, T(m)(s) :::: L A<;:S,•EA
���)

mass assignment
on S defined

·

Alternatives to the pignistic transform for decision

Belm (A) = Plm(A) = LsEA m(s), hence, Belm and
Plm coincide and are identical to the probability mea­

making using belief functions are given in [Strat, 1994].

sure induced by

3

m.

Therefore Dempster-Shafer theory

Extrapolation from observations

generalizes probability theory on finite universes.
The Dempster-Shafer theory of evidence enables an
explicit distinction between randomness and ignorance

that probability theory cannot2. Another crucial ad­
vantage of the theory of belief functions is that it is
2This is clear from the following two mass functions:

mi{head,tails} = 1; m2({head}) = m2({tails}) = %·
m2 represents a true random phenomenon such as toss­

ing a regular coin, while m, would correspond to a case
where it is not reasonable to define prior probabilities on
imagine for instance that you were just

{head, tails} -

given a parrot with the only knowledge that the two words
knows are ''head" and "tails": there is absolutely no
reason to postulate that it says "heads" and "tails" ran­
domly with a probability
(nor with any other probabil­

it

i ty ) ; it may
"head".

%

well be the case, for instance, that it always say
This state of complete ignorance about the out­

come of the event is well represented by the neutral mass

3.1

Observations

From now on we consider a space E, i.e., a set of
"spatial points" (which could be seen as either Eu­

clidean points or atomic regions). E is equipped with
a distance3

d.

We are interested in reasoning on the evolution "in
space" of some properties. For the sake of simplicity,
the property of interest merely consists of the value of
function
1.
3Recall that a distance is a mapping
E2 -t JR+
such as (i)
= 0 if
and only if
(ii)
and (iii)
+
z)::;
z). However we do
not really require the triangular inequality (iii); hence our
formal framework only requires d to be a pseudo-distance
but these technical details will not be discussed further.

m(S) =

d(y, x)

d(x, y)
d(x, y)

d(y,

d:
x = y;
d(x,

d(x, y) =

UAI 2001

LANG & MULLER

a given variable, whose domain is a finite set S. S is

furthermore assumed to be purely qualitative, i.e., S is

not a discretized set of numerical values. S may be for
instance a set of possible soil types, or a set of weather
types. The simplest case is when S may is binary, i.e.,

the property of interest is a propositional variable the

truth value of which we are interested in -for instance
S=

{rain,-,rain}.

An

observation function 0

is

a

mapping

from

Dom(O) � E to the set of nonempty subsets of S. 0

intuitively consists of a set of pointwise observations
(x, O(x)) where x E E and O(x) is a nonempty subset

of S; such a pointwise observation means that it has
been observed that the state of the world at point

longs to

x

be­

O(x) . 0 is said to be complete at x ifO(x) is a

singleton and

trivial at x ifO(x) =

S. The

range R(O)

of 0 is the set of points where a nontrivial observation
has been performed, i.e.,
3.2

R(O)

=

{x!O(x) f. S}.

degree which is all the higher

y is close to x. In

O(y) has a maximal (and absolute) impact on

while,

x

when y gets too far from x, this impact becomes null.

By default (and like to [Dean and Kanazawa, 1989] for

temporal persistence ) we will use exponential decay

functions

f(obs, a )

= exp(-

>.(;bs))

where

is a

J..(obs)

real strictly positive number expressing the "persis­
tence power" of the observation

obs

(such a function is

called an exponential decay function ) . This deserves
further comments.

We first consider the case of complete observations,

i.e.,

obs

is a singleton

{v}. J..({v}),

written

J..(v)

with­

out any risk of misunderstanding, characterize the per­

sistence degree of the value

J..( v ) ,

the lower

v:

stronger the spatial persistence of the property V

The two limit cases for

The question is now how to extrapolate from an ob­
servation function 0.

as

particular, if x = y (thus d(x, y) = 0) then O(x) =

•

Spatial persistence

287

J..(v)

is

As explained in the introduc­

=

exp (- >.

0;

(v))

J..( v)

are:

the

=

v.

by passage to the limit we write

= 0 and therefore the property V

non-persistent:

::::: v

as soon as d(x, y) > 0, the fact

that V = v holds at point y does not support

tion, the spatial persistence principle stipulates that

the belief that V

=

v

should hold at

x

too. As

as long as nothing contradicts it, a property observed

an example, consider the property "the 5th dec­

with a quantity of belief decreasing with the distance

this property is non-persistent (provided that the

at a given point is believed to hold at points nearby,

imal of the temperature at x is even".

to the observation. This principle is now formally en­
coded in the framework of belief functions.
Let x be a given point of E, called the

focus point.

What we are interested in is to infer some new (plau­
sible) beliefs about what holds at x. For this we con­

sider a set of mass assignments

{my y:z: , y

E

R(O)}

where each myYx is the simple support function de­
fined by

{ my<-+x(O(y))
myyx(S)

where

f IS

a mapping from

X

JR+ to [0, 1] s.t.

1. f is non-increasing in its second argument,
a� f3 implies f(X,a):::; f(obs,{3);
2. f(obs, a )
3.

f

f(obs, a )

=

1 if and only if

a=

i.e.,

decay function.

modelling decreasing beliefs over

used in [Dean and Kanazawa, 1989].

have first been

The intuitive reading of the mass assignment

= +oo

is

>.tv))

=

:

by passage to the limit we write

1 and

therefore the property V

strongly persistent:

= v

as soon as it is true some­

where in space, it is true everywhere in E.

How J..( v ) is determined depends on the variable V and
the value

v

involved. It may be determined by expe­

relevant persistence

of

V = v from y to x (which may sometimes be under­

stood as the probability of

continuous persistence from

x to y - this will be discussed later), according to
the formula above, is exp(-

d l (V = v)

t(�)) ) .

In particular, if

is the "half persistence" of V =

tence is equal to

Decay functions for

time

J..( v)

exp(-

v,

i.e., the

dfstance for which the probability of "relevant" persis­

0;4

-+a-Hoo 0

will be called a

•

to hold, the probability of the

f(O(y), d(x, y))

(25 \ 0)

granularity of space is coarse enough);

rience. Considering a point x where V = v is known

= f(O(y), d(x, y))
= 1-

Clearly,

�, then

we have J..( v) =

t·:f).

Now, when V is not a singleton, the persistence decay

function of

V

will be taken to be the persistence func­

tion of the most weakly persistent element of

v,

The critical point is the reference to

persis­

J..(V) = minvEV J..(v).

my<-+"

is the following: the fact that O(y) is observed at y

supports the belief that O(y) holds at x as well, to a
4 as noticed by a referee, there are intuitive cases where
this condition could be weakened.

relevant

tence rather than with simple persistence.

i.e.,

Assume

that we try to build an approximately valid weather
map and that the property rain

=

true observed at

point x. Clearly, this property being known to have a

288

LANG & MULLER

UAI2001

�

significant spatial persistence, this piece of knowledge

explanation of this value

is a strong evidence to believe that it is also raining at

pieces of evidence that it is raining at y and not rain­

a very close pointy, such as, say,

ing at

d(:r, y) = 1 k m.

This

is not at all an evidence to believe that is raining at

x and are in conflict.

y and the
z both can be considered as informa­
the first one telling that it is raining at x

absence of rain at

may well be the case that it is raining at :r; but in this

and the second one that it is not, the reliability of the

z

d(x, z) = 8000km,

hence, the impact of

have a strong impact on

An analogy with information merging from multiple

x on
This does not mean
that the probability of raining at z is (almost) zero. It

z

where

z

is the following: the two

regarding rain is (almost ) zero.

z is ( almost certainly )
unrelated to the fact that it is raining at x, because, for

case, the fact that it is raining at

sources is worthwhile: the rain observed at
tion sources,

sources being function of the distance between them
and the focus point

In the absence of a reason to

x.

instance, the air masses and the pressure at these two

believe more one source than the other one, the prob­

points

ability that it is raining at

pact

do with the prior probability of p ersi st enc e: had this

( at the same time) are independent. The im­
/(rain, d(x, z)) =true of x on z regarding rain

can be interpreted as the probability that, knowing

prior been

that it is raining at

would still have been

x,

it is also raining at

z and

two points are in the same "raining region".

these

Hence

the terminology "relevant persistence", which may also
be interpreted as "continuous persistence" (i.e., persis­
tence along a continuous path) if we assume moreover
that a raining region is self-connected5.

0.25,

x is �.

This has nothing to

the probability that it is raining at

x

�·

w as the focus point. w being very far
y and z, their impact is almost zero and the prob­
ability of rain at w is (extremely close to) the prior

Consider now
from

probability of rain, i.e.,
and comes from

�·

ignorance

This value of

� is a

prior

rather than with conflict.

This is where the difference between pure probability

Therefore, probability cannot distinguish from what

and be li e f

happens at

ability

tween

functions (recall that they generalize prob­
theory ) is the most significant: in a pure prob­

abilistic framework, this impact degree, or probability

x and

at

w,

i.e., it cannot distinguish be­

confiictual information and lack of information.

Belief functions, on the other hand, would do this dis­

of relevant persistence, cannot be distinguished from

tinction: while the belief of raining at

a usual probability degree. If we like to express prob­

been close to

abilities of persistence in a pure probabilistic frame­

the belief of not raining at

[0, 1] s.t.
Prob(H olds(x, rain)IH olds(y, r ain ))= 9rain(d(:c, y)).
This mapping g is different from f. More precisely,

to

work, we need a mapping

9rain

:

E2

--+

g 2: f holds, and g and f are closer and closer to f

d(x, y) is

as

d(x,y) becomes
large ( with respect to the persistence degree of rain),
the impact g tends to 0 while g tends to the prior prob­
ability of raining at x. From this we draw the follow­
ing conclusion: a pure probabilistic modelling of spatial
persistence needs not only some knowledge about how
properties persist over space but also a prior probabil­
ity that the property holds at each point of space; the
latter, which may be hard to obtain, is not needed with
the belief function modelling of persistence.

0.

�,

the belief of raining at
w,

x would have
w, as well as

would have been dose

Hence the second conclusion:

a pure probabilis­
tic modelling of spatial persistence does not allow for a
distinction between confiictual information and lack of
information, while the belief function modelling does.

smaller and smaller; when

3.3

Combination

Once each observation is translated into a simple sup­
port function

my<-+x,

the bel i ef about the value of the

variable V is computed by combining all mass assign­
ments my<-+x for

y

E

R(O).

A first way of combining them consists in applying
mere Dempster combination, i.e.,

The second drawback of a pure probabilistic modelling
of spatial persistence is the lack of distinguishability

between ignorance and conflict. Suppose ( without loss

of generality ) that the
persistence is

where

x is

both, and

�·

prior probability of

Consider the four points

very close to

w

(uniform )
x and y

is very far from

x.

z.

Suppose that it has

y

and that it is

The probability, as well

lief, that it is raining at

:r,

y, z

and half way between

been observed that it is raining at
not raining at

w, x,

as

are very close to

the be­

�.

The

5and, in a stronger way, by "linearly continuous per­
sistence" if we assume that a raining region is not only
self-connected but also convex.

If one wishes to keep explicitly track of the measure
of conflict then one may use unnormalized Dempster
combination instead. However, a naive use of Demp­
ster combination has a severe drawback. Consider the

== {x,y,z,w} where d(x,y) = 1;
d(x, w) = d(y, w) = 10; d(z, w ) = 10; d(x, z) =
d(y, z) = 19 and the observation function 0 concern­
ing rain: O(x) = O(y) = true; O(z) = false. The

following space E

focus point is w.We take an exponential decay func­
tion with a uniform >.

mx<-tw, my'-+w

= 30.

The

mass

assignments

and mx<-+w are the following:

UA12001

LANG & MULLER

X � E and for any xE E \X,
J.l(xiX) = min( 1, J1. (X U {x})- J-t.(X)) where J.l.( 0) = 0 ,
J.l(X) = 1 if lXI = 1 and for any X of cardinality
n � 2,
.
-�
2
J.l(X) = 2- n L{y,z}�X,y;tz e ). where ). IS a pos-

X

•---- • .lP..
w
w
z
\·------- - ---------+-------------.:')
10
?
y

itive real number.

mx<-+w ({true})
mx<-+w ( {true, false})

�

= exp(- ) �

m<-+w

=

�

0.5

= 1- exp(- )

my<-tw({true})
mx<-+w ({true, false})
mz<-+w({false})
mx<-+w ({true, false})
The combination

mx<-+w

�
�
�
�
Ef)

�

In particular we have J.l.(xl0) = 1 and Ji.(xi{Y}) = 1d(.r,y)
.
e- -----r- . Takmg ). = 10, on the exampie of figure 1 we
have J.l.({yl{x}}) � 0.095 and J-t. ( z l {x} ) � 0.85 .

0.5

Now,

0.5
0.5
0.5
0.5

R( 0)

�

�

1. sort the points in

x,

and at

y

x

and y

2.

by increasing order of

i.e., let

mx<-tw

=

1 to n do
Yi I{Yl , Yi-d ) ;
J.l(
- J.li f_1et m� · m�(o(ill = 1- ( 1- f(O(i), d(x, y;))�·
,
mHS)
= ( 1- f( O(i), d(x , y;))�'
for

i

f-

•

·I

are clearly not inde­

pendent, and thus the mass assignments

R(O)

Lo(x) be the ordered
(Yt,-··,Yn)} where R(O)
{Yt,···,Yn} and
d(x, yl) ::; ... ::; d(x, Yn);

being close to each other, the pieces of information
z

the focus point and

list

0.6
0.2
0. 2

Clearly, this is not what we expect, because
that it is raining at

x be

the points where a nontrivial observation has

been performed.

my'-+w Ef) mz<-t w

�

mass assignments

n

with respect to J1. is done by

the following algorithm. Let

the distance to

m<-+w ( {true})
m<-+w({false})
mx<-tw ({true, false})

and

. . .

should not be combined as if they were inde­

pendent. On the other hand, on the following figures,

mx<-tw, mx<-tw and mx'-+w are identical to those
above but x is no longer close to y, the above result
m<-t w = mx<-tw $my<-tw EBmz<-tw is intuitively correct.
where

To remedy this problem, we introduce a

factor

the aggregation of the

my'-+x, y E R( 0) \ {x},

yields

my<-tw

289

discounting

when combining mass assignments.

The dis­

count grows with the dependence between the sources,
i.e., with the proximity between the points where ob­
servations have been made.

=

ffiio::l ..n mi

This way of combining by first reranking and then us­
ing interaction factors is reminiscent of the aggrega­
tion operator known in multi-criteria decision theory
called

Choquet integral.

Formal analogies will not be

discussed further here.
In practice, it is often the case that each pointwise

We use here a method inspired from multi-criteria
decision making (where positive or negative interac­
tions between criteria have to be taken into account
when aggregating scores associated to the different cri­

Assuming that E is fin ite, for X � E and
xE E \X, we introduce a conditional importance de­
gree J-t.(xiX) E [0, 1] expressing the importance of the
knowledge gathered at point x once the points in X
have been taken into account. The quantity 1-J-t.(xiX)
is therefore a discount due to the dependence with the
teria) .

information at x and the information already gath­
ered.Intuitively, it is desirable that "the further

x from

X", the higher J.l(xiX). When xis sufficiently far from
X, there is no discount and J.l.(xiX) is taken to be 1.
Several possible choices are possible for

3. compute mx

J.l·

In the im­

plementation we chose the following function6: for any

6lts intuitive justification, which is based on

an

anal-

observation is precise, i.e.,
y; E

R(O).

O (yi )

=

{v;}

for each

In this case, the above combination op­

eration can be written in a much simpler way: the
mass of a value

{v}

can be expressed as follows, given

Vm;, ::J!j, Vj E V/ a:; =
m;({vj}) f. 0; Vi E [l..p],P; = {k E (l..n]/mk({v;}) f.
0}; Vi E [l..p],P; = {k E [l..n]/mk({v;}) = 0}. In
a few preliminary notations :

that case, it is easy to show that combination without
discount yields:

m(v;)

=

( 1- (0kEP, (1- a:�c))) * (0kEP, ( 1- a:�c )) .

Whereas combination with discount yields:

m(v;) = ( 1- (0kEP; ( 1- o:�c )�k )) * (0xEP, ( 1- a�c)�k ).
ogy with fuzzy measures and interaction indexes in multi­
criteria decision making, would be rather long and compli­
cated to explain without introducing further several defi­
nitions. We omit it because this is not the main scope of
the paper.

LANG & MULLER

290

Experiments

4

UAI2001

reveals no information, and a purple one would reveal
conflicting values.

Since color is not possible in this

We recall that what we focus on is the plausible ex­

article we will show figures in shades of gray.

trapolation of information: given a set of observations

observation point will be in black or white, and the

Each

on E, what is the likelihood of the truth of a formula

shade of gray for each interpolation will be a difference

on a point outside of the set of observations ? For the

between the combined mass of the two values (normal­

experiments, we used a binary value domain, namely

S

=

{white, black}.

likely a point is to have a value close to a black obser­

We compute the overall mass assignment for each lo­
cation

x

in the space E, by combining the mass as­

signment induced by every point y in the observation
set

R(O).

We have two courses of action from here.

Either we make a plain Dempster combination of all
the simple support functions
ther we make

a

mx

=

EByER(O) my<-+x,

ei­

correction based on a Choquet integral

applied to the exponents (as explained at the end of
Section

3.3),

ized). In order to see the observations points, the more

to lower the influence of close concurring

observations (for which the independance hypothesis
cannot hold). After this combination is performed, we
can decide whether to normalize the results (by as­
signing the mass of the null set to the other possible
sets) or to keep a non-zero mass for contradictory in­
formation. Keeping un-normalized resulting mass as­
signments helps visualizing the conflictual regions.

vation, the more white it is, and conversely. A middle
gray will indicate similar levels of both values.

For

instance, figure 4 shows the result for three close con­
curring "black" observations next to a single "white"
observation. In one case the information is corrected to
take into account the fact that close points are related
and do not express independent sources. In the other
one we have made a plain Dempster combination. We
can see that the three black points combined have an
influence similar to a single point.

In the limit case

where the three points are exactly identical we would
have exactly the same result as with only one point
(illustrating this would not be very spectacular). Fig­
ure 2 nonetheless shows different levels of interpolation
varying with the distance between concurring observa­
tions, with or without the Choquet-like correction.

We chose the following experimental framework: we
consider a space of pixels E, ordered along two axes,
and for which a distance relation is the Euclidean dis­
tance. The distance unit is then one and the factors

>..(white), >..(black)

are uniformly fixed at

took the same value of

3 ()

3,

and we

used for the coefficient of

interaction between observations

7.

The best way to

Figure

2:

Corrected

normalized

(left)

and

non­

corrected normalized (right) interpolation, with vary­
ing distances between observations with identical val­
ues

Figure

1:

Corrected(left) and non-corrected (right) in­

terpolation, with conflicting values and three concur­

5

decision-theoretic issues

ring, close observations
illustrate our results would be to assign a color to each
pixel, assigning a red intensity to one value, a blue in­
tensity to the other (and eventually a green intensity
to the belief in the empty set, if one want to keep track
of the level of contradiction). This way a black pixel

7This settings proved empirically to give visual results
that illustrates well the principled we use here. Obviously,
these factors should be tailored for specific spatial proper­
ties with respect to the scale of the actual observed space.
Moreover, other distances could be considered where the
interaction and persistence of relevance would take into
account other factors.

Information·theoretic and

Plausible information can be very useful in the context
of decision-making, when decisions have to be made on
the basis of uncertain information.
5.1

Information intensity maps

Our framework can be used to measure the variations
of the quantity of information over space.

In order

to do so, we may compute a probability distribution
on S at each point of E, using for instance the pig­

nistic transform, and the information level at each

point can then be computed using usual information-

UAI2001

LANG & MULLER

theoretic measures such as entropy8.
build a map where each point

x

Hence we can

as a set of observations on a larger space, what lo­

is associated to the

cations are the most informative when one want to

entropy of its final probability distribution. Entropy
increases as information decreases; in other words, the
quantity 1of

p.

H (p)

291

measure the quantity of information

Minimal entropy is obtained at points at which

at a complete observation has been made.

Maximal

entropy is obtained at points associated with a uni­
form probability distribution (if any). Note that this
uniform probability distribution may come either from
conflictual observations or from a lack of information:
as explained in Section 3.2, once the combined mass
assignment has been transformed into a probability
distribution, there is no longer a way to distinguish

switch to a finer-grained representation?
An information intensity map already gives some hints
about where it should be interesting points to make
new measures: measures seem more useful in regions
in which the information quantity is low.

However,

picking the point of E with the lowest amount of in­
formation is not sufficient in general.

Especially, it

is relevant to make a difference between points where
nothing is known because the observations are too far,
and the ones where there is conflict between observa­
tions at points nearby.

conflict from lack of knowledge.

If one is interested in choosing

This is true independently of the number of values

classical heuristics is the maximum expected entropy
loss9. This, however, works well if (1) only one more

we consider for a spatial fluent, but to illustrate the
process, we show on figure 3 the level of information
using as before the 2-valued set S

=

{white, black}.

In this case, the quantity of information 1 -

H(p)

- H Information is minimal
when p(white) = p(black) = � and maximal when
p(white) = 1,p(black) = 0 or p(white) = O,p(black) =
1. The shade of gray is proportional to IP(white)-� 1grows with IP( white)

This way, a black point corresponds to a low amount
of information and a white point to a high one. Again
we show the results both with and without correction.

one

measurement, a

measurement has to be made; (2) the measurements
have uniform costs; (3) the utility of a gain of infor­
mation does not depend on the value observed nor
on the location of the measurement.

The more gen­

eral problem of determining an optimal measurement
policy over a given number of steps can be cast in
' the framework of Partially Observable Markov De­
cision Processes.

This requires the specification not

only of measurement costs but also a utility function
which grows with the global amount of information
(and which possibly takes account of the relative im­
portance of some values or of some regions). This point
is left for further research, and should be positioned to
the recent work of [D. Kortenkamp and Murphy, 1997]
extending the idea of occupancy grids with the use of
MDP.
Once a series of measurements has been done, one may
decide either to stop the measurements, or, if the quan­
tity information is considered high enough (relatively
to the expected cost of new measurements), we can
then easily compute a "plausible map" from the re­

Figure

3:

Corrected

normalized

(left)

and

non­

corrected normalized (right) level of information

sult of the combination step, by assigning each point
of the space a value with the highest probability, in
order to represent the most likely distribution of the
spatial property considered. Figure

4

shows the result

on a sample observation set, with two different levels
5.2

Decision-theoretic

map

construction

We are now interested in the following problem: given
a set of observations 0, where (and what) is it worth­
while to measure next? This problem, already consid­
ered in the field of robotics (where it has received a
pure probabilistic treatment), is relevant not only for
exploring geographical data but also for the question of
granularity dependent representations. Indeed, given a
coarse-grain representation of spatial information seen

8We recall that the entropy of a probability distribution
L:.ES -p(s) lnp( s )

p over a finite setS is defined as H(p)

=

of gray for each value. One can again observe that the
correction decreases the likeliness of a value near con­
curring measures.

In practise, it would probably be

better to decide of a threshold under which the belief
in a value is irrelevant before pignistic transformation.
If we know indeed that the belief in value 1 is 0.05,
and belief in value 2 is 0.04, (thus the belief in the
set {1,2} is 0.91), we don't want to assume it is more
likely that value 1 holds and thus we would like the
map to remain undetermined at this point.

9This heuristics is widely used in model-based diagnosis
when choosing the next test to perform.

LANG & MULLER

29 2

UAI 2001

