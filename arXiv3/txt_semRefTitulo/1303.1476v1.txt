

Problems o f probabilistic in ference and decision
making under uncertainty commonly involve
continuous random variables. O ften these are
discretized to a few points, to simpli fy
assessments and computations. An alternative
approximation is to fit analytically tractable
continuous probability distributions. This
approach has potential simplicity and accuracy
advantages, especially i f variables can be
trans formed first. This paper shows how a
minimum relative entropy criterion can drive
both trans formation and fitting, illustrating with
a power and logarithm family o f trans formations
and mixtures of Gaussian (normal) distributions,
which allow use o f efficient influence diagram
methods. The fitting procedure in this case is the
well-known EM algorithm. The selection of the
number o f components in a fitted mixture
distribution is automated with an objective that
trades o f f accuracy and computational cost.
1 INTRODUCTION

Real probabilistic in ference and decision analysis
problems o ften involve continuous ran dom variables
( RVs) and decisions. Un fortunately, exact solution
methods are not available for such problems in general,
though several methods are available in the all-discrete
case (Shachter 1986, Lauritzen and Spiegelhalter 1988,
Pearl 1988, Jensen et al. 1990). Several approximate
approaches are available, each with some disadvantages:
The discretization approach quantizes the continuous
distributions and uses a discrete solution method.
Using a large number of points, as in numerical inte­
gration, is computationally burdensome, so practitioners
o ften use only two or three points per variable. But
representing continuous distributions accurately with a
few points is tricky i f the tails o f the distributions are
significant ( Miller and Rice 1983, Kee fer 1992). Also,
•

discrete representations can hide simple continuous
relationships, such as the linear conditional means in a
multivariate Gaussian distribution .
The Monte Carlo approach uses stochastic simulation
o f the RVs (e.g., Shachter and Peot 1990). This
sometimes requires a computationally burdensome
number o f simulations, even with techniques to speed
convergence.
The mome nt approach summarizes continuous
distributions by their first few moments, either using
these directly (e.g., Howard 1971) or fitting discrete
distributions fitted to the moments (Smith 1993). These
methods can be inaccurate for irregular or multimodal
distributions, and discretization has the disadvantages
mentioned above.
The parametric approach fits analytically tractable
parametric distributions to the continuous RVs and an
analytically tractable function to the utility function.
An example is to fit a multivariate Gaussian (normal)
distribution to the variables and a quadratic or concave
exponential function o f a quadratic to the utility
function. The resulting model is easy to describe, as­
sess, and solve in the notation o f the Gaussian influence
diagram, discussed below. However, the assumptions
of the parametric approach o ften are overly restrictive.
This paper first outlines a parametric approach using
mixtures o f Gaussian distributions, which is much less
restrictive than that o f the Gaussian influence diagram. A
reduction procedure generalizing Shachter and Kenley's
Gaussian in fluence diagram procedure is propose d. For
Bayesian updating, Lauritzen's message-passing
procedure for conditional Gaussian models (Lauritzen
1992) should also be considered; it gives posterior
probabilities for discrete variables and posterior means
and variances for continuous variables. The main part o f
this paper gives procedures for trans forming an arbitrary
univariate distribution and fitting a mixture distribution to
it, using the objective o f minimizing relative entropy
(Shore 1986). ( Relative entropy is also known as
•

•

•

184

Poland and Shachter

Kullback-Leibler distance, directed divergence, cross­
entropy, etc.) This objective results in the distribution of
a desired form which maximizes the likelihood of a
random draw from the input distribution. Multivariate
generalizations of these procedures are mentioned.

parents (if any). An optimal decision policy maximizes
expected utility, using the function in the utility node. A
decision node may be thought of as a deterministic chance
node for which the deterministic function, the decision
policy, is initially unspecified.

The next seCtion reviews influence diagrams and the
Gaussian influence diagram case .and generalizes this to a
mixture-of-Gaussians influence diagram. Section 3
describes transformations to a univariate distribution to
allow a more accurate Gaussian fit or a simpler mixture­
of-Gaussians fit. Section 4 describes fitting a mixture to a
(perhaps transformed) distribution, by the EM algorithm.
Section 5 gives a procedure to automate selection of the
number of components in a mixture, based on the EM
algorithm. Finally, Section 6 describes some computa­
tional experience and gives directions for further research.

The probabilistic inference problem-finding the
probability distribution of some variables given others­
can be solved by an ID procedure based on node removal
and arc reversal operations that leave the joint distribution
of the variables of interest unchanged (Shachter 1988,
Shachter and Kenley 1989). Bayesian updating, or
finding the posterior distribution of some variables when
others have been observed, is then reduced to probabilistic
inference followed by "instantiation" of the conditioning
variables with the observations. After Bayesian updating,
an ID with decisions can be solved for the optimal
decision policies by the "reduction" procedure of Figure
1, based on similar operations and a decision optimization
operation (Shachter 1986).

2 INFERENCE AND DECISION MAKING
WITH MIXTURES OF GAUSSIANS: AN
INFLUENCE DIAGRAM APPROACH
2.1

INFLUENCE DIAGRAMS

Influence diagrams (IDs) are useful for structuring and
solving probabilistic inference problems and decision
problems. with a single decision maker (Howard and
Matheson 1981). IDs are especially effective for
representing finite mixture RVs, as will be seen in the
next subsection. An ID is a directed acyclic graph in
which the nodes represent the variables of the problem
and the arcs represent probabilistic dependence or
information. Oval nodes represent RVs, rectangular
nodes represent decision variables, and a diamond (or
rounded-rectangle or hexagonal) node represents the
utility function expressing the decision maker's
preferences. Sometimes the utility function can be
expressed as a function of a single variable, called value,
in which case often the utility node is omitted from the
diagram and the value node is given the diamond shape.

These inference and solution procedures are remarkably
simple and efficient in the case of the Gaussian influence
diagram (GID). A GID is an ID in which the joint
probability distribution of all variables other than value or
utility is multivariate Gaussian (so the decision domains
are continuous and unbounded), and the utility function (if
any) is a quadratic function of the other variables (the
"value" function), or a concave exponential function of
this (Shachter and Kenley 1989, Kenley 1986). The GID
can be used to represent and solve well-known problems
such as Kalman filters, linear-quadratic-Gaussian control
problems, and linear regression problems. The simplicity
of GID specification and operations follows largely from
the following properties of the multivariate Gaussian
distribution: ( 1) the conditional distribution of any
variable given any others is Gaussian, (2) the conditional
mean is a linear function of the conditioning variables,
and (3) the conditional variance is constant (independent
of the values of the conditioning variables).

To this standard notation, we add the placement of a
number on the perimeter of any node to indicate a discrete
variable with this number of values, or a "#" symbol on
the perimeter to leave the number of values unspecified.
The absence of a number or"#", as usual, does not restrict
the variable to be continuous.
An arc from node X to node Y makes X a parent of Y and
Y a child of X. There are two types of arcs. A
conditioning arc, into a chance (RV) node, indicates that
the child RV is conditioned on the parent variable. An
informational arc, into a decision node, indicates that the
value of the parent will be known at the time the decision
is made. If a chance node has no parents, its distribution
is marginal. A deterministic chance node, sometimes
represented by a double oval, represents a variable for
which the conditional distributions are all deterministic
(though the marginal distribution might not be).
For a decision node, a decision policy specifies an
alternative for each combination of outcomes of its

Figure 1: Reduction Procedure for Solving an ID

Mixtures of Gaussians and Minimum Relative Entropy Techniques

2.2

MIXTURE

DISTRIBUTIONS

MIXTURE-OF-GAUSSIANS

AND

Proba­
Outcome bi1i1)!.
1
P1

THE

INFLUENCE

DIAGRAM

A finite mixture RV can be defined in terms of
"component" and "selector" RVs as shown in Figure 2.
"Finite" means the number of components is finite. A
finite mixture RV is equal to one of its components; the
selector variable selects which one but is unobserved.
Figure 2b shows the selector, but not the components,
separately from the mixture variable itself; an alternative
partially expanded representation would show the
components but not the selector separately.
The

probability distributions of the components are assumed
to be of the same form, for example, Gaussian with mean
and variance varying by component.
Though the
distributions of the selector and components are known,
their outcomes are unknown, so the distribution of the
mixture is an expectation, over the selector S, of the
distributions of the components X1 to X :

�
�

m

Pm

continuous for
each outcome
ofS

(a)Unexpanded

(b)Partially expanded

ProbaQulcQm& 12i1itt
1
P1
m

185

discrete

Pm

m

F x(x)= EFx8(x)= p1 Fx (x)+ ..
I

s

fx(x)= E f x (x)= p1 f x (x)+
s
I
S

..

.

.

+ Pm Fxm (x),

+ Pm fxm(x) .

The component densities above are allowed to be Dirac
delta functions, so that X can have probability mass at any
point. Thus X can be mixed continuous and discrete, or
as a special case, all-discrete.
A mixture RV should be distinguished from a weighted
sum of the component RVs:

X -:F. Pl X 1

+ . ..

+

P m X m.

In the Gaussian-component case, for example, the latter is
another Gaussian. One interpretation of a mixture
variable is as the unknown, true model of a system or
outcome of a process, where the components are the
possible models/outcomes and the selector gives the
probability of each being con·ect. Another interpretation

is as a discrete variable with each outcome "blurred" by a
random continuous perturbation. However, the compo­
nent and selectors can be purely artificial, without any
physical interpretation, and serve only to produce a
desired distribution.
A mixture-of-Gaussians (MoG ) RV or distribution is a

mixture RV or distribution for which all components are
Gaussian.
A MoG distribution generalizes both a
Gaussian distribution, which has only one component, and
a discrete distribution, which has only zero-variance
components. MoGs combine the simplicity of Gaussians
and the flexibility of discrete distributions.
We define a MoGID as an ID such that after expanding
any mixture-variable nodes as in Figure 2c:

1. There are no arcs from continuous to discrete nodes.

2. Dropping the discrete nodes (and any arcs from them)
leaves a GID.
The MoGID could be represented as one parentless,
combined discrete node with children in a GID.

continuous
(c)Fully expanded

Figure 2: Influence Diagram Representations of a Mixture
Variable. The selector "selects" a single (typically
continuous) component for the mixture variable, but is
unobserved, resulting in a convex combination of
continuous distributions.

Alternatively, conditional dependence among MoGs in a
MoGID could be represented by only arcs between
selector nodes and other arcs between component nodes­
though a simpler representation may be possible, as in the
example below.
As an example, consider the oil wildcatter problem, a
popular tutorial example in decision analysis posed by
Raiffa (Raiffa 1968, p. xx):
An oil wildcatter must decide whether or not to
drill at a given site before his option expires. He is

uncertain about many things: the cost of drilling,
the extent of the oil or gas deposits at the site, the
cost of raising the oil, and so forth. He has
available the objective records of similar and not­
quite-so-similar drillings in this same basin, and he

has discussed the peculiar features of this particular
deal with his geologist, his geophysicist, and his
land agent. He can gain further relevant informa­
tion (but still not perfect information) about the
underlying geophysical structure at this site by con­
ducting seismic soundings. This information, how-

186

Poland and Shachter

transformation. The entropy of a continuous RV X with
density fxO is
H(X)=- E[ln fx(X)] =-

J 00fx(x) In fX(x) dx .
oo
-

(1)

The relative entropy between two continuous RVs X and
Y, with densities fxO and fyO respectively, is

Gaus­

sian

D(X,Y)=E{ ln[fx(X) I fy(X)]

}.

(2)

Relative entropy is nonnegative, and zero only when the
two densities are equal. Note also that
D(X,Y)# D(Y,X)
and that the support set of the second distribution must
contain that of the first for the relative entropy to be fmite.
For convenience later, let
(3)

Do(X,Y)= -E[ln fy(X)]
so that relative entropy is a difference:

sian

Figure 3: A MoGID Formulation of the Oil Wildcatter
Problem. Simple artificial MoGs are fitted to assessed
distributions for Oil Volume and Cost of Drilling.
ever, is quite costly, and his problem is to decide
whether or not to collect this information before he
makes his final decision: to drill or not to drill.
Figure 3 shows a possible MoGID formulation of this
problem, which recognizes that the Oil Volume RV is
mixed continuous (for positive values) and discrete (at
zero). The selector and components for both Oil Volume
and Cost of Drilling are artificial, having been introduced
only to allow tractable and reasonably faithful
representations of the variables' distributions.
As long as arcs from continuous to discrete variables can
be avoided, a MoGID can be solved by the reduction
procedure of Figure 1, using discrete or Gaussian
operations as appropriate and reducing continuous
variables before discrete ones (Poland 1993). For the oil
wildcatter problem, for example, the procedure removes
nodes in a (non-unique) order such as: Cost Of Drilling,
CJ, C2, Cost Case, Oil Volume, VJ, V2, V3, Oil Case,
Seismic Structure (first reversing the arc to Test Results),
Drill?, Test Results, and Test?.
3 TRANSFORMATIONS TOWARD
DESIRED DISTRIBUTION

(4)

D(X,Y)= Do(X,Y)- H(X) .

A

This section develops a procedure for transforming a
univariate distribution to make it as close as possible, in a
relative entropy sense, to a desired form, with emphasis
on the Gaussian; the following two sections describe how
to fit a MoG (or Gaussian) to the result of such a

Consider a family of transformations tA.(X) parametrized
by the vector A, and consider an RV Y�) with a distribu­
tion of a desired form parametrized by the vector 8. The
transformation from the given family that minimizes
relative entropy with a distribution of the desired form
minimizes D[tA.(X),Y(9)] over both A and 9.
It is convenient to minimize over 9 frrst, holding A fixed.
In particular, suppose the desired form is Gaussian. It can
be shown that of all Gaussians, a continuous distribution
has minimum relative entropy with the "moment­
matching" Gaussian: the Gaussian with the same mean
and variance.
Therefore the transformation that
minimizes the relative entropy with a Gaussian can be
found by minimizing D[tA.(X),Y] over A only, where Y
N( E[tA.(X)],Var[tA.(X)] ). This relative entropy can be
expanded using expressions for the relative entropy of a
variable with its moment-matching Gaussian and for the
relative entropy of a differentiable monotonic
transformation in terms of the untransformed variable.
The result is
�

D[tA.(X),Y]= 1/2 { 1 + In (21t) + In Var[tA.(X)]
- H(X)- E[ I n ltA.'(X)I ]
where Y � N( E[tA.(X)],Var[tA.(X)] ) .

}
(5)

For example, consider the Box-Cox parametric family of
"power/logarithm" transformations for positive variables
(Box and Cox 1964):

{

xP-1
p#O
tp(x)= pln x
p=O

for x > 0.

(6)

(For p# 0, the simpler transformation xP could have been
used, but the scaled version above provides continuity at
0.) If X is a nonnegative RV, the distribution of tp(X) can
be found by a change of variable:

Mixtures of Gaussians and Minimum Relative Entropy Techniques

Y =tp(X) �
fy(Y)=
fx[(py+1)11PJ (py+1)llp-I py+1 > 0, p ;t0
. (7)
p =0
fx(eY) eY
py+1 �0
0

{

Using tp(X) in (5) gives
D[tp(X),Y]=1/2 { 1 +In (21t) +In Var[tp(X)] } - H(X)
-(p-1) E[ln X] where Y- N(E[tp(X)],Var[tp(X)]) (8)
so the power/logarithm trans formation that has minimum
relative entropy with a Gaussian can be found by a one­
dimensional search (Hernandez and Johnson 1980):
p*=arg min D[tp(X),Y]
p
= argpmin {1/2ln Var[tp(X)] -(p-1) E[ln X] }. (9)
I f p* is close to a small integer or power that has a
physical interpretation, it might be reasonable to round it
to that number (and i f p ;t 0, to substitute the simpler
trans formation xP at this time) . Then the distribution o f
tp*(x) can be found by (7).
The Box-Cox trans formation family applies only to
nonnegative RVs, and most naturally to ones with support
on [O,oo) . Others can be trans formed in advance to make
them nonnegative. A variable on [a,b) can be trans formed
to one on [O,oo) by the "scaled odds" trans formation
(x- a)/(b- x). For a variable on (-oo,oo), it might be
possible to find a practical upper and/or lower bound and
use an a f fine or scaled odds trans formation to make the
support set [O,oo). This suggests the following trans for­
mation procedure for any RV: (i) trans form it in advance
to make it nonnegative, (ii) find p* from (9) and
optionally round it, and (iii) find the trans formed
distribution with (7) . Figure 4 shows the result o f this
procedure for exponential and uni form distributions. For
the latter, first the support set is made [O,oo) by a scaled
odds trans formation; then the optimal power/logarithm
trans formation is found to be logarithmic. For further
examples, see Hernandez and Johnson (1980).
Separate univariate trans formations o f dependent RVs
may be undesirable i f conditional dependence
relationships among continuous variables have desired
forms be fore trans formation- for example, the linear
conditional means and constant conditional variances
required in the GID. One approach to this problem is to
use the multivariate form o f the power/logarithm
trans formation toward a Gaussian (Hernandez and
Johnson 1980), which compromises between the objective
o f Gaussian marginal distributions and the objective o f
linear conditional means and constant conditional
variances a fter trans formation.
4 FITTING MIXTURE DISTRIBUTIONS
WITH THE EM ALGORITHM

The EM (expectation, maximization) "algorithm" is an
approach, rather than a single algorithm, for maximum-

D
e
n
s
i
t
y

187

Transformed
...._v_ ariable
(p* ""0.27)

(a) Exponential
D
e
n
s

Transfonned
._variable
(p*=0)

Moment­
matching
Gaussian

�

t
y

(b) Uni form with scaled odds pre-trans formation

Figure 4: Some Densities a fter Power/Logarithm
Trans formations Toward a Gaussian. The results are
close to the moment-matching (same mean and variance)
Gaussians shown in gray.
likelihood estimation with incomplete data. Dempster et
at. (1977) wrote the definitive paper on the approach, and
Redner and Walker (1984) specialized it in detail to
estimation o f the parameters o f mixture distributions,
especially MoGs (with a given number o f components).
The usual input data for the EM algorithm is a sample o f
exchangeable observations . Since the purpose here is to
fit a mixture distribution to a given p r obability
distribution, rather than to given observations, a
generalization o f the EM algorithm is given which uses
either kind o f input data. Though only the univariate case
is considered here, a multivariate generalization is
straight forward and is interpreted in the final section .
Let X be an RV with a given distribution, and let Y(8) be
a mixture RV o f a desired form (such as MoG), with m
components and parameter vector 9= (Pt. .. ., Pm· 81, ...,
8m). For example, in a MoG,8i = (J.li> Oi2), the mean and
variance o f component i. Then the density o f Y(9) is
fy(y I 9)=PI ft(y l8t) + ... + Pm fm(y I 9m)
where f1( I 81), ..., fm ( I 9m) are the component density
functions. The objective considered here is to find the
density o f Y (not necessarily unique)-or equivalently,
the value o f 8-that minimizes the relative entropy be­
tween X and Y over the feasible region e for 9:
·

·

188

Poland and Shachter

8* = arg min D[X,Y(8)]
9e 9

= arg min Do[X,Y(8)]
9e 9

= arg max E[ln fy(X 18)] .

(10)

9e 9

In the usual EM algorithm, the probability distribution o f
X is taken to be an empirical ( frequency) distribution o f a
sample o f exchangeable observations x (x1 , ..., xn) o f
draws Y= (Yl, ..., Y n) from Y. Thus X= x1 or ... or xn
and D[X,Y(8)] is unde fined, since X is discrete
while Y(8) is continuous. However, the definition o f
relative entropy for continuous variables, (2), can be
generalized to apply to this discrete-with-continuous case
by using (3) and (4) with X allowed to be discrete. With
this generalized definition, since each o f the n values o f X
has probability ( frequency) 1/n,
=

8*

=

LJ=lIn fy(xj I 8)
arg maxIn f1"=1 fy(xj I 8)
arg max 1/n
9e 9

=

9e 9

=

arg maxIn fy(x I 8)
9e 9

= arg max fy(x I 8),
9e 9

which is the maximum-likelihood estimate o f 8. There­
fore the objective o f minimizing the "relative entropy"
between X and Y (extended to the discrete-with-contin­
uous case) generalizes the objective o f the EM algorithm:
to maximize the likelihood o f the observations x o f draws
from Y's distribution (Titterington et al. 1985, p. 115).
Since (1 0) cannot be solved in closed form, the EM
algorithm iterates with a modi fication o f it that replaces
In fy(X I 8) by an expectation over the unknown selector
associated with Y, �(Y), given the data X and the current
parameter estimate 8:
8 rarg max
8e9
E E [In fy 'S(Y)(Y,S(Y) 18) I Y = X,f t]. (11)
X S(Y)
When X is allowed to be an arbitrary RV (continuous or
discrete), the EM algorithm has the same, attractive
convergence properties as it does for the empirical
distribution case, because the probability distribution o f X
can always be interpreted as a limiting case o f a frequency
distribution. (Convergence is discussed in Redner and
Walker (1984). Techniques such as Aitken acceleration
(Gerald and Wheatley 1984) are help ful to accelerate the
typically slow convergence near the solution.)
The EM algorithm leads to the assignments
P f (X I
(12)
, t = 1, ..., m
Pi rE i i
fy(X l it)
A

[

�)] .

D
e
n
s
t
y

���==���--�

0
�

n

o

(a) Three-Component Fit to Exponential
D
e
n
s
t
y
o --��----�a-

(b) Two and Five Component Fits

to

Uni form

Figure 5: Some MoG Fits by the EM Algorithm. MoGs
with few components can approximate very non-Gaussian
distributions.

(13)
These assignments become equalities when the first-order
necessary conditions for (10) are satisfied; in fact the EM
algorithm for the MoG was developed without conver­
gence theory from these conditions (Hasselblad 1966).
Figure 5 shows examples o f MoGs fitted to exponential
and uni form distributions (untrans formed). Even without
the help o f trans formations, densities far from Gaussian in
shape can be fitted well with mixtures o f a small number
o f components.
5 SELECTING THE MIXTURE SIZE

The EM algorithm provides the minimum-relative­
entropy fit o f a mixture distribution o f a given "size," or
number o f components. How should this size be selected?
I f the distribution being fitted is known to be a finite
mixture with components o f the assumed form, the goal is
to determine the true size. However, ifthe mixture serves
as an artificial approximation o f an arbitrary known

Mixtures of Gaussians and Minimum Relative Entropy Techniques

distribution, a reasonable goal is to select the size that
maximizes a utility function trading off preferences for
accuracy and computational cost. An infinite mixture on
(-oo,oo) can replicate any distribution (for example, via an
infinitude of zero-variance components along the desired
distribution). On the other hand, the computational
burden of calculations with mixtures rises with size at
least as fast as for discrete distributions, which mixtures
generalize.

Let

8(m)

be the parameter vector for a mixture of size m.

X

is the empirical
If the probability distribution of
distribution of exchangeable observations x = (XJ, ... , Xn)

of draws Y
(Y 1, ..., Y n) from a true mixture Y
Y(8(m)), the likelihood for size m can be related to
Do[X,Y(9(m))]:
= - Do[X,Y(9(m))]
E[ln fy(X I 9(m))l
Do[X,Y(8(m))J
1/n LJ=lln fy(xj f9(m))
Do[X,Y(8(m))]
1/n In fy(x I fl(m))
exp{ -n Do[X,Y(9(m))]}.
fy(x l8(m))
(14)

=

=

=-

=-

=

If instead, an artificial mixture is to be fitted to a given
distribution for
the right-hand side of (14) is no longer
a true likelihood, but it can be interpreted as an accuracy
measure if n-is interpreted as an "equivalent sample size,"
the size of an unspecified, hypothetical exchangeable
sample that underlies the distribution of
Dissociating n
from the distribution of X this way is useful even in the
empirical distribution case: as n increases, the EM
algorithm need not be burdened by an increasingly
detailed representation of the distribution of
In

X,

X.

X.

practice, it may be possible to base n on an actual number
of points, as when the distribution of
is found as a
smooth curve drawn through n assessed points on a
subjective cumulative distribution. For example, the
software behind the interface in Figure 6, described
below, uses a cubic spline through assessed cumulative

X

points entered at the top of the window.

A convenient utility function for the artificial-mixture
case is proportional to this accuracy measure and
inversely proportional to the mixture size raised to a

power k, as a computational cost measure.
(Some
statistical criteria for selecting model dimensionality, such
as Akaike's information criterion and cross-validation, are
similar in effect and could be substituted easily.) A
possible setting for the parameter is the total number of

k

selectors in the probabilistic inference or decision

problem; the cost measure is then a worst-case number of
combinations of selector outcomes used in probability
calculations if all mixtures have the same size. The

maximum-utility size for this utility function can be
estimated with the following heuristic: find the maximum­
likelihood parameters for size m,

algorithm, for m
fy(x

=

with the EM

1, 2, ... ; stop with size m when

l8{m+l))

(m+ 1)k

fl(m).

fy(x
<

l9(m))

mk

'

189

or by (14) and (4), in terms of a relative entropy decrease
and the combined parameter kin, when

D[X,Y(9(m))]-D[X,Y(8(m+l))l<

�

ln

; 1.

m

(15)

(To reduce the risk of a local but not global maximum,
one or more subsequent sizes could be checked too.) If
k = 0 the search would not terminate, unless modified to

incorporate prior probabilities for each size, resulting in a
maximum a posteriori estimate (a geometric prior
distribution on size is convenient; see Poland 1993). A
similar search heuristic that decrements size is given in
Cheeseman et al. (1988).
Figure 6 shows a user interface for fitting a MoG to an
assessed cumulative distribution, with or without an
automatic size search. It includes a "fast fit" option for a
two-component mixture. For speed, this replaces the EM
algorithm with the method of moments, which matches
the first moments of the mixture and the input distribution

(Cohen 1967). The results tend to diverge from those of
EM as the input distribution diverges from a true mixture
of two Gaussians. Unfortunately, the method of moments
becomes intractable for larger mixture sizes.

6 CONCLUDING REMARKS
In experience with some theoretical distributions and
empirical distributions from a semiconductor
manufacturing process, power/logarithm transformations
toward a Gaussian followed by EM fitting of a MoG with
the size heuristic described above resulted in accurate fits
requiring very few components. For a wide range of
values of kin in {15), the mixture size was typically
greater than one but smaller Ulan if the variable had not
been transformed. The mixture size was one for the
distributions of Figure 4.
It would be possible to nest inside the search over sizes a
minimum-relative-entropy transformation toward a
mixture of the current size. Whether this could be made
computationally attractive is a research question. Another

research direction is development of practical multivariate
versions of the transformation and EM procedures, or less
restrictive methods, to fit dependence relationships in an
analytically convenient way.
The multivariate EM
algorithm can be thought of as fitting multiple univariate
mixtures with a common selector; multiple,. dependent
selectors would provide more generality while allowing
efficient calculations in a MoGID. Also, research is
needed to generalize existing models that exploit the
tractability of Gaussians but suffer from their restrictions.
Simple MoGs might provide valuable flexibility at an
acceptable computational price.

Acknowledgments

This research was supported in part by a gift from Intel
Corporation and benefited from discussions with our
colleagues in the Engineering-Economic Systems
Department.

190

Poland and Shachter

eO

MoG from cumulotiue Pts. ====

( Help )

Uolues from min. to moH.:

l o 100 200 4oo 5oo

Corresponding cum. probs.

1 o .1 1/2 .9

1

I nterpolote

�

(0

I

to 1 ):

I

points between

eoch poir oboue for smoothing.
- --- Fit
-component miHture
(blonk=outo. selection).

D
-

-

D

If

2

--------------------------

components, use fost fit.

(

Speciol fitting options: Option Help

I

�

)

I

---------------------------------

PLOT:

®

Cumulotiue

0

Density.

esult (mou toke owhile to find):

< 77592<N( 178.23,68.897 "'2)>
.22408<N(388.47,52.1 06"'2)> >
•

+

1

0

500

Figure 6: A User Inter face for Fitting a MoG to an As­
sessed Distribution. The graph shows a 2-component
MoG fitted to a distribution passing through 5 input cum­
ulative points. The fit is slightly low at the middle point.
