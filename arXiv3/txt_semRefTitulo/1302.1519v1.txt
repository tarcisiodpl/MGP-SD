

new framework introduced by Kivinen and Warmuth [13].

This paper re-examines the problem of parameter esti­
mation in Bayesian networks with missing values and
hidden variables from the perspective of recent work in
on-line learning [13]. We provide a unified framework

rithms for the traditional task of batch parameter estimation for

Our analysis leads to the definition of a family of new algo­

for parameter estimation that encompasses both on-line
learning, where the model is continuously adapted to new
data cases as they arrive, and the more traditional batch
learning, where a pre-accumulated set of samples is used
in a one-time model selection process. In the batch case,

our framework encompasses both the gradient projection
algorithm [2, 3] and the EM algorithm [15] for Bayesian

networks. The framework also leads to new on-line and
batch parameter update schemes, including a parameter­
ized version of EM. We provide both empirical and the­
oretical results indicating that parameterized EM allows
faster convergence to the maximum likelihood parameters
than does standard EM.

Bayesian networks. We show, both theoretically and empiri­
cally, that the convergence properties of these new algorithms
can be significantly better than those of the current state-of­
the-art algorithms for this problem such as EM [15, 9]. We
then use the same framework to provide an initial foundation
for on-line parameter estimation for Bayesian networks.

In many real-world domains,

the data available for learning is

incomplete: some of the variables may be difficult or even im­

possible to observe. It is therefore important that our learning
algorithm be able to make effective use of partially specified
data cases.

This need is particularly crucial in the context

of on-line learning, where the primary source of data cases

lies within the queries presented to the network for infer­
ence. Clearly, there is little point in presenting to the network

1

queries about data cases where all variables have already been

Introduction

observed. Thus, we focus on the problem of learning from a

Over the past few years, there has been a growing interest in
the problem of learning Bayesian networks from data. The

data set consisting of data cases where some of the variables
may be permanently

or occasionally unobserved.

Manually engineering

In the presence of missing data, the problem of Bayesian

a large Bayesian network is a difficult and time-consuming

network learning becomes much more difficult [9]. In fact,

reasons behind this trend are clear.

process. Furthermore, it is never clear whether the network
designed by the expert is really the most appropriate model for
the domain. Finally, the world is not always static; we want
our model to adapt itself automatically to changing conditions.
So far, most of the work on learning Bayesian networks has
been devoted to the batch

learning task: we are given a train­

ing set consisting of some number of data cases, and our task

is to construct a Bayesian network which best models the
data. We can also consider the problem from the somewhat
different perspective of on-line

learning. In this task, we as­

even the relatively simple task of adapting the numerical pa­

rameters (conditional pro�ability table entries) for a given

network structure becomes nontrivial. Parameter estimation
is an important task

both because of the difficulty of elicit­

ing accurate numerical estimates from people, and because

parameter learning is part of the inner loop of more general
algorithms that also learn the structure (see [9] for a survey).

In this paper, we choose to focus on this aspect of the learning
task, deferring treatment of structure learning to future work.
(See [8] for some preliminary work on this problem.)

B, described
ii of numerical parameters, and a set D of one

sume that we have an existing model which we must adapt to

Formally, we are given some Bayesian network

some data D, thus forcing us to balance the desire to improve

by a vector

our model's fit to

or more data cases.

D

with the desire to maintain information

B,

We want to use

D

to construct a new

described by a new parameter vector

0.

As we

already included in the model. On-line learning is designed

network

to deal with situations where we want to fine-tune an existing

will see, this formalization applies equally well to the batch

model, either because the model was initially inaccurate or

because the environment has changed.

We derive a precise formulation for these tasks using the

learning task and the on-line learning task.
Two factors guide the choice of
fits

D,

0:

the extent to which it

and the fact that we don't want to move too far away

4

Bauer, Koller, and Singer

from our existing model e. In Section 2, we formalize this
intuition by optimizing a function F which incorporates both
the log-likelihood of D and the distance between lJ and 0. An
analysis of the optimal value for this function results in an
update rule, which tells us how 0 can be constructed from lJ.

The exact form of the function F depends on our choice of
the distance function used to compare ii and e, and on the
relative weight we give the distance and the log-likelihood.
Each choice leads to a different update rule. We perform the
analysis for various choices of distance measure , including
.C2-norm, relative entropy (KL-divergence), and x2 -distance
(a linear approximation to relative entropy).
In Section 3, we instantiate our update rules from Section 2 to
the batch learning context. In this case, we typically construct
a model by iterating through the same data set D multiple
times. Then, i!i is the model constructed in the previous it­
eration, and 0 is the result of another pass of adapting it to
the data. Surprisingly, it turns out that well-known parameter
estimation algorithms are special cases of our framework. For
example, our update rule for .C2-norm results in the gradient­
projection scheme of [3, 22, 2]. Our update rule for the x2
distance results in a family of update rules with varying learn­
ing rates 1]. This family, which we denote EM(1J), includes the
standard EM algorithm [6, 9] as the special case EM( I). From
the relative entropy distance, we derive an analogous family
of multiplicative update rules which, following [13, 11], we
call EG(1J).
We provide both theoretical and empirical evidence showing
that EM(17) can lead to much faster convergence than standard
EM while still using a very simple update rule. (In contrast to
more complex and expensive second order methods such as
those of [23]). In particular, we show in Section 4 that, while
1 is the largest value of r; for which convergence to a local
maximum is guaranteed, some valuer;* which is bigger than l
provides the optimal convergence rate. More precisely, for any
(local or global) maximum ofthe likelihood function, there is a
value 17* > 1 and a neighborhood around the local maximum,
such that EM(17*) provides the fastest convergence (of any
EM(1J) algorithm) to the maximum in that neighborhood.
While the optimal value 1]* cannot be computed, evidence
acquired when applying EM(TJ) algorithms to other learning
tasks shows that certain values of r; seem to work well for
most problems. In Section 5, we provide experiments with
EM(1J) for various values of 1]. These experiments indicate
that a value of r; :::::: 1.8 appears to work well, and certainly
much better than r; :::::: 1. In particular, we show that EM( 1.8)
often requires approximately half the number of iterations to
converge. Since each iteration of a parameter estimation al­
gorithm (whether EM or EM(ry)) involves a Bayesian network
computation for each instance in our data set, the computa­
tional savings resulting from a large reduction in the number
of iterations can be very significant.

Finally, we return in Section 6 to the problem of on-line
learning. In this case, our basic framework of Section 2 should

be interpreted as applying to a D consisting of a single new
sample, and the current modellJ is simply the one constructed
based on the samples seen so far. We examine the resulting
update rules, and discuss their applicability to the problem of
gradually adapting network parameters over the entire lifetime
of a Bayesian network. We conclude in Section 7 with some
discussion and open questions.
2

The framework

In this section, we present a basic framework which can be
used to interpret both the on-line learning and the more stan­
dard batch learning tasks. Our presentation and notation fol­
low that of [9].
Recall that our task is to learn the numerical parameters for a
discrete valued Bayesian network of a given structureS. Most
simply, a Bayesian network is parameterized using a vector
of conditional probability table (CPT) entries, one entry for
each value of each node and each instantiation of the nodes
parents. More precisely, let X; be a node in the network,
and let Pa; be the set of parent nodes of X; in S. We let
x 7 fork :::::: 1, . . , r; denote the possible values taken by X;
and let pa{ for j :::::: 1, .., q; denote the set of possible values
taken by Pa;. (Where, as usual, a value for Pa; determines
a value for each of the variables in Pa; .) We can now define
a parameter eijk to represent the conditional probability table
entry P(X; :::::: x 7 I Pa; :::::: pa{). Finally, we use() to denote
the entire vector of parameter values eijk·
.

.

2.1

The basic equations

Our task is as follows. We have a current model (assignment of
parameters) lJ. We also have some set of (new or previously
used) data cases D :::::: {y1, . , y N}. Each data case Yl is
a (possibly) partial assignment of values to variables in the
network. We want to construct a new model 0 based on lJ and
D.
.

.

One important metric for our choice of 0 is the extent to which
it explains our data D. We quantify that, as usual, via the log
likelihood of Din 0, log P9(D). However, the log-likelihood
should not be the only factor. Since our current model 0 is
already the result of some previous learning process (whether
from the same sample set D or from other samples), we do
not want to completely ignore it, as we would if we based
our choice of 0 purely on the log-likelihood. Thus, we want
to balance potential increases to the log-likelihood with the
extent to which we move away from our current model.
We therefore choose 0 so that it maximizes the following
function F, which balances these two factors:
(1)

The first term, LD(O), is the normalized
log-likelihood of Din
N
thenewmodel9, namely, LD(9):::::: tr 'L1=1logPe(
YI)- We
use the normalized likelihood (rather than the un-normalized
version) so as to eliminate the explicit dependency on the

5

Update rules for parameter estimation in Bayesian networks

number of samples in our analysis. The "penalty" term d( 0,

0)

is an estimate of the distance between the new and old models.
Its effect is to keep

8 close to e.

The parameter TJ >

0 is a

learning rate, which determines the extent to which we are

We define the£ 2-norm based distance between two parameter
vectors

0 and e to be:

willing to let our samples move us away from our current
model.
Since it is computationally difficult to maximize

( 1) , we

choose to maximize a simplified version, obtained by lin­
earizing the log-likelihood term. Let
vector of

Ln(O).

Let

V'ijkLn(O)

V' Ln ( 0) be the gradient

e,

(4), we get that

be the entry in the gradi­

ent vector corresponding to the parameter
vicinity of

Plugging this expression into

()ijk· Thus, in the

we approximate the log-likelihood by its first

order Taylor approximation,

i, j, k.

for all

Summing over

k, i.e., the r; different possible values that X;

can take, we get that:
TJ

L Y'ijkLn(e)- (1- 1) + rn;J
k

The above linear approximation degrades the further we move

from the old parameter vector e. However, by subtracting the

distance term

d( 0, e) from L D ( 8), we can force the new

{J

parameter vector

e.

Therefore,

0.

� Lk V';jkLn(e).

=

"/ij back into (5),

0 will not be too far away from e, we can
L D ( 0) term in (I), thereby changing our task to

assuming that
replace the

to stay relatively close to

Thus, rij

=

If we plug these values of
_
we obtain the following update rule for 0:

one of maximizing:

If we substitute for

V'ijkLD (e)

its value according to

(3), we

obtain precisely the standard gradient projection algorithm
described in
It turns out (see

Pe

[2] ) that the decomposition of the distribution

implied by the network structure results in a particularly

simple expression for the gradient vector:

&e(x7, pa{ I

ture, define two probability distributions over the the same
space-the joint probability space of the variables in the
Bayesian network. Thus, we can compare the distance be­

N

tween the two parameter vectors using the distance between

D)

(3)

()ijk

.

where&e(x7,paf I

�

D)denotesl/N·"L;=l

i.e., the sample-based average of

x7, pai.

Relative entropy

The two parameter vectors, with the associated network struc­

l "L�1 P e(x7,pa{l Yt)
__
B;Jk

2.3

[2l.

the distributions they induce over this space.
One of the widely-used distance measures for two distribu­

.

Pe(x7,pai I Yt).

The maximization problem in (2) is a constrained optimization

tions over the same space is the
as KL-divergence

have that

Lk Bijk

=

1.

p and

�

eachi,j,k. That is,

W hile this definition does not seem particularly amenable to
analysis within

(4), the relative entropy between two distri­

butions over the same network has the following very nice

-

- {)
TJV'ijkLD (0)- -- -d(8, 0)

[J(Jijk

for all

p(x)
dKL(PIIq) = ""
.
L...- p(x) log
q
(x)
X

Introducing Lagrange mul pliers

"1-Jk

q over

q is defined to be

i, j, we must

for these constraints, we conclude that our solution 0 must
h a
.
sat1sfy -"
a ·. (F(O) + Li' ' "/i'J'(Lk' ()i'J'k' - 1 )) = 0 for
J

relative entropy (also known

Given two distributionsp and

some joint probability space X, the relative entropy between

problem, since our solution must be a legal assignment of
CPT entries in the network. That is, for every

[14]).

decomposition property:
+

rij = 0,

(4)

i, j, k. The result of solving this set of simultaneous

equations will allow us to find the vector 8 which maximizes

(2), as desired. The answer, of course, depends on our choice
of d. We now analyze this expression for three choices of d.

dKL(fille)
where

8;1

=

L L P8(Pa;
j

is the vector

=

pai) dKL(O;JIIe;J);

( B;j1, . .. };1r,), or (equivalently) the

distribution over the different values

Pe(X; I

Pa;

=

pa{).

(7)

x7

of X; defined by

6

Bauer, Koller, and Singer

To take the derivative, we first replace P9(Pa; = pa{) with
some known estimate F(pa;) (whose exact nature will be
determined later). It now follows that:

�dxL(e[[iJ)
a ij
e k

=

F(pa{)

ijk + 1
eijk

(log � )

-

�

(

)

+ /ij

=

0.

After some simple algebraic manipulation, and plugging in
the value for the gradient as in (3), we get:
__!1.....

e..

eijk

2.4

t'q(x� ,pa; ID)

(
)
----.......,.
.:.... .-- x�,pa;ID))
..,......:.
(_!Lt'q(
•Jk exp 8•1• F(pai)
·
"'
L...k BzJk exp 9 lJk.
P( pa', )

=

_ ___

""7'"

(8)

x2 distance

The x2 distance [5] between two distributionsp and q as above
is:

X2(PI Iq)

=

1
2 L (p(x)- q(x))2 I q(x).
X

This distance function is, in fact, a linear approximation of
the relative entropy distance [5]. We use the x2 distance
to approximate the relative entropy by first decomposing the
relative entropy as in (7), and then using the x2 distance to
approximate each of the distributions distributions Pe(X; I
Pa; pa{) which appear in the summation. That is, we use
as our distance function
=

;e(8[[9) =

L L P9(Pa; = pa{) x2(8iJIIiJ;j).
j

(9)

Approximating P9(pa{) using P(pa{), as before, and
then taking the derivative, we get 888 :e(8119)
•Jk

P(pa{ )( eijk/Bijk 1). Plugging this result into (4), we get:
ry'V;jkLv(iJ)- P(pa{)(e;jk/B;jk- 1) + /iJ = o. Thus,

-

-

B;jk =

7]

-

-

/ij

-

-

. 'V;jkLv(O)B;Jk+
. B;Jk+B;Jk· {10)
P(paf)
P (pai)

,

•

Substituting 'V ij k L D(9) for its value according to (3), and
summing over k to compute /ij, we get:

1

=

.

T]

·

and therefore /ij
we have that

j

"" vO
(' (X;k pa; I D)+
L-t

P(pai) k
=

,

/ij

.

,
P(pan

+ 1,

-ry£q(pa{i D). Now, returning to (10),

.
. T] . £q(xk
; ,pai I D)P(pai)
j
.,
.
. £o(Pa; I D)eiJk + eiJk
P(pai )

Batch update rules

In the previous section, we derived some basic rules (corre­
sponding to three different distance functions) for updating a
given parameter vector given some data set D. The inter­
pretation of these rules is different in the batch and on-line
contexts. We begin with the batch learning task, since it has
received far more attention, so that the problem and the metrics
for evaluating solutions are much better understood. Thus, we
assume that we are given a fixed data set D, and that our goal is
to find a parameter vector 8 which best explains D. When the
likelihood function cannot be computed analytically, as in the
case of Bayesian networks, learning algorithms of this type
usually employ a hill climbing scheme that requires multiple
iterations, where in each iteration the current set of parameters
is used to derive a new set of parameters.

iJ

.

Plugging this value into ( 4), we get that:

j
eijk
- +1
TJ'VijkLo(O)- P(pa;) log-eijk

3

(11)

This type of procedure fits directly into our the basic frame­
work described in the previous section. Our current model 9
is the one resulting from the previous pass over D; the goal
of the current pass is to update iJ, resulting in a new model 8.
Therefore, we can instantiate the three update rules described
above in the context of this problem.
We have already seen that the update rule for the £2-norm
leads directly to the gradient projection algorithm of [3, 2].
In order to apply the other two update rules in this context,
we must only decide on the estimate F(pa;j), introduced as
an approximation to P9(Pa; = pa{). In a batch setting, a
reasonable solution is to use the sample-based expectation as
an approximation:

.P(pa;j) =

N

� L Po(Pa; = pai I Yt) = £o(pa{ I D ).
1=1

(12)

Let us begin by considering the x 2 update rule. Plugging in
this last expression into (11), we get:

TJ£e(x7, pa{ I D)
£o(pa{i D)
£q(x7, pa{ I D)
T] £q(pai I D)

+

(l

_

TJ)B·zJk

.

(13)

This equation describes a weighted average between the the
parameter obtained by dividing the sample-based average of
the pair x7, pa{ by those for paf and the current parameter
Bijk· When 17 = 1, this update rule reduces to the standard
EM algorithm. We therefore call this parameterized update
rule EM(ry).
For 7J < 1, EM(TJ) instantiates the new parameter values to
be a weighted combination between the EM update and the
current vector of parameters. (A form of parameter update
that belongs to the family of stochastic approximation algo­
rithms [20, 7].) The new parameters are therefore somewhere
between the old ones and the ones induced by the data. Thus,
EM(ry) updates the parameter values more slowly than stan­
dard EM.

Update rules for parameter estimation in Bayesian networks

For 17 > 1, EM(Tf) does the reverse: rather than interpolating
between these two points, it uses the parameters induced from
the data to extrapolate the direction of update. It then speeds
up the update process, going even further in that direction than
what is implied by the data. While this extrapolation might
seem somewhat counterintuitive, it has been used successfully
in density estimation problems [ 18, 19]. As we show in
the next two sections, this faster update rate can speed up
convergence considerably.

We want to analyze the behavior of <ll around one of the local
maxima 8*. Our first observation is that (}* is necessarily a
fixpoint of <fl.
Lemma

8* is a local maximum of the likelihood func­
<ll(9*) = 8*, for any value of 7].

1: If

tion, then

Proof: Due to the constraint Lk eijk = 1, we get (by intro­
ducing the appropriate Lagrange multiplier) that the following
equation must hold at any local maximum:

Finally, we consider the relative entropy update rule (8). Using
(12) as our estimate for F( pai}, we get:

(14)

0.

Therefore,
1

N

k
j
()�. �
L.. Pe· (x;, pa;

Essentially, each parameter Bijk is multiplied by a factor
which is exponential in the relative difference between the
parameters induced by the data (via the ratio of sample based
expectations) and the current values of the parameters. Intu­
itively, larger differences cause the parameter to be updated
more rapidly. Again, Tf serves to guide the rate at which the
parameters are changed. This update rule is called EG(TJ)
(see [ 13, 11]), due to its use of an exponentiated gradient as
its main term. We note that, in our experimental results, the
batch version of EG(TJ) performed quite poorly. Therefore,
we focus the rest of our discussion of batch update on the
EM(TJ) procedure.

N

Convergence properties

The basic technique is as follows. We view the EM(TJ) update
rule as an operator over parameter vectors 8. In a sufficiently
small neighborhood of 8*, we can approximate this operator
using the linear component of its Taylor expansion. The con­
vergence rate of a linear operator is determined by the eigen
values of the matrix which describes it. By analyzing these
eigen values, we get precise bounds on the rate of convergence
in the vicinity of 9*.
as

an operator <fl. Formally,

=

0 .

(16)

Multiplying the equation by 8fJk and summing over all k we

get, 1/ N Lk ,I P9• (x7, pa{ I yt) +!' Lk Bfjk
0 and there­
'
p
fore that/ = - # 2:k',t Pe·(xf , a� I yt). Substitutingthis
value for 1 in (16), we get
=

1

� Pe• (x;"' 'pa;j I Yl )
NO*IJk L..
l

=

1

N

�
L..
k1,l

Pe· ( X;k' , paj;

I Yl

)

.

and finally that
()�.

_

-

Z::::1 P9• (xf, pail yz)

j
�
D
( k'
L... k', l •e• X; , pa;

which immediately implies that(}*

Since the likelihood function of Bayesian networks with miss­
ing data has multiple local maxima, it is impossible to derive
global convergence bounds on the performance of the any
local parameter update scheme. We therefore show in this
section results about the uniform rate of local convergence of
the EM(ry)-update rule. More precisely, we analyze the rate
of convergence of EM(TJ) in a neighborhood of some local
maximum 8*. Throughout this section we assume that 8* is
in the interior of the simplex of feasible parameters, that is,
Vi, j, k : 1 > Bfjk > 0. (Similar convergence results can be
obtained when the solution is on the simplex boundary.)

We begin by defining EM(ry)

I yt) + 1

•Jk 1=1

•Jk

4

7

=

I Yl )

'

(1 7)

<ll(9 *), as desired. I

Letting {J denote <ll ( 9), we now have:

0- 8* = �(8)- <ll(8*) = V'<fi(B*)(8- 8 *)

+

o(8- 9*) .
(18)

The term V'<ll(8*) is an m x m matrix (where m is the di­
mension of 8), whose uvth entry is the derivative of the uth
component of <I> with respect to the vth parameter, evaluated
at the local maximum likelihood parameter set 8*.
In the vicinity of 8*, V'<ll( 8*) ( 8- 8*) forms a linear approx­
imation to <fl. We therefore analyze this derivative, and show
that, in the vicinity of 9*, for 0 < 71 < 2, there exists a norm
11·11 on IRm and a number 0 �pry < 1 such that

IIY'<ll(9*) (8 - 9*) II

�

Pry

118 - 9*11

Thus, the operator <ll forms a contraction mapping around 9*,
one which induces a convergence rate of.>..
Our analysis generalizes a technique first used by Peters and
Walker [1 8] in the context of estimating the parameters of a
mixture of normal distributions. For simplicity, assume that
all the parameters of the network are known and fixed, except
for B;jk' for k' = 1, . .. , r;. (The general case is sketched at
the end of the section.) Hence, we can take m = r;.

8

Bauer, Koller, and Singer

Let aLk denote Pe· (x7 pai I Yl ). Let skk' = 1 if k = k'
and 0otherwise. Then,by a process of taking derivatives and
algebraic simplifications,we get:
1

""'
v

'J!kk'

_

-

8.;,1�cJ

a<t>(0),1•
ao IJkl
.

LD

,
k>"l a:ijlcn

- U,kk ' (1 - TJ) +
(skk' L1 alJ k - L1 a;J.ka;J'k')
_

The matrix Mcan be decomposed into two matrices M QR
where Qis a diagonal matrix with Qkk
1/()ijk· and R is
=

matrix with Rkk'

=

I

=

I

L
i o:'l:�;z•'. Denote by a1
k"f

IJkl!

the vector (a�jl, o:lj2, ... , a;jm), and let f3 = Lkl a;jk' The
matrix R can be rewritten as R = � 2::1 rif a1 Clearly, Qis
symmetric and positive definite if ()ijk > 0. R is symmetric
m
and positive semi-definite since for any e E JR ,

We can now define the norm for which 'll is a contraction.
Intuitively, Q is a diagonal matrix, and therefore has no in­
fluence on the rate of contraction. The rate of contraction is
determined by R. Hence, our norm should factor out Q, so
as to isolate the infu
l ence of R. Therefore,for a vector e,we
define n ell to bee Q-1 e.
As we discuss below, the rate of contraction with respect to
this norm is essentially determined by the eigen values of R.
We therefore begin by analyzing these eigen values. We first
show that the eigen values of R all lie in [0, 1]. We use the
following lemmas [ 12]:
Lemma 2: Let Aand Bbe two matrices of dimensions n x m
and m x n, respectively, where m ;::: n. Then, every eigen
value of ABis also an eigen value of EA.

Denote by Am ax (A) the largest eigen value of a
matrix A. Let {A;}�1 be a set of N matrices of the same
Lemma

1

Let Amin and Amaxbe the smallest and largest non-zero eigen
value of R, respectively. The rate at which the operator
'V'll(£1*)
I- ryQR contracts its various components is
determined by its largest eigen value. It is easy to see that this
value is the larger of 11 - 7JAmin Iand 11 - 7JAmax 1- Formally,
the operator nonn of 'V'll(B*) within 0 and with respect to
the above norm is p1)
max{ll-7]Amin I, ll-7]Amaxl}. For
0 < 7] < 2,we get that pry < 1, so that

=

or in a matrix form,

an m x m

.

Now, let .\1, . . , Ap be the non-zero eigen values of R, and
let n be the subspace defined by the corresponding eigen
vectors v1, ... Vp. We begin by considering the convergence
properties of 'll within 0. More precisely, assume that fJ and
()*are both within fl. Now, consider e = fJ- fJ*. Since e
is also in rl, it is expressible as a linear combination of the
Vj s
' . The application of R to e causes the linear coefficient
of each v; to be multiplied by a factor of.>.;. Thus,the rate at
which the various components of� shrink as R is applied is
determined by the size of the corresponding eigen values.

3:

(

dimension. Then, Amax 2::�1 A;
Based on these lemmas, we get

) ::; 2::�1 Amax (A;).

=

II'V<I>(fJ*)(fJ- f)*) II ::S Peta116 - 6*11 < 116- 6*11 . (19)
Thus,for any 11in the range 0to 2,we are guaranteed conver­
gence of EM(7])to 6*in a neighborhood around it.
Note that the above argument only applies to vectors that are
within the subspace n. The vector 6* in this subspace,since it
is a linear combination of the 5.1,but()may not be. However,
since <I>(6) is a weighted average of() and a vector in !l (a
weighted average of the a1 ),the component of 6which is not
in !lshrinks by a factor of 11 - 11 Iwith every application of <I>.
Therefore,for 0 < 7J < 2,we have that repeated applications
of <I> to a vector 6 result in exponential convergence to this
subspace.
The more general case when all the variables of 6are updated
uses a similar, albeit more complex, derivation. Roughly
speaking,in the general case, the matrix Mis a block matrix
where the ith block is an ( r; q;) x ( 7'; q;) square matrix. We
use the fact that Ljk o:Lk
Ljk P(x7 pa� I Yl} = 1 to
show that each block is a positive semi-definite matrix with
eigen values smaller than 1 and use this to bound the entire
operator norm, as above. We therefore obtain the following
theorem:

=

1

Theorem 1: For any data set D, and any 0 < 7J < 2, the
EM(7])update rule (Equation (13))converges to a local max­
imum of the likelihood function P8• (D), within some neigh­
borhood 116- 8*11 < S.

While local convergence to a local maximum is guaranteed for
any 7Jin the right range, thereis a particular value 71*, called
"the optimal learning rate", which yields, within a neigh­
borhood of a local maximum fJ*, the fastest uniform rate of
convergence of the EM(7]) update rule. From the derivation
above, the optimal rate of convergence is obtained when the
contraction factors, 11 - TJAmin I and ll - 7]Amax I are equal.
Since 0 < Amin $ Amax ::; 1 this equality is obtained when
1- 71* Amin = 71* Amax - 1. Hence,
71* = 2/(Amin+Amax);::: 2/(1+>-min) > 1

·

Update rules for parameter estimation in Bayesian networks

Therefore. the local optimal rate of convergence is greater
than 1, which implies that standard EM is inferior (in a neigh­
borhood of the local maximum) to the EM(ry*) update rule!
Furthermore, if Am ax is strictly smaller than one, the optimal
learning rate r( is actual ly greater than 2, even though local
convergence of the EM(ry)-update cannot be guaranteed for
such value of ry. In fact, we have observed in practice cases
where an aggressive learning of 77 > 2 is beneficial.
The imp ac t of the choice of 77 is clear from (19).

Each iteration

ofEM(ry) shrinks the distance between our current parameter
vector fJ to (J* by a factor p17• Thus, if p17 < p17•, then the
parameter vector e produced by EM(ry) after some number [{
of iterations will be closer to (J* than the vector e' produced by
a similar number of iterations ofEM(ry1). In fact, the distance
between e and fJ* will be smaller than the comparable distance
for 0' by an exponential factor (p'l / P1J' )K!
Thus, substantial improvements in convergence can be ob­
tained by runningEM with the optimal learning rate 77". Un­
fortunately, direct calculation of TJ* requires knowledge of
..\min and ..\max at the local maximum of interest. This pre­
vents determination of 77* prior to parameter estimation. How­
ever, if we assume that our current parameter vector fJ is fairly
close to (J*, then A min and ..\max at the current position can be
used to provide a fairly good estimate for 77". These eigenval­
ues can be estimated fairly efficiently using techniques similar
to those used in [16]. Of course, empirical testing would be
necessary to check whether the additional computational cost
of approximating ry· is worthwhile in practice.
Finally, we note that our convergence theorem and the result
on the optimal rate of convergence hold only in a neighbor­
hood of the local maximum. By contrast, the standard EM
update rule is guaranteed to converge to a local maximum
from any point in the space (except perhaps for pathologi­
cal cases [6]). A similar guarantee cannot be made for other
values of 7]. However, our experimental results in the next
section show that, in practice, we do get convergence from
random starting positions forT/ > 1.
We also note that the convergence rate analysis forEM(ry) was
only for vectors within the subspace n. The components of 0
that are orthogonal to n contract with a rate of 11
7] I, which
is clearly optimal when 77 = 1. By interspersing iterations of
EM(l) withEM(ry) for large f/, we can trade off convergence
within n with convergence outside of D. In practice, however,
this has not turned out to be necessary. Furthermore, in the
-

full paper we will show that, if our network structure allows
a sufficient variety of qualitatively different data cases, and
if our data set D is sufficiently large, the vectors iit will
(with high probability) span the entire space, in which case
R is guaranteed to have full dimension. Then, any vector
fJ is within the space n, and our convergence rate argument
applies.

5

9

Experimental results

We tested the performance of the EM(1J) algorithm in practice
by using it for parameter estimation in two well-known net­

works: the Alarm network for ICU ventilator management [ 1]
and the Insurance network for car insurance underwriting [2].
Since the results on the two networks were fairl y comparable,
we present major results for Alarm. Both individual runs of
the EM(ry) update rule and 1 0-fold cross-validated update runs
gave similar results; we present data from individual runs to
show performance of the update rule over a single parameter
estimation task.

To test the algorithm, we generated random training and test
cases from the original network, made both types of data
cases partially observable (asdescribed below), estimated the
network parameters from the training data, and then tested
the performance of the resulting network on the test data. We
attempted to introduce partial observability in a way that is
compatible with the use of the network in practice. Thus,
we partitioned the network variables into three categories:
input nodes, hidden nodes, and output nodes. The hidden
nodes correspond to variables like "Heart Rate", which are
never observed in either the training or the test data. The
input nodes are variables like "Heart Rate EKG", which are
often observable both in the training data and when the net­
work is used in practice. The output nodes are variables like
"Left Ventricular Failure", whose value we are interested in
querying. We therefore generated training data as follows:
complete data cases were generated from the distribution in
the network. In each data case, the values of the hidden vari­
ables were obscured; for each input and output variable, its
value was obscured with some fixed probability.1
In the learning phase, we began with an initial random choice
of network parameters. We proceeded to update parameters
using an initialEM(1) iteration followed by a fixed number of
EM(7J) iterations.For each task, we experimented with several
values of TJ· We also experimented with various proportions
of randomly unobserved variables in the training input ob­
servations. In each update run, we measured (i) average log
marginal likelihood over non-hidden nodes, and (ii) absolute
and relative error for the conditional probability of various out­
put nodes given the (partial) instantiation of the input nodes in
the training case.2 We measured these parameters over both
our training and test data.3
1

Since the process of obscuring values does not depend on the

actual values of any of the variables in the data case, this process is
ignorable as defined by Rubin [21 J.

2 If the probability of the relevant output node given the observed
input nodes is pin the learned network and p* in the correct network,

then the absolute error is lp-p* and the relative error is (lp-p"l)/p".
The distribution of missing data in the test cases was identical, in
each case. to the distribution used in the training data.

3For comparison, we also experimented with training data where
there were no hidden variables, and the missing values were simply

selected unifonnly over all variables in the network. The results
were qualitatively similar to the ones shown here, and were omitted
for lack of space.

10

Bauer, Koller, and Singer

EM111EM117)
Et.,!(l .�I
Et.lp &)
Er.l(le.J---

...

,,. (a)

Figure 1: Average marginal log-likelihood for the Alarm network with 2000 training cases (a) over the training data, and (b) over 2000 test
cases, using

20% unknown input values in both.

I

..

!

·0.115

·1,()6

I

j

.r

EM(l)­
l:;lrol(l-2)---­
EM1.6)
EM Ul) - · ·

'"11.4)

·7.1S
,,.

"' (b
)

,.. (a)

Figure 2: Average log marginal likelihood for the Alarm network with 2000 training cases (a) over
cases, using 40% unknown input values in both. Here all values of '7 converge to similar values.
Figures 1 and 2 show average log-likelihood convergence re­
sults with two proportions of unknowns in the input. The
graphs are fairly typical, in that there is an initial phase of
rapid convergence, with additional steep but smaller improve­
ments later on. Between the phases where significant progress
is made, there are long stretches where the log-likelihood im­
proves only slightly. In these stretches, the log-likelihood
of the training data continues ot increase consi st ently (as it
should). However, the log-likelihood of the test data often de­
creases in these long stretches. This phenomenon is a typical
example of overfitting the training data.
In both Figure 1 and 2, we see that the convergence rate of
EM(17) increases consistently with 11· For high values of 17, the
difference in convergence rates can be quite dramatic. The ini­
tial convergence for EM( 1.8) occurs around the 20th iteration,
while EM( 1) requires around 40 iterations. The difference be­
comes even more pronounced as the process continues, with
EM(L8) reaching a certain convergence level as much as 60
iterations before EM( 1 ) This increasing separation between
the different algorithms as the number of iterations grows sup­
ports our analysis of the impact of the convergence rate from
the previous section.
.

The convergence in log-likelihood is reflected by a similar
convergence in both error rates and parameter values, as shown
in Figures 3 and 4. In all cases, we observe faster convergence
using high values of17. In general, there is (as expected) a close
correlation between the graphs for the different metrics-

the training data

,

and (b) over 2000 test

training set log-likelihood, test set log-likelihoo d, absolute
and relative errors, and values for various parameters-e.g.,
in the iterations where significant progress takes place).
More interestingly, there is often noticeable repetition in con­
vergence behavior over EM( 17) update runs for different values
of 11· This repetition or "stuttering" indicates that these runs
follow a similar trajectory in parameter space, but that this
process is simply much faster for larger values of 1]. This in­
tuition is supported by the parameter graphs of Figure 4, which
are typical of the graphs for other parameters in the network.
Thus, higher values of 17 accelerate the progress of the up­
date algorithm, but the shape of the path within the parameter
space is often preserved. This phenomenon raises interesting
conjectures about the shape of the likelihood function over
the parameter space.
However, the trajectories are not a! ways identical. In Figure I,
EM(l.8) and EM(l.6) converge to a different log-likelihood
value than EM with other values of 17; likewise, in Figure 4(c),
EM with higher values of 17 converges to different parameters.
In this case, it appears that the larger step size resulting from
higher values of TJ caused the parameter vector to move to a
different region in the parameter space, resulting in conver­
gence to a a different local maximum. In general, the exact
local maximum to which EM(17) converges depends both on
17 and on the initial random assignment of parameter values.
In this case, the higher values of TJ converge to a better maxi­
mum in likelihood space, but this is not necessarily the case.

11

Update rules for parameter estimation in Bayesian networks

E�I'I
Er..(UI
�lol ll,ll
Elolil.olli
Ei�o�IUI

­
····
··
· ···

"' (c)
Figure 3: Error graphs for specific output nodes in the Alann network over test cases. (a) Absolute error for "Left Ventricular Failure" node
with

40% unknowns in the inputs; (b) Relative error for "Pulmonary Embolus" node with 20% unknowns in the inputs; (c) Absolute error for
20% unknowns in the inputs.

"Insufficient Anesthesia" node with

it.lp.2] ·•·•

Elloi(T I ­

�r.,l(1,il •••
Elolll 1 .8i - · ·
Er.IIUI

Elollll (iloll'.2) • "
U.lil.ll
"-11 1.1

Elol�11 i.,.�1 �I •·•·

E

Er.lil�l
E��'-11
Elrol(i ll · · ·

1

fi:W(I.il • • •

, 00

"!, -::------o:--::---:
. .r;
:_----:=---�-�-::.� (c)

(a)

Figure 4: Conditional prob ability table value graphs for the Alann network with

2000 training

cases and 20% unknown input values for

subparameters of (a) "Central Venous Pressure" node, (b) "Pulmonary Capillary Wedge Pressure" node, and (c) "Minute Volume" node.

EM(1.8) converges to a

task is very relevant in the context of Bayesian networks: An

less optimal point. Furthermore, as we can see in Figure 3(b)

operational Bayesian network system is constantly presented

We h ave also observed cases where

and (c), a higher likelihood setting of parameters does not

with new cases for inference purposes. We would like the

imply a l ower error rate for all of the output variables of inter­

s ystem to fine-tune itself based on these cases, adapting its

est. A different p arameter setting may improve the accuracy

network model to the environment. Nevertheless, the problem

of some probability estimates while reducing the accuracy of

of on-line learning has been left largely unexplored in the

[ 1 7]
[8] on structure learning with fully

others. In general, we can say nothing about the relative qual­

context of Bayesian networks (with the exception of

ity of the local maxima to which the different processes will

and the recent work of

converge or how they will affect the error for some specific

observable data). In this section, we take a first step towards

output node. However, in all our experiments, higher values

providing a formal model for this task.

o f T) resulted in faster convergence to whatever maximum the
process ended up at.

The key assumption in the on-line setting is that the learner
cannot store past examples. Thus, one cannot simply accu­

As mentioned above, we also experimented with the

EG(TJ)
update rule, with disappointing results. Using the EG(TJ) rule

mulate lots of examples and then apply batch learning tasks.

often led to unstable updating, which made it difficult to con­

(or perhaps a few) that the learner maintai ns. Thus, the learner

verge upon a good set of parameters. I nitial exploration of

faces contradictory demands: it has to keep track of what has

Learning takes place only by modifying a single hypothesis

EG(T)) ' s performance i ndicated a tendency to overcompensate

been learned so far while adjusting its hypothesis based on

for differences between estimated parameters and data. Such

the new examples. The on-line learning framework of Kivi­

overcompensation caused parameters to quickly move away

nen and Warmuth

from good local maxima, which in turn caused other param­

these two requirements. In Section 2, we laid the foundation

eters to shift. The end result was often slow convergence to

for applying these tools in the context of Bayesian networks.

mediocre parameters. Despite this, more exploration of the

In this section, we interpret the resulting rules in the on-line

EG(TJ) algorithm is necessary to determine its effectiveness as

setting.

a parameter estimation algorithm.

6

In the on-line setting, the learner repeatedly gets one new

sample Yt at a time. At each time step t, the learner has a
current model, which we denote by Bt . An on-line update rule

On-line parameter estimation

tells the learner how to transform

In an on-line setti ng, a learner observes one example at a time
and needs to update its current model of the world.

[ 1 3] provides an analytical tool to balance

This

based o n the sample Yt .

o t into a new model ot+l

12

Bauer, Koller, and Singer

The basic framework is exactly as described in Section 2.
At each step t , we want to solve Equation ( I ), with D be­
ing our current sample Y t · The gradient vector used in
our approximation (2) is now the instantaneous gradient:
'V Lt (B)

�

Pe(x1 , pa� I Yt)/8ij k ·

The gradient projection update rule (6) remains unchanged.
To instantiate our other update rules (8) and ( I I ), we must
define our approximation P for P9 (pai ) . Since we cannot
store the previous exampl es we simply use our current model
f i as the estimate for 8. Thus, we set P(pai )
Pe• ( Pa;
,

=

=

paf ) . Now, if we follow the same steps as in the batch setting

with the above modifications we get that the EM(1J) and EG(ry)
updates are,
EM(ry)

:

et+
kt
'J

where ZfJ is a normalization constant for time step t .
A s written, these update rules implicitly assume that the learn­
i ng rate 7) is the same for all t . However, as t grows, our model
f i is based on more and more data cases. Intuitively, it seems
that a new sample should have less effect on a well-established
model (one based on many prior samples) than on a new one.
Therefore, we may want to adapt our learning rate 17 over
time, based on the number of examples seen so far (see, for
instance, [4]). This is, in fact, precisely the behavior we would
get from a full Bayesian updating scheme for our model (a
scheme which is unfortunately infeasible in the presence of
partially-observable data cases [9]).
On the other hand, most of the worst case analyses of on-line
learning algorithms do, in fact, employ a fixed learning rate to
derive bounds on the performance of the on-line algorithm (see
for instance [ 1 3] and the references therein). Furthermore, a
fixed learning rate yielded very good results for other learning
tasks, even when applied to natural data [ 1 0] . It is therefore
an interesting and challenging research problem to determine
whether an adaptive learning rate or a fixed one should be
empl oyed in on-line learning of B ayesian n etworks.

7

Conclusion and future work

In this paper, we re-examined the problem of parameter esti­
mation in B ayesian networks. By applying a recently devel­
oped theoretical framework for this task [ 1 3] , we derive two
families of update rules EM(ry) and EG(ry), where EM( l ) is
simply the standard EM algorithm for Bayesian networks.
We applied the EM(ry) algorithm to the traditional batch learn­
ing task, and showed, both theoretically and empirically, that
EM(ry) for values of 7) larger than 1, converges to a locally
maximal parameter assignment in fewer iterations than EM.
Theoretically, we showed that for any (local or global) maxi­
mum of the likelihood function, there is a value ry• > 1 and a

neighborhood around the local maximum, such that EM(r( )
provides the fastest convergence (of any EM(ry) algorithm) to
the maximum in that neighborhood.
We note that our theoretical convergence guarantees for
EM(ry) are weaker than those for EM . In particular, EM is
guaranteed to converge to a local maximum no matter its
initial starting point. EM(ry), on the other hand, is only guar­
anteed to converge to any local maximum if its initial starting
point is in a neighborhood of that maximum. However, our
empirical results demonstrate that, in practice, this is not a
concern. In almost all cases, EM(ry) does converge to a lo­
cal maximum from a random initi al position. In many cases,
EM(ry) converges to the same local maximum as EM. In those
cases where it converges to a different maximum, the outcome
may be better or worse.
In all cases, however, the convergence of EM(ry) for large val­
ues of 1] is significantly faster. It sometimes takes as many
as 60 fewer i terations to reach the same point. The savings
resulting from the accelerated convergence can be substantial :
each i teration of EM involves running a Bayesian network in­
ference algorithm on each one of the data cases in our training
set. For large networks, a large number of data cases may be
required in order to guarantee the robustness of our solution,
and each application of Bayesian network inference can be
very expensive. Moreover, the complexity of larger networks
requires more iterations of EM to reach convergence. Since,
as we discussed, the improvement of EM(ry) over EM grows
exponentially with the number of iterations, the benefits of
EM(ry) should be particularly significant in this context. Fi­
nally, it is important to note the main advantage of EM(ry)
over other possible schemes for accelerating EM: EM(ry) is
no more complicated to implement than standard EM, and
each iteration of EM(ry) requires exactly the same amount of
computation as EM.
Our work leads to several interesting directions for future
work. It is possible to modify the EM update rule to find the a
(local) MAP (maximum a posteriori) parameter assignment.
It should be fairly easy to similarly adapt the EM(7J) rule. It is
also important to test the convergence of EM(ry) from a wider
variety of random starting point, verifying that, in practice, it
does converge to a local maximum from any starting point. If
it does not, one could consider hybrid algorithms that adapted
7) during the algorithm, using i terations with 1] close to 1 to
bring the algorithm close to a local maximum, and iterations
with large 1J to speed up convergence in the neighborhood.
A somewhat larger scale project is a more comprehensive
i nvestigation of the on-line learning task. We would like
to prove convergence properties for the on-line update rules
derived in Section 6, and to investigate their performance in
practice. The evaluation metrics for this task are not so well­
established, but we believe the on-line learning task to be a
very important one in practice. Finally, we would like to
i nvestigate the applications of our basic framework, and of
other recent results in on-line learning, to the task of learning
Bayesian network structure.

Update rules for parameter estimat ion in Bayesian networks
Acknowledgements

We would like to thank Nir Friedman, Yan n Le Cun, Kevin
Murphy, Manfred Wannuth, and the anonymous referees for
useful comments and discussions. The work of Eric B auer
and Daphne Koller was supported by Silicon Graphics Inc.
and by ONR gran t NOOO 14-96- ! -07 1 8. Some of the work
was done while Daphne Koller was visiting AT&T Research.
The experiments described here were done using M .CC++.4

13

[ 1 4] S. Kullback. Information Theory and Statistics. Wiley, New­
York, 1 959.
[ 15) S . L . Lauritzen. The EM al gorithm for graphical association
models with missing data. Computational Statistics and Data
Analysis, 1 9 : 1 9 1 -20 1 , 1 995.

( 1 6] Y. Le Cun, l . Kanter, and S. A. Solla. Eigenvalues of covariance
matrices: Application to neural- network learning. Physical
Review Letters, 66( 1 8):2396-2399, 1 99 1 .
( 1 7] K . G . Olesen, S . L. Lauritzen, and F. V. Jensen.

HUGIN:

A system for creating adaptive causal probabilistic networks.

In Proceedings of the Eighth Conference on Uncertainty in

