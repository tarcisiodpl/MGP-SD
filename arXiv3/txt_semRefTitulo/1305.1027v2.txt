. In some reinforcement learning problems an agent may be
provided with a set of input policies, perhaps learned from prior experience or provided by advisors. We present a reinforcement learning
with policy advice (RLPA) algorithm which leverages this input set and
learns to use the best policy in the set for the reinforcement learning
√
e T)
task at hand. We prove that RLPA has a sub-linear regret of O(
relative to the best input policy, and that both this regret and its computational complexity are independent of the size of the state and action
space. Our empirical simulations support our theoretical analysis. This
suggests RLPA may offer significant advantages in large domains where
some prior good policies are provided.

1

Introduction

In reinforcement learning an agent seeks to learn a high-reward policy for selecting actions in a stochastic world without prior knowledge of the world dynamics
model and/or reward function. In this paper we consider when the agent is provided with an input set of potential policies, and the agent’s objective is to
perform as close as possible to the (unknown) best policy in the set. This scenario could arise when the general domain involves a finite set of types of RL
tasks (such as different user models), each with known best policies, and the
agent is now in one of the task types but doesn’t know which one. Note that
this situation could occur both in discrete state and action spaces, and in continuous state and/or action spaces: a robot may be traversing one of a finite set
of different terrain types, but its sensors don’t allow it to identify the terrain
type prior to acting. Another example is when the agent is provided with a set
of domain expert defined policies, such as stock market trading strategies. Since
the agent has no prior information about which policy might perform best in its
current environment, this remains a challenging RL problem.
Prior research has considered the related case when an agent is provided with
a fixed set of input (transition and reward) models, and the current domain is an
(initially unknown) member of this set [5,4,2]. This actually provides the agent
with more information than the scenario we consider (given a model we can
extract a policy, but the reverse is not generally true), but more significantly, we

2

Azar, Lazaric, and Brunskill

find substantial theoretical and computational advantages from taking a modelfree approach. Our work is also closely related to the idea of policy reuse [6],
where an agent tries to leverage prior policies it found for past tasks to improve
performance on a new task; however, despite encouraging empirical performance,
this work does not provide any formal guarantees. Most similar to our work is
Talvitie and Singh’s [14] AtEase algorithm which also learns to select among an
input set of policies; however, in addition to algorithmic differences, we provide
a much more rigorous theoretical analysis that holds for a more general setting.
We contribute a reinforcement learning with policy advice (RLPA) algorithm.
RLPA is a model-free algorithm that, given an input set of policies, takes an
optimism-under-uncertainty approach of adaptively selecting the policy that may
have the highest reward for the current task. We prove the regret of our algorithm
relative to the (unknown) best in the set policy scales with the square root of the
time horizon, linearly with the size of the provided policy set, and is independent
of the size of the state and action space. The computational complexity of our
algorithm is also independent of the number of states and actions. This suggests
our approach may have significant benefits in large domains over alternative
approaches that typically scale with the size of the state and action space, and
our preliminary simulation experiments provide empirical support of this impact.

2

Preliminaries

A Markov decision process (MDP) M is defined as a tuple hS, A, P, ri where
S is the set of states, A is the set of actions, P : S × A → P(S) is the transition kernel mapping each state-action pair to a distribution over states, and
r : S × A → P([0, 1]) is the stochastic reward function mapping state-action
pairs to a distribution over rewards bounded in the [0, 1] interval.3 A policy π is
a mapping from states to actions. Two states si and sj communicate with each
other under policy π if the probability of transitioning between si and sj under
π is greater than zero. A state s is recurrent under policy π if the probability
of reentering state s under π is 1. A recurrent class is a set of recurrent states
that all communicate with each other and no other states. Finally, a Markov
process is unichain if its transition matrix consists of a single recurrent class
with (possibly) some transient states [12, Chap. 8].
We define the performance of π in a state s as its expected average reward
X

T
1
µπ (s) = lim
E
r(st , π(st )) s0 = s ,
(1)
t=1
T →∞ T
where T is the number of time steps and the expectation is taken over the
stochastic transitions and rewards. If π induces a unichain Markov process on
M , then µπ (s) is constant over all the states s ∈ S, and we can define the bias
3

The extension to larger bounded regions [0, d] is trivial and just introduces an additional d multiplier to the resulting regret bounds.

Regret Bounds for Reinforcement Learning with Policy Advice

3

function λπ such that


λπ (s) + µπ = E r(s, π(s)) + λπ (s′ ) .

(2)

Its corresponding span is sp(λπ ) = maxs λπ (s) − mins λπ (s). The bias λπ (s) can
be seen as the total difference between the reward of state s and average reward.
In reinforcement learning [13] an agent does not know the transition P and/or
reward r model in advance. Its goal is typically to find a policy π that maximizes
its obtained reward. In this paper, we consider reinforcement learning in an MDP
M where the learning algorithm is provided with an input set of m deterministic
policies Π = {π1 , . . . , πm }. Such an input set of policies could arise in multiple
situations, including: the policies may represent near-optimal policies for a set
of m MDPs {M1 , . . . , Mm } which may be related to the current MDP M ; the
policies may be the result of different approximation schemes (i.e., approximate
policy iteration with different approximation spaces); or they may be provided
by m advisors. Our objective is to perform almost as well as the best policy in
the input set Π on the new MDP M (with unknown P and/or r).
Our results require the following mild assumption:
Assumption 1 There exists a policy π + ∈ Π, which induces a unichain Markov
+
process on the MDP M , such that the average reward µ+ = µπ ≥ µπ (s) for any
+
state s ∈ S and any policy π ∈ Π. We also assume that sp(λπ ) ≤ H, where H
is a finite constant.4
This assumption trivially holds when the optimal policy π ∗ is in the set Π. Also,
in those cases that all the policies in Π induce some unichain Markov processes
the existence of π + is guaranteed.5
A popular measure of the performance of a reinforcement learning algorithm
over T steps is its regret relative to executing the optimal policy π ∗ in M . We
evaluate the regret relative to the best policy π + in the input set Π,
XT
rt ,
(3)
∆(s) = T µ+ −
t=1

where rt ∼ r(·|st , at ) and s0 = s. We notice that this definition of regret differs
from the standard definition of regret by an (approximation) error T (µ∗ − µ+ )
due to the possible sub-optimality of the policies in Π relative to the optimal
policy for MDP M . Further discussion on this definition is provided in Sec. 8.

3

Algorithm

In this section we introduce the Reinforcement Learning with Policy Advice
(RLPA) algorithm (Alg. 1). Intuitively, the algorithm seeks to identify and use
4

5

One can easily prove that the upper bound H always exists for any unichain Markov
reward process (see [12, Chap. 8]).
Note that Assumption 1 in general is a weaker assumption than assuming MDP M
is ergodic or unichain, which would require that the induced Markov chains under all
policies be recurrent or unichain, respectively: we only require that the best policy
in the input set must induce a unichain Markov process.

4

Azar, Lazaric, and Brunskill

Algorithm 1 Reinforcement Learning with Policy Advice (RLPA)
Require: Set of policies Π, confidence δ, span function f
1: Initialize t = 0, i = 0
2: Initialize n(π) = 1, µ
b(π) = 0, R(π) = 0 and K(π) = 1 for all π ∈ Π
3: while t ≤ T do
b = f (Ti )
4:
Initialize ti = 0, Ti = 2i , Πi = Π, H
5:
i=i+1
6:
while ti ≤ Ti & Π
i 6= ∅ do (run trial)
q
b
b K(π)
7:
c(π) = (H + 1) 48 log(2t/δ) + H
n(π)

8:
9:
10:
11:

12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:

n(π)

B(π) = µ
b(π) + c(π)
π
e = arg maxπ B(π)
v(e
π) = 1
while ti ≤ Ti & v(e
π) < n(e
π) &
q
R(e
π)
b +1) 48
µ
b(e
π) −
≤c(e
π)+(H
n(e
π )+v(e
π)

log(2t/δ)
n(e
π )+v(e
π)

b K(eπ ) do
+H
n(e
π )+v(e
π)

(run episode)
t = t + 1, ti = ti + 1
Take action π
e (st ), observe st+1 and rt+1
v(e
π ) = v(e
π) + 1 , R(e
π) = R(e
π ) + rt+1
end while
K(e
π ) = K(e
π) + 1
q
π)
b +1) 48 log(2t/δ) + H
b K(eπ) then
> c(e
π ) + (H
if µ
b(e
π) − n(eπR(e
)+v(e
π)
n(e
π )+v(e
π)
n(e
π )+v(e
π)
Πi = Πi − {e
π}
end if
π)
n(e
π ) = n(e
π ) + v(e
π) , µ
b(e
π) = R(e
n(e
π)
end while
end while

the policy in the input set Π that yields the highest average reward on the
current MDP M . As the average reward of each π ∈ Π on M , µπ , is initially
unknown, the algorithm proceeds by estimating these quantities by executing
the different π on the current MDP. More concretely, RLPA executes a series of
trials, and within each trial is a series of episodes. Within each trial the algorithm
selects the policies in Π with the objective of effectively balancing between the
exploration of all the policies in Π and the exploitation of the most promising
ones. Our procedure for doing this falls within the popular class of “optimism in
face uncertainty” methods. To do this, at the start of each episode, we define an
upper bound on the possible average reward of each policy (Line 8): this average
reward is computed as a combination of the average reward observed so far for
this policy µ̂(π), the number of time steps this policy has been executed n(π) and
b which represents a guess of the span of the best policy, H + . We then select
H,
the policy with the maximum upper bound π
e (Line 9) to run for this episode.
Unlike in multi-armed bandit settings where a selected arm is pulled for only one
step, here the MDP policy is run for up to n(π) steps, i.e., until its total number
b ≥ H + then the confidence bounds
of execution steps is at most doubled. If H
computed (Line 8) are valid confidence intervals for the true best policy π + ;

Regret Bounds for Reinforcement Learning with Policy Advice

5

b
however, they may fail to hold for any other policy π whose span sp(λπ ) ≥ H.
Therefore, we can cut off execution of an episode when these confidence bounds
fail to hold (the condition specified on Line 12), since the policy may not be an
b ≥ H + .6 In this case, we can eliminate
optimal one for the current MDP, if H
the current policy π
e from the set of policies considered in this trial (see Line 20).
After an episode terminates, the parameters of the current policy π
e (the number
of steps n(π) and average reward µ
b(π)) are updated, new upper bounds on the
policies are computed, and the next episode proceeds. As the average reward
estimates converge, the better policies will be chosen more.
Note that since we do not know H + in advance, we must estimate it online:
b is not a valid upper bound for the span H + (see Assumption 1),
otherwise, if H
a trial might eliminate the best policy π + , thus incurring a significant regret.
We address this by successively doubling the amount of time Ti each trial is run,
b that is a function f of the current trial length. See Sec. 4.1
and defining a H
for a more detailed discussion on the choice of f . This procedure guarantees
the algorithm will eventually find an upper bound on the span H + and perform
trials with very small regret in high probability. Finally, RLPA is an anytime
algorithm since it does not need to know the time horizon T in advance.

4

Regret Analysis

In this section we derive a regret analysis of RLPA and we compare its performance to existing RL regret minimization algorithms. We first derive preliminary
results used in the proofs of the two main theorems.
We begin by proving a general high-probability bound on the difference between average reward µπ and its empirical estimate µ
b(π) of a policy π (throughout this discussion we mean the average reward of a policy π on a new MDP
M ). Let K(π) be the number of episodes π has been run, each of them of length
vk (π) (k = 1, . . . , K(π)). The empirical average µ
b(π) is defined as
X
X
vk (π)
K(π)
1
rtk ,
(4)
µ
b(π) =
t=1
k=1
n(π)

where rtk ∼ r(·|skt , π(skt )) is a random sample
of the reward observed by taking
P
the action suggested by π and n(π) = k vk (π) is the total count of samples.
Notice that in each episode k, the first state sk1 does not necessarily correspond
to the next state of the last step vk−1 (π) of the previous episode.

Lemma 1. Assume that a policy π induces on the MDP M a single recurrent
class with some additional transient states, i.e., µπ (s) = µπ for all s ∈ S. Then
the difference between the average reward and its empirical estimate (Eq. 4) is
s
2 log(2/δ)
K(π)
π
π
|b
µ(π) − µ | ≤ 2(H + 1)
+ Hπ
,
n(π)
n(π)
with probability ≥ 1 − δ, where H π = sp(λπ ) (see Eq. 2).
6

See Sec. 4.1 for further discussion on the necessity of the condition on Line 12.

6

Azar, Lazaric, and Brunskill

Proof. Let rπ (skt ) = E(rtk |skt , π(skt )), ǫr (t, k) = rtk − rπ (skt ), and P π be the statetransition kernel under policy π (i.e. for finite state and action spaces, P π is the |S|×|S|
matrix where the ij-th entry is p(sj |si , π(si ))). Then we have


 K(π)
 K(π)
k (π)
k (π)
X vX
X vX
1
1
µ
b(π) − µπ =
(rtk − µπ ) =
(ǫr (t, k) + rπ (skt ) − µπ )
n(π)
n(π)
t=1
t=1
k=1

=

1
n(π)

k=1

 K(π)
k (π)
X vX
k=1

t=1


(ǫr (t, k) + λπ (skt ) − P π λπ (skt )) ,

where the second line follows from Eq. 2. Let ǫλ (t, k) = λπ (skt+1 ) − P π λπ (skt ). Then we
have

 K(π)
k (π)
X vX
1
(ǫr (t, k) + λπ (skt+1 ) − λπ (skt+1 ) + λπ (skt ) − P π λπ (skt ))
µ
b(π) − µπ =
n(π)
t=1
k=1

≤

1
n(π)

 K(π)
X
k=1

vk (π)−1

vk (π)

(H π +

X

ǫr (t, k) +

X
t=1

t=1

P


ǫλ (t, k)) ,

where we bounded the telescoping sequence t (λπsk − λπ (skt+1 )) ≤ sp(λπ ) = H π . The
t
sequences of random variables {ǫr } and {ǫλ }, as well as their sums, are martingale
difference sequences. Therefore we can apply Azuma’s inequality and obtain the bound
p
p
K(π)H π + 2 2n(π) log(1/δ) + 2H π 2(n(π) − K(π)) log(1/δ)
µ
b(π) − µπ ≤
n(π)
s
2 log(1/δ)
K(π)
≤ Hπ
+ 2(H π + 1)
,
n(π)
n(π)
with probability ≥ 1 − δ, where in the first inequality we bounded the error terms ǫr ,
each of which is bounded in [−1, 1], and ǫλ , bounded in [−H π , H π ]. The other side of
the inequality follows exactly the same steps.
⊓
⊔

In the algorithm H π is not known and at each trial i the confidence bounds are
b = f (Ti ), where f is an increasing function.
built using the guess on the span H
For the algorithm to perform well, it needs to not discard the best policy π +
(Line 20). The following lemma guarantees that after a certain number of steps,
with high probability the policy π + is not discarded in any trial.
Lemma 2. For any trial started after T ≥ T + = f −1 (H + ), the probability of
policy π + to be excluded from ΠA at anytime is less than (δ/T )6 .
b = f (Ti ) ≥
Proof. Let i be the first trial such that Ti ≥ f −1 (H + ), which implies that H
H + . The corresponding
step
T
is
at
most
the
sum
of
the
length
of
all
the
trials before
Pi−1 j
i, i.e., T ≤ j=1
2 ≤ 2i , thus leading to the condition T ≥ T + = f −1 (H + ). After
T ≥ T + the conditions in Lem. 1 (with Assumption 1) are satisfied for π + . Therefore
the confidence intervals hold with probability at least 1 − δ and we have for µ
b(π + )
s
K(π + )
2 log(1/δ)
+ H+
µ
b(π + ) − µ+ ≤ 2(H + + 1)
+
n(π )
n(π + )
s
+
b + 1) 2 log(1/δ) + H
b K(π ) ,
≤ 2(H
n(π + )
n(π + )

Regret Bounds for Reinforcement Learning with Policy Advice

7

where n(π + ) is number of steps when policy π + has been selected until T . Using a
similar argument as in the proof of Lem. 1, we can derive
s
2 log(1/δ)
K(π + )
R(π + )
+
b + 1)
b
≤ 2(H
+H
,
µ −
+
+
+
+
+
n(π ) + v(π )
n(π ) + v(π )
n(π ) + v(π + )
with probability at least 1−δ. Bringing together these two conditions, and applying the
union bound, we have that the condition on Line12 holds with at least probability 1−2δ
and thus π + is never
p discarded. More precisely
p Algo. 1 uses slightly larger confidence
intervals (notably 48 log(2t/δ) instead of 2 2 log(1/δ)), which guarantees that π +
is discarded with at most a probability of (δ/T )6 .
⊓
⊔

We also need the B-values (line 9) to be valid upper confidence bounds on the
average reward of the best policy µ+ .
Lemma 3. For any trial started after T ≥ T + = f −1 (H + ), the B-value of π
e is
an upper bound on µ+ with probability ≥ 1 − (δ/T )6 .

Proof. Lem. 2 guarantees that the policy π + is in ΠA w.p. (δ/T )6 . This combined with
Lem. 1 and the fact that f (T ) > H + implies that the B-value B(π + ) = µ
b(π + ) + c(π + )
+
is a high-probability upper bound on µ and, since π
e is the policy with the maximum
B-value, the result follows.
⊓
⊔

Finally, we bound the total number of episodes a policy could be selected.

Lemma 4. After T ≥ T + = f −1 (H + ) steps of Algo. 1, let K(π) be the total
number of episodes π has been selected and n(π) the corresponding total number
of samples, then
K(π) ≤ log2 (f −1 (H + )) + log2 (T ) + log2 (n(π)),

with probability ≥ 1 − (δ/T )6 .

Proof. Let
k (π) be the total number of samples at the beginning of episode k (i.e.,
Pnk−1
′
nk (π) =
k′ =1 vk (π)). In each trial of Algo. 1, an episode is terminated when the
number of samples is doubled (i.e., nk+1 (π) = 2nk (π)), or when the consistency condition (last condition on Line12) is violated and the policy is discarded or the trial
is terminated (i.e., nk+1 ≥ nk (π)). We denote by K(π) the total number of episodes
truncated before the number of samples is doubled, then n(π) ≥ 2K(π)−K(π) . Since the
episode is terminated before the number of samples is doubled only when either the trial
terminates or the policy is discarded, in each trial this can only happen once per policy.
Thus we can bound K(π) by the number of trials. A trial can either terminate because
its maximum length Ti is reached or when all the polices are discarded (line 6). From
Lem. 2, we have that after T ≥ f −1 (H + ), π + is never discarded w.h.p. and a trial only
terminates when ti > Ti . Since Ti = 2i , it follows that the number of trials is bounded
−1
+
by K(π) ≤ log2 (f −1 (H + )) + log2 (T ). So, we have n(π) ≥ 2K(π)−log2 (f (H ))−log2 (T ) ,
which implies the statement of the lemma.
⊓
⊔

Notice that if we plug this result in the statement of Lem. 1, we have that
the second
term converges to zero faster than the first term which decreases as
p
O(1/ n(π)), thus in principle itpcould be possible to use alternative episode
stopping criteria, such as v(π) ≤ n(π). But while this would not significantly
affect the convergence rate of µ
b(π), it may worsen the global regret performance
in Thm. 1.

8

Azar, Lazaric, and Brunskill

4.1

Gap-Independent Bound

We are now ready to derive the first regret bound for RLPA.
Theorem 1. Under Assumption 1 for any T ≥ T + = f −1 (H + ) the regret of
Algo. 1 is bounded as
p
√
∆(s) ≤ 24(f (T ) + 1) 3T m(log(T /δ)) + T + 6f (T )m(log2 (T + ) + 2 log2 (T )),

with probability at least 1 − δ for any initial state s ∈ S.

Proof. We begin by bounding the regret from executing each policy π. We consider
the k(π)-th episode when policy π has been selected (i.e., π is the optimistic policy
π
e ) and we study its corresponding total regret ∆π . We denote by nk (π) the number
of steps of policy π at the beginning of episode k and vk (π) the number of steps in
episode k. Also at time step T , let the total number of episodes, vk (π) and nk , for each
policy π be denoted as K(π), v(π) and n(π) respectively. We also let π ∈ Π, B(π),
c(π), R(π) and µ
b(π) be the latest values of these variables at time step T for each
policy π. Let E = {∀t = f −1 (H + ), . . . , T, π + ∈ ΠA & π
e ≥ µ+ } be the event under
which π + is never removed from the set of policies ΠA , and where the upper bound of
the optimistic policy π
e , B(e
π), is always as large as the true average reward of the best
policy µ+ . On the event E , ∆π can be bounded as
K(π) vk (π)

∆π =

X X
k=1

t=1

(1)

(µ+ − rt ) ≤

K(π) vk (π)

X X
k=1

t=1

(2)

≤ (n(π) + v(π)) 3(f (T ) + 1)

(3)

≤ 24(f (T ) + 1)

p

s

(B(π) − rt ) ≤ (n(π) + v(π))(b
µ(π) + c(π)) − R(π)

log(T /δ)
K(π)
48
+ 3f (T )
n(π)
n(π)

!

3n(π) log(T /δ) + 6f (T )K(π),

where in (1) we rely on the fact that π is only executed when it is the optimistic policy,
and B(π) is optimistic with respect to µ+ according to Lem. 3. (2) immediately follows
from the stopping condition at Line 12 and the definition of c(π). (3) follows from the
condition on doubling the samples (Line 12) which guarantees v(π) ≤ n(π).
We now bound the total regret ∆ by summing over all the policies.
X
X
p
∆=
24(f (T ) + 1) 3n(π) log(T /δ) + 6f (T )
K(π)
π∈Π

(1)

≤ 24(f (T ) + 1)

(2)

s

p

π∈Π

3m

X

π∈Π

n(π) log(T /δ) + 6f (T )

X

K(π)

π∈Π

3mT log(T /δ) + 6f (T )m(log2 (f −1 (H + )) + 2 log2 (T )),
P
where in (1) we use Cauchy-Schwarz inequality and (2) follows from
π n(π) ≤ T ,
Lem. 4, and log2 (n(π)) ≤ log 2 (T ).
Since T is an unknown time horizon, we need to provide a bound which holds with
high probability uniformly over all the possible values of T . Thus we need to deal with
the case when E does not hold. Based on Lem. 1 and by following similar lines to [7],
we can prove that the total regret of the episodes in which the true model is discarded
≤ 24(f (T ) + 1)

Regret Bounds for Reinforcement Learning with Policy Advice

9

√
is bounded by T with probability at least 1 − δ/(12T 5/4 ). Due to space limitations,
we omit the details, but we can then prove the final result by combining the regret in
both cases (when E holds or does not hold) and taking union bound on all possible
values of T .
⊓
⊔

A significant advantage of RLPA over generic RL algorithms (such as UCRL2)
is that the regret of RLPA is independent of the size of
√ the state and action
spaces: in contrast, the regret of UCRL2 scales as O(S AT ). This advantage
is obtained by exploiting the prior information that Π contains good policies,
which allows the algorithm to focus on testing their performance to identify the
best, instead of building an estimate of the current MDP over the whole stateaction space as in UCRL2. It is also informative to compare this result to other
methods using some form of prior knowledge. In [8] the objective is to learn
the optimal policy along with a state representation which satisfies the Markov
property. The algorithm receives as input a set of possible state representation
models and under the assumption that one of them is Markovian, the algorithm
is shown to have a sub-linear regret. Nonetheless,
√ the algorithm inherits the
regret of UCRL itself and still displays a O(S A) dependency on states and
actions. In [5] the Parameter Elimination (PEL) algorithm is provided with a
set of MDPs. The algorithm is analyzed in the PAC-MDP framework and under
the assumption that the true model actually belongs to the set of MDPs, it is
shown to have a performance which
√ does not depend on the size of the stateaction space and it only has a O( m) a dependency on the number of MDPs
m.7 In our setting, although no model is provided and no assumption on the
optimality of π ∗ is made, RLPA achieves the same dependency on m.
The span sp(λπ ) of a policy is known to be a critical parameter determining
how well and fast the average reward of a policy can be estimated using samples
(see e.g., [1]). In Thm. 1 we show that only the span H + of the best policy
π + affects the performance of RLPA even when other policies have much larger
spans. Although this result may seem surprising (the algorithm estimates the
average reward for all the policies), it follows from the use of the third condition
on Line12 where an episode is terminated, and a policy is discarded, whenever
the empirical estimates are not consistent with the guessed confidence interval.
b > H + but H
b < sp(λπ ) for a policy which is
Let us consider the case when H
selected as the optimistic policy π
e . Since the confidence intervals built for π are
not correct (see Lem. 1), π
e could be selected for a long while before selecting
a different policy. On the other hand, the condition on the consistency of the
observed rewards would discard π (with high probability), thus increasing the
chances of the best policy (whose confidence intervals are correct) to be selected.
We also note that H + appears as a constant in the regret through log2 (f −1 (H + ))
and this suggests that the optimal choice of f is f (T ) = log(T ), which
would
√
e
lead to a bound of order (up to constants and logarithmic terms) O( T m + m).
7

Notice that PAC bounds are always squared w.r.t. regret bounds, thus the original
√
m dependency in [5] becomes O( m) when compared to a regret bound.

10

Azar, Lazaric, and Brunskill

4.2

Gap-Dependent Bound

Similar to [7], we can derive an alternative bound for RLPA where the dependency on T becomes logarithmic and the gap between the average of the best
and second best policies appears. We first need to introduce two assumptions.
Assumption 2 (Average Reward) Each policy π ∈ Π induces on the MDP
M a single recurrent class with some additional transient states, i.e., µπ (s) = µπ
for all s ∈ S. This implies that H π = sp(λπ ) < +∞.
Assumption 3 (Minimum Gap) Define the gap between the average reward
of the best policy π + and the average reward of any other policy as Γ (π, s) =
µ+ − µπ (s) for all s ∈ S. We then assume that for all π ∈ Π − {π + } and s ∈ S,
Γ (π, s) is uniformly bounded from below by a positive constant Γmin > 0.
Theorem 2 (Gap Dependent
p Bounds). Let Assumptions 2 and 3 hold. Run
Algo. 1 with the choice of δ = 3 1/T (the stopping time T is assumed to be known
here). Assume that for all π ∈ Π we have that Hπ ≤ Hmax . Then the expected
regret of Algo. 1, after T ≥ T + = f −1 (H + ) steps, is bounded as


(f (T ) + Hmax )(log2 (mT ) + log2 (T + ))
E(∆(s)) = O m
,
(5)
Γmin
for any initial state s ∈ S.
Proof. (sketch) Unlike for the proof of Thm. 1, here we need a more refined control
on the number of steps of each policy as a function of the gaps Γ (π, s). We first
notice that Assumption 2 allows us to define Γ (π) = Γ (π, s) = µ+ − µπ for any state
s ∈ S and any policy π ∈ Π. We consider the high-probability event E = {∀t =
f −1 (H + ), . . . , T, π + ∈ ΠA } (see Lem. 2) where for all the trials run after f −1 (H + )
steps never discard policy π + . We focus on the episode at time t, when an optimistic
policy π
e 6= π + is selected for the k(π)-th time, and we denote by nk (e
π ) the number
of steps of π
e before episode k and vk (π) the number of steps during episode k(π).
The cumulative reward during episode k is Rk (e
π) obtained as the sum of µ
bk (e
π)nk (e
π)
(the previous cumulative reward) and the sum of vk (e
π) rewards received since the
beginning of the episode. Let E = {∀t = f −1 (H + ), . . . , T, π + ∈ ΠA & π
e ≥ µ+ } be
+
the event under which π is never removed from the set of policies ΠA , and where the
upper bound of the optimistic policy µ
e, B(e
π ), is always as large as the true average
reward of the best policy µ+ . On event E we have

(2)

b + 1)
3(H

≥ µ+ −

(3)

s

48

log(t/δ)
k(π) (1)
Rk (e
π)
+3
≥ B(e
π) −
nk (e
π)
nk (e
π)
nk (e
π ) + vk (e
π)

Rk (e
π)
1
≥ µ+ − µπe +
nk (e
π) + vk (e
π)
nk (e
π ) + vk (e
π)

1
≥ Γmin +
nk (e
π) + vk (e
π)

nk (e
π )+vk (e
π)

X
t=1

π
e

(4)

nk (e
π )+vk (e
π)

X
t=1

(µ − rt ) ≥ Γmin − H

π
e

(µπe − rt )
s

48

log(t/δ)
K(e
π)
− H πe
,
nk (e
π)
nk (e
π)

Regret Bounds for Reinforcement Learning with Policy Advice

11

with probability 1 − (δ/t)6 . Inequality (1) is enforced by the episode stopping condition on Line12 and the definition of B(π), (2) is guaranteed by Lem. 3, (3) relies on
the definition of gap and Assumption 3, while (4) is a direct application of Lem. 1.
Rearranging the terms, and applying Lem. 4, we obtain
p
p
b + 3 + H πe ) n(e
nk (e
π)Γmin ≤ (3H
π ) 48 log(t/δ) + 4H πe (2 log2 (t) + log2 (f −1 (H + ))).
By solving the inequality w.r.t. nk (e
π ) we obtain
p
p
b + 3 + H πe ) 48 log(t/δ) + 2 H πe Γmin (2 log2 (t) + log2 (f −1 (H + ))
p
(3H
n(e
π) ≤
, (6)
Γmin

w.p. 1 − (δ/t)6 . This implies that on the event E , after t steps, RLPA acted according
2
to a suboptimal policy π for no more than O(log(t)/Γmin
) steps. The rest of the proof
follows similar steps as in Thm. 1 to bound the regret of all the suboptimal policies in
high probability. The expected regret of π + is bounded by H + and standard arguments
similar to [7] are used to move from high-probability to expectation bounds.
⊓
⊔

Note that although the bound in Thm. 1 is stated in high-probability, it is easy
to turn it into a bound in expectation with almost identical dependencies on the
main characteristics of the problem and compare it to the bound of Thm. 2. The
major difference is that
√ the bound in Eq. 5 shows a O(log(T )/Γmin) dependency
on T instead of O( T ). This suggests that whenever there is a big margin
between the best policy and the other policies in Π, the algorithm is able to
accordingly reduce the number of times suboptimal policies are selected, thus
achieving a better dependency on T . On the other hand, the bound also shows
that whenever the policies in Π are very similar, it might take a long time to the
algorithm
√ before finding the best policy, although the regret cannot be larger
than O( T ) as shown in Thm. 1.
We also note that while Assumption 3 is needed to allow the algorithm to
“discard” suboptimal policies with only a logarithmic number of steps, Assumption 2 is more technical and can be relaxed. It is possible to instead only require
that each policy π ∈ Π has a bounded span, H π < ∞, which is a milder condition
than requiring a constant average reward over states (i.e., µπ (s) = µπ ).

5

Computational Complexity

As shown in Algo. 1, RLPA runs over multiple trials and episodes where policies
are selected and run. The largest computational cost in RLPA is at the start of
each episode computing the B-values for all the policies currently active in ΠA
and then selecting the most optimistic one. This is an O(m) operation. The total
number of episodes can be upper bounded by 2 log2 (T ) + log2 (f −1 (H + )) (see
Lem. 4). This means the overall computational of RLPA is of O(m(log2 (T ) +
log2 (f −1 (H + )))). Note there is no explicit dependence on the size of the state
and action space. In contrast, UCRL2 has a similar number of trials, but requires solving extended value iteration to compute the optimistic MDP policy.
Extended value iteration requires O(|S|2 |A| log(|S|)) computation per iteration:
if D are the number of iterations required to complete extended value iteration,

12

Azar, Lazaric, and Brunskill

then the resulting cost would be O(D|S|2 |A| log(|S|). Therefore UCRL2, like
many generic RL approaches, will suffer a computational complexity that scales
quadratically with the number of states, in contrast to RLPA, which depends
linearly on the number of input policies and is independent of the size of the
state and action space.

6

Experiments

In this section we provide some preliminary empirical evidence of the benefit of
our proposed approach. We compare our approach with two other baselines. As
mentioned previously, UCRL2 [7] is a well known algorithm for generic RL problems that enjoys strong theoretical guarantees
in terms of high probability regret
√
bounds with the optimal rate of O( T ). Unlike our approach, UCRL2 does not
make use of any policy
p advice, and its regret scales with the number of states
and actions as O(|S| |A|). To provide a more fair comparison, we also introduce
a natural variant of UCRL2, Upper Confidence with Models (UCWM), which
takes as input a set of MDP models M which is assumed to contain the actual
model M . Like UCRL2, UCWM computes confidence intervals over the task’s
model parameters, but then selects the optimistic policy among the optimal policies for the subset of models in M consistent with the confidence interval. This
may result in significantly tighter upper-bound on the optimal value function
compared to UCRL2, and may also accelerate the learning process. If the size
of possible models shrinks to one, then UCWM will seamlessly transition to following the optimal policy for the identified model. UCWM requires as input a
set of MDP models, whereas our RLPA approach requires only input policies.
We consider a square grid world with 4 actions: up (a1 ), down (a2 ), right
(a3 ) and left (a4 ) for every state. A good action succeeds with the probability
0.85, and goes in one of the other directions with probability 0.05 (unless that
would cause it to go into a wall) and a bad action stays in the same place with
probability 0.85 and goes in one of the 4 other directions with probability 0.0375.
We construct four variants of this grid world M = {M1 , M2 , M3 , M4 }. In model
1 (M1 ) good actions are 1 and 4, in model 2 (M2 ) good actions are 1 and 2, in
model 3 good actions are 2 and 3, and in model 4 good actions are 3 and 4. All
other actions in each MDP are bad actions. The reward in all MDPs is the same
and is −1 for all states except for the four corners which are: 0.7 (upper left), 0.8
(upper right), 0.9 (lower left) and 0.99 (lower right). UCWM receives as input
the MDP models and RLPA receives as input the optimal policies of M.
We evaluate the performances of each algorithm in terms of the per-step
ˆ = ∆/T (see Eq. 3). Each run is T = 100000 steps and we average the
regret, ∆
performance on 100 runs. The agent is randomly placed at one of the states of
the grid at the beginning of each round. We assume that the true MDP model
is M4 . Notice that in this case π ∗ ∈ Π, thus µ+ = µ∗ and the regret compares
to the optimal average reward. The identity of the true MDP is not known by

Regret Bounds for Reinforcement Learning with Policy Advice

13

2
RLPA
UCWM
UCRL2

b
∆

1.5

1

0.5

0
10

25

50
Number of states

75

100

Fig. 1. Per-step regret versus number of states.
2

100
80
Run time (sec)

b
∆

1.5

1

0.5

0
0

RLPA
UCWM
UCRL2

RLPA
UCWM
UCRL2

60
40
20

2

4
6
Time steps

8

10
4
x 10

(a) Avg. per-step regret vs time step.

0
10

25

50
Number of states

75

100

(b) Running time versus |S|.

the agent. For RLPA we set f (t) = log(t).8 We construct grid worlds of various
sizes and compare the resulting performance of the three algorithms.
Fig. 1 shows per-step regret of the algorithms as the function of the numb of
ber of states. As predicted by the theoretical bounds, the per-step regret ∆
UCRL2 significantly increases as the number of states increases, whereas the
average regret of our RLPA is essentially independent of the state space size9 .
Although UCWM has a lower regret than RLPA for a small number of states,
it quickly loses its advantage as the number of states grows. UCRL2’s per-step
regret plateaus after a small number of states since it is effectively reaching the
maximum possible regret given the available time horizon.
To demonstrate the performance of each approach for a single task, Fig. 2(a)
shows how the per-step regret changes with different time horizons for a gridworld with 64 states. RLPA demonstrates a superior regret throughout the run
with a decrease that is faster than both UCRL and UCWM. The slight periodic
increases in regret of RLPA are when a new trial is started, and all policies
are again considered. We also note that the slow rate of decrease for all three
8
9

See Sec. 4.1 for the rational behind this choice.
The RLPA regret bounds depend on the bias of the optimal policy which may be
indirectly a function of the structure and size of the domain.

14

Azar, Lazaric, and Brunskill

algorithms is due to confidence intervals dimensioned according to the theoretical
results which are often over-conservative, since they are designed to hold in the
worst-case scenarios. Finally, Fig. 2(b) shows the average running time of one
trial of the algorithm as a function of the number of states. As expected, RLPA’s
running time is independent of the size of the state space, whereas the running
time of the other algorithms increases.
Though a simple domain, these empirical results support our earlier analysis,
demonstrating RLPA exhibits a regret and computational performance that is
essentially independent of the size of the domain state space. This is a significant advantage over UCRL2, as we might expect because RLPA can efficiently
leverage input policy advice. Interestingly, we obtain a significant improvement
also over the more competitive baseline UCWM.

7

Related Work

The setting we consider relates to the multi-armed bandit literature, where an
agent seeks to optimize its reward by uncovering the arm with the best expected
reward. More specifically, our setting relates to restless [9] and rested [15] bandits,
where each arm’s distribution is generated by a an (unknown) Markov chain that
either transitions at every step, or only when the arm is pulled, respectively.
Unlike either restless or rested bandits, in our case each “arm” is itself a MDP
policy, where different actions may be chosen. However, the most significant
distinction may be that in our setting there is a independent state that couples
the rewards obtained across the policies (the selected action depends on both
the policy/arm selected, and the state), in contrast to the rested and restless
bandits where the Markov chains of each arm evolve independently.
Prior research has demonstrated a significant improvement in learning in a
discrete state and action RL task whose Markov decision process model parameters are constrained to lie in a finite set. In this case, an objective of maximizing
the expected sum of rewards can be framed as planning in a finite-state partially observable Markov decision process [10]: if the parameter set is not too
large, off-the-shelf POMDP planners can be used to yield significant performance improvements over state-of-the-art RL approaches [2]. Other work [5] on
this setting has proved that the sample complexity of learning to act well scales
independently of the size of the state and action space, and linearly with the size
of the parameter set. These approaches focus on leveraging information about
the model space in the context of Bayesian RL or PAC-style RL, in contrast to
our model-free approach that focuses on regret.
There also exists a wealth of literature on learning with expert advice (e.g. [3]).
The majority of this work lies in supervised learning. Prior work by Diuk et
al. [4] leverages a set of experts where each expert predicts a probabilistic concept (such as a state transition) to provide particularly efficient KWIK RL. In
contrast, our approach leverages input policies, rather than models. Probabilistic
policy reuse [6] also adaptively selects among a prior set of provided policies, but
may also choose to create and follow a new policy. The authors present promis-

Regret Bounds for Reinforcement Learning with Policy Advice

15

ing empirical results but no theoretical guarantees are provided. However, we
will further discuss this interesting issue more in the future work section.
The most closely related work is by Talvitie and Singh [14], who also consider
identifying the best policy from a set of input provided policies. Talvitie and
Singh’s approach is a special case of a more general framework for leveraging
experts in sequential decision making environments where the outcomes can
depend on the full history of states and actions [11]: however, this more general
setting provides bounds in terms of an abstract quantity, whereas Talvitie and
Singh provide bounds in terms of the bounds on mixing times of a MDP. There
are several similarities between our algorithm and the work of Talvitie and Singh,
though in contrast to their approach we take an optimism under uncertainty
approach, leveraging confidence bounds over the potential average reward of
each policy in the current task. However, the provided bound in their paper is
not a regret bound and no precise expression on the bound is stated, rendering
it infeasible to do a careful comparison of the theoretical bounds. In contrast, we
provide a much more rigorous theoretical analysis, and do so for a more general
setting (for example, our results do not require the MDP to be ergodic). Their
algorithm also involves several parameters whose values must be correctly set
for the bounds to hold, but precise expressions for these parameters were not
provided, making it hard to perform an empirical comparison.

8

Future Work and Conclusion

In defining RLPA we preferred to provide a simple algorithm which allowed
us to provide a rigorous theoretical analysis. Nonetheless, we expect the current version of the algorithm can be easily improved over multiple dimensions.
The immediate possibility is to perform off-policy learning across the policies:
whenever a reward information is received for a particular state and action, this
could be used to update the average reward estimate µ
b(π) for all policies that
would have suggested the same action for the given state. As it has been shown
in other scenarios, we expect this could improve the empirical performance of
RLPA. However, the implications for the theoretical results are less clear. Indeed, updating the estimate µ
b(π) of a policy π whenever a “compatible” reward
is observed would correspond to a significant increase in the number of episodes
K(π) (see Eq. 4). As a result, the convergence rate of µ
b(π) might get worse and
could potentially degrade up to the point when µ
b(π) does not even converge to
the actual average reward µπ . (see Lem. 1 when K(π) ≃ n(π)). We intend to
further investigate this in the future.
Another very interesting direction of future work is to extend RLPA to leverage policy advice when useful, but still maintain generic RL guarantees if the
input policy space is a poor fit to the current problem. More concretely, currently
if π + is not the actual optimal policy of the MDP, RLPA suffers an additional
linear regret to the optimal policy of order T (µ∗ − µ+ ). If T is very large and π +
is highly suboptimal, the total regret of RLPA may be worse than UCRL, which
always eventually learns the optimal policy. This opens the question whether it

16

Azar, Lazaric, and Brunskill

is possible to design an algorithm able to take advantage of the small regret-tobest of RLPA when T is small and π + is nearly optimal and the guarantees of
UCRL for the regret-to-optimal.
To conclude, we have presented RLPA, a new RL algorithm that leverages
an input set of policies. We prove the regret of RLPA relative to the best policy
scales sub-linearly with the time horizon, and that both this regret and the
computational complexity of RLPA are independent of the size of the state and
action space. This suggests that RLPA may offer significant advantages in large
domains where some prior good policies are available.

