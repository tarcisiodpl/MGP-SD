

:

TOULOUSE

This paper discusses how a measure of

uncertainty representing a state of knowledge can be

updated when a new information, which may be
pervaded with uncertainty, becomes available. This

Cedex

(FRANCE)

(1978)'s possibility distributions, Spohn ( 1988,
1989)'s ordinal conditional functions are the various

candidates considered for the representation of the
contents of a knowledge base. Thus when discussing

problem is considered in various framework, namely :

updating issues, we do not take into account the

Spohn's updating approach. In the two first cases,

having the same semantic de scription o f their

Shafer's evidence theory, Zadeh's possibility theory,

existence of syntactically different knowledge bases

analogues of Jeffrey's rule of conditioning are

contents. The syntactic view would enable us to take

introduced and discussed. The relations between

care of the origin of each piece of information in a

Spohn's model and possibility

revision process. For instance let us consider the
simple case of two knowledge bases (not pervaded

theory are

emphasized

and Spohn's updating rule is contrasted with the

Jeffrey-like rule of conditioning in possibility theory.

1

•

One of the strong assets of probability

then we may be led to revising them in different ways

theory

for

reasoning with uncertain information is the existence
of Bayes rule of conditioning that serves as a basis for

apparently missing in alternative theories of
uncertainty such as belief f unctions and possibility
theory, despite the existence of conditioning notions

I

that are more and more discussed currently. Especially

I

Bayesian inference is debatable. In this paper we make

I

c o n ditioning

whether Dempster rule

and other symmetric

combination rules can serve as a substitute to

a step toward addressing these problems by proposing
alternative updating rules that extend the notion of
while

pres erving

the

intrinsic

dissymmetry of the process of updating k nowledge
bases.

Here knowledge bases are considered from a

I

se mantic point of view. This means that their

I

uncertainty modeling framework. In the following,

1

ESPRIT Basic Research Action number 3085, entitled

contents are supposed to be represented by a unique

weight distribution on a suitable universe, in a given
Shafer (1976)'s basic probability assignments, Zadeh
This work was partially supported by the European

"Defeasible Reasoning and Uncertainty Management
Systems" (DRUMS).

with uncertainty) {A, A�B) and {A,B) which are
semantically equivalent since they have the same set
of consequences, namely the consequences of A A B ;

Belief updating in an uncertain
environment

an efficient theory of belief updating. This feature is

I

Henri PRADE

when receiving the new information -.A. Although
more simple, the semantic view raises problems

which are the topic of this paper. Indeed, most of the

available combination rules (e.g. Dempster's rule,

Zadeh's min-combination) are symmetrical ; thus if
we u s e them f or combining the distribution

expressing the contents of the whole knowledge base
with the one representing the new information, we
consider the old and the new information at the same

level, which is debatable.

The next two sections discuss conditionalization
operations for updating purposes in Shafer's evidence

th eory and Zadeh's possibility theory respectively.

These operations are not symmetrical and can be
regarded as analogues of Jeffrey (1965)'s rule in

probability theory. Then it is shown that Spohn's
ordinal conditional functions are equivalent to
possibility distributions and then conditionalization
operations introduced by Spohn can

be compared

to

the ones studied in the two other frameworks (since

mathematically speaking, possibility measures can be

regarded as particular cases of Shafer's plausibility
functions).

2

•

Updating in Shafer's evidence theory

In probability theory conditioning is defmed by
,

Bayes' formula

308

I
I

P(B I A) =

P(A n B)
P(A)

(1)

where P( ) denotes the prior probability, A is
observed (with complete certainty) and P( I A)
denotes the a posteriori probability measure, taking A
for granted. A and B are supposed to be subsets of a
referential set 0.
Jeffrey (1965)'s rule extends Bayes' conditioning to
the case where the observation is pervaded with
uncertainty. Let a be the (probabilistic) certainty with
which A is observed, and thus 1 - a corresponds to
the certainty that A is actually observed. Then the
updated probability measure P' is defmed �
P'(B) =a· P(B I A)+ (1 -a) · P(B I A) (2)
·

·

where P(B I A) and P(B I A) are given by (1). This
expression is generalized to the case where the
possible observations A1, ..., An make a partition,
and where ai is the certainty of having observed At.
(with 2;=1,n ai = 1), by
P'(B) = Li=l,n ai · P(B IAi)
(3)

In his book Pearl (1988) tries to cast this rule within
the classical Bayesian framework, noticing that a i
could be interpreted as a conditional probability
P(AiiE) where E denotes the event producing an
uncertain observation. Then P'(B) is of the form
P(BIE) provided that E and B are conditionally
independent given Ai, for all i. See Shafer (1981) for
a comparison with Dempster's rule of combination.
Nevertheless let us recall that the linear convex
com b ination i s the unique way of com bining
probability measures in an eventwise manner (the
same combination law applies for each event) which
leads to a probability measure as a result (Lehrer &
Wagner, 1981 ; Berenstein et al., 1986). "Thus the
expression {3), whatever its other justifications, is not
at all surprising.
Lastly note that if n = {ro1, ... , roml and Aj =
{roi}, 'V i = l,m, ai = P2({ro i}) for a probability
measure P2, then Jeffrey's rule (3) comes down to a
simple substitution of the prior probability P by the
uncertain observation P2, i.e. P'(B) = P2(B), 'VB,
since P(B I { roi}) = 1 if roi e B, and 0 otherwise.
Let us now consider Shafer's evidence theory. In
this framework the available knowledge is represented
in terms of a basic probability assignment m, which
is a set function from the set of subsets 2° of a so­
called frame of discernment n to [0,1] w ith the
constraints m(0) = 0 and LA m(A) = 1. The subsets
A � n such that m(A) > 0 are called focal elements.
Note that there is no constraint on the structure of the
set ff of focal elements (here supposed finite and
which does not make a partition in general). Let us

emphasize that the pair (ff ,m) can be viewed as a
random set (Goodman & Nguyen, 1985 ; Dubois &
Prade, 1986a). This means that each focal element Ai
represents the most accurate description of the reality
with certainty m(Ai). The subsets Ai are the possible
realizations of the observation perv aded w ith
uncertainty. Due to the incomp leteness of the
available information, A i is not nece ssar ily a
singleton. A plausibility function PI as well as a
belief function Bel can be bijectively associated with
m (Shafer, 1976) and are defmed by
(4)
Pl(B) = L A:AnB� m(A)
Bel(B) = 1 - PlcB) = L0;eA�B m(A)

(5)

In terms of plausibility functions, Dempster rule of
conditioning is expressed by
Pl(A n B) ; Bel(B
I A) = 1 - PlcB I A) (6)
Pl(B lA) =
Pl(A)
This rule of conditioning can be justified on the basis
of an axiom that defines a conditional function
associated to any set-function f defined on n as
follows (Cox, 1946):
(7)
f(A n B) = f(A I B) • f(B)
which expresses that the degree attached to A n B is a
function • of the degree attached to B combined with
the degree attached to A, given that B is taken for
�ted. It is well known that the Boolean structure of
20 forces • to be a product up to an isomorphic
transformation, when • is strictly monotonic in both
places (e.g. Cox, 1946 ; Aczel, 1966). Note that (7)
justifies Dempster's conditioning rule as well as the
geometric rule of conditioning (Suppes & Zanotti,
1977 )
Belg(A I B)=Bel(A n B)· Plg(A I B)=l-Belg(A I B) (8)
Bel(B)

In terms of basic probability assignments, PI( I B)
defmed by (6) is obtained by transferring all masses
m(A) over to A n B, followed by a normalization
step, while Belg( I B) is obtained by letting
m (A I B) = m(AJ if A� B and 0 otherwise, followed
·

I
I
I
I
I
I
'I
I
I
I
I
I

·

8
by normalization, i.e. a more drastic way of
condition ing (see Dubois & Prade, 1 986 b).
Dempster's rule of conditioning looks more attractive
from the point of view of updating since Pl(A I B) is
undefined only if Pl(B) = 0 (i.e. B is impossible)
while Belg(A I B) is undefined as soon as Bel(B) = 0
(i.e. B is unknown). This unability to update with a
vacuous prior is very counterintuitive, with the
geometric rule.
Another approach to conditioning has been
proposed by De Campos et al. (1989) and Fagin &
Halpern (1989) under the form

I
I
I
I
I

309

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

P*(A I B)=
P��<(A I B)=

P:..:::l(A�f'I_;B
:...) .:: ­
__..:..
=Pl(A n B)+ Bel(A n B)
Bel(A
Bel(A

n

n

(9)

B)

B)+ Pl(A

n

(10)
B)

These definitions are justified by interpreting belief
and plausibility functions as lower and up per
probabilities, since it has been proved that
P*(A I B)= sup{P(A I B) I P e �(Bel)}
(11)
P��<(A I B)= inf{P(A I B) I P e �(Bel)}

(12)

where £P(Bel) = {P I Bel(A) ::;:; P(A) ::;:; Pl(A), '\1A}.
These conditional functions are actually upper and
lower conditional probabilities and have been
considered by Dempster himself. A lthough very
satisfying from a probabilistic point of view, these
defmitions lead to a rather uninformative conditioning
process since P*( I B) � Pl( I B) � Bel( I B) �
P . ( I B). Especially, complete ignorance is
obtained (P*(A I B)= 1, P.(A I B)= 0) as soon as
Bel(A n B)= 0 and Bel(A n B) = 0, i.e. as soon as
the conditioning set B refines the granularity of the
prior evidence by producing smaller focal elements. In
that case the updating process corresponds to oblivion
rather than learning.
Although difficult to justify from the point of
view of upper and lower probability, Dempster rule of
·

·

·

·

conditioning is more informative (increasing the
precision of focal elements is permitted). Moreover
this rule can be viewed as the intersection of the
random set underlying Bel, and the conditioning set,
i.e. it is completely justified from the standpoint of
random sets and corresponds to a conjunctive set­
theoretic operation. Then normalization is justified if
the conditioning set must be taken for granted. Note
that from the point of view of belief functions, the
upper-lower probability view makes no sense just
because belief functions are supposed to reflect a
degree of certainty that uses a convention differing
from probability functions (Bel{A) = 1 means
certainty, Bel(A) = 0 means uncertainty) and that is
not viewed as a lower probability (although from a
mathematical point of view it is so). This point, i.e.
that any set function can be used to represent certainty
(up to further foundational issues) without referring to
an unreachable probability function has often been
overlooked by belief function opponents. Belief
functions can be used as a model for evaluating
certainty (this view is advocated by Smets (1988)) or
as a model for capturing imprecision in probability
(this view is that of Fagin & Halpern (1989), among
others). Adopting the first point of view, Dempster
rule of conditioning can be justified from a set of
intuitive axioms (e.g. Cox conditioning axioms

(Dubois & Prade, 1988b) or, the approach by Smets
(1988)) that never uses the set of probabilities
underlying the mathematical model of the belief
functions.
Dempster rule of combination can be defined as a
normalized intersection of two independent random
sets (ff1,m1) and (ff2•mz)
1
z m 1 (A 1) . mz(A2.) (13)
m(B)=[m ESmz](B )='LAtr'�A =B
lA1nAz;t0 mt<At) mz(A2)
·

This rule has been justified by Smets (1988, 1990)
from axiomatic arguments. When the random set
(ff2•m2) associated with m2 reduces to the ordinary
subset A, i.e. m2(A) = 1 and V A' :F. A, m2(A ')= 0, it
can be easily checked that (13) and (4) give (6). Thus
(13) extends (6) to the case of an uncertain
observation represented by (ff 2.m2) , but in a
symmetrical manner. This is unfortunate from an
updating point of view. Indeed Dempster r ule
embodies the combination of information from
parallel sources that play the same role, while the
notion of updating is basically dissymetrical : new
information does not play the same role as a priori
information. A non-symmetrical extension of (6) in
case of uncertain observation, in the spirit of Jeffrey's
rule (3), is provided by the formula
Pl(B I (ff2.mz))= LAd"l m2(A) Pl1(B I A) (14)
·

Pl 1(A n B)

. The expression (14)
PI1 (A)
can be interpreted in the following way : the subset A
is the accurate description of what is observed with
where PI1 (B I A)=

.

probability m 2 (A) and (14) is nothing but the
expected plausibility of B given the uncertain
observation. Formula (14) was suggested by Dubois
& Prade (1986b; p. 140) up to a normalization factor
and further discussed by lchihashi & Tanaka (1989)
among different alternatives to Dempster's rule. (13)
and (14) coincide when the normalization factor of
Dempster rule is 1.
The counterparts of ( 14J in terms of functions m
or Bel can be easily obtained since it can be checked
that the convex combination Ii= 1 n a i Pli with
= l ,n ai = 1 corresponds to the plausibility function
generated by the basic probability assignment =1,n
ai mi and is the dual, in the sense of (5), of the
belief function defined by Li=l,n ai Be� (where
as well as Pli is defined from mi). Thus we have
m(B I (ff2.mz)) = L Ad"l m2(A) m1(B I A) (15)
·

li

l:.j

·

·

Be�,

·

where

m1(B I A)=

(_l_)
l>lt(A)

·

�B=CnA rnt(C)

(16)

is the normalized basic probability assignment
associated with Pit ( I A) (indeed LBO} m l (B I A)=
·

310

I
I

1). In tenns of belief functions we have
Bel(B I (fF"2,m2)) =LAO} m2(A) Bell (B I A) (17)
·

where

Belt(B u A)· Bel1(A)
Bel}(B 1 A)=
1- Bel1(A)

(18)

It can be easily checked that (14) or (17) reduce to

Jeffrey's rule when (ff l •m l) defines a probability
measure (i.e. fF 1 only contains singletons of Q) and
(fF2,m 2) is such that ff2 is a partition of n. Wagner
(1989) has recently established that the only

eventwise combination of plausibility (or belief

functions) is the linear convex combination, as it is
the case for probability measures. This is a fonnal
justification for (14) or (17), since as soon as we have
in mind the random set view of a basic probability

assignment, it is natural to require that the
(plausibility or belief) function conditionalized by a

random event depends only on the different conditional

functions induced by the different realizations of this
event.

It is important to point out that conditioning is

meaningful only when observa tion does not

completely contradict a priori knowledge. This is the
case for Bayes rule where P(B I A) is defined only if

P(A) > 0 or for Dempster rule of conditioning where
Pl(B I

A) is defined only if Pl(A) > 0. This is still the
case for Jeffrey's rule where in (3) we should have
P(A·)

0 a s soon as ai > 0, as well as for its

>

(8) which is defined only if V'A,
0, 3C, m 1(C) > 0 and A (i C * 0 (i . e.
P11(A) > 0). Note �h at � e m �s � er ' s rule of
.
.
combination is less reqwnng smce It JS still defined
when 3A, m2(A) > 0 and Pl 1(A) = 0 (provided that it
is not blle for all A) ; it may seem a bit disturbing
exte ded version

�

m 2(A) >

it allows that the new information states, as
somewhat probable, something which was held as
certainly false according to previous infonnation. We
now examine the difference in behaviour of
Dempster's rule of combination and of the extended
Jeffrey's rule on a small example.
since

Example:

Let S:� =

(A1.B1}

I

while the extended Jeffrey's rule gives

1...:..Jl.. = 1 if a :;t: 1 ;
1 (X
B2 I B2) =a; m(B 1 (i B2 I B 2) = 1 ·a

m(Bt n A 2l A 2)

m(A1 n

and finally

=

•

m(Bt n A2l (fF2.mz)) �; m(At n Bzl (�z.mz))=
a(l - p) ; m(B 1 (i B 2 1 (fF2.m2))= (1 - a)(l - p).
=

As it can be seen on this example, and easily

proved in the general case from (15)-(16), the basic

probability assignments obtained by Dempster's rule

and the extended Jeffrey's rule have exactly the same

focal elements but their weights are different.
Moreover the extended Jeffrey's rule gives a non­
symmetrical result as expected. When a = 1 this latter

rule does not apply since then fF 1 = lA1} and one of
the focal elements of fF2, namely, A2 is such that
Pl 1 (A 2) = 0 due to A 1 n A2 = 0. In this case
Dempster's rule gives m(A l (i �) = 1 whatever the

value of � (provided that � * 1), i.e. a conclusion
which is not pervaded with uncertainty in spite of the

fact we may have a strong conflict between ff1 and
fF2 if� is close to 1 (then 1 - a.� is close to 0). Also

in the example, the extended Jeffrey's rule looks more

robust partly because it does not apply when the
b e haviour of Dempster's rule is particularly

questionable, and also because the normalization is

performed in a global way

in Dempster's rule, while

in the other case it takes place at the level of each

focal elements of the body of evidence corresponding
to the uncertain observation upo n which w e

conditionalize.
Figure 1 provides a summary of the relationships
between the various rules. It is clear that we can
define a Jeffrey-like extension of the geometric rule
(1 7) the
A)), a n d more
generally, definitions like (14) or ( 17) are compatible

mentioned above (by substitut in g in
alternative definition of B el 1 (B I

with the use of
case of a

other

definitions of conditioning

in

sure observation.

Bz * 0.

Dempster's rule yields m = m 1 $

mz with
1
(
a
�) : m(B1 n A2) =
m(At n B2)= ·
1-a·�
)_(1. a)(l-�)
(1-a) . � ; {Bt (i
•

1-a·�

m

B2

1-a·�

I
I
I
I
I
I
I
I
I
I

•

:�; 0 : A2 (i

I

I

wi� m1(A1) = a and

m1(B1) = 1 a, ff2 = {A2.B2} wtth m2(A2 ) = � am
m2<B2) = 1 -�·Let us assume that A1 "' A2 = 0 :
A1 (i B1 :�;0; A 1 n Bz :�;0; B1 n Bz :�;0; Az n
B1

I

Exlaldcd
Jeffrey's rule (14)

Jeffrey-like
eJlmSionof
geometric Nle

Fiwre 1 : GeneralizationsofBayes' rule

I
I
I

311

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

3

•

Updating in possibility theory

Possibility measures and necessity measures are
respectively particular cases of plausibility functions
and belief functions when the focal elements are
nested. However the linear convex combination of
possibility measures (or of necessity measures) does
not yield a possibility measure (or a necessity
measure) generally (see Dubois & Prade, 1986a).
Thus the approach presented in the preceding section
cannot be appl ied to possibility and necessity
measures for updating them under an uncertain
observation (expressed in a possibilistic way), if we
further require that the result be a possibility measure.
Let us first recall that a possibility measure I1 over .0
can be defined, through a so-called possibility
d istri buti on 1t, which is a function from .0 to [0,1],
by the formula (Zadeh, 1978)
(19)
'r/ A!;;;; .0, Il (A) = SUProe A 1t(ro)

where 1t(ro) estimates to what extent it is possible
( i.e. compatible w ith w hat i s known) that ro
corresponds to the true state of the reality in n. In
other words 7t restricts the more or less possible states
of the reality giv en the available i ncomplete
information about the reality 7t is supposed to be
normalized, i.e. 3 ro e n, 7t(ro) = 1 in other words,
.0 is supposed to be an exhaustive set of alternatives,
one of which is completely posible. To fl is
associated a necessity measure N, by duality, namely
'r/A�. N(A)=1-Il(A) = infroi! A [1-7t(ro )]
(20)

Recently, it has been shown (Dubois & Prade, 1990)
that the only way of combining possibility measures
rr1• .... Il n into a possibility measure rr. in an
eventwise manner, was a max-combination of the
form
'r/A, Il(A) = max( f1 (I1 1(A) ), ... , fn(Iln(A))) (2 1)
where fi is a monotonically increasing function such
that fi(O) • 0, 'v'i and 3j, fj(l) • 1 which modifies the
shape of the possibility distribution 7ti underlying IIi·
An example of admissible possibility consensus
function is the weighted maximum operation, i.e.
Il(A) = maxj=l ,n min(Aj, Ilj(A))
(22)
with maxj=1 n Aj = 1, where Aj represents the relative
importance ,of the source yielding llj (Dubois &
Prade, 1986c). However, in (22), the minimum can be
changed into a product, or into the linear operation
max(O, a + b - 1), and more generally into any
operation • with 1 • 1=1, 0 • 1 = 0 = 1 • 0, and
increasing in both places�
In fact, the weighted max-combination is the
counterpart in possibility theory of the linear convex
combination in probability theory ; the weighted

max-combination can be interpreted in the
possibilistic framework just as the con v ex
combination can be interpreted in term s of
probabilistic expectation. This leads to the following
updating form ula for an a priori possibility
distr ibution 1t 1 in the face of a new p iece of
information in the form of another possibility
measure 1t2 . It can be expre ssed in term s of
possibility distributions
(1ttl1t2](ro) = supae( 0,1] min(<X,7tl (roiB2a)) (23)

and in terms of possibility measures
[fliifl2J(A) = SUPae (0,1] min(<X,I1I(AIB2a.))

(24)

The observ ation (per vaded with uncertainty)
represented by 1t2 is here viewed as a weighted family
of observations in the usual sense, i.e. the weight a
reflects to what extent B2a. is an admissible crisp
representation of the fuzzy set B2 (such that J.Ls =
2
1t2). Namely we have (Zadeh, 1971)
J.LB2(ro) = supae(0,1] min(<X,J.LB 2a. (ro))

(25)

where B2 a is the a-level-cut of B2 , namely B2a =
(ro e .0, Jl� (ro) � a}. A "sup" in (23)-(24) is used
a
since, in the general case, there an infmite number of
distinct level-cuts B 2a . The counterpart of formula
(24) in terms of necessity measure is
[N1 II12](A) =
infa.e(O,l] max(l- a., N1 (A I B2a)) (26)
In other words {(B2a. ,a) I a e (0,1]} can be viewed
as a basic possibilistic assignment, and a is indeed
the possibility that the "possibilistic set" attached to
1t2 , ( just as a random set is attached to a basic
probability assignment) is precisely equal to B2a· It
is worth noticing that the expressions defining
[n 1 I 7t2 ] or CI11 I n2 J are integrals in the sense of
(Sugeno, 1977) when ro or A is fixed, just as Jeffrey's
rule viewed as an expectation, is an integral in the
usual sense (in a finite setting). Observe that the rule
of conditioning gives the maximal importance to the
core of the fuzzy set B2 (the set of elements with
membership 1) which, being the smallest level cut, is
the more informative, and less and less importance
when level cuts become larger (a.<� � Bza�BzR).
We have now to give the Bayes-like rufe of
conditioning in possibility theory, i.e. a possibility
distribution 7t1 conditionalized by B results in the
possibility distribution 7t( · I B), defined in �K:Cordance
with Dempster rule of conditioning, by
1t1 (ro)
(27)
7t(ro I B) = -- if ro e B
llt(B)
= 0 otherwise
where I11(B ) = sup00e B 1t1 (ro)and

312

I
I

(

B)

SUProeA 1tt(ro)-11I Ar1
ll(AIB)- 1
Ili(B)
ll1(B)

(28)

together with N(A I B)= 1 - il(A I B). See (Dubois &
Prade, 1988b) for a discussion of conditioning in
possibility theory and a justification of this formula.
Note also that the rule of conditioning in possibility
theory is a particular case of the more general
symmetric rule of combination (see Dubois & Prade
(1988a), Shafer ( l987) for instance)
1t1 (ro) * 1t2(ro)
1t(ro) =
(29)
sup00e .a (1tl (ro) * 1t2(ro))

where * is a conjunctive operation which is
symmetrical, non-decreasing, and such that Vae [0,1],
a * 1 = a. This rule is the possibilistic counterpart of
Dempster rule of combination, while (2 4) plays the
same role with respect to Jeffrey's updating rule.
Introducing (27) into (24) leads to the following
updating formula

[1t1 11tz](ro)=

1tt(ro)
SUPae (0,1] min(a, fl CBza)

1

•

�'Bza(ro))

(30)

Observe that when a decreases, 111(B2 a) can only
1ti(ro)

increase (since B2a become larger) and thus=--Il t <Bza>
can only decrease. Moreover, �Bz (ro) = I only if

a S 1t2 (ro) and 0 otherwise. Henc� the suprenum in
(30) is attained for a = 1t 2(ro ). The updating fonnula
can thus be expressed in a more compact way :
1t (ro )
l
[1ttl1t2](ro) = min(1t2(ro},
) (31)
where B2

llt<B2 �(ro))

1t2( ro) (ro' 1 1t2(ro ) 2: 1tz(ro)}. The effect
o f this updating formula is pictured on Figure 2.
=

'

exist ro such that 1t1(ro) > 0 and 1tz(ro)

>

0).

Another important propeny is that [1t 1 I 1t2] =
min(1t 1 · ': 2 ) wh�n the c�res of 1t 1 and 1t 2 are
.
. similar
to the coincidence
overlappmg. Thts IS
between the extended Jeffrey's rule and Dempster rule
when no normalization factor is necessary in the
latter. This is well in accordance with the fact that if
the available information is of the fonn x e A, then
upon arrival of a sure piece of information x e B, the
updating process consists in producing x e A n B.
More generally the denominator IT1(Bz1t (roY in (31)
helps producing a normalized result on e basis that

�

1t2 is considered as certain, in the spirit of
conditioning.
Let us examine the panicular case where
1tz(ro) = max(J..Ls(ro). A)
where B is an ordinary subset of .0. Then it means
that B is completely possible (Ilz(B) = 1) and there is
a possibility equal to A that the observation is outside
B (ll2 (B) = A). In that case the result of the
conditionalization of 1t 1 by the uncertain observation
represented by 1tz is given by
1 (ro)
,IJ.B (ro)),min(A,1tl (ro)))
[1tl 11t2](ro)=max (min(�
Tit(B)
This is illustrated by Figure 3, where we see that in
that case the result

A.t---�....;
-��---�
o��---0

Figure 3

and 1t 2, which is
renormalized over the subset B only. As expected,
[ll1Jll2 J(B) = A, a nd the combination is
dissymetrical ; i t favors the new information (1t2 )

is the i n te r s e c t i o n of 1t 1

over the old one.

Fi�MJ<2

It is worth noticing that [1t1 I 1t2] is normalized as
soon as the core of 1t2 overlaps the suppon of 1t2 (i.e.
3 ro e n, 1t2(ro) = 1 and 1t I ( ro) > 0). This is
satisfying ; if it is not th e case, it would mean that
the main pan of

B 2 focuses on values which are

conditionalization

is debatable. The conditionalization

completely impossible according to 1t 1 and then the

becomes completely undefined in case of total
conflict between 1t2 and 1t 1 (i.e. when it does not

4

•

Possibility theory and Spohn's ordinal
conditional

functions

An ordinal condi tional

a function

function (OCF for shon) is

K from a complete field of propositions

into the class of ordinals. Here, for simplicity we

consider a function from a finite Boolean algebra B to

the set of natural integers n. This Boolean algebra
consists of a family of subsets of a universe .0

indo� by a fmite �tion {A1, ... , Am } of n. By
defininon an OCF verifies the following properties

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

31 3

I
I
I
I
I
I
I
I
I
I
I
I
I

I
I
I
I
I
I

(Spohn, 1988a,b) :
i)

ii)
iii)

V'i, V'ro, ro' e

C

Aj. K(ro) = K(ro ')

c A and

C overlaps A but is not contained in A,

where C is the core of 1t 1.

3 Ai £:. 0, K(Aj) = 0

V' A£:. 0, K(A) =min { K(ro) I ro e A}.

It is easy to see that the set function NK defmed by

NlC(A) = 1 - e-K(A) is a necessity measure, with
values in a subset of the unit interval. Moreover

because K(A) e [N, N K(A) <
the
is
{K(ro) I ro e n}

1, V' A

:t:. 0. The set

counterpart

a

of

possibility distribution 1t on n, such that Il(A) =

max{1t(ro) I roe A}. Namely, let Il (A)=l-NK(A)=
K
K(A)
, it is easy to check that 1tK(ro) is equal to
ee- K(ro), where 1t K is the possibility distribution

associated with IlK. K(ro) can be viewed as a degree of
impossibility of ro, and K(A) = 0 means A is

completely possible. Since K(ro) e [N, 1tK(ro) > 0 for
all ro's, i.e. nothing is considered as fully impossible
in Spohn's approach.

Spo hn (1988) also intro d uces conditioning

concepts, especially :

- the A-part of K such that

2

(3 )
V' roE A, K(ro I A) = K(ro) - K(A)
- the (A,n)-conditionalization of K, say K(ro I (A,n))
defined by

K(ro I (A,n)) = K(ro I

A) if roe A
= n + K(ro I A) if roe A

A

(33)

It is interesting to translate this notion into the
possibilistic setting. Definitions

respectively become
1tK(ro I

A) =

1tK(ro) .
tf roe

--

IlK(A)

1tK(ro I (A,n))

=

1tK(ro)
IlK(A)

n
=e- ·

(3 2 ) and (33)

(34)

A

if roe A

1tK(ro)
--

if ro E

A
..
---.,..._.;
c

(35)

--- 711
7t 2

IlK(A)
(34) corresponds to
Dempster rule of conditioning but (35) does not

correspond to Dempster rule of combination although

(34) is

(35) for n � +oo (it says
A is considered as impossible). In fact (34)

a particular case of

is exactly the Bayes-like conditioning of possibility
measures, i.e. equation (27).

In order to compare Spohn's rule to the
possibilistic updating rule, we let a = e-n and note
that it comes down to updating a possibility

1t = 1tK on the basis of an uncertain
L
Il (J\) = a (or equivalently N(A) = 1 a
in terms of the degree of certainty). A comparison of
the two rules is given in Figure 4 for the three
mutually exclusive and exhaustive cases A n C = 0,

distribution
observation

Spohn
Possibilistic Nle

FiMC4

A

It is trivial to check that

then that

-

-

Some comments are in order. First, the two rules

apparently produce very similar results, except that

S� hn's rule retains the shape of 1t 1 on A more
fatthfully than the possibilistic rule. The difference

can be attenuated on the example, by substituting a

product rule to the minimum rule in

(31). However

the comparison is limited by t h e fact that the

possibi listic rule applies in more cases than Spohn's

rule does. In order to make a m o re extended

comparison, Spohn's rule must be generalized to the

possibility distribution 1t1 by another
1t2. (34) and (35) can be straightforwardly
extended to a partition {A1, A2, ..., A n} of n, with
ai = 112(<\) as follows :
1tt(ro)
forroekl• i=l,n (36)
x( roi{(Aj,llj_)li=1,n})=a;
Il t(Ai)

� of a

u�tin

distnbuuon

·

3 14

I
I

with the condition max i a. i

=1

(normalization of

Ih). (34) and (35) are obtained letting A1 =A, a.1 =
1, A2 = A,

a.2

=

a..

A

special case of

(36)

1t2(ro)

gives

instead of the a. 's. Then
i

n(ro ln2)

=

(36)

n2(ro ), 'V ro e .a

i.e. Spohn's rule comes down to changing
in a systematic way, for all

ro

such that

ro e .Q).

the theories. Th e d if f erence between the

possibilistic rule and Spohn's rule has been studied,

obviously

conditional function into a possibility measure. This

using a scale transformation that changes an ordinal
examination has pointed out that there may be two
kinds of updating rules : the ones that preserve the

1t1 into 1t2
1t1(ro) > 0

(and Spohn's assumptions are such that this is the

case for all

to

is when

we choose the set of singletons of .Q as the partition,

and

the sake of preserving closure conditions with respect

This is exactly the same as for

Jeffrey's rule in the probabilistic setting.

available knowledge while possibly refining it, the
ones that system atically consi der the a priori
knowledge as being kind of obsolete in the face of
new information. The possibilistic rule and Spohn's
rule correspond to the flrst and the second point of

The difference between the two rules becomes

view respectively. Interestingly Shenoy

(1989)

has

patent Spohn's rule always strongly accounts for the

recently discussed Spohn's rule in comparison with

new information, possibly forgetting the old one,

Dempster rule of combination ; our paper and his are

even if it was more precise (i.e.

1t2

�

n1).

On the

thus complementary in this respect.

cont rary the possibilistic rule al ways tries to

Lastly the introduced rules have been constructed

precisiate the new information by means of the old

on the basis of analogy with Jeffrey's rule. But the

one when it applies. Particularly we have the

latter can be justified in terms of principles of

following noticeable property : if

minimum change; namely the result of Jeffrey's rule

possibilistic rule gives

is the probability distribution obtained by minimizing

nz � 1t1 then the
[n1 lnz] =n1 (while Spohn's
rule prefers n2). Indeed if nz � 1t 1 then
TI 1 <Bzn2(ro y=1, 'V ro, in (31). Particularly if in the

above example where

TI2(A) =a., if n1

then the possibilistic rule would leave

� max(I.1. ,a.)

1t1

A
unchanged

because TI1(A) < a., i.e. the input information is too

weak to question the available knowledge. This

behavior, i.e. productive updating when the new
information

is

not already entailed by the old one is

very natural since, when n2 �

that 1t2
tells the same as n1 but is only weaker. This is
exactly what happens when in a propositional
knowledge base a redundant proposition is added.
Spohn's rule (extended by (36)) v iolates this
requirement ; it corresponds to the idea that the new
information must be kept even if it leads us to forget
something that is already known. Conversely, if n2 s;
1t , then both rules produce n2 as the updated
1
knowledge, because n2 is completely consistent with
1t1 but more precise.
1t 1,

it means

Concl u sion

In this paper, we have demonstrated that there are
t w o kind s of rules for the combination of
information : symmetric rules (that combine sources
in parallel) and dissymmetric rules that correspond to
the idea of updating. Dempster rule and fuzzy set
intersections are among the fllSt kind of rules while
Jeffrey's rule, is of the other kind, as well as the rules
proposed in this paper for belief functions and
possibility measures. These two kinds of uncertainty
measures lead to different updating formulas only for

the relative entropy with respect to the prior
probability, using the uncertain observations as
constraints (Domotor,

1985). A

further topic of

research would be to look at information theoretic
justifications of rules introduced in this paper,
following the methodology outlined in a previous
paper

(see Dubois & Prade, 19 87 ) , for instance using

information closeness indices recently introduced by

Ramer (1989)

in the possibilistic

setting.

