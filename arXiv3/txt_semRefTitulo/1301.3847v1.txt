

We present a new approach for inference in
Bayesian networks, which is mainly based
on partial differentiation. According to
this approach, one compiles a Bayesian net­
work into a multivariate polynomial and
then computes the partial derivatives of
this polynomial with respect to each vari­
able. We show that once such derivatives
are made available, one can compute in
constant-time answers to a large class of
probabilistic queries, which are central to
classical inference, parameter estimation,
model validation and sensitivity analysis.
We present a number of complexity results
relating to the compilation of such polyno­
mials and to the computation of their par­
tial derivatives. We argue that the com­
bined simplicity, comprehensiveness and
computational complexity of the presented
framework is unique among existing frame­
works for inference in Bayesian networks.

1

Introduction

A

B
true
false
true
false

¢

.03
.27
.56
.14

Pr ( a, b), etc.1 We can do this in the usual way.
To compute Pr (e) we simply identify the rows in
the table which are consistent with e and sum up
their probabilities. Alternatively, we can compile
the probability table as follows. We parameterize
the table as shown on the right of Figure 1, where
we introduce what we call evidence indicators into
each row. We then add up all rows to generate the
multivariate polynomial:

which is depicted graphically in Figure 2.

Consider the probability table on the left of Figure 1
and suppose that our goal is to compute probabili­
ties of events with respect to this table, say Pr ( a ) ,

true
true
false
false

Figure 2: A probability-table compilation.

A

B

true
true
false
false

true
false
true
false

¢

AaAb.03
AaAij.27
AaAb.56
AaAiJ.l4

Figure 1: Probability table and its parameterization.

Now, given any evidence e, we can compute the
probability of e by a simple evaluation of polynomial
:F. We consider each indicator Ax. If x is consistent
with evidence e, we set the value of variable Ax to 1.
Otherwise, we set its value to 0. We then evaluate
the polynomial :F, where its value is guaranteed to
1

We are using the standard notation:

denoted by upper-case letters
lower-case letters

(a).

(A)

variables are

and their values by

Sets of variables are denoted by

bold-face upper-case letters

(A)

and their instantiations

are denoted by bold-face lower-case letters

variable

A= true

A

with values true and false, we use

and a to denote

A= false.

(a) .

a

For a

to denote

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

124

Evidence

a,b
a,b
a

true

e

Aa
0
0
0
1

,\b
0
1
1
1

,\b
1

Aa
1

0
1
1

1
1
1

validation [4] and sensitivity analysis [15, 2, 3]
in constant-time once the partial derivatives of
the compiled polynomial are computed.

3. We show the following on the complexity of
computing the derivatives of such polynomials:

Figure 3: Examples of evidence indicators.

(a) first partial derivatives, 8:Fj8>.x and
8:Fj8Bc, can all be computed simultane­
ously in time linear in the size of polyno­
mial :F, that is, in O(nexp(w)) time.
(b) second partial derivatives, such as
82:Fj8>.x8Bc, can all be computed simul­
taneously in O(n2 exp(w)) time.
Following are examples of the queries that can be
answered in constant time once these partial deriva­
tives are computed:

Figure 4: Evaluating a probability-table compilation
under evidence e = a.

1. the posterior marginal of any network variable
X, Pr(x I e);

be the probability of evidence e. All we are doing
here is using the indicators to select which rows of
the probability table to add together. Figure 3 de­
picts some examples of evidence and the correspond­
ing values of indicators. Figure 4 shows an example
evaluation to compute the probability of a.

2. the posterior marginal of any network family
{X} U U, Pr(x, u I e);

The previous compilation technique can be applied
to Bayesian networks, allowing one to generate fac­
tored polynomials that are not necessarily exponen­
tial in the number of network variables. We have
promoted the compilation of Bayesian networks in
earlier work, where we referred to polynomials such
as :F as Query-DAGs [6]. Our original motivation
for compiling Bayesian networks was to simplify in­
ference systems to the point where they could be
implemented cost-effectively on a variety of software
and hardware platforms. This paper is based on a
number of new results, which make this notion of
compilation much more interesting and useful than
was originally conceived:

1. We show how to compile a Bayesian network
into a polynomial that includes two types of
variables: evidence indicators Ax and network
parameters Or. Given a variable elimination or­
der of with w and length n, we show how to
compile the polynomial in O(nexp(w)) time us­
ing a simple variable elimination algorithm.
2. We show how to answer a large number of
queries relating to classical inference [10, 13, 7,
21, 20], parameter estimation [19, 16], model

3. the sensitivity of Pr(e) to change in any net­
work parameter Br;
4. the probability of evidence e after having
changed the value of some variable E to e,
Pr(e- E, e);2
5. the posterior marginal of some variable E after
having retracted evidence onE, Pr(e I e-E).
6. the posterior marginal of any pair of network
variables X and Y, Pr(x, y I e);
7. the posterior marginal of any pair of network
families F1 and F2, Pr(f1, f2l e);

8. the sensitivity of conditional probability Pr(y I
e) to a change in network parameter Br;
9. the amount of change to parameter Br needed
to ensure that Pr(y I e) � Pr(ij I e).
This paper is structured as follows. We introduce
the polynomial representation of a Bayesian network
in Section 2 and explicate some of its key proper­
ties. We then introduce the partial derivatives of
this polynomial in Section 3 and present two key
theorems that explicate their probabilistic seman­
tics, followed by a number of corollaries showing how
key probabilistic queries can be retrieved from such
2

We define this notation formally later.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

derivatives. Section 4 presents a simple variable­
elimination algorithm for computing the polynomial
representation of a Bayesian network while present­
ing structure-based guarantees on its time and space
complexity. Section 5 presents an algorithm for eval­
uating the polynomial and for computing its partial
derivatives efficiently. We finally close in Section 6
with some concluding remarks. Proofs of all theo­
rems can be found in the full paper [5].
2

Polynomial Representation of

125

2000

A

c/>A

true
false

.3
.7

A

B

true
true
false
false

true
false
true
false

c/>B
.

1

.9

.8
.2

Figure 5: A Bayesian network. Table ¢A provides
the prior probability of variable A and Table ¢B pro­
vides the conditional probability of B given A.

Bayesian Networks

Our goal in this section is to show how to represent a
Bayesian network as a polynomial :F and to explicate
some of the key properties of this polynomial.
We will distinguish between canonical and factored
polynomial representations of a Bayesian network.
The canonical representation is unique, has expo­
nential size, but is all we need to discuss the seman­
tics of polynomial representations and their partial
derivatives. Section 4 will then concern itself with
computing factored polynomials, which size is deter­
mined by the network topology.

the same canonical polynomial. We formalize the
notion of a quantification below.

Definition 2 A quantification e of a Bayesian net­
work is a function that assigns a value e(f) to each
instantiation f of family F.
For example, according to the quantification e in
Figure 5, we have e(ab) = .1 and e(a) = .7.
The probability distribution represented by a
Bayesian network is completely recoverable from its
canonical polynomial:

We start with some key notation first. Let F =
{X} U U be the family of variable X and let f = xu
be a corresponding instantiation. We will then use Br
and/ or Bxu to represent the conditional probability
Pr(x I u) . Moreover, we will write x ,...., f to mean
that instantiations x and f are consistent.

Definition 3 The value of indicator Ax at instan­
tiation e, denoted e(x), is 1 if x is consistent with
e, and is 0 otherwise. The value of polynomial :F
under evidence e and quantification e is defined as:

Definition 1 Let N be a Bayesian network with
variables X = X1, . . . , Xn and families F1, .. ., Fn.
Then

Consider the canonical polynomial

is called the canonical polynomial of network N,
where variables Ax, are called evidence indicators
and variables Br, are called network parameters.
As an example, the canonical polynomial of the net­
work in Figure 5 is:

F = BaBabAaAb + BaBaliAaAr, + BaBabAaAb + BaBar,AaAr,.
In Definition 1, the instantiation x ranges over
ab, ab, ab, ab; the instantiation fi ranges over
a, a, ab, ab, ab, ab; and the instantiation Xi ranges
over a, a, b, b.
Clearly, the size of a canonical polynomial is expo­
nential in the number of network variables. More­
over, the polynomial is independent of its quantifi­
cation: two networks with the same structure have

de/

F (e, e) = :F ( Ax, = e(xi),Bri = e(fi)).

F = BaBabAaAb + BaBaliAaAr, + BaBabAaAb + BaBar,AaAr,
of the network in Figure 5. If the evidence e is ab
and the quantification e is as given in Figure 5, then
F (e, e) stands for :F( Aa = 1,Aa = O,Ab = O,Ar, =

1,Ba = .3,Ba = .7,Bab = .1,Bab = .9,Bab = .8,()ab =
.2), which equals to .27 in this case.

Theorem 1 Let N be a Bayesian network specify­
ing distribution Pr and having canonical polynomial
F. Then :F(e,e) = Pr(e I e) for every evidence e
and quantification e.
A canonical polynomial is then a function of many
variables, where each variable corresponds to either
an evidence indicator or a network parameter. For
each instantiation e and quantification e, the poly­
nomial can be evaluated to compute the probability
of e given e. We will often write F (e ) instead of
:F(e, e) when no ambiguity is anticipated.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

126

Note that the canonical polynomial :F of a Bayesian
network is a multilinear function ( i.e., each of its
variables has degree one). Moreover, a monomial in
:F cannot contain two indicators of the same vari­
able; neither can it contain two parameters of the
same family. These properties are quite important
as they imply that :F is a linear function in the in­
dicators of any variable; and is a linear function in
the parameters of any family.
It is already observed in [19] that Pr(e) is a linear
function in each network parameter. More generally,
it is shown in [1, 2] that Pr(e) can be expressed as a
polynomial of network parameters in which each pa­
rameter has degree one. The polynomials discussed
in [1, 2], however, are constructed for a given evi­
dence e: they only contain one type of variables cor­
responding to network parameters, with no variables
corresponding to evidence indicators. In fact, these
polynomials correspond to our canonical polynomi­
als when evidence indicators are fixed to a particular
value. Hence, they do not represent network com­
pilations as they cannot be used to answer queries
with respect to varying evidence. Note also that
the size of such polynomials is always exponential
in the number of network variables, but we show in
Section 4 how to factor them using the technique of
variable elimination. The main insight we bring into
such polynomial representations, however, is our re­
sults on the probabilistic semantics ( and computa­
tional complexity) of their partial derivatives, a topic
which we discuss next.
3

Probabilistic Semantics of Partial
Derivatives

Our goal in this section is to show the probabilistic
semantics associated with the partial derivatives of a
network polynomial. Given these semantics, we then
enumerate the class of probabilistic queries that can
be answered once these derivatives are computed.
We need the following key notation first. Let e be
an instantiation and X be a set of variables. Then
e-X denotes the subset of instantiation e pertaining
to variables not appearing in X. For example, if
e = abc, then e - A = be and e- AC = b.
We start with the semantics of first derivatives:

------+------­
//+� /+��
.J

/\
ea

A�/\��

')..a e ab')..b eab
.3

.3

.03

eab')..b
0

.3

8:F(e,8)/8>-x

Pr(x, e- Xl8)

e��,t...
0

.7

e.
0

Figure 6: Partial derivatives are computed under
and
(>.a,>-a,>.b ,>.b)
(1,0,1,1)
(Ba, Ba,Bab,()a b,()ab,()ab) = (.3, .7, .1,.9,.8,.2).

Pr(f, el8)/8(f).

8:F(e,8)/8Br

That is, if we differentiate the polynomial :F with
respect to indicator Ax and evaluate the result at
evidence e, we obtain the probability of instantiation
x, e - X.
A similar interpretation is given to the
derivative of :F with respect to parameter Br. Note
that the second identity of Theorem 2 has indirectly
been shown in [19], given that :F( e) = Pr(e).
Figure 6 depicts a factored polynomial representa­
tion of the Bayesian network in Figure 5, evalu­
ated under evidence a, which we shall use as a run­
ning example. The partial derivatives of :F with re­
spect to each network indicator and parameter is
shown below the indicator/ parameter. For example,
8:F(a)/8Ba = 1 and 8:F(a)/ Ba = 0.
We now discuss some classical queries that can be
answered based on first partial derivatives.3
The first class of queries relates to "what-if" anal­
ysis. Specifically, after having computed the prob­
ability of some evidence e, in which variable X is
instantiated to some value x', we ask: what would
be the probability of e if the evidence on X was dif­
ferent, say, x. According to Theorem 2, the answer
to this query is exactly 8:F(e)/8'Ax· In Figure 6 for
example, where e = a, suppose we ask: what would
be the probability of evidence if A was instantiated
to a instead. That would be 8:F(e)/8'Aa = .7, which
3Recovering

probabilistic

quantities

from

partial

derivatives seems to be a standard technique in certain

statistical literatures. Th er e

,

Theorem 2 (Semantics of 1st PDs) Let N be a
Bayesian network representing probability distribu­
tion Pr and let :F be its canonical polynomial. For
every variable X, family F and evidence e:

.27

a probability distribution

is represented using its generating function, allowing one
to recover means and variances from the partial deriva­
tives of such a function.

The main attraction of this

approach is that the generating function tends to have

a closed form, allowing the derivatives to have closed

forms too-see
its benefits.

[8]

for an overview of the technique and

2000

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

is also the prior probability of

ii.

Another class of queries that is immediately obtain­
able from partial derivatives is posterior marginals:

Corollary 1 (Posterior Marginals) For X fl. E:

fJ:F(e)l8>..x
Pr (x I e)=
.
F(e)
Therefore, the partial derivatives will give us the
posterior marginal of every variable in constant
time. In Figure 6, where e = a, Pr(b I e)
(8F(e)l8>..b)IF(e) = .03/.3 = .1 and Pr(b I e) =

(8F(e)l8>..r ,)IF(e)= .27/.3= .9.

The ability to compute such posteriors efficiently
is probably the key celebrated property of join­
tree algorithms [10, 11], as compared to variable­
elimination algorithms [20, 7, 21]. The latter class
of algorithms is much simpler except that they can
only compute such posteriors by invoking themselves
once for each network variable, leading to a com­
plexity of O(n2 exp ( w)). Jointree algorithms can do
this in 0( n exp(w)) , however, but at the expense of
a more complicated algorithm. The algorithm we
are presenting based on the computation of partial
derivatives can also compute all such marginals in
only 0(n exp(w)) and seems to represent a middle
ground in this efficiencyIsimplicity tradeoff.
One of the main complications in Bayesian network
inference relates to the update of probabilities after
having retracted evidence. This seems to pose no
difficulties in the presented framework. For exam­
ple, we can compute the posterior marginal of every
instantiated variable, after the evidence on that vari­
able has been retracted immediately.

Corollary 2 (Evidence Retraction) For
variable X and evidence e, we have:

Pr(e - X)
Pr(x I e-X)

every

"' 8F(e) .
L...... 8>..X '
X
8F(e)I8Ax
Lx 8F(e)I8Ax

·

In Figure 6, where e = a, Pr(e - A)
8F(e)l8>..a + 8F(e)I8Aa = 1 and Pr(ii I e- A) =
(8F(e)l8>..a)I(8F(e)l8>..a + 8F(e)l8>..a) = .711 =
.7.
The above computation is the basis of an investiga­
tion of model adequacy [4, Chapter 10] and is typi­
cally implemented in the jointree algorithm using the

127

2000

technique of fast retraction, which requires a modifi­
cation to the standard propagation method in join­
trees [4, Page 104]. As given by the above theorem,
we get this computation for free once we have partial
derivatives with respect to network indicators.
Note that once we have the partial derivatives
oF I8Br, we can implement the APN algorithm for
learning network parameters as shown in [19]. As for
implementing the EM algorithm, we need the poste­
rior marginals over network families [16], which are
easily obtainable from the partial derivatives since

Pr(f I e, e)=
For example,
8F(a)j8(Jab

F(a)

Bab

a

F;Ce��tBr e(f).

in Figure 6, Pr(b, a

_ ·1 ·
- �
.3 · 1 -

I

a

) is

_

We now turn to the semantics of second derivatives:

Theorem 3 (Semantics of 2nd PDs) Let N be
a Bayesian network representing probability distri­
bution Pr and let F be its canonical polynomial.
For every pair of variables X =/:- Y, pair of families
F1 =/:- F2, and evidence e:
Pr(x, y, e-XY I e);
82 F(e,e)l8>..x8>..y
Pr (X' fl' e-X I e)Ie(fi);
82 F(e, e)l8>..x8Br1
Pr(f1, f2, e I e)le(fi )e(f2 ) ·
82 F(e, e)I8Br18Br2
For example, the first identity reads: evaluating the
derivative 82FIO AxOAy at evidence e and quantifi­
cation e gives the probability Pr(x, y, e-XY I e).
The significance of Theorems 2 and 3 is two fold:

1. They show us how to compute answers to clas­
sical probabilistic queries by differentiating the
polynomial representation of a Bayesian net­
work. Therefore, if we have an efficient way
to generate the polynomial representation and
to differentiate it, then we also have an efficient
way to perform probabilistic reasoning. This is
the view we promote in this paper.
2. They show us how to compute valuable partial
derivatives using classical probabilistic quanti­
ties. The partial derivatives of Pr(e) and Pr(y I
e) are important when estimating Bayesian net­
work parameters and when performing sensitiv­
ity analysis. The third identity of Theorem 3,
for example, shows us how to compute the sec­
ond partial derivative of Pr(e) with respect to
two network parameters, Br1 and Br2, using the
joint probability over their corresponding fami­
lies, Pr(f1,f ,e).
2

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

128

We have to note, however, that expressing partial
derivatives in terms of classical probabilistic quanti­
ties requires some conditions: e(f), e(fl) and 8(f )
2
cannot be 0 in Theorems 2 and 3. In the context
of jointree algorithms, this complication can possi­
bly be addressed using lazy propagation-see (12].
There is no such concern, however, if partial deriva­
tives are computed directly as we do in this paper.
Theorems 2 and 3 facilitate the derivation of results
relating to sensitivity analysis. Here's one example:

Theorem 4 (Sensitivity) Let N be a Bayesian
network specifying distribution Pr and let F be its
polynomial representation. For variable Y � E:

aPr(y I e)
8Bxu
1 _ 8 2F(e)
_
F(e )
F(e ) 2 8Bxu8>..y

(

a:F(e ) a:F(e)
8Bxu a>..y
Pr(y, x, u I e) - Pr(y I e)Pr(x, u I e)
Pr(x I u)
_

)

This theorem provides an elegant answer to the most
central question of sensitivity analysis in Bayesian
networks, as it shows how we can compute the sen­
sitivity of a conditional probability to a change in
some network parameter. The theorem phrases this
computation in terms of both partial derivatives and
classical probabilistic quantities-the second part,
however, can only be used when Pr(x I u) =f. 0.
One has to note an important issue here relating
to co-varying parameters. Let X be a variable in a
Bayesian network and let U be its parents. We must
then have L:x G(xu) = 1 for fixed u. Therefore,
when one of the parameters Bxu changes its value
from G(xu) to G'(xu), all related parameters must
also change so that L:x e'(xu) = 1 is maintained.
It is common to assume that parameters Bxu, for a
fixed u, are all linear functions of some meta param­
eter Tx. That is, Bxu= axTx + f3x, where ax and f3x
are constants and L:x axTX + f3x = 1. Then using
the generalized chain rule of differential calculus:

aPr(y I e)
aPr(y I e) jaTx = ""'
� ax 8Bxu .
X

Consider now the following three problems relating
to sensitivity analysis. We want to compute the
derivative aPr(y I e )jaTx for

Problem (1): every Y and every Tx;
Problem (2): some Y and every Tx;

Problem (3): every Y and some Tx.
Given Theorem 4, and given that we can compute all
first and second partial derivatives in O(n2exp(w))
time, it follows immediately that Problem (1) can be
solved in O(n2exp(w)) time. We also show in (18]
that the second partial derivative 8 2F(e)jaBxuOAy
can be computed in O(nexp(w)) time for every
parameter Bxu and a fixed indicator >..y , or for a
fixed parameter Bxu and every indicator Ay· There­
fore, Problems (2) and (3) can each be solved in
O(nexp(w)) time.
There seems to be two approaches for computing
the derivative aPr(y I e)jaBxu, which has been re­
ceiving increased attention recently due to its role
in sensitivity analysis and the learning of network
parameters. We have just presented one approach
where we found a closed form for aPr(y I e)jaBxu,
using both partial derivatives and classical proba­
bilistic quantities. The other approach capitalizes
on the observation that Pr(y I e) has the form
(aBxu + (3)/('yBxu + 6) for some constants a,(3, "(and
6(1]. According to this second approach, one tries
to compute the values of these constants based on
the given Bayesian network and then computes the
derivative of (aBxu + (3)/('yBxu + 6) with respect to
Bxu· See (12, 14] for an example of this approach,
where it is shown how to compute such constants
using a limited number of propagations in the con­
text of a jointree algorithm.
We now present yet another example of results that
are facilitated by the view we promote in this paper.
That is, we consider one of the key questions that
arise in real-world diagnostic applications. Suppose
we are given some evidence e and some hypothe­
sis Y. We propagate the evidence and find that
Pr(y I e) > Pr(Y I e); that is, y is the fault given
evidence e. An expert may conclude differently, that
fj is more probable in this case, hence, indicating a
problem in the Bayesian network model. Assuming
that the network structure is correct, a key question
is: which network parameters should we tweak in
order to correct the problem - that is, leading to
Pr(y I e) :::; Pr(fj I e). There is typically more than
one parameter to tweak, but the following theorem
provides key insights into answering this question:

Theorem 5 (Parameter Tweaking) Let Y and
X be binary variables in a Bayesian network N
with polynomial representation F. Let e and G' be
two quantifications of N which agree on all param­
eters except those for xu and xu. If Y � E, then

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

Pr(y I e,8') � Pr(y I e,8') iff
8(xu)8(xu)(G- H) �
(8'(xu) - 8(xu)] (8(xu)(I - J)

=

+ 8(xu)(K-

L)] ,
The algorithm we present requires an ordering 1r of
the variables in a Bayesian network N. The algo­
rithm goes as follows:

aF(e,8)fa>.y
aF(e,8)/a>..g
8(xu)82 F(e,8)ja>.y8Bxu
8(xu)82 F(e,8)ja>.g8Bxu
8(xu)82 F(e,8)ja>.g8Bxu
8(xu)82 F(e,8) / a>.yaBxu·

Let us call 8 the pre-tweak quantification and 8'
the post-tweak quantification. The above theorem
is stating conditions on the pre-tweak quantifica­
tion, and the amount of tweaking, which would en­
sure that a certain hypothesis ranking is achieved
in the post-tweak quantification. That is, the theo­
rem allows us to compute the change to parameters
Bxu and Bxu, respectively, which ensures a particular
ranking on the hypotheses y and 'f}.
For an example application of Theorem 5, consider
the network in Figure 5 where Pr(b) = .59 >
Pr(b) = .41. We want to compute the amount of
change to parameter Ba, initially set to 8(a) = .3,
of root node A which is needed to ensure that
Pr(b) � Pr(b) . Applying Theorem 5:

8'(a)-8(a) �

(.3)(.7)(.sg-.41)
. 3(.56-.14) + .7(.27-.03)

=

.12857.

1. For every variable X with parents U, parame­
terize the CPT of X, ¢x, as follows. Replace
each entry in ¢x which corresponds to instan­
tiation XU by BxuAx.
2. Consider every variable X according to order ?T:

( a) Multiply all tables that contain variable X

to yield table ¢.
(b) Replace the multiplied tables by the result
of summing out X from ¢, sum_out(¢, X).

3. Only one table ¢ will be left, with a single entry.
Return the single entry of ¢.
We will call this algorithm VE_COMPILE(N, 1r) .
Following is an illustrative example of VE_COMPILE:
we will use the elimination order 1r =< B, A > to
compile the network in Figure 5. The parameterized
CPTs are given below:

That is, 8'(a) � .42857 ensures that Pr(b) � Pr(b) .
We close this section by a note on the complexity
of applying Theorem 5. Since we can compute all
first and second partial derivatives in O(n2 exp ( w))
time, we are able to compute in O(n2 exp ( w)) time,
for each variable Y and each variable X, the amount
of change needed in the parameters of X to reverse
the probabilistic ranking of Y's values. Moreover, we
show in (18] that we can perform this computation
in 0(n exp(w)) time only, for a fixed Y or a fixed X.
4

Compiling a Bayesian Network

Our goal in this section is to present a variable­
elimination algorithm for compiling a factored poly­
nomial representation of a Bayesian network. This
is to be contrasted with a canonical polynomial rep­
resentation ( Definition 1), which is straightforward
to obtain but requires exponential space.
As an example, following is the canonical polynomial
of the network in Figure 5:

F

=

129

and following is one of its factored representations:

where
G
H
I
J
K
L

2000

BaBab>.a>.b + BaBali>.a>.li + BaBab>.a>.b + BaBal>>.a>.li,

A
true
false

A
true
true
false
false

c/JA
BaAa
BaAa

B
true
false
true
false

c/JB
Bab>.b
()ab)..b
BabAb
()al)b

To eliminate variable B, we sum it out from table
¢B, since it is the only table containing B:

sum_out(¢B, B)
BabAb + ()ab)..b
BabAb + ()ab)..b

A
true
false

To eliminate variable A, we must multiply the above
table with table ¢A and then sum-out variable A
from the result. Multiplying gives:
A
true
false

¢Asum_out(¢B, B)
BaA.a(BabAb + ()ab>.b)
Ba>.a(BabAb + ()ab>.b)

Summing out, we get

I

sum_out(¢Asum_out(¢B, B), A)

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

130

The polynomial,

:F(>.a, >.a:,>.b,>.li,Ba,Ba:,Bab,Bar,Ba:b,Ba:l;)
Ba>.a(Bab>.b + ()ali>.!i) + Ba:>.a:(Ba:bAb + Ba:iiAii)
is then the result of this compilation and is depicted
in Figure 6 as a rooted DAG. When the parame­
ters in such a compilation are fixed to some numeric
values, the result is still a polynomial and is a spe­
cial case of a Query-DAG [6]. Note, however, that a
Query-DAG can have multiple roots corresponding
to different queries which must be specified before
the compilation process takes place. The compi­
lation as presented above, however, is only meant
to answer one query: computing the probability
of given evidence. But as we have shown earlier,
many other queries can be computed from the par­
tial derivatives of this compilation.
We can now state the soundness/complexity result:

Theorem 6 Given a Bayesian network N and a
corresponding variable elimination order 1r of width
w and length n, the call VE_COMPILE(N, 1r ) returns
a polynomial representation of network N of size
O(n exp(w)) and in O( n exp(w)) time.
5

Evaluating and Differentiating a
Polynomial Representation

We now turn to the process of evaluating a network
compilation :F under some evidence e and quantifi­
cation e, and then computing all its partial deriva­
tives. We will first describe the computation pro­
cedurally using a message-passing scheme and then
provide the semantics of such computation.
As it turns out, the evaluation of :F and comput­
ing its partial derivatives can be accomplished us­
ing a two-phase message passing scheme in which
each message is simply a number. In the first phase,
messages are sent from nodes to their parents in :F,
leading to an evaluation of :F. In the second phase,
messages are sent from nodes to their children, lead­
ing to the computation of all partial derivatives.
We will associate two attributes with each node i
in the compilation :F: a value val ( i ) and a partial
derivative pd(i) . The goal of message passing is to
set the value of each of these attributes under some
evidence e and quantification e. We are mainly in­
terested in the value of root node r, val ( r ) , as it rep­
resents :F (e, e), and the partial derivative of each
leaf node I, pd (l ) , at it represents 8:F(e, e)j81. Mes­
sage passing proceeds in two phases: the first phase

Figure 7: Passing val-messages under evidence e
a and quantification e: (Ba,Ba:,Bab,()ali,Ba:b,B a: )
!i
(.3, .7, .1, .9, .8, .2).

=
=

sets the value of each node, and the second phase
sets the partial derivatives.

Passing val-messages In the first phase, a val­
message is passed from each node to all its parents
according to the following rules:

1. Initiation: Each leaf node I sets its value:
- val ( l ) f- e (x ) when I is an indicator Axi
- val (l ) +- e(xu) when I is a parameter Bxu·
Node I then sends a message mes(l, k)
to each parent k.

=

val (l )

2. Iteration I: When an addition node i receives
messages from all its children j, it sets its value
val ( i ) +- Lj mesU, i) and then sends a message
mes(i, k) = val ( i ) to each parent k.
3. Iteration II: When a multiplication node i re­
ceives messages from all its children j, it sets
its value val(i) +- ITj mesU, i) and then sends a
message mes(i, k) = val ( i ) to each parent k.
4. Termination: The first phase terminates when
the root node r receives messages from all its
children. In that case, :F(e , e) = val ( r) .
This process is illustrated in Figure 7, where it leads
to assigning val ( r ) = .3, indicating that the proba­
bility of evidence is :F( e, e) = .3.

Passing pd-messages In the second phase, a pd­
message is passed from each node to all its children:

1. Initiation: The root node r sets pd ( r ) +- 1 and
sends message mes(r,j) = 1 to each child j.

2000

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

13 1

an intermediate variable V; of function :F. Our
message passing scheme guarantees that pd(i) =
a:F(e, 8)/aV;.
These results follow directly from the work of [9],
which addresses the more general problem of com­
puting partial derivatives for a function that has an
acyclic computation graph. Our network compila­
tions :F fall into that category.

Figure 8: Passing pd-messages under evidence e
a and quantification 8: (Ba, Ba., Bab, Bat,, Ba.b, Ba.t,)
(.3, .7,.1,.9,.8,.2).

=
=

2. Iteration I: When an addition node i re­
ceives messages from all its parents k, it sets
pd(i) +-- Lk mes(k, i) and then sends a message
mes(i,j) = pd(i) to each child j.
3. Iteration II: When a multiplication node i re­
ceives messages from all its parents k, it sets
pd(i) +-- Lk mes(k, i) and then sends a message
mes(i,j) = pd(i) ITj';tj vaiU') to each child j,
where j' is a child of i.

4. Termination: The second phase terminates
when each leaf node I receives messages from
all its parents. In that case, pd(l ) is guaranteed
to be the partial derivative a:F(e, 0) 1al.
This process is illustrated in Figure 8, assuming that
val-messages have been propagated as in Figure 7.
The process leads to assigning pd(l ) to each leaf node
in the compilation, shown in a box below the leaf
node. For example, a:F(e, 8 )ja>.. b = .03.
Clearly, the number of messages passed in both
phases is twice the number of edges in the compi­
lation :F. And since each message can be computed
in constant time, the whole computation is linear in
the size of :F. We are currently working on count­
ing the number of operations performed by our mes­
sage passing scheme and comparing that to other
frameworks for multiple-query inference, such as the
HUGIN and Shenoy /Shafer architectures.

The Semantics of message passing We now
turn to the semantics of our message-passing scheme.
Specifically, any node i in function :F can be viewed
as a subfunction F, of :F. For any evidence e, our
message passing scheme guarantees that val(i)
F,(e, 8). Alternatively, node i can be viewed as

We close this section by two remarks. The first re­
gards the computation of rounding errors when eval­
uating :F(e, 8) using a limited-precision computer.
Specifically, let 8i be the rounding error "generated"
when computing the value of node i-that is, the dif­
ference between the value computed using limited­
precision and infinite-precision. If I 8i I:::; E I val(i) I
for a machine-specific E, then the rounding error in
computing :F(e, 8), �:F (e, 8), can be bound as fol­
lows (see [17] for details):

�:F(e, 8):::;

E

L
non-leaf

I pd(i )val (i ) I .
i

Note that bounding the rounding error can be done
simultaneously during the passage of pd-messages,
and requires no extra space.
Our final remark is on the computation of
second partial derivatives. The partial derivative
pd(i) is by itself a function of many variables and can
be differentiated again using the same approach. In
particular, if the compilation :F has O(n) variables,
then its second partial derivatives can be computed
simultaneously in O(n I :F I) time and space, where
I :F I is the size of :F. Again, this follows from a
more general result in [17].
Therefore, if the compilation :F of a Bayesian net­
work of size n is induced using an elimination
order of width w, as suggested earlier, then the
value of polynomial :F; all its first partial deriva­
tives; and all its second partial derivatives can be
computed simultaneously in O(n2 exp(w)) time and
space. This is quite interesting given the number of
queries that can be answered in constant time given
such derivatives. We are unaware of any computa­
tional framework for Bayesian networks, which com­
bines the simplicity, comprehensiveness and com­
putational complexity of the presented framework
based on partial derivatives.
6

Conclusion

We have presented one of the simplest, yet most
comprehensive frameworks for inference in Bayesian

132

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

networks. According to this framework, one com­
piles a Bayesian network into a multivariate poly­
nomial :F using a variable elimination scheme. For
each piece of evidence e, one then traverses the poly­
nomial :F twice. In the first pass, the value of :F is
computed. In the second pass, all its partial deriva­
tives are computed. Once such derivatives are made
available, one can compute in constant time answers
to a very large class of probabilistic queries, relating
to classical inference, parameter estimation, model
validation and sensitivity analysis.
The proposed framework makes two key contribu­
tions to the state-of-the-art on probabilistic reason­
ing. First, it highlights a key and comprehensive role
of partial differentiation in this form of reasoning,
shedding new light on its utility in sensitivity analy­
sis. Second, it presents one of the simplest, yet most
comprehensive frameworks for inference in Bayesian
networks, which lends itself to cost-effective imple­
mentations on a variety of software and hardware
platforms. Note that compiling a Bayesian network
can be performed off-line. The only computation
that is needed on-line is that of passing val-messages
and pd-messages, which is quite simple. We are cur­
rently investigating the development of dedicated
hardware for implementing the proposed message
passing scheme. Similar dedicated hardware has
been developed for other reasoning frameworks, such
as genetic programming and neural networks, and
can provide a valuable tool for computationally in­
tensive tasks in Bayesian networks, such as the esti­
mation of network parameters using APN/EM.
The development of such hardware is also expected
to help in migrating Bayesian-network applications
to embedded systems - such as consumer electron­
ics - which are characterized by their primitive
computational resources, therefore, affording to host
only the very simplest reasoning frameworks.
