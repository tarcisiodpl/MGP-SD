130

I
I
A Randomized

I

Approximation Analysis of

I

Logic Sampling

I
R.

I
I

chavez@sumex-aim.stanford.edu

cooper@sumex-aim.stanford.edu

Section on Medical Informatics

I

Medical School Office Building, X-215
Stanford University
Stanford, C&llfornla 94305-5479

I
I
I
I
I
I
I
I
I
I
I

Martin Chavez and Gregory F. Cooper

March 1990

In recent years, researchers in decision analysis and artifi­
cial intelligence (AI) have used Bayesian belief networks
to build models of expert opinion. Using standard methods
drawn from the theory of computational complexity, work­
ers in the field have shown that the problem of exact prob­
abilistic inference on belief networks almost certainly
requires exponential computation in the worst case [3]. We
have previously described a randomized approximation
scheme, called BN-RAS, for computation on belief net­
works [1, 2, 4]. We gave precise analytic bounds on the
convergence of BN-RAS and showed how to trade running
time for accuracy in the evaluation of posterior marginal
probabilities. We now extend our previous results and dem­
onstrate the generality of our framework by applying sim­
ilar mathematical techniques to the analysis of convergence
for logic sampling [7], an alternative simulation algorithm
for probabilistic inference.

1.0

Introduction

Given truth assignments for a set E of random variables in a
belief network, an algorithm for Probabilistic Inference in

Belief NETworks (PIBNET) computes the posterior prob­
abilities for the outcomes of a specified node X. PIBNET is
hard for NP, by reduction from 3-satisfiability in the prop­
ositional calculus [3]. That classification has focused re­
search on approximate methods, special-case techniques,
heuristics, and analyses of average-case behavior.
There now exists a number of algorithms for exact prob­
abilistic inference in belief networks: the message-passing
algorithm of Pearl [12], the triangulation method of Lau­
ritzen and Spiegelhalter [10], and others. Previous approx­
imation algorithms include the Markov-simulation scheme
of Pearl [13, 14], Henrion's logic sampling [7], and the ran­
domized approximation scheme (ras), known as BN-RAS,
which we have previously demonstrated [1]. Heckerman
has proposed a special-case algorithm for certain kinds of
two-level belief networks [6]. Each algorithm has compu­
tational properties that render it attractive for inference on
certain kinds of networks. The NP-hard classification sug­
gests, however, that no algorithm can provide a definitive
efficient solution for all inference problems.
A randomized approximation scheme (ras) is, by defini­
tion, an algorithm that computes, with low probability of

13 1

I

Methods and Procedures

I
failure, a result that lies arbitrarily close to the true answer
[8, 9]. Moreover, a ras must terminate within time that is a
linear function of the reciprocol of the error and the re­
ciprocol of the failure probability. We can define approxi­
mation schemes that guarantee either interval or relative
error bounds, with high probability.
BN-RAS derives from Markov-simulation algorithms orig­
inally proposed by Pearl. We now introduce a new ras, BN­
RAS-LS, based on logic sampling. We do not modify the
original algorithm for logic sampling; rather, we specify a
convergence analysis that transforms the logic-sampling
method into a ras.

2.0

Methods and Procedures

grows. Trial samples that are consistent with F are called
successful trials. Samples that do not correspond to the
findings in F must be discarded, and the algorithm's per­
formance deteriorates as a result (See [7] for additional de­
tails.)
We view logic sampling over a belief network with a set of
findings F to be a Bernoulli process with a binomial dis­
tribution
N

= 1- L, � �njq"-j

(1)

The detailed argument, based on Chebyshev's inequality
[9], reveals that
(2)

guarantees the (a, �) convergence criterion, whereN is the
total number of simulation trials. Each trial corresponds to
the choice of a joint instantiation, consistent with known
evidence, for all the nodes in the belief network. Inasmuch
as N is a polynomial in 1 I� and 1 Ia, our sampling scheme
with its convergence criterion defines a ras for PIDNET.
An algorithm for logic sampling simulates a value for every
variable in the model, in graphical order, at each trial.
When a variable X is simulated, the values of its condi­
tioning variables have already been select�d; logic sam­
pling chooses a new value for X based on its conditional­
probability matrix. The forward-propagation technique of
logic sampling suffers as the size of the set of findings F

I
I

(3)

I

where K is the number of successful logic-sampling trials,

I

sure (a, �) convergence, n is the total number of samples
(including successes and failures}, p=P (F) is the prior
probability of the set of findings (i.e., the probability of a
successful trial}, and q= 1- p.

I

P(K>N}

i=O

(

l

T

N= 1 I [ 4 ( 1- �) a2] is the number of trials needed to en­

Suppose that we wish to compute all posterior probabilities
in a belief network to within an interval error a. Suppose,
in addition, that we are willing to tolerate a small proba­
bility 1 - � that the algorithm fails to converge within the a
bound. Let J.1. represent the true posterior marginal proba­
bility of the node under consideration; f denotes the ap­
proximate probability computed by the algorithm. The
(a, �) convergence criterion may be written

I

If K>N, then we have performed more thanN successful
t r i a l s , a n d t h e r e f o r e P<if-�1 �a) ��- T h u s , i f
P (K>N) �a, then b y substitution o f P <it-�� �a) ��
for K>N, we obtain that P (P<lf-���a) ��) � a .
We now introduce an intermediate result:

I
I

LEMMA 1.

Let I= I P' (zl e) - P (zl e) 1. where P' (zl e) is an unbiased
estimator of P (zl e) , and where z and e are random vari­
ables. If P (P (I�b) �c) > d, then P (I � b) > cd, for arbi­
trary constants b, c, and d.
Proof. First, by Markov's inequality:

E (X)
P (X�c) <--.
c

I

(4)

Let X=P (I�b) . Then, by applying (4) to the antecedent
of the theorem, we have that P (X�c) � ( (E (X)) Ic)
Note that the theorem also specifies that d < P (X�c) .
Combining, we see that d < P (X�c) � (£(X) ) Ic . Thus,
E(X) >cd. Now, E(X) =X=P (l�b) , so P (l�b) >cd
Q.E.D.
and the lemma is proved.
•

I
I
I
I
I
I
I
I

132

I

Methods and Procedures

I
I

If P(K>N) C!:o, then
by Lemma 1:

P (Pclf-J.Li Sa)

(5)

Now define a function

I
I
I
I
I

I
I
I
I
I
I
I

1 -0.5 [ 1.0 + (0.115194) i
-4

(11)

which carries a maximum interval error of 2.5xl0-4[11,
page 51]. (We give approximations, with interval errors,
because tables are not available for the wide range of val­
ues that we may require.) If we let

(6)
f'(n,p,N) = c!l,

(

N- np +

r,.;npq

4) r- 4)
,

-c!l

np -

�
,.;npq

,

(12)

In addition, let
g (a,p,N)

=

min {n: 1-f(n,p,N) �a}.

(7)

If we perform g ( o, p, N) trials, then P (K>N) C!: o and re­
lation 5 is satisfied. We therefore require an efficient algo­
rithm for computing an upper bound on g ( o, p,N)

then li<n,p,N) -f'(n,p,N)I S5.0x10-4, and we have
g(a,p,N)

s

min {n: f'(n,p,N)

S

1

•

Consider the following normal approximation to the bino­
mial distribution f(n,p, N) :

I
I

=

+ 0.000344i- 0.019527x4]

I
I

42>' (x)

C!:i;) <!: a and thus

where 4!l is the standard normal distribution. In [11, page
170], it is shown that
0.14
1'2v(n,p,N) -f(n,p,N ) < r--

I

-

(9)

,.Jnpq

-

0 14

a - �-5.0 x10-4 } . (13)
,.Jnpq

To find an appropriate value for n, we perform a binary
search in the range [N,N2lp] and look for values that sat­
isfy the inequality in (13). (The approximation f' ( n,p, N)
is a monotonically decreasing function· on n .) Clearly,
n C!:N, because we require at least N successful trials. The
choice of til/p is purely arbitrary. Notice that as long as

�

2
0
o S 0.99954- ·1 -/'(l!._,p,N),
p
N ,.;q
n =

(14)

til/p will satisfy (13) and will therefore ensure that

P (K > N) C!: o. Hence, for most o of interest, we have given

a binary-search algorithm that y ields an upper bound
g,. ( o, p,N) on g ( o, p,N) We can accommodate o closer
to 1 by choosing finer approximations to the normal dis­
tribution, or by expanding the upper end of the search in­
terval beyond tilIp.
.

Therefore,
g(a,p,N)

.

S mm

0.14

{n: }(n,p,N ) < l -a·
r--: (10)
,.Jnpq

Now consider the approximation

_

Note that, in general, p = P (F) is not known exactly. We
can, however, calculate a lower bound p' on p as follows:
p �p' =

IT min"x [ P (XI1tx) l,

XeF

(15)

133
Discussion

I
I
I

where the X are nodes in the finding set F, and the "x de­

note the parents of X. If we perform g., (a, p', N) trials, then
P (K >N) � a. Poor lower bounds will, of course, cause the

Figure 1. u= 0.5

7
10 �����������
------�
t
I
I
I
-N•IOO

performance of the algorithm to deteriorate.
Our arguments prove the following theorem:

<It-�� �a) � �a = Cll for logic sampling,

--·N•IOOO
----·N•IOOOO

10d

1:

108

r-

THEOREM 1.
To obtain P

1:

1\,

...,
_

g ., (a,p',N) trials using t h e standard protocol for logic

sampling, and score the outcomes in the usual fashion.
103

As trials are scored, we can incrementally reapply a similar
analysis to compute a new value for g., (a,p', N) in light of

�

po s s i bl e t o c o m p u t e a l o w e r b o u nd g 1(a,p,N) o n

---

-

'--.._

min {n: f'(n,p,N) :S 1-a+

�

0

..

___

---o-

E

I

0.1

g (a,p, N) , derived from the relation

g(CJ,p,N) �

-;

..•____

� ""'- ..___

102
0.0

the trial successes already encountered. Moreover, it is also

'·

'

�\
4
10 r

with 5 a n d a s u c h that a s a t i sf i e s (14 ) , p e r f o r m

_

____ ... ___ _--·
_
·-----

"'!
__

_ D---

....___
I

0.2

--

p

�>-- - -

�:

-- I
I

0.3

0.4

0.5

+ 5.0x10 }. (16)
proximating the binomial distribution, we avoid making as­
sumptions about asymptotic behavior, such as convergence

Also,

to the normal distribution. Without such an analysis, a

p �p" =

TI

XeF

max11
X

[P (XI "x))

.

method for computing probability estimates cannot be clas­

(17)

sified as a ras.

Initially, we expect to have to perform at least g 1 (a,p",N)

Figure 1 plots the number of trials suggested by our meth­
odology, for various values of the probability of the set of

trials in order to achieve the convergence criterion. Know­

findings and for several values of N, with a held to 0.5.

ing the lower bound can be very useful in indicating wheth­

Figures 2 and 3 plot corresponding values for a = 0.9 and

er simulation for a given set of parameters is worth doing.

a = 0.99, respectively. On log-linear scales, the value of a

Note that estimate-based approaches to error analysis [7]

makes little difference; the probability p, on the other hand,

cannot yield such a priori lower bounds, because those ap­

strongly determines the expected number of trials.

proaches require several successful trials; that requirement,
in tum, defeats the objective of avoiding simulation in cas­
es where successful trials are rare.

3.0

Other workers in the field have previously given proce­

We have shown how to compute a priori bounds on the ran­
domized computation of marginal posterior probabilities in

dures for estimating the accuracy of logic-sampling esti­
mates. In particular, Henrion [7] uses a sample to compute

an estimate of the standard error of a probability of interesL
Our approach differs in that we use Chebyshev's and Mark­
ov's inequalities, as applied to the probabilities of the belief
network, to specify a priori upper and lower bounds on the
expected number of trials required for convergence. By ap-

I
I
I
I
I

4

-.�npq

I

Discussion

belief networks. We have applied our analytic techniques
both to Markov simulation [I] and, in this paper, to logic
sampling. To characterize the performance of logic sam­
pling in analytic terms, we have employed the area-esti­
mation technique of Karp a n d Luby, C h e b y s h e v ' s
inequality, Markov's inequality, a n d normal approxima-

I
I
I
I
I
I
I
I
I
I

134

I

Acknowledgments

I
I

fies the expected number of trials needed for convergence
as a function of the interval error, the failure probability,
and the prior probability of the evidence set.

Figure 2. u=0.9

I
''
'

I
I
I
I

1011
s::

'

\,

--.+-, ___ __ ... _
_

__
__

'

104

'G._._

--

+__

------··---- - -

o-

_ - - a--___ o-- __

...____ ___

_

-----

102 �������
0.0
0.1
0.2
0.3
0.5
0.4
p

I
I
Figure 3. u=0.99

I
I
I
I
I

-

''
1011 ... ---.+-,
:-.\.
'
'
104

-

..,
--

�--:G._._

-- ...

--a.-

...___

-

- -- -

..

...

-- --

_ --o-

...... ..__ _ _..,
_ _
_

.

I
I

The analysis of running time has important practical con­
sequences. The selection of an appropriate algorithm for
probabilistic inference depends crucially on the parameters
of the problem. A meta-algorithm that selects inference
techniques can apply those stochastic algorithms with the
best expected convergence for a given inference task.
The techniques we have described do not apply directly to
modified stochastic algorithms for probabilistic inference,
including importance sampling. We expect that future re­
search will illuminate the running time of those more so­
phisticated algorithms, and thereby further facilitate the
tailoring of belief-network inference techniques to knowl­
edge bases for real-world applications.

4.0

Acknowledgments

Lyn Dupre edited the manuscript.

- -- o- ___

----- � ----�

··�

102 �������
0.0
0.1
0.2
0.3
0.4
0.5
p

I
I

The number of trials specified by gu(n,p', N) is a sufficient
number of trials, such that P <I Y -1..1-l sa) ;::: ro holds true at
the start, before we begin simulation, for every posterior
p r o b a b i l i t y ll in t h e n e t w o r k . A f t e r w e p e rf o r m
g u(n,p', N) trials, however, w e are n o t guaranteed that
P <if-1..1- l s a) ;::: ro still holds, because it is possible that K
is less than N. Note, on the other hand, that we gain infor­
mation about convergence from the number of successful
trials as the simulation proceeds. We view that adaptive be­
havior, which distinguishes BN-RAS-LS from its Markov­
simulation predecessor BN-RAS, as one of the algorithm's
most appealing properties.

tions to the binomial distribution. The resulting ras speci-

This work has been supported by grant IRI-8703710 from
the National Science Foundation, grant P-25514-EL from
the U.S. Army Research Office, the Medical Scientist
Training Program under grant GM07365 from the National
Institutes of Health, and grant LM-07033 from the National
Library of Medicine. Computer facilities were provided by
the SUMEX-AIM resource under grant LM-05208 from
the National Institutes of Health.

135
