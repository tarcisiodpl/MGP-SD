

Research on Symbolic Probabilistic Inference
(SPI) [2, 3) has provided an algorithm for re­
solving general queries in Bayesian networks.
SPI applies the concept of dependency­
directed backward search to probabilistic in­
ference, and is incremental with respect to
both queries and observations. Unlike tra­
ditional Bayesian network inferencing algo­
rithms, SPI algorithm is goal directed, per­
forming only those calculations that are re­
quired to respond to queries. Research to
date on SPI applies to Bayesian networks
with discrete-valued variables and does not
address variables with continuous values.
In this paper1, we extend the SPI algorithm
to handle Bayesian networks made up of
continuous variables where the relationships
between the variables are restricted to be
"linear gaussian". We call this variation of
the SPI algorithm, SPI Continuous (SPIC).
SPIC modifies the three basic SPI opera­
tions: multiplication, summation, and sub­
stitution. However, SPIC retains the frame­
work of the SPI algorithm, namely building
the search tree and recursive query mecha­
nism and therefore retains the goal-directed
and incrementality features of SPI.
1

Introduction

The Bayesian networks technology provides a language
for representing uncertain beliefs and inference algo­
rithms for drawing sound conclusions from such repre­
sentations. A Bayesian network is a directed, acyclic
graph in which the nodes represent random variables,
and the arcs between the nodes represent possible
probabilistic dependence between the variables. The
1 This work is based on research supported by
WRDC under Contract F33615-90-C-1482.

success of the technology is in part due to the devel­
opment of efficient probabilistic inference algorithms
[5, 6, 7). These algorithms have for the most part been
designed to efficiently compute the posterior proba­
bility of each node or the result of simple arbitrary
queries. They have not efficiently addressed the more
general problem of answering multiple queries with re­
spect to differing sets of evidence.
Recent research in Symbolic Probabilistic Inference
(SPI) [2, 3) has made a significant step in this direc­
tion. Unlike traditional Bayesian network inference
algorithms, SPI algorithm is goal directed, performing
only those calculations that are required to respond to
queries. In addition, SPI is incremental with respect to
both queries and observations. However, the research
to date on SPI applies only to Bayesian networks with
discrete-valued variables.
There have been several inference algorithms designed
to handle networks that are made up of continuous
variables where the relationships between the variables
are restricted to be "linear gaussian". The algorithms
include the distributed algorithm (7] and the influence
diagram approach (4, 8).
In this paper, we extend the SPI algorithm to perform
this function-handle Bayesian networks with contin­
uous linear gaussian variables. We call the extension,
SPI Continuous (SPIC). The framework of this algo­
rithm is the same as that for SPI. However, the ba­
sic SPI operations of multiplication, integration and
substitution are quite different. Because SPIC stays
within the SPI framework, the goal-directed and in­
crementality features of the algorithm are preserved.
The paper is organized as follows. Section 2 briefly
summarize the SPI algorithm which includes the con­
struction of the SPI node tree and the recursive query
processing. Section 3 describes the new algorithm with
continuous variables. The representation are described
as well as the "basic" operations. Finally, some con­
cluding remarks are given in Section 4.

78

Chang and fung

2

Overview of the SPI Algorithm

The SPI algorithm consists of several major process­
ing steps. The first step is to organize the nodes of a
Bayesian network into a tree structure for query pro­
cessing. We call these structures SPI trees. In the
second step, queries are directed to the root node of
the SPI tree. The query is decomposed into queries
for the node's subtrees. This recursive procedure con­
tinues until a particular query can be answered at the
node at which it is directed. The answer is then com­
puted and returned to the next higher level in the SPI
tree. Once a node has responses from all of its sub­
trees it can compute its own response to its predeces­
sor node. This process terminates when the root node
processes all the responses from its subtrees.
An SPI tree is constructed by organizing the nodes of
a Bayesian network into a tree structure. The only
constraint on the construction process is that if there
is an arc between two nodes in the original network,
then one of the nodes must be a direct or indirect pre­
decessor of the other in the SPI tree. This constraint
allows many possible SPI trees.
The first step in building a SPI tree is to choose the
root node. This done by computing the maximum
node to node distance for each node. The node that
has the smallest maximum distance is chosen as the
root node. This heuristic is designed to produce a.
"bushy" SPI tree which can take advantage of dis­
tributed processing. The second step is to use max­
imum cardinality search [9] to build the tree from the
root node. This step constructs the tree based on the
connectivity principle and guarantees satisfaction of
the tree construction constraint.

Figure 1: An Example Network

The general format for a query received by SPI is as
a conditional probability, namely, P{XIY}, where X
and Y are sub sets of nodes in the Bayesian networks.
To be processed by SPI, queries of this form are first
transformed into another format. The format consists
of two set of nodes L and M which satisfy:
M=(XUY)nD(L)

(1)
where X= MnL, Y =M\L, and D(L) is the dimen­
sion of the distribution associated with the node set L.
Intuitively, L represents the minimum set of node dis­
tributions needed to respond to the query and M is
dimension of the desired response. L and M can be
computed in linear time and are simple to implement
[3]. Figure 1 and 2 show a simple Bayesian network
and the corresponding SPI tree. For example, if the
query is to find the joint probability of a1 and c2, the
query being sent to the root node will be consisting of
L = {a1,a2,c1,c2} and M= {a1,c2}.
The heart of the SPI algorithm is as follows; at any
node i, a request arrives for a probability distribution
represented by L and M, the algorithm responds to
the request by computing the "generalized" distribu-

Figure 2: The SPI tree

Symbolic Probabilistic Inference with Continuous Variables

tion Q(M) [3]. Q(M) is obtained by multiplying the
distributions in node L and summing over dimensions
L \ M. If such a distribution had already been com­
puted earlier and cached, it can be returned immedi­
ately. However, usually it will be necessary to send
requests to node's successors in the SPI tree in order
to compute the response. It is obvious that if a par­
ticular subtree has nothing to do with the query (i.e.,
there is no intersection), then no query will be sent to
that subtree. For the same example above, the query
can be obtained as,
Q(a1,c2)

= 1l"a1

L 1l"a01l"c11l"c,

(2)

where 1r; represents the probability distribution asso­
ciated with node i. There are three major operations
in SPI algorithm: multiplication, summation and sub­
stitution. Multiplication calculates the product of two
distributions; summation calculates the sum of a dis­
tribution over a set of variables; and substitution cal­
culates the result of substituting an observed value for
a node into a distribution. For networks with con­
tinuous variables, the SPI algorithm can be applied
directly. However the multiplication, summation, and
substitution operations must be modified. In the next
section, we will describe the corresponding multiplica­
tion, integration, and substitution operations for the
networks with continuous variables.
3

The SPI with Continuous Variables

Algorithm

The continuous SPI (SPIC) algorithm requires redef­
inition of the SPI operations: multiplication, integra­
tion, and substitution. The general mechanism for the
continuous SPI algorithm is basically the same as the
discrete one except for the caching operation and the
handling of evidence. We first describe the representa­
tion for conditional probability distributions in linear
gaussian continuous variables and then the three op­
erations in detail.
3.1

Node Representation

A SPIC node represents a vector of continuous vari­
ables. SPIC restricts the conditional probability dis­
tribution of each node to be "linear gaussian". "Linear
gaussian" distributions are the sum of a deterministic
component and a probabilistic component. The de­
terministic component is a linear combination of the
node's predecessor values. The probabilistic compo­
nent is restricted to be gaussian (i.e., normal) which
can be specified with mean vectors and covariance ma­
trices. For a Bayesian network of this type, the nec­
essary prior information needed before any inference
can be drawn are the the prior distributions of the
root nodes (i.e., mean and covariance) and the links
between nodes in the network.

For example, for a random vector represented by a
node x, if it is a root node, only mean x and the corre­
sponding covariance matrix Q, need to be specified. If
it is not a root node, and has a set of predecessor nodes
x�, .. , x�, then the relation between xand its predeces­
sors represented by the following linear equation need
to be specified,
x= Btx{ + .. + BNxj, + w,

(3)

where B1, .. , BN are constant transition matrices rep­
resenting the relative contribution made by each of
the predecessor variables to the determination of the
dependent variable x, and w, is a noise vector sum­
marizing other factors affecting x. w, is assumed to
be normally distributed with mean x and covariance
Q,. For most applications, w, will have a zero mean.
However this is not always true apriori and can occur
when distributions are multiplied together (e.g., a root
node and a non-root node).
For each node xin a network, the sufficient informa­
tion describing the node itself and the relationship to
its predecessors can therefore be represented in the fol­
lowing form:
(4)
With this simple representation, we will then describe
the multiplication, integration, and substitution oper­
ations.
3.2

Multiplication

In this section, we describe the multiplication oper­
ation for SPIC. We describe when distributions can
be multiplied, what the result will look like, and then
describe in detail how each part of the result is com­
puted.
There is a constraint on what distributions can be mul­
tiplied. This constraint called "combinability" was de­
veloped in [1]. According to the theorem derived in [1],
a set of nodes S is "combinable" (i.e., able to be ag­
gregated) if and only if every pair of nodes in the set
is combinable. Two nodes are combinable if all nodes
in the path(s) between the two nodes are in the setS.
It can be shown that if the set of nodes corresponding
to the distributions to be multiplied are "combinable",
then multiplying the distributions is the same as find­
ing the "joint" distribution of those nodes, or in other
words, to find the new probability representation of
the "combined" node.
Based on the separation principle in the SPI tree (i.e.,
any node separate its successors rooted from itself )
(3], it can be easily shown that any distributions that
will be multiplied from any query request during SPI
processing are always combinable. In other words, we
can transform the multiplication operation in SPI al­
gorithm into a node combination operation for the con­
tinuous nodes.

79

80

Chang and fung

The dimension of the resulting distribution will be the
sum of the dimensions of those nodes to be multiplied.
For instance, two representations of nodes (variables)
x1 and x2 each with dimension D(xt) and D(x2) are
to be multiplied, the resulting representation will have
dimension D(xt) + D(x2) , which can be interpretated
as the description of the combined variable xwith x1
stack over x2, i.e., x=

[ :� ] . The question now is

how to calculate the new probability representation of
xbased on the old ones of x1 and x2• It is dear that
this is nothing but to identify the new predecessors xr
of x and calculate the links B; between x and the new
predecessors as well as to calculate the new conditional
mean x and the associated covariance Qx.
F irst of all, the new predecessors of xis just the union
of the predecessors of x1and x2 (excluding Xi or X2
when one is the direct predecessor of the other).· The
new linear relation (coefficients) between xand xf are
obtained based on the old links. Conceptually, this can
be accomplished by first breaking down the combined
node into the original element nodes, and then find
the relation between the predecessor and the element
nodes individually. To do so, all paths between the
predecessor and the desired element node are found
and then combined together. The contribution of a
path is obtained by multiplying the transition matrices
of links along the path in the original network.
The mean and associated noise covariance matrix for
the combined node is the last part of the representa­
tion that needs to be calculated. The first quantity
that requires computation is the implicit linear rela­
tionship T between the two nodes that are to be mul­
tiplied. Based on the SPI tree structure, it can be
shown that the T for two nodes is non-zero only when
one node is the direct or indirect predecessor of the
other. If the nodes do stand in this relation, T can be
calculated by first finding all paths between the two
nodes and adding the contribution from all paths. As
in the process for finding the link coefficients for the
new predecessors, the path contributions are obtained
by multiplying together the transition matrices of links
along the path. Given T, the new mean and covari­
ance matrix of x can be obtained as below, given that
x1 is the direct or indirect predecessor of x2.

C!2 =

[ 2CC1]

(7)
where c12 is the "combined node", Txy are the links
(transition matrices) between xand y, and a1 and a2
are the new predecessors. In addition to the links ob­
tained as above, the new conditional mean and corre­
sponding covariance are obtained as,
(8)
which is zero since both c-1and c2 are zeros, and

(9)
In the discrete case, multiplying distributions can be
expensive since the size of the resulting distribution
grows exponentially with the number of distribution
(nodes) to be multiplied. However, in the linear gaus­
sian case, the resulting representation only increases
quadratically. This is because a gaussian distribution
can be sufficiently represented by a mean vector and
a covariance matrix.
3.3

Integration

The integration operation is relatively simple. All
one has to do is to identify and keep the appropriate
slots from the representation (i.e., links to predeces­
sors, mean, and covariance) and discard the rest of
them. It can be easily shown that the reduced repre­
sentation precisely describes the resulting distribution
after the integration. For example, for the distribution
of the combined node c1 2 obtained from the multipli­
cation above. If the goal is to integrate c1 out of the
joint probability representation, all one has to do is
to grab the appropriate slots from the links to a, and
a2, the mean vector c12, and the covariance matrix
Qc1•2 These slots are corresponding to the c2 vari­
able, namely,

(5)

Q X-

[ TQQx,x,

' T'
QX}
TQx,T + Qx,

]

(11)
(6)

For example, the results of multiplying 'll"e1 and 'll"e, of
the previous example given in F igure 1 can be repre­
sented by

and
(12)
3.4

Substitution

Evidence is represented in the form of exact observa­
tion of the values of variables. The set of variables

Symbolic Probabilistic Inference with Continuous Variables

which have been observed is denoted by E. One easy
way of incorporating evidence is to include evidence in
the query, for instance, P{XIY, E} and then substitute
the observed values E* for E after the more general
query is computed. Suppose the query results before
the substitution of the observation is represented as

(13)
with the associated covariance matrix :!::x. To "sub­
stitute" the observation E*, we first remove the link
KE from the representation, then replace the mean
X by X + KEE*. The covariance matrix remains
the same. It can be easily shown that the new rep­
resentation correctly describe the results of the query,
P{XIY,E = E*}. Other more efficient methods such
as to do substitution before query are currently under
our investigation.
4

Conclusion

Recent research in Symbolic Probabilistic Inference
(SPI) has made a significant step in improving effi­
ciency of general query processing. Unlike traditional
inference algorithms, SPI algorithm is goal directed,
performing only those calculations that are required
to respond to queries. In addition, SPI is incremental
with respect to both queries and observations. In this
paper, we extent the SPI algorithm to handle Bayesian
networks with linear gaussian variables. We call the
algorithm SPIC. The framework of this algorithm is
the same as that for SPI. However, the basic opera­
tions of multiplication, integration and handling ob­
servation are quite different. The equivalent operation
to the multiplication of discrete case is similar to the
"node combination" operation in the continuous case
and the equivalent operation to the summation of dis­
crete case is the integration operation. Because SPIC
stays within the SPI framework, the goal-directed and
incrementality features of the algorithm are preserved.
In this paper, we have only addressed the problem
of continuous variables that are restricted to have
linear gaussian models. Many real-world problems
may require nonlinear or nongaussian models. Classi­
cal methods such as approximation with linearization
(e.g., extended Kalman filters) or sum of gaussians de­
serve attention in further investigation. A version of
the SPIC algorithm has been implemented, prelimi­
nary results show expected performance.
