
In this paper we address the problem of learning
the structure of a Bayesian network in domains
with continuous variables. This task requires
a procedure for comparing different candidate
structures. In the Bayesian framework, this is
done by evaluating the marginal likelihood of the
data given a candidate structure. This term can be
computed in closed-form for standard paramet­
ric families (e.g., Gaussians), and can be approx­
imated, at some computational cost, for some
semi-parametric families (e.g., mixtures of Gaus­
sians).
We present a new family of continuous variable
probabilistic networks that are based on Gaus­
sian Process priors. These priors are semi­
parametric in nature and can learn almost ar­
bitrary noisy functional relations. Using these
priors, we can directly compute marginal like­
lihoods for structure learning. The resulting
method can discover a wide range of functional
dependencies in multivariate data. We develop
the Bayesian score of Gaussian Process Net­
works and describe how to learn them from data.
We present empirical results on artificial data as
well as on real-life domains with non-linear de­
pendencies.

1

Introduction

Bayesian networks are a language for representing joint
probability distributions of many random variables. They
are particularly effective in domains where the interactions
between variables are fairly local: each variable directly
depends on a small set of other variables. Bayesian net­
works have been applied extensively for modeling com­
plex domains. This success is due both to the flexibility
of the models and to the naturalness of incorporating ex­
pert knowledge into the domain. An important ingredient

for many applications is the ability to induce models from
data. This ability allows to complement expert knowledge
with data to improve performance of a system.
In the last decade there has been an active research effort to
develop the theory and algorithms for learning of Bayesian
networks from data. This includes methods for parameter
learning [1, 19, 27] and structure learning [3, 6, 16, 26].
Using structure learning procedures we can learn about the
structure of interactions between variables in an unknown
domain.
Part of our motivation comes from an ongoing project that
applies such structure learning methods to molecular bi­
ology problems [10]. This project attempts to understand
transcription of genes: A gene is expressed via a process
that transcribes it into an RNA sequence, and this RNA
sequence is in tum translated into a protein molecule. Re­
cent technical breakthroughs in molecular biology enable
biologists to measure of the expression levels of thousands
of genes in one experiment [7, 20, 32]. The data gener­
ated from these experiments consists of instances, each one
of which has thousands of attributes. These data sets can
help us understand how a gene's transcription is effected by
various aspects of the cell's metabolism, including the ex­
pression levels of other genes. The challenge is to recover
this biological knowledge from such experiments (see, e.g.,
[18]).
There are several problems in learning from such data. In
particular, in this paper we examine the problems raised
by the quantitative nature of these measurements. In the­
ory we might think of a gene as being either in "activated"
and "suppressed" modes. Experience with this data, how­
ever, shows that discretization of the data loses much of
the information [10]. Thus, we seek methods that can di­
rectly represent and learn interactions among continuous
variables.
Another problematic aspect of this type of data is the large
number of attributes (genes) that are measured (i.e., thou­
sands) and the relatively few samples (i.e., dozens). Thus ,
we seek methods that are statistically robust and can detect

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

21 2

dependencies among many possible alternatives.
The best understood approach for modeling continuous dis­
tributions in Bayesian network learning is based on Gaus­
sian distributions [11]. This form of continuous Bayesian
network can be learned using exact Bayesian derivations
quite efficiently. Unfortunately, the expressive power of
Gaussian networks is limited. Formally, "pure" Gaussian
networks can only learn linear dependencies among the
measured variables.
This is a serious restriction when learning in domains with
non-linear interactions, or domains where the nature of the
interactions is unknown. A common way of avoiding this
problem is to introduce hidden variables that represent mix­
tures of Gaussians (e.g., [28, 34]). An alternative approach
that has been suggested is to learn with non-parametric den­
sities [17].
In this paper we address the problem of learning continuous
networks by using Gaussian Process priors. This class of
priors is a flexible semi-parametric regression model. We
call the networks learned using this method Gaussian Pro­
cess Networks. The resulting learning algorithm is capable
of learning a large range of dependencies from data.
This approach has several important properties. First, the
Gaussian Process is a Bayesian method. Thus, the inte­
gration of this form of regression into the Bayesian frame­
work of model selection is natural and fairly straightfor­
ward. This allows us to interpret the results of the learning
as posterior probabilities, and to assess the posterior proba­
bility of various networks structures (e.g., using methods
such as [9]). Second, the semi-parametric nature of the
prior allows to learn many continuous functional depen­
dencies. This is crucial for exploratory data analysis where
there is little prior knowledge on the form of interactions
we may encounter in data. In addition, the Gaussian Pro­
cess is biased to find functional dependencies among the
variables in the domain. This is a useful prior for domains
where we believe there is a direct causal dependency be­
tween attributes.
In the remainder of this paper we review the Bayesian ap­
proach for learning Bayesian networks. We then review
the definition of the Gaussian process prior in this setting
and discuss how to combine the two to learn networks. Fi­
nally, we validate our approach on series of artificial exam­
ples that test its generalization capabilities and apply to few
real-life data problems.

2
2.1

Learning Continuous Networks

ment to the variables X 1, . . . , Xn. The Bayesian learn­
ing paradigm requires us to specify a prior probability dis­
tribution P(G) over the space of possible Bayesian net­
work structures, and for each structure, a prior over the
conditional probabilities. This prior is then updated us­
ing Bayesian conditioning to give a posterior distribution
P(G I D) over this space.
We start with the prior over structures, P(G). Several
priors have been proposed, all of which are quite simple.
Without going into detail, a key property of all these priors
is that they satisfy:
•

Structure modularity

in the form
P(G)

The prior P(G) can be written

II p(Xi, Pac(Xi));
i

<X

That is, the prior decomposes into a product, with a term
for each family in G. In other words the choices of the
families for the different nodes are independent a priori.
The next question we need to address is the prior over the
conditional probability distributions (or density functions)
P(Xi I Pac(Xi)) needed for each network. Usually, these
conditional distributions are represented in a parametric
form. Thus, once we fix the parametric family, we can
specify the conditional distribution by a vector of param­
eters Bx;IPaa(X;)· The prior distribution over conditional
distributions can now be phrased as a prior over values of
the different Ox, IPaa(X;} 's. For our purpose, we need only
require that the prior satisfies two basic assumptions, as
presented in [16]:
•

Parameter independence: Let Bx,IPaa(X;} be the
parameters specifying the behavior of the variable xi
given the various instantiations to its parents U. We
require that

P(Ba

I

G)

II P(Bx;�Paa(X;) I G)

=

(1)

That is, we assume that the parameters for different
conditional distributions are a priori independent.
•

in which Paa(Xi)

Let G and G' be two graphs
Paa,(Xi) = U then

Parameter modularity:
=

P(Bx;�u

I

G)

=

P(Bx;�u

I

G')

(2)

That is, the prior for a conditional distribution depends
only on the choice of parents for xi and is indepen­
dent of other aspects of the graph G.

Bayesian Structure Learning

Our goal is to learn Bayesian networks from fully ob­
servable data; i.e., we are given a data set D
{x[l], ... , x[M]}, where each x(j] is a complete assign-

Once we define the prior, we can examine the form of the
posterior probability. Using Bayes rule, we have that
P(G

I

D)

<X

P(D

I

G)P(G).

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

The term P(D I G )is the marginalprobability of the data
given G and is defined as the integration over all possible
parameter values for G

P(D I G )=

j P(DI G ,Ba )P(Ba I G )dBa

Alternatively, we can define

P(D I G ) =

II

P(DI G )using the chain rule:

P(x[i] l x1
[ ], ...,x[i- 1], G )

where P(x[i] I x[1], ...,x[i - 1], G ) is the probability
given to the i'th instance after observing the previous i- 1
instances.
Using the above assumptions, one can show (see [16]) that
if Dis complete and the prior probability satisfies parame­
ter ind ependence, and parameter mod ularity, then

P(DI G ) =

II score(Xi,Paa( X i)I D).

where the conditional score

score(Xi, UI D)is

[ ], A[Paa(Xi)= U]).
P(xi1
[ ], ..., xi[MJ I u1
[ ], ..., uM
That is, the probability assigned to the sequence of values
Xi given the observed values of Uand the assumption that
Xi's parents are exactly U.
If the prior P( G )satisfies structure modularity, we can also
conclude that the posterior probability decomposes:

This decomposition of the score is crucial for learning
structure. A local search procedure that changes one arc
at each move can efficiently evaluate the gains made by
adding or removing an arc. An example of such a proce­
dure is a greedy hill-climbing procedure that at each step
performs the local change that results in the maximal gain,
until it reaches a local maximum. Although this proce­
dure does not necessarily find a global maximum, it does
perform well in practice. Examples of other local search
procedures that advance in one-arc changes include beam­
search, stochastic hill-climbing, and simulated annealing.
2.2

Continuous Variable Networks

To learn Bayesian networks we need to choose parametric
families for representing and learning conditional densities.
There are several possible choices. We briefly mention
these here.
The simplest and best understood families of conditional
densities are the linear Gaussian models. In this model, if
Paa(X)= {U1 , ... , Uk}, we assume that

P(X I u1, ...,uk)"'N(ao+ L ai· ui, lT2 ).

213

That is, X is normally distributed around a mean that de­
pends linearly on the values of its parents. The variance of
this normal distribution is independent of the parents' val­
ues. In this representation Bx({U1, ,Uk} = (ao, ...,ak, lT).
.••

Bayesian learning of such families is developed by Geiger
and Heckerman [11, 15, 12]. W hile we do not go into de­
tails, we note that for this parametric family, the Bayesian
score for each family can be computed exactly and quite
efficiently.
The drawback of Gaussian networks is that their represen­
tation is limited to modeling linear dependencies between
variables. Thus, if the dependencies in the data are signif­
icantly non-linear, the score of the parents choices can be
misleading and thus result in a network that poorly reflects
the dependencies in the data (and also performs poorly in
predictions).
A possible approach to overcome the limitations of Gaus­
sian models is to consider mixtures of Gaussians [8, 34].
In this approach we model the conditional distribution as a
weighted mixture

P(X I U)

=

L

j /j

W

( X I U)

where each fJ is a linear Gaussian distribution. In theory,
such mixtures can approximate a wide range of conditional
distributions. In particular, they can represent multi-modal
distributions, and thus can represent relationships that are
not purely functional.
j

Learning such mixture models, however, presents prob­
lems. Exact computation of the marginal likelihood of such
a family cannot be done in closed form. Instead, we have to
resort to approximations, such as the Laplace approxima­
tion [23, 4, 22]. This, in tum, requires us to find the MAP
parameters given the data, which is a non-linear optimiza­
tion problem in a space with many local maxima. Thus,
in practice, we usually need nontrivial amount of data and
running time to learn a mixture with moderate number of
components.
An alternative approach which is non-Bayesian in nature
was proposed by Hofmann and Tresp [17]. They use non­
parametric kernel methods for predictions. Roughly speak­
ing, given training examples x1
[ ], the kernel es­
[ ], ...,xM
timate for P(X)is

Pkernel(x) =

1

M

)
[ JII2
f1 g ( 1-;; llx- xm
M

g()

where
is a kernel function and lT is a "smoothing"
parameter. A common choice is to take to be the pdf
of a normal distribution with zero mean and unit vari­
ance. Hofmann and Tresp use such estimates to find
the conditional distribution by setting P(x I u) =

Pkre nt
e (x,u)j Pkernet(u).

g

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

214

Kernel methods are extremely flexible density estimators.
Their performance depends crucially on the smoothness
parameter. Thus, we need to tune this parameter to en­
sure that the data is not over-fit or over-smoothed. This is
usually done by cross-validation testing. In particular, Hof­
mann and Tresp use a leave-one-out cross-validation proce­
dure. In addition, we need to find a way of comparing the
score of different network structures in this non-parametric
setting. Hofmann and Tresp suggest to do so by comparing
a cross-validated estimate of the logarithmic loss of each
family. This is essentially an estimate of the out-of-sample
loss the family will incur on new data. To summarize, for
each family, Hofmann and Tresp's procedure searches for
the parameters that minimize the log-loss in cross valida­
tion estimate, and then return this log-loss estimate as the
score of the family.

3

Gaussian Process priors

In recent years, there has been much interest in the use
of Gaussian Process priors for regression [33] as well as
for classification [13]. It can be shown that predictors like
feed-forward neural networks and radial-basis function net­
works converge to Gaussian process predictors as the num­
ber of internal nodes goes to infinity [21]. We now review
the basics of the Gaussian Process prior and its use in re­
gression.
Consider a set of variables U. We want to model a prior
over a variable X which we believe to be a function of U.
We can treat the value of X for each value u as a random
variable. More formally, a stochasticprocess over U is a
function that assigns to each u E
( U)a random variable
X(u). The process is said to be a Gaussian process (GP)
if for each finite set of values ,u1:M = {u[l],...,u[M]},
the distribution over the corresponding random variables
x1:M = {X[l],...,X[M]} (where X[m] = X(u[m])) is
a multivariate normal distribution.

val

To specify such a process, we need a way of describing
the mean value of each variable X(u) and the co-variance
matrix for each finite subset of values we chose. This is
done, by specifying two functions:
•

A mean function J,t(u), so that E[X(u)]

•

A

covariance

Cov[X(u),X(u ')]

C(u,u'),
C(u,u').

function
=

=

JL(u).
so

Before we discuss the covariance function C and its param­
eters, let us see how we use the GP to predict the value of
the process at a new point. We shall assume JL(u) = 0 from
now on.
Assume we already observed M points x1:M given u1:M .
and we are given a parametrized covariance function. By
the definition of the Gaussian process P(Xl:M ,XM+l I
U1:M , UM + l) is aM + !-dimensional Gaussian distribu­
tion. We since we observed the values X1:M, we com­
pute the conditional distribution over XM+l given these
observation. A basic property of multivariate Gaussian
distributions is that the conditional distribution given the
value of some of the variables is also a Gaussian dis­
tribution. Thus, the conditional distribution P(XM+l I
X1:M , U1 :M ,UM +d is a univariate Gaussian distribution.
Using properties of Gaussian distributions we compute de­
fine the mean and variance of this distribution using:
(4)
(5)
W here k = (C(u[M + 1],u[l]),... ,C(u[M + 1],u[M]))
and"' = C(u[M + 1],u[M + 1]). In other words, hav­
ing observedM values of the process we can represent the
conditional density at any new coordinate x using CM, the
covariance matrix calculated for the firstM points.
3.2

Covariance Functions

We now deal with the issue of covariance functions. As we
can see, this function determines the prior over functions.
The only constraint on the covariance is that it should pro­
duce positive semidefinite matrices.
In general, if the covariance of two close points is large,
then the prior prefers smooth functions. The covariance
between points further away determines properties like pe­
riodicity, smoothness, and amplitude of the learned func­
tions. These aspects of the covariance functions are con­
trolled by its hyperparameters B. For example, Williams
and Rasmussen [33] suggest the following function:

,

:

())

=

1 � (Uk- uD2 }
()o exp { -2 �
_x2
UkEU

that

d

+

fh + ()2

L
UkEU

(3)

� ( - �(Xl:M- Jll:M)TC!;Jw(Xl:M- Jll:M))
exp

where Jll:M is the vector of means (JL(u[l]),...,J,t(u[M]))
and C1:M is the covariance matrix with the (i,j ) entry

C(u[i],u[j]).

Prediction

C(u,u

The joint distribution of x1:M is therefore:

P(xl:M lnl:M )

3.1

UkU�

k

+ ()3Ju,u1

(6)

In this function each hyperparameter controls a different
characteristic of the learned functions. The hyperparameter
Bo controls the amplitude of variation of the function. The
hyperparameter ()1 controls how far can the whole function
be shifted from the zero line. The hyperparameter ()2 ac­
counts for linear tendencies in the function. And the hyper­
parameter 03 is the variance of an uncorrelated white noise,

215

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

which is added on top of the function. The hyperparame­
ters >.k are the length scales of the different directions in u,
over which the function changes considerably.

W hat value of hyperparameters should we use in C when
constructing the Gaussian Process density ? The Bayesian
approach is to assign the hyperparameters a prior, and then
integrate over them. Let D = {u1,M,Xl:M}, then we
should make predictions as

P(xM
[ + 1]luM
[ + 1],D)
=

I P(x[M

+ 1Jiu[M+ 1], D,8)P(8ID)d8

As this integral is usually intractable, we can try to approx­
imate it. One way is to use iJ, the maximum a posteriori
estimator for 8, as suggested in [13]. Another option is
performing a numerical integration using a Monte Carlo
method (as in [33]).

4

Learning Networks with Gaussian Process
priors

We now examine Networks with Gaussian Process priors.
As stated above, we make the parameter independence and
modularity assumptions. Thus, to define the prior we need
to evaluate the score of a variable Xgiven a set Uof parent
variables.
Recall that the

score(X, U I D)=

I score(X, UI D,8)P(8)d8.

Unfortunately, we do not know how to perform this integra­
tion in closed form, since score(X, U I D, 8)is a complex
function of 8.
The approach we take, which is quite common in many
other applications of Bayesian methods, is to approximate
this integral with the MAP hyperparameters. Thus, we ap­
proximate

score(X, UI D)� maxscore(X, UI D, 8)P(8).
{}

This approximation is reasonable if the posterior probabil­
ity over hyperparameters is sharply peaked over a single
maximum. In such situations, most of the integral is de­
termined by the area near the MAP parameters. A slightly
better approximation is the Laplace approximation, where
the posterior probability in the integral is approximated as
a Gaussian distribution over the parameters 8(see, e.g. [5]).
This however requires the calculation of the Hessian of the
log posterior probability, which can be time consuming.
We therefore use an estimate for this term, which scales like
where Kin our case is the number of hyperpa­
rameters of the covariance function. The resulting estimate
is in the spirit of the Bayesian information criterion (BIC)
of Schwarz [25], and the MDL score of Rissanen [24], hav­
ing a term which penalizes the model for over-complexity.

lflog(N),

score(X, UI D)is defined

[ ], A[Paa(Xi) = U]).
P(x1
[ ], ...,xM
[ ], ... , uM
[ ]I u1
To define these terms, we need to define a Gaussian Process
prior for X as a function of U.As before, we will assume
that the mean function is 0, and thus we only need to choose
a covariance function CxiU ·
Once we choose such a covariance function, the score is
easy to compute. The Gaussian Process prior implies that
are normally distributed with the covari­
x1
[ ], ...,xM]
[
ance function specified by U[1], ..., UM
[ ]. Thus, the
score with respect to the covariance function C( ·, · : 8)
is

score(Xi, UI D, 8)
1
M..
!
1
(21r)_ 2 ICu I_ 2 exp -2xlr:M cu X!:M ,

(

In practice, we usually do not fix the parametrized covari­
ance function in advance. Instead, we select a family of pri­
ors, such as the ones of (6). Thus, the proper score would
require us to integrate over the hyperparameters:

)

where Cu is the covariance matrix defined by the covari­
ance function C() and the values u1, ... , uM.
We see that given a Gaussian Process prior, the computa­
tion of marginal probability can be done is closed form. In
this sense, the Gaussian Process prior is very appealing. It
can learn a wide range of functional dependencies, and we
can compute the Bayesian score exactly. In this sense, the
Gaussian Process priors fit well with the Bayesian model
selection approach of learning Bayesian network structure.

To score a family X given U, we perform conjugate gradi­
ent ascent to search for the MAP parameters. The eval­
uation of each point during the search requires to invert
and to compute the determinant of an M by M matrix.
Thus, the computational costs of this closed form equation
is O(M3) in naive implementations. This operation is re­
peated in each iteration of the hyperparameter optimization
step. In practice this optimization converges quite rapidly
(10-20 iterations).

5

Experimental Evaluation

We first want to test the GP score on the simplest case. We
therefore ask the following question: given two variables,
Xand Y, with some noisy functional dependence between
them, will the GPN learner prefer the network where X is
independent of Y, or the one in which they are dependent.
Furthermore, we expect that, up to a certain noise level,
the GP learner will prefer the direction for which it can fit
a "nice" function, since such a function is more likely in a
GP prior. For example, in Figure 1 we see a noisy quadratic
dependence. The GP prior will assign a very low likelihood

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

216

X-+Y

.-.
' ,

Y-+X

GP family score
3

�.---�----7x-�.vO.soo�
..��
X Kld Y Score ""''"""'
300
Y·>X Score ----

=
300

r --�----7x-�.vO.soo�
..��
X ind Y Score -·""""'
Y·>X Score ----

2.50

200
:

150

·.
'.

.
:;_ .. '
. ··.
.

.

:

:-

..

.

·

-·

-

.

100

;

.

3

•'

4

35

2.5

cube
..

350
300

�
-100
-1SO

to the X-+ Y dependence, since it is hard to fit a function
in this direction, while the dependence Y -+ X will get
a higher probability, as it can be explained by a quadratic
functional dependence with a certain noise width at each
point.
To test this, we produced data sets of two variables with de­
pendencies of linear, quadratic (as in Figure 1), cubic and
sinusoidal nature. On top of the functional dependence, a
non-correlated Gaussian noise was added. For each case
we compared between the different network models, in
terms of the GP network scores for the training set, and
the log likelihood of the test set when that particular model
was used for prediction. This was done for different noise
levels, and different training set sizes.
Figures 3 and 2 show the dependence of those measures
on the function noise level. We observe that for the true
dependency model, the prediction quality and the GP fam­
ily score rise as the level of noise drops. We see that even
for noise levels as high as 1.5 times the dependent vari­
able amplitude, the true dependence is still preferred over
the no-dependence model. We also see that the direction
of dependence is clear in the non-invertible cases, like the
sinusoidal and the quadratic dependencies. In those cases,
the score of the "wrong" direction dependency is as low as
the no-dependency model. The cubic data set in our case
is borderline-invertible, and so the distinction is less clear
cut. For the linear case, if the slope is not too steep, both di­
rections have a functional form, and so no one direction is
preferred over the other. The Gaussian Process preference
for functional direction can be useful when learning causal
networks if we assume the interactions in our domain are
functional.
We next compare the GP network learning method against
the two continuous variable models described in Sec­
tion 2.2: the Gaussian network model (with the BGe scor­
ing metric [11]), and the kernel network. We start with two
variable networks, with the same four types of functional
relations as described before. Figure 4 shows the predic­
tion quality of the three methods on those data sets, com-

quad

3.5

4

lin

X->YScore­
X indYScore ......,. . ..
Y->X Score ----

Figure 1: An example of a non-invertible dependence be­
tween X and Y. The explanation X-+ Y does not have a
functional form, whereas Y -+ X can be explained as a
noisy function.

3

So
::
o ..��
x_-:>; , vo.
.-�----------;
X indY Score •.•,.,..._
Y->X Score----

.......--;:._:::
...
::,....,."""'-.--J,
�::---�----o---��_j
--::':""

G

0.5

1

1.5

2

2. 5

3

3.5

4

sin

Figure 2: GP family scores as a function of sample noise,
for different functional dependencies. Plots are shown for
3 network models: the no-dependency network, the X -+
Y "true" network and the Y -+ X "opposite direction"
network. The shown functional dependencies of Y on X
are linear, quadratic, cubic, and sinusoidal. The Y axis is
the GP score for each family, and the X axis is the sample
noise units in standard deviations of the dependent variable

(Y).

paring the log loss of the predictions made by the depen­
dent model to those made by the independent model. One
can see that for the quadratic and sinusoidal relations, both
far from linear, the Gaussian method prediction quality is
the same for both models, while the GP learner continu­
ally performs better with the dependent model. The kernel
method, which is insensitive to directionality or linearity,
also performs better with the dependent model.
We now tum to comparing the reconstruction capability
of the three methods. We start with small artificial net­
works with different functional relations, and check which
method reconstructs the true network with higher accu­
racy. We sampled 50 and 100 instance data sets from 3
variable networks of all possible architectures, with linear,
quadratic, sinusoidal or mixed functional relations. A non­
correlated noise of width 0.4 of the variable's amplitude
was added. We applied the three network learning meth­
ods on these data sets. Both GP and kernel methods per­
formed well in reconstructing the true PDAG of the gener­
ating network, with the GP performing only slightly better.
However, the GP does significantly better in identifying the
original DAG for data sets with non-invertible connections
(quadratic and sinusoidal). In those cases, as expected, the
GP Ieamer orients the arcs in the "true" functional direc­
tion, while the kernel method does not necessarily do so.
The Gaussian network model does not perform as well in
this task, where in most of the cases with non-linear con-

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Log Likelihood ratio

GP family score
·3

X·>Yl.Dgl...XirldYLogl... .............
Y->XL..ogl.. ----

05

1

1.5

2

2.5

3

3.5

217

X ind V LagL ..-.,.·-··
Y->X LogL ----

r--����-.:-_,,.,y...._
=-�·
_

..

cube

lin

cube

lin

X->YL.ogi...X lnd Y Logl ...........
Y->XL.ogl ----

:: I/.----�--- �����
o•

o.•

0.2

-7

quad

�2

sin

Figure 3: Prediction accuracy as a function of sample noise,
for the same sets of Figure 2. The Y axis is the average log
loss of a test data set following parameter optimization on
a different set. The X axis is the sample noise in standard
deviations of the dependent variable.

nections, the learned networks are missing some of the arcs.
This is no surprise, since the best linear-Gaussian model
one can fit to a non-linear function often has a large vari­
ance, making this connection low scoring.
5.1

Real life data

We next wanted to test Gaussian Process Networks on real
world data sets of continuous attributes, comparing it to the
other two methods. We use three data sets from the UCI
machine learning repository [2]. These data sets are:
•

•

•

a data set describing differ­
ent aspects of neighborhoods in the Boston area, and
the median price of houses in those neighborhoods.
The data set contains 506 samples with 14 attributes.
300 samples were used as a test set.

Boston housing data set-

- a data set of physical measure­
ments of abalones. The data set contains 4177 sam­
ples with 9 attributes. 300 samples were used as a test
set.

Abalone data set

- a data describing the
material concentrations in glasses, with a class at­
tribute denoting the type of the glass. The data set
contains 214 samples with 10 attributes. 64 samples
were used as a test set.
Glass identification data set

For each data set, we performed structure learning with
each method, using subsets of the original data set, which
was permuted in a random order. We then used the learned
structure and the optimized parameters to predict the like-

/
----�
/-�
, � -------f,f
�,���������
0·

L...����-:-:�..,.-,-J
..

0

20 40 60

80 100 120 140 160 180

___
___
.....
.
.............._

I

..j

0

200

quad

20

4() 60

80 100 120 140 160 180

200

sm

Figure 4: Sample complexity comparison for the Gaussian,
Gaussian Process and Kernel methods. The plots show log
likelihood ratio of the test set, between the no-dependency
model and the X -+ Y model. Four different functional
relations between X and Y were tested. Both training and
test set have a noise level of 0.4 standard deviation of the
dependent variable.

lihood of the corresponding test sets, which were not in­
cluded in the training sets. Some of the attributes in those
data sets are either discrete (such as class attributes), or
have only few values in the data. To accommodate these
variables, we used the hybrid approach as described, for
example, in [14]. In this approach, all discrete variables
are forced to precede all continuous variables. For each
continuous variable X having some mixture of continuous
parents Uc and discrete parents Ud, we model the distribu­
tion P(X I Uc) separately for each state of Ud. The score
for such a family is given by

score(X, Uc, ud I D)=

L
UdEUd

score(X, Uc I DuJ

where Ud is the set of values taken by Ud, and
subset of data where Ud have values ud.

Dud

is the

Table 1 lists the average log likelihood of the test set,
for each method and each training set size. We note that
both in the glass and abalone domains the Gaussian pro­
cess method performs well in comparison to the Gaussian
model, while the kernel method does not do as well. On the
Boston domain, however, the kernel method seems to rate
quite high. This is due to one variable (index of accessibil­
ity to radial highways) which only has nine values appear­
ing in the data set. The kernel method assigns no parents to
this variable, and learns a distribution composed of sharp
"delta" peaks around those values. In cases like this, the
kernel method has to be bounded not to learn distributions
which are too sharp. Another option is to treat those vari-

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

218

Table 1: Average Log Loss on an independent test set achieved by the three methods for different training set sizes.
Boston

Size
10
20
50
100
150
200
300

Gaussian
-53.78
-40.92
-37.10
-34.44
-32.27
-30.97

GP
-28105.00
-447.85
-44.68
-50.75
-70.35
-43.52

Abalone

Kernel
-56.24
-40.65
-47.71
-132.27
4.37
8.32

Gaussian
-322.39
0.57
4.55
7.56
9.27
10.48
12.03

abies as discrete. In general, however, the Gaussian and GP
methods, modeling only function-like relations, can not ac­
count for multi modal distributions.
Figure 5 shows two examples from the abalone domain of
connections learned by the Gaussian Process network, plot­
ted with the training samples. The GP learner clearly fits
a non-linear function to the data, whose width varies ac­
cording to the density of points in each area. Beyond the
range of sample points, the width of the predicted function
rises, as the uncertainty increases. The figure on the right is
an example where the dependent variable is semi-discrete
(number of rings), showing that the method is capable of
handling this type of data as well. These examples show
that the Gaussian Process Network methods can reveal in­
teresting relations even under noisy measurements.

6

Discussion

In this paper we introduced the notion of Gaussian Pro­
cess networks and developed the Bayesian score for learn­
ing these. We report on preliminary results that show that
this method generalizes well from noisy data. The combi­
nation of this powerful regression technique with the flexi­
ble language of Bayesian networks seems like a promising
tool for exploratory data analysis, causal structure discov­
ery, prediction, and Bayesian classification.
There are several methods closely related to Gaussian Pro­
cesses that are relevant to this work. Wahba [31, 29] makes
a connection between Gaussian processes and reproducing
kernel Hilbert spaces (RKHS), showing that the solution
to the posterior Bayesian estimate of the Gaussian process
(as in Equation 4) is also the solution to a spline smooth­
ing problem posed as a variational minimization problem
in an RKHS. The smoothing parameter is optimized using
cross validation methods, whereas in the case of Gaussian
process priors, we use a MAP estimate for the hyperpa­
rameters. In related works (e.g. [30]), the relevance of the
different components of the function is estimated from the
learned smoothing parameters. In Gaussian process meth­
ods there is a similar notion, judging the relevance of dif­
ferent input dimensions by their estimated lengthscales in

GP
-319.84
-0.12
10.34
11.46
13.07
13.10
12.95

Glass

Kernel
-410.47
-9.28
-8.06
-7.01
-6.48
-34.10
-5.34

Gaussian
-43.81
-10.40
-6.61
-3.27
-2.47

GP
-153.77
-74.44
-51.34
-52.93
-2.02

Kernel
-72.76
-52.82
-84.01
-35.42
-42.80

the covariance function [21]. Inputs with estimated large
lengthscales are deemed less relevant, because the func­
tion hardly changes in those directions. A promising direc­
tion for future research is guiding the search in the network
space by those learned lengthscales, resulting in a more ef­
ficient and accurate search procedure. This is important
when using the Gaussian process score method, as its com­
putation is costly.
We are currently applying the Gaussian process network
method to analyze biological time series data. Our hope is
that by learning DBNs and the influences within them, we
would be able to understand the structure of the dynamics
that controls the generating processes. For example, we
might learn that dX depends on Y, which would give us a
clue as to the effects of Y's presence on X.
Acknowledgements

This work was supported by Israel Science Foundation
grant number 224/99-1 and by the generosity of the
Michael Sacher fund. Nir Friedman was also supported
by Harry & Abe Sherman Senior Lectureship in Computer
Science. Iftach Nachman was also supported by the Center
for Neural Computation, Hebrew University. Experiments
reported here were run on equipment funded by an ISF Ba­
sic Equipment Grant.
