
The paper concerns the problem of
predicting the effect of actions or interventions on a system from a combination of (i) statistical data on a set
of observed variables, and (ii) qualitative causal knowledge encoded in
the form of a directed acyclic graph
(DAG). The DAG represents a set
of linear equations called Structural
Equations Model (SEM), whose coefficients are parameters representing
direct causal effects. Reliable quantitative conclusions can only be obtained from the model if the causal
effects are uniquely determined by
the data. That is, if there exists
a unique parameterization for the
model that makes it compatible with
the data. If this is the case, the
model is called identified. The main
result of the paper is a general sufficient condition for identification of
recursive SEM models.
1

Introduction

Structural Equation Models (SEM) is one of
the most important tools for causal analysis in
the social and behavioral sciences [2, 5, 8, 1, 6,
7]. Although most developments in SEM have
been done by scientists in these areas, the theoretical aspects of the model provide interesting problems that can benefit from techniques
developed in computer science.
In a structural equation model, the relationships among a set of observed variables are
expressed by linear equations. Each equation
describes the dependence of one variable in

terms of the others, and contains a stochastic error term accounting for the influence of
unobserved factors.
An attractive characteristic of SEM models
is their simple causal interpretation. Specifically, the linear equation Y = Œ≤X + e encodes
two distinct assumptions: (1) the possible existence of (direct) causal influence of X on Y ;
and, (2) the absence of (direct) causal influence on Y of any variable that does not appear on the right-hand side of the equation.
The parameter Œ≤ quantifies the (direct) causal
effect of X on Y . That is, the equation claims
that a unit increase in X would result in Œ≤
units increase of Y , assuming that everything
else remains the same.
Let us consider an example taken from [10].
This model investigates the relations between
smoking (X) and lung cancer (Y ), taking into
account the amount of tar (Z) deposited in a
person‚Äôs lungs, allowing for unobserved factors
to affect both smoking (X) and cancer (Y ):
X = e1
Z = aX + e2
Y = bZ + e3
cov(e1 , e2 ) = cov(e2 , e3 ) = 0
cov(e1 , e3 ) = Œ≥
The first three equations claim, respectively,
that the level of smoking of a person depends
only on factors not included in the model, the
amount of tar deposited in the lungs depends
on the level of smoking as well as external factors, and the level of cancer depends on the
amount of tar in the lungs and external factors. The remaining equations say that the
external factors that cause tar to be accumulated in the lungs are independent of the external factors that affect the other variables,
but the external factors that have influence on
smoking and cancer may be correlated.

Œ≥

a

holds for criteria based on instrumental variables (IV) [12], since these require search for
variables (called instruments) that are uncorrelated with the error terms in specific equations.

b

X

Z

Y

(smoking)

(tar)

(cancer)

Figure 1: Smoking and lung cancer example
All the information contained in the equations
can be expressed by a graphical representation, called causal diagram, as illustrated in
Figure 1.
The process of data analysis using Structural
Equation Models consists of four steps [7]: (1)
specification of the model, (2) analysis of identification, (3) estimation of parameters, and
(4) evaluation of fit. In this work, we will concentrate on the problem of Identification. The
identification of a model is important because,
in general, no reliable quantitative conclusion
can be derived from a non-identified model.
1.1

Related Work

The question of identification has been the object of extensive research [6, 5, 10, 8, 11]. Despite all this effort, the problem still remains
open. That is, we do not have a necessary and
sufficient condition for identification in SEM.
Traditional approaches to the Identification
problem are based on algebraic manipulation
of the equations defining the model. Powerful algebraic methods have been developed
for testing whether a specific parameter, or a
specific equation in the model is identifiable
[6, 9]. However, those methods are limited in
scope. The rank and order criteria [6], for example, do not exploit restrictions on the error
covariances (if such are available). Identification methods based on block recursive models [6, 11], for another example, insist on uncorrelated errors between any pair of ordered
blocks.
Recently, some advances have been achieved
on graphical conditions for identification [10,
4]. Examples of such conditions are the ‚Äúbackdoor‚Äù and ‚Äúsingle-door‚Äù criteria [10]. A problem with such conditions is that they are applicable only in sparse models, that is, models
rich in conditional independence. The same

1.2

Overview of Results

In our approach to the problem, we state Identification as an intrinsic property of the model,
depending only on its structural assumptions.
Since all such assumptions are captured in the
graphical representation of the model, we can
apply graph theoretic techniques to study the
problem of Identification in SEM. Thus, our
main result consist of a graphical condition for
identification, to be applied on the causal diagram of the model.
The basic tool used in the analysis is Wright‚Äôs
decomposition of correlations, which allows us
to express correlation coefficients as polynomials on the parameters of the model.
Based on the observation that these polynomials are linear on specific subsets of parameters,
we reduce the problem of Identification to the
analysis of systems of linear equations. As one
should expect, conditions for linear independence of those systems (which imply a unique
solution and thus identification of the parameters), translate into graphical conditions on
the paths of the causal diagram.
2
2.1

Background
Structural Equation Models and
Identification

A structural equation model M for a vector of
observed variables Y = [Y1 , . . . , Yn ]0 is defined
by a set of linear equations of the form
X
Yj =
cji Yi + ej
, for j = 1, . . . , n.
i

Or, in matrix form, Y = C ¬∑ Y + Œµ, where
C = [cji ] and Œµ = [e1 , . . . , en ]0 .
The term ej in each equation corresponds to a
stochastic error, assumed to have normal distribution with zero mean. The model also
specifies independence assumptions for those
error terms, by the indication of which entries
in the matrix Œ® = [œàij ] = Cov(ei , ej ) have
value zero.
In this work, we consider only recursive models, which are characterized by the fact that

the matrix C is lower triangular. This assumption is reasonable in many domains, since it
basically forbids feedback causation.
The set of parameters of model M , denoted
by Œò, is composed by the (possibly) non-zero
entries of matrices C and Œ®.
A parameterization œÄ for model M is a function œÄ : Œò ‚Üí < that assigns a real value to
each parameter of the model. The pair hM, œÄi
determines a unique covariance matrix over
the observed variables, given by [2]:
h
‚àí1 iT

‚àí1
Œ®(œÄ) I ‚àí C(œÄ)
Œ£M (œÄ) = I ‚àí C(œÄ)
where C(œÄ) and Œ®(œÄ) are obtained by replacing each non-zero entry of C and Œ® by the
respective value assigned by œÄ.
Now, we are ready to define formally the problem of Identification in SEM.
Definition 1 (Model Identification) A
structural equation model M is identified if,
for almost every parameterization œÄ for M ,
the following condition holds:
Œ£M (œÄ) = Œ£M (œÄ 0 )

=‚áí

œÄ = œÄ0

(1)

More precisely, if we view parameterization œÄ
as a point in <|Œò| , then the set of points in
which condition (1) does not hold has Lebesgue
measure zero.
In general, if a model M is non-identified, for
each parameterization œÄ there exists an infinite number of distinct parameterizations œÄ 0
such that Œ£M (œÄ) = Œ£M (œÄ 0 ). However, there
are models in which, for almost every parameterization, there exist a finite number of
distinct parameterizations that generate the
same covariance matrix. According to the definition above, those models are classified as
non-identified. However, it is important to distinguish this situation from the general case of
non-identification. This motivates the following definition:
Definition 2 (K-Identification) A structural equation model M is k-identified if, for
almost every parameterization œÄ for M , the
number of distinct parameterizations that
generate the covariance matrix Œ£M (œÄ) is at
most k.


 
 
 
!#" $%'&$ (*),+-/+. 0
!#" $ (&$ 1*),+32/+. 0
!#" $ 1 &$4),+53+. 0

W

Œ±
Œ≤
a

Z

b
c

X

Œ≥

Y

Figure 2: A causal diagram
2.2

Graphical Representation

The causal diagram of a model M consists of a
directed graph whose nodes correspond to the
observed variables Y1 , . . . , Yn in the model. A
directed edge from Yi to Yj indicates that Yi
appears on the right-hand side of the equation
for Yj with a non-zero coefficient. A bidirected
arc between Yi and Yj indicates that the corresponding error terms, ei and ej , have non-zero
correlation. The graphical representation can
be completed by labeling the edges with the
parameters of the model. Figure 2 shows a
simple causal diagram.
A path between variables X and Y in a
causal diagram consists of a sequence of edges
he1 , e2 , . . . , en i such that e1 is incident to X, en
is incident to Y , and every pair of consecutive
edges in the sequence has a common variable.
We say that the path points to X if the edge
e1 has an arrow head pointing to X.
A path p = he1 , . . . , en i between X and Y is
valid if variable X only appears in e1 , variable
Y only appears in en , and every intermediate
variable appears only once in the path.
The special case of a path composed only by
directed edges, all of which oriented in the
same direction, is called a chain.
We make use of a few family terms to refer
to variables in particular topological relationships. Specifically, if the edge X ‚Üí Y is
present in the causal diagram, then we say that
X is a parent of Y . Similarly, if there exists a
chain from X to Y , then X is an ancestor of
Y , and Y is a descendant of X.
Given a path p between X and Y , and an intermediate variable Z in p, we denote by p[X..Z]
the path consisting of the edges of p that appear between X and Z.
Variable Z is a collider in path a p between X
and Y , if both p[X..Z] and p[Z..Y ] point to Z.
A path that does not have any collider is said
to be unblocked.

Z1
a

e
f

X1
Œ≥1

 
 

Z1
b

c1 c2

X2
Œ≥2

  
 
 
  

  !"$#
 % &  (') #
*,+.- /0132 46572498:; 4(<)=>?1$@
* +A/ 0B8:; 4(<)=>?1$CEDF4!1 @ 4G5 @

Y

Figure 3: Wright‚Äôs equations.
The depth of a node Y in a causal diagram is
defined as the length (i.e., number of edges) of
the longest chain from any ancestor of Y to Y .
Nodes with no ancestors have depth 0.
The next lemma gives a restriction on the
depth of intermediate variables in unblocked
paths:
Lemma 1 Let p be an unblocked path between X and Y , and let Z be an intermediate variable in p. Then, depth(Z) <
max{depth(X), depth(Y )}.
2.3

Wright‚Äôs method of Path Analysis

The method of path analysis [13] for identification is based on a decomposition of the correlations between observed variables into polynomials on the parameters of the model. More
precisely, for variables X and Y in a recursive
model, the correlation coefficient of X and Y ,
denoted by œÅXY , can be expressed as:
X
T (pl )
(2)
œÅX,Y =
paths pl

where the term T (pl ) represents the product
of the parameters of the edges along path pl ,
and the summation ranges over all unblocked
paths between X and Y . Figure 3 shows a
simple model and the decompositions of the
correlations of each pair of variables.
The set of equations obtained from Wright‚Äôs
decompositions summarizes all the statistical
information encoded in the model. Therefore,
any question about identification can be decided by studying the solutions for this system
of equations.
3

Analysis of Identification

The starting point for our analysis is the set of
equations provided by Wright‚Äôs decomposition

of correlations. Each term in this decomposition corresponds to an unblocked path in the
causal diagram. Now, observe that if we have
two edges pointing to the same variable, say
Y , then they cannot both appear in an unblocked path (because otherwise Y would be
a collider blocking the path). Hence, the expressions for the decomposition of correlations
are linear on the parameters of any subset of
edges incoming a variable Y (i.e., edges with
an arrow head pointing to Y ). This observation leads to the following method to decide
the identification of the model.
First, partition all the edges in the causal diagram into subsets of incoming edges. Then,
study the identification of the parameters associated with each subset by analyzing the solution of a system of linear equations.
Two conditions must be satisfied to obtain the
identification of the parameters. First, there
must exist a sufficient number of linearly independent equations. Second, the coefficients of
these equations, which are functions of other
parameters in the model, must be identified.
To address the first issue, we obtained a graphical characterization for linear independence,
called the G Criterion. The second point is
addressed by establishing an appropriate order to solve the systems of equations.
The following sections will formally develop
this graphical analysis of identification.
3.1

Basic Systems of Linear Equations

We begin by partitioning the set of edges in
the causal diagram into subsets of incoming
edges.
Fix an ordering ‚àÜ for the variables in the
model, with the only restriction that if
depth(X) < depth(Y ), then X must appear
before Y in ‚àÜ. For each variable Y , we define
Inc(Y ) as the set of edges in the causal diagram that connect Y to any variable appearing
before Y in the ordering ‚àÜ.
The next lemma formalizes some observations
made above.
Lemma 2 Any unblocked path between Y and
some variable Z can include at most one
edge from Inc(Y ). Moreover, if depth(Z) ‚â§
depth(Y ), then any such path must include exactly one edge from Inc(Y ).
Now, fix an arbitrary variable Y , and let

Z2
Z1
a

X1
Œª1

d
c

b

Œª2

Œª3

c

X2
Œª4

b
Œª3

b

c

X2
Œª2

a

Z1

X1
Œª1

Z2

a

Z1

Z2

X1
Œª1

Œª4

X2
Œª2

Œª3

Y

Y

Y

M1

M2

M3

Œª4

Figure 4: Models M1 , M2 and M3
Œª1 , . . . , Œªm denote the parameters of the edges
in Inc(Y ). Then, Lemma 2 allows us to express the correlation between Z and Y as a
linear equation on the Œªj ‚Äôs:
œÅZ,Y = a0 +

m
X

a j ¬∑ Œªj

j=1

Observe that the independent term a0 is 0 if
depth(Z) ‚â§ depth(Y ).
Now, given a set of variables Z =
{Z1 , . . . , Zk }, we let Œ¶Z,Y 1 denote the system
of equations corresponding to the decompositions of correlations œÅZ1 Y , . . . , œÅZk Y :

Œ¶Z,Y

Ô£±
m
X
Ô£¥
Ô£¥
=
a
+
a1j ¬∑ Œªj
œÅ
Ô£¥
10
Z1 Y
Ô£¥
Ô£¥
Ô£≤
j=1
...
=
m
Ô£¥
X
Ô£¥
Ô£¥
Ô£¥
Ô£¥
œÅ
=
a
+
akj ¬∑ Œªj
k0
Ô£≥ Zk Y

(3)

Consider the models in Figure 4. In each of
those cases, the only possible choice for an
auxiliary set for Y is {X1 , X2 , Z1 , Z2 }. However, this set only satisfies the definition for
models M1 and M3 . The problem with model
M2 involves variables Z1 and Z2 , because the
decomposition of their correlation with Y are
not linearly independent:

œÅZ1 Y = aŒª2 + bŒª3
œÅZ2 Y = caŒª2 + cbŒª3 = c ¬∑ [aŒª2 + bŒª3 ]
This situation is reflected in the causal diagram by the fact that every unblocked path
between Z1 and Y can be extended by the edge
Z2 ‚Üí Z1 to give an unblocked path between
Z2 and Y , and those are all unblocked paths
between Z2 and Y .
The problem is avoided in the other models
because, in M1 there exist disjoint paths connecting Z1 and Z2 to Y , and in M3 if we extend the path Z1 ‚Üî X2 ‚Üí Y with the edge
Z2 ‚Üí Z1 we obtain a blocked path.
In general, the situation can become much
more complicated, with one equation being a
linear combination of several others. However,
these examples illustrate the essential graphical properties that characterize linear independence.
G Criterion: A set of variables Z =
{Z1 , . . . , Zk } satisfies the G criterion with respect to Y if there exist p1 , . . . , pk such that:

j=1

3.2

Auxiliary Sets and Linear
Independence

Following the ideas presented in the beginning
of the section, we want to find a set of variables
that provides a system of linearly independent
equations. This motivates the following definition:
Definition 3 (Auxiliary Set) A set of variables Z = {Z1 , . . . , Zk } is an Auxiliary Set for
Y if |Z| = |Inc(Y )| and the system of equations Œ¶Z,Y is linearly independent.
Next, we obtain a graphical characterization
for Auxiliary Sets. We first analyze a few examples, and then introduce our G criterion.
1
Whenever clear from the context, we drop the reference
to Y and simply write Œ¶Z .

(i) pi is an unblocked path between Zi and Y
including some edge from Inc(Y );
(ii) If paths pi and pj have a common variable
U , then either
a) both pi [Zi ..U ] and pj [U..Y ] point to U ;
or
b) both pj [Zj ..U ] and pi [U..Y ] point to U .
Note that the second condition above basically
states that two paths pi and pj cannot be broken at a common variable U and their pieces
be rearranged to form two unblocked paths.
As it turns out, the graphical conditions in the
G criterion precisely characterize the linear independence of the system (3). This is formally
stated in the next theorem (see appendix A for
a proof):

Theorem 1 A set of variables Z
=
{Z1 , . . . , Zk }, with |Z| = |Inc(Y )|, is an
auxiliary set for Y if and only if it satisfies
the G criterion.


  
 
 

X

Y

3.3

Model Identification Using
Auxiliary Sets

Z

Suppose now that we can find an auxiliary set
AY for each variable Y in the model. This
implies that for each Y there exists a system
of linear equations Œ¶AY that can be solved
uniquely for the parameters Œª1 , . . . , Œªm of the
edges in Inc(Y ). This fact, however, does not
guarantee the identification of the Œªi ‚Äôs, because the solution for each Œªi is a function of
the coefficients in the linear equations, which
may depend on non-identified parameters.
To prove identification we need to find an appropriate order to solve the systems of equations. Let us consider a simple situation. Suppose that for each variable Y the following condition holds:
depth(Zi ) < depth(Y ), for all Zi ‚àà AY

(4)

Now, consider the linear equation provided by
the decomposition of œÅZiY . The coefficients
in this equation are sums of terms associated
with unblocked paths between Zi and Y . From
condition (4) and lemma 1, it follows that
all such paths include only variables at depth
smaller than depth(Y ). Thus, if we solve the
systems associated with all those variables before solving Œ¶AY , then the coefficients of Œ¶AY
will be identified.
Theorem 2 If every variable Y has an auxiliary set satisfying condition (4), then the
model is identified.
In the general case, however, the auxiliary set
for some variable Y may contain variables at
greater depths than Y , or even descendants
of Y . This forces us to solve the systems of
equations in a different order than the one established by the depth of the variables.
A close inspection on the coefficients of Œ¶AY
shows that it is sufficient to solve the systems
associated with some Zi ‚Äôs in AY before solving
Œ¶AY . There are basically two cases:
1) Zi is a descendant of Y ; or
2) Zi is a non-descendant of Y , but there is
an unblocked path between Zi and Y of the
form Zi ‚Üê . . . ‚Üê ¬∑ ‚Üî Y .

W

X

W

Z

Y

Figure 5: Example Auxiliary Sets method.
We can represent those restrictions by a directed graph, called dependence graph.
The next theorem states our general sufficient condition for model identification (see
appendix A for a proof):
Theorem 3 If there exist auxiliary sets for
the variables in the model such that the associated dependence graph is acyclic, then the
model is identified.
Figure 5 shows an example that illustrates the
method just described. Apparently, this is a
very simple model. However, it actually requires the full generality of our method.
4

Discussion

The graphical condition presented in this paper is the most general sufficient condition
for identification of recursive SEM available
in the literature. Hence, a natural question
is whether it is also necessary for the identification of the model.
We first observe that it is not hard to derive a proof for the non-identification of the
model if there exists a variable with no auxiliary set. Thus, we only need to investigate
if models with a cyclic dependence graph are
non-identified.
An interesting situation occurs if the dependence graph has a single cycle. For example,
suppose there is a cycle with the variables X,
Y and Z (i.e., X ‚Üí Y ‚Üí Z ‚Üí X). If this is
the only cycle in the graph, then we may solve
all systems that need to be solved before Œ¶AX ,
except for Œ¶AZ .
At this point, we fix some parameter in Œ¶AX ,
say Œª, as a constant, and remove the equation
associated with œÅZX from Œ¶AX . Using the remaining equations in Œ¶AX , we obtain expres-

sions for the other parameters in terms of Œª.
Once Œ¶AX is solved, we can proceed to solve
Œ¶AY and then Œ¶AZ , obtaining expressions in
terms of Œª for all parameters in those systems. Finally, substituting those expressions
back into the equation associated with œÅZX ,
we obtain a polynomial on the parameter Œª.
If the polynomial does not vanish, this implies
that Œª can assume only a finite number of distinct values (namely, the roots of the polynomial). For each such value we have a distinct
parameterization for the model that generates
the same covariance matrix. Hence, the model
is k-identified, for some k. In [3] we present a
2-identified model.
We believe that the polynomial mentioned
above never vanishes, and conjecture that if
the dependence graph has only isolated cycles
(i.e., cycles with no common variable) then the
model is k-identified.
When the dependence graph has multiple cycles with common variables, the application
of the method above leads to systems of nonlinear equations on two or more parameters.
Perhaps a closer examination of the structure
of these systems may allow to decide the identification status of the model.
In [4], we provided a procedure to find an auxiliary set for a given variable Y . The procedure reduces the problem to a max-flow computation and executes in time O(n3 ). Together with Theorem 2, this gives an algorithm for testing the identification status of
the model. An algorithm implementing the
more general result of Theorem 3 would require finding sets of auxiliary variables that
give rise to an acyclic dependence graph.
