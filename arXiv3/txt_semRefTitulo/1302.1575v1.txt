

policy

1!'.

The value function of an optimal policy is

usually referred to as the

denoted by v·.

P lanning problems where effects of actions

optimal value function

and

are non-deterministic can be modeled a8

MDPs have been studied extensively in the dynamic

Markov decision processes.

programming literature (e.g.

Planning prob­

lems are usually goal-directed.

man

This paper

1990,

BertsekaB

1987,

1960, Puter­
1993}. Dean and
Wellman (1991) ini­

Howard

White

proposes several techniques for exploiting the
goal-directedness to accelerate value itera­

Kanazawa (1989) and Dean and
tiated the use of MDPs in planning problems where

tion, a standard algorithm for solving Markov

effects of actions are not deterministic. Planning prob­

decision processes.

Empirical studies have

lems typically have a large number of states. Solving

shown that the techniques can bring about

MDPs with large state space has hence become a hot

significant speedups.

topic in AI (e.g. Dean

et al1993,

Boutillier

e t al1995).

A planning problem can be modeled as an MDP in
such way that (1) there is a state designated to be
the

Keywords: decision-theoretic planning, Markov de­
cision processes, value iteration, efficiency.

1

goal

( ) {

r sa '

INTRODUCTION

ing an actiona ha8 two consequences: The agent re­

ceives an immediate reward r (s,a), which depends on
the current state 8 of the world a8 well a8 the action
executed, and the world probabilistically moves into

transition probability

P(s'ls,a).
The action is chosen based on the current state of the
world. A

poli cy

1r

prescribes an action for each possi­

ble state. In other words, it is a mapping from the set

S of all possible states to A. The set of possible states
is assumed to be finite in this paper. The quality of

a policy 1r is mea8ured by its

(

for any states,
reward V,..

8)

V'"(s)

-

and (3) the

In a Markov de cision process (MDP), an agent must,
at each time point, choose an action from a finite set
A of possible actions and execute the action. Execut­

another state s' according to a

and an action called

value function V,..(s);

1

if

0

otherwise;

1r,

re­

optimalif v1r• (s)� V,..(s) for any states and any other

ceives starting from an initial state s. A policy rr• is

the

a=delcare-goal and s=goal,

action declare_goal cannot

(1)

be executed

more than once. We call MDPs with such properties

goal-directed MDPs.
Value iteration is a standard algorithm for solving
MDPs. This paper proposes several techniques for ac­
celerating value iteration in goal-directed MDPs. Let
us begin with a brief review of value iteration and of
previous works

2

on

speeding up value iteration.

VALUE ITERATION

A value function is a mapping from the set S of pos­
sible states to the real line. Given a value function V,
define another value function TV by

TV(s)= max0[r(s,a) + 'Y

L P(s'ls,a)V(s')]

(2)

s'

is the expected total discounted

the agent, under the guidance of

declare-goal; (2)

reward function r ( 8, a) is given by

for each state

s,

where 0::;"(<1 is a discount factor.

T is an mapping from the space of value functions to
itself. For any function V, its norm !lVII is defined

490

Zhang and Zhang

IIVII = max.!V(s)l. Tis the contraction mapping
Puterman 1990) in the sense that for any two
value functions U and V,

by

(e.g.

IITU -TVII � 'YIIU- VII·

2, subtracts an appropriate value function from Vn
and MacQueen (1969) proposes to subtract V..(s 0) ­
the value of

by Schweitzer
tanon

For any positive number £, we say that a value function

V

is

Vn

(1989)

et al (1985}

and Bertsekas and Cas­

interleave standard VI steps with aggre­

gation/disaggregation steps, which improve the cur­
rent value function by solving the optimality equation

f.-contracted if

for an simpler MDP obtained from the original MDP

through state aggregation. Dean and Lin
The optimal value function satisfies the

tion

optimal equa­

(1995)

and

decompose an MDP with a large

state space into a number of MDPs with smaller state

used to construct a solution to the original MDP.
Three pieces of previous workd are of direct relevance

induces a policy through

1r(s) = arg maxa[r(s, a)+'Y

L

P(s'ls , a)V(s'))]. (3)

s'

IT the value function is f.-contracted for a small number
€1

et al (1997)

are solved using standard VI and their solutions are

and hence is 0-contracted.

V

Dean

spaces through state aggregation. The smaller MDPs

V* = TV*,

A value function

so.

itself at a predetermined state

The aggregation/disaggregation techniques introduced

the induced policy is "good enough" in the sense

that

to this paper. The first one is the Gauss-Seidel variant
of standard VI proposed by Hastings

(1969).

Let p be

an ordering among the possible states. Instead of the
operator

T

defined in equation

variant uses another operator

(2),
T' to

the Gauss-Seidel
improve the cur­

rent value function. For any value function V,

T'V(s)

is defined for each state s by starting from the state

IIV11" - V*ll � 2£")'
1--y

(4)

•

Proof of this inequality can be found in, for instance,
Puterman (1990). It is evident the policy induced by

that comes first in the ordering p and moving back­
wards. The values

T'V(s)

for earlier states are used

in defining the values for later states. Specifically,

T'

is given by

the optimal value function is optimal.
Value iteration (VI) (Bellman

1957)

starts with an ar­

T'V(s) = maxa[r(s,a)+1' LP(s'ls,a)V (s'),]

bitrary value function and improves it iteratively until

(5)

•'

the value function becomes t:-contracted. Here is the
pseudo-code.
VI

1.

Choose an initial value function
set n=O.

2. Vn+l =TVn.
3. If IIVn+l- Vnii>E,
go to step 2.
4. Else return

V0

where V(s')=T'V(s1) when s' comes before
ordering p and V(s')=V(s') otherwise.

and

The anytime algorithm presented in Dean

in the

et al (1993)

is also closely related to the methods to be proposed in
increment n by

1

this paper. The algorithm restricts standard VI inside

and

an envelope, a subset of possible states, that contains
at least one path from the initial state to the goal state.

Vn+l·

The envelope is gradually enlarged to get better and

T is a contraction mapping, IITVnH - Vn+l II =
IITVn+l - TVn ll � 1'IIVnH- Vnl l � €. Hence, Vn+l is

Since

£-contracted.

3

s

better solutions.
Boyan and Moore

(1996)

study value iteration in

acyclic goal-directed MDPs. A goal-directed MDP is
acyclic if once leaving a. state, the world can never
come back to that state again.

PREVIOUS WORK

Boyan and Moore

point out that value iteration for goal-directed acyclic

VI converges geometrically at rate 'Y·

MDPs can be carried out in one sweep by starting from
Convergence

the goal state and working backwards. Thereby the

Various modifica­

tions to standard VI have been proposed and all have

amount of computations needed to compute the opti­

been theoretically or empirically shown to lead to

iteration of standard VI. The method is an extension

is slow when 1' is close to 1.

faster convergence.

Morton and Wecker

(1977)
T in

gest that one, before applying the operator

sug­
step

mal value function is reduced to that needed in one
of the DAG-SHORTEST-PATH algorithm (Carmen

al1990) for finding

shortest path in acyclic graphs.

et

Value Iteration for Goal-Directed MOPs

4

491

PARSIMONIOUS VALUE

There is no guarantee that the value function returned

ITERATION

by PVI is €-contracted. However, the value function
should be close to be €-contracted. We suggest to use

We introduce several new variants to standard VI for
goal-directed MDPs. Called

tion (PVI),

parsimonious value itera­

the first variant relies on the following in­

PVI as a preprocessing step to VI, i.e.

to use the

value function it returns as the initial value function
of VI. This way an €-contracted value function can

tuition. Suppose value iteration begins with the zero

be obtained. Since the value function return by PVI is

value function.

numer of steps. In our experiments, it t erminat ed in

Then at early iterations, the value

function remains zero for states far away from the goal.
At later iterations, the value function does not change
much for states close to the goal. The number of states
whose values change significantly from one iteration to
the next can be much smaller than the total number
of states.

At each iteration, PVI updates the value

for a state only when the value is expected to change
significantly.

iteration n+1 (n�1),

PVI performs a test to

detect states whose values change substantially from
iteration n to iteration

n+1.

The value of a state is

updated only if it passes the test.
Let

Vn-1

and

Vn be the value functions PVI computed

at the previous two iterations.

At the current itera­

s if IVn(81) - Vn-1 (81) I '$ 5 for a small positive con­
stant 8 and each state 81 such that maxaP(81 j s, a)>O.
Since the number of states reachable from s by execut­

ing one action is usually small, this test is cheap. It
is usually much cheaper than calculating

TVn(s),

es­

pecially when one maintains a list of nodes reachable
from each state by executing one action.

Theoretical underpinings of the test are as follows. If

the value functions

Vn

and

Vn-1

were the value func­

tions computed by VI, one could easily show that if

IVn+l (s) - Vn(8) I'$ -yo. In

s passes the test then

words, the value for
n

8

other

does not change much from it­

to iteration n+ 1.

Here is the pseudo-code for PVI.

PVI
1. Vo(s)=O for

any 8, n=O.

2. For each states,
(a)

The idea behind PVI is rather similar to the idea
underlying Boyan and Moore's one--sweep algorithm;
start from the goal and work backwards. PVI does not

assume acyclicity and hence is more general. W hen the
MDP is acyclic, it is almost identical to the one--sweep

Ifn�1 and 1Vn(8')-Vn-1(8')1 � Hor
all 81 such that maxaP(s'ls, a)>O,
Vn+l(s) = V..(s).

et al ( 1993)

in the sense that values are updated only

for some states at each iteration.

The difference lies

in the fact that in PVI the states whose values are

the anytime algorithm whether the value for a state

is updated depends on whether it is in the envelop
and does not change with iteration.

Also the entire

value iteration process needs to be carried out for each
envelop.

5

GREEDY AND DOUBLE VALUE
ITERATION

Even though the test in PVI is cheap, the fact that
it has to be carried out for each state

is

somewhat

unsatisfying. Greedy value iteration (GVI) avoids the
test by working

in a way similar to DAG-SHORTEST­

PATH.
Before describing GVI, we need to introduce t he con­

s' is ideally
reachable in one step fr om another state s if after exe-­
cuting a certain action in state s the probability of the
world ending up in state 81 is the highest. A state s�e
is ideally reachable in k steps from another state s0 if
there are states 811
, s��:-1 such that si+l is ideally

cept of ideal reachability. We say a state

reachable from Bi in one step for all O�i:::;k-1. Any
• • •

state is ideally reachable from itself in 0 step.
For any state s, let d{s) be the minimum number of
steps in which the goal is ideally reachable from

(b) Else

shall refer to d( s) as the

Vn+l(s) = TVn(s).
3. If IIVn+l - Vn II>£, inc re ment n by 1 and
go to step 2.

4. Else return

PVI is also related to the anytime algorithm by Dean

updated change from iteration to iteration, while in

tion n+ 1, PVI does not update the value for a state

eration

just one iteration.

algorithm.

Specifically, PVI begins with the zero value function.
At each

close to be co-contracted, VI should terminate in a small

Vn.H·

distancefrom s

8.

We

to the goal.

At each iteration n, GVI only updates the values for
the states from which the goal is ideally reachable in
n steps. Let N be the maximum number of steps that
the goal can be ideally reached from any state. Then
GVI terminates in exactly N iterations. For later con-

492

Zhang and Zhang

venience, we assume that GVI takes a value function

where

a.s input and uses it a.s the initial value function. Here

,
V(s,s) =
A

is the psuedo-code for GVI.

1. For n=O

ping.
toN,

if d(s' ) <
otherwise.

2. Return

Hence the value function returned by DVI is

It is evident see that DVI is almost identical to the
Gauss-Seidel variant of standard VI, except that it pro­

If d(s)==n, set
Vn+I(s) :::::: TVn(s).

poses one particular way to order the possible states;
the states are ordered according to their distances to

(b) Else

the goal. By introducing DVI through GVI, we hope
to provide another way of looking at the Gauss-Seidel
variant of standard VI in the context of goal-directed
MOPs.

VN.

When the MDP is acyclic, GVI is identical to Boyan
and Moore's one-sweep algorithm and hence returns
the optimal value function. When the MDP is cyclic,
however, the value function it returns could be of very
poor quality. Using it a.s a preprocessing step to VI
might not help much.

6

IMPROVING PVI

The alternative understanding of DVI can be used to
improve PVI. We call the improved algorithm PVIl.
The pseudo-code is a.s follows.

PV!l

On the positive side, the amount of computations GVI

1. Vo(s)=O for any s, n=O.

does is identical to that carried out by one iteration of

2. For

standard VI. Also because GVI is an approximation of
the entire value iteration process, the extent to which
it improves the input value function should be greater
than that brought about by one iteration of standard

m=O

toN,

(a) For each state

(b)

VI. Thus we can expect VI to converge faster if the

s such that d(s)=m
lfn�1 and IVn(s')- Vn-l(s')l � o for
ails' such that maxa.P(s'ls,a)>O,
Vn+l(s) = Vn(s).

second line is replaced by "Vn+l = GVI(Vn)". This
leads to new algorithm called double value iteration

(c) Else

(DVI) .

Vn+l(s)
Choose an initial value function

n=O.
2. Vn+l = GVI(Vn)·
3. If IIVn+t(8)- Vn(8)11>e,
1 and go to step 2.
4. Else return Vn.

V0

3. IfiiV.-.+1- V.-.li>e,

and

set

increment

n by

new operator T', instead of the operator T given in
Equation (2), to update the value function Vn· For

any value function V, T'V(s) is defined for each state
8 by starting with the goal state and gradually moving
away. The value T'V(s) for a state s is defined after
the values T'V(s') for all the states s1 closer to the

8

having been defined. It is given by

T'V(s) :::::: max8[r(s, a)+')' 2:: P(s'ls,a)V(s', s), ]
s'

=

T'Vn(s).

increment

n by 1 and

go to step 2.

As it turns out, DVI can be described directly with­
out the reference to GVI. At each iteration, it uses a

goal than

d(s),

€-contracted.

For each state s,

(a)

DVI
1.

V(s')

It can be proved that r is also a contract map­

GVI(VQ)

•

{ TV(s')

4. Else return Vn+l·
As PVI, PVIl should be used a.s a preprocessing step
to VI.

7

EXPERIMENTS

Preliminary experiments have been carried to compare
the algorithms proposed in this paper with standard
value iteration. Four office environment navigation

problems borrowed from Ca.ssandra et al (1996) were
used . The problems differ in corridor layout and the
total number of states. There are two sets of transition
probabilities, referred to a.s standard and noisy tran­
sition probabilities respectively. Effects of actions are
less certain under noisy transition probabilities than
under standard transition probabilities.

493

Value Iteration for Goal·Directed MDPs

-- -

-- -

1.$

..,.. ......
---

1.4

'VI"-+"DVV" .....
wr ......
W11" ......

'Dill"�

Wl1.. ..,._

1.2

l

l

l

l

0.8

...

o.e
OA
0.2
0
2<)0

210

220

1.4

1
I

...,

... ...
--

vo

no

DO

0
100

:100

...

�- -

Ul

...

""'

&DO
--

..,.

700

60(>

-� - -

'VI'+"D\11" .....

---

""F�·u·...-

1.2

I

"'Il"+"OVI" -+'Wr+"PV''f" .......

1

I
---·-

�00�-��-�---�--�&00�--�-�-�700-�60<>
-Figure 1: Convergence times of the algorithms in four
navigation problems.

Figure 2: Differences in perfor manc e among the
rithms as problem size increases.

algo­

The threshold for the Bellman residual was set at 0.001
and the discount factor at 0.99. Figure 1 shows that
convergence times of the algorithms in the four prob­
lems. The X·axis repr es ents the sizes of the probl em ,
while theY-axis represents convergence time in CPU
seconds. Data were collected using a SPARC20. The
curves VI and DVI display the convergence times of
VI and DVI respectively, while PVI and PVIl display
the convergence times for the combinations of PVI and
PVIl with VI.

The convergence times are shown in Figure 2. We see
that the differences in performance among the algo­
rithms become larger as the problem size increases.
In the smallest problem PVIl converges about three
times faster than VI, while in the largest problem it

Under both standard and noisy transition probabili­
ties, DVI and PVI converges much faster than VI and
PVIl converges even faster. DVI converges slightly
faster than PVI in the smallest problem but slower in
all other problems. Performances of all algorithms are
slightly worse under noisy transition probabilities than
under standard transition probabilities. Their differ­
ences are also slightly larger.

We propose several techniques for exploiting the goal­
directedness of planning problems to speed up value it­
eration for their MDP models. Empirical studies have
shown that the techniques can bring a bo ut significant
speedups.

gain an idea about how the comparisons change
with problem sizes, we made copies of one environment
and glu e them together to form larger environmen ts .

be modeled as partially obs ervable

To

converges six times faster.

8

CONCLUSIONS AND FUTURE
DIRECTIONS

MDPs
world.

know

assume

perfect observation

of

the

state

of

the

In many real-world problems, one does not

Such problems can
MDPs (POMDPs).
POMDPs are much harder to solve than MDPs. We
are currently investigating the possibility of applying
the true state

of the

world.

494

Zhang and Zhang

the ideas introduced in this paper to POMDPs.
Aclcnowledgements

We thank Peter Dayan, Thomas L. Dean, and Michael
Littman for pointers to references and thank Wenju
Liu and D. Y. Yeung for useful discussions. Re­
search was supported by Hong Kong Research Coun­
cil under grants HKUST 658/95E and Hong Kong
University of Science and Technology under grant
DAG96/97.EG01(RI).
