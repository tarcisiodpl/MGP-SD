

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

We analyzed the convergence properties of likelihood­
weighting algorithms on a two-level, multiply
connected, belief-network representation of the QMR
knowledge base of internal medicine. Specifically, on
two difficult diagnostic cases, we examined the effects
of Markov blanket scoring, importance sampling, and
self-importance sampling, demonstrating that the
Markov blanket scoring and self-importance sampling
significantly improve the convergence of the simulation
on our model.

Each of then diseases {DJ,...,Dn} may be present or
absent in a patient, and each of the m findings
{FJ,....Fm} may be unobserved or observed to be
present or absent. We define d; as the state of disease
D;. If D; is present, then d;= present; ifD; is absent,
then d;= absent. We use P(Di I e) as a shorthand for
P(di= present I e), where e is any evidence observed.
We refer to a disease hypothesis H as an assignment of
present or absent to each disease in D
{ t .Dn}. that is,
•...

,.

H=U d
1. Introduction

The Quick Medical Reference (QMR) program is a
decision-support tool for diagnosis in internal medicine
that was developed at the University of Pittsburgh as
the successor to IN1ERNIST-1 [Miller, Pople, et al.,
1982]. Designed to assist a physician in making
difficult diagnoses, QMR is built on one of the largest
knowledge bases (KBs) in existence. We are developing
the foundation for a decision-theoretic version of QMR,
which we call QMR-DT for Quick Medical Reference­
Decision Theoretic.
Our research to date has focused on building the
QMR-DT KB, a probabilistic reformulation of the
QMR KB, and on developing a method for inference on
the QMR-DT KB. In this paper, we concentrate our
discussion on likelihood weighting as a method of
inference. Before describing the likelihood-weighting
algorithm, we briefly examine the QMR-DT KB.
2.

Also, we define H+ to be the set of diseases Di such
that di=present in H.
Let Np be a set of findings observed for a particular
patient, where Np+ is the set of findings observed to be
present and N p- is the set of findings observed to be
absent. Note that many findings may be unobserved and
thus appear in neither Np+ nor Np-. We define .fj as the
state of finding F j.

The QMR-DT Model

We have reformulated the associations between
diseases and findings of the INTERNIST-1 disease
profiles [Miller, Pople, et al., 1982] into a belief­
network representation. I This reformulation is described
in [Beckerman, 1989; Henrion, 1988; Shwe, Middleton,
et al., 1990]. The QMR-DT KB consists of a two-level
belief network of n diseases and m findings, as shown
in Figure 1.
1 We are currently using the INTERNIST-I KB (circa 1986),
rather than the more recent QMR KB. These two KBs are
qu ite similar, to the extent that the methods in this paper
are applicable to transforming the latter KB as well. For
simplicity,
where the d istincti on between the
INTERNIST-I KB and QMR KB is inconsequential, we will
refer to the INTERNIST-1 KB as the QMR KB

Figure 1 The two-level belief-network representation of
the current QMR-DT KB. The disease nodes are labeled
DJ, ....Dn and the finding nodes are labeled FJ, ... .Fm·
The probabilistic dependencies between diseases and
findings are specified with directed arcs between nodes,
where an arc points in the causal direction that we
assume; that is, we assume that diseases cause findings.
An arc of probabilistic dependency between nodes
representing a diseaseD; and finding F j exists in the
QMR-DT KB if and only if there exists a link between
D; and F j in the QMR disease profile of D;. Disease­
to-disease dependencies are not modeled presently in the
QMR-DT KB. The current QMR-DT KB contains n =

499

I
I

534 adult diseases and m = 4040 fmdings, with 40,740

arcs depicting disease-to-finding dependencies.
To reduce the representational and computational
complexity of QMR-DT, we made several simplifying
assumptions. Assumptions evident from Figure 1
include marginal independence of diseases, conditional
independence of findings given any hypothesis of
diseases, and the assumption that findings are
manifestations of disease. Also, we assume that diseases
and findings are binary valued. The assumption that
diseases are marginally independent allows us to
calculate P(H) by
"

P(H)

fl P(d;)

=

i•l

(1 )

The assumption that findings are conditionally
independent given any disease hypothesis allows us to
calculate P(Np 1 H) as

P(Np Ill)= ll Pifi I H)
FjeNp

(2)

We model the influence of multiple diseases on a
finding assuming causal independence in a noisy-OR
gate interaction [Pearl, 1988]. Under the assumption of
a noisy-OR gate, we can avoid representation of the
full set of conditional probabilities of the state of a
finding given each possible state of the finding's
parents. Consider a belief network with binary finding
Fj where Fj has binary parents Dt.D2 .... ,Dt. To
construct the complete conditional probability table
associated with the arcs from D1.D2.. . . ,Dk to Fj. we
would need to acquire a conditional probability for each
of the 2k states of the parents of Fj. If we assume
causal independence, we need to acquire only k
conditional probabilities of the form P(Fj lonly D;),2
where 1 � i � k. These conditional probabilities are
derived from a mapping of QMR frequencies to
probabilities [Shwe, Middleton, et al., 1990]. This
mapping was assessed from Randy Miller, one of the
primary developers of INTERNIST-I and QMR. The
results of the mapping appear in T abl e 1.
Assuming a noisy-OR gate interaction among
diseases on a finding, we compute Pifj I H) as

where n(Fj) are the diseases that are parents of Fj.
In addition to obtaining the conditional probabilities
relating findings to diseases, we derived prior
probabilities on diseases in the QMR-DT KB from data
compiled by the National Center for Health Statistics
(NCHS) on approximately 192,000 inpatients
discharged from short-stay nonfederal hospitals in 1984
[Lawrence, 1986].
Table 1 A mapping between QMR frequencies and
probabilities
QMR

P (Fj I only Di)

1

0.025
0.20
0.50
0.80
0.985

frequency
2
3
4
5

I
I
I

3. Algorithms for Inference

3.1 Exact Algorithms

We refer to Bayes' rule under the assumptions of single­
disease hypotheses and conditional independence of
findings as tabular Bayes' rule'3:
R

I

I

Given a set of positive and negative findings Np and a
model of the dependencies between diseases and fmdings
in internal medicine, our goal is to compute P(Di I Np),
the posterior marginal probability for each disease
D;: 1 � i � n. We contrast P(D; I Np) with P(only
D; I Np, J.J.), where J1. is the assumption that diseases are
mutually exclusive. This assumption is clearly not
applicable to the general problem of diagnosis in
internal medicine, where patients often have several
diseases simultaneously. The posterior marginal
probability, on the other hand, implicitly acknowledges
the possibility of there being one or more diseases in a
patient. In the next subsection, we first discuss the
complexity of calculating P(only D; I Np, J.l.), and then
describe the complexity of calculating P(D; I Np).

P(only D; 1 Np, J.l) =

I

P(NF I only D;) P(only D;)

I, P(Np I only D�c) P(only D�c)

I
I
I
I
I
I
I

i-1

(4)

2 We distinguish P(Fj I only D;) from P(Fj I D;), where the

fonner denotes the probability of the event that Fj occurs
given that only D ; occurs, and that, for all k�i. D k is
absent. By contrast, we use the notation P(Fj I D;) to mean
the probability of the event that F occur s given that D i
occurs and that for all ui, each Dk occurs based on ics prior
probability.

tabular B�yes' rule is derived from the notion
that we can compute P(only D; I Np, JL) as in Equation 4

3 The name

from a n X m table of probabilities of the

D;), where 1 S iS nand 1 Sj Sm.

form Pifj I

only

I
I
I
I

500

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

Although the single-disease assumption is extremely
restrictive, the tabular Bayes' formulation is appealing
because of its low degree of computational complexity,

O(nm).
Consider generalizing to allow the diagnostic
hypothesis H to contain any subset of diseases in the
KB. This generalization is consistent with the QMR­
DT model. Straightforward application of Bayes' rule to
the QMR-DT two-level belief network yields an

O(m'n2n) [Shwe, Middleton,
2n term arises since the denominator

inferential complexity of
et al., 1990]. The

must be summed over 2n disease hypotheses:

L

P(Np I H) P(H)

described by Fung and Chang [Fung & Chang, 1989]
and by Shachter and Peat [Shachter & Peot, 1989].
The likelihood-weighting scheme instantiates each
nonevidence node based on the state of the node's
parents and computes efficiently the likelihood of the
instantiated state of the unobserved nodes given the
evidence. The likelihood score, called the sample score,
is then added to a slot for each event that occurs in the
trial. After a specified number of trials, we estimate the
probability of an event by dividing the aggregate sample
scores of the event by the total aggregate sample scores.
More formally, in the case of the two-level QMR-DT
belief network described in Section 2, we estimate the
marginal posterior probability of Equation 5 by

H :D ;e H •______

P(di I Np) = ;;.,_..:.. _ _
L P(Np I H) P(H)

t

H

(5)

Suppose that we were to perform inference using
Equation 5 on machinery that could support 100 billion
multiplications per second. By comparison, a Cray Y­
MP/832 with eight processors has a limit of 2.7 billion
floating-point operations per second. Consider the time
it would take to compute only the terms P(H) in the
summation of the denominator of Equation 5. Each
P(H) term requires 534 multiplications. Thus, we would
need

more

than

25 3 4

x

5 34

...

3

x

101

63

multiplications to compute the denominator of Equation
5. On our hypothetical machinery, it would take more

than 101 44 years to complete the computation.
Clearly, the brute-force application of Bayes' theorem to
inference on the QMR-DT belief network is impractical.
Moreover, the problem of probabilistic inference on
two-level binary-valued belief networks such as QMR­
DT is known to be NP-hard [Cooper, 1990].
Accordingly, we have sought to develop special-case
algorithms [Heckerman, 1989] and approximation
algorithms [Henrion, 1988] to perform more efficient
inference on the QMR-DT belief network.

3.2 Approximation Algorithms
Because the general problem of probabilistic inference
on belief networks is NP-hard, we have focused on
app roxi m ati o n algori t hms. The
developing
approximation algorithms we have explored compute
estimates of the posterior marginal probabilities of
diseases that converge in the limit to the true posterior
marginal probabilities, given the QMR-DT model.
We have implemented an approximation algorithm
called likelihood weighting, which places no a priori
restrictions on the connectivity of the belief network.
Our goal is to investigate the performance of likelihood
weighting on the current QMR-DT belief network and
then to use the algorithm on future versions of the
network that contain a richer collection of dependencies
(for example, dependencies among diseases). Likelihood
weighting, a stochastic simulation algorithm, has been

L Z(Hj I Np) U(di,Hj)

......

P(d; I Np)

=

. 1
:._JC______
t

LZ(HjiNp)

j =!

(6)

where Hj is the state of all the diseases as instantiated in
thejth trial,

t is the total number of trials, Z(Hj INp)
is the sample score for the jth trial, and U(di, H) is 1 if
the value of D i in H is dj, and is 0 otherwise. The
sample score is given by
Z(H·I
' Np) =

P(NF I Hj) P(Hj)
P'(Hj)

(7)

where P' is the sampling distribution. In the simplest
case, we instantiate the disease nodes based on their
prior probabilities; that is, P' = P.
Alternatively, we can focus the likelihood-weighting
simulation on certain instantiations of the network.
Using this technique, called importance sampling, the
algorithm instantiates the diseases not by their prior
probabilities P (D ;) , but rather by any sampling
distribution P'(Di) [Shachter & Peot, 1989]. The only

restriction on P' is that P'(dj) > 0 whenever P(dj) > 0.

The simulation's estimates of the posterior distribution
will converge in the limit of infinity to the true
posterior distribution as long as P' follows this
restriction [Rubinstein, 1981]. The estimates will
converge most quickly when P'(Di) is equal to the true
posterior distribution P(D i IN p) for each D j, where 1 �

i �

n. Of course, if we knew the true posterior
distribution, then we would not have to perform the
simulation. We can attempt to approximate the true
posterior distribution using any method of our
choosing-a heuristic method, for example-to
improve the convergence of the simulation. We can
also update P' based on the simulation's current
estimates of the posterior distribution. This technique is
called self-importance sampling [Shachter & Peot,

1989].

501

The self-importance updating function that we use is

P'new(d;) =

P'o(d); +

,...

g (t) Pcum:nt(diI F)
g (t) +1

(8)

where g(t) is a linear function of the number of trials
and P'o is the original sampling distribution. We use
P' o

in the updating function so that very early in

simulation the update will not converge to extreme
probabilities (that is, close to 0 or 1). We use the set of
likely diseases generated by a heuristic algorithm to set
P'o . This heuristic algorithm, which we call the

iterative tabular Bayes' algorithm (ITB), applies
successively the tabular Bayes' calculation to various
subsets of the observed findings Np; see [Shwe,
Middleton, et al., 1990] for a detailed description of this
algorithm. We refer to the set of diseases generated by
the ITB algorithm as the heuristic-importance set,
which contains approximately 25 diseases.
We set the original importance distribution such that
the expected number of diseases instantiated from the
heuristic-importance set is 1. That is, for all Din the
heuristic-importance set, P'o(Di) = 1/N, where N is the
cardinality of the heuristic importance set. We then set
P'o(D)i for all Di not in the heuristic-importance set to

the greater of w-3 or the prior probability on Di. We

use the threshold of w-3 so that we can expect each

disease to be instantiated during simulation a number
of times before the sampling distribution is updated
based on the simulation's probability estimates. For
e�a�pl�, suppose that we update the sampling
dtstnbuuon after the first 10,000 trials of simulation.
Then, we would expect each disease not in the heuristic­

importance Set to be instantiated 10,000 X I 0-3 = 10
times.
In addition to using importance sampling and self­
importance sampling to improve the convergence
properties of the simulation, we use Markov blanket
scoring [Shachter & Peot, 1989]. To apply the Markov
blanket scoring modification to the QMR-DT belief
network, we add a fraction of the sample score to slots
for both the present and the absent states of each
disease. For a specific disease, this fraction of the
sample score is proportional to the probability of each
state of the disease given the state of the rest of the
network, or-as Pearl demonstrates in [Pearl, 1987]­
the state of the disease node's Markov blanket. To
i�corporate Markov blanket scoring into Equation 6, we
stmply redefine the function U such that
U(d;,

P(d; 1 WD;) oc P(d;)

H;) = P(d; I WD;),

(9)

where wD i is the state of all the variables in the
network except D i. We can compute P (di I WD i)
efficiently using the Markov blanket of d:i

n

FjE {S(D;)f"'INF}

I

p lfi 1 WD;. d;)
,

(10)

whereS(Di) are the children of Di.
Note that the complexity of calculating the sample
score of Equation 7 is O(INpln). If, during the
computation of the sample score, we store the values of
I H), we can compute P(di I WDi) in O(IS (Di) l)
P

ifj

additional time. In the worst case, where each Di is
connected to each

Fj e

Np, we require O(INpln)

additional time to compute the Markov blanket
probabilities for all the diseases in the network.
In summary, our simulation algorithm incorporates
an importance distribution from a heuristic algorithm,
self-importance sampling, and Markov Blanket scoring.
We shall refer to this algorithm asS. In Section 4, we
discuss among other experiments, a study of the
convergence properties of S.

4. Experimental Design
We examined the convergence properties of the S
algorithm on two test cases. Also, in a sensitivity
analysis of components of S, we examined the effect on
convergence of importance-sampling from a
heuristically derived set of diseases, self-importance
sampling, and Markov blanket scoring. We
implemented and evaluated S and the variations of S in
LightSpeed Pascal, version 2.03, on a Macintosh Ilci.

4.1 Test Cases
We used two diagnostic cases abstracted from published
clinicopathological conference (CPC) exercises. CPCs
represent some of the more difficult diagnostic
challenges to physicians, often containing many
positive manifestations and multiple diseases in the
diagnosis, which is determined by pathological
investigation at autopsy. The two cases we used in this
study appeared in [Cryer & Kissane, 1974] a n d
[Castleman, Scully, e t al., 1972]. W e shall refer to
these cases as CPCl and CPC2, respectively. Both of
these �s were abstracted by the INTERNIST-1 group
for tesung of the INTERNIST-I system. Information
on the test cases appears in Table 2.
W� used CPCl while developing the S algorithm,
but did not use CPC2 prior to the study reported here.
We selected CPC l from the set of CPCs that we had
run previously because of the multiple diseases in the
diagnosis and the large number of positive
manifestations. We selected CPC2 randomly from a set
of four test cases, each of which met our criteria of
great�r than or equal to 30 positive findings, 10
.
negabve findmgs, and three diseases in the diagnosis.
We used these criteria to select one of the more difficult
CPCs .

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

502

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

Table 2 Test cases
Case
CPCl
CPC2

Number of diseases
in diagnosis
5
3

INp+l

51
50

INp-I

2
24

4.2 Reference Distributions

Because we do not know of any practical algorithm
that can compute the exact posterior marginal
probabilities of disease using the QMR-DT belief
network, we use S to create a reference distribution for
CPC1 and CPC2. We henceforth refer to the 64-hour
reference runs of S on a Macintosh Ilci as the REF
algorithm. The output from REF represents our best
estimates of the true posterior marginal probabilities of
disease, given the QMR-DT belief network.
Although we are not certain that the estimates
provided by the REF algorithm are reasonably close to
the posterior probabilities implied by the QMR-DT
model, we observed during our development of S that
successive, independent 64-hour runs of S produced
similar probability distributions. In Section 4.3, we
describe the metric we used for comparing probability
distributions.

the greater of w-3 or the prior probability on Di·
Finally, we refer to the S algorithm with no Markov
blanket scoring as S/NMBS.
5. Results and Analysis

We ran REF, S, S/NMBS, S/NITB, and S/NSI on test
cases CPC1 and CPC2. Recall that we ran the REF
algorithms for 64 hours. We began self-importance
sampling after t = 20,000 trials, updating P' every
10,000 trials thereafter. Also, we set g(t) = t/20,000
(Equation 8) in the REF algorithms. We ran the S,
S/NMBS, S/NITB, and S/NSI algorithms for 16 hours
on each of the two CPCs. For the S, S/NMBS, and
S/NITB algorithms, we began self-importance sampling
after t = 10,000 trials, updated P' every 5,000 trials
thereafter, and set g(t) = t/10,000. The ranks and
posterior marginal probabilities that REF assigned to
the diagnoses of CPC1 and CPC2 appear in Table 3.4
The total number of trials completed in each of the
runs is listed in Table 4. Note that the Markov blanket
scoring modification to the simulation algorithm
approximately doubles the time per trial, as our analysis
in Section 3.2 suggests.
Table 3 Ranks and posterior marginal probabilities that
REF assigned to the diagnoses of two CPC cases

4.3 Convergence Metric

To compare probability distributions over the set of
diseases in the QMR-DT KB, we used a correlation
coefficient. Let A(i) be the disease number for the
disease that is ranked as the ith most probable disease
by algorithm A. Thus, for example, DA(l) is the most
probable disease according to algorithm A. Let
Px(DA(i) I Np) be the probability that algorithm X
assigns to disease DA(l) given the finding set Np. We
define the correlation r(A, B) as the correlation
coefficient over the pairs (PA(DA(i) I Np) , PB(DA(i) I
N F)) for 1 � i � 20. In general, r( A, B ) is not
symmetric. In the analysis of the convergence properties
of S and variations of S, we assign A to be REF and B
to be S or one of the modified versions of S that we
describe in Section 4.4.
4.4

Sensitivity

Analysis

We examined the sensitivity of the simulation
algorithm to its component heuristics by comparing the
probabilistic output of three modified versions of S to
the probabilistic output of REF. We refer to the S
algorithm with no self-importance sampling as SINS/.
Similarly we refer to the S algorithm with no ITB
initialization heuristic as S/NITB. Like S, S/NSI
obtains its initial importance distribution P'o from the
heuristic-importance set of ITB. (For all D i in the
heuristic-importance set, P'o(D;) = 1/N , where N is the
cardinality of the heuristic- importance set) By contrast,
S/NITB does not use the heuristic-importance set to
generate P'o. Rather, S/NITB sets P'o(D;) for all D; to

Disease
CPC1
acute thrombophlebitis of the
lower extremities
mitral stenosis
pulmonary infarction
hepatic congestion with
centrolobular necrosis
secondary pulmonary
hypertensiona
CPC2
hemiplegia
atheromatous embolism
cerebral embolism

Rank

P(Di lNp)

2

0.99

4
19
61

0.80
0.14
0.01

211

0.00066

1
2
3

0.99
0.99
0.95

aREF assigned a rank of 1 and a P(D; I N/J
primary pulmonary hypertension.

=

0.99 to

4 The

REF algorithm estimated an extremely low posterior
marg i n a l proba b i l i t y
for s e c ondary pulmonary
hypertension, whereas the estimate for P(PRIMARY
PULMONARY HYPERTENSION INp) was 0.99. We believe that

this discrepancy was due largely to the absence of disease­
to-disease dependencies in the current QMR-DT model­
particularly the absence of an arc between MITRAL STENOSIS
and SECONDARY PULMONARY HYPERTENSION. We note that
such a link exists in the QMR knowledge base.

503

Table 4 Number of trials completed by simulation
algorithms

Algorithm

Time
(minutes)

Number
of trials

Trials/
minute

3,840
960
960
960
960

450,731
103,327
219,010
113,501
108,650

117
108
228
118
113

3,840
960
960
960
960

283,872
71,141
173,713
75,352
80,059

74
74
181
78
83

CPC1
REF
s
S/NMBS
S/NITB
SINS I
CPC2
REF
s
S/NMBS
S/NITB
SINS I

did those of S (which did use the ITB heuristic as a
starting point). Moreover, the fact that probabilities of
S/NITB correlated well with those of the reference
distribution increases our belief that the reference
distribution is similar to the true posterior probabilities
given the QMR-DT model. Recall that S/NITB did not
use the heuristic-importance set to set the original
sampling distribution P'o, but rather used the prior
probabilities of diseases to set P'0· Despite the
difference in the initial sampling distributions of
S/NITB and REF, the estimates of the posterior
distribution from S/NITB converged to those of REF,
demonstrating that the posterior estimates of REF are
not overly sensitive to the initial sampling distribution.
1.0

�
<:;
.::-

Table 5 Correlation coefficients r(REF ,B) after 960
minutes of simulation
TestCase
Algorithm
s
S/NMBS
S/NITB
SINS I

CPC1 CPC2
0.95
0.83
0.91
0.14

0.97
0.82
0.96
0.54

Table 6 Statistics on output from the ITB algorithm
Test
ca<;e
CPC1
CPC2

I H IS 1a
30

I HIS n {D REF(l).... .DREFC20)} I
12

20

8

ams: heuristic-importance set from ITB

.

The correlation coefficients r (REF, B) where
algorithm B is S, S/NMBS, S/NSI, or S/NITB, appear
in Table 5. We see that, in both CPC1 andCPC2, the
probabilities of the S algorithm correlated most closely
with those of the reference distribution. The scatterplot
of the 20 pairs of probabilities used to calculate
r(REF,S) for CPCI appears in Figure 2. Figure 3
shows a similar scatterplot forCPC2.
In addition to recording the correlation coefficients
after the 960 minutes of simulation time, we recorded
r(REF, B) every 5000 trials. Graphs of these correlation
coefficients as a function of time appear in Figures 4
and 5. We see that, in both CPCl and CPC2, the S
algorithm is not particularly sensitive to the ITB
heuristic, since the probabilities of S/NITB converged
to those of the reference distribution nearly as well as

I

ib
1>:
s
til

Q,

"':'

v
/
0.8
I·
/
/
0.6
1:1
.£' 1:11:1
Va
0.4
{v 1:1
1:1
v
0.2
L7:"
0.0 /Ill
0.0

0.4
0.6
0.8
1 .0
Pm: (Dm:<•) I NF)
Figure 2
A scatterplot of the probabilities
corresponding to r(REF,S) forCPCl.
0.2

1.0
0.8
.... ::1

/
'/ 1:1
i/
trll v
0.2
:�:.�
?
P.
" 1:1..
0.0 v

/

7
1.:.1
/
/!!liD
m

I
I
I
I
I
I
I
I
I
I
I
I
I
I

1.0

I

Pm: (Dm:c•) I Np)
Figure 3 A scatterplot of the probabilities corresponding
to r(REF,S) for CPC2.

I

0.0

0.2

0. 4

0.6

0.8

I
I

504

I
I
I
I
I

We might expect the posterior distribution of SJNITB
to be slower than that of S to converge to the reference
distribution, because S/NITB initially did not bias its
sampling distribution toward likely disease candidates.
From the convergence on CPC1 depicted in Figure 4,
we see that the convergence as a function of time of
S/NITB is generally slower than that of S. By contrast,
the convergence on CPC2 depicted in Figure 5 reveals
that, although the distribution of S was initially better
correlated with REF distribution than was the
distribution of S/NITB, after about 250 minutes of

I
I
I
I
I
I
I
I
I
I
I
I

.

1.0

I
I

simulation time, the SINITB distribution actually
converges to REF more quickly than does that of S. We
would expect S/NITB to converge more quickly than S
if the ITB heuristic were poor; however, ITB performed
well. ITB suggested 30 disease candidates for CPC1, 1 2
of which were ranked in the top 2 0 diseases of the REF
distribution for CPCl. (See Table 6 .) On CPC2, ITB
suggested 20 disease candidates, eight of which were
ranked in the top 20 diseases of REF

0.8

0.6
=�

+
.....

�

�
""

.....
.....

0.4

s
S/NMBS
S/NITB
SINS I

0.2

0.0 4-----�----�--��---r0

200

400

Time (minutes)

600

800

1000

Figure 4 Correlation coefficients r(REF, B) of simulation algorithms as a function of time for CPC1. A correlation
coefficient of 1 denotes that the probabilities assigned by REF to the top 20 diseases in the differential were
perfectly correlated with the probabilities assigned by algorithm B to those diseases.
Also from Figures 4 and 5, we see that, without
Markov blanket scoring, the convergence of the
simulation algorithm degrades noticeably. Although the
Markov blanket modification more than doubles the
computational time per trial, the improvement in
convergence to the simulation outweighs the added
computational cost In both CPC1 and CPC2, after 960
minutes, the distribution of S/NMBS did not converge
to the reference distribution as well as the distribution
of S did. For the REF run on CPC1 and the REF run
on CPC2, we recorded the distribution of the sample
scores and joint probabilities P(Np, Hi) generated during
the course of the simulation. In Figure 6 appears a
distribution of the log of P(Np, H;) for the hypotheses

H; generated by REF, where Np are from CPCl. Note
that the hypotheses in the distribution are not unique.
The distribution in Figure 6 is similar to that in Figure
7, which depicts the instantiations of S on CPC 1. In
both Figures 6 and 7, the peak at about P(Np, H;) =

10-130 represents the null hypotheses (that is, the
absence of disease) that were instantiated by the
simulation. An important feature of the distributions in
Figures 6 and 7 is the absence of outliers on the right­
hand side of the curve. (The largest values P(Np, H;) for
hypotheses instantiated by REF were approximately I0-

44.) An enlargement of the upper tail of the distribution

pictured in Figure 7 appears in Figure 8. The presence
of outliers typically indicates that the simulation has
not yet sampled a sufficient number of hypotheses.
Since outliers with high joint probabilities greatly
increase the variance in the distribution of the joint
probabilities of the instantiations of the network, the
outliers also increase the variance in successive
estimates of the posterior marginal probabilities of

disease.

I

5 05

I

1.0

I

0.8

I

=0.6

I

�

... 0.4

0.2

0.0

I

-a- s
... S/NMBS
+ S/NITB
... S/NSI

I

+-----�---.--�--.---��---r--r--�
0

200

400

800

600

10 0 0

Time (minutes)

Figure 5 Correlation coefficients r(REF, B)of simulation algorithms as a function of time for CPC2.
10000

contrast, the distribution of S on CPCl (Figures 7 and
8) did not contain such outliers, and the largest values
P(Np, H;) of hypotheses instantiated were about 10-44.
The largest values P(Np, H;)for hypotheses instantiated
by REF were about 10-44. Thus, the S/NSI simulation
failed toinstantiate many of the disease hypotheses with
the largest P(N F, H ;) values, indicating that either not
enough trials were performed or the importance
distribution was poor.

8000

{';'
c
II)

�

�

6000

5000

4000

..
0 -+-....
-150
-130

-110

t'
-90

70

•

Log (P (NF, H,))

-50

Figure 6 A distribution of the log of the joint
probabilities P (N F, H ;) of the hypotheses Hi
instantiated during the REF simulation on the findings
Np ofCPCl.
For example,the distribution of P{Np, H;) generated
by S/NSI on CPCl contains a number of outliers with
large P(Np, Hi) values. This distribution appears in
Figure 9,and an enlargement of the upper-tail end of the
distribution appears in Figure 10. Note the two outlier
instantiations with P(Np, H;) values of about w-57. By

!

I
I
I
I

4000

2000

I

I

3000

I

12000
1000
0

4--..,.....J.·=

-150

-130

-110

-90

Log (P (NF, H,))

-70

-50

Figure 7 A distribution of the log of the joint
probabilities P (N F , H ;) of the hypotheses Hi
instantiatedduring the S simulation on CPCl.

I
I
I

506

I
500

I

400

I
I
I

>-

u
c
Cl)
:I

i 200
...
100
0

I
I
I
I
I
I
I
I
I

-47

-45

-46

Log (P (NF, H1))

-44

Figure 8 An enlargement of the right-hand side of the
distribution that appears in Figure 7.
10000

I
I

300

LL

I
I

S/NSI on CPCl retained the general shape of the
distribution in Figure 9 over the entire period of
simulation, since the importance distribution of S/NITB
did not change as the simulation progressed.
Although we limit our discussion of the distributions
of P(Np, H;) to CPCl, we observed similar results on
the corresponding distributions fromCPC2.

8000

g

6000

l

4000

>-

Cl)
:I

-67 -66 -65 -64 -63 -62 -61 -60 -59 -58 -57

Log (P (NF, H1))

f ;::�

Figure 10 An enlargement of the right-hand side of the
WMbution iliM -= F
cy

2000
0

-+-"T""""

-150

-130

-110

-90

-70

Log (P (NF, H1))

-50

Figure 9 A distribution of the log of the joint
probabilities P (N F, H ; ) of the hypotheses Hi
instantiated during the S/NSI simulation on the fmdings
Np ofCPCl.
We also recorded the time during simulation at which
each hypothesis was instantiated. In Figure 11 appears
the distribution as a function of time of the log of
P (N p, H;) for the hypotheses Hi generated by REF,
where N F are from CPCl. Early in the REF
simulation, the variance of the P(NF, H ; ) values was
large, and the mean of the distribution was relatively
small. However, soon after self-importance sampling
begins at trial 20,000 (after approximately 1fl.O of the
total simulation time had elapsed), the variance of the
distribution decreased, and the mean increased
substantially. By contrast, the distribution (as a
function of time) of the P(Np, H;) values produced by

Figure 11 A plot as a function of time of the
distribution of Log (P(Np, H;)) of the hypotheses H;
instantiated during the REF si mulation onCPCl.

507

We recorded the average number of diseases present in
a disease hypothesis H; as a function of the log of the
joint probability P(Np, H;). Figures 12 and 13 display
this distribution for the REF runs on CPC1 and CPC2,
respectively. Note that the distributions in Figures 12
and 13 represent the hypotheses instantiated by the
simulation, not the actual distribution of disease
hypotheses.
Observe that, in both Figures 12 and 13, the
instantiations of the QMR-DT belief network with the
largest joint probabilities contained approximately 10
diseases. This value is quite large, given thatCPC1 had
five diseases in its pathological diagnosis andCPC2 had
three. We are currently investigating the reasons that the
QMR-DT model exhibits this behavior. In particular,
the noisy-OR gate may be responsible for this behavior.

sampling, self-importance sampling, and Markov
blanket scoring were able to produce similar estimates
of posterior marginal probabilities of disease, when the
simulation was presented with two difficult diagnostic
cases. This reproducibility supports our belief that the
estimates of the reference distributions have converged
appreciably to the posterior probabilities implied by the
QMR-DT model. The heuristic ITB algorithm provides
the simulation with a set of likely diseases given the
findings observed; however, the convergence of the
simulation algorithm does not depend on the heuristic.
Rather, the simulation appears to be more sensitive to
the self-importance sampling and Markov blanket
scoring.

14

10

12

+
:z:

10

r::::

I

==

+

;;.a
r::::

m

==

6

0

6

- 150

������

- 150

- 130

- 1 10

Log

-90

-70

(P (NF, H1))

-50

- 1 30

- 1 10

Log

-90

-70

(P (NF, H1))

-50

Figure 13 A plot of the mean size of n+ (the number
of diseases present in disease hypothesis H) as a
function of the log of the joint probability P(Np, H;)
for hypotheses instantiated by REF on CPC2.

Figure 12 A plot of the mean cardinality of n+ (the
number of diseases present in disease hypothesis H) as a
function of the log of the joint probability P(Np, H;)
for hypotheses instantiated by REF on CPCl.
In this section, we discussed and depicted various
distributions as a function of the joint probabilities of
the QMR-DT belief network during simulation. For the
sake of brevity, we do not show the corresponding
distributions as a function of the sample score. We
found a corresponding similarity in the distributions of
sample scores generated by REF and S. Also, the
distribution of the sample scores generated by S/NSI
contained a number of outliers of high probability. On
both CPC1 and CPC2, we observed that the largest
sample scores were generated by hypotheses with
approximately 10 diseases .
In summary, we observed that successive runs of
likelihood-weighting simulation with importance

I

I

4

o;-�����

2

I

I

8

2

4

I

I

12

14

I

I

16

16

I

The simulation also provides us with insight on the
behavior of the underlying model. For example, we
found that the instantiations of the network with the
largest joint probability had approximately 10 diseases.
We plan to investigate further the behavior of the model
using the results of simulation.

6. Discussion
On two large diagnostic cases with multiple diseases,
we observed, after 960 minutes of simulation, that the
posterior estimates of the S algorithm correlated well
with the estimates of the reference distribution. In a
previous study of the behavior of S/NMBS on smaller
test cases with single-disease diagnoses, we observed
appreciable convergence within 2 hours of simulation
time on a Macintosh Ilci [Shwe, Middleton, et al.,
1990]. Since the simulation is readily amenable to
parallelization, we do not believe that the current

I
I
I
I
I

I
I
I
I
I

508

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

running time of our serial implementation of the S
algorithm on a personal computer will be a long-term
limitation for practical applications.
Both the Markov blanket scoring component and the
self-importance sampling heuristic of the S algorithm
are particularly suited to the two-level connectivity of
the current QMR-DT belief network. Because we have,
for the time being, assumed diseases to be marginally
independent, the Markov blankets of the disease nodes
are relatively small, to the extent that the time needed to
compute the Markov blanket probability of all the
disease nodes in the network is comparable to the time
required to compute the sample score of an instantiation
of the network.
Because we assumed marginal independence, we
could perform self-importance updating by simply
sampling the diseases based on the the simulation's
current estimates of their posterior marginal
probabilities. If we were to introduce disease-to-disease
dependencies to the QMR-DT model, the self­
importance sampling would require that we sample a
disease node based on the state of that node's parents in
the current instantiation of the network. In general, this
requirement necessitates additional storage and
computation to provide estimates of the conditional
probability of a node.
The convergence results that we present in this paper
demonstrate that likelihood-weighting simulation is a
viable method of inference for the two-level QMR-DT
belief network. Moreover, the results provide promise
that likelihood-weighting simulation will be a useful
inference tool not only on future versions of the QMR­
DT belief network, which will have richer sets
dependencies, but also on other large multiply connected
belief networks, for which exact inference algorithms
are not practical.
Acknowledgments

We are grateful to other members of the QMR-DT
project group-David Heckerman, Max Henrion, Eric
Horvitz, Harold Lehmann, and Blackford Middleton­
who were instrumental in developing the QMR-DT
model. We thank Ross Shachter for providing insight
on the likelihood-weighting simulation and for
encouraging us to implement self-importance sampling
and Markov blanket scoring. Randy Miller graciously
provided us with the INTERNIST-I knowledge base.
Lyn Dupre provided valuable comments on drafts of this
paper.
This work was supported by the National Science
Foundation under Grant ffii-8703710 and the U. S.
Army Research Office under Grant P-25514-EL.
Computing facilities were provided by the SUMEX­
AIM Resource under the National Library of Medicine
Grant LM05208.

