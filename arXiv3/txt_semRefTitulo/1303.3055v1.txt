
We study the problem of learning Markov decision processes with finite state and action
spaces when the transition probability distributions and loss functions are chosen adversarially and are allowed to change with time. We introduce an algorithm whose regret with
respect to any policy in a comparison class grows as the square root of the number of rounds
of the game, provided the transition probabilities satisfy a uniform mixing condition. Our
approach is efficient as long as the comparison class is polynomial and we can compute
expectations over sample paths for each policy. Designing an efficient algorithm with small
regret for the general case remains an open problem.

1

Notation

Let X be a finite state space and A be a finite action space. Let ∆S be the space of probability
distributions over set S. Define a policy π as a mapping from the state space to ∆A , π : X → ∆A .
We use π(a|x) to denote the probability of choosing action a in state x under policy π. A random
action under policy π is denoted by π(x). A transition probability kernel (or transition model) m
is a mapping from the direct product of the state and action spaces to ∆X : m : X × A → ∆X .
Let P (π, m) be the transition probability matrix of policy π under transition model m. A loss
function is a bounded real-valued
function over state and action spaces, ℓ : X × A → R. For a
P
vector v, define kvk1 = i |vi |. For a real-valued function f defined over X × A, define kf k∞,1 =
P
maxx∈X a∈A |f (x, a)|. The inner product between two vectors v and w is denoted by hv, wi.

2

Introduction

Consider the following game between a learner and an adversary: at round t, the learner chooses a
policy πt from a policy class Π. In response, the adversary chooses a transition model mt from a set
of models M and a loss function ℓt . The learner takes action at ∼ πt (.|xt ), moves to state xt+1 ∼
mt (.|xt , at ) and suffers loss ℓt (xt , at ). To simplify the discussion, we assume that the adversary is
oblivious, i.e. its choices do not depend on the previous choices of the learner. We assume that
ℓt ∈ [0, 1]. In this paper, we study the full-information version of the game, where the learner
observes the transition model mt and the loss function ℓt at the end of round t. The game is shown
in Figure 1. The objective of the learner is to suffer low loss over a period of T rounds, while the
performance of the learner is measured using its regret with respect to the total loss he would have
achieved had he followed the stationary policy in the comparison class Π minimizing the total loss.
Even-Dar et al. (2004) prove a hardness result for MDP problems with adversarially chosen
transition models. Their proof, however, seems to have gaps as it assumes that the learner chooses a
deterministic policy before observing the state at each round. Note that an online learning algorithm
only needs to choose an action at the current state and does not need to construct a complete
deterministic policy at each round. Their hardness result applies to deterministic transition models,
while we make a mixing assumption in our analysis. Thus, it is still an open problem whether it is
possible to obtain a computationally efficient algorithm with a sublinear regret.
Yu and Mannor (2009a,b) study the same setting, but obtain only a regret bound that scales
with the amount of variation in the transition models. This regret bound can grow linearly with
time.

Initial state: x0
for t := 1, 2, . . . do
Learner chooses policy πt
Adversary chooses model mt and loss function ℓt
Learner takes action at ∼ πt (.|xt )
Learner suffers loss ℓt (xt , at )
Update state xt+1 ∼ mt (.|xt , at )
Learner observes mt and ℓt
end for
Figure 1: Online Markov Decision Processes

Even-Dar et al. (2009) prove regret bounds for MDP problems with a fixed and known transition
model and adversarially chosen loss functions. In this paper, we prove regret bounds for MDP
problems with adversarially chosen transition models and loss functions. We are not aware of any
earlier regret bound for this setting. Our approach is efficient as long as the comparison class is
polynomial and we can compute expectations over sample paths for each policy.
MDPs with changing transition kernels are good models for a wide range of problems, including
dialogue systems, clinical trials, portfolio optimization, two player games such as poker, etc.

3

Online MDP Problems

Let A be an online learning algorithm that generates a policy πt at round t. Let xA
t be the state
at round t if we have followed the policies generated by algorithm A. Similarly, xπt denotes the state
if we have chosen the same policy π up to time t. Let ℓ(x, π) = ℓ(x, π(x)). The regret of algorithm
A up to round T with respect to any policy π ∈ Π is defined by
RT (A, π) =

T
X
t=1

πt (xA
t ).

ℓt (xA
t , at ) −

T
X

ℓt (xπt , π) ,

t=1

where at =
Note that the regret with respect to π is defined in terms of the sequence of
states xπt that would have been visited under policy π. Our objective is to design an algorithm that
achieves low regret with respect to any policy π.
In the absence of state variables, the problem reduces to a full information online learning
problem (Cesa-Bianchi and Lugosi, 2006). The difficulty with MDP problems is that, unlike the full
information online learning problems, the choice of policy at each round changes the future states
and losses. The main idea behind the design and the analysis of our algorithm is the following regret
decomposition:
RT (A, π) =

T
X
t=1

Let

ℓt (xA
t , at ) −

T
X

BT (A) =

T
X

t=1

t=1

CT (A, π) =

ℓt (xπt t , πt ) +

T
X
t=1

T
X
t=1

ℓt (xA
t , at ) −

T
X

ℓt (xπt t , πt ) −

ℓt (xπt t , πt ) −

T
X

ℓt (xπt , π) .

(1)

t=1

ℓt (xπt t , πt ) ,

t=1

T
X

ℓt (xπt , π) .

t=1

Notice that the choice of policies has no influence over future losses in CT (A, π). Thus, CT (A, π)
can be bounded by a specific reduction to full information online learning algorithms (to be specified
later). Also, notice that the competitor policy π does not appear in BT (A). In fact, BT (A) depends
only on the algorithm A. We will show that if algorithm A and the class of models satisfy the
following two “smoothness” assumptions, then BT (A) can be bounded by a sublinear term.
Assumption A1 Rarely Changing Policies Let αt be the probability that algorithm A changes
its policy at round t. There exists a constant D such
√ that for any 1 ≤ t ≤ T , any sequence of models
m1 , . . . , mt and loss functions ℓ1 , . . . , ℓt , αt ≤ D/ t.
2

N : number of experts, T : number of rounds.
Initialize wi,0 = 1 for each expert i.
W0 = N .
for t := 1, 2, . . . do
For any i, pi,t = wi,t−1 /Wt−1 .
Draw It such that for any i, P (It = i) = pi,t .
Choose the action suggested by expert It .
The adversary chooses loss function ct .
The learner suffers loss ct (It ).
For expert i, wi,t = wi,t−1 e−ηct (i) .
P
Wt = N
i=1 wi,t .
end for
Figure 2: The EWA Algorithm
N : number
p of experts, T : number of rounds.
η = min{ log N/T , 1/2}.
Initialize wi,0 = 1 for each expert i.
W0 = N .
for t := 1, 2, . . . do
For any i, pi,t = wi,t−1 /Wt−1 .
With probability βt = wIt−1 ,t−1 /wIt−1 ,t−2 choose the previously selected
expert, It = It−1 and with probability 1 − βt , choose It based on the
distribution qt = (p1,t , . . . , pN,t ).
Learner takes the action suggested by expert It .
The adversary chooses loss function ct .
The learner suffers loss ct (It ).
For all experts i, wi,t = wi,t−1 (1 − η)ct (i) .
PN
Wt = i=1 wi,t .
end for
Figure 3: The Shrinking Dartboard Algorithm

Assumption A2 Uniform Mixing There exists a constant τ > 0 such that for all distributions
d and d′ over the state space, any deterministic policy π, and any model m ∈ M ,
kdP (π, m) − d′ P (π, m)k1 ≤ e−1/τ kd − d′ k1 .
As discussed by Neu et al. (2010), if Assumption A2 holds for deterministic policies, then it holds
for all policies.
3.1

Full Information Algorithms
We would like to have a full information online learning algorithm that rarely changes its policy.
The first candidate that we consider is the well-known Exponentially Weighted Average (EWA)
algorithm (Vovk, 1990, Littlestone and Warmuth, 1994) shown in Figure 2. In our MDP problem,
the EWA algorithm chooses a policy π ∈ Π according to distribution
!
t−1
X
E [ℓs (xπs , π)] , λ > 0 ,
(2)
qt (π) ∝ exp −λ
s=1

The policies that this EWA algorithm generates most likely are different in consecutive rounds and
thus, the EWA algorithm might change its policy frequently. However, a variant of EWA, called
Shrinking Dartboard (SD) (Geulen et al., 2010) and shown in Figure 3, satisfies Assumption A1.
Our algorithm, called SD-MDP, is based on the SD algorithm and is shown in Figure 4. Notice
that the algorithm needs to know the number of rounds, T , in advance.
3

T : number
p of rounds.
η = min{ log |Π| /T , 1/2}.
For all policies π ∈ {1, . . . , |Π|}, wπ,0 = 1.
for t := 1, 2, . . . do
For any π, pπ,t = wπ,t−1 /Wt−1 .
With probability βt = wπt−1 ,t−1 /wπt−1 ,t−2 choose the previous policy, πt =
πt−1 , while with probability 1 − βt , choose πt based on the distribution
qt = (p1,t , . . . , p|Π|,t ).
Learner takes the action at ∼ πt (.|xt )
Adversary chooses transition model mt and loss function ℓt .
Learner suffers loss ℓt (xt , at ).
Learner observes mt and ℓt .
Update state: xt+1 ∼ mt (.|xt , at ).
π
For allP
policies π, wπ,t = wπ,t−1 (1 − η)E[ℓt (xt ,π)] .
Wt = π∈Π wπ,t .
end for
Figure 4: SD-MDP: The Shrinking Dartboard Algorithm for Markov Decision Processes

Consider a basic full information problem with N experts. Let RT (SD, i) be the regret of the SD
algorithm with respect to expert i up to time T . We have the following results for the SD algorithm.
Theorem 1. For any expert i ∈ {1, . . . , N },
p
RT (SD, i) ≤ 4 T log N + log N ,

and also for any 1 ≤ t ≤ T ,

P (Switch at time t) ≤

r

log N
.
T

Proof. The proof of the regret bound can be found in (Geulen et al., 2010, Theorem 3). The proof
of the bound on the probability of switch is similar to the proof of Lemma 2 in (Geulen et al., 2010)
and is as follows: As shown in (Geulen et al., 2010, Lemma 2), the probability of switch at time t is
αt =

Wt−1 − Wt
.
Wt−1

Thus, Wt = (1 − αt )Wt−1 . Because the loss function is bounded in [0, 1], we have that
Wt =

N
X
i=1

wi,t =

N
X
i=1

wi,t−1 (1 − η)ct (i) ≥

Thus, 1 − αt ≥ 1 − η, and thus,

αt ≤ η ≤

r

N
X
i=1

wi,t−1 (1 − η) = (1 − η)Wt−1 .

log N
.
T

3.2 Analysis of the SD-MDP Algorithm
The main result of this section is the following regret bound for the SD-MDP algorithm.
Theorem 2. Let the loss functions selected by the adversary be bounded in [0, 1], and the transition
models selected by the adversary satisfy Assumption A2. Then, for any policy π ∈ Π,
p
E [RT (SD-MDP, π)] ≤ (4 + 2τ 2 ) T log |Π| + log |Π| .

In the rest of this section, we write A to denote the SD-MDP algorithm. For the proof we use
the regret decomposition (1):
RT (A, π) = BT (A) + CT (A, π) .
4

3.2.1 Bounding E [CT (A, π)]
Lemma 3. For any policy π ∈ Π,
" T
#
T
X
X
p
πt
π
E [CT (A, π)] = E
ℓt (xt , πt ) −
ℓt (xt , π) ≤ 4 T log |Π| + log |Π| .
t=1

t=1

Proof. Consider the following imaginary game between a learner and an adversary: we have a set
of experts (policies) Π = {π 1 , . . . , π |Π| }. At round t, the adversary chooses a loss vector ct ∈ [0, 1]Π ,
whose ith element determines the loss of expert π i at this round. The learner chooses a distribution
over experts qt (defined by the SD algorithm), from which it draws an expert πt . Next, the learner
observes the loss function ct . From the regret bound for the SD algorithm (Theorem 1), it is
guaranteed that for any expert π,
T
X
t=1

hct , qt i −

T
X
t=1

p
ct (π) ≤ 4 T log |Π| + log |Π| .

Next, we determine how the adversary
h chooses
ithe loss vector. At time t, the adversary chooses a
i
πi
i
loss function ℓt and sets ct (π ) = E ℓt (xt , π ) . Noting that hct , qt i = E [ℓt (xπt t , πt )] and ct (π) =

E [ℓt (xπt , π)] finishes the proof.

3.2.2 Bounding E [BT (A)]
First, we prove the following two lemmas.
Lemma 4. For any state distribution d, any transition model m, and any policies π and π ′ ,
kdP (π, m) − dP (π ′ , m)k1 ≤ kπ − π ′ k∞,1 .
Proof. Proof is easy and can be found in (Even-Dar et al., 2009), Lemma 5.1.
p
Lemma 5. Let αt be the probability of a policy switch at time t. Then, αt ≤ log |Π|/T .

Proof. Proof is identical to the proof of Theorem 1.
Lemma 6. We have that
E [BT (A)] = E

" T
X
t=1

ℓt (xA
t , at ) −

T
X
t=1

#

ℓt (xπt t , πt ) ≤ 2τ 2

p
log |Π|T .

Proof. Let Ft = σ(π1 , . . . , πt ). Notice that the choice of policies are independent of the state
variables. We can write
" T
#
T
X
X
πt
A
E [BT (A)] = E
ℓt (xt , at ) −
ℓt (xt , πt )
t=1

=E
=E

"

"

=E

"

≤E

"

=E

"

≤E

"

T
X

t=1

X

t=1 x∈X

T X
X

t=1 x∈X

T X
X

t=1 x∈X

T
X
t=1

T
X
t=1

T
X
t=1

#



I{xA
− I{xπt t =x} ℓt (x, πt (x))
t =x}

E

h



I{xA
− I{xπt t =x} ℓt (x, πt (x)) FT
t =x}

#
i

i

h
πt
F
ℓt (x, πt (x))E I{xA
−
I
T
{xt =x}
t =x}

kℓt k∞ E

h

I{xA
− I{xπt t =x}
t =x}

kℓt k∞ kut − vt,t k1
#

kut − vt,t k1 ,
5

#



FT

i

1

#

#

(3)

i
h


A
πt
where us = E I{xA
is the
is
the
distribution
of
x
for
s
≤
t
and
v
=
E
I
F
F
s,t
T
T
=x}
s
{x
=x}
s
s
distribution of xπs t for s ≤ t.1 Let Et be the event of a policy switch at time t. From inequality
kπt−k − πt k∞,1 ≤ kπt−k − πt−k+1 k∞,1 + · · · + kπt−1 − πt k∞,1 ≤ 2

t
X

I{Es } ,

s=t−k+1

and Lemma 5, we get that
h

i

E kπt−k − πt k∞,1 ≤ 2

r

log |Π|
k.
T

(4)

Let Ptπ = P (π, mt ). We have that




πt−1
πt
E kut − vt,t k1 = E ut−1 Pt−1
− vt−1,t Pt−1
1


πt−1
πt
πt
πt
= E ut−1 Pt−1
− ut−1 Pt−1
+ ut−1 Pt−1
− vt−1,t Pt−1
1


πt−1
πt
πt
πt
+ ut−1 Pt−1
− vt−1,t Pt−1
≤ E ut−1 Pt−1
− ut−1 Pt−1
1
1
h
i
≤ E kπt−1 − πt k∞,1 + e−1/τ kut−1 − vt−1,t k1
h
πt−2
πt
≤ E kπt−1 − πt k∞,1 + e−1/τ ( ut−2 Pt−2
− ut−2 Pt−2
1
i
πt
πt
)
+ ut−2 Pt−2
− vt−2,t Pt−2
1
h
i
≤ E kπt−1 − πt k∞,1 + e−1/τ kπt−2 − πt k∞,1 + e−2/τ kut−2 − vt−2,t k1
≤ ...
≤
≤

t
X

k=0
t
X

k=0

≤2

r

h
i
e−k/τ E kπt−k − πt k∞,1 + e−t/τ ku0 − v0,t k1
2e

−k/τ

r

log |Π|
k+0
T

By (4)

log |Π| 2
τ ,
T

(5)

where we have used the fact that ku0 − v0,t k1 = 0, because the initial distributions are identical. By
(5) and (3), we get that
r
T
X
p
log |Π|
2
E [BT (A)] ≤ 2τ
= 2τ 2 log |Π|T .
T
t=1

What makes the analysis possible is the fact that all policies mix no matter what transition
model is played by the adversary.
Proof of Theorem 2. The result is obvious by Lemmas 3 and 6.
The next corollary extends the result of Theorem 2 to continuous policy spaces.
Corollary 7. Let Π be an arbitrary policy space, N (ǫ) be the ǫ-covering number of space (Π, k.k∞,1 ),
and C(ǫ) be an ǫ-cover. Assume that we run the SD-MDP algorithm on C(ǫ). Then, under the same
assumptions as in Theorem 2, for any policy π ∈ Π,
p
E [RT (SD-MDP, π)] ≤ (4 + 2τ 2 ) T log N (ǫ) + log N (ǫ) + τ T ǫ .
1

Notice that FT contains only policies, which are independent of the state variables.

6

hP
i
T
π
Proof. Let LT (π) = E
ℓ
(x
,
π)
be the value of policy π. Let uπ,t (x) = P (xπt = x). First,
t
t
t=1
we prove that the value function is Lipschitz with Lipschitz constant τ T . The argument is similar
to the argument in the proof of Lemma 6. For any π1 and π2 ,
" T
#
T
X
X
π1
π2
|LT (π1 ) − LT (π2 )| = E
ℓt (xt , π1 ) −
ℓt (xt , π2 )
t=1

≤2
≤2

T
X
t=1

T
X
t=1

t=1

kuπ1 ,t − uπ2 ,t k1 kℓt k∞
kuπ1 ,t − uπ2 ,t k1 .

With an argument similar to the one in the proof of Lemma 6, we can show that
kuπ1 ,t − uπ2 ,t k1 ≤ τ kπ1 − π2 k∞,1 .
Thus,
|LT (π1 ) − LT (π2 )| ≤ τ T kπ1 − π2 k∞,1 .

Given this and the fact that for any policy π ∈ Π, there is a policy π ′ ∈ C(ǫ) such that kπ − π ′ k∞,1 ≤
ǫ, we get that
p
E [RT (SD-MDP, π)] ≤ (4 + 2τ 2 ) T log N (ǫ) + log N (ǫ) + τ T ǫ .
In particular if Π is the space of all policies, N (ǫ) ≤ (|A|/ǫ)|A||X |, so regret is no more than
r
|A|
|A|
2
+ |A||X | log
+ τT ǫ .
E [RT (SD-MDP, π)] ≤ (4 + 2τ ) T |A||X | log
ǫ
ǫ
p
By the choice of ǫ = T1 , we get that E [RT (SD-MDP, π)] = O(τ 2 T |A| |X | log(|A|T )).

