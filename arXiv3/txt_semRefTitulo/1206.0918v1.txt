: - Within the framework proposed in this paper, we address the issue of extending the certain networks to a fuzzy certain
networks in order to cope with a vagueness and limitations of existing models for decision under imprecise and uncertain
knowledge. This paper proposes a framework that combines two disciplines to exploit their own advantages in uncertain and
imprecise knowledge representation problems. The framework proposed is a possibilistic logic based one in which Bayesian nodes
and their properties are represented by local necessity-valued knowledge base. Data in properties are interpreted as set of
valuated formulas. In our contribution possibilistic Bayesian networks have a qualitative part and a quantitative part, represented
by local knowledge bases. The general idea is to study how a fusion of these two formalisms would permit representing compact
way to solve efficiently problems for knowledge representation. We show how to apply possibility and necessity measures to the
problem of knowledge representation with large scale data. On the other hand fuzzification of crisp certainty degrees to fuzzy
variables improves the quality of the network and tends to bring smoothness and robustness in the network performance. The
general aim is to provide a new approach for decision under uncertainty that combines three methodologies: Bayesian networks
certainty distribution and fuzzy logic
.

Key-Words: - Possibilistic logic, Bayesian networks, Certain Bayesian networks, Local knowledge bases

1 Introduction
Bayesian networks have attracted much attention
recently as a possible solution to complex problems
related to decision support under uncertainty. These
networks are systems for uncertain knowledge
representation and have a big number of applications
with efficient algorithms and have strong theoretical
foundations [1],[2],[3],[4],[5] and [11]. They use graphs
capturing causality notion between variables, and
probability theory (statistic data) to express the causality
power.
Although the underlying theory has been around for a
long time, the possibility of building and executing
realistic models has only been made possible because of
recent improvements on algorithms and the availability
of fast electronic computers. On the other hand, one of
the main limits of Bayesian networks is necessity to
provide a large number of numeric data; a constraint
often difficult to satisfy when the number of random
variables grows up. The goal of this paper is to develop
a qualitative framework where the uncertainty is
represented in possibility theory; an ordinal theory for
uncertainty developed since more than ten years [6], [7],
and [8]. Our framework propose to define a qualitative
notion of independence (alternative to the probability
theory), to propose techniques of decomposition of

joined possibility distributions, and to develop some
efficient algorithms for the revision of beliefs. Thus, on
the first hand limitations of quantitative structure in
Bayesian networks that use simple random variables
have been noted by many researches. These limitations
have motivated a variety of recent research in
hierarchical and composable Bayesian models.
On the other hand, another limitation of the use of
probabilistic Bayesian networks in expert systems is
difficulty of obtaining realistic probabilities. So to solve
these problems we use a new modified possibilistic
Bayesian method. Our new modified possibilistic
Bayesian networks simultaneously make use of both
possibilistic measures: necessity measure and possibility
measure.
Our work extends and refines these proposed
frameworks in a number of crucial ways. The language
defined in [12] [13] and in [15] has been modified to
enhance usability and to support a more powerful
system. We are trying in this paper to describe a
language that provides the important capability of
uncertainty modeling. We have also combined different
element from works cited above to describe our
possibilistic networks based on local necessity-valued
knowledge bases.

225

WSEAS TRANS. on INFORMATION SCIENCE & APPLICATIONS Issue 2, Volume 3, February 2006 ISSN: 1790-0832

In this paper we consider a type of possibilistic network
that is based on the context model interpretation of a
degree of possibility and focused on imprecision [14].
The first section presents an overview of standard
possibilistic networks and their extensions. The
following section describes our contribution with the use
of necessity-valued knowledge bases as quantitative
representation for uncertainty in nodes. And eventually
we will talk about the transformations between average
fuzzyBayesian networks and average knowledge bases.

2

Necessity-possibility measures
possibilistic networks

and

In order to be able to discuss our framework for
possibilistic networks we shall in this section give a few
preliminary definitions and notational conventions. At
the same time, we present a brief outline of few
important notations and ideas in possibility theory and
possibilistic networks relevant to the subject of this
paper.
2.1 Possibilistic logic
Let L be a finite propositional language. p; q; r; . . .
denote propositional formulae.
and ⊥, respectively, denote tautologies and
denotes the classical syntactic
contradictions.
inference relation. Ω is the set of classical interpretations
ω of L, and [p] is the set of classical models of p (i.e,
interpretations where p is true {ω | ω
p}) [13].
2.1.1

Possibility-necessity
distributions
and
possibility-necessity measures
The basic element of possibility theory is the possibility
distribution ∏ which is a mapping from Ω to the interval
[0 1]. The degree π(ω) represents the compatibility of ω
with the available information (or beliefs) about the real
world. By convention, π(ω)= 0 means that the
interpretation ω is impossible, and π(ω) = 1 means that
nothing prevents ω from being the real world [13].
Given a possibility distribution π, two different ways of
rank ordering formulae of the language are defined from
this possibility distribution. This is obtained using two
mappings grading, respectively, the possibility and the
certainty of a formula p:
• The possibility (or consistency) degree:
∏(p) = max ( π (ω) : ω ∈ [p])

(1)

Which evaluates the extent to which p is consistent with
the available beliefs expressed by p [16]. It satisfies:
∀p, ∀q

∏(p ∨ q) = max (∏ (p), ∏ (q))

(2)

• The necessity (or certainty, entailment) degree
N(P) = 1 - ∏(¬p)
(3)
Which evaluates the extent to which p is entailed by the
available beliefs. We have [17]:
∀p, ∀q N (p∧q) = max (N(p), N (q))
(4)
To note here that in our case, we consider that both
necessity degree and possibility degrees for a given
formulae should be given by an expert. On the other
hand, when a data is required (a possibility degree or
necessity degree) one should deduce it by applying
equation (3).
2.1.2 Fuzzy knowledge base
A fuzzy formula is a tripley (ϕ, α,β) where ϕ is a
classical first-order closed formula and (α,β )∈ [0,1] are
a positive numbers. (ϕ, α,β) expresses that ϕ is possible
at least to the degree α , and certain at least to the degree
β i.e. ∏(ϕ) ≥ α and β N(ϕ) ≥ β, where ∏ and N are
respectively
a possibility and necessity measures
modelling our possibly incomplete state of knowledge.
The right part of a possibilistic formula, i.e. α and β, are
respectively called the possibility and necessity weights
of the formula.
A fuzzy knowledge base ∑ is defined as the set of
weighted
formulae
[18].
More
formally
∑= {(ϕι , αi,β i) , i = 1….m} where ϕι is a propositional
formula αi is the higher bound of possibility and βI is
the lower bound of necessity accorded to this formula
(certainty degree).

3 Fuzzy Bayesian networks
A standard possibilistic network is a decomposition of a
multivariate possibility distribution according to:
π (A1,…..,An) = mini=1..n π (Ai | parents(Ai))

(5)

where parents(Ai) is the set of parents of variable Ai,
which is made as small as possible by exploiting
conditional independencies of the type indicated above
[9] and [10]. Such a network is usually represented as a
directed graph in which there is an edge from each of the
parents to the conditioned variable.

226

WSEAS TRANS. on INFORMATION SCIENCE & APPLICATIONS Issue 2, Volume 3, February 2006 ISSN: 1790-0832

In our work an average fuzzy Bayesian networks is
considered as a graphical representation of uncertain
information. It offers an alternative to probabilistic
causal network when numerical data are not available.

The joint average distribution is obtained then by
applying the chain rule:
A(A1,.,An) = min( π (Ai|U(Ai))*min (N(Ai|U(Ai))

Let V= {A1,A2,..An} be a set of variables (i.e attributes or
proprieties). The set of interpretations is the Cartesian
product of all domains of attributes in V. When each
attribute is binary, domains are denoted by Di={ai,¬ai}.
An average fuzzy graph denoted by ΠGA is an acyclic
graph where nodes represents attributes i.e. a patient
temperature and edges represent causal links between
them. Uncertainty is represented by possibilities
distribution, certainties distribution and conditional
possibilities and necessities for each attribute explaining
the link force between them.
The conditional possibilities and necessities distributions
are associated to the graph as follow:

Where:
A(A1,.,An) is The joint average distribution.
min ( π(Ai | U(Ai)) is the lower bound of the
possibilities degrees associated to (Ai|U(Ai)).
min (N(Ai|U(Ai)) is the lower bound of the
necessities degrees associated to (Ai|U(Ai)
Example: let the prior possibilities-necessities and the
conditional possibilities-necessities be as described in
table 1:
N
A
π
a
1
0.6 0.6
¬a 0.5 0.1 0.05

For each root attribute Ai, we specify prior possibility
distribution Π(ai),Π(¬ai) and the prior normalization)
and the prior necessity distribution N(ai), N(¬ai) with
the constraint that :
N(ai) = 1

N
π
π
a
B|A a
¬a
b
1 0.5 0.75
¬b 0.5 0.25 0.3

N(¬ai) =0
(6)

N(¬ai)=1

N(ai|uj) = 1

N(¬ai)|uj) =0
(7)

N(¬ai)|uj)=1

π
C|A a
C
1
¬c 0.6

N(ai) =0

- For other attributes Aj, we specify the conditional
Π(aj|uj), Π(¬aj|uj) with
possibilities distribution
max(Π(ai|uj), Π(¬ai| uj)) =1 where uj is an instance of
aj parents and the conditional necessity distribution
N(ai), N(¬ai) with the constraint that :

N(ai)|uj) =0

Example: the next figure gives an example of
possibilistic Bayesian networks with four nodes and
their conditional possibilities.
A
B
π (B|A)
N (B|A)

π (A)
N(A)
C

D

π (C|A)
N(C|A)

π (D|BC)
N (D|BC)

Fig. 1: example of a fuzzy Bayesian network

(8)

N
a
0.3
0.1

π
¬a
0.7
0.4

N
¬a
0.2
0
N
¬a
0.2
0.1

N
N
N
π
π
π
D|BC Bc bc b¬c b¬c else Else
d
1 0.2 0.5 0.1
1
0.3
¬d 0.5 0.4 0.3 0.1 0.7 0.2
Table 1: possibilities-necessities distribution
By the use of the chain rule defined by equation (8) we
obtain the average distribution associated with the
average fuzzy Bayesian network cited above as
described in table.
A
a
a
a
a
a
a
a

B
b
b
b
b
¬b
¬b
¬b

C
c
c
¬c
¬c
c
c
¬c

D minΠ minN
d
1
0.2
0.5
0.3
¬d
d
0.5
0.1
0.3
¬d 0.3
d
0.5
0.2
0.1
¬d 0.5
d
0.5
0.1

A
0.2
0.15
0.05
0.09
0.1
0.05
0.05

227

WSEAS TRANS. on INFORMATION SCIENCE & APPLICATIONS Issue 2, Volume 3, February 2006 ISSN: 1790-0832

a
¬a
¬a
¬a
¬a
¬a
¬a
¬a
¬a

¬b
b
b
b
b
¬b
¬b
¬b
¬b

¬c
c
c
¬c
¬c
c
c
¬c
¬c

¬d
d
¬d
d
¬d
d
¬d
d
¬d

0.5
0.5
0.5
0.4
0.3
0.3
0.3
0.3
0.3

0.1
0.1
0.1
0.1
0.1
0
0
0
0

0.05
0.05
0.05
0.04
0.03
0
0
0
0

Definition 1:
Two average knowledge bases ∑A1 and and ∑A2 are said
to be equivalent if their associated possibility
distributions (respectively necessity distributions) are
equal, namely:
∀ω ∈Ω, π∑A1 (ω) = π∑A2 (ω)
and
∀ω ∈Ω, N∑A1 (ω) = N∑A2 (ω)

Table 2: joint average possibility-necessity distribution

4 Average possibilistic and necessary
valued knowledge base
We would like to represent a class of possibilistic
Bayesian networks using a local average fuzzyvalued
knowledge base consisting of a collection of possibilistic
logic sentences (formulae) in such a way that a network
generated on the basis of the information contained in
the knowledge base is isomorphic to a set of ground
instances of the formulae. As the formal representation
of the knowledge base, we use a set of possibilistic
formulae. We represent random variables with
necessities and possibilities weights and restrict
ourselves to using only the average of these two
measures.
Formally an average necessity-possibility
knowledge base is defined as the set :
∑= {(ϕι , αi, βi) , i = 1….m}

valued

(9)

Where ϕι denotes a classical propositional formula, αi
and βi denote respectively the lower bound of certainty
(i.e necessity) and the lower bound of possibility.
We can represent the information contained in each node
of a Bayesian network, as well as the quantitative
information contained in the link matrices, if we can
represent all the direct parent/child relations. We express
the relation between each random variable and its
parents over a class of networks with a collection of
quantified formulae. The collection of formulae
represents the relation between the random variable and
its parents for any ground instantiation of the quantified
variables. The network fragment consisting of a random
variable and its parents with a set of formulae of the
form (ϕ , α ,β).
We give next some definitions inspired from [12] and
[13].

(10)

Definition 2:
Let (ϕ , α ,β) a formula in ∑A Then (ϕ , α ,β) is said to
be subsumed by ∑A if ∑A and ∑A\{(ϕ , α ,β)} are
equivalent knowledge bases.
This is means that each redundant formula should be
removed from the average valued knowledge base since
it can be deduced from the rest of formulae.

5 From fuzzy Bayesian network to fuzzy
knowledge base
In this section, we describe the process that permit to
deduce an average valued knowledge base from an
average network.
Let ΠGA be an average and necessary Bayesian network
consisting of a set of labeled variables V= {A1,A2,..An}.
Now let A be a binary variable and let (a ¬a) be its
instances.
Given the two measures π (ai|ui) and N(ai|ui) witch
represent respectively the local possibility degree and
the local necessity degree associated with the variable A
where ui ∈ UA is an instance of parents(ai). the local
average knowledge base associated with A should be
defined using the next equation :
∑AA = {( ¬ai ∨ ui, αι, βi), αι = 1- π (ai|ui) ≠ 0 and
βi =1- N(ai|ui) ≠ 0 }

(11)

To note here that in [15] the authors prove the possibility
to recover conditional possibilities from ∑A where ∑A is
a possibilistic knowledge base.
Based o the results obtained in [15] , we can check in
our case that it is possible to recover both conditional
necessities from ∑AA according to equations (12) and
(13).
1 if ∀ (ϕi , αi) ∈∑ ω ϕi
A
Π∑ (ω) =
(12)
1- max { αi : ω ϕi } otherwise

228

WSEAS TRANS. on INFORMATION SCIENCE & APPLICATIONS Issue 2, Volume 3, February 2006 ISSN: 1790-0832

= π∑AAi(ai∧ui) and ∑AAi(ai∧ui) is defined using equation
(12) and equation (13).

and
1 if ∀ (ϕi , αi) ∈∑ ω

ϕi

N∑A (ω) =

(13)
0 otherwise

Example: by applying equation (11), we get the average
knowledge base associated to the average fuzzy
Bayesian network described in section 3.
∑AA = {(a, 0.5, 0.9 )}
= {(a, 0.45 )}
∑AB = {(b∨a,0.7),(b∨¬a,0.5,0.75)(¬b∨a,0.25, 0.8)}
= {(b∨a, 0.7), (b∨¬a, 0.375) (¬b∨a, 0.2)}

Respectively the conditional necessity degree N(ai|ui) is
defined by N(ai|ui) = N(ai∧ui) = N∑AAi(ai∧ui).
Example:
From the average knowledge base associated to the node
A and by the use of equations 11 and 12
∑AA = {(a, 0.5, 0.9 )}
= {(a, 0.45)}
We can deduce the conditional average table for node A
by the use of equations 11 and 12

∑AC={(c∨a,0.6,0.9),(c∨¬a,0.4,0.9) (¬c∨a,0.3, 0.8)}
= {(c∨a, 0.54), (c∨¬a, 0.36 0.9) (¬c∨a, 0.24)}
∑AD =

{{(d∨b∨c, 0.3, 0.8), (d∨b∨¬c, 0.3, 0.8), (d∨¬b∨c,
0.7, 0.9), (d∨¬b∨¬c, 0.5, 0.6 ), (¬d∨¬b∨c, 0.5, 0.9
)}
= {{(d∨b∨c, 0.24), (d∨b∨¬c, 0.24), (d∨¬b∨c, 0.63),
(d∨¬b∨¬c, 0.3 ), (¬d∨¬b∨c, 0.45 )}
Remark: for each knowledge base the first equality
represents the initial knowledge base weighted by
possibilities and necessities when the other represents
the average based knowledge base (namely average
necessity-possibility valued knowledge base).
Next section shows the other face of transformation
between average valued knowledge base and average
fuzzy Bayesian network.

6 From Average valued knowledge base
to average fuzzy Bayesian network
In [15] the authors describe a process permitting to
deduce a possibilistic network from a possibilistic
knowledge base. In this section we follow the same way
to transform our average necessity-valued knowledge
bases into an average fuzzy Bayesian network.
To note here that the average possibilistic and necessary
Bayesian network deduced from an average necessityvalued knowledge bases will have the same graphical
structure as the starting network
The conditional average distributions are simply the
ones associated with the average knowledge bases. More
precisely, let Ai be variable and ui be an element of
parents(Ai). Let∑AAi be the local average knowledge base
associated with the node Ai. Then, the conditional
average degree A(ai|ui) is defined by π (ai|ui) = π (ai∧ui)

a
¬a

π
1
0.5

N
A
0.6 0.6
0.1 0.05

Same to rest of nodes we can deduce the rest of
conditional averages associated to other nodes and so we
can recover the average distribution presented in table 2.

7 Fuzzy Bayesian networks based on
fuzzy necessity distribution
Logical formulae with a weight strictly greater than a
given levels (lower bounds of necessity degrees) are
immune to inconsistency and can be safely used in
deductive reasoning [19]. However in order to perform
reasoning for both imprecise and uncertain information,
two important issues should be addressed. First, any
improvement of the possibility level for a piece of
information can only be achieved at the expense of the
specificity of the information; second the accorded
levels to the causality explained in terms of rules (case
of fuzzy logic) and conditional dependencies (case of
Bayesian networks) are somewhat expensive due to the
fact that these confidence level is somewhat critical.
We propose so to combine these three approaches
(Bayesian networks certainty distribution and fuzzy
logic) to develop a method for uncertain and imprecise
knowledge representation that may improve decision
based systems.
Our fuzzy beliefs are to emulate a certain Bayesian
necessity measure. For simplicity each variable here has
two states: the presence or absence of an entity. The
belief that A is present takes the form of a fuzzy truth
fA. The extent to witch the belief of variable state
influences the state beliefs of parent or child is modelled
by a fuzzy set membership function: one for each
influence direction.

WSEAS TRANS. on INFORMATION SCIENCE & APPLICATIONS Issue 2, Volume 3, February 2006 ISSN: 1790-0832

Example:
Let our certain network be as described in figure
representing a Bayesian network in metastatic cancer.

Increased total
serum calcium

A

B

Metastatic cancer
C

D

Coma

associated with the fuzzy formula (d| b,¬c) and is
associated with a membership function µ ( χ1 ) supposed
to be a triangular function (respectively µ can be
trapezoid or other kind of functions). µ is represented as
follow (figure 3):
µ( χ1 )

Brain tumor
E

Severe headaches

βD|BC1

Fig 2. A Bayesian network for metastatic cancer[20]
Fig. 2 shows a Bayesian network representing the above
cause and effect relationships. Table 3 lists the causal
influences in terms of fuzzy certainty distributions. Each
variable is characterized by an unknown necessity
degree given the state of its parents. For instance:
C ∈ [0, 1] represents the dichotomy between having a
brain tumor and not having one, c denotes the assertion
C = 1 or “Brain tumor is present”, and ¬c is the
negation of c, namely, C =0. The root node, A, which
has no parent, is characterized by its prior fuzzy
certainty distribution.
Example
Le the conditional fuzzy necessities associated to the
graph presented in figure 2 be as described in table 3.
For reason of simplicity we kept here four nodes only as
in the graph presented in figure 1.
A
[βA11 , βA12 ]

¬a
[βA21 , βA22 ]

A
¬a
B|A
b [βB|A11 , β B|A12 ] [βB|A21 , β B|A12 ]
¬b [βB|A31 , β B|A32 ] [βB|A41 , β B|A42 ]
C|A
a
c [βC|A11 , β C|A12 ]
¬c [βC|A31 , β C|A32 ]

¬a
[βC|A21 , β C|A12 ]
[βC|A41 , β C|A42 ]

bc
b¬c
Else
D|BC
d [βD|BC11,β D|BC12] [βD|BC21,βD|BC22 ] [βD|BC31,βD|BC32]
¬d [βD|BC41,βD|BC42] [βD|BC51,βD|BC52 ] [βD|BC61,βD|BC62]
Table 3: fuzzy necessity distribution
For instance N(d| b,¬c) cannot be 0.1 as described in
table 1 but rather is a fuzzy number say χ1 ∈[βD|BC1 ,
βD|BC2 ] where χ1 = ℵ(d| b,¬c) is the fuzzy necessity

βD|BC2

χ1

Fig. 3 a membership function
Then we can deduce the next possible representation of
µ(χ1) as:
µ(χ1) = k1 x (χ1 –βD|BC1) – k2 x (|χ1 – βD|BC2| + χ1 – α)
Where:
- α, k1 and k2 are two defined constants..
- | * | is the absolute value of term *
The above expression and figure mean that the interval
of χ1 is [βD|BC1 , βD|BC2 ]. If χ1 = α then µ(χ1)=1,
implying that the fuzzy necessity χ1 = α is the most
possible situation. If χ1 ≥ βD|BC2 or χ1 ≤ βD|BC1 then
µ(χ1) = 0, the possible manifestation of χ1.

8. Transformation between FBN and
fuzzy Knowledge bases
Analogously, when the given necessities degree are
fuzzy numbers as we described in section 5, the
necessity distribution N(X) associated to a node X is
considered as a fuzzy distribution defined by a
membership function
µ : [β1, β2]
χ

[0 1]
µ (χ)

(14)

Example: consider the graph of figure 2. For simplicity
each variable here has two states: the presence or
absence of an entity and we will define the same
membership function to a as to ¬a.
a
[βA11 , βA12 ]
µ1(χ)

¬a
[βA21 , βA22 ]
µ1(χ)

229

230

WSEAS TRANS. on INFORMATION SCIENCE & APPLICATIONS Issue 2, Volume 3, February 2006 ISSN: 1790-0832

a
¬a
[βB|A11 , β B|A12 ] [βB|A21 , β B|A12 ]
b
µ3(χ)
µ2(χ)
[βB|A31 , β B|A32 ] [βB|A41 , β B|A42 ]
¬b
µ2(χ)
µ3(χ)

λ + ki1 x βij1

B|A

+ ki2 x βij2 +1

χ=

(16)
ki1
Analogously, the definition of the fuzzy joint necessity
distribution is obtained by applying the fuzzy chain rule:
ℵ(A1,...,An) = min(χi), χi = ℵ(Ai|U(Ai)

a
¬a
[βC|A11 , β C|A12 ] [βC|A21 , β C|A12 ]
c
µ4(χ)
µ5(χ)
[βC|A31 , β C|A32 ] [βC|A41 , β C|A42 ]
¬c
µ4(χ)
µ5(χ)

C|A

From a semantic point of view, a certain knowledge base
∑= {(ϕι , αi) , i = 1….m} where each αi a crisp necessity
value, is understood as the necessity distribution N∑
representing the fuzzy sets of models of ∑ :

D|BC

bc
b¬c
Else
[βD|BC11,βD|BC12] [βD|BC21,βD|BC22] [βD|BC31,βD|BC32]
d
µ6(χ)
µ7(χ)
µ8(χ)
[βD|BC41,βD|BC42] [βD|BC51,βD|BC52] [βD|BC61,βD|BC62]
¬d
µ1(χ)
µ7(χ)
µ8(χ)
Table 4: fuzzy necessity distribution
with membership functions

N∑(ω) = min max ( µ[Pi](ω), 1-α) where [Pi] denotes the
set of models of Pi, so that :
µ[Pi] = α

if ω ∈ Pi

µ[Pi] (ω) =

(17)
0

otherwise

Let the different membership be as follow:

From (21) we can clearly deduce clearly that N∑(ω) is
naturally a fuzzy distribution applied to a crisp set of
values and µ[Pi] is the crisp membership function.

µi(χ) = ki1 x (χ –βij1) – ki2 x (|χ – βij2| + χ – αi)

9 Conclusion

Where:
- µi(χ) is the membership function associated to
the fuzzy variable χ, supposed to be triangular.

This paper has presented a definition of fuzzy Bayesian
networks and how to use them to deduce average
knowledge bases and vice versa. Uncertainty in nodes in
our models is represented by local knowledge bases.

- ki1 and ki2 are the used constant in each
membership function supposed to be triangular.
- βij1 and βij2 are the two min and the max
boundary of a necessity degree.
Finally by maximization of each membership function,
we can deduce an optimal value for the certainty degree
associated to each fuzzy variable (i.e. proposition).
Namely:
ℵ( χ ) = µ( χ ) = 1
Then it will be easy to deduce the value of χ as follow:
λ + ki1 x βij1

+ ki2 x βij2 +1

χ=

(15)
ki1

By replacing λ by 1 (the maximization of µ( χ )), the
value of χ will be:

The key benefits of this representation to the
practitioner are that both knowledge declaration and
possibilistic inference are modular. Individual
knowledge bases should be separately compilable and
query complete. Also this representation specifies an
organized structure for elicitation of the graph structure.
We only defined the transformation process for
knowledge bases.
Certain Bayesian networks with fuzzy knowledge bases
approach in a natural way gives us the subsethood of the
evidence for each logical formula. Although the
methodology proposed in this paper, is aimed and
illustrated by some typical examples, the developed
techniques require experimental results.
A future work is to extend this representation by
definition of efficient algorithms for locally inferences.

WSEAS TRANS. on INFORMATION SCIENCE & APPLICATIONS Issue 2, Volume 3, February 2006 ISSN: 1790-0832

