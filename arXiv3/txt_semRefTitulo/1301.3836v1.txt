
Planning for distributed agents with partial
state information is considered from a decision­
theoretic perspective. We describe generaliza­
tions of both the MDP and POMDP models
that allow for decentralized control. For even a
small number of agents, the finite-horizon prob­
lems corresponding to both of our models are
complete for nondeterministic exponential time.
These complexity results illustrate a fundamen­
tal difference between centralized and decentral­
ized control of Markov processes. In contrast to
the MDP and POMDP problems, the problems
we consider provably do not admit polynomial­
time algorithms and most likely require doubly
exponential time to solve in the worst case. We
have thus provided mathematical evidence corre­
sponding to the intuition that decentralized plan­
ning problems cannot easily be reduced to cen­
tralized problems and solved exactly using estab­
lished techniques.

1

Introduction

Among researchers in artificial intelligence, there has been
growing interest in problems with multiple distributed
agents working to achieve a common goal (Grosz & Kraus,
1996; Lesser, 1998; desJardins et al., 1999; Durfee, 1999;
Stone & Veloso, 1999). In many of these problems, intera­
gent communication is costly or impossible. For instance,
consider two robots cooperating to push a box (Mataric,
1998). Communication between the robots may take time
that could otherwise be spent performing physical actions.
Thus, it may be suboptimal for the robots to communi­
cate frequently. A planner is faced with the difficult task
of deciding what each robot should do in between com­
munications, when it only has access to its own sensory
information. Other problems of planning for distributed
agents with limited communication include maximizing the

throughput of a multiple access broadcast channel (Ooi &
Womell, 1996) and coordinating multiple spacecraft on a
mission together (Estlin et al., 1999). We are interested
in the question of whether these planning problems are
computationally harder to solve than problems that involve
planning for a single agent or multiple agents with access
to the exact same information.
We focus on centralized planning for distributed agents,
with the Markov decision process (MDP) framework as
the basis for our model of agents interacting with an envi­
ronment. A partially observable Markov decision process
(POMDP) is a generalization of an MDP in which an agent
must base its decisions on incomplete information about
the state of the environment (White, 1993). We extend
the POMDP model to allow for multiple distributed agents
to each receive local observations and base their decisions
on these observations. The state transitions and expected
rewards depend on the actions of all of the agents. We
call this a decentralized partially observable Markov de­
cision process (DEC-POMDP). An interesting special case
of a DEC-POMDP satisfies the assumption that at any time
step the state is uniquely determined from the current set
of observations of the agents. This is denoted a decen­
tralized Markov decision process (DEC-MDP). The MDP,
POMDP, and DEC-MDP can all be viewed as special cases
of the DEC-POMDP. The relationships among the models
are shown in Figure 1.
There has been some related work in AI. Boutilier (1999)
studies multi-agent Markov decision processes (MMDPs),
but in this model, the agents all have access to the same in­
formation. In the framework we describe, this assumption
is not made. Peshkin et al. (2000) use essentially the DEC­
POMDP model (although they refer to it as a partially ob­
servable identical payoff stochastic game (POIPSG)) and
discuss algorithms for obtaining approximate solutions to
the corresponding optimization problem. The models that
we study also exist in the control theory literature (Ooi
et al., 1997; Aicardi et al., 1987). However, the compu­
tational complexity inherent in these models has not been
studied. One closely related piece of work is that of Tsit-

33

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

agent given that it chose action a in state s.
There are several different ways to define "long-term re­
ward" and thus several different measures of optimality. In
this paper, we focus on finite-horizon optimality, for which
the aim is to maximize the expected sum of rewards re­
ceived over T time steps. Formally, the agent should maximize
Figure 1: The relationships among the models.

siklis and Athans (1985), in which the complexity of non­
sequential decentralized decision problems is studied.
We discuss the computational complexity of finding opti­
mal policies for the finite-horizon versions of these prob­
lems. It is known that solving an MDP is P-complete
and that solving a POMDP is PSPACE-complete (Papadim­
itriou & Tsitsiklis, 1987). We show that solving a DEC­
POMDP with a constant number, m ;::: 2, of agents is com­
plete for the complexity class nondeterministic exponen­
tial time (NEXP). Furthermore, solving a DEC-MDP with
a constant number, m ;::: 3, of agents is NEXP-complete.
This has a few consequences. One is that these problems
provably do not admit polynomial-time algorithms. This
trait is not shared by the MDP problems nor the POMDP
problems. Another consequence is that any algorithm for
solving either problem will most likely take doubly expo­
nential time in the worst case. In contrast, the exact al­
gorithms for finite-horizon POMDPs take "only" exponen­
tial time in the worst case. Thus, our results shed light
on the fundamental differences between centralized and de­
centralized control of Markov decision processes. We now
have mathematical evidence corresponding to the intuition
that decentralized planning problems are more difficult to
solve than their centralized counterparts. These results can
steer researchers away from trying to find easy reductions
from the decentralized problems to centralized ones and to­
ward completely different approaches.
A precise categorization of the two-agent DEC-MDP prob­
lem presents an interesting mathematical challenge. The
extent of our present knowledge is that the problem is
PSPACE-hard and is contained in NEXP.

2

Centralized Models

A Markov decision process (MDP) models an agent acting
in a stochastic environment to maximize its long-term re­
ward. The type of MDP that we consider contains a finite
setS of states, withs0 ES as the start state. For each state
s E S, As is a finite set of actions available to the agent.
P is the table of transition probabilities, where P(s'Js, a)
is the probability of a transition to state s' given that the
agent performed action a in states. R is the reward func­
tion, where R(s, a) is the expected reward received by the

where r(st, at) is the reward received at time step t. A

policy <5 for a finite-horizon MDP is a mapping from each
states and timet to an action <5(s,t). This is called a non­
stationary policy. The decision problem corresponding to
a finite-horizon MDP is as follows: Given an MDP M, a
positive integer T, and an integer K, is there a policy that
yields total reward at least K?
An MDP can be generalized so that the agent does not nec­
essarily observe the exact state of the environment at each
time step. This is called a partially observable Markov de­
cision process (POMDP). A POMDP has a state setS, a
start stateso E S, a table of transition probabilities P, and
a reward function R, just as an MDP does. Additionally, it
contains a finite set n of observations, and a table 0 of ob­
servation probabilities, where O(oJa,s') is the probability
that o is observed, given that action a was taken and led to
state s'. For each observation o E 11, Ao is a finite set of
actions available to the agent. A policy <5 is now a mapping
from histories of observations o , ..., Ot to actions in Ao, .
1
The decision problem for a POMDP is stated in exactly the
same way as for an MDP.

3

Decentralized Models

A decentralized partially observable Markov decision pro­
cess (DEC-POMDP) is a generalization of a POMDP to
allow for distributed control by m agents that may not
be able to observe the exact state.
A DEC-POMDP
contains a finite set S of states, with so E S as the
start state. The transition probabilities P(s' Is, a1, ..., am)
and expected rewards R(s, a1, ..., am) depend on the ac­
tions of all agents. ni is a finite set of observations
for agent i, and 0 is a table of observation probabilities,
where O(o1, ..., omJa1, ..., am,s') is the probability that
o1, ..., om are observed by agents 1, . . . , m respectively,
given that the action tuple (a1, ..., am) was taken and led
to state s'. Each agent i has a set of actions A� for each
observation oi E Oi. Notice that this model reduces to a
POMDP in the one-agent case.
For each a1, ..., am, s', let w(a1,
, am,s') denote the
set of observation tuples that have a nonzero chance of
occurring given that the action tuple (a1, ..., am) was
taken and led to state s'. To form a decentralized Markov
decision process (DEC-MDP), we add the requirement
• • .

34

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

that for each a1,.. .,am,s', and each (o1, . . . ,om) E
w(a 1,...,am,s') the state is uniquely determined by
(o1,...,om). In the one-agent case, this model is essentially the same as an MDP.
We define a local policy, oi, to be a mapping from local
histories of observations of, ... , o� to actions ai E A�,.
A joint policy, o = (81,...,om), is defined to be a tu­
ple of local policies. We wish to find a joint policy that
maximizes the total expected return over the finite hori­
zon. The decision problem is stated as follows: Given a
DEC-POMDP M, a positive integer T, and an integer K,
is there a joint policy that yields total reward at least K?
Let DEC-POMDP and DEC-MDPm denote the deci­
m
sion problems for them-agent DEC-POMDP and them­
agent DEC-MDP, respectively.

4

Complexity Results

It is necessary to consider only problems for which T <
lSI. If we place no restrictions on T, then the upper
bounds do not necessarily hold. Also, we assume that
each of the elements of the tables for the transition prob­
abilities and expected rewards can be represented with a
constant number of bits. With these restrictions, it was
shown in (Papadimitriou & Tsitsiklis, 1987) that the de­
cision problem for an MDP is P-complete. In the same
paper, the authors showed that the decision problem for a
POMDP is PSPACE-complete and thus probably does not
admit a polynomial-time algorithm. We prove that for all
m 2: 2, DEC-POMDP is NEXP-complete, and for all
m
m 2: 3, DEC-MDP is NEXP-complete, where NEXP =
m
c
NTIME ( 2n ) (Papadimitriou, 1994). Since P ::J. NEXP, we
can be certain that there does not exist a polynomial-time
algorithm for either problem. Moreover, there probably is
not even an exponential-time algorithm that solves either
problem.
For our reduction, we use a problem called TILING (Pa­
padimitriou, 1994), which is described as follows: We
are given a set of square tile types T = {to, ... , tk }, to­
gether with two relations H, V � T x T (the horizontal
and vertical compatibility relations, respectively). We are
also given an integer n in binary. A tiling is a function
f: {O,. . . , n - 1} x {O,. . . , n - 1 } -t T. A tiling f
is consistent if and only if (a) f(O,0) = to, and (b) for all
i,j (f(i,j),f(i+1,j)) E H, and (f(i,j),f(i,j+1)) E V.
The decision problem is to tell, given T, H, V, and n ,
whether a consistent tiling exists. It is known that TILING
is NEXP-complete.
Theorem 1

complete.

For all m

>

2, DEC-POMDP

m

is

NEXP­

Proof. First, we will show that the problem is in NEXP. We
can guess a joint policy o and write it down in exponential

time. This is because a joint policy consists of m map­
pings from local histories to actions, and since T < lSI,
all histories have length less than lSI. A DEC-POMDP
together with a joint policy can be viewed as a POMDP to­
gether with a policy, where the observations in the POMDP
correspond to the observation tuples in the DEC-POMDP.
In exponential time, each of the exponentially many possi­
ble sequences of observations can be converted into belief
states. The transition probabilities and expected rewards
for the corresponding "belief MDP" can be computed in
exponential time (Kaelbling et al., 1998). It is possible to
use dynamic programming to determine whether the policy
yields expected reward at least K in this belief MDP. This
takes at most exponential time.
Now we show that the problem is NEXP-hard. For sim­
plicity, we consider only the two-agent case. Clearly, the
problem with more agents can be no easier. We are given
an arbitrary instance of TILING. From it, we construct a
DEC-POMDP such that the existence of a joint policy that
yields a reward of at least zero is equivalent to the existence
of a consistent tiling in the original problem. Furthermore,
T < lSI in the DEC-POMDP that is constructed. Intu­
itively, a local policy in our DEC-POMDP corresponds to
a mapping from tile positions to tile types, i.e., a tiling, and
thus a joint policy corresponds to a pair of tilings. The pro­
cess works as follows: In the position choice phase, two
tile positions are randomly "chosen" by the environment.
Then, at the tile choice step, each agent sees a different
position and must use its policy to determine a tile to be
placed in that position. Based on information about where
the two positions are in relation to each other, the environ­
ment checks whether the tile types placed in the two posi­
tions could be part of one consistent tiling. Only if the nec­
essary conditions hold do the agents obtain a nonnegative
reward. It turns out that the agents can obtain a nonnega­
tive expected reward if and only if the conditions hold for
all pairs of positions the environment can choose, i.e., there
exists a consistent tiling.
We now present the construction in detail. During the posi­
tion choice phase, each agent only has one action available
to it, and a reward of zero is obtained at each step. The
states and the transition probability matrix comprise the
nontrivial aspect of this phase. Recall that this phase intu­
itively represents the choosing of two tile positions. First,
let the two tile positions be denoted (i1,jt) and ( i2 ,jz),
where 0 ::; i 1,i2,j1,jz ::; n - 1. There are 4log n steps in
this phase, and each step is devoted to the choosing of one
bit of one of the numbers. (We assume that n is a power
of two. It is straightforward to modify the proof to deal
with the more general case.) The order in which the bits
are chosen is important, and it is as follows: The bits of
i1 and i2 are chosen from least significant up to most sig­
nificant, alternating between the two numbers at each step.
Then j1 and jz are chosen in the same way. As the bits of

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

35

the numbers are being determined, information about the
relationships between the numbers is being recorded in the
state. How we express all of this as a Markov process is
explained below.

6) Vertically Adjacent Tile Positions
This component is used to check whether the first tile posi­
tion is directly above the second one. Its regular expression
is as follows:

Each state has six components, and each component rep­
resents a necessary piece of information about the two tile
positions being chosen. We describe how each of the com­
ponents changes with time. A time step in our process
can be viewed as having two parts, which we refer to as
the stochastic part and the deterministic part. During the
stochastic part, the environment "flips a coin" to choose
either the number 0 or the number 1, each with equal prob­
ability. After this choice is made, the change in each com­
ponent of the state can be described by a deterministic finite
automaton that takes as input a string of O's and 1 's (the en­
vironment's coin flips). The semantics of the components,
along with their associated automata, are described below:

(11 + 00) ...(11 + 00)(10)*(01)(11 + 00)*.

1) Bit Chosen in the Last Step

This component of the state says whether 0 or 1 was just
chosen by the environment. The corresponding automaton
consists of only two states.
2) Number of Bits Chosen So Far
This component simply counts up to 4logn, in order to
determine when the position choice phase should end. Its
automaton consists of 4logn + 1 states.

3) Equal Tile Positions
After the 4logn steps, this component tells us whether the
two tile positions chosen are equal or not. For this automa­
ton, along with the following three, we need to have a no­
tion of an accept state. Consider the following regular ex­
pression:
(00 + 11)*.
Note that the DFA corresponding to the above expression,
on an input of length 4logn, ends in an accept state if and
only if (i1,i1) = (iz,jz).
4) Upper Left Tile Position

This component is used to check whether the first tile posi­
tion is the upper left comer of the grid. Its regular expres­
sion is as follows:
(0(0 + 1))*.
The corresponding DFA, on an input of length 4logn, ends
in an accept state if and only if ( i1,j1) = (0,0).
5) Horizontally Adjacent Tile Positions
This component is used to check whether the first tile po­
sition is directly to the left of the second one. Its regular
expression is as follows:

(10)*(01)(11 + 00)* (11 + 00) ...(11 + 00) .

logn

The corresponding DFA, on an input of length 4logn, ends
in an accept state if and only if(i1,j1 + 1) = (iz,jz).
So far we have described the six automata that determine
how each of the six components of the state evolve based on
input (0 or 1) from the environment. We can take the cross
product of these six automata to get a new automaton that
is only polynomially bigger and describes how the entire
state evolves based on the sequence of O's and 1 's chosen
by the environment. This automaton, along with the en­
vironment's "coin flips," corresponds to a Markov process.
The number of states of the process is polylogarithmic inn,
and hence polynomial in the size of the TILING instance.
The start state s0 is a tuple of the start states of the six au­
tomata. The table of transition probabilities for this process
can be constructed in time polylogarithmic in n.
We have described the states, actions, state transitions, and
rewards for the position choice phase, and we now describe
the observation function. In this DEC-POMDP, the obser­
vations are uniquely determined from the state. For the
states after which a bit of i1 or i1 has been chosen, agent
one observes the first component of the state, while agent
two observes a dummy observation. The reverse is true for
the states after which a bit of i2 or jz has been chosen. Intu­
itively, agent one "sees" only(i1, jl), and agent two "sees"
only ( iz,)z) .
When the second component of the state reaches its limit,
the tile positions have been chosen, and the last four com­
ponents of the state contain information about the tile po­
sitions and how they are related. Of course, the exact tile
positions are not recorded in the state, as this would require
exponentially many states. This marks the end of the posi­
tion choice phase. In the next step, which we call the tile
choice step, each agent has k + 1 actions available to it,
corresponding to each of the tile types, to, ... , tk. We de­
note agent one's choice t1 and agent two's choice t2. No
matter which actions are chosen, the state transitions de­
terministically to some final state. The reward function for
this step is the nontrivial part. After the actions are chosen,
the following statements are checked for validity:

(i2 , j2 ), then t1 = t2.
2)If(i1,jl) (0,0), then t1 = t0.
3)If(il + 1,jl) = (iz,jz) , then (t1 , t2 )
4)If(i1,i1 + 1) (iz,jz) , then (tl , t2 )
1)If(h,jl)

=

=

=

logn

The corresponding DFA, on an input of length 4logn, ends
in an accept state if and only if ( i1 + 1, j1) = ( iz,jz) .

E
E

H.
V.

If all of these are true, then a reward of 0 is received. Oth­
erwise, a reward of -1 is received. This reward function
can be computed from the TILING instance in polynomial

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

36

time. To complete the construction, the horizon T is set
to 4log n (exactly the number of steps it takes the process
to reach the tile choice step, and fewer than the number of
states lSI).
Now we argue that the expected reward is zero if and only
if there exists a consistent tiling. First, suppose a consis­
tent tiling exists. This tiling corresponds to a local policy
for an agent. If each of the two agents follows this policy,
then no matter which two positions are chosen by the en­
vironment, the agents choose tile types for those positions
so that the conditions checked at the end evaluate to true.
Thus, no matter what sequence of O's and 1's the environ­
ment chooses, the agents receive a reward of zero. Hence,
the expected reward for the agents is zero.
For the converse, suppose the expected reward is zero.
Then the reward is zero no matter what sequence of O's
and 1's the environment chooses, i.e., no matter which two
tile positions are chosen. This implies that the four condi­
tions mentioned above are satisfied for any two tile posi­
tions that are chosen. The first condition ensures that for
all pairs of tile positions, if the positions are equal, then the
tile types chosen are the same. This implies that the two
agents' tilings are exactly the same. The last three condi­
tions ensure that this tiling is consistent. 0
Theorem 2

For all m

2:

3, DEC-MDPm is NEXP­

complete.
Proof. (Sketch) Inclusion in NEXP follows from the fact

that a DEC-MDP is a special case of a DEC-POMDP. For
NEXP-hardness, we can reduce a DEC-POMDP with two
agents to a DEC-MDP with three agents. We simply add a
third agent to the DEC-POMDP and impose the following
requirement: The state is uniquely determined by just the
third agent's observation, but the third agent always has just
one action and cannot affect the state transitions or rewards
received. It is clear that the new problem qualifies as a
DEC-MDP and is essentially the same as the original DEC­
POMDP. 0
The reduction described above can also be used to con­
struct a two-agent DEC-MDP from a POMDP and hence
show that DEC-MDP2 is PSPACE-hard. However, this
technique is not powerful enough to prove the NEXP­
hardness of the problem. In fact, the question of whether
DEC-MDP2 is NEXP-hard remains open. Note that in
the reduction in the proof of Theorem 1, the observa­
tion function is such that there are some parts of the state
that are hidden from both agents. This needs to some­
how be avoided in order to reduce to a two-agent DEC­
MDP. A simpler task may actually be to derive a better up­
per bound for the problem. For example, it may be pos­
sible that DEC-MDP2 E co-NEXP, where co-NEXP =
{LIL E NEXP } . Regardless of the outcome, the problem
provides an interesting mathematical challenge.

5

Discussion

Using the tools of worst-case complexity analysis, we
analyzed two models of decision-theoretic planning for
distributed agents.
Specifically, we proved that the
finite-horizon m-agent DEC-POMDP problem is NEXP­
complete form 2: 2 and the finite-horizonm-agent DEC­
MDP problem is NEXP-complete form 2: 3.
The results have some theoretical implications. First, un­
like the MDP and POMDP problems, the problems we
studied provably do not admit polynomial-time algorithms,
since P # NEXP. Second, we have drawn a connection be­
tween work on Markov decision processes and the body
of work in complexity theory that deals with the exponen­
tial jump in complexity due to decentralization (Peterson
& Reif, 1979; Babai et al., 1991). Finally, the two-agent
DEC-MDP case yields an interesting open problem. The
solution of the problem may imply that the difference be­
tween planning for two agents and planning for more than
two agents is a significant one in the case where the state is
collectively observed by the agents.
There are also more direct implications for researchers try­
ing to solve problems of planning for distributed agents.
Consider the growing body of work on algorithms for ob­
taining exact or approximate solutions for POMDPs (e.g.,
Jaakkola et al., 1995; Cassandra et al., 1997; Hansen,
1998). It would have been beneficial to discover that a
DEC-POMDP or DEC-MDP is just a POMDP "in dis­
guise," in the sense that it can easily be converted to a
POMDP and solved using established techniques. We have
provided evidence to the contrary, however. The complex­
ity results do not answer all of the questions surrounding
how these problems should be attacked, but they do sug­
gest that the fundamentally different structure of the de­
centralized problems may require fundamentally different
algorithmic ideas.
Finally, consider the infinite-horizon versions of the afore­
mentioned problems. It has recently been shown that the
infinite-horizon POMDP problem is undecidable (Madani
et al., 1999) under several different optimality criteria.
Since a POMDP is a special case of a DEC-POMDP, the
corresponding DEC-POMDP problems are also undecid­
able. In addition, because it is possible to reduce a POMDP
to a two-agent DEC-MDP, the DEC-MDP problems are
also undecidable.
Acknowledgments

The authors thank Micah Adler, Andy Barto, Dexter
Kozen, Victor Lesser, Frank McSherry, Ted Perkins, and
Ping Xuan for helpful discussions. This work was sup­
ported in part by the National Science Foundation under
grants IRI-9624992, IRI-9634938, and CCR-9877078 and
an NSF Graduate Fellowship to Daniel Bernstein.

37

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Proceedings of the Sixteenth National Conference on Arti­

