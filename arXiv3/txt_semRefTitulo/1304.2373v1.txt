

An approximation method is presented for probabilistic inference with continuous random
variables. These problems can ari.se in many practical problems, in particular where there are
"second order" probabilities. The approximation, based on the Gaussian influence diagram,
iterates over linear approximations to the inference problem.
Introduction

There have been a number of techniques developed in recent years for the efficient analysis of
probabilistic inference problems, represented as Bayes' networks or influence diagrams
[Lauritzen and Spiegelhalter 1988, Pearl1986, Shachter1988]. To varying degrees these
methods exploit the conditional independence assumed and revealed in the problem structure to
analyze problems in polynomial time, essentially polynomial in the number of variables and the
size of the largest state space encountered during the evaluation. UnfortuQ.ately, there are many
problems ofinterest for which the vanables of interest are continuous rather than discrete, so
the relevant state spaces become infinite and the polynomial complexity is of little help .
In this paper, an algorithm is presented which is based on a linear approximation to the problem
structure. Each of the variables in the model is transformed, and the transformed variables are
assumed to have a Gaussian joint distribution. Through successive iterations, this linear
approximation is refined until it converges to a consistent solution. Although this method is an
approximation rather than an analytical solution, it has proven quite accurate for a variety of
problems in health technology assessment. It has tended to converge rapidly and, since each step
is polynomial in the number of variables, this provides a polynomial heuristic for probabilistic
infe�nce with continuous variable.
The algorithm presented in this paper was motivated by a technique for medical technology
assessment based on second order probabilities [Eddy1988, Shachter et al19871988] . The
parameters of interest are the probabilities for different well-defined physical events. The
probabilities are uncertain quantities and our prior knowledge about them is described by
(usually "noninformative") probability distributions. The relevant medical evidence is then
incorporated within a model to provide defensible, posterior distributions for these parameters.
There is an established philosophical basis for this approach, which provides a solid framework
for knowledge acquisition in uncertain environments. Recent work argues persuasively that the
established methodology for probabilistic reasoning applies theoretically to these second-order
probabilities just as it does to the first-order kind [Kyburg 1987, Pearl1987]. Nonetheless, the
299

I
practical problems are considerable since the higher order probabilities are as a rule continuous
distributions while the first order ones are usually discrete.
How then can these continuous probabilistic inference problems be analyzed? There are a
several other approaches for dealing with this additional complexity besides the linear method.
1.

Conjugate priors:

If a model's structure allows it, prior distributions for parameters can be

chosen from convenient families of conjugate distributions, so that the posterior
distributions given the experimental evidence stay within those families [DeGroot

This is

an

1972].

analytical solution to continuous inference problem and the Gaussian model is

one example of this approach. Unfortunately, conjugate families are not closed in general
when experimental evidence bears (indirectly) on more than one basic parameter or for
different forms of experimental evidence.

2. Discretization:

Discrete techniques can be used by divide the sample space into intervals.

However, processing time goes up with some power of the refinement, while resolution is
only grows linearly with it. A similar approach to the one in this paper could be used to
iterate, detennining new discretizations after each solution.

3. Numerical integration: This is discretization of ano�her sort. It is impractical for more than
4.

a few dimensions.

Monte Carlo integration: This is the state-of-the-art approach to numerical integration
[Geweke 1988]. It can successfully solve the types of problems discussed here, without
the distributional assumptions imposed by the linear approximation method. While it

provides additional accuracy, it does so at substantially greater cost in computer time.
Although some of these other techniques might be more appropriate for a particular problem, the
linear approximation possesses a unique combination of speed and generality, providing an
efficient approximation to a large class of problems.

·

Notation and Basic Framework
The original model consists of n variables, Y 1· ... , Yn• represented by an influence diagram, a
network with an acyclic directed graph. Each variable Yj corresponds to a node j in the diagram,

{ 1, .. , n } contains the nodes in the diagram. YJ denotes the set of variables
corresponding to the indices J. Thus, the direct predecessors of node j, denoted by CG), represent
thus the set N

=

.

the set of conditioning variables Y C(j) for Yj- When the order of the variables is significant the
sequence s, a vector of indices, is used to represent the vector of variables,

Ys· A sequence

s is

called ordered if every predecessor for any node in the sequence precedes it in the sequence.
There are three types of variables represented in the influence diagram in this paper. Basic

parameters are quantities for which a simple prior distribution is known. They have no

conditioning variables, C(j) = 0, and they are assumed to be mutually independent a priori.
Deterministic parameters are quantities defined in terms of other parameters. They have
conditioning variables but are assumed to be detenninistic functions of those conditioning
variables, so that their realizations would be known with certainty if the values of their

conditioning variables were known. Finally, there are experimental evidence variables, whose
realizations have been observed. They are characterized by a conditional distribution (a priori)
and an observed value (a posteriori) and are assumed to have exactly one conditioning variable,

300

I

·I
I
I
I
I
l
I
I

I
I
II
·I
·t
I
I

I
I

I

I
I

either a basic or deterministic parameter. These variables and the assumptions about them are
depicted in Figure 1. [For information, see Shachter et al 1988:]

I

1
I
I
I
I
I
I

I
I
I
I
I
I

I
I

•

Figure 1. The model assumptions and the different types of variables

In the linear approximation method, a Gaussian variable, X j , is associated with each parameter
variable Yj> by a deterministic function, Xj = Tj ( Yj ) . The set of variables XN is assumed to
have a multivariate normal joint distribution characterized by its means E XN and covariance
matrix
L = LNN = Var [XN] = E [X xT] -E [X] E [ xT].
Alternatively, the Gaussian influence diagram [Shachter and Kenley 1988] represents the
multivariate Gaussian distribution through its conditional regression equations
X j = E Xj + BeG), jT [XcG) -E XcG)] + Ejo
where Ej is a normal random variable with mean 0 and variance Vj , and B is a strictly upper
triangular matrix of linear coefficients.
The resulting Gaussian model (if the original variables were integrated out) has the same
structure as the original model, with basic and deterministic parameters and experimental
evidence variables, except that they are assumed to have a multivariate normal distribution so
that they can be manipulated using the operations of the Gaussian influence diagram. (Other
similar techniques have been developed to exploit the Gaussian properties in a network
representation [Pearl 1985]. Although the linear approximation method will be explained in
terms of the Gaussian influence diagraJll, these other techniques could be used to implement it.)
One last bit of notation denotes the the revision of probabilities over time. The superscript t as in
Et X represents the prior expectation of X in the tth iteration and Et [X I D ] represents its
301

I
expectation after observing the experimental evidence. The superscript t will be omitted for
readability whenever it is unambiguous to do so.

Variable Transformations
The fundamental property of the approach is that every variable in the model is transformed into
a Gaussian variable, and the resulting multivariate Gaussian model will be maintained and
manipulated, in order to provide indirect insight into the original variables and their dependence.
Although the model could be embellished further, there are three basic transformations: scaled,
log-scaled and logistic-scaled. These allow the representation of unbounded, semi-bol}nded,
and bounded variables, respectively. Denoting a variable in the original model as Y, one in the
transformed model as X, and the transformation function as T, the transformations are expressed
in terms of scaling parameters a and b, where a * b:

1. Scaled Transformation: Y e ( -oo, +oo)
X=T(Y )=( Y-a) I( b-a).
T-1 (X)=a +( b-a)X
T'(Y)= 1 I( b-a)

I
l
I
I
I
I
I

2. Log-Scaled Transformation: Y e ( a, +oo) if a < b and Y
X=T(Y)=ln(Y -a)l( b-a)
T- 1(X)=a +( b-a) eX
T'( Y ) = 1 I I b a I

e

( -oo, a) if a > b

I
.I

-·

3. Logistic-Scaled Transformation: Y e ( a, b) if a < b and Y .e ( b, a ) if a > b
X = T( Y)=ln( Y-a) I( b-Y)
T-1 (X)=b +( a-b) I( 1 + eX)
T''(Y)=liiY-al +liiY-bl
Of course, X andY are random variables, so we must be able to transform from the distribution
forX to the distribution forY. We approximate this more complicated transformation by the
function T. which maps the mean and variance of Y into the mean and variance forX, based on
the distributional form forY. These transformations would be exact if theXN were truly

multivariate normal.
1.

Normal Distribution: ( Y-a ) I( b-a) - Normal( J.L, cr2 ))
with X =( Y - a) I (b - a)(scaled transformation)
(EX, VarX)=T(EY, VarY)=(( EY-a) I( b-a), VarY I( b - a ) 2)
(E Y, VarY)=T-1 (EX, VarX)=( a +( b-a) EX, ( b- a )2 Var X)

2. Lognormal Distribution: (Y-a) I (b-a) - Lognormal( J.l., cr2 )
with X=In( Y-a ) I(b-a)(log-scaled transformation)
(EX, VarX)= T(EY, VarY)=( Jl, cr2)
where cr2 =V arX= In [ 1 + VarY ( E Y - a ) -2]

302

I
I
I
I
I
I
I
I
I

I

.I
I
I
I
I
I
I
,,
I
I
I
I
I
I
I
I

I
I

and � =E X =In [( E Y - a )I( b - a ) ] - cr2I2
( E Y, Var Y ) = T -1 ( E X, Var X )
. =( a+ ( b _a ) e{ E X+Var. XI2 } , (b _a)2 ( eVar X_ 1 ) e { 2 E X+Var X } ).
o

( a,

B)
3. Beta Distribution: ( Y - a )I(b - a ) - Beta
with X =In ( Y - a)I ( Q Y ) (logistic-scaled transformation)
-

( E X, Var X )= T ( E Y, Var Y ) =( 'JI (a)- 'JI (B), 'I'' (a )+'JI' (B))
where 'I' and 'I'' are the digamma and trigamma functions [Abramowitz and Stegun 1972],
'JI( z ) =

d 1nr( z )
dz

r'" (
=

9

)

r(:) =. L
1

and w =z+ 10

= 0

1 +In w
z� i

w -1
2

-2

0

There· is no closed-form expression for the inverse function T- 1. If
enough, then they can be approximated by
=.5 +( 1 eE X ) IVar X
and

ao

+
a
ak]
[.5ak
[
]
,
�k �k

-6

-4

�2 +�2 0- �52

a

and Bare large

Bo =.5+( 1+e-E X )IVar X .

andBean be estimated using Newton's method and curve fitting
using the iterative formula:
-1
'JI(Clk)- 'Jf(J30- E [ X]
'JI'(Ok) -'Jf'(�
}
max {
-[
.5
'JI"(qJ'JI"(�
'JI'(Ok)+'JI'(�- Var[ X]
In general, however,

[a�kk+1]-+1

J

[

l

where '\jl" is the tetragamma function [Abramowitz and Stegun 1972],
3
d lnr( z)
_w-4 w-6�w-8
-2
+
=
- w -2 - w-3
'l'"( z ) =
3
6
6
2
dz3
i = o( z+i )

f

Experimental Observations
The linear approximation requires that likelihood functions for experimental observations be
derived in terms of the transformed, Gaussian parameter X on which the experimental evidence
bears. Three kinds of experimental evidence are consider here, assuming samples from either a
binomial or normal distribution. Of course, the method could be extended to other experimental
designs.
1. Normal experiment, n exchangeable samples from Normal ( X, cr2 ) where cr2 is known and

the observed values have sample mean m =

Lj djIn .

The likelihood function for the evidence is given by
D I X - Normal ( X, cr2In ) with observation d =m
303

0

I
2. Normal experiment, n

>

2 exchangeable samples from Normal (X, cr2) where cr2 is fixed but

u�own and the observed values have sample mean m and sample variance s =
The likelihood is t-distributed but can be approximated by
D I X- Normal (X, s I ( n - 3) with observation d =m.

� (dj - m)2In.

(Note: The preceding likelihoods can· also be used for exchangeable samples from a lognormal
distribution by transforming each sample.)
3. Binomial experiment, D - Binomial ( ( Y - a ) I ( b - a ), n ) with s successes observed
with X =In Y I (I-Y ) (logistic-scaled transformation, a= 0, b = 1)
The_ likelihood is binomial-distributed but can be approximated by
D I X- Normal ( X, v ) with observation d = v [ x2 I v2 - x 1Iv 1 ]
where v =I I [ llv2 - llvi] ,

and

(

J}) ,
v2 = 'If' ( a + s ) + 'If' ( Jl + n - s ) ,
XI='If(a)- 'If ( J}),
x2 = 'If(a + s )-'I' ( J} + n - s ) .
VI = '�" ( a) + 'I''

(The estimate is most accurate ifa and Bare the prior p arameters for Y. Alternatively,
they can be set equal, to values such as .5 or I, but they need not be. )

Linear Approximation Algorithm
I. The first step in the algorithm is to compute the linear approximations for each of the basic
parameters and each of the experimental observations. These values will be used in each
iteration of the algorithm. To estimate the original value for the remaining, deterministic
parameters compute, in order,
EO Yj =EO [ ( YcG) )] = fj ( EO Y C(j) )
(approximating the expected value of the function by the function of the expected value) and set
the conditional variance VarO [ Yj I Y C(j)] = 0 .

�

2. . The iterative step in the algorithm proceeds until the algorithm has either converged or
diverged. Define the relative difference from one iteration to the next as
rl = 0
if Et [X j ID ].= Et- 1 [X j ID]
t
t- 1 [Xj ID ] I I max { I Et [Xj ID ] I, I Et- 1 [Xj ID ] I }
otherwise .
= I E [ Xj ID ] - E

Letting rmaxt = maxj { ljt }, convergence occurs when rmax t
rmaxt > rmaxt- 1 > ... > rmax t-m for some m such as 3.

<e

and divergence occurs when

2a. The first step in each iteration is to compute the linear coefficient in the Gaussian influence
diagram for the transformed variables. For each variable, in order, compute
ax·
t
J
B··=
dxi XN= Et- 1 [X NI D]
lJ

[ ]

'

304

I,

'I
I
I
I
I
I
a.
I
I
I
I
I
I
'
I
I
I

I
I
I
I
I
I
I.
I
I
I
I
I
I
I
I
I
I
I
I

taking advantage of the linearity of XN . Now, using the approximation in 2 d,
.

t�))

[

l

)
aTj < fj < Y
t
Bij=
oTi (Yt1 )
Y N=E t- 1 [Y NID]
an
t
t
T'j ( fj (E -1 [Y C(j)l D])
E -1 [Y C(j)l D])

)�(

·

t])
T'i (E l [Y iiD]

=

2b. For basic parameters and experimental outcomes, set the. mean and conditional variances for
the transformed variables to their original values. For each deterministic parameter, in order,
1
t
t
t
E X j=E [T j (Y j )]=E [T j ( fj (Y C(j)) )]=T j ( fj (E Y CG)) )
=

t
T j ( fj (E - 1 [yC (j)l D]) ) +

i

L
E

C(j)

t
t
sfj (E [Xi]-E - 1 [Xi I D ] )

.and set the conditional variance to zero. (This is the first order approximation to E X, relative to
the posterior from the previous iteration, in the same spirit as Maybeck [ 98 2].)
1
Afterwards, compute the unconditional variance L , (assuming the variables are ordered),
for j = 1, ... , n :
let s = ( 1,
, j-1 )
T
Lsj =L js =Lss B sj
Ljj = V j BsjT Lss Bsj .
. . .

+

2c. The evidence must now be instantiated. This can be performed in several ways, but the
theoretical process is represented by the two matrix equations:
E [XN I D] =E XN + LND LDD - 1 ( d -E D )
and

Var [X N I D]= I.NN- LND LDD-1 L D N .

2d. Finally, compute the estimated posterior value for each basic and deterministic parameter in
the model, using the inverse transform approximation,
( Et [Yj I D], Vart [Yj I D]) = Tfl ( Et· [Xj I D ], Vart [Xj I D]) .

Conclusions
The method presented here provides a simple, efficient framework for approximating
probabilistic inference over continuous distributions. The empirical evidence with the procedure
has shown it to ·be fairly accurate and fast when there is sufficient data. (It can have convergence
problems when the priors are flat and there is little experimental evidence.)
Some simple changes can improve the accuracy of the method. First, multiple (conditionally
independent) experimental evidence for the same parameter can be "pooled" into a single
experiment for the purposes of the approximation. Second, deterministic relationships which are
305

analytically linear can be recognized symbolically, and the corresponding regression coefficients

computed in advance. These include linear combinations of scaled variables, products of log­
scaled variables. and odds-ratios of logistic-scaled variables.
Finally, there is a useful byproduct of the linear approximation algorithm:

correlation between any two of the model parameters,
Corr [Xio X j I D]
= 0
=

Cov [Xi, Xj

ifVar [Xi I D]

an

=

estimate of the

0 or Var [Xj I D]

I D J ( Var [Xi I D] Var [Xj I D] )-112

=

0

otherwise.

This provides insight into the sensitivity of the posterior estimates to changes in prior
distributions or additional experjq1ental evidence.

Acknowle dgements
This research was supported by the John A. Hartford Foundation and the National Center for Health Services

Research under Grants 1R01-HS-05531-01 and 5R01-HS-05531-02. I thank my colleagues David Eddy, Vic
Hasselblad, and Robert Wolpert for their encouragement and collaboration.

