
Bayesian optimisation has gained great popularity as a tool for optimising the parameters of machine learning algorithms and models. Somewhat ironically, setting
up the hyper-parameters of Bayesian optimisation methods is notoriously hard.
While reasonable practical solutions have been advanced, they can often fail to
find the best optima. Surprisingly, there is little theoretical analysis of this crucial
problem in the literature. To address this, we derive a cumulative regret bound
for Bayesian optimisation with Gaussian processes and unknown kernel hyperparameters in the stochastic setting. The bound, which applies to the expected
improvement acquisition function and sub-Gaussian observation noise, provides
us with guidelines on how to design hyper-parameter estimation methods. A simple simulation demonstrates the importance of following these guidelines.

1

Introduction

Bayesian optimisation has become an important area of research and development in the field of
machine learning, as evidenced by recent media coverage [7] and a blossoming range of applications to interactive user-interfaces [4], robotics [16, 19], environmental monitoring [18], information
extraction [27], combinatorial optimisation [13, 28], automatic machine learning [3, 23, 25, 26, 11],
sensor networks [8, 24], adaptive Monte Carlo [17], experimental design [1] and reinforcement
learning [5].
In Bayesian optimisation, Gaussian processes are one of the preferred priors for quantifying the
uncertainty in the objective function [5]. However, estimating the hyper-parameters of the Gaussian
process kernel with very few objective function evaluations is a daunting task, often with disastrous
results as illustrated by a simple example in [2]. The typical estimation of the hyper-parameters by
maximising the marginal likelihood [22, 15] can easily fall into traps; as shown in [6]. To circumvent
this, [28] introduced adaptive bounds on the range of hyper-parameter values.
Several authors have proposed to integrate out the hyper-parameters using quadrature and Monte
Carlo methods [21, 4, 23]. Despite the advantages brought in by this more sophisticated treatment
of uncertainty, Bayesian optimisation can still fall in traps, as illustrated with a simple simulation
example in this paper.
To the best of our knowledge, the work of Bull [6] provides the only known regret bound for
Bayesian optimisation when the hyper-parameters are unknown. By introducing lower and upper
bounds on the possible range of hyper-parameters values, Bull obtains convergence rates for deterministic objective functions, when estimating the hyper-parameters by maximum likelihood. Here,
we extend the work of Bull to stochastic objective functions. Our results apply to sub-Gaussian
noise, e.g., symmetric Gaussian, Bernoulli, or uniform noise.
1

2

Bayesian optimisation

We consider a sequential decision approach to global optimization of smooth functions f (Â·) : X 7â†’
R over an index set X âŠ‚ Rd . At the t-th decision round, we select an input xt âˆˆ X and observe the
value of a black-box reward function f (xt ). The returned value may be deterministic, yt = f (xt ),
PT
or stochastic, yt = f (xt ) + t . Our goal is to maximise the cumulative rewards t=1 f (xt ). That
is, we wish to approach the performance of the optimiser xâˆ— = arg maxxâˆˆX f (x) rapidly. Since the
optimiser is unknown, we have to trade-off exploitation and exploration in the search process.
This sequential optimisation approach is natural when the function does not have an obvious mathematical representation (e.g., when querying people to maximize some objective) or when the function is too expensive to evaluate (e.g., as in control problems and automatic algorithm configuration
with massive datasets and models).
Although the function is unknown, we assume that it is smooth. It is natural to adopt a Bayesian
modelling approach whereby one introduces a prior to encode our beliefs over the smoothness of
the function, and an observation model to describe the data Dt = {x1:t , y1:t } up to the t-th round.
Using these two models and the rules of probability, we derive a posterior distribution p(f (Â·)|Dt )
from which we can carry out inference about properties of f (Â·) in light of the data, such as the
location of its maxima.
2.1

Bayesian optimisation with Gaussian processes

A Gaussian processes (GP) offer a flexible and relatively simple way of placing priors over functions;
we refer the reader to [22] for details on these stochastic processes. Such priors are completely
characterised by a mean function m(Â·) and a covariance kernel k(Â·, Â·) on the index sets X and
X âŠ— X . In particular, given any finite collection of inputs x1:t the outputs are jointly Gaussian,
f (x1:t )|Î¸ âˆ¼ N (m(x1:t ), KÎ¸ (x1:t , x1:t )),
where KÎ¸ (x1:t , x1:t )ij = k Î¸ (xi , xj ) is the covariance matrix (parametrised by Î¸) and m(x1:t )i =
m(xi ) the mean vector. For convenience, we assume a zero-mean prior. We consider the following
types of covariance kernels
Î¸
kSE
(x, x0 )

=

Î¸
kMateÌrn(5/2)
(x, x0 )

=

where r

=

exp(âˆ’ 12 r2 )
âˆš
âˆš
exp(âˆ’ 5r)(1 + 5r + 53 r2 )
2 âˆ’1

(x âˆ’ x0 )T diag(Î¸ )

(1)
(2)

(x âˆ’ x0 ).

Both kernels are parametrised by d length-scale hyper-parameters Î¸i . These kernels work well in
situations where little is known about the space in question, although the MateÌrn tends to make less
stringent smoothness assumptions, thus making it a good fit for Bayesian optimization.
We assume that the observations of the function at any point xt are corrupted by Ïƒ-sub-Gaussian
noise yt = f (xt ) + t . Our theoretical results cover this general type of noise, which encompasses
symmetric Gaussian and Bernoulli noise. However, for ease of presentation, we will focus on the
tractable case of Gaussian noise t âˆ¼ N (0, Ïƒ 2 ) in this section. We refer the reader to [4] for an
example of discrete noise, which necessitates the introduction of approximate inference methods.
Given the data Dt = {x1:t , y1:t }, the joint distribution of the data and an arbitrary evaluation point
x is


  Î¸

y1:t
Kt + Ïƒ 2 I kÎ¸t (x)
Î¸ âˆ¼ N 0,
.
f (x)
kÎ¸t (x)T
k Î¸ (x, x)
where KÎ¸t = KÎ¸ (x1:t , x1:t ) and kÎ¸t (x) = kÎ¸ (x1:t , x). It is well known that the predictive posterior
distribution of any evaluation point x is marginally Gaussian f (x)|Dt , Î¸ âˆ¼ N (Âµt (x; Î¸), Ïƒt (x; Î¸))2 ,
where
Âµt (x; Î¸)
KtÎ¸ (x, x0 )
Ïƒt (x; Î¸)2

=

E [f (x)|Dt ] = kÎ¸t (x)T (KÎ¸t + Ïƒ 2 I)âˆ’1 y1:t ,
0

Î¸

0

= E [f (x)f (x )|Dt ] = k (x, x ) âˆ’
=

KtÎ¸ (x, x).

kÎ¸t (x)T (KÎ¸t

(3)
+Ïƒ

2

I)âˆ’1 kÎ¸t (x0 )

(4)
(5)

2

Having specified a distribution over the target function and a mechanism for updating this distribution as data arrives, we turn our attention to the problem of selecting an acquisition function Î±(Â·|Dt )
for choosing the next query point,
xt+1 = arg max Î±(x|Dt ).
xâˆˆX

The choice of acquisition function is crucial. It must be efficiently computable since it will be
optimized at every decision round. More subtly, it must use the statistics of p(f (x)|Dt , Î¸) to tradeoff exploitation (where Âµt (x; Î¸) is high) and exploration (where Ïƒt (x; Î¸) is high) effectively.
Although many acquisition functions have been proposed (see for example [20, 14, 10, 9, 23, 11]),
the expected improvement (EI) criterion remains a default choice in popular Bayesian optimisation
packages, such as SMAC and Spearmint [13, 23]. If we let x+
t = arg maxiâ‰¤t f (xi ; Î¸) denote the
current incumbent, the EI acquisition function can be written in closed form as
Î±Î¸EI(f) (x|Dt ) = E[max{0, f (x) âˆ’ f (x+ )}|Dt ] = Ïƒt (x; Î¸)[aÎ¦(a) + Ï†(a)]
with a =

Âµt (x;Î¸)âˆ’f (x+ )
,
Ïƒt (x;Î¸)

(6)

and Ï† and Î¦ are the standard normal density and distribution functions

respectively. In the special case of Ïƒt (x; Î¸) = 0, we set Î±Î¸EI(f) (x|Dt ) = 0. The expected improvement is best understood as a family of one-step-decision heuristics [5], with many members in this
family. While the above member is reasonable for deterministic optimization, the noise in the evaluation of the incumbent, f (x+ ), causes it to be brittle in the stochastic case. In the stochastic setting,
the improvement over the best mean value Âµ+
Î¸ = maxxâˆˆX Âµt (x; Î¸) seems to be a more reasonable
alternative. For this choice, we obtain a similar expression for EI,
Î±Î¸EI(Âµ) (x|Dt ) = E[max{0, f (x) âˆ’ Âµ+
Î¸ }|Dt ] = Ïƒt (x; Î¸)[uÎ¦(u) + Ï†(u)],
where u =

Âµt (x;Î¸)âˆ’Âµ+
Î¸
Ïƒt (x;Î¸)

(7)

. In this paper, we will consider a re-scaled version of this criterion:

u
u u
(8)
Î±Î¸EI (x|Dt ) = E[max{0, f (x) âˆ’ Âµ+
Î¸ }|Dt ] = Î½Ïƒt (x; Î¸)[ Î¦( ) + Ï†( )]
Î½ Î½
Î½
where Î½ is a parameter must be estimated. Intuitively, this parameter enables us to rescale the
kernel. In the deterministic case, it plays an equivalent role to multiplying the kernel by an unknown
coefficient Î½. (For notational simplicity, we are not making dependence of EI on Î½ explicitly in the
expression Î±Î¸EI (x|Dt ).)
2.2

An algorithm inspired by the theory

Our main theorem (Theorem 1) establishes sufficient conditions to guarantee that the regret of a
Bayesian optimisation algorithm with EI and hyper-parameter estimation, vanishes as the number
of function evaluations increases. To illustrate the value of Theorem 1, we use its guidelines to
construct an algorithm in this section.
For Theorem 1 to hold, it is necessary that we adapt the hyper-parameters in a particular manner.
First, we must ensure that that there exist upper-bounds on the hyper-parameters Î¸, which we group
in the vector Î¸ U ,such that the objective function f (Â·) is an element of the reproducing kernel Hilbert
space induced by this narrower kernel HÎ¸U (X ) (these spaces will be explained in Section 3.4).
Figure 1 (right) shows what happens to the confidence intervals as the entries of Î¸ U shrink with t,
by narrowing the kernel.
In practice, it is difficult to assess this condition. To surmount this difficulty, we draw inspiration
from [28], and propose to reduce the upper bound of the length scales Î¸ U when the algorithm
becomes over confident. In particular, we adaptively reduce Î¸ U whenever the model repeatedly
samples points of low posterior variance in comparison to the noise variance Ïƒ 2 . Once the algorithm
optimizes to the precision of the noise variance, it suffers from a slower convergence rate.
By choosing to lower the upper bound as proposed in Algorithm 1, we essentially enable the algorithm to explore more, as opposed to over-exploiting a local mode. This is illustrated in Figure 1,
which depicts the result of running the proposed algorithm and a standard Bayesian optimisation
scheme. We will explain the experiment in more detail at the end of this section.
3

Algorithm 1 Bayesian Optimization with Hyper-parameter Optimization.
input Threshold tÏƒ > 0, percentage of reduction parameter p âˆˆ (0, 1), and c2 > c1 > 0.
input Lower and upper bounds Î¸ L , Î¸ U for the hyper-parameters.
input Initial length scale hyper-parameter Î¸ L â‰¤ Î¸ 1 â‰¤ Î¸ U .
1: Initialize E = 0
2: for t = 1, 2, . . . do
3:
Select xt = arg maxxâˆˆX Î±Î¸EIt (x|Dtâˆ’1 )
2
4:
if Ïƒtâˆ’1
(xt ; Î¸t ) < tÏƒ Ïƒ 2 then
5:
E =E+1
6:
else
7:
E=0
8:
end if
9:
Augment the data Dt = Dtâˆ’1 âˆª (xt , yt )
10:
if E = 5 then



11:
Restrict Î¸ U such that Î¸iU = max min p maxj {Î¸jU }, Î¸iU , Î¸iL
12:
E=0
13:
end if
14:
Choose hyper-parameters Î¸ t+1 such that Î¸ L â‰¤ Î¸ t+1 â‰¤ Î¸ U .
Î¸
Î¸ t+1
Î¸ t+1
Î¸ t+1
15:
Choose hyper-parameter Î½t t+1 such that c1 Î¾t+1
â‰¤ Î½t+1
â‰¤ c2 Î¾t+1
, where Î¾tÎ¸t is defined in
Equation (9).
16: end for
As Î¸ U is successively decreased, after a finite number of iterations, we can ensure that f (Â·) âˆˆ
HÎ¸U (X ) as long as there exists Î¸ â‰¥ Î¸ L such that f (Â·) âˆˆ HÎ¸ (X ). In practice, we advocate a
conservative choice of Î¸ L whenever we have little knowledge of the range of possible values of Î¸.
Theorem 1 also imposes a condition on Î½. To satisfy it, we constrain Î½tÎ¸t to be in the interval
c1 Î¾tÎ¸t â‰¤ Î½tÎ¸t â‰¤ c2 Î¾tÎ¸t , where


p
Î¾tÎ¸t = IÎ¸t (ytâˆ’1 ; ftâˆ’1 ) + log1/2 (2t2 Ï€ 2 /3Î´) IÎ¸t (ytâˆ’1 ; ftâˆ’1 ) + log(t2 Ï€ 2 /3Î´) .
(9)
(The information gain will be defined in Section 3.3.) The careful reader may have noticed that
the above condition does not match perfectly the condition detailed in Theorem 1. Upon closer
examination, however, we see that replacing the maximum information gain Î³TÎ¸ with IÎ¸ (yT ; fT )
does not break the convergence result. We have used Î³TÎ¸ in Theorem 1 instead of IÎ¸ (yT ; fT ) simply
to simplify the presentation.
In practice, we could use a number of strategies for estimating the hyper-parameters, provided they
fall within the bounds set by Theorem 1. In particular, we could use maximum likelihood to estimate
the hyper-parameters in this constrained space. Note that the Î½ parameter could also be treated as a
kernel hyper-parameter (kernel scale), therefore removing the need of estimating it separately.
Finally, the astute reader would have noticed the parameters tÏƒ , p, c2 and c1 in the algorithm. If
we want to achieve an accuracy comparable to the noise variance, we should set tÏƒ = 1. The other
parameters simply determine how fast the algorithm converges and should be set to reasonable fixed
values, e.g. p = 0.5, c2 = 1 and c1 = 0.001. Provided tÏƒ > 0, p âˆˆ (0, 1) and c2 > c1 > 0, the
theory is satisfied.
If we have strong beliefs about our GP prior model, it may seem unnecessary to estimate our parameters with Algorithm 1. When our prior belief is misplaced, however, we could fail to converge if
we were to follow the traditional probabilistic approach. We provide an illustration of this effect by
optimize the following stochastic function:
1
2
f (x) = 2kÎ¸SE
(x1 , x) + 4kÎ¸SE
(x2 , x) + 

over the interval [0, 1], where Î¸1 = 0.1, Î¸2 = 0.01, x1 = 0.1, x2 = 0.9, and  is zero-mean Gaussian
with 10âˆ’2 standard deviation. Figure 1 compares Algorithm 1 against standard Bayesian optimisation with the same EI function, but using slice sampling to infer the kernel hyper-parameters (without
imposing the theoretical bounds on the hyper-parameters). We see that, in the absence of reasonable
4

4

4

3

3

2

2

1

1

0

0

1

1

2
0.0

0.2

0.4

0.6

0.8

2
0.0

1.0

4

4

3

3

2

2

1

1

0

0

1

1

2
0.0

2
0.0

0.2

0.4

0.6

0.8

1.0

4

4

3

3

2

2

1

1

0

0

1

1

2
0.0

0.2

0.4

0.6

0.8

2
0.0

1.0

t = 20

0.2

0.4

0.6

0.8

1.0

t = 40

0.2

0.4

0.6

0.8

1.0

t = 60

0.2

0.4

0.6

0.8

1.0

Figure 1: Convergence of EI with slice sampling over the kernel hyper-parameters [left] and EI
using Algorithm 1 [right] at three function evaluation steps (t). The objective function (in blue)
was constructed so that it has a trap. Unless EI with slice sampling hits the narrow optimum by
random chance, it becomes too confident and fails to converge after 60 evaluations. In contrast,
the confidence bounds for Algorithm 1 can increase enabling it to sample the function in a more
reasonable way and thus find the optimum.

prior beliefs, conditions like the ones detailed in our theoretical results are necessary to guarantee
reasonable sampling of the objective function. (The same behaviour for the plot on the left is observed if we replace slice sampling with maximum likelihood estimation of the hyper-parameters.)
While heteroskedastic GP approaches could mitigate this problem, there are no theoretical results to
guarantee this to the best of our knowledge.

3

Theoretical analysis

Our theoretical analysis uses regret to measure convergence and information gain to measure how
informative the samples are about f (Â·). It assumes that the noise process t is sub-Gaussian, and
that the function f (Â·) is smooth according to the reproducing kernel Hilbert space (RKHS) associated with the GP kernel k Î¸ (Â·, Â·). Before presenting our main result, we briefly review these four
background areas.
5

3.1

Background: Regret

As in [24], we will measure the performance of the Bayesian optimization algorithm using regret.
The instantaneous regret at iteration t is defined as rt = f (xâˆ— ) âˆ’ f (xt ). The corresponding cumulaPT
tive regret after T iterations is RT = t=1 rt . While the regret measures are never revealed to the
algorithm, bounds on these enable us to assess how rapidly the algorithm is converging.
3.2

Background: Sub-Gaussian noise

We assume independent Ïƒ-sub-Gaussian noise. Formally, we say t is Ïƒ-sub-Gaussian if there exists
a Ïƒ â‰¥ 0 such that
 2 2
Ï Ïƒ
E [exp(Ït )] â‰¤ exp
âˆ€Ï âˆˆ R.
2
In other works, t is Ïƒ-sub-Gaussian if its Laplace transform is dominated by the Laplace transform
of a Gaussian random variable with zero mean and variance Ïƒ 2 . It is easy to show that if t is
sub-Gaussian, then E[t ] = 0 and Var[t ] â‰¤ Ïƒ 2 .
There are many examples of sub-Gaussian variables, including zero-mean Gaussian random variables with variance Ïƒ 2 , symmetric Bernoulli random variables and symmetric uniform distributions.
3.3

Background: Information gain

To measure the reduction in uncertainty about f (Â·) from observing yA for a set of sampling points
A âŠ‚ X , we need to introduce the concept of information gain, which is defined as the mutual
information between f (Â·) and a set of observations yA :
I(yA ; f (Â·)) = H(yA ) âˆ’ H(yA |f (Â·)).

(10)

This concept plays a central role in the results of [24], who also define the maximum information
gain Î³T after T decision rounds as
Î³T =

I(y1:t ; f (Â·)).

(11)

1
log |I + Ïƒ âˆ’2 KÎ¸A |.
2

(12)

max

AâŠ‚X :|A|=T

Note that for Gaussian distributions,
Î³TÎ¸ =

max

AâŠ‚X :|A|=T

Our regret bounds will be given in terms of Î³TÎ¸ . It should perhaps be clarified that the bounds apply
to Ïƒ-sub-Gaussian noise, despite the appearance of the variable Î³TÎ¸ in their statements.
3.4

Background: Reproducing kernel Hilbert spaces

To discuss convergence, we must state formally what we mean by f (Â·) being smooth. In short, we
assume that f (Â·) is an element of an RKHS with reproducing kernel k(Â·, Â·). For an intuitive grasp
of this formalisation of smoothness, we need to briefly review some RKHS fundamentals. These
fundamentals are also evoked in our proofs.
Let Lx be an evaluation functional: Lx f (Â·) = f (x). A (real) RKHS H is a Hilbert space of real
valued functions with the property that for each x âˆˆ X , the evaluation functional is bounded. That
is, there exists a positive constant M = Mx such that |Lx f (Â·)| = |f (x)| â‰¤ M kf (Â·)kH for all
functions f (Â·) âˆˆ H, where k Â· kH denotes the norm in the Hilbert space. If H is an RKHS, by the
Riesz Representation Theorem, there exists an element k(Â·, x) âˆˆ H with the property,
f (x) = Lx f (Â·) = hk(Â·, x), f (Â·)i

(13)

for all x âˆˆ X and f (Â·) âˆˆ H, where hÂ·, Â·i denotes the inner product in H.
Pn
To construct H, we consider the linear manifold t=1 Î»t k(Â·, xt ) for all choices of n, Î»1 , . . . , Î»n
and x1 , . . . , xn âˆˆ X , with inner product
n
n
n X
n
n
X
X
X
X
h
Î»i k(Â·, xi ),
Î»j k(Â·, xj )i =
Î»i k(xi , xj )Î»j = k
Î»i k(Â·, xi )k2H â‰¥ 0.
i=1

j=1

i=1 j=1

i=1

6

(14)

The above norm is non-negative because of the positive-definiteness of k(Â·, Â·). Clearly, for any
element f (Â·) of this linear manifold,
n
n
X
X
f (xj ) = hf (Â·), k(Â·, xj )i = h
Î»i k(Â·, xi ), k(Â·, xj )i =
Î»i k(xi , xj )
i=1

(15)

i=1

A consequence of this is that for any Cauchy sequence {fn (Â·)}, we have the following bound by
Cauchy-Schwartz: |fn (x)âˆ’f (x)| = hfn (Â·)âˆ’f (Â·), k(Â·, x)i â‰¤ kfn (Â·)âˆ’f (Â·)kH kk(Â·, x)kH . In words,
norm convergence implies point-wise convergence.
The preceding steps illustrate that we can construct a unique RKHS for any positive definite kernel
k(Â·, Â·). The converse is also true (Moore-Aronszajn Theorem).
A positive definite function k(Â·, Â·), under general Rconditions,
has an eigenvector-eigenvalue deR
composition. Suppose k(Â·, Â·) is continuous and X X k 2 (x, y)dxdy < âˆ, then there exists
an orthonormal sequence of continuous
eigenfunctions q1 (Â·), q2 (Â·), . . . and eigenvalues âˆ†1 â‰¥
Pâˆ
âˆ†2 â‰¥ . P
. . â‰¥ 0, with k(x, y) = Î½=1 âˆ†Î½ qÎ½ (x)q
R Î½ (y). Next, consider the orthonormal expansion
âˆ
f (Â·) = Î½=1 fÎ½ qÎ½ (Â·) with coefficients fÎ½ = X f (x)qÎ½ (x)dx. It is easy to prove that f (Â·) is an
element of the RKHS associated with k(Â·, Â·) if and only if
kf (Â·)k2H =

âˆ
X
fÎ½2
< âˆ.
âˆ†Î½
Î½=1

(16)

To obtain the above finiteness condition, the coefficients fÎ½ of the expansion of f (Â·) must decay
quickly. For the kernels we consider in this paper, elements of RKHS can uniformly approximate any
continuous function with compact support. Therefore, RKHS is well suited as a tool for analyzing
convergence behaviors of Bayesian optimization algorithms.
3.5

Main result

In this section, we present our regret bound and sketch its proof. For space considerations, detailed
proofs appear in the appendix provided in the supplementary material.
As discussed when presenting the algorithm, our theorem assumes bounds on the kernel hyperparameters of the form Î¸ L â‰¤ Î¸ t â‰¤ Î¸ U for all t â‰¥ 1 with f (Â·) âˆˆ HÎ¸U (X ). While we could recall
all the conditions on the kernel function necessary for our theorem to apply, we simply restrict the
family of kernels to one that satisfies the conditions detailed in [6]. Without loss of generality, we
assume that k(x, x)= 1.
Our theorem characterising the growth in the cumulative regret RT with the number of function
evaluations T follows.
Qd Î¸U
Theorem 1. Let C2 := i=1 Î¸iL . Suppose Î¸ L â‰¤ Î¸ t â‰¤ Î¸ U for all t â‰¥ 1 and f (Â·) âˆˆ HÎ¸U (X ). If
i
q



1/2
Î¸ 2
Î¸
2 2
Î¸
Î½t = Î˜ Î³tâˆ’1 + log (2t Ï€ /3Î´) Î³tâˆ’1
+ log(t2 Ï€ 2 /3Î´) for all t â‰¥ 1. Then with probability at least 1 âˆ’ Î´, the cumulative regret obeys the following rate:
 q

L
Î¸
RT = O Î²T Î³T T ,
(17)
where Î²T = 2 log
C2 kf k2H

Î¸U

T
Ïƒ2



L

Î³TÎ¸ âˆ’1 +

âˆš

8 log

T
Ïƒ2



log1/2 (4T 2 Ï€ 2 /6Î´)


âˆš

C2 kf kHÎ¸U (X ) +


q
L
Î³TÎ¸ âˆ’1 +

(X ) .

Our result is analogous to Theorem 3 of [24] which proves convergence rates for the GP-UCB
algorithm in the agnostic setting. Their result, however, does not allow for the estimation of hyperparameters. In addition, our algorithm does not require explicit knowledge of the RKHS norm of
the objective function while GP-UCB does require this.
Using the results of Srinivas et al. we can further detail these rates as follows.
Theorem 2 (Theorem 5 of [24]). Let X âŠ† Rd be compact and convex, d âˆˆ N. Assume the kernel
function satisfies k(x, x0 ) â‰¤ 1.
7


1. Exponential spectral decay. For the squared Exponential kernel: Î³TÎ¸ = O (log T )d+1 .
Î¸
2. Power law spectral decay. For
 MateÌrn kernels with degree of freedom Î½ > 1: Î³T =
d(d+1)/(2Î½+d(d+1))
O T
log T .

The proof of Theorem 1 is provided in the appendix. We sketch the main ideas here. Our proof
methodology is inspired by the works of [24] and [6].
We start the proof-sketch by considering the instantaneous regret:
rt

= f (xâˆ— ) âˆ’ f (xt )
=

+
(f (xâˆ— ) âˆ’ Âµ+
Î¸ t ) âˆ’ (f (xt ) âˆ’ ÂµÎ¸ t )

EI
where Âµ+
Î¸ t = maxxâˆˆX Âµtâˆ’1 (x; Î¸ t ) and xt = arg maxxâˆˆX Î±Î¸ t (x|Dtâˆ’1 ). T

The first challenge of the proof is to bound the difference between the posterior mean of the GP and
the objective function. Such a bound allows us to quantify the difference between our belief about
the objective function and the true objective. Specifically, we bound |Âµtâˆ’1 (x, Î¸ t ) âˆ’ f (x)| âˆ€x âˆˆ X
in each iteration with high probability. By way of the Cauchy-Schwarz inequality,

1/2
Î¸t
|Âµtâˆ’1 (x, Î¸ t ) âˆ’ f (x)| â‰¤
Ktâˆ’1
(x, x)
kÂµtâˆ’1 (Â·; Î¸) âˆ’ f (Â·)kKÎ¸t
tâˆ’1

â‰¤

Ïƒtâˆ’1 (x; Î¸ t )kÂµtâˆ’1 (Â·; Î¸) âˆ’ f (Â·)kKÎ¸t .
tâˆ’1

The first part of our proof (Section A.1 in the appendix) is then dedicated to providing a probabilistic bound for kÂµtâˆ’1 (Â·; Î¸) âˆ’ f (Â·)kKÎ¸t by means of concentration inequalities. In more detail,
tâˆ’1

Lemma 3 and 4 bound separate terms that appear in kÂµtâˆ’1 (Â·; Î¸) âˆ’ f (Â·)kKÎ¸t using properties of retâˆ’1
producing kernel Hilbert spaces and concentration results for sub-Gaussian random variables [12].
Proposition 1 combines the aforementioned results via a union bound.
The second challenge of the proof is to relate EI with quantities that are easier to analyse, such as
the posterior variance and the improvement function ItÎ¸ (x) = max{0, f (x) âˆ’ Âµ+
Î¸ }. To bound the
instantaneous regret, we observe that
i
h
Î¸t
+
+
Î¸
Ïƒ
(x
;
Î¸
)
.
âˆ’
Âµ
(x
;
Î¸
))
+
Ï•
)
â‰¤
I
(x)
+
(Âµ
)
âˆ’
(f
(x
)
âˆ’
Âµ
(f (xâˆ— ) âˆ’ Âµ+
tâˆ’1
t
t
tâˆ’1
t
t
t
t
t
Î¸t
Î¸t
Î¸t
(Here Ï•Î¸t t is a quantity that arises in the concentration bound of Proposition 1.) The improvement function is upper-bounded by a constant times the expected improvement Î±Î¸EIt (xt |Dtâˆ’1 ) via
Lemma 9, which builds on results by [6]. The expected improvement is in turn bounded by a multiple of the posterior standard deviation Ïƒtâˆ’1 (xt ; Î¸ t ).
Next, we turn our attention to the term (Âµ+
âˆ’ Âµtâˆ’1 (xt ; Î¸ t )). We bound this term in Lemma 10,
pÎ¸t
log(t
âˆ’ 1 + Ïƒ 2 ) âˆ’ log(Ïƒ 2 )Î½Ïƒtâˆ’1 (xt ; Î¸ t ).
which states that Âµtâˆ’1 (xt ; Î¸ t ) âˆ’ Âµ+
â‰¤
Î¸t
By now, we have bounded the instantaneous regret in each iteration by a multiple of the pos2
terior variance Ïƒtâˆ’1
(x; Î¸ L ). Finally, we can sum over t and use Lemma 7, which states that
PT
L
2
2
Î¸L
t=t0 Ïƒtâˆ’1 (x; Î¸ ) â‰¤ log(1+Ïƒ 2 ) Î³T , and subsequently bound the cumulative regret by the maximal information gain, which as we said is related to the posterior variance of the GP (Lemma 5). To
accommodate different hyper-parameters, we make use of Lemma 8 from [6].

4

Conclusion

Despite the rapidly growing literature on Bayesian optimisation and the proliferation of software
packages that learn the kernel hyper-parameters, to the best of our knowledge, only Bull [6] and us
have attacked the question of convergence of GP-based Bayesian optimisation with unknown hyperparameters. Bullâ€™s results focused on deterministic objective functions. Our new results apply to the
abundant class of noisy objective functions.

8


